Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  A Comparison of Statistical and Rule-Induction Learners for Automatic Tagging of Time Expressions in English 14th International Symposium on Temporal Representation and Reasoning (TIME'07)  Jordi Poveda, Mihai Surdeanu and Jordi Turmo TALP Research Center Technical University of Catalonia (UPC) Barcelona, Spain {jpoveda,surdeanu,turmo}@lsi.upc.edu  June 28st, 2007  Conclusions  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  1  Time Expression Recognition Information Extraction TERN (Time Expression Recognition and Normalization)  2  Machine Learning for TE Recognition Problem description: Chunking Statistical: Support Vector Machines Rule Induction: Inductive Logic Programming  3  Results Experiments Support Vector Machines Inductive Logic Programming Comparison  4  Conclusions  Conclusions  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  1  Time Expression Recognition Information Extraction TERN (Time Expression Recognition and Normalization)  2  Machine Learning for TE Recognition Problem description: Chunking Statistical: Support Vector Machines Rule Induction: Inductive Logic Programming  3  Results Experiments Support Vector Machines Inductive Logic Programming Comparison  4  Conclusions  Conclusions  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  Conclusions  Information Extraction  Example "Yesterday, German giant E.ON's board of directors announced plans for takeover of Spanish ENDESA for $20 million at an undisclosed date, just after receiving former CEO Bernotat's resignation notice."
from Reuters, 11-24-2005 1  1 takeover EVENT: takeover EVENT(id(EVENT1), acquirer(E.ON), target(ENDESA), amount($20 million), date(TIME1))  2  1 resignation EVENT: resignation EVENT(id(EVENT2), company(E.ON), person(Bernotat), position(CEO))  3  1 precedes RELATION: precedes(EVENT2, EVENT1)  4  2 time expressions: timex(id(TIME1), type(date), mention(an undisclosed date), value(??-??-????))
timex(id(TIME2), type(date), mention(yesterday), value(11-23-2005))  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  Conclusions  Information Extraction  Definition Information Extraction (IE) is a subtask in Natural Language Processing whose objective is extracting information from unstructured machine-readable documents and arranging it into an structured, processable form.
Information is usually represented in a relational form, or structured by using metadata such as XML tags.
Objectives: Populating relational databases Monitoring information sources within a domain (e.g.
news feeds on corporate mergers and acquisitions) Inference Structuring information for use in other NLP problems: QA (Question Answering), AS (Automatic Summarisation), .
.
.
Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  TERN (Time Expression Recognition and Normalization)  Example To identify (Recognition) the mentions in text of time-denoting expressions and to capture their meaning in a canonical form (Normalization) But even <TIMEX2 VAL="1999-07-22"> last Thursday </TIMEX2>, there were signs of potential battles <TIMEX2 VAL="FUTURE REF" ANCHOR DIR="AFTER" ANCHOR VAL="1999-07-22"> ahead </TIMEX2>.
L. Ferro, L. Gerber, I. Mani, B. Sundheim, and G. Wilson.
TIDES Standard for the Annotation of Temporal Expressions v1.3.
Technical Report, MITRE Corporation, 2003.
Conclusions  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  Conclusions  TERN (Time Expression Recognition and Normalization)  Examples of time expressions Fully-specified time references: 16th June 2006, the twentieth century, Monday at 3pm  Context-dependent: the previous month, three days after the meeting, February the following year  Anaphoric and relative to the time when the expression is written: that day, yesterday, currently, then  Durations or intervals: a month, three days, some hours in the afternoon  Frequencies or recurring times: monthly, every other day, once a week, every first Sunday of a month  Culturally dependent time denominations: Easter, the month of Ramadan, St. Valentine  Fuzzy or vaguely specified time references: the future, some day, eventually, anytime you so desire  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  1  Time Expression Recognition Information Extraction TERN (Time Expression Recognition and Normalization)  2  Machine Learning for TE Recognition Problem description: Chunking Statistical: Support Vector Machines Rule Induction: Inductive Logic Programming  3  Results Experiments Support Vector Machines Inductive Logic Programming Comparison  4  Conclusions  Conclusions  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  Conclusions  Problem description: Chunking  Chunking  Chunking: Assigning B (begin), I (inside), O (outside) tags to each token in a sequence But/O even/O last/B Thursday/I ,/O there/O were/O signs/O of/O potential/O battles/O ahead/B ./O Limited to non-overlapping, non-recursive chunks (i.e.
a chunk inside a longer chunk) Chunk need not be bounded in length  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  Conclusions  Problem description: Chunking  Token features (I) Lexical: Token form, token in lowercase, token w/o alphabetic chars (e.g.
3 for "3pm"), the token w/o alphanumeric chars (e.g.
- - - for 1995-07-12)  Morphological: POS (Part Of Speech) tag (e.g.
NN - noun, JJ - adjective, CD - cardinal number, MD - modal verb)  Syntactic: Basic syntactic chunk type (e.g.
I-NP - inside noun phrase, B-VP - beginning of verb phrase, .
.
. )
Format features: 1 2 3 4 5  isAllCaps like "THU" isAllCapsOrDots like "I.B.M" isAllDigits like "2004" isAllDigitsOrDots like "10.24" initialCap like "February"  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  Conclusions  Problem description: Chunking  Token features (II)  Bag-of-words: 1 2 3 4  isNumber (e.g.
one, two, ten, .
.
. )
isMultiplier (e.g.
hundred, thousands, .
.
. )
isDay (e.g.
monday, mon, saturday, sat, .
.
. )
isMonth (e.g.
january, jan, june, jun., .
.
. )
Contextual features: All of the above features, w.r.t.
context tokens Dynamic features: The BIO tags for a window of previous tokens  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  Conclusions  Statistical: Support Vector Machines  YamCha (Yet Another Multi-purpose CHunk Annotator) YamCha 1 : Multipurpose chunker based on SVM (Vapnik, 1995) SVMs: Max-margin discriminative classifiers based on quadratic optimization Map a feature vector into a vector space of higher dimension, exploring combinations of features ("kernel trick") Requires with numeric features: 1 categorial feature with N tags - N binary features One-vs-rest classification: Train 3 classifiers (B against I/O, I against B/O, O against B/I) Classifiers' outputs are combined based on margins and previous tokens 1  http://chasen.org/~taku/software/yamcha/  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  Statistical: Support Vector Machines  Sample training data  POS: POS: POS: POS: POS: POS: POS:  -3 -2 -1 0 +1 +2 +3  FORM But even last Thursday , there were  POS tag CC RB JJ NNP , EX VBD  SYNTAX O B-ADVP B-NP I-NP O B-NP B-VP  BIO tag O O B-TIMEX I-TIMEX O O O  Conclusions  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  Conclusions  Rule Induction: Inductive Logic Programming  Training: FOIL Inductive Logic Programming (ILP) attempts to learn a logic program for a set of target concepts from: Target predicates: pi (X1 , .
.
.
, Xni ) Examples and counterexamples E: ground facts < x1 , .
.
.
, xn > Background knowledge predicates B: qi (X1 , .
.
.
, Xmi ) Hypothesis language L  FOIL: An empirical (top-down, non-interactive) ILP system (Quinlan, 1993) The hypothesis language of FOIL are Horn clauses without functions Train 3 objective predicates: one for B (begin), one for I (inside), one for O (outside) Background knowledge predicates are the token features described earlier  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  Conclusions  Rule Induction: Inductive Logic Programming  Sample input and output Input: form last(tok100).
// token 100 is 'last' form Thursday(tok101).
// token 101 is 'Thursday' POS NNP(tok101).
// token 101 is a proper noun syn I NP(tok101).
// token 101 is inside a noun phrase context r1 form Thursday(tok100).
// token right of tok100 is 'Thursday' context l1 B NP(tok101).
// token left of tok101 is at the start of a noun phrase ...  Output: begin timex(X) :- form Thursday(X).
begin timex(X) :- syn I NP(X), context l1 B PP(X), not(context l1 form(with)).
... inside timex(X) :- form ago(X), context l2 POS CD(X).
inside timex(X) :- POS CD(X), not(context l1 t O(X)).
...  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  Conclusions  Rule Induction: Inductive Logic Programming  Evaluation: PROLOG For evaluation, load learned predicates into PROLOG and a knowledge base with declarations of all the token features in the test data More than one predicate B/I/O can return yes for a given token - Combination of classifiers' outputs Assign a confidence to each learned rule (supporting evidence): conf (A = B) = #(A[?
]B) #B Two approaches: 1  2  "best" - Take conf (A = B) to be that of best clause satifistied by token "sum" - Take conf (A = B) to be the sum of confidences of all satisfied clauses  Enforce consistency rule: I cannot follow O or be the first tag beginning a sentence If all three B/I/O return no - assign most probable (O)  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  1  Time Expression Recognition Information Extraction TERN (Time Expression Recognition and Normalization)  2  Machine Learning for TE Recognition Problem description: Chunking Statistical: Support Vector Machines Rule Induction: Inductive Logic Programming  3  Results Experiments Support Vector Machines Inductive Logic Programming Comparison  4  Conclusions  Conclusions  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  Conclusions  Experiments  Corpus  ACE (Automatic Content Extraction) 2005 corpus 550 documents from five categories (NW, BN, BC, CTS and WL) 257K tokens, 8809 tokens in time expressions (3.42%), 4650 time expression mentions 80% for training, 20% for testing  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Experiments  Experiments  Support Vector Machines (YamCha): temp.
cost for training = 8 +- 4 hours 1 2 3 4  5-fold cross-validation with optimal parameters Incremental feature sets Varying kernel degree (1 .
.
.
3) Varying context window size (1 .
.
.
3)  ILP (FOIL): temp.
cost for training = in the order of weeks 1 2  Same optimal parameters as SVM Simplifying the training data  Results  Conclusions  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  Conclusions  Experiments  Measures Precision: The rate of returned temporal expressions that are correctly identified (i.e.
correctly tagged divided by total tagged).
Recall: The rate of existing temporal expressions that are correctly identified (i.e.
correctly tagged divided by those that should have been tagged).
F1 Score: It is the harmonic mean of the two previous values: F1 = 2xPrecisionxRecall (Precision+Recall) .
Accuracy: The percentage of correct BIO tag assignments predicted by the classifier at the token level (i.e.
whether the predicted tag coincides with the target tag).
Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  Support Vector Machines  Optimal Model  Round 1 Round 2 Round 3 Round 4 Round 5 AVERAGE STD DEV.
PREC 81.33 77.74 75.92 80.05 80.34 79.08 2.20  RECALL 75.23 70.46 71.22 73.71 72.54 72.63 1.91  F1 78.16 73.92 73.50 76.75 76.24 75.71 1.97  ACC 98.68 98.60 98.47 98.65 98.66 98.61 0.17  Conclusions  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  Conclusions  Support Vector Machines  Degree of polynomial kernel  KERNEL pol.
lineal pol.
quadratic pol.
cubic  PREC 72.39 (-7.66) 80.05 81.30 (+1.25)  RECALL 70.08 (-3.63) 73.71 71.73 (-1.98)  F1 71.21 (-5.54) 76.75 76.21 (-0.54)  ACC 98.25 (-0.40) 98.65 98.65 (+0.00)  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  Conclusions  Support Vector Machines  Incremental Feature Sets  FEATURES Model 1 Model 2 Model 3  PREC 80.00 (-0.05) 80.10 (+0.05) 80.05  RECALL 66.89 (-6.82) 71.73 (-1.98) 73.71  F1 72.86 (-3.89) 75.68 (-1.07) 76.75  ACC 98.56 (-0.09) 98.60 (-0.05) 98.65  Model 1: token form + lowercase Model 2: Model 1 + POS tags + format features (isAllCaps, isAllDigits, etc) + form w/o alphabetic chars + form w/o alphanumeric chars Model 3: Model 2 + syntactic chunks + bag-of-words (isNumber, isMultiplier, isDay, isMonth)  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  Support Vector Machines  Context window size  WINDOW -1 .. +1 -2 .. +2 -3 .. +3  PREC 74.47 (-5.58) 80.05 80.30 (+0.25)  RECALL 72.83 (-0.88) 73.71 71.29 (-2.42)  F1 73.64 (-3.11) 76.75 75.52 (-1.23)  ACC 98.41 (-0.24) 98.65 98.59 (-0.06)  Conclusions  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  Conclusions  Inductive Logic Programming  Optimal model (for SVM)  CLASSIFIER FOIL (best) FOIL (sum)  PREC 77.58 81.32  RECALL 52.15 50.28  F1 62.37 62.13  ACC 97.95 97.98  best - Take conf (A = B) to be that of best clause satifistied by token sum - Take conf (A = B) to be the sum of confidences of all satisfied clauses  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  Conclusions  Inductive Logic Programming  Reducing model complexity  Unaffordable temporal complexity with the full model (over 3 21 weeks each classifier B/I/O) With 1-arity predicates, FOIL's complexity is quadratic on kBk (predicates) and kEk (examples) Reducing the volume of the training data: 1 2  Filtering less common predicates Filtering less relevant counterexamples  Temporal cost considerably reduced (in the order of days), at the expense of approx.
-8% prec/recall  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  Conclusions  Comparison  SVM and ILP side by side  CLASSIFIER FOIL SVM  PREC 81.32 (+1.27) 80.05  SVM clearly superior  RECALL 52.15 (-21.56) 73.71  F1 62.37 (-14.38) 76.75  ACC 97.98 (-0.67) 98.65  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  1  Time Expression Recognition Information Extraction TERN (Time Expression Recognition and Normalization)  2  Machine Learning for TE Recognition Problem description: Chunking Statistical: Support Vector Machines Rule Induction: Inductive Logic Programming  3  Results Experiments Support Vector Machines Inductive Logic Programming Comparison  4  Conclusions  Conclusions  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Results  Conclusions  Final thoughts ILP is a dead end: elegant representation for "toy" problems and/or small datasets, unusable for large corpora Alternatives approaches for rule induction: Statistical Rule Learning, simpler rule languages (propositional, N-term clauses), semi-supervised IE pattern learning Combination methods (Statistical + Rules) Machine-learning vs. grammar-based approaches (complementary?)
Best performance with statistical ML around 80% (depending on "feature engineering" and training corpus size) Best performance with handwritten grammars around 90%-95% Difficult to define a grammar to cover difficult cases ("easy" cases account for a majority) Grammars must be specifically written for each new extraction domain On the other hand, Normalization lends itself to the grammar approach  Outline  Time Expression Recognition  Machine Learning for TE Recognition  Thanks  Many thanks for your attention Any questions?
Comments?
Results  Conclusions