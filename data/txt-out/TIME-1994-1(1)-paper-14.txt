Belief Revision in a Discrete Temporal Probability-Logic  Scott D. Goodwin  Department of Computer Science University of Regina, Canada  Howard J. Hamilton  Department of Computer Science University of Regina, Canada  Abdul Sattar  School of Computer & Information Technology Grifith University, Australia  Abstract  We describe a discrete time probabilitylogic for use as the representation language of a temporal knowledge base.
In addition to the usual expressive power of a discrete temporal logic, our language allows for the specication of non-universal generalizations in the form of statistical assertions.
This is similar to the probability-logic of Bacchus, but diers in the inference mechanisms.
In particular, we discuss two interesting and related forms of inductive inference: interpolation and extrapolation.
Interpolation involves inferences about a time interval or point contained within an interval for which we have relevant statistical information.
Extrapolation extends statistical knowledge beyond the interval to which it pertains.
These inferences can be studied within a static temporal knowledge base, but the further complexity of dynamically accounting for new observations makes matters even more interesting.
This problem can be viewed as one of belief revision in that new observations may conict with current beliefs which require updating.
As a rst step toward a fulledged temporal belief revision system, we consider the tools of inductive logic.
We suggest that Carnap's method of conrmation may serve as a simple mechanism for belief revision.
1 Introduction  Standard discrete temporal logics allow the representation of what is true at a point, in a situation, or over an interval.
To introduce uncertainty, many researchers in AI have turned to nonmonotonic \logics," but semantic and computational diculties have led some to consider probability as a representational device.
Here we describe a discrete time probabilitylogic for use as the representation language of a tem-  Eric Neufeld  Department of Computational Science University of Saskatchewan, Canada  Andre Trudel  Jodrey School of Computer Science Acadia University, Canada  poral knowledge base.
In addition to the usual expressive power of a discrete temporal logic, our language allows for the specication of non-universal generalizations in the form of statistical assertions.
This is similar to the probability-logic of Bacchus 1], but diers in the inference mechanisms.
In particular, we discuss two interesting and related forms of inductive inference: interpolation and extrapolation.
Interpolation involves inferences about a time interval or point contained within an interval for which we have relevant statistical information.
Extrapolation extends statistical knowledge beyond the interval to which it pertains.
These inferences can be studied within a static temporal knowledge base, but the further complexity of dynamically accounting for new observations makes matters even more interesting.
This problem can be viewed as one of belief revision in that new observations may conict with current beliefs which require updating.
As a rst step toward a full-edged temporal belief revision system, we consider the tools of inductive logic.
We suggest that Carnap's method of conrmation 3] may serve as a simple mechanism for belief revision.
We begin by introducing our temporal logic and then turn to the problem of inferencing.
The rst form of inference we consider is what Carnap calls direct inference: the inference from a population to a sample.
In the case of temporal information, this amounts to inference from an interval statistic to a subinterval or point.
Before moving on to more complex kinds of inference, we introduce the learning (or belief revision) component, Carnap's method of conrmation, which incorporates new observations into the direct inference process.
Next we consider the general case of direct inference: interpolation.
Then we turn our attention to the problem of extrapolation of statistical information (what Carnap calls predictive inference).
Finally, we consider the problem of belief revision in connection with these temporal inferences.
2 Discrete temporal probability-logic  In this section, we introduce a discrete probabilitylogic which serves as a representation language for temporal applications.
The probability-logic, which we call PL(T ), is similar to that of Bacchus 1].
The most important dierence is in the inference machinery and the addition of time into the ontology.
PL(T ) allows the expression of standard rst order logic expressions plus two kinds of probability statements.
Before examining the probability-logic, we rst explore the two kinds of probability.
2.1 Statistical and inductive probabilities  Carnap 3] has suggested the need for two distinct concepts of probability (the relevance of this view to AI was recently suggested 1, 8]).
The statistical concept of probability, having the sense of relative frequency, is needed for empirical knowledge (e.g., most birds y).
As well, the inductive concept of probability, measuring the degree of conrmation of a hypothesis on the basis of the evidence, is needed for beliefs (e.g., to what degree is the belief that Tweety ies supported by the evidence that Tweety is a bird and most birds y).
While statistical probability is empirically based, inductive probability is epistemologically based that is, inductive probabilities constitute a logical relationship between belief (or hypothesis) and evidence.
To give such beliefs empirical foundations, a connection must be made between the statistical and inductive probabilities.
This connection is made on the basis of an appeal to some form of the principle of indierence which says that if our knowledge does not favour the occurrence of one event over another, then the evidence provides equal conrmation for the events.
The inference of inductive probabilities from statistical probabilities via a principle of indierence is called direct inference.
As Carnap 4] has noted, the form of indierence used must be carefully restricted to avoid the introduction of contradictions at the same time, it must remain strong enough to sanction the appropriate conclusions.
The principle of indierence comes into play when choosing the prior probabilities of hypotheses.
Each consistent assignment of priors constitutes a dierent inductive method.
Carnap 4] described two inductive methods which we outline next.
2.2 Two inductive methods  Carnap's two methods are most easily explained with reference to the example shown in Figure 1.
In this example, we have four individuals (balls in an urn) and one property (colour).
Since each ball is either blue (B) or white (W), we regard colour as a binary property (blue or not-blue).
An individual distribution is specied by ascribing one colour to each individual e.g., in individual distribution #2, the rst three balls are blue and the last ball is not.
A statistical distribution is specied by stating the number of individuals for which the property is true, without identifying the individuals e.g., in statistical distribution #2, three of the balls are blue and one is not.
There are 16 possible individual distributions and 5 statistical distributions.
As can be seen in Figure 1, several individual distributions may correspond to a single statistical distribution.
If equal prior probabilities are assigned to each of the individual distributions, the result is Carnap's Method I, and if equal prior probabilities are assigned to each of the statistical distributions, the result is Method II.
Method I consists of applying the principle of indierence to individual distributions and, in the examples, gives each individual distribution a prior probability of 1/16.
Method II consists of rst applying the principle of indierence to the statistical distributions, and then, for each statistical distribution, applying the principle to its individual distributions.
In the example, each of the ve statistical distribution is assigned 1/5, and each 1/5 is divided equally among the individual distributions of the appropriate statistical distribution.
Method II assigns 1/20 to each of individual distributions #2 to #5 because they are the four possibilities (arrangements) for statistical distribution #2 (3 blue balls and 1 white ball).
Method II is consistent with the principle of learning from experience, but Method I is not.
The principle of learning from experience is: \other things being equal, a future event is to be regarded as the more probable, the greater the relative frequence of similar events observed so far under similar circumstance" 4, p. 286].
Suppose we draw three blue balls in sequence, and then consider the probability of the fourth ball being blue.
There are two individual distributions consistent with the evidence: #1 (in which the fourth ball is blue) and #2 (in which the fourth ball is not blue).
Using Method I, the probability is 1/2 because each of individual distributions #1 and #2 is assigned a probability of 1/16, and 1/2 is the relative weight of 1/16 to (1/16 + 1/16).
Using Method II, the probability of the fourth ball being blue is 4/5 because individual distribution #1 is assigned a probability of 1/5 and individual distribution #2 is assigned a probability of 1/20, and 4/5 is the relative weight of 1/5 to (1/5 + 1/20).
Because Method II incorporates the principle of learning from experience, it is better suited to our intended application of temporal reasoning in dynamic situations where new observations are being made.
In Section 3.1, we apply Method II to direct inference from temporal statistical knowledge, but rst we turn to the description of our temporal probability logic.
2.3 PL(T ): A 	rst order temporal probability-logic PL(T ) is a four sorted, rst order, modal logic.1 The 1  Some material in this section is derived from 2].
STATISTICAL INDIVIDUAL METHOD I METHOD II DISTRIBUTIONS DISTRIBUTIONS Initial Initial Probability of Number Number Probability Statistical Individual of of of Individual Distributions Distributions Blue White Distributions 1.
4 0 1.
    1/16 1/5 1/5 = 12/60 2.
3  1  2.
3.
4.
5.
                    1/16 1/16 1/16 1/16  1/5  1/20 = 3/60 1/20 = 3/60 1/20 = 3/60 1/20 = 3/60                              1/16 1/16 1/16 1/16 1/16 1/16  1/5  1/30 = 2/60 1/30 = 2/60 1/30 = 2/60 1/30 = 2/60 1/30 = 2/60 1/30 = 2/60            3.
2  2  6.
7.
8.
9.
10.
11.
4.
1  3  12.
13.
14.
15.
          1/16 1/16 1/16 1/16  1/5  1/20 = 3/60 1/20 = 3/60 1/20 = 3/60 1/20 = 3/60  5.
0  4  16.
    1/16  1/5  1/5 = 12/60  Figure 1: Carnap's Two Methods (from 3, p. 285]) sorts are: object types, object instances, numbers, and times.
Suitable function and predicate symbols are associated with each sort.
Time invariant assertions can be made about domain objects via their object types, while object instances are used to describe domain objects at particular times.
The numeric sort is used to make numeric assertions, specically, assertions about the numeric values of certain probabilities.
The temporal sort allows assertions to include references to the time.
Both the numeric and temporal sort include the constants 1, -1, and 0.
The functions + and fi and the predicates = and < are \overloaded" to provide for all necessary combinations of argument and result sorts.
Additional inequality predicates, numeric and temporal constants can easily be added by denition, and we use them freely.
The formulas of the language are generated by applying standard rst order formula formation rules.
Additional rules are used to generate object instance terms from object type terms and temporal terms: 1.
If o is an object type term, t is a temporal term, ~o is a vector <o1 ,: : : ,on> of n object type terms, and ~t is a vector <t1 ,: : : ,tn> of n temporal terms, then (a) o@t is an object instance term (b) ~o@~t is a vector of n object instance terms,  specically, <o1 @t1,: : : ,on @tn> (c) ~o@t is a vector of n object instance terms, specically, <o1 @t,: : : ,on@t> (d) o@~t is a vector of n object instances terms, specically, <o@t1 ,: : : ,o@tn>.
Two additional rules are used to generate new numeric terms (specically probability terms) from existing formulas: 2.
If  is a formula and ~x is a vector of n distinct object type, object instance and/or temporal variables, then ]~x is a statistical probability term.
3.
If  is a formula, then prob() is an inductive probability term.
These new (numeric) terms denote numbers (that correspond semantically to the values of the probability measure) which can in turn be used as arguments of numeric predicates in the generation of additional new formulas.
We dene conditional probability terms (of both types): j ]~x =df  ^  ]~x = ]~x, and prob(j ) =df prob( ^  )=prob ( ).2 Semantically the language is interpreted using For ease of exposition, we ignore the technicalities of dealing with division by zero.
See 1] for details.
2  models of the form3 M = hO S #  O   S i where: 1.
O is a domain of objects types (i.e., the domain of discourse).
S is a set of states or possible worlds.
# is a state dependent interpretation of the symbols.
Numbers are interpreted as reals IR and the set of times T is taken as integers ZZ .
The associated predicate and function symbols are interpreted as relations and functions over the appropriate domain while +, fi, 1, -1, 0, < and = are given their normal interpretation in every state.4 2.
O is a discrete probability measure P over O. ThatPis, for every A  O,  O (A) = o2A  O (o) and o2O  O (o) = 1.
3.
S is a discrete probability measure P over S .
ThatPis, for every S 0  S ,  S (S 0 ) = s2S  S (s) and s2S  S (s) = 1.
4.
T is a discrete probability measure P over T .
ThatPis, for every T  T ,  T (T ) = t2T  T (t) and t2T  T (t) = 1.
5.
O@T is a discrete probability measure over the set of object instances O@T .
That is, for every O @T P  O@TP ,  O@T (O@T ) = o@t2O@T  O@T (o@t) and o@t2O@T  O@T (o@t) = 1.
O@T is a product measure formed from  O and  T .
The formulas of the language are interpreted with respect to this semantic structure in a manner standard for modal languages.
In particular, the interpretation of a formula depends on a structure M , a current state s 2 S , and a variable assignment function .
The probability terms are given the following interpretation:   1.
(]~x)(Msfi) =  nfi f~as:t:(M s ~x=~a]) j= g , where ~x=~a] is the variable assignment function identical to  except that (xi ) = ai , and  nfi is the n-fold product measure formed from  fii where i = O or O@T or T depending on whether xi is an object type variable, an object instance variable, or a temporal variable.
 ; 2.
(prob())(Msfi) =  S fs0 s:t:(M s0  ) j= g .
So we see that ]~x denotes the measure of the set of satisfying instantiations of ~x in  and prob() denotes the measure of the set of states that satisfy .
Unicorns have a single horn: 8o: unicorn(o) !
singleHorn(o).
Unicorns have never, do not, and will never exist: 8o t: t < now & unicorn(o) !
:exists(o@t), 8o: unicorn(o) !
:exists(o@now), 8o t: t > now & unicorn(o) !
:exists(o@t).
or, more simply: 8o t: unicorn(o) !
:exists(o@t): Most birds y: fly(o)jbird(o)]o > 0:5.
Most birds y now: fly(o@now)jbird(o@now)]o > 0:5.
At any time, most birds y: 8t: fly(o@t)jbird(o@t)]o > 0:5.
For most object instances, if the object type is a bird at the time then it ies at the time: fly(o@t)jbird(o@t)]o@t > 0:5.
Most of the time, most birds y: fly(o@t)jbird(o@t)]o > 0:5]t > 0:5.
Informally, the above expression says that if we pick a time at random, chances are that more than 50% of the birds y at that time.
In addition to statistical assertions, we can also represent inductive probability assertions (which correspond to an agent's beliefs).
For example,  Halpern has called such structures type III probability structures.
4 We ignore the technicalities of dealing with overloading and argument/result type conversions.
The degree of belief in the proposition \Tweety is ying now" is 0.9: prob(fly(tweety @now)) = 0:9:  0  3  In addition to the statistical and inductive probabilities, we need an extension that allows us to represent epistemic expectation.
Specically, if p is a statistical probability term, then E(p) is a new numeric term whose denotation is dened as follows: X (E(p))(Msfi) =  S (s0 ) fi p(Ms fi) : 0  s 2S 0  That is, the expected value of a term is the weighted (by  S ) average of its denotation over the set of states.
2.4 Representation in PL(T ) PL(T ) allows for the representation of a rich variety of statistical and probabilistic temporal information.
Because time is associated with object instances rather than with properties of objects, we can describe objects that come into existence or cease to exist.
We can also talk about properties of object types that have no instances, such as unicorns.
The following examples gives some idea of the expressive power of the language.
The degree of belief in the proposition \Most birds y" is 0.75:   prob fly(o)jbird(o)]o > 0:5 = 0:75: Two remarks are in order here.
First, although PL(T ) supports the representation of beliefs about temporal assertions, there is no support for temporal beliefs, i.e., only the current set of beliefs is representable.
This shortcoming while be addressed in future work.
Second, some form of direct inference is needed to connect the inductive probabilities to the statistical ones as was discussed in section 2.1.
We are now in a position to provide this connection.
3 Inferences in PL(T )  The choice of the distributions  O ,  S , and  T affect inferences in PL(T ).
Choosing a \uniform" distribution for  O ,  S , and  T corresponds to Carnap's Method I.
In the case of  T , we can not have a true uniform distribution since T is innite, so we take  T (T ) = jT j/jTnj, where Tn = f0 ;1 1 ;2 2 :: : ;1n;1 fibn=2cg, and then we consider the situation in the limit as n !
1.
For any nite set of times T , the measure is 0 so we must amend the interpretation of conditional statistical probabilites.
With j ]~x =df limTn !T  ^  ]~x= ]~x, what matters is the relative sizes of the sets of times involved in the numerator and denominator.
We can also choose distributions which result in inferences corresponding to Carnap's Method II.
To do this, the distributions  O and  T are taken as above, but to dene the distribution  S , we need to introduce the concept of structures which are equivalence classes of states.
Two states, s1 and s2 are considered isomorphic if replacing the individuals of s1 with one of their permutations results in s2 .
Let S be the set of structures corresponding to the set of states S .
We can now dene the distribution  S in such a way that every subset S 0 of S which is a member of S has the same measure and every member of S 0 has the same measure.
This results in inferences corresponding to Carnap's Method II.
We examine both Method I and II inferences in section 3.1.
Then in sections 3.2 and 3.3, we discuss interpolation and extrapolation.
Finally, we consider temporal belief revision issues in section 3.4.
1  3.1 Direct inference  We can connect inductive and statistical probabilities in a similar manner as Bacchus did in 1].
We start by assuming that an agent expresses assertions about his environment in a xed statistical language Lstat.
Assertions in Lstat, which are all the assertions of PL(T ) excluding those involving inductive probability, are called objective assertions.
The agent's degree of belief in the objective assertions are represented in another language Lcomb which extends Lstat with the inductive probability operator prob and an  royal elephant(clyde) & elephant(clyde) 8x:royal elephant(x) !
elephant(x) gray(x)jelephant(x)]x > 0:5 :gray(x)jroyal elephant(x)]x > 0:5:  Figure 2: Redundant Information epistemic expectation operator E. Formulas of Lcomb that are also in Lstat are called objective formulas.
The knowledge base KB is the complete nite collection of objective formulas   which are fully believed by the agent i.e., prob KB = 1.
De	nition 1 (Randomization 1]) stat  Let  be a formula of L .
If hc1 : : : cni are the n distinct object constants5 that appear in  ^ KB and hv1  : : : vni are n distinct object variables that do not occur in  ^ KB, then let KBv (v ) denote the new formula which results from textually substituting ci by vi in KB (), for all i.
(KBv is referred to as the randomization of KB or KB randomized.)
De	nition 2 (Direct Inference Principle 1] ) If the agent fully believes that KBv ]~v > 0 and if  is a formula of Lstat then the agent's degree of belief in  should be determined by the equality prob() = E(v jKBv ]~v ): Method I inferences: If the distributions are chosen for Method I as described in section 3, inferences in PL(T ) have similar properties to those described in 6], e.g., desirable inheritance properties.
For example, in Figure 2, PL(T ) infers that prob(:gray(clyde)) > 0:5.
That is, we have inheritance with specicity in spite of the redundant information elephant(clyde).
This method supports a number of desirable inferences such as those involving simple inheritance, multiple inheritance with specicity, ambiguity, cascaded ambiguity, cycles, redundant information, and negative paths (see 6]).
Such a system might be sucient for most needs.
It even includes the ability to revise beliefs about individuals, i.e., inheritance of properties is aected by receiving more specic information about an individual.
Furthermore, the inclusion of additional statistical assertions may aect properties inherited to individuals.
What is lacking, however, is an ability to revise beliefs in statistical formulas given individual observations.
This can be addressed by Method II.
Method II inferences: If the distributions are chosen for Method II as described in section 3, inferences in PL(T ) have in addition to the desirFor our purposes, these refer to object types, object instances, and/or times.
5  able inheritance properties described in 6], the ability to dynamically account for observations in beliefs about statistical assertions.
For example, in Figure 3, if O contains only ve object types, s1, s2, s3, s4, and s5, then, as reported in 7], initially prob(fly(s5)jsparrow(s5)) = 0:6 (see Figure 3).
The table in Figure 3 was computed by the method of exhaustive enumeration as described in 7].
In the table, probI means prob with the distributions set for Method I probII means prob with the distributions set for Method II.
Upon learning that s1 is a ying sparrow, prob(fly(s5)jsparrow(s5)) = 0:5714 under Method II as compared to 0.5 under Method I.
Comparing this to prob(fly(s5)jbird(s5)) which is 0.5, we see that in spite the the observed ying sparrow, Method I sticks to straight inheritance of the ying birds statistic to sparrows, whereas Method II adjusts to the observation and infers sparrows are even more likely to y than birds.
So far, the examples in this section have not involved time.
In sections 3.2 and 3.3, we examine the temporal inferences we call interpolation and extrapolation.
3.2 Interpolation  Suppose we have the following situation: Over the year, it rains 40% of the time.
During winter (December 21{March 20), it rains 75% of the time.
Over the summer (June 21{September 20), it rains 20% of the time.
What percentage of rainfall occurs during December?
What is the chance of rain on December 24th?
We can represent this in PL(T ) by letting the integers 1 through 365 represent the days (i.e., each day is a time point) and provide axioms such as shown in Figure 5.
Inferences about the rainfall in December or on December 24th based on the given statistical information are in a class of inferences we call interpolation.
These inferences involve using interval statistics to induce subinterval statistics or point probabilities.
For instance, the actual percentage of rainfall in December is: P3 = P rain(t)jr3(t)]t rain(t) = R3jR3 j (t)jr3b(t)]tjR3b j = 	rain(t)jr3a(t)]tjjRR33aajj++	jRrain 3b j where the value of the numerator is unknown.
(Note in the summation, we are treating rain as if it were a 0-1 function with value 1 at t if there is rain at time t and value 0 at t otherwise.)
To compute the amount of rainfall in December we divide the month (region R3 from Figure 5) into subregions R3a = dec1 dec20] and R3b = dec21 dec31]: The specic information about R3a is obtained from R5 where R5 = mar21 jun20] + sep21 dec20] = R1 ; R2 ; R4.
rain(t)jr1(t)]t = 0:4, rain(t)jr2(t)]t = 0:75, rain(t)jr4(t)]t = 0:2, % plus axioms dening the % regions r1, r2, r3, r4 R3 : ??
R2 : 75%  Dec1 Dec21 Dec31 Jan1  Mar20  R1 : 40% R4 : 20% Jun21  Sep20  Figure 5: Rainfall Interpolation The statistic for R5 can be computed from the statistics for R1, R2, and R4, and from the relative sizes of these intervals.
We compute the actual percentage of rain P5 over R5 to be approximately 33% (see Figure 4).
By assuming every subset of R5 has the same expected percentage of rain (i.e., using Method I), we conclude the expected percentage of rain over R3a is P5 : The most specic reference class (for which we have or can compute the actual percentage of rainfall) that contains R3b is R2.
By assuming every subset of R2 has the same expected percentage of rain (Method I), we conclude the expected percentage of rain over R3b is 75%.
The expected percentage of rain over R3 equals a weighted average based on R3a and R3b:   E(P3) = E rain(t)jr3(t)]t :75jR3bj = P5 jR3a jj+0 R3 j :7511  0:3320+0 31  48%.
The answer to the original question is that it rains roughly half the time during December.
3.3 Extrapolation  Persistence (the frame problem) has been viewed in two ways: 1) action-invariance of a property: whether a property that is true before an action or event will remain true afterwards, cf.
temporal projection 9] or, 2) time-invariance of a property: if a property is true at some point in time, how long is it likely to remain true 5].
Under these views, a property such as raining at a given point in time is highly action-invariant (few actions aect rain) and slightly time-invariant (it rains for a while and then stops).
Here we consider a previously unexplored aspect of the frame problem: action and time invariance of statistical knowledge.
Given statistical information about various time  A Statistical KB fly(x)jbird(x)]x = 0:6, 8x:sparrow(x) !
bird(x).
Method I and II Inferences Known ying sparrows => none probI (fly(s5)jbird(s5)): 0.6 probII (fly(s5)jbird(s5)): 0.6 probI (fly(s5)jsparrow(s5)): 0.6 probII (fly(s5)jsparrow(s5)): 0.6  s1 0.5 0.5 0.5 0.5714  s1, s2 0.3333 0.3333 0.3333 0.4286  Figure 3: Belief Revision  P5 = rain(t)jr5(t)]t = 	rain(t)jr1(t)]tjR1 j;	rain(t)jjrR2(5tj)]tjR2 j;	rain(t)jr4(t)]tjR4 j ;0:75	79+11];0:292  33%.
= 0:4jR1j;0:75jRj5 jR2j;0:2jR4j = 0:4365365 ;	79+11];92 Figure 4: Calculation of P5.
intervals, we wish to make reasonable inferences about past or future intervals.
For example, Figure 6 depicts a situation where we know that it rained 75% of the time in the winter, and 20% of the time during the summer.
We have no statistical information about the coming year (R6: December 1 to November 30) so the interpolation technique in the previous section is not applicable.
The temporal projection technique of Hanks and McDermott 9] is also inappropriate.
We cannot determine from the statistical information whether it was raining on September 20.
Even if we knew it was raining at that time, it does not make sense to allow raining to persist indenitely.
We have no information about actions or events that may aect raining.
Finally, Dean and Kanazawa's 5] probabilistic temporal projection cannot be used as it requires the construction of a survivor function for raining based on many observations of raining changing from true to false.
In our example, we have no observations of raining at particular points.
We only have interval statistics.
Instead of considering persistence at the level of individual time points, we can view it at the interval level and describe the persistence of statistical information.
If we take the observed statistics to be samples of raining over time (i.e., over the whole time line), we can base our inferences for other intervals on these samples.
For instance, we can infer a statistic for R6 in Figure 6 using R2 and R4 as follows:   E rain(t)jr6(t)]t rain(t)jr4(t)]tjR4 j = 	rain(t)jr2(t)]tjjRR22 jj+	 +jR4 j :292 = 0:7590+0 182  47%.
This result corresponds to that obtained by both Method I and II.
Space considerations force us to omit a detailed discussion of the precise mechanics of interpolation and extrapolation inferencing in PL(T ), but we have provided enough detail to highlight relevant issues.
As well, our discussion of interpolation and extrapo-  lation, so far, has not touched on belief revision issues.
We turn to consideration of this next.
3.4 Temporal belief revision  In the preceeding two subsections, we have described two forms of inferencing in PL(T ).
For a xed temporal knowledge base which includes only interval level statistics (such as in the examples of Figures 5 and 6), the results for Method I and Method II are the same.
The situation becomes more interesting when the knowledge base is updated with new statistics and point information.
There are three important cases to consider: 1) new interval statistics 2) new point information aecting the relevancy of interval statistics and 3) new point information aecting the predicted value of interval statistics.
New interval statistics: Suppose in the example of Figure 6, as time passed, we came to observe the rainfall in December of the coming year (R7, a subinterval of R6) and found it to be 60%.
Prior to learning this, we had predicted the rainfall for the coming year to be about 47%.
The newly acquired interval statistic for December should cause us to revise our prediction for the coming year.
Under both Method I and II, this is indeed the case.
Referring to December of the coming year as region R7, the result under either method would be approximately 50% (see Figure 7).
New point information (relevance): Suppose in the example of Figure 5, we wanted to predict the chances of rain on the day of a party to be held in December (R3).
Since we do not know the exact day, the prediction about rain on the day of the party given the day will be in December is based on the inferred statistic for R3, i.e., prob(rain(party day)) is about 48% (cf.
the example of Figure 4).
As time passes, we come to learn the party will be held on December 24.
This (point level) information should cause us to base our prediction of rain on the statistic for R3b (which is derived from R2) which is more relevant than the inferred statistic for R3 given that the  R2 : 75%  R7 : 60% R4 : 20%  Dec21  Mar20 Jun21  Sep20  Dec1  R6 : ?
?%  Jan1  Figure 6: Rainfall Extrapolation  Nov30      E rain(t)jr6(t)]t = E 	rain(t)jr7(t)]tjR7 j+	rainjR(t6)jjr6(t) & :r7(t)]t jR6 ;R7 j     = E 	rain(t)jjrR7(6tj)]tjR7 j  + E 	rain(t)jr6(t) &jR:6rj7(t)]t jR6 ;R7 j  	 rain ( t ) j r 2( t )] j R t 2 j+	rain(t)jr 4(t)]t jR4 j+	rain(t)jr 7(t)]t jR7 j jR6 ;R7 j   = 	rain(t)jjrR7(6tj)]tjR7 j + jR2 j+jR4 j+jR7 j jR6 j = 0:631 + (0:7590+0:292+0:631)334  50%.
365  213365  Figure 7: Calculation of next year's expected rainfall.
day of the party is in R3b.
Again, this is indeed the case in both Method I and II, and the revised belief becomes: prob(rain(party day)) is 75%.
New point information (value): So far, there is has been no reason to choose between Method I and II.
A dierence arises, however, as we incorporate point level observations that aect the predicted value of interval statistics.
To see this, again consider the example from the previous paragraph about rain on the day of the party.
Suppose we observe the rain on certain days in December (but not the day of the party).
Let us suppose that, although we have made these observations, we have not come to learn the party is not on one of those days.
(This could happen, say, if a friend was telling us about the party and we had independently observed the weather.)
Now suppose each of the days we observed was a rainy day.
This should cause us to revise our belief in rain on the party day, i.e., we should increase our belief in rain on the party day.
Method I does not do this.
It stubbornly holds to the belief prob(rain(party day)) is about 48% based on an unchanged R3 (inferred) statistic.
Method II, however, increases the predicted value of the R3 statistic and hence increases the value of prob(rain(party day)).
4 Conclusion  We have described the discrete temporal probabilitylogic we call PL(T ) which is expressive enough to represent and reason with a rich variety of problems.
Underlying the probability-logic is a choice of distributions over objects, states, and times.
Dierent choices correspond two dierent inductive methods.
We have focused on two methods described by Carnap.
For most purposes, either method seems adequate, but we found there are cases in the context of belief revision where Method II is superior.
This is  particularly true when new point level observations are made which aect the value of predicted interval statistics.
References  1] F. Bacchus.
Representing and Reasoning with Probabilistic Knowledge.
MIT Press, Cambridge, Massachusetts, 1990.
2] F. Bacchus and S.D.
Goodwin.
Using statistical information in planning.
unpublished extended abstract], May 1991.
3] R. Carnap.
Logical Foundations of Probability Theory.
University of Chicago Press, Chicago, Illinois, 1950.
4] R. Carnap.
Statistical and inductive probability.
In Readings in the Philosophy of Science.
Prentice-Hall, 1989.
5] T. Dean and K. Kanazawa.
Probabilistic temporal reasoning.
In Proceedings of the Seventh National Conference on Articial Intelligence, pages 524{528, St. Paul, Minnesota, August 1988.
6] S.D.
Goodwin.
Second order direct inference: A reference class selection policy.
International Journal of Expert Systems: Research and Applications, 5(3):1{26, 1992.
7] S.D.
Goodwin and H.J.
Hamilton.
An inheritance mechanism for default reasoning that learns.
In International Symposium on Articial Intelligence, pages 234{239.
Monterrey, Mexico, 1993.
8] J. Halpern.
An analysis of rst-order logics of probability.
In Proceedings of the Eleventh International Joint Conference on Articial Intelligence, pages 1375{1381, August 1989.
9] S. Hanks and D.V.
McDermott.
Nonmonotonic logic and temporal projection.
Articial Intelligence, 33(3):379{412, November 1987.