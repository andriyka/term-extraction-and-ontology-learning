Bidirectional Inference in Probabilistic Temporal Reasoning Ahmed Y .
Tawfik Wilfrid Laurier Tiniversity Waterloo, Ontario N2L 3C5, Canada at awfikQmach1.
wlu.
ca  Eric M. Neufeld University of Saskatchewan Saskatoon, Saskatchewan S7N 5A9, Canada eric@cs.usask.ca  Abstract  1.1  Belief update based on observations is a fundamental problem in A I .
I n this paper we propose a belief update strategy suitable for temporal probabilistic domains.
W e assume that the causal structure of the world is specified in probnbilistic terms.
The causal specifications are used to perform two basic operations: explanation (backward propagation) and projection (forward propagation).
W e show that probabilistic belief update results from interleaving explanution and projection.
1  Here, we use dynamic probabilities which are functions of time.
These probabilities allows us t o represent change in fluent probabilities as well as variations in causal relationships over time [la].
In this representation, events, their persistence, their effects, and the persistence of effects are also represented as probabilistic functions of time.
The representation relies on the theory of survival analysis.
Survival analysis represents the probability that an event takes place using a hazard function reflecting its rate of occurrence, a survival function or a failure function.
While survival analysis finds its root in the study of failure times, the techniques are general and useful for analyzing the occurrence of events [l].Some earlier works which considered the use of survival techniques for temporal representation, reported some difficulties [4].
To overconie these difficulties, we use regressive survival models.
Regressive survival modcls express the probability of an event conditioned on observations and other events.
To account for events' interaction, survival models provide a compact and efficient solution.
The proper choice oi survival model guarantee that each factor is accounted for appropriately.
For example, t h e competing risks model can be used to model the airport pickup problem [4].
In this problem, John who has arrived at the airport tends to wait for some time.
His tendency to wait decays with time as he gets bored.
We are interested in the probability that John stays at the airport until we arrive to pick him.
Fred can meet John at the airport and give him a ride.
Introduction  In a theory of a,ction, inference is a two-stage process of explanation followed by prediction.
The explanation stage is also the backward stage because it tries t o find plausible causes for an observation.
The prediction or forward phase tries to find the possible consequences of an event.
We define an observation as a report describing a state at a particular time.
An event is an occurrence which has consequences.
Performing explanation or prediction based on new evidence usually changes the set of probabilities assigned t o uncertain beliefs.
Starting with a set of beliefs I<, K V A denotes the new set of beliefs incorporating possible explanations for A .
The set ICAA incorporates the possible consequences of A .
31  0-8186-7937-9/97 $10.00 0 1997 IEEE  Elements of the Knowledge Representation  Now, Fred and boredom constitute two competing risks, each of them can cause the failure John leaves the airport.
If we have a probability distribution for the time when Fred arrives at the airport, we can deduce using the competing risks model, the distribution for John leaves the uirport.
This general equation corresponds t o a persistent cause, persistent effect and delaypd causation.
Seven special cases, corresponding t o the other seven persistence/causation patterns can be derived by omitting terms from the general form above.
According to regressive survival niodels, the quantities C,(t), C p c ( t ) C , p ( t )D , ( t ) and E p ( t )are in general functions of the state as well as time.
The dependence of these quantities on the state reflects their conditional dependencies.
In general, it may be useful to consider our probabilistic functions as the non-recursive equivalent for the recursive representation usually found in time-sliced Bayesian networks [3] where for a set of states X , P t ( X ) is expressed in terms of Pt-A(X).
1.2  1.3  Explanatory Inference  For explanatory reasoning, it is assumed that the time when the effect manifests itself is known exactly or as a distribution.
Now, given the effect start time E,(t), we are interested in finding the cause time C,(t), knowing the delay D ( t ) , the causation persistence of the cause C p c ( t )and the persistence of the effect E p ( t ) .The tirne separating E&) and C , ( t ) is the delay time.
Predictive Inference  In a causal system, there are eight cases corresponding t o four possible persistence properties (cause and effect persist, only cause persists, only effect persists, neither persists) paired with two causation properties (immediate or delayed).
We assume that the time of occurrence, the duration of persistence of the cause, the duration of persistence of the effect and the causation delay are all independent of each other.
Let the cause starting time be a function C,(t), the cause persistence necessary to start producing effects is Cpc(t),the causation delay D(f).
In this case, the probability that effect E starts at time t is given by:  C,(t) = E,(t) - D ( t ) - C&).
A deconvolution operation, usually performed using a transform, can be used to determine the distribution of the time of the cause knowing the distributions of the time of the effect and delay.
Deconvolution is generally harder to evaluate than convolution.
If the starting time of the effect,s is known, the deconvolution is no longer required and the distribution of the causation point is given by a shifted scaled mirror image of the delay distribution.
E&) = G ( t )EUR4 C p c ( t ) 63 D ( t ) where 63 is the convolution operator.
In general the convolution of any two independent distribu' and Fy gives the distribution of 2 where tions F 2 = X + Y .
Given that the cause persists according C p ( t ) the , probability that the cause holds is given by C ( t ) = CS(t)8 C p ( t ) and similarly given the effect persistence E p ( t ) , E ( t ) = E,(t) EUR4 E p ( t ) .
Depending on the causation-persistence pattern, terms in the above equations may be omitted.
For instance, if there is no causation delay, the delay term D ( t ) is omitted.
Similarly, C p ( t )and E p ( l )disappear if the cause and the effect do not persist, respectively.
32  Observation 1 Assuming a, uniform prior distribution for C,(t) over the interval te maxD,te - minD where maxD and ming are maximum and minimum possible causation delay times respectively, (ignoring the cuusation persistence time), the distribution of cause start time given the eSfect start time C,9(tlE,(t)= t e ) is a scaled mirror image of the distribution of the delay D ( t ) .
To justify this observation, we apply Bayes rule as follows: Cs(tlEs(t)= t e ) = D(te - t ) C s ( t ) / E s ( t e )  .
Under the uniform distribution assumption, which is a reasonable assumption to mate in the absence of information regarding the prior distribecomes conbution C,(t) the term C,(t)/E,9(te) stant.
2  In the above equation, X , is the starting time of X and X , denotes its persistence.
It is clear that a single observation indicates that X can start at most L time units before to and it can persist for at most L units after to.
This results in X being possible over a 2L interval as indicated by the equation.
This approach allows a formalization of the idea of regions of bidirectjonal persistence [51.
From Subintervals t o Intervals  Observing the status of a fluent or observing an event restricts the set of possible worlds t o those where the fluent can hold or the event can happen.
This restriction is captured by the causal theory.
The question that we address here is: what does the observation tell us about what is being observed?
In general, an observation is made over a short duration (a subinterval).
We study here the relationship between this subinterval and longer intervals.
The study of temporal relationships between intervals and subintervals has been a main point of research interest in temporal logic [ll].In a probabilistic theory of change, studying these relationships introduces a new set of issues.
The first issue is that of a sensor model (or reliability of observation).
The second issue results from the fact that different observations may belong t o the sa>meoccurrence (same token) or t o different tokens.
The third issue deals with bidirectional persistence of the observed entity.
How far in the past and in the future is it likely for the observation to hold?
Example 1 If Joe is running now he probably was also running a minute ago and will continue to run the next minute.
Dynamic probabilities can deal quantitatively with thris type of persistence.
A function f i .
( t ) gives the probability that Joe's running continues for at least time t after starting.
Given that he is now running, what is the probability that he was running at time t?
Let p minutes be the ezpected duration of Joe's run.
I n a discrete time model, there are N possibilities to justify that he is running now.
He may be just starting now, he may have started one minute ago or N minutes ago.
The probability of 'Joe is running ' at time ti is equal to the probability that he starts at time to and continues to run f r o m to until ti.
The present treatment ignores the reliability of observations.
Appropriate probabilistic solutions to this problem generally adjust the probability of evidence depending on the reliability of the observer [6].For our purposes here, observations are reliable.
Given that Joe started running at t o , the conditional probability P( Running at ti IRunning at t } is given by f r ( t i - t o ) / f T ( t- t o ) according to survival analysis.
Two identical observations, 01 and 0 2 at times to, and to, may belong t o the same occurrence of X or to two different occurrences.
Dynamic properties like minimum duration required for X's recurrence, or the duration between observations, can determine which case applies.
It can be shown [lo] that given a predefined confidence threshold 6 and the transition probabilities p l and p z in a two state Markov processes, the probability that two observations belong to  Let X be a persistent cause or effect.
An observation 0 at t , indicates that X holds at this time point.
First consider the persistence properties of X .
If X has a limited persistence time L , it is possible that the observation corresponds to any point within a 2L duration.
The probability distribution of X is therefore given as  33  this change.
Pt(,C) is a temporal probability distribution for the cause.
This distribution has a nonzero value at least at one point during the interval [tZ, t,]' This condition guarantees that the cause could have posszbly occurred resulting in the is given by eflect.
The probability P[t,,t31(CIE)  the same occurrence can be determined depending on the time separating the two observations and the transition probabilities.
The result can be generalized t o n-state Markov process.
It may also be possible t o determine the answer by estimating the probabilities in each situation.
If 01 and 0 2 belong t o different tokens, then the analysis in the previous paragraph continue to apply.
If 01 and 0 2 belong to a, single occurrence of X , then the possible starting times are limited to the interval (to2- L , to,).
Moreover, X is assumed to hold over the interval (tal, to*).Additional observations regarding the interval (tal, to*)do not add new information.
If C is the only possible cause for the change E then C is certain or P[tl,t,l(CIE) = samk probability regardless of the time of occurrence of the cause, the temporal profile of Pt(CIE) from the above expression becomes a scaled version of P ( E ) .
Therefore, the car was most likely stolen at the time of the night when most thefts occur.
Example 2 The red trafic light at a given intersection persists for four minutes.
All cars leave the intersection once light turns green.
Arriving at the intersection, a driver makes five equiprobable hypothesis regarding the time the light became red (corresponding to 4,3,2,1 and 0 minutes ago).
The driver of the car in the next lane, says that when she came two minutes earlier, the light was red.
This new information limits the possible hypothesis to three (4,3 and 2 minutes).
The techniques described in this section till now 5serve as inference rules for our formalism.
These rules are similar to logical inference rules in that they allow inference.
Instead of inferring whether X is true or false, only the probability that x' holds is inferred.
In many practical situations, it is not possible or useful t o assume a time limit for the persistence of X .
Reasons for this include the possibility that X may hold indefinitely and that its persistence time varies widely.
In such situations, it is not possible to proceed with the analysis discussed earlier.
To proceed in such situations, additional information about X is useful, in particular, the nearest time point where -IX was observed.
It is possible t o deduce that at least one transition from 1 X t o X , or vice versa, took place during the interval between the two observations.
3  Belief Update  The incorporation of new information is performed through a process called belief update.
The term belief update denotes the change in beliefs resulting from changes in the world.
Belief update can be viewed as a form of belief revision.
Belief revision generally refers t o changing beliefs about a static world while belief update reflects belief change in dynamic situations [a].
Starting with a set of probabilistic beliefs K , an observation or event A (possibly uncertain) results in an updated set of beliefs K ' , such that  Example 3 Parking a car in the driveway in the evening and not finding it the next mornsing m a y mean that it was stolen [8].
Here, we have two observations at two diflerent tim,e points, car parked in the evening at t , and car not parked in the morning at t,.
A change has occurred during the interval [t;,tj] and some event C caused  K' = K O A 'If the cause can result in a delayed effect a corresponding interval [ t k , t i ] where t k = t , - At.tl = t, - At and At is the delay between event and effect.
In this case, t h e techniques for delayed response can be used.
34  where 0 is an update operator.
PU3.
PU4 introduces equivalence of updated theories resulting from equivalent observations or effects.
Consider the following example: A: the front right tire of my car is flat, B:the front left tire of my car is pat, 1i':If I am driving to work at 8$5, what is thx probability that 1'11 be there at 9:00?.
Both A and B are equivalent, therefore, they will result in the same updated beliefs.
A correct update has to maintain the integrity of the probabilities assigned t o different beliefs.
For instance, an update is not correct if it results in probability values greater than unity or less than zero.
Moreover, if the probabjlities of all beliefs in li are consistent and correct and the probabilities corresponding t o A and its effects are also consistent and correct, the probabilities of all beliefs in K' are consistent and correct if the update is performed appropriately.
Poor probability estimates may result in conflicts.
In the present framework, surprises serve as a tool for detecting problems arising from poor estimates.
Surprises [13] play a role identical to miracles [9] in dealing with poor probability estimates and theory incompleteness.
We further require the update theory to satisfy the following conditions: PU5.
(Ir'0A)OB  PU6.
If A and B occur necessarily together, then P ( B ) = P ( A ) in K O A or KOB.
While the order of events may significantly affect their net effect, the order in which we learn about them should not affect our final conclusions (PU5).
For example, finding a note from some friends in the mailbox saying that they dropped by our place at 6:OO pm, and then finding a message froin them on the answering machine saying that they will be coming to visit that afternoon, will have the same net effect on the belief as if the findings took place in the opposite order.
An appropriate probabilistic update (PU) has to satisfy the following properties: PU1.
The probability of the observation or event A is greater than zero in the updated set of beliefs KOA.
PU2.
If V B , P ( B ) remains unchanged in K O A , then K = KOA.
PU3.
A proper update is consistent.
PU4.
If A  (K0B)OA  Flipping a fair coin has two outcomes, each consists of a conjunction of two propositions occurring together, the first is up(heads) A down(tai1s) and the second is down(h,euds) A up(tai1s).
Let A be the observation up(heads) and down(tails), according t o PU6, P ( A ) = P ( B ) .
B then K O A E K O B .
The above four conditions are a probabilistic equivalent for the conditions common t o all admissible update theories [7].
U1EUR' simply states that an update must allow A to hold at the time it was observed.
Katsuno and Mendelzoii [7] show that nonmonotonic belief revision systems tend t o satisfy a subset of Gardenfors' rationality postulates.
The nature of the preference criterion based on which an extension is chosen limits the range of postulates that can be satisfied.
According to Ka,tsuno and Mendelzon analysis, no update theory satisfies all the rationality postulates.
PU2 establishes the notion that if the knowledge K predicted A , then observing A will not change K. For example, given a coin let K Se a set of beliefs that allow us t o estimate P(1leads).
A device flips the coin and determines P ( H e a d s ) .
Let A be the observation made by the device.
If the predicted probability is the same as the observed one, then this observation does not change li.
3.1  Interleaving Backward Explanation and Forward Projection  The update procedure proposed here uses a sequence of backward inference (explanation) and  The update has to maintain the consistency of probabilities as discussed earlier arid stated in  35  For ii) to hold, P ( g j ) in KVfi must be diflerent from P ( g j ) in K .
Given that P( f i ) is in the projection of K then the probability of U (f;) is given b y projection K riihich contains g j and therefore cannot change P ( g j ) according to PU2.
In fact, if we consider K' -- K - g j , then f; belongs to K ' A g j .
l f f; and gj are independent, the probability P( f;) remains the same in the projection of K' and that of K .
I n this case, fi cannot aflect g j .
A causal dependence between f; and g j would transfer the effect of P ( g j ) on the predicted value of P ( f i ) and therefore this value cannot change  forward reasoning (projection).
The backward inference phase accounts for the causa.1precursors of the new information while the forward projection phase projects the future consequences of this information.
Each piece of new information requires a bidirectional sweep for proper incorporation.
Observation 2 A n update requires a backward inference phase (temporal explanation).
To justify the above concept, consider some new observation A and a set of beliefs A'.
In general, A can be an observation or an event.
If A is an observation, it possible that it held before the moment of observation.
Moreover, this observation could have resulted from an unobservable event.
This event may have other interesting effects that need t o be taken into consideration.
If A is an event, its occurrence may indicate that some preconditions were satisfied and those preconditions may have future ramifications.
The set of beliefs K V A is the result of performing a backward inference phase incorporating the past explanations of A.  mj).
Iff; and g j have a common cause C k , then the probability of both f .
; and g j are governed by that of C k .
Moreover, f; cannot cause Ck and cannot aflect its probability as discussed for g j .
From the above, all three cases are ncd consistent with the assumption and therefore the theorem holds.
The above theorem is interesting from a reasoning point of view because it establishes the fact that future projections (without any new information) cannot invalidate current beliefs if the upda,te and the probabilities are consistent.
Theorem 1 Assuming an update procedure satisfying PUI through PU6, the eflects predicted by forward projection based on present knowledge IC' cannot change current beliefs.
Corollary 1 A single backward inference followed by a forward projection, is required to properly incorporate A.
The probabilistic update is therefore given b y K O A = ( K O A ) A A , where K A A denotes the set of beliefs resulting from the forward projection of K incorporating A.
Proof.
To prove this, we use contradiction.
Assume that the the above statement is not correct and that for a certain belief f, the predicted probability o f f at time i denoted by P ( f i ) aflects Q previously held belief g at time j where j < i .
Observation 3 To answer a query regarding a moment predating A, K V A provides the required answer.
For the above assumption to hold, one of the following three cases must hold i)  fi  causes g j (directly or indirectly)  ii) f; is a result (direct or indirect) of g,, or  Theorem 2 Repeating the backward forward sweep for each piece of information a; E A in chronological order, provides a proper update for  iii) f; and g, have a common cause.
li .
But f; cannot be a cause for g j because j < i , so either ii) or iii) above must be true.
Proof.
The above case can be proven by induction.
l f there is no need to update (no events, actions  36  [6] S. Hanks and D. McDermott.
h4odeling a dynamic and uncertain world i: Symbolic and probabilistic reasoning a,bout change.
Artificial Intelligence, 66( 1):l-55, March 1994.  or observations), then no update operations are done.
This is the base case.
Now assuming that a set of update operations have correctly updated the set of beliefs IC, it is possible to prove that (KVAjAA correctly updates IC.
[7] H. Katsuno and A. Mendelzon.
Propositional knowledge base revision and minimal change.
Artificial Intelligence, 52263-294, 1991.
A may be an eflect of a set of causes.
Adjusting the probabilities of the possible causes of A is performed during the K V A step of the update operation.
The possible eSfects of A are reflected on the probabilities during the K A A phase of the  [8] H. Kautz.
The logic of persistence.
In Proceedings of the fifih National Conference on Artificial Intelligence, Los Altos, California, August 1986.
Morgan Kaufmann.
update operation.
[9] V. Lifschitz and A. Rabinov.
Miracles in formal theories of action.
Artificial Intelligence, 381225-237, 1989.
Conclusions We have described a belief update system that uses dynamic probabilities.
Three operations are required for a proper update: backward explanation to find common causes of observed effects, forward projection t o predict the effects of actions and causes, and bidirectional persistence to generalize observations made at certain time points to intervals.
[lo] Y. Shahar.
A framework for knowledgebased temporal abstraction.
Technical Report KSL-95-29, Knowledge Systems Laboratory, Stanford University, 1995.
[ll]Y. Shoham.
Reasoning about change: Time and Causation from the standpoint of ArtiJicial Intelligence.
MIT Press series in AI.
MIT Press, Boston, MA, 1988.
References [l]P. Allison.
Event History Analysis.
Sage, Beverly Hills, 1984.
[12] A. Tawfik and E. Neufeld.
Temporal Bayesian networks.
In Proceedings of Time94: International Workshop on Temporal Knowledge Representation and Reasoning, pages 85-92, Pensacola, Florida, 1994.
[a] C. Boutilier.
Abduction to plausible causes: An event-based model of belief update.
Artificial Intelligence, 83:143--166, 1996.
[13] A. Tawfik and E. Neufeld.
Surprises in probabilistic reasoning.
In Proceedings of the Eighth Florida Artificial Intelligence Research Symposium (FLAIRS-96), 1996.
[3] T. Dean and K. Kanazawa.
A model for reasoning about persistence and c,ausation.
Computational Intelligence, 5(3):142150, August 1989.
[4]T. Dean and M. Wellman.
Planning and Control.
Morgan Kaufmann, San Mateo, California, 1991.
[5] S. Goodwin, E. Neufeld, and A. Trudel.
Probabilistic regions of persistence.
In Proceedings of the European Conference on Symbolic and Quantitative Approaches for Uncertainty, Marseille, France, 1991.
37