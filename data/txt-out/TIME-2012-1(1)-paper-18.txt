2012 19th International Symposium on Temporal Representation and Reasoning  A Review on Temporal Reasoning using Support Vector Machines Renata C. B. Madeo, Clodoaldo A. M. Lima, Sarajane M. Peres School of Arts, Sciences and Humanities University of SaEo Paulo SaEo Paulo, Brazil {renata.si, c.lima, sarajane}@usp.br data in order to incorporate temporal dynamics into each datapoint to be submitted to traditional SVM models; or developing mathematical models considering time, and use a traditional SVM to estimate some parameters for these models.
Otherwise, regarding approaches that deal with temporal aspects in a internal way, we can adapt SVM model to incorporate temporal reasoning, building more complex strategies that are able to interpret and take advantage of the temporal dependencies among the data.
In this paper we present a review on studies that incorporate temporal reasoning into SVM, considering mainly approaches applied in researches executed in the last dZve years.
This paper is organized as follows: Section II and Section III briedZy presents traditional SVM and some of its variation; Section IV describes approaches that incorporate temporal reasoning into SVM; dZnally, our dZnal considerations are delineated in Section V.  AbstractaRecently, Support Vector Machines have presented promissing results to various machine learning tasks, such as classidZcation and regression.
These good results have motivated its application to several complex problems, including temporal information analysis.
In this context, some studies attempt to extract temporal features from data and submit these features in a vector representation to traditional Support Vector Machines.
However, Support Vector Machines and its traditional variations do not consider temporal dependency among data.
Thus, some approaches adapt Support Vector Machines internal mechanism in order to integrate some processing of temporal characteristics, attempting to make them able to interpret the temporal information inherent on data.
This paper presents a review on studies covering this last approach for dealing with temporal information: incorporating temporal reasoning into Support Vector Machines and its variations.
Keywords-Support Vector Machine; temporal reasoning; machine learning;  I. I NTRODUCTION  II.
S UPPORT V ECTOR M ACHINES  In the last decade, there has been a great increase the use of Support Vector Machines (SVM) in various applications.
The growing interest in this technique is justidZed by its good performance, presented in different studies applied to complex problems, including problems with temporal data.
Traditional SVM approaches aim at processing independent and identically distributed (iid) data, ignoring temporal dependencies among instances.
Therefore, for employing traditional SVM to temporal problems, it is necessary to extract temporal features, incorporate these features into a vector representation, and process them just as they were iid data.
Although this naive approach has provided some good results, it ignores important properties of the data.
We have adopted a taxonomy discussed in [1] in order to organize different SVM approaches regarding temporal data analysis.
First, as aforementioned, problems related to temporal analysis can be treated without considering, directly, temporal aspects of the data.
Although this approach does not take advantage of temporal dependencies within data, it is still very common because of its simplicity.
If we consider the temporal aspects, time can be externally or internally processed with respect to SVM model.
In order to treat time externally, we can build implict or explicit temporal data representations, which consist of, respectively, preprocessing  SVM are based on performing a non-linear mapping on input vectors from their original input space to a highdimensional feature space; and optimize an hyperplane capable of separating data in this high-dimensional feature space.
This section describes SVM formulation for classidZcation and regression problems.
1530-1311/12 $26.00 AS 2012 IEEE DOI 10.1109/TIME.2012.15  A. ClassidZcation Problem Considering a training set with d samples, dedZned by d {dd , dSd }d d=1 , with input dd a a and output dSd a {a1, +1}.
SVM aims at dZnding an optimal hyperplane which separates the datapoints in the feature space.
Such hyperplane is given by d (dL) = a"d a d(dd )aS + d,  where d is the optimal set of weights, d(dd ) represents a nonlinear mapping on dd , d is the optimal bias, and a"aaS is a dot product.
In order to optimize separating hyperplane, SVM aims at maximizing the functional margin, i.e., the distance between the hyperplane and the closest datapoints from the hyperplane.
As shown in [2], maximizing the margin corresponds to minimizing the set of weights d. The same author also states that, in order to achieve a good generalization, it 114  An dzd is assigned to each input vector.
After training, all non-zero dzd are called Support Vectors (SV).
In the models obtained with Lagrangian method, the terms d(dLd ) and d(dLd ) always appear multiplied.
This fact allows us to perform an implicit nonlinear mapping to a highdimensional feature space through kernels.
This approach is based on Cover Theorem, which states that a feature space with non-linearly separable data can be mapped with high probability into a input space where the data is linearly separable, provided that the mapping is non-linear and the feature space dimension is high enough [2].
Most common kernel functions can be seen in Table I.  is necessary to minimize the Vapnik-Chervonenkis (VC) dimension, which measures the capacity of the family of functions realized by a learning machine.
Thus, minimizing d also corresponds to minimizing VC dimension and dZnding an optimal hyperplane provides a SVM with the smallest VC dimension necessary to solve the classidZcation problem.
For problems which are nonlinearly separable in the feature space, we have to consider a soft margin optimisation, which assumes slack variables dd .
Such variable denotes the training error for the dth sample, d = 1, a a a , d , in order to dZnd an hyperplane allowing some classidZcation errors that are minimized into the optimisation problem.
There are two ways to implement this soft-margin optimisation using a 2norm or 1-norm [3].
Considering 2-norm soft margin, it is possible to dZnd the optimal hyperplane by minimizing:  Table I M OST COMMON KERNEL FUNCTIONS [2].
d dsa 2 1 a"d a daS + d , 2 2 d=1 d  min d(d) =  where ds is a regularization factor and dd = aLdSd a d (dLd )aL, subject to dSd (a"d a d(dd )aS + d) aL 1 a dd ,  d a  dzd a  d=1  B. Regression Problem Also, SVM may be adapted for regression tasks.
The technique is called Support Vector Regression (SVR) and considers slack variables dd decomposed in dd and dEd , which represents, respectively, errors above and below real output.
In the same way as in classidZcation problems, the formulation of the regression problem depends on the loss function.
The most common loss function is the d-insensitive loss function, which is formulated as  dzd dS d = 0  d=1  {  for d = 1, 2, a a a , d.  dzd =  Already considering 1-norm soft margin, it is possible to optimize a hyperplane by minimizing min d(d$?)
=  d a 1 a"d a daS + ds dd , 2 d=1  dSd (a"d a d(dd )aS + d) aL 1 a dd ,  min d(d) =  d = 1, ..., d  d = 1, a a a , d.  d a d=1  dzd a  d 1 a dzd dzd dSd dSd a"d(dd ) a d(dd )aS, 2 d,d=1  d ( ) a 1 a"d a daS + ds dd + dEd , 2 d=1  dd , dEd aL 0.
However, there are other loss functions, such as quadratic loss function and Huber loss function [4].
The quadratic loss function is dedZned as  dzd dS d = 0  dzddVdd = (d (dd ) a dSd )2 ,  d=1  ds aL dzd aL 0,  (1)  dSd a a"d a dd aS a d a$?
d + dd a"d a dd aS + d a$?
d + dEd  subject to d a  if aLdSd a d (dd )aL a$?
d if aLdSd a d (dd )aL > d,  subject to the restrictions  From this optimization problem, we can apply the Lagrangian method obtaining max a1 (ds) =  0, dSd a d (dd ) a d,  where d is a parameter dedZned by the user which states the maximum deviation that should be accepted by the algorithm, that is, errors below d are not considered errors.
In this case, the problem is formulated as minimizing  subject to dd aL 0,  (a"dd a dd aS + 1)d ( ) aLdd add aL2 exp 2 2dz ( ) tanh dz(a"dd a dd aS) + d  d = 1, ..., d.  d 1 a 1 dzd dzd dSd dSd a"d(dd ) a d(dd )aS + dzdd , 2 d,d=1 ds  dzd aL 0  Polynomial  Two Layer Perceptron  where dzdd = 1 if d = d and dzdd = 0 otherwise, subject to d a  Function  Radial Basis Function  From this optimization problem, we can apply the Lagrangian method obtaining max a1 (ds) =  Kernel  d = 1, 2, a a a , d.  which leads us to the problem formulation  115  min d(d) =  d ds a ( 2 E2 ) 1 a"d a daS + d + dd , 2 2 d=1 d  IV.
A NALYSED M ETHODS  (2)  This section presents a systematic organization of the papers selected by our review, covering the topic temporal reasoning using SVM.
The section is organized as follows: Section IV-A presents Recurrent LS-SVM; Section IV-B presents Support Vector Echo-State Machines; ProdZle-Dependent Support Vector Machines are described in Section IV-C; and Section IV-D and Section IV-E presents modidZed kernels for dealing with temporal data, considering respectively recurrent kernels and sequential kernel.
subject to dSd a a"d a dd aS a d a$?
dd a"d a dd aS + d a$?
dEd dd , dEd aL 0.
The Huber loss function is dedZned as dzdtdVddd =  aSS a"  1 (d (dd ) 2  a dSd ) 2 ,  aS daLd (dd a dSd )aL a  d2 , 2  if aLd (dd a dSd )aL > d  A. Recurrent Least-Squares Support Vector Machines  if aLd (dd a dSd )aL a$?
d.  Recurrent LS-SVM (RLS-SVM) is mostly applied to time series forecasting [7].
The idea is to consider as data a series of input data dVd and a series of output data dSd and an autonomous recurrent model such as  leading to a problem formulation similar to (2), as described in [4].
III.
L EAST-S QUARES S UPPORT V ECTOR M ACHINES  dSEd = d (E dSda1 , dSEda2 , ..., dSEdad ),  In [5], [6], a least squares type of SVM was introduced by changing the problem formulation so as to yield a linear set of equations in the dual space.
This is done by taking a least squares cost function, with equality instead of inequality constraints.
This modidZcation enable the classidZer to be solved through a set of linear equations instead of quadratic programming.
In LS-SVM, the classidZcation problem is formulated as min dL2 (d, d, d) =  where dSEd denotes an estimated output at the instant d, d is a nonlinear mapping, and the value d dedZnes model order.
Thus, we have that the sequence of previously estimated outputs are the input for the forecasting model.
Then, from SVM theory, we can consider [E dSda1 , dSEda2 , a a a , dSEdad ] as input1 for the model.
Also, the model is formulated in terms of error variables, i.e, RLS-SVM training depends on the error between estimated output and actual output [7].
Thus, the training is dedZned as the following optimization problem:  d ds a 2 1 a"d a daS + d , 2 2 d=1 d  subject to the equality constraints dSd [a"d a d(dd )aS + d] = 1 a dd ,  mindL (d, d, d) =  d = 1, a a a , d.  Applying Lagrangian method, we obtain  d+d ds a 2 1 a"d a daS + d , 2 2 d=d+1 d  (3)  subject to:  max a2 (d, d, d; dz) = d"(d, d, d) a  d a  dSd a dda1 = a"d a d(dda1aLdad )aS + d,  dzd {dSd [a"d a d(dd )aS + d] a 1 + dd },  where d is the lenght of the time series, dd = dSd a dSEd repdSda1 , dSEda2 , a a a , dSEdad ] a resents the error, and dda1aLdad = [E [dda1 , dda2 , a a a , ddad ].
Then, applying Lagrangian method, we obtain  d=1  with the following optimality conditions: d a aa dzd dSd ddd = 0, =0ada ad$?
d=1  max a(d, d, d; ds) = d"(d, d, d) d a [ ] + dzdad A dSd a dda1 a a"d a d(dda1aLdad )aS a d ,  d  a aa dzd dSd = 0, =0a ad d=1  aa = 0 a dzd = dsdd d = 1, a a a , d, add aa = 0 a dSd [a"d a d(dd )aS + d] a 1 + dd = 0, adzd  d = d + 1, a a a , d + d,  d=1  subject to (4), (5), (6), and (7).
As d can be dedZned in terms of ds, it is possible to discard the constraint (4), however, even discarding it, it is still computionally expensive to dZnd a solution considering all the other constraints, specially (6).
Thus, it is possible to consider the case when ds a a, which corresponds to ignoring the dZrst term in (3), i.e., aiming only at minimizing errors, subject to (5) and (7).
d = 1, a a a , d.  This set of equations can be written as the solution of a set of linear equations [5].
Thus, it is easier to calculate a solution for LS-SVM than for SVM.
However, LS-SVM has a disadvantage: the number of SV is proportional to the errors at the datapoints, losing sparsity provided by SVM.
Also, as in SVM, LS-SVM can be adapted for regression tasks [7].
1 In our notation, [dS da1 , dSda2 , a a a , dSdad ] is, therefore, equivalent to dd in the SVM model input.
116  aSS dL' dL' dL' dL' dL' dL' dL' dL' dL' dL' dL' dL' dL' dL' dL' dL' dL' a" dL' dL' dL' dL' dL' dL' dL' dL' dL' dL' dL' dL' dL' dL' dL' dL' dL' aS  d+d a aa =da dzdad d(dda1aLdad ) = 0 ad$?
d=d+1  (4)  d+d a aa = dzdad = 0 ad d=d+1  (5)  d a ] aa a [ a"d a d(dda1aLdad )aS = 0, = dsdd a dzdad a dzdad+d add addad d=1  aa = dSd a dd a a"d a d(dda1aLdad )aS a d = 0, adzdad  Some particularities of this approach must be highlighted: dZrst, it works only for unidimensional time series, since it uses previous estimated outputs to provide an estimation for the next; second, this approach considers ds a a, which means it does not control VC-dimension and does not aim at maximizing the margin.
In order to overcome these problems, [8] proposes a multidimensional RLS-SVM and [9] proposes some methods for controlling the parameter ds.
Also, in [10], RLS-SVM is used with a mixed kernel in order to obtain improve accuracy.
1) Regularized Recurrent Least-Squares Support Vector Machines: As aforementioned, the RLS-SVM ignores the regularization terms ds, due to the computational cost of calculating the derivative in the constraint (6).
However, [9] argues that this simplidZcation is not really necessary.
There are two other options: the former option is called regularized partially RLS-SVM by [8].
It consists on disconsidering the summation term in (6) and considering the optimization problem regarding to (3) as a whole.
The other option consists in considering all effects of parameter ds and the equations (3) and (6) by deriving the summation term in (6) into a set of nonlinear equation which must be solved nummerically [9].
2) Multidimensional Recurrent Least-Squares Support Vector Machines: The RLS-SVM was developed to deal with unidimensional data, which is useful for some applications, such as forecasting unidimensional time series [7].
However, most applications may use multidimensional data.
A multidimensional RLS-SVM can be dedZned by dividing a multidimensional regression problem into a series of unidimensional problems [8].
Consider a time series given by {d1 , d2 , a a a , dd }, dLd a ad , and a problem with input dLdad and output dSd a ad corresponding to an d-th element of the time series, where d is a time delay.
In order to tackle this problem, the authors in [8] propose to convert a d-dimensional problem to d unidimensional problems considering a weight matrix with d weight vectors; and d bias.
Incorporating this approach in (3), the problem can be formulated as min a(dd , dd , dd,d ) =  d = d + 1, a a a , d  d = d + 1, a a a , d  (6) (7)  subject to aSS dL' dL' dL' dL' dL' dL' dL' a" dL' dL' dL' dL' dL' dL' dL' aS  (1)  a dd,1 = a"d1 a d1 (dSda1 a dda1 )aS + d1 ,  (2) dSd  a dd,2 = a"d2 a d2 (dSda1 a dda1 )aS + d2 ,  dSd  (8)  .
.
.
(d)  dSd  a dd,d = a"dd a dd (dSda1 a dda1 )aS + dd ,  where d = d + 1, d = 2, .
.
.
, d + d a 1 and dd = dSd a dSEd .
This technique may also be applied to unidimensional time series, as long as this time series is reconstructed through Phase Space Reconstruction generating a multidimensional time series.
3) Recurrent Least-Squares Support Vector Machines with Mixed Kernel: RLS-SVM with a mixed kernel consists using a RLS-SVM with a kernel which combines Radial-Basis Function (RBF) and Polynomial Kernel (Poly), weighted by the parameter d, as ( ) ( ) ( ) dzdddL dd , dd = ddzddlds dd , dd + (1 a d)dzd dddS dd , dd .
In this context, the objective is to combine the strenghts of each approach: RBF is a local function, having stronger learning ability and weaker generalization ability; polynomial kernel is a global function, having stronger generalization ability and weaker learning ability [10].
Also, in the same paper is proposed the use of a Genetic Algorithm in order to optimize the parameters of the mixed kernel, i.e., the parameters of RBF, polynomial kernel and d. In order to evaluate the approach, the authors applied it to forecast a time series based on Rossler function.
For these data, the use of RLS-SVM with the mixed kernel presented better results than RBF kernel.
B.
Support Vector Echo-State Machines Another strategy for enabling SVM to perform a temporal analysis is called Support Vector Echo-State Machines (SVESM) and uses Reservoir Computing [4].
Reservoir Computing (RC) is a research dZeld which comprises techniques and methods for building Recurrent Neural Networks (RNN) using a areservoira.
A reservoir is a large neural network, with randomly and sparsely connected neurons.
Each neuron have an internal representation called states,  d d d a a 1a 1 a"dd a dd aS + ds d2 , 2 d=1 2 d=d+1 d=1 d,d  117  which keep some information about the previous states, meaning that the reservoir has memory.
Within the reservoir, input weights and internal weights are dedZned randomly and are not trained.
The SVESM is based on Echo-State Networks (ESN) that is a RNN which uses a reservoir to perform an accurate single-step prediction and then iterates in order to obtain multiple-step predictions [4].
The idea is that only the output weights must be trained, i.e., the reservoir performs a nonlinear mapping and a output weights are trained to perform a regression from reservoir state to the desired output.
Its equation can be written as  (or condZdence) of certain sample.
It is possible to use this penalization factor to implement an exponential memory decay based on the condZdence of past samples [13], as dd = ddAd adAd ,  d a [0, 1],  where dd is a weighting factor of parameter ds for the sample at time dAd , dAd is the time for the d-th sample and dAd is the current time.
PD-SVM is also used for classidZcation problems [13].
In this case, a weighting factor may be incorporated, depending on the problem domain, in order to improve accuracy and reduce false detections.
In [13], the classidZcation problem consists in classifying the level of Cyclosporine A in the blood of a pacient.
There are three classes: below 150ng/mL, between 150 and 400ng/mL, and above 400ng/mL.
As this problem contains temporal aspects, the weighting factor incorporates memory decay, aiming at favoring last samples, and also increases the penalization factor ds near the detection border in order to reduce false detections, such suggested in  ) ( dL(d + 1) = tansig dzd a dd + dzdd a dVd + dd+1 dS(d + 1) = a"d a dd aS,  where dAddd dd denotes the hyperbolic tangent function, dd denotes the state variables in the reservoir, dVd and dSd are the input and output of the ESN, and dd+1 is an optional noise vector.
dzd , dzdd and d are the internal connections, input and output weights, respectively.
As aforementioned, the reservoir provides a nonlinear mapping from input space to a state space.
The idea of SVESM is using the areservoir tricka instead of the akernel tricka to perform the nonlinear mapping [4].
Thus, input data is preprocessed by the reservoir and the reservoir states are used as input to a SVR with linear kernel, which performs the regression in order to obtain the desired output.
This study also presents three types of loss function which can be used in SVESM: quadratic loss function, dsensitive loss function, and Huber loss function, as described in Section II-B.
Finally, the study presents some simulations in order to evaluate SVESM performance.
The method outperforms classical methods a such as ESN, Multilayer Perceptron [11], SVM and Self Organizing Maps [12]; except in the prediction of the benchmark Mackey-Glass time series without noise and outliers, in which case ESN performs much better than any other method.
However, considering Mackey-Glass time series with noise and outliers, SVESM shows more robustness and outperforms ESN.
1 [d2 dsdA + ds1 ddAa1 + d0 ] 150 1 [d2 dsdA + ds1 ddAa1 + d0 ], = 400  ds150,dA+1 = ds400,dA+1  (9)  where dd represent additional penalization factor, which can be dZxed a priori or computed in an adaptive way [13].
D. Recursive kernels Recursive kernels operate on two discrete time series {d(1), d(2), a a a , d(d)} and {d(1), d(2), a a a , d(d)}2 [14].
These kernels are associated to an indZnite recurrent network, that is, a recurrent network with a hidden layer modeled as a continuous function3 .
A recursive kernel d at a time dA is given by ddA (dL, dS) = a"IS(dL(dA), IS(dL(dA a 1), IS(a a a ))) a IS(dS(dA), IS(dS(dA a 1), IS(a a a )))aS.
where IS(a, a) is a nonlinear mapping, dL(dA) is the current input, and IS(dL(dA a 1), IS(a a a )) corrresponds to the indZnitedimension state vector, i.e., a nonlinear mapping applied recursively to all past elements of the time series.
In [14], some examples of recursive kernels are presented.
These examples consider two parameters: d for scaling the indZnite-dimension state vector, and dd for scaling the current input.
The stability of these kernels usually depends on the choice of these parameters, as described in [14].
The list of kernels includes:  C. ProdZle-Dependent Support Vector Machines ProdZle-Dependent SVM (PD-SVM) method arises from a common practice for classidZcation problems with unbalanced data [13].
In this kind of problem, it is possible to dedZne different penalization factors for different classes, aiming at preventing false positives by favoring classes with less samples.
The same principle may be applied to time series, since more recent time seriesa samples may contain more relevant information than older samples.
In PD-SVM, the overall penalization factor ds from SVM is adjusted by a time-dependent weighting factor based on a condZdent function, i.e., a fuction that measures the relevance  2 In this section, dL and dS are used to designated two time series that are submitted to the kernel function.
As this section covers only kernel function and do not cover SVM formulation, it is not necessary to refer to SVM output.
Thus, dS does not corresponds to an output; it corresponds to one of the inputs submitted to the kernel function.
3 For further details about indZnite recurrent network and its relation to recursive kernels, see [14].
118  a  Recursive linear kernel: ddA (dL, dS) = dd2  a  a a  E. Sequence kernels In this review, we found also found another type of modidZed kernel for dealing with temporal aspects of data called sequence kernels, which are capable of dealing with sequences of vectors.
One of these sequence kernels is proposed by [15] and used by [16].
This sequence kernel is based on Dynamic Time Warping (DTW) and it is called polynomial DTW kernel.
DTW is a distance measure used for calculating distance between two sequences of vectors.
According to [15], symmetric DTW consists in considering a local distance between two vectors and a global distance, which is calculated using local distances and, indeed measures the distance between the two sequences of vectors.
Also, it is possible that these two sequences of vector do not have the same length.
In this case, it is necessary to perform an alignment between the sequences.
Considering d' and dl as sequences composed of vectors d(d) and d(d), this alignment may be denoted by dd (d) and dd (d) for d = {1, a a a , dz}, where dz is the length of the alignment, and consists in linking each vector in d(d) to a vector in d(d).
A DTW alignment distance is given by summiting local distances between the vectors under analysis:  a"d 2d dL(dA a d) a dS(dA a d)aS.
d=0  Recursive polynomial kernel: [ ]d ddA (dL, dS) = a"dd2 dL(dA) a dS(dA)aS + d 2 ddAa1 (dL, dS) .
a  Recursive gaussian (also known as Radial Basis Function a RBF) kernel: ddA (dL, dS) = exp  a  ( ) ( ) aLdL(dA) a dS(dA)aL2 ddAa1 (dL, dS) a 1 a exp .
2 2 2dd d  Recursive arcsine kernel: ddA (dL, dS) = 2 arcsin d  ( ( )) 2 dd2 a"dL(dA) a dS(dA)aS + d 2 ddAa1 (dL, dS) + dd2 a , ddA (dL)ddA (dS)  with ) ( ddA (dL) = 1 + 2 dd2 aLdL(dA)aL2 + d 2 ddAa1 (dL, dL) + dd2 .
It is important to note that SVM with recursive kernel works just as regular SVM.
The differences are: the kernel function considering recursion; and the input data which composes the training set must include d elements of the past of the time series, where d corresponds to the depth recursion.
Some experiments in [14] show the application of recursive kernels to regression and classidZcation problems.
As regression problem, a benchmark in time series processing called NARMA (nonlinear auto regressive moving average) was considered.
For comparing results, a windowed approach using SVM with RBF kernel, considering 27 frames as the size of the window, and an ESN were applied to the problem.
Both recursive kernels applied (RBF and arcsine with recursion depth of 50 frames) provided better results than classical approaches when all parameters were correctly set, with recursive RBF kernel performing slightly better than the recursive arcsine kernel.
As classidZcation problem, the authors in [14] considered a phoneme recognition problem, aiming at recognizing 39 symbols representing phonemes.
Each frame of speech was represented by a 39-dimensional feature vector obtained using Mel frequency cepstral coefdZcients analysis.
In this case, recursive RBF and recursive arcsine kernel were tested against a windowed approach using SVM with RBF kernel considering a window of 9 frames.
The recursion depth of the recursive kernels varied according to the experiment, from 5 to 15 frames.
The best results in [14] were obtained using the recursive RBF kernel on subsampled data by a factor of 5, i.e., the time series was aspeeded upa by a factor of 5, which corresponds to slowing down the effective timescale of the classidZer [14].
dVddddd (d', dl) =  dz 1a d(ddd (d) , ddd (d) ).
dz d=1  Then, global distance is obtained by calculating the global distance to each possible alignment and considering dV(d', dl) = min dVddddd (d', dl).
(10)  In order to build a kernel, DTW global distance has to be converted to a dot product.
For this conversion, a technique called Spherical Normalization is used.
It consists on projecting the sampled vectors d and d on a unit hypersphere, as described in [15], through4 d E= a  1 (d2  +  dz2 )  [ ] d .
dz  (11)  By dedZnition, the smallest curve between the two projected vectors d E and Ed in a hypersphere is given by the angle E a Ed = cos ddE,Ed , ddE,Ed between these points [15].
Considering d local DTW distance may be dedZned as the dot product: dd (E d, Ed) = arccos(E d a Ed).
(12)  Since it is possible to calculate DTW considering sequences mapped into a hypersphere, it is possible to perform DTW to get the global distance using (10), using (12) as local distance.
Then, a linear DTW kernel can be dedZned through the equation which reconverts DTW global distance to a dot product: E dl) E = cosdVd (d' E a dl).
E dzdddddddVd d (d', 4 Equation  119  11 is also applied to vector d.  From this kernel, it is also possible to dedZne the polynomial DTW kernel as:  a  E dl) E = cosd dVd (d' E a dl).
E dzddddSdVd d (d',  a  Both studies [15] and [16] presents experiments showing that polynomial DTW kernel can promote improvements on accuracy for speech recognition tasks.
The study [15] shows that polynomial DTW kernel performs better than traditional DTW and Hidden Markov Models (HMM) for recognizing speech collecting from both normal and dysarthric speakers.
For normal speakers, SVM with polynomial DTW kernel performs better when there is a small number of training samples per word; for more than 5 training samples per word, SVMdVd d performs as well as HMM and better then DTW.
For dysarthric speakers, SVMdVd d performs better than HMM and DTW for any number of training samples per word.
In [16], SVM with polynomial DTW kernel performs better than Multilayer Perceptron neural network and Elman network for recognizing speech from dysarthric speakers.
However, its performance is comparable to SVM with RBF kernel: one method outperforms the other depending on speaker intelligibility.
Also, in [15], three other examples of sequence kernels are cited: the Fisher kernel [17], the Generalized Discriminant kernel [18] and the pair HMM kernel [19].
The Fisher kernel incorporates a generative model into a discriminant classidZer, by generating a kernel using the Fisher Score on the choosen generative model.
The Generalized Linear Discriminant Sequence (GLDS) kernel is a sequence kernel based on the Generalized linear Discriminant Scoring and on a classidZer.
Finally, the pair HMM kernel is based on converting the matching function of pair HMM into a dot product for using it as a kernel.
a  a  a  a  Multidimensional RLS-SVM [8]: converts a multidimensional problem into d unidimensional problems, allowing multidimensional time series forecasting; RLS-SVM with Mixed Kernel [10]: combines kernels with different characteristics, in order to provide more efdZcient non-linear mappings to RLS-SVM; SVESM [4]: uses a reservoir to treat the temporal information, performing an explicit non-linear mapping instead of the implicit mapping performed by kernels; PD-SVM [13]: assigns greater weights to more recent data, in a context of sequential events, creating a new viewpoint to analyze a dataset; Recursive Kernels [14]: transforms the time series under analysis in a set of series organized in a training dataset, performing the analysis in a recursive way; it can be applied to classidZcaton tasks using a windowed approach to build the training dataset; Sequential Kernels [15], [16], [17], [18], [19]: enables the implict treatment of sequences of vectors, e.g., multidimensional time series.
Considering the objective of each strategy described here, note that techniques related to RLS-SVM aim mainly at time series forecasting.This phenomenon occurs because RLSSVM techniques work with output in the same domain as inputs, since delayed outputs are used to compose the input vector.
The same does not occur with PD-SVM, SVESM and modidZed kernels, since they incorporate temporal reasoning in data processing, but do not use delayed outputs as input; thus, these techniques can be used to provide outputs from any domain.
Also, it is possible discuss about how the strategies modify the SVM operation.
PD-SVM only balances the penalization factor ds using a time-dependent weighting factor, without altering any other aspect of SVM training.
In SVESM, the SVM operation also does not change, since the main difference is that the non-linear mapping, originally performed by kernels in tradition SVM approachs, is performed by a reservoir.
ModidZed kernels also do not modify SVM operation, since only the kernel is affected.
On the other hand, RLS-SVM and its variations modify SVM formulation and operation in order to process recurrence.
As we have shown in this review, incorporating temporal reasoning to SVM has provided interesting and useful strategies for temporal data analysis.
In this context, we hope that this review supplies a basis on the subject, aiming at supporting the development of others studies which can provide further insights on the related area.
Some initiatives that will deserve our attention in our future efforts of reaseach are:  V. F INAL C ONSIDERATIONS This paper presented a review of research which combines SVM and some kind of temporal reasoning to provide strategies for data analysis that join the advantages of SVM with the ability to explore temporal dependencies among data.
Although most strategies have been applied to timeseries forecasting [7], [9], [8], [10], [13], [4], there are also the possibility to apply it for temporal data classidZcation and regression [13], [14], [15], [16], [17], [18], [19].
The main contribuition of each strategy presented here can be summarize as follows: a RLS-SVM [7]: modidZes SVM in order to incorporate recurrence, including the modidZcation of the equations that dedZne SVM operation to consider the error for previous output in SVM training; a Regularized RLS-SVM [9]: introduces a way to balance errors minimization with margin maximization, which also corresponds to controlling VC-dimension, dealing with a negative aspect of RLS-SVM;  a  120  a practical study around the approach discussed here, applying them on benchmarks classidZcation, regression or prediction problems, in order to support further analysis about the advantages and limitations of each  approach; an specidZc study about the feasibility of the approaches discussed here in a gesture analysis problem (area of interest to the present paperas authors).
From these future works, the authors aim to achieve enough expertise to propose some directions about when each approach is more useful or feasible, and, dZnally, to contribute with the improvement of the relation between SVM, or even more general Machine Learning approaches, with temporal data processing.
[9] H. Qu, Y. Oussar, G. Dreyfus, and W. Xu, aRegularized recurrent least squares support vector machines,a in International Joint Conference on Bioinformatics, Systems Biology and Intelligent Computing.
IEEE Computer Society, aug. 2009, pp.
508a511.
a  [10] J. Xie, aTime series prediction based on recurrent ls-svm with mixed kernel,a in Asia-PacidZc Conference on Information Processing, vol.
1, jul.
2009, pp.
113 a116.
[11] F. Rosenblatt, Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms.
Spartan, 1962.
Acknowledgement The authors thank SaEo Paulo Research Foundation (FAPESP), Brazil, for its dZnancial support through process number 2011/04608-8.
Also, the authors thank PhD Alexandre Ferreira Ramos, for the interesting insights about some mathematical derivations used during the writing of this paper.
[12] T. Kohonen, aThe self-organizing map,a Proceedings of the IEEE, vol.
78, no.
9, pp.
1464 a1480, sep 1990.
[13] G. Camps-Valls, E. Soria-Olivas, J. Perez-Ruixo, F. PerezCruz, A. Artes-Rodriguez, and N. Jimenez-Torres, aTherapeutic drug monitoring of kidney transplant recipients using prodZled support vector machines,a IEEE Trans.
on Systems, Man, and Cybernetics, Part C: Applications and Reviews, vol.
37, no.
3, pp.
359 a372, may 2007.
R EFERENCES  [14] M. Hermans and B. Schrauwen, aRecurrent kernel machines: Computing with indZnite echo state networks,a Neural Computation, vol.
24, pp.
104a133, jan. 2012.
[1] J.-C. Chappelier and A. Grumbach, aTime in neural networks,a SIGART Bulletin, vol.
5, no.
3, pp.
3a11, jul.
1994.
[2] S. Haykin, Neural Networks: A Comprehensive Foundation, 2nd ed.
Upper Saddle River, NJ: Prentice Hall, 1999.
[15] V. Wan and J. Carmichael, aPolynomial dynamic time warping kernel support vector machines for dysarthric speech recognition with sparse training data,a in Annual Conference of the International Speech Communication Association, sep. 2005, pp.
3321a3324.
[3] N. Cristianini and J. Shawe-Taylor, An Introduction to Support Vector Machines and Other Kernel-based Learning Methods.
Cambridge, UK: Cambridge University Press, mar.
2000.
[16] F. Rudzicz, aPhonological features in discriminative classidZcation of dysarthric speech,a in IEEE International Conference on Acoustics, Speech and Signal Processing, apr.
2009, pp.
4605 a4608.
[4] Z. Shi and M. Han, aSupport vector echo-state machine for chaotic time-series prediction,a IEEE Trans.
on Neural Networks, vol.
18, no.
2, pp.
359 a372, mar.
2007.
[5] J. Suykens and J. Vandewalle, aLeast squares support vector machine classidZers,a Neural Processing Letters, vol.
9, pp.
293a300, jun.
1999.
[17] T. S. Jaakkola and D. Haussler, aExploiting generative models in discriminative classidZers,a in Conference on Advances in Neural Information Processing Systems.
Cambridge, MA, USA: MIT Press, nov. 1999, pp.
487a493.
[6] J.
A. K. Suykens, T. Van Gestel, J.
De Brabanter, B.
De Moor, and J. Vandewalle, Least squares support vector machines.
World ScientidZc Pub., 2003.
[18] W. M. Campbell, aGeneralized linear discriminant sequence kernels for speaker recognition,a in IEEE International Conference on Acoustics, Speech, and Signal Processing, vol.
1, may 2002, pp.
Ia161 aIa164.
[7] J.
A. K. Suykens and J. Vandewalle, aRecurrent least squares support vector machines,a IEEE Trans.
on Circuits and Systems-I, vol.
47, pp.
1109a1114, jul.
2000.
[19] C. Watkins, aDynamic alignment kernels,a University of London, Tech.
Rep., 1999.
[8] J.
Sun, C. Zheng, Y. Zhou, Y. Bai, and J. Luo, aNonlinear noise reduction of chaotic time series based on multidimensional recurrent ls-svm,a Neurocomputing, vol.
71, pp.
3675a 3679, oct. 2008.
121