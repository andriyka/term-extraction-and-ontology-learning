Extending Temporal Reasoning with Hierarchical Constraints Fei Song Dept.
of Computing and Information Science University of Guelph Guelph, Ontario, Canada N1G 2W1 Abstract Existing reasoning algorithms for Allen's interval algebra may produce weak results when applied to temporal networks that involve decompositions of intervals.
We present a strengthened procedure for reasoning about such hierarchical constraints, which works interactively with an existing algorithm for temporal reasoning, to produce the desired stronger results.
We further apply our algorithm to the process of plan recognition and show that such an application can both reduce the number of candidate plans and make the constraints in the remaining plans more specific.
1.
Introduction Allen's (1983a) interval algebra has shown to be useful for such applications as knowledge-based systems, natural language processing, and planning (as described in Vilain, et al., 1989).
For example, a simplified plan for making a pasta dish can be represented as the temporal network in figure 1, where a node corresponds to the time interval over which a state holds or an event occurs, and a link label represents the temporal constraint between two intervals1.
InKitchen  Includes  Meets Make Pasta Dish  Includes Includes  Includes Includes  Make BeforeMeets Noodles Make Sauce  Ready ToEat  Boil BeforeMeets Put Noodles Together BeforeMeets  Figure 1: Temporal network of a cooking plan 1 Here, "Includes" and "BeforeMeets" are high-level  constraints, defined as the sets {si, di, fi, eq} and {b, m}, where b, m, eq are the basic relations "before", "meets", "equal", and si, di, fi are the inverses of "starts", "during", and "finishes" in Allen's interval algebra.
Given a temporal network, an important reasoning task is to compute the so-called minimal labeling, that is, to find the set of minimal constraints if the network is consistent (Vilain and Kautz, 1986).
A constraint (or a link label) is minimal if each of its basic relations is part of a consistent singleton labeling, for which each link is labeled by a basic relation and all the links in the network are satisfied.
Vilain and Kautz (1986) show that the time complexity of such a reasoning task is NP-complete for the interval algebra, where the problem size is the number of intervals.
However, this does not prevent people from proposing polynomial algorithms that are approximate for the interval algebra (Allen 1983; Van Beek 1989).
Allen's algorithm has O(n3 ) time complexity and is shown to be exact only for a subset of the interval algebra (Van Beek 1989).
Van Beek then proposes an O(n4 ) algorithm which is exact for a larger subset (the subset of the interval algebra that can be translated into the point algebra).
To get more exact results for the full interval algebra, one may have to use some exponential-time algorithms (e.g., Valdes-Perez 1987).
One problem with these existing algorithms is that they may produce weak results when applied to temporal networks that involve hierarchical constraints (i.e.
the decompositions of intervals into low-level subintervals).
In Song and Cohen (1991), we proposed a strengthened algorithm for temporal reasoning about hierarchical constraints.
The algorithm guarantees that the result is no weaker than that obtained from the existing temporal reasoning algorithms.
However, whether it can derive the minimal labeling for the hierarchical constraints depends on the order in which lower-level constraints are combined.
In this paper, we present a new order-independent reasoning procedure for hierarchical constraints, along with formal proofs for its associated properties.
We further apply our  algorithm to the problem of plan recognition, and show that the observed temporal constraints can both reduce the number of candidate plans and make the constraints in the remaining candidate plans more specific.
2.
Weak results from the existing algorithms A hierarchical constraint corresponds to the decomposition of an interval to a set of subintervals.
In terms of Allen's interval algebra, this means that the interval temporally includes all the subintervals.
Suppose that initially there is no specific constraint between a1 and a2 in figure 2(a).
Then, all we can decide is that A Includes a1 and A Includes a2, where Includes stands for the constraint {si, di, fi, eq}.
(a) Includes  {si,di}  a1  A  {b}  A1  Includes  a1  a2  a1  a3  Common  (b)  A2  A1 {si,di}  {di,fi}  {b}  a1  {b}  a2 {fi}  (c)  {si}  {fi}  {b}  a3  A2  A1 {si}  {di,fi}  {si,di}  {fi}  {b}  a2  a3 Figure 3: (a) Two decompositions, (b) Weak results from Allen's , and (c) Strong results desired  (c) {si}  includes  includes  a2  a1  a2  {di,fi}  A2  includes  includes  A  a1 (b)  (a)  A  {b}  {fi}  a2  Figure 2: (a) One decomposition, (b) Weak results from Allen's algorithm, and (c) Strong results desired  Now, if we add a new constraint {b} between a1 and a2, then we can use Allen's algorithm to propagate the constraint and produce the results shown in figure 2(b).
However, since a1 and a2 are the only subintervals of A and we know that a1 is located before a2, we should be able to decide that a1 is the starting part of A and a2 is the finishing part.
In other words, we should get the desired results shown in figure 2(c).
Such weak results can be carried further for networks that consist of more than one decomposition.
Suppose that initially we have the network shown in figure 3(a).
Later, if we add the constraints a1 {b} a2 and a2 {b} a3, we get the results shown in figure 3(b) using Allen's algorithm, where "Common" stands for the constraint: {o, oi, s, si, d, di, f, fi, eq}.
However, using a similar argument as made for the previous example, we should be able to get the stronger results shown in figure 3(c).
There are also networks that are considered to be consistent by Allen's algorithm but in fact are not when decompositions are involved.
For example, the network in figure 4(a) is regarded as consistent by Allen's algorithm, since we get the same network after applying the algorithm.
However, this is actually not true because if a1 and a2 are the only subintervals of A and a1 is located before a2, a2 should be the finishing part of A, not an interior part, as shown in figure 4(b).
(a) {si}  a1  A  {b}  (b) {di}  {si}  a2  a1  A  {fi}  {b}  a2  Figure 4: (a) Weak results from Allen's, and (b) Strong results desired  Such weak results are not simply caused by the inexactness of Allen's algorithm.
In fact, Allen's algorithm is exact for all these examples since the constraints used fall into a subset of the interval algebra for which Allen's algorithm is guaranteed to find the set of minimal labels (Van Beek 1989).
The reason for these weak results is that Allen's algorithm treats all the intervals as independent of each  other.
This is certainly not true for decompositions, since the abstract intervals are temporally dependent on their subintervals.
To make these dependencies explicit in the reasoning process, we need to assume that the decomposition of an abstract interval into its subintervals is complete, that is, no more subintervals can be added to the decomposition.
As a result, we can compute how an abstract interval is temporally bounded by its subintervals based on the constraints between all the subintervals.
For instance, if there is a linear ordering between all the subintervals, then we can clearly decide that the abstract interval is temporally bounded by the subintervals that occur the earliest and the latest.
We say that a decomposition is closed if the constraints between the abstract interval and its subintervals are minimal with respect to the constraints between all the subintervals.
More formally, we describe an abstract interval as the convex hull or the minimal cover of its subintervals, denoted by the equation: A = x1 + x2 + [?][?][?]
+ xn where A denotes the abstract interval and x1 , x2 ,..., xn denote the subintervals.
For the example in figure 3, the two decompositions can be represented as: A1 = a1 + a3 and A2 = a 2 + a3 .
Closing a decomposition means closing every decomposition edge between the abstract interval and its subintervals, which further implies computing the minimal labels on the decomposition edges.
We formally define a closure operation by the following first-order formula: A(Ci1 ,Ci2 ,...,Cim )c xi = xi C1 x1 [?]
xi C2 x2 [?]
[?][?][?]
[?]
xi Cm xm [?]
(A = x1 + x2 + [?][?][?
]xm ) where Ci1 ,Ci2 ,...,Cim are the constraints between the subinterval xi and all the subintervals from x1 to xm .
This formula suggests that a decomposition edge can be closed by using the existing constraints between a subinterval and all the subintervals and the fact that the abstract interval is the minimal cover of all its subintervals.
3.
Development of a strengthened temporal reasoning algorithm  To describe our strengthened algorithm for temporal reasoning with hierarchical constraints, we start with the simple case of closing one and two subintervals, and then, we generalize the result to close more than two subintervals.
After that, we provide a recursive procedure to close more than one decomposition, and finally, we present the strengthened algorithm that closes all the hierarchical structures in a temporal network.
3.1.
Closing one and two subintervals As described earlier, closing a decomposition means computing the minimal labels on all the decomposition edges.
If an abstract interval has only one subinterval, then the minimal label on the decomposition edge is obviously {eq}, that is: A(Cii )c xi = xi Cii xi [?]
(A = xi ) = xi {eq}xi [?]
(A = xi ) = A{eq}xi In the case of two subintervals, we can derive from the definition: A(Cii ,Cij )c xi = xi Cii xi [?]
xi Cij x j [?]
(A = xi + x j ) = xi {eq}xi [?]
xi Cij x j [?]
(A = xi + x j ) Since xi {eq}xi always holds for an interval (the so-called node consistency), we define the basic closure operation on a constraint as: A Cijc xi = xi Cij x j [?]
(A = xi + x j ) .
Lemma 1: Given Cij as a set of basic relations R1 , R2 ,..., Rm , the basic closure Cijc can be computed as {R1c , R2c ,..., Rmc}, where Rc is one of the four basic relations: si, di, fi, and eq, as defined in table 1.
R  Table 1: basic closure on basic relations b bi m mi o oi s si d di f fi eq  Rc si fi si fi si fi si eq di eq fi eq eq The validity of table 1 can be easily verified.
For example, if xi {b}x j , then the closed edge between A and xi is {si}, since if A consists of only xi and x j and xi is located before x j , then xi must be the starting part of A.
This basic closure operation also applies to  the case of one subinterval, i.e., (Cii )c = Ciic , since ACiic xi = A{eq}c xi = A{eq}xi .
3.2.
Closing more than two subintervals Having defined the basic closure operation, we can now extend it to close a decomposition of more than two subintervals.
More are the constraints specifically, if Ci1 ,Ci2 ,...,Cim between the subinterval xi and all the subintervals from x1 to xm , including the subinterval xi , then we can close xi and another subinterval x j using the basic closure operation: Cijc .
To get the final closed decomposition edge to xi , however, we need to somehow combine all of the Cijc 's.
It turns out that these Cijc 's can be combined with the normal composition operation in Allen's interval algebra.
Lemma 2 Given the basic relations Ri1 , Ri2 ,..., Rin ,we have A(Ri1 , Ri2 ,[?][?][?
], Rin )c xi = A(R x R x [?][?][?]
x R )xi .
c i1  c i2  c in  Proof.
We prove this lemma by induction on the number of subintervals.
For n = 1, we showed in the last subsection that A(Rii )c xi = ARiic xi .
For n = 2, we have: A(Rii , Rij )c xi = ARijc xi A(Riic x Rijc )xi = A({eq}c x Rijc )xi = ARijc xi So, the lemma holds for both n = 1 and n = 2.
Assume the lemma holds for n = k, that is, A' (Ri1 , Ri2 ,..., Rik )c xi = A' Ri1c x Ri2c x [?][?][?]
x Rikc xi , where A' = x1 + x2 + [?][?][?]
+ xk , we need to prove that the lemma also holds for n = k+1.
We know from table 1 that Rijc can only be one of the four basic relations: si, di, fi, and eq.
By checking table 2, a sub-multiplication table drawn from Allen's (1983a), we see that these four basic relations are closed under multiplication.
Table 2: A Sub-Multiplication Table  x si di fi eq  si si di di si  di di di di di  fi di di fi fi  eq si di fi eq  It follows that Ri1c x Ri2c x [?][?][?]
x Rikc can only be one of the four basic relations: si, di, fi, and eq.
Let us denote Ri1c x Ri2c x [?][?][?]
x Rikc as R , and Rikc +1 as S. Now, from the definition of the closure operation, we have: A(Ri1 ,..., Rik , Rik +1 )c xi = xi Ri1 x1 [?]
[?][?][?]
[?]
xi Rik xk [?]
xi Rik +1 xk +1 [?]
(A = x1 + [?][?][?]
+ xk + xk +1 ) = xi Ri1 x1 [?]
[?][?][?]
[?]
xi Rik xk [?]
(A' = x1 + [?][?][?]
+ xk ) [?]
xi Rik +1 xk +1 [?]
(A"= xi + xk +1 ) [?]
(A = A' + A") = A' (Ri1 ,..., Rik )c xi [?]
A" Rikc +1 xi [?]
(A = A' + A") = A' (Ri1c x [?][?][?]
x Rikc )xi [?]
A" Rikc +1 xi [?]
(A = A' + A") = A' Rxi [?]
A"Sxi [?]
(A = A' + A") To further evaluate the above expression, we need to consider the following special cases: (1) If A' {si}xi [?]
A"{si}xi , then we have A{si}xi , since if xi is the starting part of both A' and A" and A = A' + A", then xi should also be the starting part of A.
(2) If A' {fi}xi [?]
A"{fi}xi , then we have A{fi}xi .
The reason is similar to case (1) above.
(3) If A' {si}xi [?]
A"{fi}xi , then we have A{di}xi .
The reason for this is that if xi is the starting part of A', then there must be another interval that finishes after xi .
Similarly, if xi is the finishing part of A", then there must be another interval that starts before xi .
Thus, there are intervals that starts before xi and finishes after xi , and xi must be an interior part of the covering interval A.
(4) If A' {di}xi [?]
A"Sxi , then we have  A{di}xi , since if xi is an interior part of A', it is also an interior part of A.
(5) If A' {eq}xi [?]
A"Sxi , then we have ASxi .
This is obviously true since A' equals xi .
Since conjunctions are commutative, it is easy to see that these results are exactly the same as table 2 above.
In other words, we have proved that: A(Ri1 ,..., Rik , Rik +1 )c xi = A(R x S)xi = A(Ri1c x [?][?][?]
x Rikc x Rikc +1 )xi .
that is, lemma 2 also holds for n = k+1.
Lemma 2 implies that if the constraints between one subinterval and all the subintervals are one of the basic relations, the decomposition edge to the subinterval can be closed by multiplying the basic closures of these constraints.
Theorem 1: Given Ci1 ,Ci2 ,...,Cim as the constraints between xi and all the subintervals from x1 to xm , the closed edge between A and xi can be computed as follows: c c c c A(C  i1 ,Ci2 ,...,Cim ) xi = ACi1 o Ci2 o [?][?][?]
o Cim xi .
Proof.
The theorem can be proved by expanding constraints into sets of basic relations, converting the result into disjunctions of conjunctions, and applying lemma 2 to all the conjunctions: A(Ci1 ,Ci2 ,...,Cim )c xi = xi Ci1 x1 [?]
xi Ci2 x2 [?]
[?][?][?]
[?]
xi Cim xm [?]
(A = x1 + x2 + [?][?][?]
+ xm ) = xi {R11 , R12 ,..., R1n1 }x1 [?]
xi {R21 , R22 ,..., R2n2 }x2 [?]
...... xi {Rm1 , R2 m2 ,..., Rmnm }xm [?]
(A = x1 + x2 + [?][?][?]
+ xm ) = (xi R11 x1 [?]
xi R21 x2 [?]
[?][?][?]
[?]
xi Rm1 xm [?]
A = x1 + x2 + [?][?][?]
+ xm ) [?]
(xi R11 x1 [?]
xi R21 x2 [?]
[?][?][?]
[?]
xi Rm2 xm [?]
A = x1 + x2 + [?][?][?]
+ xm ) [?]
...... (xi R1n1 x1 [?]
xi R2n2 x2 [?]
[?][?][?]
[?]
xi Rmnm xm [?]
A = x1 + x2 + [?][?][?]
+ xm ) = A(R11 , R21 ,..., Rm1 )c xi [?]
A(R11 , R21 ,..., Rm2 )c xi [?]
...... A(R1n1 , R2n2 ,..., Rmnm )c xi c = AR11c x R21c x [?][?][?]
x Rm1 xi [?]
c c c AR11 x R21 x [?][?][?]
x Rm2 xi [?]
...... c c AR1nc 1 x R2n x [?][?][?]
x Rmn xi 2 m c c c = ACi1 o Ci2 o [?][?][?]
o Cim xi .
Lemma 3 Given constraints as subsets of {si, di, fi,eq}, the composition is commutative and associative, that is,  C1 o C2 = C2 o C1 , and (C1 o C2 ) o C3 = C1 o (C2 o C3 ).
Proof.
Given two subsets of {si, di, fi,eq}, the composition is both commutative and associative since for each pair of the basic relations, the results of multiplications are symmetric, as shown previously in table 2.
Theorem 2: In closing a decomposition constraint using theorem 1, we get the same result no matter what order we do the compositions.
Proof.
This follows directly from lemma 3, since the composition is both commutative and associative for subsets of {si, di, fi,eq}.
Based on theorems 1 and 2, we now present a new procedure for closing a decomposition of any number of subintervals.
procedure CLOSE(k, S) begin for each i [?]
S do begin t - {eq} for each j [?]
S do t - t o Cijc t - t [?]
Cki if t [?]
Cki then begin Cki - t Cik - INVERSE(t) Q - Q [?]
RELATED_PATHS(k, i) end end end Figure 5: Procedure for closing a decomposition  The above procedure closes all the decomposition edges in turn, and if a closed edge is more specific than the existing edge, the existing edge will be updated and all the related paths will be queued for further propagation.
Theorem 3.
The time complexity of the CLOSE procedure is O(m2 ) where m is the number of subintervals in a decomposition.
3.3.
Closing all the decompositions in a hierarchical structure A hierarchical structure often consists of more than one decomposition.
Our strategy is to close a hierarchy in a post-order fashion, since higher-level intervals can be defined in terms of lower-level subintervals.
In other words, we start the closing process from the bottom-level decompositions and work our way up until all the decompositions are closed in the hierarchy.
procedure CLOSE_ALL (k) begin get a list S of subintervals for k if S is not empty then begin for each i [?]
S do CLOSE_ALL (i) CLOSE (k, S) end end Figure 6: Closing all the decompositions in a plan  Theorem 4.
The time complexity of the CLOSE_ALL procedure is bounded below by O(n) and above by O(n2 ), where n is the number of intervals in a plan.
3.4.
The Strengthened Algorithm The CLOSE_ALL procedure closes all the decompositions in a hierarchical structure.
To get stronger results for a temporal network, we first use an existing reasoning algorithm to compute the set of constraints to be as specific as possible.
Then, for each hierarchical structure in the temporal network, we recursively close all the decompositions using the CLOSE_ALL procedure.
After that, some of the decomposition edges may be updated,  and we call the temporal reasoning algorithm again to propagate the effects of these new constraints.
Thus, we generally need to call interactively an existing reasoning algorithm and our CLOSE_ALL procedure.
Such a process will eventually terminate since every time we update a constraint, some of its basic relations will be eliminated and there are at most 13 basic relations in any constraint.
We now give the strengthened algorithm for temporal reasoning with hierarchical constraints: algorithm STRENGTHENED begin Q-{initial paths in a temporal network} H-{roots of all hierarchical structures} while Q is not empty do begin MODIFIED_TR foreach k [?]
H do CLOSE_ALL (k) end end Figure 7: The strengthened algorithm for temporal reasoning about plans  The set H contains the roots of all hierarchical structures, and CLOSE_ALL closes all the decompositions in a hierarchy.
The set Q contains those paths whose effects need to be propagated, and MODIFIED_TR is the same as an existing algorithm for temporal reasoning except that the initialization of Q is removed from the procedure.
Theorem 5.
The time complexity of our strengthened algorithm is at most O(T log2n), where n is the number of intervals in a temporal network and T is the time complexity of an existing reasoning algorithm (n3 for the path-consistency procedure, n4 for Van Beek's procedure, and exponential for some more exact procedures).
Proof: First, each iteration of our algorithm takes O(T+n 2 ) time or O(T), since all the existing algorithms take time at least O(n3 ).
Second, the worst case corresponds to a balanced binary tree, with maximum levels of decompositions and for which the effects of closed decompositions need to be propagated upwards.
Thus, we have the factor of log2n for the number of iterations.
4.
An application to plan recognition Plan recognition is the process of inferring an agent's plan based on the observation of the agent's actions.
A recognized plan is useful in that it helps to decide an agent's goal and predict the agent's next action.
For example, if we observe that John has made the sauce and he is now boiling the noodles, then based on the plan shown in figure 1, we can decide that John's goal is to make a pasta dish and his next action is to put noodles and sauce together.
Plan recognition has found applications in such areas as story understanding, psychological modeling, natural language pragmatics, and intelligent interfaces.
(a)  Make Noodles Includes  Includes  Includes  Measure BeforeMeets Mix BeforeMeets Flour Dough Make Sauce  (b) Includes  Thaw Beef (c)  Roll Dough  Includes  BeforeMeets  Mix {b,m} Dough  Includes  Heat BeforeMeets Beef  Add Paste  Thaw {b,m} Roll Beef Dough {b,m} Heat {b,m} Boil {b,m} Beef Noodles  Add Paste  Figure 8: An extended plan for making a pasta dish  Most existing models for plan recognition assume a library of all possible plans that might occur in a particular domain.
Then, through some kind of search and matching mechanism, one can find all the plans that contain the observed actions, called candidate plans.
Since the observation of an agent's actions is often incomplete and some actions may appear in many different plans of the plan library, it is often difficult to determine the unique plan that an agent is pursuing.
Kautz (1987) suggests  that one way of reducing the number of candidate plans is to use various kinds of constraints, including the temporal relations explicitly reported in the observations, to further eliminate those inconsistent plans2 .
However, Kautz only adopted a subset of Allen's interval algebra and did not use fully the temporal constraints that correspond to the decomposition edges in a candidate plan.
Our approach to plan recognition is to represent a plan as a temporal network and perform temporal reasoning to eliminate those candidate plans that are inconsistent with the temporal constraints explicitly given in the observations.
Such a reasoning process can have two useful effects: the given constraints can be used to reduce the number of candidate plans (an example of this effect can be found in Song and Cohen (1991)) and the given constraints can be made more specific by combining them with the prestored constraints in a candidate plan.
To illustrate the second effect, we extend the plan for making a pasta dish in section 1 by adding the decompositions for MakeNoodles and MakeSauce, shown in figure 8(a) and (b).
Suppose that the observation of an agent's actions is given in figure 8(c).
We can then use Allen's algorithm or our strengthened algorithm to make some of the constraints in the plan more specific.
Using Allen's algorithm, we can make some of the constraints more specific, as shown in figure 9(a).
Using our strengthened algorithm, we can make these constraints even more specific, as shown in figure 9(b).
MakePastaDish {si, di} MakeNoodles MakePastaDish {si, di} MakeSauce MakePastaDish {di, fi} PutTogether MakeNoodles {si, di} MeasureFlour MakeNoodles {di, fi} RollDough MakeSauce {si, di} ThawBeef MakeSauce {di, fi} AddTomatoPaste MakeNoodles {o, s, d} MakeSauce MakeNoodles {b, m} BoilNoodles Figure 9(a): Results from Allen's Algorithms 2 Other solutions include the use of preference heuristics  (Allen, 1983b; Litman, 1985; Carberry, 1986) and probabilities (Goldman and Charniak, 1988).
MakePastaDish {si} MakeNoodles MakePastaDish {di} MakeSauce MakePastaDish {fi} PutTogether MakeNoodles {si} MeasureFlour MakeNoodles {fi} RollDough MakeSauce {si} ThawBeef MakeSauce {fi} AddTomatoPaste MakeNoodles {o} MakeSauce MakeNoodles {b} BoilNoodles Figure 9(b): Results from our strengthened algorithm  5.
Conclusion We presented a strengthened algorithm for temporal reasoning about plans, which improves on straightforward applications of the existing reasoning algorithms for Allen's interval algebra.
We view plans as both temporal networks and hierarchical structures.
Such a dual view allows us to design a closing procedure which makes as specific as possible the temporal constraints between abstract actions and their subactions.
The procedure is then used interactively with an existing reasoning algorithm to help obtain the strengthened results.
We applied our algorithm to the problem of plan recognition and showed that such an application can both reduce the number of candidate plans make the constraints in the remaining plans more specific.
One possible area for future work is to improve the efficiency of our algorithm, which calls interactively an existing reasoning algorithm and our closing procedure.
Although the strengthened algorithm only adds a factor of log 2 n to the time complexity of an existing reasoning algorithm, it is worth investigating whether such interactions can be localized and reduced.
Some results on localizing the propagation of temporal constraints in Allen's interval algebra have been reported (Koomen 1989).
This would form a useful starting point for our future research.
Acknowledgments This work was supported in part by the Natural Sciences and Engineering Research Council of Canada.
ALLEN, J. F. 1983a.
Maintaining knowledge about temporal intervals.
Communications of the ACM, 26: 832-843.
_____1983b.
Recognizing intentions from natural language utterances.
In Computational models of discourse.
Edited by M. Brady and R. Berwick.
The MIT press, Cambridge, Mass.
pp.
107-166.
ALLEN, J. F., and KOOMEN, J.
A.
1983.
Planning using a temporal world model.
Proceedings of the Eighth International Joint Conference on Artificial Intelligence, pp.
741-747.
CARBERRY, S. 1986.
Pragmatic modeling in information system interfaces.
Ph.D. Dissertation, University of Deleware.
GOLDMAN, R., and CHARNIAK, E. 1988.
A probabilistic ATMS for plan recognition.
Proceedings of the AAAI Workshop on Plan Recognition.
KAUTZ, H. A.
1987.
A formal theory of plan recognition.
Ph.D. Dissertation, University of Rochester, Rochester, N.Y. KOOMEN, J.
A.
1989.
Localizing temporal constraint propagation.
Proceedings of the First International Conference on Principles of Knowledge Representation and Reasoning, Toronto, Ontario, Canada, pp.
198-202.
LITMAN, D. 1985.
Plan recognition and discourse analysis: an integrated approach for understanding dialogues.
Ph.D. Dissertation, University of Rochester.
SONG, F. 1991.
A processing model for temporal analysis and its application to plan recognition.
Ph.D. Dissertation, University of Waterloo, Waterloo, Ontario, Canada.
SONG, F., and COHEN, R. 1991.
Temporal reasoning during plan recognition.
Proceedings of the Ninth National Conference on Artificial Intelligence, Anaheim, CA, pp.
247-252.
VALDES-PEREZ, R. E. 1987.
The satisfiability of temporal constraint networks.
Proceedings of the Sixth National Conference on Artificial Intelligence, pp.
256-260.
VAN BEEK, P. 1989.
Approximation algorithms for temporal reasoning.
Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, pp.
1291-1296.
VILAIN, M., and KAUTZ, H. 1986.
Constraint propagation algorithms for temporal reasoning.
Proceedings of the Fifth National Conference on Artificial Intelligence, pp.
377-382.
VILAIN, M., KAUTZ, H., and VAN BEEK, P. 1989.
Constraint propagation algorithms for temporal reasoning: a revised report.
In Readings in qualitative reasoning about physical systems.
Edited by D.S.
Weld and J. de Kleer.
Morgan Kaufman, San Mateo, CA, pp.
373-381.

Believing Change and Changing Belief Peter Haddawy  haddawy@cs.uwm.edu Department of Electrical Engineering and Computer Science University of Wisconsin-Milwaukee Milwaukee, WI 53201  Abstract  We present a rst-order logic of time, chance, and probability that is capable of expressing the relation between subjective probability and objective chance at dierent times.
Using this capability, we show how the logic can distinguish between causal and evidential correlation by distinguishing between conditions, events, and actions that 1) in	uence the agent's belief in chance and 2) the agent believes to in	uence chance.
Furthermore, the semantics of the logic captures commonsense inferences concerning objective chance and causality.
We show that an agent's subjective probability is the expected value of its beliefs concerning objective chance.
We also prove that an agent using this representation believes with certainty that the past cannot be causally in	uenced.
1 Introduction  The ability to distinguish evidential from causal correlation is crucial for carrying out a number of dierent types of problem solving.
To perform diagnosis we must be able to identify the factors that caused an observed failure in order to determine how to repair the faulty device.
If we cannot distinguish causal from evidential correlation, we may end up treating the symptoms rather than the causes of the fault.
When reasoning about plans, an agent may have goals that involve achieving a specied state of the world, or achieving a specied state of knowledge, or a combination of both.
In order to eectively reason about such goals, we need to distinguish actions that in	uence the state of the world from those that only in	uence our state of knowledge of the world.
In this paper we extend Haddawy's 3] logic of time, chance, and action L by adding a subjective probability operator.
We show how the resulting rst-order logic of time, chance, and probability, L , can distinguish between causal and evidential correlation by distinguishing between conditions and events that 1) in	uence the agent's belief in chance and 2) the agent believes to in	uence chance.
Furthermore, the semantics of the logic captures some commonsense inferences tca  tcp  This work was partially supported by NSF grant #IRI9207262.
concerning causality and the relation between objective chance and subjective probability.
We prove that an agent's subjective probability is the expected value of its beliefs concerning objective chance.
We also prove that an agent whose beliefs are represented in this logic believes with certainty that the past cannot be causally in	uenced.
On the other hand, an agent can execute actions that in	uence its subjective beliefs about the past.
2 Ontology  We brie	y present the ontology of the logic, which includes the representation of time, facts, events, objective chance, and subjective probability.
For simplicity of exposition, we will omit the representation of actions and will treat them as events.
For a more detailed development of chance, facts, events, and actions see 3].
Time is modeled as a collection of world-histories, each of which is one possible chronology or history of events throughout time.
A totally ordered set of time points provides a common reference to times in the various world-histories.
We represent an agent's beliefs with subjective probabilities.
Since beliefs may change with time, subjective probability is taken relative to a point in time.
We represent it by dening a probability distribution over the set of world-histories at each point in time.
So an agent can have beliefs concerning temporally qualied facts and events.
We represent causal correlation with objective chance.
Objectively, actions and events can only aect the state of the world at times after their occurrence.
That is to say, at each point in time, the past is xed| no occurrences in the world will cause it to change but at each point in time the future might unfold in any number of ways.
So relative to any point in time, only one objectively possible past exists, but numerous possible futures exist.
Thus we represent objective chance by dening a future-branching tree structure on the world-histories and by dening probabilities over this tree.
Like subjective probability, chance is taken relative to a point in time.
By dening chance in this way, conditions in the present and past relative to a given time are either certainly true of certainly false.
So actions and other events can only aect the chances of future facts and events.
This property distinguishes objective chance from subjective probability.
Subjectively the past can be uncertain but objectively it is completely determined.
The present characterization of objective chance is not to be confused with the frequentist interpretation of probability 10, 11] which is often called objective probability.
Frequentist theories dene probability in terms of the limiting relative frequency in an innite number of trials or events.
The current work does not rely on relative frequencies for its semantics.
Rather it models objective chance by formalizing the properties that characterize objective chance.
Thus while frequentist theories have diculty assigning meaningful probabilities to unique events like a sh jumping out of the water at a given location and time, our model has no problem in assigning nontrivial probabilities to such events.
Our model of objective chance and subjective probability is motivated by the subjectivist theories of objective chance 6, 8, 9], which dene chance in terms of properties that one would expect a rational agent to believe objective chance to possess.
This distinction between the frequentist theory of probability and our conception of objective chance puts the present work in sharp contrast with Bacchus's 1] logic of statistical probabilities which models exactly relative frequency type probabilities.
One telling dierence between the two logics is that Bacchus's logic Lp assigns only probability 0 or 1 to unique events (more precisely, to all closed formulas).
The present logic can assign any chance value to unique events in the future, while events in the past are assigned only chance values 0 or 1, as required by our denition of objective chance.
It is reasonable to expect the subjective beliefs of a rational agent concerning objective chance to obey certain constraints.
Skyrms 7, Appendix 2] has argued for a constraint he calls Millers' principle.
This asserts that an agent's subjective belief in a proposition, given that he believes the objective chance to be a certain value, should be equal to that value.
Skyrms argues that this is a plausible rule for assimilating information about chance.
We will call this relation the subjective/objective Miller's principle.
The world is described in terms of facts and events.
Facts tend to hold and events tend to occur over intervals of time.
So facts and events are associated with the time intervals over which they hold or occur in the various world-histories.
Facts are distinguished from events on the basis of their temporal properties.
A fact may hold over several intervals in any given world-history and if a fact holds over an interval then it holds over all subintervals of that interval.
Events are somewhat more complex than facts.
First, one must distinguish between event types and event tokens.
An event type is a general class of events and an event token is a specic instance of an event type.
Event tokens are unique individuals { the interval over which an event token occurs is the unique interval associated with the event token and an event token can occur at most once in any world-history.
The present work deals with event types, which for brevity are simply referred to as events.
3 The Logic of Time, Chance, and Probability 3.1 Syntax  The language of L contains two predicates to refer to facts and event types occurring in time: HOLDS (FA t1 t2) is true if fact FA holds over the time interval t1 to t2, and OCCURS (EV t1  t2) is true if event EV occurs during the interval t1 to t2 .
Henceforth we will use the symbol t, possibly subscripted, to denote time points , fi, and  to denote formulas and  and  to denote probability values.
In addition to the usual rst-order logical operators, the language contains two modal operators to express subjective probability and objective chance.
The operators are subscripted with a time since according to the ontology subjective probability and objective chance are taken relative to a point in time.
We write P () to denote the subjective probability of  at time t and we write pr () to denote the objective chance of  at time t. Probability is treated as a sentential operator in the object language.
So the probability operators can be arbitrarily nested and combined with one another, allowing us to write complex sentences like: \I believe there was a one in a million chance of my winning the lottery, yet I won."
P 3 (pr 2 (OCCURS (win t1 t2)) = 10;6^ OCCURS (win t1 t2)) = 1 where t1 < t2 < t3.
We also allow conditional probability sentences such as P (jfi) = , which is interpreted as shorthand for P ( ^ fi) =   P (fi).
The language of L is fully rst-order, allowing quantication over time points, probability values, and domain individuals.
A formal specication of the syntax is provided in the full paper 2].
tcp  t  t  t  t  t  t  t  tcp  3.2 Semantics  We describe only the more interesting aspects of the models of L .
The models are completely specied in the full paper.
A model is a tuple hW D, FN, NFN, PFN, FRL, ERL, NRL, FA, EVENTS, EV, R, X , PR , PR , F i, where: fi W is the set of possible world-histories, called worlds.
fi D is the non-empty domain of individuals.
fi FA is the set of facts, a subset of 2(<<) .
A fact is 0 a set of htemporal interval, worldi pairs: fhht1 t1 i w1i ::: hht  t0 i w ig.
If fa is a fact and hht1  t2i wi 2 fa then fa holds throughout interval ht1  t2i in world-history w. fi EVENTS is the set of event tokens, a subset of (<<)  W .
An event token is a single htemporal interval, worldi pair.
fi EV is the set of event types, a subset of 2EVENTS .
An event type is a set of event tokens: fhht1 t01 i w1i ::: hht  t0 i w ig.
If ev is an event and hht1  t2i wi 2 ev then ev occurs during interval ht1 t2i in world-history w. tcp  o  s  W  n  n  n  n  n  n  1.
HOLDS (rf (trm 1  ::: trm ) ttrm1  ttrm 2)]] = true i hh ttrm 1 ]   ttrm 2] i wi 2 F (rf )(trm 1]  :::  trm ] ): 2.
OCCURS (re(trm 1  ::: trm ) ttrm 1  ttrm 2)]] = true i hh ttrm 1 ]   ttrm 2] i wi 2 e for some e 2 F (re)(trm 1 ]  ::: trm ] ): 0 3.  prttrm ()]] =  o ttrm] (fw 2 R ttrm] :  ] = trueg).
4.
Pttrm ()]] =  s ttrm] (fw0 :  ] = trueg).
Mwg  n Mwg  Mwg  Mwg  Mwg  n  Mwg  n Mwg  Mwg  Mwg  Mwg  n  Mwg  Mwg  w  w  Mwg  w  0  Mw g  Mwg  0  Mw g  Mwg  Figure 1: Semantic denitions fi R is an accessibility relation dened on <W W .
R(t w1 w2) means that world-histories w1 and w2 are indistinguishable up to and including time t. If R(t w1 w2) we say a world-history w2 is Raccessible from w1 at time t. The set of all worldhistories R-accessible from w at time t will be designated R .
For each time t, the R partition the world-histories into sets of equivalence classes indistinguishable up to t. fi X is a -algebra over W 1 , containing all the sets corresponding to w's in the language, as well as all R-equivalence classes of world-histories.
fi PR is the objective probability assignment function that assigns to each time t 2 < and worldhistory w 2 W a countably additive probability distribution  o dened over X .
fi PR is the subjective probability assignment function that assigns to each time t 2 < and worldhistory w 2 W a countably additive probability distribution  s dened over X .
Given the models described above, the semantic definitions for the well-formed formulas can now be dened.
Denotations are assigned to expressions relative to a model, a world-history within the model, and an assignment of individuals in the domain to variables.
The denotation of an expression  relative to a model M and a world-history w, and a variable assignment g is designated by  ] .
Figure 1 shows the less familiar semantic denitions.
The remainder are provided in the full paper.
w t  w t  o  w t  s  w t  Mwg  3.2.1 Semantic Constraints  In order to obtain the properties discussed in the ontology, we impose eight constraints on the models.
The future-branching temporal tree is dened in terms of the R relation over world-histories.
To capture the property that the tree does not branch into the past, we say that if two world-histories are indistinguishable up to time t2 then they are indistinguishable up to any earlier time: (C1) If t1t2 and R(t2 w1 w2) then R(t1 w1 w2).
A -algebra over W is a class of subsets that contains W and is closed under complement and countable union.
1  Since R just represents the indistinguishability of histories up to a time t, for a xed time R is an equivalence relation, i.e., re	exive, symmetric, and transitive: (C2) R(t w w) If R(t w1 w2) then R(t w2 w1) If R(t w1 w2) and R(t w2 w3) then R(t w1 w3) As mentioned earlier, facts and events dier in their temporal properties.
This distinction is captured by the following two semantic constraints.
If a fact holds over an interval, it holds over all subintervals, except possibly at the endpoints: (C3) If t1 t2 t3t4 t1 6= t3  t2 6= t4  fa 2 FA and hht1 t4i wi 2 fa then hht2  t3i wi 2 fa : An event token occurs only once in each world-history: (C4) If evt 2 EVENTS , hht1 t2i wi 2 evt, and hht3  t4i wi 2 evt then t1 = t3 and t2 = t4 .
If two worlds are indistinguishable up to a time then they must share a common past up to that time.
And if they share a common past up to a given time, they must agree on all facts and events up to that time.
To enforce this relationship, we impose the constraint that if two world-histories are R-accessible at time t, they must agree on all facts(events) that hold(occur) over intervals ending before or at the same time as t: (C5) If t0 t1 t2 and R(t2  w1 w2) then hht0  t1i w1i 2 A i hht0  t1i w2i 2 A, where A is a fact or event.
The ontology discussed two desired characteristics of objective chance.
The rst is that the chance at a time t be completely determined by the history up to that time.
The second desired characteristic is that the chance of the present and past should be either zero or one, depending on whether or not it actually happened.
These two properties follow as meta-theorems from the following two constraints: (C6) For all 0 X 2 X  tt0 and w w0 such that R(t w w )   (R ) > 0 !
(X ) =   (X jR ).
w t  w t  0  w t0  0  w t  w t0  0  (C7)   (R ) > 0.
Meta-theorem 1 The probability of the present and w t  w t  past is either zero or one.
(R ) = 1 w t  w t  Theorem 7 Objective Miller's Principle (OMP)  1.
(R ) > 0 (C7) 2.
(R ) =   (R jR ) Modus Ponens: (C6),1 3.
(R ) = 1 def of c-prob w t w t w t  w t w t w t  w t  w t  w t  tcp  t  Dening the probabilities in this way makes good intuitive sense if we look at the meaning of R. R designates the set of world-histories that are objectively possible with respect to w at time t. It is natural that the set of world-histories that are objectively likely with respect to w at time t should be a subset of the ones that are possible.
w t  Meta-theorem 2 If two worlds are indistinguishable  up to time t then they have identical probability distributions at that time.
If R(t w w0) then   (X ) =   (X ) w t  1.
2.
3.
4.
5.
0  w t  (R ) > 0 (C2), (C7) (R ) =   (X jR ) Modus Ponens: (C6),1 (X jR ) =   (X jR ) (C2) (R ) = 1 Meta-theorem 1 (X ) =   (X ) def of c-prob  w t w t w t w t 0 w t  0  w t 0 w t  w t  w t  w t  0  w t  w t  0  w t  w t  In the ontology, we argued that subjective probability and objective chance should be related to one another by Millers' principle.
This relation is enforced by the following constraint, which says that the probability of a set of worlds X , given some R equivalence class, should just be the objective chance in that equivalence class.
(C8)  s (X jR ) =  o (X ) w t  w t  w t  4 Theorems  We rst provide several simple theorems that will be used in later proofs.
Then we prove two forms of Miller's principle and provide two associated expected value properties.
Proofs not provided here appear in the full paper.
Theorem 3 From  $ fi infer pr () = pr (fi): t  t  Theorem 4 Stronger sentences have lower probability.
From  !
fi infer P ()  P (fi).
Theorem 5 Certainty cannot be conditioned away from.
P ( ^ fi) = P (fi) !
P ( ^ fi ^  ) = P (fi ^  ) Theorem 6 The present and past are objectively certain.
Let be a fact or event: HOLDS ( t  t0 ) or OCCURS ( t  t0 ) then 8t t  t0 (t0  t) !
pr ( ) = 0 _ pr ( ) = 1] t  t  t  t          t  t      t  All instances of the following sentence schema are valid in L .
8 t0 t1 (t0  t1 ) !
pr 0 ( j pr 1 () = ) =  Proof:    t  The semantic constraints on objective chance give us a version of Miller's principle that relates objective chance at dierent times.
It says that the chance of a sentence  at a time, given that the chance of  at the same or a later time is , should be .
t  We rst prove an expected value property and then use it to prove Miller's principle.
Let t t0 be two time points t  t0 and consider the R-equivalence classes of worlds at time t0 .
Let the variable r range over these equivalence classes.
The r form a partition of W , so the probability of a set X can be written as the integral over this partition: Z  o (X ) =  o (X jr) o (dr)  Since the history up to time t0 determines the probability at time tZ0, this can be written as  o (X ) =  o (X ) o (dr)  where  o denotes the probability at time t0 in equivalence class r. Since the probability at a given time is assumed to be constant over all worlds in an Requivalence class, the probability at a given time is the expected value Z of the probability at any future time:  o (X ) =  o (X ) o (dw0 ): Next we show that Miller's principle is valid in the probability models.
By the expected value property,  o (ZX \ fw0 :  o (X ) = g) =  o (X \ fw0 :  o (X ) = g) o (dw00): Now, by semantic constraints (C6) and (C7) it follows that 8 w 2fw0 :  o (X ) = g  o (fw0 :  o (X ) = g) = 1 8 w 62fw0 :  o (X ) = g  o (fw0 :  o (X ) = g) = 0: So we can restrict the integral to the set fw0 :Z o (X ) = g: =  o (X \ fw0 :  o (X ) = g) o (dw00): w t  w t  r  W  r  W  w t  w t  r t0  w t  r t0  w t0  w t  0  w t  W  w t  w t0  w t0  0  00  w t0  0  w t  W  w t0  w t0  w t0  0  w t0  0  w t0  w t0  w t0  0  0  0  w  00  w t0  t0 0 w 0 fiow0 X t  f  ( )=g  :  0  w t  And by the above property again  o (XZ\ fw0 :  o (X ) = g) = , so =   o (dw00): w t0  00  w t0  f  0  ( )=g  0 w 0 fiow0 X t  :  w t  =    o (fw0 :  o (X ) = g): By the semantic denitions it follows that P ( ^ P () = ) =   P (P () = ): And by a slight generalization of the proof it follows that 8(t  t0) P ( ^ P ()  )    P (P ()  ): 2 From the Objective Miller's Principle it follows directly that current chance is the expected value of current chance applied to current or future chance.
w t  t  w t0  t0  0  t  t  t0  t0  t  t0  Theorem 8 Objective Expected Value Property  All instances of the following sentence schema are valid in L .
8  t1 t2 (t1  t2 ) !
pr 1 (pr 2 ()  )   !
pr 1 ()     ] tcp  t  t  t  As discussed in the ontology, the current subjective probability of a sentence, given that the current or future chance is some value should be that value.
The following theorem shows that this property follows from the semantics of the logic.
Theorem 9 Subjective/Objective Miller's Principle (SOMP)  All instances of the following sentence schema are valid in L .
8 t0 t1(t0  t1 ) !
P 0 (jpr 1 () = ) =  Proof: We rst prove an expected value property and tcp  t  t  then use it to prove Miller's principle.
Let t t0 be two 0 time points t  t and consider the R-equivalence classes of worlds at time t0 .
Let the variable r range over these  equivalence classes.
The r form a partition of W , so the probability of a set X can be written as the integral over this partition:   s (X ) =  Z  w t  r   s (X jr) s (dr) w t    W  w t  Z  w t  r   o (X ) s (dr) r t    W  w t  where  o denotes the objective chance at time t in equivalence class r. Since the chance at a given time is assumed to be constant over all worlds in an Requivalence class, the subjective probability at any time is the expected value of the subjective probability applied to the objective chance at that time: r t   s (x) =  Z  w t   o (X ) s (dw0) w t  W  0  w t  Next we show that the Subjective/Objective Miller's principle is valid in the probability models.
By the above subjective/objective expected value property,  s (ZX \ fw0 :  o (X ) = g) =  o (X \ fw0 :  o (X ) = g) s (dw00) By Objective Miller's Principle,  s (XZ\ fw0 :  o (X ) = g) =   o (fw0 :  o (X ) = g) s (dw00) w t  w t0  w t  0  00  w t0  0  w t  W  w t  w t0  w t  0  00  w t0  W  0  w t  Finally, by the subjective/objective expected value property,  s (X \ fw0 :  o (X ) = g) =  s (fw0 :  o (X ) = g) So by the semantic denitions it follows that 8t t0 (t  t0 ) !
P (jpr () = ) =  w t  w t0  w t  0  w t0  t  0  t0  t  t0  Theorem 10 Subjective/Objective Value Property  Expected  All instances of the following sentence schema are valid in L .
8  t1 t2 (t1  t2) !
P 1 (pr 2 ()  )   !
P 1 ()     ] tcp  t  t  t  5 Distinguishing Evidential and Causal Correlation  We wish to distinguish between two situations in which an agent may believe that two conditions are correlated.
An agent may believe that two conditions are correlated because one is simply evidence for another and an agent may believe that they are correlated because one causes the other.
Let stand for the formula HOLDS ( t  t0 ) or OCCURS ( t  t0 ) and let !
stand for the formula HOLDS (fi t  t0 ) or OCCURS (fi t  t0 ).
We represent evidential correlation as correlation in the subjective probability distribution, which is the standard approach in Bayesian decision theory.
Denition 11 We say that !
is evidence for or   By semantic constraint (C8), this can be written as   s (X ) =  And by a slight generalization of the proof it follows that 8t t0 (t  t0 ) !
P (jpr ()  )   2 From the subjective/objective Miller's principle it follows directly that subjective probability is the expected value of current subjective probability applied to present or future chance.
          against i Pnow ( j !)
= 6 Pnow ( )      (1) It follows from this denition that !
is not evidence for or against i Pnow ( j !)
= Pnow ( ) We represent causal correlation by reference to the objective chance distribution.
We represent an agent's belief that !
causally in	uences by saying that there is some value for the objective chance of such that the agent's belief in given the objective chance of just before !
holds or occurs is not the same as the agent's belief given also knowledge of !.
In other words, knowledge of !
overrides knowledge of the objective chance of .
Denition 12 We say that !
is a cause of i 9 Pnow ( j pr ( ) =  ^ !)
6= : (2) Note that this does not necessarily imply that Pnow ( j!)
6= Pnow ( ).
Thus we may have causal correlation without evidential correlation and, conversely, we may have evidential correlation without causal correlation.
It follows from this denition that !
is not a cause of i 8 Pnow ( j pr ( ) =  ^ !)
= : t  t  5.1 Example  We now present an example demonstrating the use of the denitions and theorems.
We wish to describe the following situation.
You have a coin that may be biased 3:1 towards heads or 3:1 towards tails.
You believe there is an equal probability of each.
You can observe the coin.
If the coin looks shiny, this increases your belief that the coin is biased towards heads.
You also have a magnet that you can use to in	uence the outcome of the coin toss.
Turning on the magnet biases the coin more toward heads.
We can describe the situation with the following set of sentences in which \heads" is the event of the coin landing heads, \shiny" is the event of the coin being observed to be shiny, and \magnet" is the fact that the magnet is on.
(now < t0 < t1 < t2 < t3 < t4 ) Turning on the magnet in	uences the chance of heads.
Pnow (OCCURS (Heads t2 t3)j (3) pr 1 (OCCURS (Heads t2 t3)) = 3=4 ^ HOLDS (Magnet t1  t4)) = 7=8 Pnow (OCCURS (Heads t2 t3)j (4) pr 1 (OCCURS (Heads t2 t3)) = 1=4 ^ HOLDS (Magnet t1  t4)) = 1=2 The probability that the coin is biased toward heads and the probability that the coin is biased toward tails are equal.2 Pnow (pr 1 (OCCURS (Heads t2 t3)) = 3=4) = (5) Pnow (pr 1 (OCCURS (Heads t2 t3)) = 1=4) = 1=2 Observing the coin doesn't in	uence the chance of heads.
(6) 8 t (t > now) !
Pnow (OCCURS (Heads t2 t3) j pr (OCCURS (Heads t2 t3)) =  ^ OCCURS (Shiny t0  t2)) =  Observing the coin gives us knowledge of its bias.
Pnow (pr 0 (OCCURS (Heads t2 t3)) = 3=4 j (7) OCCURS (Shiny t0  t2)) = 5=8 Pnow (pr 0 (OCCURS (Heads t2 t3)) = 1=4 j (8) OCCURS (Shiny t0  t2)) = 3=8 Turning on the magnet does not give us knowledge of the coin's bias.
8 Pnow (pr 1 (OCCURS (Heads t2 t3)) =  j (9) HOLDS (Magnet t1  t4)) = Pnow (pr 1 (OCCURS (Heads t2 t3)) = ) The coin is either biased toward heads or toward tails.
8t pr (OCCURS (Heads t2 t3 )) = 3=4 _ (10) pr (OCCURS (Heads t2 t3)) = 1=4 (11) t  t  t  t  t  t  t  t  t  t  t  It would be more appropriate to say that our belief that the current chance is 3/4 or 1/4 is 1/2 and that in the absence of events that will inuence the chance, chance will remain unchanged till time t1 .
Such an inference would require some kind of theory of persistence, which is beyond the scope of this paper.
2  Using this information, we can make several useful inferences.
First we can derive the unconditional probability that the coin will land heads.
From (5) by SOMP we have Pnow (OCCURS (Heads t2 t3)) = (12) (1=2)(3=4) + (1=2)(1=4) = 1=2 Next, we can use the above information to derive the probability that the coin will come up heads given that it is observed to be shiny.
Instantiating (6) with  = 3=4 and t = t0 and multiplying the result by (7) we get Pnow (OCCURS (Heads t2 t3)^ (13) pr 0 (OCCURS (Heads t2 t3)) = 3=4 j OCCURS (Shiny t0  t2)) = (5=8)(3=4) = 15=32 And instantiating (6) with  = 1=4 and t = t0 and multiplying the result by (8) we get Pnow (OCCURS (Heads t2 t3)^ (14) pr 0 (OCCURS (Heads t2 t3)) = 1=4 j OCCURS (Shiny t0  t2)) = (3=8)(1=4) = 3=32 From (10), (13), and (14) by the law of total probability we get Pnow (OCCURS (Heads t2 t3) j (15) OCCURS (Shiny t0  t2)) = 9=16 We can also derive the probability of heads given that we activate the magnet.
From (3), (5), and (9) we get Pnow (OCCURS (Heads t2 t3)^ (16) pr 2 (OCCURS (Heads t2 t3)) = 3=4 j HOLDS (Magnet t1 t4)) = (1=2)(7=8) = 7=16 From (4), (5), and (9) we get Pnow (OCCURS (Heads t2 t3)^ (17) pr 2 (OCCURS (Heads t2 t3)) = 1=4 j HOLDS (Magnet t1 t4)) = (1=2)(1=2) = 1=4 From (10), (16), and (17) by the law of total probability we get Pnow (OCCURS (Heads t2 t3) j (18) HOLDS (Magnet t1 t4)) = 11=16 t  t  t  t  5.2 The temporal ow of causality  Using our denition of causal in	uence and SOMP we can now show that an agent whose beliefs are represented with L believes that the past cannot be in	uenced.
tcp  Theorem 13 Let be a fact or event: 0 0  HOLDS ( t  t ) or OCCURS ( t  t ) and let !
be a fact or event: HOLDS (fi t  t0 ) or OCCURS (fi t  t0 ).
Then all instances of the following sentence schema are valid in L .
8 t t  t0  t  t0 (t0  t ) ^ (t  t ) !
P ( jpr ( ) =  ^ !)
=              tcp    t      t              Proof: We prove a slightly more general result of which  the above sentence is an instance.
By the Subjective/Objective Miller's Principle, 8 t t0 t  t0 (t0  t0 ) ^ (t  t0) !
(19) P ( ^ pr ( ) = ) =   P (pr ( ) = ) Since valid formulas have probability one, it follows by Theorem 6 that, 8 t t0 t  t0  t  t0 (t0  t0 ) ^ (t  t0) !
(20) P ( ^ pr ( ) =  ^ pr ( ) = 0 _ pr ( ) = 1]) =   P (pr ( ) =  ^ pr ( ) = 0 _ pr ( ) = 1]) Since pr ( ) = 0 and pr ( ) = 1 are mutually exclusive, we have 8 t t0 t  t0 (t0  t0 ) ^ (t  t0) !
(21) P ( ^ pr ( ) =  ^ pr ( ) = 0) + P ( ^ pr ( ) =  ^ pr ( ) = 1) =   P (pr ( ) =  ^ pr ( ) = 0) +   P (pr ( ) =  ^ pr ( ) = 1) Now we have three cases to consider: i)  = 0, ii)  = 1, iii) 0 <  < 1.
      t0  t            t0  t  t0  t0  t  t0  t  t0  t0  t0  t0  t0      t  t0  t  t0    t0 t0  t  t0  t0  t  t0  t0  Case i)  Expression (21) reduces to (22) 8t t0 t  t0  t  t0 (t0  t0 ) ^ (t  t0) !
P ( ^ pr ( ) = 0) = 0  P (pr ( ) = 0) So by Theorem 4 and universal generalization, 8t t0 t  t0  t  t0 (t0  t0 ) ^ (t  t0) !
(23) P ( ^ pr ( ) = 0 ^ !)
= 0  P (pr ( ) = 0 ^ !)
          t0  t      t0  t        t0  t  t0  t  Case ii)  Expression (21) reduces to (24) 8t t0 t  t0 (t0  t0 ) ^ (t  t0) !
P ( ^ pr ( ) = 1) = P (pr ( ) = 1) So by Theorem 5 and universal generalization, 8t t0 t  t0  t  t0 (t0  t0 ) ^ (t  t0) !
(25) P ( ^ pr ( ) = 1 ^ !)
= P (pr ( ) = 1 ^ !)
      t0  t    t          t0  t  t0  t  t0  Case iii)  For 0 <  < 1, P (pr ( ) = ) = 0.
So by Theorem 4 and universal generalization, 8 t t0 t  t0  t  t0 (26) 0 0 0 (t  t ) ^ (t  t ) ^ (0 <  < 1) !
P ( ^ pr ( ) =  ^ !)
= P (pr ( ) =  ^ !)
Therefore we have proven that the following sentence is valid (27) 8 t t0 t  t0  t  t0 (t0  t0 ) ^ (t  t0) !
P ( jpr ( ) =  ^ !)
=  from which it follows that the past cannot be in	uenced.
2 t0  t            t0  t    t    t0    t      t0  6 Related Work  Three outstanding subjective theories of objective chance from the philosophical literature are those of van Fraassen 9], Lewis 6], and Skyrms 7].
van Fraassen's model of objective chance is more constrained than Lewis's model which is more constrained than Skyrms's model.
Thus, in van Fraassen's model, chance has more inherent properties than in either Lewis's or Skyrms's models.
van Fraassen's theory is the only one of the three that is cast in a temporal framework.
All three are semantic theories and do not provide logical languages.
The model of objective chance used in L is based on van Fraassen's 9] model of objective chance.
He presents a semantic theory that models subjective probability and objective chance, using a future-branching model of time points.
van Fraassen places two constraints on objective chance: 1.
The chance of a past is either 0 or 1, depending on whether or not it actually occurred.
2.
Chance at a time is completely determined by history of the world up to that time.
From these assumptions, he shows the following relation between subjective probability and objective chance P (X jY ) = E C (X )] where P is the subjective probability at time t, C is the objective chance at time t, E is the expected value given Y , and provided the truth of Y depends only on the history up to t. This relation entails both Miller's principle and Lewis's principal principle, discussed below.
Note that van Fraassen does not show that a similar relation holds between objective chances at dierent times.
In van Fraassen's models, objective chance can change with time but truth values cannot.
Lewis's 6] theory of objective chance is based on his assertion that ... we have some very rm and denite opinions concerning reasonable credence (subjective probability) about chance (objective chance).
These opinions seem to me to afford the best grip we have on the concept of chance.
He describes a number of intuitive relationships between subjective probability and objective chance and shows that these are captured by his principal principle: Pr(Ajpr (A) =  ^ E ) =  where Pr is subjective probability, pr is objective chance, and E is any proposition compatible with pr (A) =  and admissible at time t. The interesting thing here is the proposition E .
The constraint that E be compatible with pr (A) =  means that Pr(E ^ pr (A) = ) > 0.
Admissibility is less readily dened.
Lewis does not give a denition of admissibility but he does characterize admissible propositions as \the sort of information whose impact on credence about outcomes comes entirely by way of credence about the chances of those outcomes."
So objective chance is invariant with respect to conditioning on tcp  t  Y  t  t  t  Y  t  t  t  t  admissible propositions.
This concept of invariance under conditioning is the central notion of Brian Skyrms's theory of objective chance.
Skyrms 7] works with the notion of resiliency.
A probability value is resilient if it is relatively invariant under conditionalization over a set of sentences.
The resiliency of Pr(q) being  is dened as 1 minus the amplitude of the wiggle about : The resiliency of Pr(q) being  is 1;maxj; Pr (q)j over p1  ::: p , where the Pr 's are gotten by conditionalizing on some Boolean combinationof the p 's which is logically consistent with q. Skyrms then denes propensity (objective chance) as a highly resilient subjective probability.
Independent of his resiliency notion, Skyrms requires that propensities and subjective probabilities be related by Miller's principle: Pr(Ajpr(A) = ) =  where Pr is a subjective probability and pr is a propensity.
He shows that Millers' principle entails that subjective probabilities are equal to the expectation of the subjective probabilities applied to the objective probabilities.
But Skyrms 7, p158] points out that, counter to intuition, independence in every possible objective distribution does not imply independence in the subjective distribution.
This observation provided the motivation for our use of the two probabilities to distinguish causal from evidential correlation.
Halpern 4, 5] presents a probability logic that can represent both statistical and subjective probabilities.
Statistical probabilities represent proportions over the domain of individuals, while propositional probabilities represent degrees of belief.
The two probability operators in the language can be nested and combined freely with other logical operators.
So the language is capable of representing sentences like \The probability is .95 that more than 75% of all birds can 	y."
The models for the language contain a domain of individuals, a set of possible worlds, a single discrete probability function over the individuals, and a single discrete probability function over the possible worlds.
The rst probability function is used to assign meaning to the statistical probability operator, while the second is used to assign meaning to the propositional probability operator.
Although he does not place constraints within the logic on the relation between the two probabilities, he does discuss a form of Miller's principle that relates subjective and objective probabilities.
His version of the principle states that \for any real number r0 the conditional probability of (a), given that the probability of a randomly chosen x satises  is r0, is itself r0."
He points out that this could be used as a rule for inferring degrees of belief from statistical information.
Bacchus 1] presents a logic essentially identical to that of Halpern.
He goes further than Halpern in exploring the inference of degrees of belief from statistical probabilities.
According to his principle of direct inference, an agent's belief in a formula is the expected j  n  i  j  value with respect to the agent's beliefs of the statistical probability of that formula, given the agent's set of accepted objective assertions.
References  1] F. Bacchus.
Representing and Reasoning With Probabilistic Knowledge.
MIT Press, Cambridge, Mass, 1990.
2] P. Haddawy.
Believing change and changing belief.
Technical Report TR-94-02-01, Dept.
of Elect.
Eng.
& Computer Science, University of Wisconsin-Milwaukee, February 1994.
Available via anonymous FTP from pub/tech_reports at ftp.cs.uwm.edu.
3] P. Haddawy.
Representing Plans Under Uncertainty: A Logic of Time, Chance, and Action, volume 770 of Lecture Notes in Arti	cial Intelligence.
Springer-Verlag, Berlin, 1994.
4] J.Y.
Halpern.
An analysis of rst-order logics of probability.
In Proceedings of the International Joint Conference on Arti	cial Intelligence, pages 1375{1381, 1989.
5] J.Y.
Halpern.
An analysis of rst-order logics of probability.
Arti	cial Intelligence, 46:311{350, 1991.
6] D. Lewis.
A subjectivist's guide to objective chance.
In W. Harper, R. Stalnaker, and G. Pearce, editors, Ifs, pages 267{298.
D. Reidel, Dordrecht, 1980.
7] B. Skyrms.
Causal Necessity.
Yale Univ.
Press, New Haven, 1980.
8] B. Skyrms.
Higher order degrees of belief.
In D.H. Mellor, editor, Prospects for Pragmatism, chapter 6, pages 109{137.
Cambridge Univ.
Press, Cambridge, 1980.
9] B.C.
van Fraassen.
A temporal framework for conditionals and chance.
In W. Harper, R. Stalnaker, and G. Pearce, editors, Ifs, pages 323{340.
D. Reidel, Dordrecht, 1980.
10] J. Venn.
The Logic of Chance.
MacMillan, London, 1866.
(new paperback edition, Chelsea, 1962).
11] R. von Mises.
Probability, Statistics and Truth.
Allen and Unwin, London, 1957.
Version Management and Historical Queries in Digital Libraries Fusheng Wang Siemens Corporate Research Integrated Data Systems Department 755 College Road East, Princeton, NJ 08540 fusheng.wang@siemens.com  Abstract Historical information can be effectively preserved using XML and searched through powerful historical queries written in XQuery.
Indeed, by storing the successive versions of a document in an incremental fashion, XML repositories and data warehouses can achieve (i) the efficient preservation of critical information, (ii) its representation using a temporally grouped data model and (iii) the ability of supporting historical queries on the evolution of documents and their contents using XQuery.
The proposed approach can be applied uniformly to (a) XML document archives and (b) transaction-time databases published in XML and queried in XQuery.
Our case studies include the UCLA course catalog, W3C Xlink standards, and the CIA WorldFact Book, besides relational databases.
The experience described here and in [15, 16] suggests that current standards provide reasonable support for temporal applications at the logical level, whereas many challenges remain at the physical level.
1.
Introduction The computer technology that makes digital libraries possible also makes it all too easy to revise documents and publish new versions.
Thus version management has become a critical issue characterized by significant new challenges and opportunities [1].
Indeed we are faced with the problem of how to organize, search, and query effectively multi-version documents.
On the other hand, we now have a unique opportunity of querying the evolution history of documents and their contents, and learning information that would have been hard to acquire from separate snapshots.
Historical queries represent an excellent example of the ability of digital libraries to enhance content delivery services well beyond those of traditional libraries.
Our ICAP project proves this point via three interesting case studies on the successive version history of various XML docu-  Carlo Zaniolo  Xin Zhou  Hyun J.
Moon  University of California, Los Angeles Computer Science Department 405 Hilgard Ave, Los Angeles, CA 90095 {zaniolo, xinzhou, hjmoon}@cs.ucla.edu  ments, including the UCLA course catalog [3], W3C Xlink [5] standards, and the CIA World FactBook [4].
2.
Version Management in XML Our technical approach is based on the observation that simply storing the successive snapshots of a document is insufficient because of (i) storage inefficiency, (ii) explosion of hits on content-based searches, and (iii) difficulty of answering queries on the evolution of a document and its content.
Therefore, we developed XML-based techniques whereby a multiversion document is managed as a unit where its successive versions are represented by the delta changes between versions [12, 6].
This approach optimizes storage utilization, and makes it possible to support complex historical queries on the evolution of the document and its elements (e.g., abstract, figures, bibliography, etc.).
For instance in our ICAP project [2], we store the successive versions of the UCLA course catalog as one document, where each element in the document is time-stamped with its period of validity.
This representation makes it easy to ask queries such as: "When was course CS143 introduced?"
and "How many years did it take for 'nanotechnology' topics to migrate from graduate-level courses to undergraduate ones?"
In the spirit of 'e-permanence' standards for Web sites of public interest [9], we can now preserve the whole history of normative documents, city plans, street maps, and natural preserves.
Using XML annotation capability we can also annotate each change with an explanation for its cause--in ICAP we annotate each revision of W3C Xlink standards with references to memos that led to the changes.
We have automated the process of building the version history of a document (called the V-Document) from its successive versions by (i) first computing the diff between the last version and the new version [12, 6] and (ii) using the results of this process to update the time- stamped V-Document.
The V-Document so obtained supports services, such as color-marking the changes between any two versions of the document (not just successive ones) and ad-  vanced features inspired by the Version Machine [7].
4.
Historical Queries  3 MultiVersion XML Documents  A unique benefit offered by V-Documents is that they support complex historical queries, using the standard query languages of XML.
Indeed, in XML an expressive temporal representation can be obtained by simply adding temporal attributes to the elements [15].
Furthermore, XML's query language, XQuery, achieves native extensibility via userdefined functions.
Therefore, temporal extensions can be defined in XQuery, without requiring changes in the standards (which proved unattainable for SQL).
In the ICAP project [2], we have defined a temporal library of XQuery functions to facilitate the formulation of historical queries and isolate the user from lower-level details, such as the internal representation of the 'now' timestamp.
The complete gamut of historical queries, including snapshot and time-slice queries, element-history queries, 'since' and 'until' queries, can now be expressed in standard XQuery.
This approach can also be applied to transaction-time relational databases, and it requires no change in existing standards.
This is significant, given that support for temporal information and historical queries proved to be difficult in standard SQL.
Such difficulties led to a number of proposed temporal extensions for SQL [14]; but these have not been incorporated into commercial DBMS.
However, historical information can be managed and queried in XML, without requiring extensions to the current standards, since:  Our basic approach to represent multi-version XML documents is demonstrated by the ICAP project [2], where we: (i) use structured diff algorithms [6, 12] to compute the validity periods of the elements in the document, (ii) use the output generated by the diff algorithm, to represent concisely the history of the documents with a temporally grouped data model.
Then, we (iii) use XQuery, to formulate temporal queries on the evolution of these documents and their contents.
For instance consider a very simple document in three successive versions, as shown in Figure 1.
* XML provides a richer data model, whose structured hierarchies can be used to support temporally grouped data models by simply adding temporal attributes to the elements.
Temporally grouped representations have long been recognized to provide a very natural data model for historical information [10, 11], and * XML provides query languages with higher expressive power than SQL; in particular, XQuery [8] achieves native extensibility and Turing completeness via userdefined functions [13].
Thus, the constructs needed for temporal queries can be introduced as user-defined libraries, without requiring extensions to existing standards.
Figure 1.
Snapshot-based representation of XML document versions To store and query efficiently the history of this evolving document, we compute the differences between its successive versions, using a structure diff algorithm such as those described in [6, 12].
Then, we represent the history of the document by time-stamping and temporally grouping these deltas as shown in Figure 2.
We obtain a temporally grouped representation - VDocument.
V-Documents can be easily queried using XQuery.
5.
Testbeds and Demos Interesting case studies have been implemented and are available for demoing [2], including the UCLA course catalog [3], the W3C Xlink standards [5], and the CIA World FactBook [4].
These studies lead to interesting historical findings that would have been difficult to obtain from the original documents.
Also available for demoing there is the ArchIS system [16], which demonstrates that significant performance improvements can be obtained by mapping temporal XML views and queries into relational tables  Figure 2.
Temporally grouped representation of XML document versions and SQL/XML.
Compression and temporal clustering techniques can also be used to further improve the performance of this approach [16], although much more work is needed in that area.
The ArchIS experience also underscored that the performance of native XML database systems might not scale up as well as relational DBMS, and simple techniques are not at hand for achieving effective temporal indexing and clustering in this context.
6.
Conclusions As discussed in a companion paper [15], when dealing with the history of relational databases, SQL:2003 can be used as effectively as XML/XQuery in managing temporal information.
This suggests that the approach of carefully grafting temporal extensions standards might no longer be practical or desirable for either SQL or XML.
While SQL:2003 can be used for database histories, XML/XQuery can provide a uniform solution to the logical problems involved in publishing and querying the histories of databases and documents.
However, achieving scalability and performance for these queries poses major problems, and temporal database researchers should focus on physical issues with renewed energy.
Acknowledgment  [4] CIA: The World Factbook.
http://www.cia.gov/cia/ publications/factbook/.
[5] XML Linking Language http://www.w3.org/TR/xlink/.
(XLink).
[6] Microsoft XML Diff.
http://apps.gotdotnet.com/ xmltools/xmldiff/.
[7] The Versioning Machine.
products/ver-mach/.
http://mith2.umd.edu/  [8] XQuery 1.0: An XML Query http://www.w3.org/TR/xquery/.
Language.
[9] National Archives of Australia: Policy Statement Archiving Web Resources.
http://www.naa.gov.au/.
[10] J. Clifford.
"Formal Semantics and Pragmatics for Natural Language Querying".
Cambridge University Press, 1990.
[11] J. Clifford, A. Croker, F. Grandi,and A. Tuzhilin, "On Temporal Grouping", in Proc.
of the Intl.
Workshop on Temporal Databases, 1995.
[12] Gregory Cobena, Serge Abiteboul, Amelie Marian, "Detecting Changes in XML Documents", in ICDE 2002.
[13] S. Kepser.
"A Simple Proof for the TuringCompleteness of XSLT and XQuery".
In Extreme Markup Languages, 2004.
The authors would like to thank Vassilis Tsotras for many illuminating discussions.
[14] G. Ozsoyoglu and R.T. Snodgrass, "Temporal and Real-time Databases: A Survey".
in TKDE, 7(4):513- 532, 1995.
References  [15] F. Wang, C. Zaniolo and X. Zhou, "Temporal XML?
SQL Strikes Back!
", Time 2005.
[1] A.R.
Kenney, et.
al., "Preservation Risk Management for Web Resources - Virtual Remote Control in Cornell's Project Prism", D-Lib Magazine, Jan 2002, 8(1).
[2] The ICAP Project.
wis.cs.ucla.edu/projects/icap/.
[3] UCLA Catalog.
www.registrar.ucla.edu/catalog/.
[16] F. Wang and C. Zaniolo: "Publishing and Querying the Histories of Archived Relational Databases in XML", in WISE 2003.

Analysis of timed processes with data using algebraic transformations Michel A. Reniers Yaroslav S. Usenko Department of Mathematics and Computer Science, Technical University of Eindhoven, P.O.
Box 513, 5600 MB Eindhoven, The Netherlands  Abstract The language of timed uCRL is an extension of an ACP-style process algebra-based language uCRL with time-related features.
In this paper we describe this language and its equational axiomatization, and give an example specification.
We outline the method of simplifying transformations based on this equational axiomatization, and illustrate it on this example.
This transformation method allows for a time-free abstraction of the specification, which in turn enables the use of tools and techniques for verification of untimed systems.
We prove some properties of the example using a known invariants technique.
Key words: Process Algebra, Real Time, Axioms, Verification.
1  Introduction  The language uCRL, see [13], offers a uniform framework for the specification of data and processes.
Data are specified by equational specifications (cf.
[5, 19]): one can declare sorts and functions working upon these sorts, and describe the meaning of these functions by equational axioms.
Processes are described in process algebraic style, where the particular process syntax stems from ACP [6, 4, 9], extended with data-parametric ingredients: there are constructs for conditional composition, and for data-parametric choice and communication.
As is common in process algebra, infinite processes are specified by means of (finite systems of) recursive equations.
In uCRL such equations can also be data-parametric.
As an example, for action a and adopting standard semantics for uCRL, each solution for the equation X = a * X specifies (or "identifies") the process that can only repeatedly execute a, and so does Y(17) where Y(n) is defined by the data-parametric equation Y(n) = a * Y(n + 1) with n [?]
Nat.
Several timed extensions have been proposed for different kinds of process algebras.
For an overview of ACP extensions with time we refer to [3].
According to [3], timed process algebras can be categorized by three criteria.
Discrete vs. continuous time; relative vs. absolute time; two-phase vs. timed-stamped model.
In [11] the language uCRL is extended with time, and in [21] a sound and complete axiomatization of timed uCRL is presented.
In [15] some examples of specification and reasoning in timed uCRL are given.
Timed uCRL makes use of absolute time, timed stamped model, and the time domain can be defined by the user (both discrete and continuous domains are possible).
For a way to interpret timed automata [1] in timed uCRL we refer to [23, Chapter 6].
In this paper we present a method to describe and analyze real-time systems using uCRL and timed uCRL.
It is assumed that some real-time system is described by means of a timed uCRL specification (see Section 2).
Mostly such descriptions will contain operators such as parallel composition that complicate analysis.
As a first step towards the analysis of such systems, we linearize the given description using the algorithm from [22] (see Section 3).
The result is a Timed  1  Linear Process Equation (TLPE) which is equivalent to the original description and has a very simple structure.
On this TLPE it is already possible to perform some analysis.
Next, in Section 4, we describe how a TLPE can be transformed into an LPE, i.e., a linear process equation without time.
This transformation, called time-free abstraction, has been used for a more restricted class of timed uCRL specifications in [21].
Crucial steps in this transformation are that the TLPE is first transformed into a well-timed TLPE and second that it is transformed into a deadlock-saturated well-timed TLPE.
Finally, all time-stamping is captured in the parameters of atomic actions.
The result is an LPE for which the machinery of untimed uCRL can be put to use for further analysis.
These are based on symbolic analysis of the specifications, such as invariants, term rewriting and theorem proving, or on explicit state space generation and model-checking.
We illustrate the respective steps of the proposed method on the Bottle Filling System from [15, Section 3] and [3, Section 4.2.5].
In comparison to those expositions, we apply a proved transformation method [22] and a general analysis technique based on invariants [7].
2  uCRL and Timed uCRL  Timed uCRL specifications contain algebraic specifications of several abstract data types.
The only data types that are required are booleans and time.
The algebraic specifications of booleans are standard and can be found for instance in [8, Chapter IV].
We assume a constant t and functions !
: Bool - Bool and [?]
: Bool x Bool - Bool .
Time Time can be represented in many different ways.
In timed uCRL the time domain has to satisfy a set of properties.
We present these properties as an algebraic specification of sort Time by defining its signature and the axioms.
The signature of sort Time consists of: a constant 0; functions leq, eq : Time x Time - Bool , which are often abbreviated as <= and =, respectively; function if : Bool x Time x Time - Time; and functions min, max : Time x Time - Time.
The axioms of sort Time are presented below.
Many of the axioms are taken from, or inspired by [11, 16].
The axioms say that <= is a total order on the Time domain, and 0 is the least element.
(Time10 )  t<=u[?
]u<=w [?]t<=u[?]u<=w[?
]t<=w t<=u[?]!
w <=u[?]t<=u[?]!
w <=u[?]!
w <=t  (Time100 )  !
u<=t[?
]u<=w [?]!
u<=t[?]u<=w[?]!
w <=t  (Time1000 )  0<=t[?
]t  (Time2)  t<=u[?]u<=t[?
]t  (Time3)  eq(t, u) [?]
t <= u [?]
u <= t  (Time5)  min(t, u) [?]
if (t <= u, t, u)  (Time6)  max (t, u) [?]
if (u <= t, t, u)  (Time60 )  if (t, t, u) [?]
t  (Time7) (Time80 )  if (!b, t, u) [?]
if (b, u, t) if (b1 [?]
b2 , t, u) [?]
if (b1 , t, if (b2 , t, u))  (Time9)  if (b1 , if (b2 , t, u), w) [?]
if (b1 [?]
b2 , t, if (b1 , u, w))  (Time10)  if (t <= u [?]
u <= t, t, u) [?]
u  (Time11)  t <= if (b, u, w) [?]
(b [?]
t <= u) [?]
(!b [?]
t <= w)  (Time12)  if (b, u, w) <= t [?]
(b [?]
u <= t) [?]
(!b [?]
w <= t)  (Time13)  The last seven axioms allow to eliminate if from any boolean expression containing subterms of sort Time.
Every term of sort Time can be represented as 0, a variable, or as if (b, t, u) where t and u are terms of sort Time.
The above mentioned form has two extremes: one where all boolean terms b are variables, and another, where every variable of sort time (and 0) occurs at most once.
The latter form is useful for proving time identities in the following way: if we order the time variables occurring in a term as 0 < t1 < .
.
.
< tn , then with the help of the axioms we can transform every term of sort Time to the form if (b1 , ti0 , if (b2 , ti1 , .
.
.
if (bm , tim-1 , tim ) .
.
. ))
with indices such that tik < tik+1 .
Moreover, the conditions b1 , .
.
.
, bm can be made pairwise distinct,  2  i.e.
having the property that i 6= j - bi [?]
bj [?]
f .
In addition, the conditions b1 , .
.
.
, bm can be made such that if eq(tik , tik+1 ) [?]
t, then bk [?]
t. This gives us a method for proving identities of sort Time.
Other Data Types.
Any other data type in uCRL is specified in a similar way by providing a signature and axioms from which all other identities are derived.
Other data sorts have generally different axioms, and sometimes induction principles (cf.
[14]) are required to describe them.
Processes Next we define the binding-equational theory of timed uCRL by defining its signature and the axioms.
The signature of timed uCRL consists of data sorts (or 'data types') including Bool and Time as defined above, and a distinct sort Proc of processes.
Each data sort D is assumed to be equipped with a binary function eq : D x D - Bool .
(This requirement can be weakened by demanding such functions only for data sorts that are parameters of communicating actions).
The process operations are the ones listed below: - - - - * actions a : Da - Proc where a [?]
ActLab is an action label and Da is a list of parameter types of a.
It is assumed that the signature of timed uCRL is parameterized by the finite set of action labels ActLab.
* deadlock d :- Proc.
The constant d models inaction, or the inability to perform actions.
* alternative composition + : Proc x Proc - Proc.
The process p + q behaves like p or like q, depending on which of the two performs the first action.
* sequential composition * : Proc x Proc - Proc.
The process p * q first performs the actions of p, until p terminates, and then continues with the actions from q.
* conditional operator _ C _ B _ : Proc x Bool x Proc - Proc.
The process term p C b B q behaves like p if b is equal to t, and if b is equal to f it behaves like q. P * alternative P quantification d:D : Proc - Proc, for each data variable d of sort D. The process d p behaves like p[d1 /d] + p[d2 /d] + * * * , i.e., as the possibly infinite alternative composition of processes p[di /d] for any data term di of sort D. * at-operator , : Proc xTime - Proc.
A key feature of timed uCRL is that it can be expressed at which time a certain action must take place.
This is done using the at-operator.
The process p,t behaves like the process p, with the restriction that the first action of p must start at time t. The process p , t can delay till at most time t. If p consists of several alternatives, then only those with the first actions starting at time t will remain in p , t. The alternatives that start earlier than t will express that p , t can delay till that earlier time.
The alternatives that start later than t will express that p , t can wait till time t (but not till that later time).
* initialization operator  : Time xProc - Proc and weak initialization operator  : Time x Proc - Proc.
The initialization operator tp expresses the process in which all alternatives of p that start earlier than t are left out, but an alternative to delay till time t is added.
The weak initialization operator t [?]
p expresses the process in which all alternatives of p that start earlier than t are replaced by the ability to delay till those earlier times.
Thus the process t [?]
p can delay till the same time as p, while t  p can delay till at least time t, which can be longer than p could delay.
* parallel composition k : Proc x Proc - Proc, left-merge T : Proc x Proc - Proc, and communication merge | : Proc x Proc - Proc.
The process p k q can first perform an action of p, first perform an action of q, or start with a communication, or synchronization, between p and q.
The process p k q exists at time t only if both p and q exist at time t. The process p T q is as p k q, but the first action that is performed comes from p. The action can only be performed if the other party still exists at that time.
The process p | q also behaves as  3  the process p k q, except that the first action must be a communication between p and q.
Furthermore, these actions must occur at the same time and have the same data parameters.
The action resulting from such a communication is defined by the partial commutative and associative function g : ActLab x ActLab - ActLab such that g(a1 , a2 ) [?]
ActLab implies that a1 , a2 and g(a1 , a2 ) have parameters of the same sorts.
It is assumed that the signature of timed uCRL is parameterized by this function g. * encapsulation [?
]H : Proc - Proc, for H [?]
ActLab.
The process [?
]H (p) behaves as the process p where the execution of actions from the set H is prohibited.
* ultimate delay [?
]U : Proc - Proc.
The ultimate delay operator [?
]U(p) expresses the process, which can delay as long as p can, but cannot perform any action.
* before operator  : Proc - Proc.
The before operator p  q expresses the process in which all alternatives of p that start later than [?
]U(q) are replaced by the abilities to delay till [?]U(q).
Thus p  q cannot delay longer than both p and q.
The ultimate delay [?
]U(p) of process p can be expressed in terms of  as d  p. This process cannot perform actions and can delay as long as p could (because d can delay till any time).
Another key feature of timed uCRL is that it can be expressed that a process can delay till a certain time.
The process p + d , t can certainly delay till time t, but can possibly delay longer, depending on p. Consequently, the process d , 0 can neither delay nor perform actions, and the process d can delay for an arbitrary long time, but cannot perform any action.
We follow the intuition that a process that can delay till time t can also delay till an earlier moment, and a process that can perform a first action at time t can also delay till time t. P The descending order of binding strength of operators is: ,, *, {, [?
], }, {k, T, |}, CB, , +.
In Appendix A the axioms of timed uCRL are given.
Many of these axioms are taken from, or inspired by [21, 12].
To prove identities in timed uCRL we use a combined many-sorted calculus, which for the sort of processes has the rules of binding-equational calculus, for the sorts of booleans and time has the rules of equational calculus, while other data sorts may include induction principles which could be used to derive process identities as well.
We note that the derivation rules of binding-equational calculus do not allow to substitute terms containing free variables if they become bound.
The operational semantics (SOS) of timed uCRL and soundness and completeness proofs of the axiomatization are presented in [21].
The axiomatization used here is an extension of the axiomatization in [21] with a number of axioms that are derivable in the setting of [21] for all closed terms.
These extra axioms are needed to prove correctness of the linearization.
Timed uCRL Specifications For the purpose of this paper we restrict to the timed uCRL specifications that do not contain left merge (T), communication (|), ultimate delay ([?
]U), and before () operators explicitly.
These operators were introduced to allow the finite axiomatization of parallel composition (k) and timing constructs in the bisimulation setting, and they are hardly used explicitly in timed uCRL specifications.
We consider systems of process equations with the right hand sides from the following subset of timed uCRL terms P - - - - p ::= a( t ) | d | Y( t ) | p + p | p * p | p k p | d:D p | p C c B p | [?
]H (p) | p , t | t  p | t [?]
p - - For a system of process equations G containing a process equation for X, (X( t ), G) is a process - - definition if t is a list of data terms that corresponds to the type of process X.
The combination - - of the given data specification with a process definition (X( t ), G) of process equations determines a timed uCRL specification.
Such a specification depends on a finite subset Act of ActLab and on Comm, an enumeration of g restricted to the labels in Act.
4  2.1  Example: Bottle Filling System  This example is taken from [15, Section 3] and [3, Section 4.2.5].
We start from the informal specification from [3, page 153]: "Bottles on a conveyor belt are filled with 10 liters of liquid poured from a container with a capacity of m liters.
The container is filled at a constant rate of r liters per second.
When a bottle is under the container, a tap is opened and the bottle is filled at a rate of 3 liters per second until the container becomes empty.
From that moment, the bottle is filled at the same rate as the container.
When the bottle is full, the tap is closed and the conveyor belt starts moving to put the next bottle under the container which takes 1 second.
Obviously, it is highly preferable that overflow (of the container) never occurs.
Of course, it is also preferable that the container does not get empty during the filling of each bottle."
Specification in Timed uCRL The time domain used in this specification is nonnegative rational numbers.
For the specification of the conveyor belt we distinguish three 'modes of operation': mv - moving, nf - normal filling, and sf - slow filling.
CBmv (t:Time) =!start , (t + 1) * CBnf (t + 1) X CBnf (t:Time) = ?empty , (t + t0 ) * CBsf (t + t0 , 3t0 ) C t0 < 10/3B  t0 :Time !stop , (t + 10/3) * CBmv (t + 10/3) CBsf (t:Time, l:Q) =!stop , (t + (10 - l)/r) * CBmv (t + (10 - l)/r) The process CBmv executes the !start action and then behaves as the process CBnf .
The latter process can synchronize with the container process by ?empty at time period t + t0 , where t0 [?]
[0, 10/3), or it can synchronize by !stop action at time t + 10/3.
The further behavior of CBnf depends on which action it synchronized.
For the specification of the container also three modes are distinguished: inc - increasing the amount of liquid, dec - decreasing, and dry - liquid goes through the empty container directly into the bottle.
X Cinc (t:Time, h:Q) = ?start,(t + t0 ) * Cdec (t + t0 , h + rt0 ) C t0 < (m - h)/rB  t0 :Time !overflow , (t + (m - h)/r) * d , (t + (m - h)/r) X Cdec (t:Time, h:Q) = ?stop,(t + t0 )*Cinc (t + t0 , h - (3 - r)t0 ) C t0 <= h/(3 - r) B d,0 t0 :Time  + !empty , (t + h/(3 - r)) * Cdry (t + h/(3 - r)) X Cdry (t:Time) = ?stop ,t0 * Cinc (t0 , 0) t0 :Time  The process Cdec behaves nondeterministically at time t + h/(3 - r).
Depending on the parallel process it can either perform ?stop or !empty in order to synchronize with that process.
Using the above descriptions of the conveyor belt and the container, the system can be given as: T(t:Time, h:Q) = [?
]H (CBmv (t) k Cinc (t, h)) where g(?s, !s) = g(!s, ?s) = s for s [?]
{start, stop, empty} and g is undefined otherwise, and H = {?s, !s | s [?]
{start, stop, empty}}.
In words, the system is a parallel composition of the conveyor belt and the container processes, that are forced to synchronize on all actions except !overflow.
Let G contain all of the above equations.
Then the process definition (T(0, h), G) forms the specification of the bottle-filling system.
5  3  Linearization  - - The problem of linearization of a timed uCRL specification defined by (X( t ), G) consists of generation of a new timed uCRL specification which * depends on the same Act and Comm, * contains all data definitions of the original one, and, possibly, definitions of the auxiliary data types, - - * is defined by (Z(mX ( t )), L), where L contains exactly one process equation for Z in linear form (defined later), and mX is a mapping from the parameters of X to the parameters of X.
- - - - such that all processes that are solutions of (X( t ), G) are also solutions of Z(mX ( t )), L).
It is not possible to linearize a timed uCRL specification which is unguarded, e.g.
X = X cannot be brought to the linear form.
The exact notion of guardedness in uCRL is rather complicated.
In a nutshell, in a guarded process every occurrence of a recursive call is preceded (with sequential composition) by an action.
We refer to [22, Section 3.6] for a precise definition.
We define Timed Parallel Greibach Normal Form (TPEGNF) and Timed Linear Process Equation (TLPE) as special forms of process equations in timed uCRL.
TPEGNF and TLPE are similar to the Greibach Normal Form [10] for context-free languages.
A timed uCRL process equation is in TPEGNF if it is of the form: XX --- - --- - --- --- --- X(d:D) = ai ( fi (d, ei )) , ti (d, ei ) * pi (d, ei ) C ci (d, ei ) B d,0 --- i[?
]I - ei :Ei  +  X X  - --- - --- --- aj ( fj (d, ej )) , tj (d, ej ) C cj (d, ej ) B d,0  --- j[?
]J - ej :Ej  +  X  --- --- d , td (d, ed ) C cd (d, ed ) B d,0  ---- ed :Ed  --- where I and J are disjoint, and all pi (d, ei ) have the following syntax: - - - - - - p::=a( t ) | d | Y( t ) | p * p | p k p | [?
]H (pkp) | [?
]H (Y( t )) | p,t | t  p | t [?]
p A timed uCRL process equation is called Timed Linear Process Equation (TLPE) if it is of the --- --- - same form as above, but the terms pi (d, ei ) are recursive calls of the form X(- gi (d, ei )) for some - - function vectors gi .
- - The equation is explained as follows.
The process X, being in a state vector d , can for any - - - - - --- - - ei , that satisfy the condition ci (d, ei ), perform an action ai parameterized by fi (d, ei ) at the --- --- - - absolute time ti (d, ei ), and then proceed to the state - gi (d, ei ).
Moreover, it can for any - ej , that --- - --- - satisfy the condition cj (d, ej ), perform an action aj parameterized by fj (d, ej ), and then terminate --- ---- successfully.
The last summand indicates that for any ed :Ed , that satisfies cd (d, ed ), the process --- can wait till the absolute time td (d, ed ).
- - As input for the linearization procedure we take a timed uCRL process definition (X( t ), G).
Further on, the process goes through a number of intermediate forms, including TPEGNF, and finally we get to TLPE.
All the steps are described in [22, Chapter 6] and are proved to transform a system of process equations in timed uCRL to an equivalent one.
3.1  Linearization of the Example  In this subsection we illustrate some of the linearization steps on our example.
The example does not contain double bound variables, so we can start with reducing right hand sides of the equations  6  with the help of conventional term rewriting [2].
This step is described in [22, Section 6.2.2].
By doing this step the equations for CBnf and Cinc change to the following: X CBnf (t:Time) = ?empty , (t + t0 ) * CBsf (t + t0 , 3t0 ) C t0 < 10/3 B d,0 t0 :Time  + !stop , (t + 10/3) * CBmv (t + 10/3) X Cinc (t:Time, h:Q) = ?start , (t + t0 ) * Cdec (t + t0 , h + rt0 ) C t0 < (m - h)/r B d,0 t0 :Time  + !overflow , (t + (m - h)/r) * d , (t + (m - h)/r) In both equations we move the alternative composition operator (+) outside the sum operator and eliminate the sum in the second summand.
At this point all our equations, except the one for T are in TPEGNF.
We proceed by guarding [22, Section 6.2.4] the equation for T. We have to consider the term [?
]H (CBmv (t) k Cinc (t, h)) and apply the guarding procedure to it.
We get the following: [?
]H (CBmv (t) k Cinc (t, h)) = start , (t + 1) * [?
]H (CBnf (t + 1) k Cdec (t + 1, h + r)) C h < m - r B d,0 + !overflow , (t + (m - h)/r) * d , (t + (m - h)/r) C m - r <= h B d,0 Here we use the fact that only !overflow action is not forced to synchronize, and the rest of the actions have to.
It is interesting to see how the !overflow action gets its condition.
It has to happen before time t + 1, otherwise the action !start , (t + 1) should occur first.
This means that t + (m - h)/r <= t + 1 should hold, which is equivalent to m - r <= h. Now we consider the term [?
]H (CBnf (t)kCdec (t, h)) and apply the guarding procedure to it.
Here we again use the fact that the all the actions of the two processes have to synchronize.
We get the following: [?
]H (CBnf (t) k Cdec (t, h)) = stop , (t + 10/3) * [?
]H (CBmv (t + 10/3) k Cinc (t + 10/3, h - 10(3 - r)/3)) C 10(3 - r)/3 <= h B d,0 +empty , (t + h/(3 - r)) * [?
]H (CBsf (t + h/(3 - r), 3h/(3 - r)) k Cdry (t + h/(3 - r))) C h < 10(3 - r)/3 B d,0 Now we consider the term [?
]H (CBsf (t, l) k Cdry (t)) and apply the guarding procedure to it.
We get the following: [?
]H (CBsf (t, l) k Cdry (t)) = stop , (t + (10 - l)/r) * [?
]H (CBmv (t + (10 - l)/r) k Cinc (t + (10 - l)/r, 0)) At this point we are ready to make a single equation for the whole system (cf.
[22, Section 4.3]).
Here we use the fact that only the parallel processes are reachable.
We define a new sort  7  State = {mv_inc, nf_dec, sf_dry, dl} and use it as a parameter of the resulting process T: T(s:State, t:Time, h, l:Q) = start , (t + 1) * T(nf_dec, t + 1, h + r, 0) C s = mv_inc [?]
h < m - r B d,0 + !overflow , (t + (m - h)/r) * T(dl, t + (m - h)/r, 0, 0) C s = mv_inc [?]
m - r <= h B d,0 + stop , (t + 10/3) * T(mv_inc, t + 10/3, h - 10(3 - r)/3, 0) C s = nf_dec [?]
10(3 - r)/3 <= h B d,0 + empty , (t + h/(3 - r)) * T(sf_dry, t + h/(3 - r), 0, 3h/(3 - r)) C s = nf_dec [?]
h < 10(3 - r)/3 B d,0 + stop , (t + (10 - l)/r) * T(mv_inc, t + (10 - l)/r, 0, 0) C s = sf_dry B d,0 + d , t C s = dl B d,0 This equation is in TLPE format.
Let system of equations L contain the above equation only.
Then (T(mv_inc, 0, h, 0), L) is the linearized specification of the bottle-filling system.
4  Time-free Abstraction and Analysis  - - An important notion of timed uCRL processes is well-timedness.
A term a( t ) , t * p is well-timed - - if p [?]
t  p. If t is such that c(t) [?]
t implies p [?]
t  p, then a( t ) , t * p C c(t) B d , 0 is also - - well-timed.
Terms a( t ) , t and d , t are also well-timed.
If p and q are well-timed terms, then p + q, P d:D p and p C c B d,0 are also well-timed terms.
- - An equation in TPEGNF is well-timed if for all i [?]
I the terms ai ( ti ) , ti * pi C ci B d , 0 are well-timed.
The linearization method for timed uCRL ensures that the resulting TLPE is well-timed, e.g., in our example, t  T(s, t, h, l) [?]
T(s, t, h, l).
The time-free abstraction (cf.
[21, Section 4.2]) of well-timed TLPEs can be used for further analysis with methods that are designed for untimed uCRL.
For instance, strong bisimilarity of time-free abstractions of two well-timed TLPEs is equivalent to the timed bisimilarity of them.
In the initial timed uCRL specification time has a direct influence on the specified behavior, for instance on the interleavings of parallel components (for example a , 1 k b , 2 [?]
a , 1 * b , 2 in timed uCRL).
This is why performing the time-free abstraction on the initial specification will not work (because a(1) k b(2) 6[?]
a(1) * b(2) in uCRL).
However, after linearization the influence of time on the specified behavior is encoded in the parameters and conditions of resulting TLPE, i.e.
time becomes just a conventional data type in untimed uCRL.
Applying time-free abstraction to our example gives us the following uCRL equation: T(s:State, t:Time, h, l:Q) = start(t + 1) * T(nf_dec, t + 1, h + r, 0) C s = mv_inc [?]
h < m - r B d + !overflow(t + (m - h)/r) * T(dl, t + (m - h)/r, 0, 0) C s = mv_inc [?]
m - r <= h B d + stop(t + 10/3) * T(mv_inc, t + 10/3, h - 10(3 - r)/3, 0) C s = nf_dec [?]
10(3 - r)/3 <= h B d + empty(t + h/(3 - r)) * T(sf_dry, t + h/(3 - r), 0, 3h/(3 - r)) C s = nf_dec [?]
h < 10(3 - r)/3 B d + stop(t + (10 - l)/r) * T(mv_inc, t + (10 - l)/r, 0, 0) C s = sf_dry B d + [?
](t) C s = dl B d Analysis We try to prove some properties of the bottle-filling system.
For this we assume that in the initial state m > r > 0.
It is easy to see that both r and m do not change in T, and 8  therefore these properties are invariants of T. It is also easy to see that h >= 0 is an invariant of the system.
Having assumed that, we see that if h >= m - r in the initial state, then the overflow is eminent at time (m - h)/r.
It is interesting to see what happens if h < m - r in the initial state.
To this end, we can see that the following formula is an invariant (cf.
[7]) of the LPE: r <= 30/13 [?]
(s = mv_inc == h < m - r) [?]
(s = nf_dec == h < m) This gives us the fact that if r <= 30/13, then overflow is not reachable provided h < m - r holds in the initial state.
In case r > 30/13, the process T is equal to T(s:State, t:Time, h, l:Q) = start(t + 1) * stop(t + 13/3) * T(mv_inc, t + 13/3, h + r - 10(3 - r)/3, 0) C s = mv_inc [?]
h < m - r B d + !overflow(t + (m - h)/r) * T(dl, t + (m - h)/r, 0, 0) C s = mv_inc [?]
m - r <= h B d + [?
](t) C s = dl B d This is because r > 30/13 and h >= 0 implies that h+r < 10(3-r)/3 is always false.
It is clear that the value of h increases with every sequence of start, stop actions by a constant, so the overflow is eminent.
The next question is whether the container may become empty.
From the previous analysis follows that this can only happen if r <= 30/13 and h < m - r holds in the initial state.
If r < 30/13, the value of h will decrease with each sequence of start, stop actions by a constant.
So, eventually, its value will become smaller than 10(3 - r)/3 and the container will become empty.
In case r = 30/13, the value of h is constant in state nf_dec and equal to the initial value of h plus 10(3 - r)/3 = 30/13, so, the container does not become empty in this case.
5  Conclusions and Future Work  We presented the language of timed uCRL with an example specification.
We outlined the method of simplifying transformations based on equational axiomatization, and illustrated it on the example.
This transformation allows for a time-free abstraction of the specification, which in turn enables the use of tools and techniques for verification of untimed systems.
For proving properties of the presented example we used known invariant [7] techniques.
An interesting direction for future work is in adopting efficient real-time abstraction techniques similar to the regions and zones methods [1] for timed automata.
Another interesting approach is to make use of model checking techniques, similar to the ones available for timed automata in tools like UPPAAL [18].
A symbolic model checking approach for untimed uCRL has been recently proposed in [17].
It looks more applicable to the time setting than the explicit model checking [20] of modal mu-calculus formulas.
In order to apply any of these methods for timed setting a well-thought extension of modal mu-calculus (or another action-based temporal logic) to real-time is needed.
References [1] R. Alur.
Timed automata.
In Proc.
CAV'99, LNCS 1633, pages 8-22, 1999.
[2] F. Baader and T. Nipkow.
Term Rewriting and All That.
Cambridge University Press, August 1999.
[3] J. C. M. Baeten and C. A. Middelburg.
Process Algebra with Timing.
Monographs in TCS.
Springer, 2002.
9  [4] J. C. M. Baeten and W. P. Weijland.
Process Algebra.
Cambridge Tracts in TCS 18.
Cambridge University Press, 1990.
[5] J.
A. Bergstra, J. Heering, and P. Klint, editors.
Algebraic Specification.
ACM Press, ACM Press Frontier Series, 1989.
[6] J.
A. Bergstra and J. W. Klop.
Process algebra for synchronous communication.
Information and Computation, 60(1/3):109-137, 1984.
[7] M. A. Bezem and J. F. Groote.
Invariants in process algebra with data.
In B. Jonsson and J. Parrow, editors, Proc.
CONCUR'94, LNCS 836, pages 401-416.
Springer, 1994.
[8] S. N. Burris and H. P. Sankappanavar.
A Course in Universal Algebra.
Number 78 in Graduate Texts in Mathematics.
Springer-Verlag, 1981.
[9] W. J. Fokkink.
Introduction to Process Algebra.
Texts in TCS.
An EATCS Series.
Springer, 2000.
[10] S. A. Greibach.
A new normal-form theorem for context-free phase structure grammars.
JACM, 12(1):42-52, 1965.
[11] J. F. Groote.
The syntax and semantics of timed uCRL.
Report SEN-R9709, CWI, Amsterdam, 1997.
[12] J. F. Groote and S. P. Luttik.
Undecidability and completeness results for process algebras with alternative quantification over data.
Report SEN-R9806, CWI, Amsterdam, July 1998.
Available from http://www.cwi.nl/~luttik/.
[13] J. F. Groote and M. A. Reniers.
Algebraic process verification.
In J.
A. Bergstra, A. Ponse, and S. A. Smolka, editors, Handbook of Process Algebra, chapter 17, pages 1151-1208.
Elsevier, 2001.
[14] J. F. Groote and J. J. v. Wamel.
Algebraic data types and induction in uCRL.
Report P9409, University of Amsterdam, Programming Research Group, 1994.
[15] J. F. Groote and J. J. v. Wamel.
Analysis of three hybrid systems in timed uCRL.
SCP, 39:215-247, 2001.
[16] J. F. Groote and J. J. v. Wamel.
The parallel composition of uniform processes with data.
TCS, 266(1-2):631-652, 2001.
[17] J. F. Groote and T. A. C. Willemse.
A checker for modal formulae for processes with data.
In F. S. de Boer, M. M. Bonsangue, S. Graf, and W.-P. de Roever, editors, Proc.
FMCO'04, LNCS 3188, pages 223-239, 2004.
[18] K. G. Larsen, P. Pettersson, and W. Yi.
UPPAAL in a nutshell.
International Journal on Software Tools for Technology Transfer, 1(1-2):134-152, 1997.
[19] J. Loeckx, H.-D. Ehrich, and M. Wolf.
Algebraic specification of abstract data types.
In S. Abramsky, D. Gabbay, and T. S. E. Maibaum, editors, Handbook of Logic in Computer Science, Vol 5, chapter 4, pages 217-316.
Oxford University Press, 2000.
[20] R. Mateescu and M. Sighireanu.
Efficient on-the-fly model-checking for regular alternation-free mu-calculus.
SCP, 2002.
[21] M. A. Reniers, J. F. Groote, J. J. v. Wamel, and M. B. v. d. Zwaag.
Completeness of Timed uCRL.
Fund.
Inf., 50(3-4):361-402, 2002.
[22] Y. S. Usenko.
Linearization in uCRL.
PhD thesis, Eindhoven University of Technology, December 2002.
[23] T. A. C. Willemse.
Semantics and Verification in Process Algebras with Data and Timing.
PhD thesis, Eindhoven University of Technology, 2003.
10  A  Axioms of Timed uCRL  We assume that * x, y, z are variables P of sort Proc; c, c1 , c2 are variables of sort Bool ; d, d1 , dn , d0 , .
.
.
are data variables (but d in d:D is not a variable); and t, u, w are variables of sort Time.
- - * b stands for either a( d ), or d; - - - - - - 1 n * d = d0 is an abbreviation for eq(d1 , d0 ) [?]
* * * [?]
eq(dn , d0 ), where d = d1 , .
.
.
, dn and -0 - 1 n d = d0 , .
.
.
, d 0 ; * the axioms where p and q occur are schemata ranging over all terms p and q of sort Proc, including those in which d occurs freely; * the axiom (SUM2) is a scheme ranging over all terms r of sort Proc in which d does not occur freely.
11  x+y [?
]y+x  (A1)  x + (y + z) [?]
(x + y) + z  (A2)  x+x[?
]x  (A3)  (x + y) * z [?]
x * z + y * z  (A4)  (x * y) * z [?]
x * (y * z)  (A5)  x + [?
]U (x) [?]
x  (A6T)  d + [?
]U (x) [?]
d  (A6T0 )  d*x[?
]d  x k y [?]
(x T y + y T x) + x | y  b , t T y [?]
(b , t  y) * y  (b , t * x) T y [?]
(b , t  y) * ((t  x) k y) (x + y) T z [?]
x T z + y T z 0  0  xCtBy [?
]x  (Cond1) (Cond2)  x C c B y [?]
x C c B d , 0 + y C !c B d , 0  0  (x + y) | z [?]
x | z + y | z  (Cond3T)  (ATA8)  0  otherwise  x|y [?
]y|x  (x C c B d , 0) * y [?]
(x * y) C c B d , 0  (Cond6T)  (x T y) T z [?]
x T (y k z)  (x + y) C c B d , 0 [?]
x C c B d , 0 + y C c B d , 0  (Cond7T)  (x C c B d , 0) T y [?]
(x T y) C c B d , 0  (Cond8T)  (x C c B d , 0) | y [?]
(x | y) C c B d , 0  p C eq(d, e) B d , 0 [?]
p[d := e] C eq(d, e) B d , 0 X x[?
]x  (PET)  x | (y T z) [?]
(x | y) T z  xTd [?
]x*d  x | d [?]
[?
]U (x)  r[?]
e:D  X d:D  X  (r[e := d])  X  p+p  X X d:D  X d:D  X  X  X  d:D  (p * x) [?]
(  X  q  (SUM4) (SUM5)  d:D  (p T x) [?]
( (p | x) [?]
(  (SCT2) (AT1) (AT2) (ATA10 )  (x + y) , t [?]
x , t + y , t  (ATA2)  (x * y) , t [?]
x , t * y X X ( p) , t [?]
p,t  (ATA3)  d:D  (ATA4)  d:D  (SUM6)  (x C c B d , 0) , t [?]
x , t C c B d , 0  (ATA50 )  p) | x  (SUM7)  t[?]x[?]
tx[?]t[?
]x+d,t X x , u C t <= u B d ,0  (ATD0)  X  (SUM8)  xb[?
]x  X d:D  X  (x , t T y) C u <= t B d , 0  x , t , u [?]
x , t C t = u B d , 0 + [?
]U (x) , min(t,u)  d:D  p) * x  x,t  b,t*y [?
]b,t*ty  (SUM3) p+  X  (SC5) (SCD1)  t:Time  d:D  (p + q) [?]
d:D  d:D  (SUM2)  d:D  p[?]
d:D  X  x[?]
(SC1) (SC4)  (SCT1)  (x , t T u [?]
y) C u <= t B d , 0 [?]
d:D  X  (CF2)  (SCDT2)  x , t T y [?]
(x T y) , t  (SUM1)  (CF1)  (SC3)  (x | y) | z [?]
x | (y | z)  (Cond9T) (ScaT)  X  (CM8)  (x | y) , t [?]
x | y , t - -0 - -0 - - - - - 0 0 - a( d ) | a ( d ) [?]
g(a, a )( d ) C d = d B d  (x C c1 B d , 0) + (x C c2 B d , 0) [?]
x C c1 [?]
c2 B d , 0 (Cond5T)  (x C c B d , 0) * (y C c B d , 0)[?
](x * y) C c B d , 0  (CM7) (ATA7)  - -0 - - 0 a( d ) | a ( d ) [?]
d  (Cond4T)  (CM4)  (x | y) , t [?]
x , t | y  if g(a, a ) is defined  (x C c1 B d , 0) C c2 B d , 0 [?]
(x C c1 [?]
c2 B d , 0)  (CM3T) (CM5)  (b * x) | (b * y) [?]
(b | b ) * (x k y)  (A7)  xCf By [?
]y  0  (b * x) | b [?]
(b | b ) * x  (CM1) (CM2T)  p) T x  d:D  ([?
]H (p)) [?]
[?
]H (  (ATB0)  u:Time  p)  d:D  (p C c B d , 0) [?]
(  d:D  X  p) C c B d , 0  (SUM12T)  d:D  (ATC10 )  x  (y + z) [?]
x  y + x  z  (ATC2)  x  (y * z) [?]
x  y X X x p[?]
xp  (ATC3)  - - [?
]H (b) [?]
b if b = a( d ) and a [?]
/H  (D1)  [?
]H (b) [?]
d otherwise  (D2)  x  (y C c B d , 0) [?]
(x  y) C c B d , 0+x , 0  [?
]H (x + y) [?]
[?
]H (x) + [?
]H (y)  (D3)  [?
]H (x * y) [?]
[?
]H (x) * [?
]H (y)  (D4)  x  (y T z) [?]
x  (y  z)  [?
]H (x C c B d , 0) [?]
[?
]H (x) C c B d , 0 [?
]H (x , t) [?]
[?
]H (x) , t  d:D  x  (y | z) [?]
x  (y  z)  (D5T) (D7)  x  ([?
]H (y)) [?]
x  y X (x  y) , u C u <= t B d , 0  x  (y , t) [?]
u:Time  12  (ATC4)  d:D  (ATC50 ) (ATC6) (ATC7) (ATC8) (ATC11)
Belief Revision in a Discrete Temporal Probability-Logic  Scott D. Goodwin  Department of Computer Science University of Regina, Canada  Howard J. Hamilton  Department of Computer Science University of Regina, Canada  Abdul Sattar  School of Computer & Information Technology Grifith University, Australia  Abstract  We describe a discrete time probabilitylogic for use as the representation language of a temporal knowledge base.
In addition to the usual expressive power of a discrete temporal logic, our language allows for the specication of non-universal generalizations in the form of statistical assertions.
This is similar to the probability-logic of Bacchus, but diers in the inference mechanisms.
In particular, we discuss two interesting and related forms of inductive inference: interpolation and extrapolation.
Interpolation involves inferences about a time interval or point contained within an interval for which we have relevant statistical information.
Extrapolation extends statistical knowledge beyond the interval to which it pertains.
These inferences can be studied within a static temporal knowledge base, but the further complexity of dynamically accounting for new observations makes matters even more interesting.
This problem can be viewed as one of belief revision in that new observations may conict with current beliefs which require updating.
As a rst step toward a fulledged temporal belief revision system, we consider the tools of inductive logic.
We suggest that Carnap's method of conrmation may serve as a simple mechanism for belief revision.
1 Introduction  Standard discrete temporal logics allow the representation of what is true at a point, in a situation, or over an interval.
To introduce uncertainty, many researchers in AI have turned to nonmonotonic \logics," but semantic and computational diculties have led some to consider probability as a representational device.
Here we describe a discrete time probabilitylogic for use as the representation language of a tem-  Eric Neufeld  Department of Computational Science University of Saskatchewan, Canada  Andre Trudel  Jodrey School of Computer Science Acadia University, Canada  poral knowledge base.
In addition to the usual expressive power of a discrete temporal logic, our language allows for the specication of non-universal generalizations in the form of statistical assertions.
This is similar to the probability-logic of Bacchus 1], but diers in the inference mechanisms.
In particular, we discuss two interesting and related forms of inductive inference: interpolation and extrapolation.
Interpolation involves inferences about a time interval or point contained within an interval for which we have relevant statistical information.
Extrapolation extends statistical knowledge beyond the interval to which it pertains.
These inferences can be studied within a static temporal knowledge base, but the further complexity of dynamically accounting for new observations makes matters even more interesting.
This problem can be viewed as one of belief revision in that new observations may conict with current beliefs which require updating.
As a rst step toward a full-edged temporal belief revision system, we consider the tools of inductive logic.
We suggest that Carnap's method of conrmation 3] may serve as a simple mechanism for belief revision.
We begin by introducing our temporal logic and then turn to the problem of inferencing.
The rst form of inference we consider is what Carnap calls direct inference: the inference from a population to a sample.
In the case of temporal information, this amounts to inference from an interval statistic to a subinterval or point.
Before moving on to more complex kinds of inference, we introduce the learning (or belief revision) component, Carnap's method of conrmation, which incorporates new observations into the direct inference process.
Next we consider the general case of direct inference: interpolation.
Then we turn our attention to the problem of extrapolation of statistical information (what Carnap calls predictive inference).
Finally, we consider the problem of belief revision in connection with these temporal inferences.
2 Discrete temporal probability-logic  In this section, we introduce a discrete probabilitylogic which serves as a representation language for temporal applications.
The probability-logic, which we call PL(T ), is similar to that of Bacchus 1].
The most important dierence is in the inference machinery and the addition of time into the ontology.
PL(T ) allows the expression of standard rst order logic expressions plus two kinds of probability statements.
Before examining the probability-logic, we rst explore the two kinds of probability.
2.1 Statistical and inductive probabilities  Carnap 3] has suggested the need for two distinct concepts of probability (the relevance of this view to AI was recently suggested 1, 8]).
The statistical concept of probability, having the sense of relative frequency, is needed for empirical knowledge (e.g., most birds y).
As well, the inductive concept of probability, measuring the degree of conrmation of a hypothesis on the basis of the evidence, is needed for beliefs (e.g., to what degree is the belief that Tweety ies supported by the evidence that Tweety is a bird and most birds y).
While statistical probability is empirically based, inductive probability is epistemologically based that is, inductive probabilities constitute a logical relationship between belief (or hypothesis) and evidence.
To give such beliefs empirical foundations, a connection must be made between the statistical and inductive probabilities.
This connection is made on the basis of an appeal to some form of the principle of indierence which says that if our knowledge does not favour the occurrence of one event over another, then the evidence provides equal conrmation for the events.
The inference of inductive probabilities from statistical probabilities via a principle of indierence is called direct inference.
As Carnap 4] has noted, the form of indierence used must be carefully restricted to avoid the introduction of contradictions at the same time, it must remain strong enough to sanction the appropriate conclusions.
The principle of indierence comes into play when choosing the prior probabilities of hypotheses.
Each consistent assignment of priors constitutes a dierent inductive method.
Carnap 4] described two inductive methods which we outline next.
2.2 Two inductive methods  Carnap's two methods are most easily explained with reference to the example shown in Figure 1.
In this example, we have four individuals (balls in an urn) and one property (colour).
Since each ball is either blue (B) or white (W), we regard colour as a binary property (blue or not-blue).
An individual distribution is specied by ascribing one colour to each individual e.g., in individual distribution #2, the rst three balls are blue and the last ball is not.
A statistical distribution is specied by stating the number of individuals for which the property is true, without identifying the individuals e.g., in statistical distribution #2, three of the balls are blue and one is not.
There are 16 possible individual distributions and 5 statistical distributions.
As can be seen in Figure 1, several individual distributions may correspond to a single statistical distribution.
If equal prior probabilities are assigned to each of the individual distributions, the result is Carnap's Method I, and if equal prior probabilities are assigned to each of the statistical distributions, the result is Method II.
Method I consists of applying the principle of indierence to individual distributions and, in the examples, gives each individual distribution a prior probability of 1/16.
Method II consists of rst applying the principle of indierence to the statistical distributions, and then, for each statistical distribution, applying the principle to its individual distributions.
In the example, each of the ve statistical distribution is assigned 1/5, and each 1/5 is divided equally among the individual distributions of the appropriate statistical distribution.
Method II assigns 1/20 to each of individual distributions #2 to #5 because they are the four possibilities (arrangements) for statistical distribution #2 (3 blue balls and 1 white ball).
Method II is consistent with the principle of learning from experience, but Method I is not.
The principle of learning from experience is: \other things being equal, a future event is to be regarded as the more probable, the greater the relative frequence of similar events observed so far under similar circumstance" 4, p. 286].
Suppose we draw three blue balls in sequence, and then consider the probability of the fourth ball being blue.
There are two individual distributions consistent with the evidence: #1 (in which the fourth ball is blue) and #2 (in which the fourth ball is not blue).
Using Method I, the probability is 1/2 because each of individual distributions #1 and #2 is assigned a probability of 1/16, and 1/2 is the relative weight of 1/16 to (1/16 + 1/16).
Using Method II, the probability of the fourth ball being blue is 4/5 because individual distribution #1 is assigned a probability of 1/5 and individual distribution #2 is assigned a probability of 1/20, and 4/5 is the relative weight of 1/5 to (1/5 + 1/20).
Because Method II incorporates the principle of learning from experience, it is better suited to our intended application of temporal reasoning in dynamic situations where new observations are being made.
In Section 3.1, we apply Method II to direct inference from temporal statistical knowledge, but rst we turn to the description of our temporal probability logic.
2.3 PL(T ): A 	rst order temporal probability-logic PL(T ) is a four sorted, rst order, modal logic.1 The 1  Some material in this section is derived from 2].
STATISTICAL INDIVIDUAL METHOD I METHOD II DISTRIBUTIONS DISTRIBUTIONS Initial Initial Probability of Number Number Probability Statistical Individual of of of Individual Distributions Distributions Blue White Distributions 1.
4 0 1.
    1/16 1/5 1/5 = 12/60 2.
3  1  2.
3.
4.
5.
                    1/16 1/16 1/16 1/16  1/5  1/20 = 3/60 1/20 = 3/60 1/20 = 3/60 1/20 = 3/60                              1/16 1/16 1/16 1/16 1/16 1/16  1/5  1/30 = 2/60 1/30 = 2/60 1/30 = 2/60 1/30 = 2/60 1/30 = 2/60 1/30 = 2/60            3.
2  2  6.
7.
8.
9.
10.
11.
4.
1  3  12.
13.
14.
15.
          1/16 1/16 1/16 1/16  1/5  1/20 = 3/60 1/20 = 3/60 1/20 = 3/60 1/20 = 3/60  5.
0  4  16.
    1/16  1/5  1/5 = 12/60  Figure 1: Carnap's Two Methods (from 3, p. 285]) sorts are: object types, object instances, numbers, and times.
Suitable function and predicate symbols are associated with each sort.
Time invariant assertions can be made about domain objects via their object types, while object instances are used to describe domain objects at particular times.
The numeric sort is used to make numeric assertions, specically, assertions about the numeric values of certain probabilities.
The temporal sort allows assertions to include references to the time.
Both the numeric and temporal sort include the constants 1, -1, and 0.
The functions + and fi and the predicates = and < are \overloaded" to provide for all necessary combinations of argument and result sorts.
Additional inequality predicates, numeric and temporal constants can easily be added by denition, and we use them freely.
The formulas of the language are generated by applying standard rst order formula formation rules.
Additional rules are used to generate object instance terms from object type terms and temporal terms: 1.
If o is an object type term, t is a temporal term, ~o is a vector <o1 ,: : : ,on> of n object type terms, and ~t is a vector <t1 ,: : : ,tn> of n temporal terms, then (a) o@t is an object instance term (b) ~o@~t is a vector of n object instance terms,  specically, <o1 @t1,: : : ,on @tn> (c) ~o@t is a vector of n object instance terms, specically, <o1 @t,: : : ,on@t> (d) o@~t is a vector of n object instances terms, specically, <o@t1 ,: : : ,o@tn>.
Two additional rules are used to generate new numeric terms (specically probability terms) from existing formulas: 2.
If  is a formula and ~x is a vector of n distinct object type, object instance and/or temporal variables, then ]~x is a statistical probability term.
3.
If  is a formula, then prob() is an inductive probability term.
These new (numeric) terms denote numbers (that correspond semantically to the values of the probability measure) which can in turn be used as arguments of numeric predicates in the generation of additional new formulas.
We dene conditional probability terms (of both types): j ]~x =df  ^  ]~x = ]~x, and prob(j ) =df prob( ^  )=prob ( ).2 Semantically the language is interpreted using For ease of exposition, we ignore the technicalities of dealing with division by zero.
See 1] for details.
2  models of the form3 M = hO S #  O   S i where: 1.
O is a domain of objects types (i.e., the domain of discourse).
S is a set of states or possible worlds.
# is a state dependent interpretation of the symbols.
Numbers are interpreted as reals IR and the set of times T is taken as integers ZZ .
The associated predicate and function symbols are interpreted as relations and functions over the appropriate domain while +, fi, 1, -1, 0, < and = are given their normal interpretation in every state.4 2.
O is a discrete probability measure P over O. ThatPis, for every A  O,  O (A) = o2A  O (o) and o2O  O (o) = 1.
3.
S is a discrete probability measure P over S .
ThatPis, for every S 0  S ,  S (S 0 ) = s2S  S (s) and s2S  S (s) = 1.
4.
T is a discrete probability measure P over T .
ThatPis, for every T  T ,  T (T ) = t2T  T (t) and t2T  T (t) = 1.
5.
O@T is a discrete probability measure over the set of object instances O@T .
That is, for every O @T P  O@TP ,  O@T (O@T ) = o@t2O@T  O@T (o@t) and o@t2O@T  O@T (o@t) = 1.
O@T is a product measure formed from  O and  T .
The formulas of the language are interpreted with respect to this semantic structure in a manner standard for modal languages.
In particular, the interpretation of a formula depends on a structure M , a current state s 2 S , and a variable assignment function .
The probability terms are given the following interpretation:   1.
(]~x)(Msfi) =  nfi f~as:t:(M s ~x=~a]) j= g , where ~x=~a] is the variable assignment function identical to  except that (xi ) = ai , and  nfi is the n-fold product measure formed from  fii where i = O or O@T or T depending on whether xi is an object type variable, an object instance variable, or a temporal variable.
 ; 2.
(prob())(Msfi) =  S fs0 s:t:(M s0  ) j= g .
So we see that ]~x denotes the measure of the set of satisfying instantiations of ~x in  and prob() denotes the measure of the set of states that satisfy .
Unicorns have a single horn: 8o: unicorn(o) !
singleHorn(o).
Unicorns have never, do not, and will never exist: 8o t: t < now & unicorn(o) !
:exists(o@t), 8o: unicorn(o) !
:exists(o@now), 8o t: t > now & unicorn(o) !
:exists(o@t).
or, more simply: 8o t: unicorn(o) !
:exists(o@t): Most birds y: fly(o)jbird(o)]o > 0:5.
Most birds y now: fly(o@now)jbird(o@now)]o > 0:5.
At any time, most birds y: 8t: fly(o@t)jbird(o@t)]o > 0:5.
For most object instances, if the object type is a bird at the time then it ies at the time: fly(o@t)jbird(o@t)]o@t > 0:5.
Most of the time, most birds y: fly(o@t)jbird(o@t)]o > 0:5]t > 0:5.
Informally, the above expression says that if we pick a time at random, chances are that more than 50% of the birds y at that time.
In addition to statistical assertions, we can also represent inductive probability assertions (which correspond to an agent's beliefs).
For example,  Halpern has called such structures type III probability structures.
4 We ignore the technicalities of dealing with overloading and argument/result type conversions.
The degree of belief in the proposition \Tweety is ying now" is 0.9: prob(fly(tweety @now)) = 0:9:  0  3  In addition to the statistical and inductive probabilities, we need an extension that allows us to represent epistemic expectation.
Specically, if p is a statistical probability term, then E(p) is a new numeric term whose denotation is dened as follows: X (E(p))(Msfi) =  S (s0 ) fi p(Ms fi) : 0  s 2S 0  That is, the expected value of a term is the weighted (by  S ) average of its denotation over the set of states.
2.4 Representation in PL(T ) PL(T ) allows for the representation of a rich variety of statistical and probabilistic temporal information.
Because time is associated with object instances rather than with properties of objects, we can describe objects that come into existence or cease to exist.
We can also talk about properties of object types that have no instances, such as unicorns.
The following examples gives some idea of the expressive power of the language.
The degree of belief in the proposition \Most birds y" is 0.75:   prob fly(o)jbird(o)]o > 0:5 = 0:75: Two remarks are in order here.
First, although PL(T ) supports the representation of beliefs about temporal assertions, there is no support for temporal beliefs, i.e., only the current set of beliefs is representable.
This shortcoming while be addressed in future work.
Second, some form of direct inference is needed to connect the inductive probabilities to the statistical ones as was discussed in section 2.1.
We are now in a position to provide this connection.
3 Inferences in PL(T )  The choice of the distributions  O ,  S , and  T affect inferences in PL(T ).
Choosing a \uniform" distribution for  O ,  S , and  T corresponds to Carnap's Method I.
In the case of  T , we can not have a true uniform distribution since T is innite, so we take  T (T ) = jT j/jTnj, where Tn = f0 ;1 1 ;2 2 :: : ;1n;1 fibn=2cg, and then we consider the situation in the limit as n !
1.
For any nite set of times T , the measure is 0 so we must amend the interpretation of conditional statistical probabilites.
With j ]~x =df limTn !T  ^  ]~x= ]~x, what matters is the relative sizes of the sets of times involved in the numerator and denominator.
We can also choose distributions which result in inferences corresponding to Carnap's Method II.
To do this, the distributions  O and  T are taken as above, but to dene the distribution  S , we need to introduce the concept of structures which are equivalence classes of states.
Two states, s1 and s2 are considered isomorphic if replacing the individuals of s1 with one of their permutations results in s2 .
Let S be the set of structures corresponding to the set of states S .
We can now dene the distribution  S in such a way that every subset S 0 of S which is a member of S has the same measure and every member of S 0 has the same measure.
This results in inferences corresponding to Carnap's Method II.
We examine both Method I and II inferences in section 3.1.
Then in sections 3.2 and 3.3, we discuss interpolation and extrapolation.
Finally, we consider temporal belief revision issues in section 3.4.
1  3.1 Direct inference  We can connect inductive and statistical probabilities in a similar manner as Bacchus did in 1].
We start by assuming that an agent expresses assertions about his environment in a xed statistical language Lstat.
Assertions in Lstat, which are all the assertions of PL(T ) excluding those involving inductive probability, are called objective assertions.
The agent's degree of belief in the objective assertions are represented in another language Lcomb which extends Lstat with the inductive probability operator prob and an  royal elephant(clyde) & elephant(clyde) 8x:royal elephant(x) !
elephant(x) gray(x)jelephant(x)]x > 0:5 :gray(x)jroyal elephant(x)]x > 0:5:  Figure 2: Redundant Information epistemic expectation operator E. Formulas of Lcomb that are also in Lstat are called objective formulas.
The knowledge base KB is the complete nite collection of objective formulas   which are fully believed by the agent i.e., prob KB = 1.
De	nition 1 (Randomization 1]) stat  Let  be a formula of L .
If hc1 : : : cni are the n distinct object constants5 that appear in  ^ KB and hv1  : : : vni are n distinct object variables that do not occur in  ^ KB, then let KBv (v ) denote the new formula which results from textually substituting ci by vi in KB (), for all i.
(KBv is referred to as the randomization of KB or KB randomized.)
De	nition 2 (Direct Inference Principle 1] ) If the agent fully believes that KBv ]~v > 0 and if  is a formula of Lstat then the agent's degree of belief in  should be determined by the equality prob() = E(v jKBv ]~v ): Method I inferences: If the distributions are chosen for Method I as described in section 3, inferences in PL(T ) have similar properties to those described in 6], e.g., desirable inheritance properties.
For example, in Figure 2, PL(T ) infers that prob(:gray(clyde)) > 0:5.
That is, we have inheritance with specicity in spite of the redundant information elephant(clyde).
This method supports a number of desirable inferences such as those involving simple inheritance, multiple inheritance with specicity, ambiguity, cascaded ambiguity, cycles, redundant information, and negative paths (see 6]).
Such a system might be sucient for most needs.
It even includes the ability to revise beliefs about individuals, i.e., inheritance of properties is aected by receiving more specic information about an individual.
Furthermore, the inclusion of additional statistical assertions may aect properties inherited to individuals.
What is lacking, however, is an ability to revise beliefs in statistical formulas given individual observations.
This can be addressed by Method II.
Method II inferences: If the distributions are chosen for Method II as described in section 3, inferences in PL(T ) have in addition to the desirFor our purposes, these refer to object types, object instances, and/or times.
5  able inheritance properties described in 6], the ability to dynamically account for observations in beliefs about statistical assertions.
For example, in Figure 3, if O contains only ve object types, s1, s2, s3, s4, and s5, then, as reported in 7], initially prob(fly(s5)jsparrow(s5)) = 0:6 (see Figure 3).
The table in Figure 3 was computed by the method of exhaustive enumeration as described in 7].
In the table, probI means prob with the distributions set for Method I probII means prob with the distributions set for Method II.
Upon learning that s1 is a ying sparrow, prob(fly(s5)jsparrow(s5)) = 0:5714 under Method II as compared to 0.5 under Method I.
Comparing this to prob(fly(s5)jbird(s5)) which is 0.5, we see that in spite the the observed ying sparrow, Method I sticks to straight inheritance of the ying birds statistic to sparrows, whereas Method II adjusts to the observation and infers sparrows are even more likely to y than birds.
So far, the examples in this section have not involved time.
In sections 3.2 and 3.3, we examine the temporal inferences we call interpolation and extrapolation.
3.2 Interpolation  Suppose we have the following situation: Over the year, it rains 40% of the time.
During winter (December 21{March 20), it rains 75% of the time.
Over the summer (June 21{September 20), it rains 20% of the time.
What percentage of rainfall occurs during December?
What is the chance of rain on December 24th?
We can represent this in PL(T ) by letting the integers 1 through 365 represent the days (i.e., each day is a time point) and provide axioms such as shown in Figure 5.
Inferences about the rainfall in December or on December 24th based on the given statistical information are in a class of inferences we call interpolation.
These inferences involve using interval statistics to induce subinterval statistics or point probabilities.
For instance, the actual percentage of rainfall in December is: P3 = P rain(t)jr3(t)]t rain(t) = R3jR3 j (t)jr3b(t)]tjR3b j = 	rain(t)jr3a(t)]tjjRR33aajj++	jRrain 3b j where the value of the numerator is unknown.
(Note in the summation, we are treating rain as if it were a 0-1 function with value 1 at t if there is rain at time t and value 0 at t otherwise.)
To compute the amount of rainfall in December we divide the month (region R3 from Figure 5) into subregions R3a = dec1 dec20] and R3b = dec21 dec31]: The specic information about R3a is obtained from R5 where R5 = mar21 jun20] + sep21 dec20] = R1 ; R2 ; R4.
rain(t)jr1(t)]t = 0:4, rain(t)jr2(t)]t = 0:75, rain(t)jr4(t)]t = 0:2, % plus axioms dening the % regions r1, r2, r3, r4 R3 : ??
R2 : 75%  Dec1 Dec21 Dec31 Jan1  Mar20  R1 : 40% R4 : 20% Jun21  Sep20  Figure 5: Rainfall Interpolation The statistic for R5 can be computed from the statistics for R1, R2, and R4, and from the relative sizes of these intervals.
We compute the actual percentage of rain P5 over R5 to be approximately 33% (see Figure 4).
By assuming every subset of R5 has the same expected percentage of rain (i.e., using Method I), we conclude the expected percentage of rain over R3a is P5 : The most specic reference class (for which we have or can compute the actual percentage of rainfall) that contains R3b is R2.
By assuming every subset of R2 has the same expected percentage of rain (Method I), we conclude the expected percentage of rain over R3b is 75%.
The expected percentage of rain over R3 equals a weighted average based on R3a and R3b:   E(P3) = E rain(t)jr3(t)]t :75jR3bj = P5 jR3a jj+0 R3 j :7511  0:3320+0 31  48%.
The answer to the original question is that it rains roughly half the time during December.
3.3 Extrapolation  Persistence (the frame problem) has been viewed in two ways: 1) action-invariance of a property: whether a property that is true before an action or event will remain true afterwards, cf.
temporal projection 9] or, 2) time-invariance of a property: if a property is true at some point in time, how long is it likely to remain true 5].
Under these views, a property such as raining at a given point in time is highly action-invariant (few actions aect rain) and slightly time-invariant (it rains for a while and then stops).
Here we consider a previously unexplored aspect of the frame problem: action and time invariance of statistical knowledge.
Given statistical information about various time  A Statistical KB fly(x)jbird(x)]x = 0:6, 8x:sparrow(x) !
bird(x).
Method I and II Inferences Known ying sparrows => none probI (fly(s5)jbird(s5)): 0.6 probII (fly(s5)jbird(s5)): 0.6 probI (fly(s5)jsparrow(s5)): 0.6 probII (fly(s5)jsparrow(s5)): 0.6  s1 0.5 0.5 0.5 0.5714  s1, s2 0.3333 0.3333 0.3333 0.4286  Figure 3: Belief Revision  P5 = rain(t)jr5(t)]t = 	rain(t)jr1(t)]tjR1 j;	rain(t)jjrR2(5tj)]tjR2 j;	rain(t)jr4(t)]tjR4 j ;0:75	79+11];0:292  33%.
= 0:4jR1j;0:75jRj5 jR2j;0:2jR4j = 0:4365365 ;	79+11];92 Figure 4: Calculation of P5.
intervals, we wish to make reasonable inferences about past or future intervals.
For example, Figure 6 depicts a situation where we know that it rained 75% of the time in the winter, and 20% of the time during the summer.
We have no statistical information about the coming year (R6: December 1 to November 30) so the interpolation technique in the previous section is not applicable.
The temporal projection technique of Hanks and McDermott 9] is also inappropriate.
We cannot determine from the statistical information whether it was raining on September 20.
Even if we knew it was raining at that time, it does not make sense to allow raining to persist indenitely.
We have no information about actions or events that may aect raining.
Finally, Dean and Kanazawa's 5] probabilistic temporal projection cannot be used as it requires the construction of a survivor function for raining based on many observations of raining changing from true to false.
In our example, we have no observations of raining at particular points.
We only have interval statistics.
Instead of considering persistence at the level of individual time points, we can view it at the interval level and describe the persistence of statistical information.
If we take the observed statistics to be samples of raining over time (i.e., over the whole time line), we can base our inferences for other intervals on these samples.
For instance, we can infer a statistic for R6 in Figure 6 using R2 and R4 as follows:   E rain(t)jr6(t)]t rain(t)jr4(t)]tjR4 j = 	rain(t)jr2(t)]tjjRR22 jj+	 +jR4 j :292 = 0:7590+0 182  47%.
This result corresponds to that obtained by both Method I and II.
Space considerations force us to omit a detailed discussion of the precise mechanics of interpolation and extrapolation inferencing in PL(T ), but we have provided enough detail to highlight relevant issues.
As well, our discussion of interpolation and extrapo-  lation, so far, has not touched on belief revision issues.
We turn to consideration of this next.
3.4 Temporal belief revision  In the preceeding two subsections, we have described two forms of inferencing in PL(T ).
For a xed temporal knowledge base which includes only interval level statistics (such as in the examples of Figures 5 and 6), the results for Method I and Method II are the same.
The situation becomes more interesting when the knowledge base is updated with new statistics and point information.
There are three important cases to consider: 1) new interval statistics 2) new point information aecting the relevancy of interval statistics and 3) new point information aecting the predicted value of interval statistics.
New interval statistics: Suppose in the example of Figure 6, as time passed, we came to observe the rainfall in December of the coming year (R7, a subinterval of R6) and found it to be 60%.
Prior to learning this, we had predicted the rainfall for the coming year to be about 47%.
The newly acquired interval statistic for December should cause us to revise our prediction for the coming year.
Under both Method I and II, this is indeed the case.
Referring to December of the coming year as region R7, the result under either method would be approximately 50% (see Figure 7).
New point information (relevance): Suppose in the example of Figure 5, we wanted to predict the chances of rain on the day of a party to be held in December (R3).
Since we do not know the exact day, the prediction about rain on the day of the party given the day will be in December is based on the inferred statistic for R3, i.e., prob(rain(party day)) is about 48% (cf.
the example of Figure 4).
As time passes, we come to learn the party will be held on December 24.
This (point level) information should cause us to base our prediction of rain on the statistic for R3b (which is derived from R2) which is more relevant than the inferred statistic for R3 given that the  R2 : 75%  R7 : 60% R4 : 20%  Dec21  Mar20 Jun21  Sep20  Dec1  R6 : ?
?%  Jan1  Figure 6: Rainfall Extrapolation  Nov30      E rain(t)jr6(t)]t = E 	rain(t)jr7(t)]tjR7 j+	rainjR(t6)jjr6(t) & :r7(t)]t jR6 ;R7 j     = E 	rain(t)jjrR7(6tj)]tjR7 j  + E 	rain(t)jr6(t) &jR:6rj7(t)]t jR6 ;R7 j  	 rain ( t ) j r 2( t )] j R t 2 j+	rain(t)jr 4(t)]t jR4 j+	rain(t)jr 7(t)]t jR7 j jR6 ;R7 j   = 	rain(t)jjrR7(6tj)]tjR7 j + jR2 j+jR4 j+jR7 j jR6 j = 0:631 + (0:7590+0:292+0:631)334  50%.
365  213365  Figure 7: Calculation of next year's expected rainfall.
day of the party is in R3b.
Again, this is indeed the case in both Method I and II, and the revised belief becomes: prob(rain(party day)) is 75%.
New point information (value): So far, there is has been no reason to choose between Method I and II.
A dierence arises, however, as we incorporate point level observations that aect the predicted value of interval statistics.
To see this, again consider the example from the previous paragraph about rain on the day of the party.
Suppose we observe the rain on certain days in December (but not the day of the party).
Let us suppose that, although we have made these observations, we have not come to learn the party is not on one of those days.
(This could happen, say, if a friend was telling us about the party and we had independently observed the weather.)
Now suppose each of the days we observed was a rainy day.
This should cause us to revise our belief in rain on the party day, i.e., we should increase our belief in rain on the party day.
Method I does not do this.
It stubbornly holds to the belief prob(rain(party day)) is about 48% based on an unchanged R3 (inferred) statistic.
Method II, however, increases the predicted value of the R3 statistic and hence increases the value of prob(rain(party day)).
4 Conclusion  We have described the discrete temporal probabilitylogic we call PL(T ) which is expressive enough to represent and reason with a rich variety of problems.
Underlying the probability-logic is a choice of distributions over objects, states, and times.
Dierent choices correspond two dierent inductive methods.
We have focused on two methods described by Carnap.
For most purposes, either method seems adequate, but we found there are cases in the context of belief revision where Method II is superior.
This is  particularly true when new point level observations are made which aect the value of predicted interval statistics.
References  1] F. Bacchus.
Representing and Reasoning with Probabilistic Knowledge.
MIT Press, Cambridge, Massachusetts, 1990.
2] F. Bacchus and S.D.
Goodwin.
Using statistical information in planning.
unpublished extended abstract], May 1991.
3] R. Carnap.
Logical Foundations of Probability Theory.
University of Chicago Press, Chicago, Illinois, 1950.
4] R. Carnap.
Statistical and inductive probability.
In Readings in the Philosophy of Science.
Prentice-Hall, 1989.
5] T. Dean and K. Kanazawa.
Probabilistic temporal reasoning.
In Proceedings of the Seventh National Conference on Articial Intelligence, pages 524{528, St. Paul, Minnesota, August 1988.
6] S.D.
Goodwin.
Second order direct inference: A reference class selection policy.
International Journal of Expert Systems: Research and Applications, 5(3):1{26, 1992.
7] S.D.
Goodwin and H.J.
Hamilton.
An inheritance mechanism for default reasoning that learns.
In International Symposium on Articial Intelligence, pages 234{239.
Monterrey, Mexico, 1993.
8] J. Halpern.
An analysis of rst-order logics of probability.
In Proceedings of the Eleventh International Joint Conference on Articial Intelligence, pages 1375{1381, August 1989.
9] S. Hanks and D.V.
McDermott.
Nonmonotonic logic and temporal projection.
Articial Intelligence, 33(3):379{412, November 1987.
(c) 2005 IEEE.
Personal use of this material is permitted.
However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from the IEEE.
Probabilistic Calculation of Execution Intervals for Workflows Johann Eder and Horst Pichler Institute for Informatics-Systems, University of Klagenfurt, Austria [eder|pichler]@uni-klu.ac.at  Abstract The comprehensive treatment of time and time constraints is crucial in designing and managing business processes.
Process managers need tools that help them predict execution durations, anticipate time problems, pro-actively avoid time constraint violations, and make decisions about the relative process priorities and timing constraints when significant or unexpected delays occur.
Variations of activity durations and branching decisions at run-time make it necessary that we treat time management in workflows in a probabilistic way.
Therefore we introduce the notion of probabilistic time management and discuss the application of this new concepts for workflow design as well as time aware, predictive and proactive workflow execution management.
1 Introduction Systems for business process automation, like workflow management or enterprise resource planning systems, are used to improve processes by automating tasks and getting the right information to the right place for a specific job function.
As automated business processes often span several enterprises, the most critical need in companies striving to become more competitive is a high quality of service, where the expected process execution time ranks among the most important quality measures [7].
Additionally it is a necessity to control the flow of information and work in a timely manner by using time-related restrictions, such as bounded execution durations and absolute deadlines, which are often associated with process activities and sub-processes [3].
However, arbitrary time restrictions and unexpected delays can lead to time vi-  olations, which typically increase the execution time and cost of business processes because they require some type of exception handling [9].
Although currently available commercial products offer sophisticated modelling tools for specifying and analyzing workflow processes, their time management functionality is still rudimentary and mostly restricted to monitoring of constraint violations and simulation for process reengineering purposes [1, 6].
In research several attempts have been made to provide solutions to advanced time management problems (e.g.
[1, 2, 6, 8]).
Most of them suffer from the vagueness of information which stems mainly from two aspects: The duration of a task can vary greatly without any possibility of the workflow system to know beforehand.
The second is that in a workflow different paths may be chosen with decisions taking place during the execution.
The main motivations for our probabilistic approach are: a) the improvement of estimations about the (remaining) duration of a workflow (predictive time management), and b) to make forecasts for the likelihood of deadline misses and automatically trigger escalation-avoiding actions if a possible future deadline violation has been detected (proactive time management).
Our calculation algorithms utilize the knowledge about the control flow of a workflow and stochastic information about the uncertainties mentioned above.
2 Timed Workflow Graph A workflow can be defined as directed acyclic graph, which is a collection of nodes and edges between nodes.
Edges determine the execution sequence of nodes, thus a successor can start if its predecessor(s)  Proceedings of the 12th International Symposium on Temporal Representation and Reasoning (TIME'05) 1530-1311/05 $20.00 (c) 2005 IEEE  Start  4 A  4 B  End  3 C  deadline=13  A  deadline  B.st B.d  A.d  B.d 0  4  1  2  3  4 B.eps  5  6 7 B.las  8 9 B.epe  10 11 B.lae  Start  C.d time 12  p = 0.3 AS  p = 0.7  9 B  1 C  p = 0.3 AJ  p = 0.7  2 D End  Figure 2.
Workflow with or-structure  13  Figure 1.
Implicit Time Properties  3 Probabilistic Timed Workflow Graph are finished.
A node can be of type activity, which corresponds to individual tasks of a business process, or a control node (e.g.
start, and-split, and-join, etc.).
Additionally time properties can be attached to each node.
Time properties are either explicit, if defined by the workflow designer, or implicit, if they follow implicitly from the workflows structure and explicit time properties [6].
We use a linear time model, where time is discrete with a universal predefined chronon, called basic time unit.
The time line starts at 0, which denotes the start time of the workflow.
All other points in time are declared or calculated relative to this start time.
Figure 1 visualizes a workflow consisting of three activities executed in sequence.
Explicit time properties are the estimated duration of activities in basic time units, which are A.d = 4, B.d = 4 and C.d = 3 and a deadline of d = 13, stating that the overall workflow execution must not exceed 13 time units.
Based on this information four implicit time properties can be calculated for each node (e.g.
for activity B): a) Considering the sum of durations of preceding activities the earliest possible start of activity B is B.eps = 4. b) The according earliest possible end is B.epe = B.eps + B.d = 8. c) To take the deadline of 13, into account, the point of view has to be reversed, now starting from the end of the workflow.
By subtracting the durations of succeeding activities from the deadline, the latest allowed end B.lae of activity B is determined as B.lae = 13 - 3 = 10.
That means if B ends at 10 it is still possible to reach the overall deadline of 13. d) Analogously the latest allowed start time is B.las = B.lae - B.d = 6.
This information can be utilized to specify a valid time interval for the execution of each activity, which ensures no time violations.
E.g.
we can state that B can not start before 4 and must end until 10 in order to hold the deadline.
Additionally we can state that the expected duration of the workflow is equal to the sum of all activity durations, which is equal to C.epe = 11.
The example presented above is rather simplistic, as some essential problems have not been addressed: a) The expected execution duration of a node is represented as a single average value (without variance, which is unusable for administrative workflows with human participants), b) in and-structures (parallel execution of nodes) the longest of all concurrently executed routes must be considered, and c) in or-structures (conditional execution of several alternative nodes) different paths may be chosen with decisions taking place during the execution, whose outcome we can not know when modelling the workflow.
Therefore, it is impossible to unambiguously determine implicit time constraints or the overall workflow duration, especially if the workflow-graph contains complex control structures.
To tackle these problems we introduced the probabilistic timed graph, which is an extension of the above presented basic model augmented with branching probabilities and time histograms.
Consider the graph in Fig.
2: The or-split after A forces a decision during the execution of the workflow.
One of two possible routes will be chosen, therefore it is impossible to calculate one scalar value for the earliest possible start time of D. According to the branching probabilities (expert estimations or extracted from the log) D will start at 13 with a probability of 30% or at 5 with a probability of 70%.
The same can analogously be stated for the latest allowed end time of A (as latest allowed times are calculated in a reversed fashion starting from the deadline defined on the last activity).
Additionally a workflow modeler will rather use distribution functions to represent activity durations than single scalar values.
Therefore we introduced the concept of a time histogram, which is defined as a set of tuples (p, t) where p is the probability and t is the according time value.
Time histograms are used to represent time properties in the form of probability distributions.
A graph where 2  Proceedings of the 12th International Symposium on Temporal Representation and Reasoning (TIME'05) 1530-1311/05 $20.00 (c) 2005 IEEE  1.0  without violating the overall deadline.
This switches the status to red (for further details we refer to [5]).
According to the new state different escalation actions can be invoked, e.g.
for orange this could be skipping unnecessary (optional) tasks.
At status red an early escalation of this workflow might be invoked which aims at avoiding useless resource consumption of future activities (see also [9]).
The important contribution of this approach is that threshold values can be expressed as the probability of a deadline violation.
green orange  0.5  T.lae  red  5  10  15  now  Figure 3.
Traffic light model for activity T  5 Conclusion each time property is represented by a time histogram is called Probabilistic Timed Workflow Graph.
Details about time histograms, how to cumulate, interpret and query them and how to calculate the probabilistic timed model, considering different types of control nodes, is explained in [5].
How to cope with large histograms by compressing them and how to deal with blocked loop structures is explained in [4].
Probabilistic time management will produce major advantages like better scheduling decisions and improved escalation strategies for workflow execution, as well as it provides the means to implement quality insurance components based on probabilistic time properties.
The integration of probabilistic time management applications into workflow environments are subject of ongoing research.
4 Application areas  References [1] C. Combi and G. Pozzi.
Temporal conceptual modelling of workflows.
LNCS 2813.
Springer, 2003.
[2] P. Dadam and M. Reichert.
The adept wfms project at the university of ulm.
In Proc.
of the 1st European Workshop on Workflow and Process Management (WPM'98).
Swiss Federal Institute of Technology (ETH), 1998.
[3] J. Eder and E. Panagos.
Managing Time in Workflow Systems.
Workflow Handbook 2001.
Future Strategies Inc. Publ.
in association with Workflow Management Coalition (WfMC), 2001.
[4] J. Eder and H. Pichler.
Duration Histograms for Workflow Systems.
In Proc.
of the Conf.
on Engineering Information Systems in the Internet Context 2002, Kluwer Academic Publishers, 2002.
[5] J. Eder and H. Pichler.
Probabilistic Workflow Management.
Technical report, Universitat Klagenfurt, Institut fur Informatik Systeme, 2005.
[6] J. Eder, E. Panagos, and M. Rabinovich.
Time constraints in workflow systems.
LNCS 1626.
Springer, 1999.
[7] M. Gillmann, G. Weikum, and W. Wonner.
Workflow management with service quality guarantees.
In Proc.
of the 2002 ACM SIGMOD Int.
Conf.
on Management of Data.
ACM Press, 2002.
[8] O. Marjanovic and M. Orlowska.
On modeling and verification of temporal constraints in production workflows.
Knowledge and Information Systems, 1(2), 1999.
[9] E. Panagos and M. Rabinovich.
Predictive workflow man-  We differ between predictive and proactive time management applications.
Predictive time management is used to provide users (customers) with predictions about a expected execution durations or the likelihood of coming activity assignments (scheduling forecasts).
Proactive time management tries to asses the current situation, corresponding to possible future time violations, e.g.
deadline misses.
Time histograms are the basis for simple but effective escalation warning mechanisms, using an adaption of the traffic light model introduced in [3]: Two thresholds must be defined on the histogram representing the latest allowed end time of an activity.
The first determines the workflows state change from green (ok) to yellow (warn) and the second one determines the state change from yellow to red (alarm).
As long as the workflows state is green everything is ok, if the state changes something has to happen.
Example: Assume that activity T just finished, at a point in time denoted by now = 12.
Figure 3 shows the descending cumulated time histogram for T.lae with two thresholds defined at 90% and 50%.
Applying a selection-operation on the histogram yields a probability of 30% that the workflow can be finished  agement.
In Proc.
of the 3rd Int.
Workshop on Next Generation Information Technologies and Systems, Neve Ilan, ISRAEL, 1997.
3 Proceedings of the 12th International Symposium on Temporal Representation and Reasoning (TIME'05) 1530-1311/05 $20.00 (c) 2005 IEEE

L OLA: Runtime Monitoring of Synchronous Systems Ben DaAngelo a Sriram Sankaranarayanan Bernd Finkbeiner a  Henny B. Sipma a  a  CeEsar SaEnchez a Sandeep Mehrotra aA  Will Robinson Zohar Manna  a a  a  Computer Science Department, Stanford University, Stanford, CA 94305 {bdangelo,srirams,cesar,sipma,manna}@theory.stanford.edu a   Department of Computer Science, Saarland University finkbeiner@cs.uni-sb.de  Abstracta We present a specification language and algorithms for the online and offline monitoring of synchronous systems including circuits and embedded systems.
Such monitoring is useful not only for testing, but also under actual deployment.
The specification language is simple and expressive; it can describe both correctness/failure assertions along with interesting statistical measures that are useful for system profiling and coverage analysis.
The algorithm for online monitoring of queries in this language follows a partial evaluation strategy: it incrementally constructs output streams from input streams, while maintaining a store of partially evaluated expressions for forward references.
We identify a class of specifications, characterized syntactically, for which the algorithmas memory requirement is independent of the length of the input streams.
Being able to bound memory requirements is especially important in online monitoring of large input streams.
We extend the concepts used in the online algorithm to construct an efficient offline monitoring algorithm for large traces.
We have implemented our algorithm and applied it to two industrial systems, the PCI bus protocol and a memory controller.
The results demonstrate that our algorithms are practical and that our specification language is sufficiently expressive to handle specifications of interest to industry.
I. I NTRODUCTION Monitoring synchronous programs for safety and liveness properties is an important aspect of ensuring their proper runtime behavior.
An offline monitor analyzes traces of a system post-simulation to spot violations of This research was supported in part by NSF grants CCR-0121403, CCR-02-20134, CCR-02-09237, CNS-0411363, and CCF0430102, by ARO grant DAAD19-01-1-0723, by NAVY/ONR contract N00014-03-1-0939, by the Siebel Graduate Fellowship, and by the BMBF grant 01 IS C38 B as part of the Verisoft project.
aA  Synopsys, Inc.  the specification.
Offline monitoring is critical for testing large systems before deployment.
An online monitor processes the system trace while it is being generated.
Online monitoring is used to detect violations of the specification when the system is in operation so that they can be handled before they translate into observable and cascading failures, and to adaptively optimize system performance.
Runtime monitoring has received growing attention in recent years [1], [2], [3].
While static verification intends to show that every (infinite) run of a system satisfies the specification, runtime monitoring is concerned only with a single (finite) trace.
Runtime monitoring can be viewed as an extension of testing with more powerful specification languages.
The offline monitoring problem is known to be easy for purely past or purely future properties.
It is well known that for past properties, the online monitoring problem can be solved efficiently using constant space and linear time in the trace size.
For future properties, on the other hand, the space requirement generally depends on the length of the trace, which suggests that online monitoring may quickly become intractable in practical applications with traces exceeding 106 simulation steps.
In this paper, we present a specification language, intended for industrial use.
The language can express properties involving both the past and the future.
It is a functional stream computation language like L USTRE [4] and E STEREL [5], with features that are relevant to our problem at hand.
It is parsimonious in its number of operators (expressions are constructed from three basic operators), but the resulting expressiveness surpasses temporal logics and many other existing formalisms  including finite-state automata.
We provide a syntactic characterization of efficiently monitorable specifications, for which the space requirement of the online monitoring algorithm is independent of the size of the trace, and linear in the specification size.
An analysis of some industrial specifications provided by Synopsys, Inc. showed that a large majority of these specifications lie in this efficiently monitorable class.
For the offline monitoring problem, we demonstrate an efficient monitoring strategy in the presence of mixed past/future properties.
We have implemented our algorithm and specification language in a system called L OLA.
L OLA accepts a specification in the form of a set of stream expressions, and is then run on a set of input streams.
Two types of specifications are supported: properties that specify correct behavior, and properties that specify statistical measures that allow profiling the system that produces the input streams.
An execution of L OLA computes arithmetic and logical expressions over the finite input and intermediate streams to produce an output consisting of error reports and the desired statistical information.
A.
Related Work Much of the initial work on runtime monitoring (cf.
[6], [7], [8]) was based on temporal logic [9].
In [10], non-deterministic automata are built from LTL to check violations of formulas over finite traces and the complexity of these problems is studied.
LTL based specifications have already been pursued in tools such as the Temporal Rover [7] and Java PathExplorer [11].
One limitation of this approach is that the logic must be adapted to handle truncated traces.
The approach taken in [12] considers extensions of LTL for the case of truncated paths with different interpretations (weak and strong) of the next operator at the end of the trace.
The choice of handling success/failure on a finite trace frequently depends on the situation being modeled.
Another important difference between runtime verification and static verification is that liveness properties can never be violated on a finite trace.
An appealing solution is to extend the specification language to compute quantitative measures based on the trace.
Temporal properties can be specified in L OLA, but one of the main goals is to go beyond property checking to the collection of numerical statistics.
For example, instead of checking the property athere are only finitely many retransmissions of each package,a which is vacuously true over finite traces, we desire to evaluate queries like awhat is the average number of retransmissions.a Our  first approach to combine the property proving with data collection appeared in [13].
Following this trend, runtime verifiers can be used not only for bug-finding, but also for profiling, coverage, vacuity and numerous other analyses.
L OLA models runtime verification as a stream computation.
The definition of L OLA output streams in terms of other streams resembles synchronous programming languages (notably L USTRE [4], E STEREL [5], Signal [14]), but there is a significant difference: these languages are designed primarily for the construction of synchronous systems.
Therefore, output values for a time instant are computed directly from values at the same and previous instants.
This assumption makes perfect sense if we desire that the systems we specify be executable, and therefore be causal.
However, runtime specifications are descriptive in nature.
They include future formulas whose evaluation may have to be delayed until future values arrive.
This requires stronger expressiveness in the language and the corresponding evaluation strategies.
Other efforts in run-time verification include [15], which studies the efficient generation of monitors from specifications written as extended regular expressions, and [16], which studies rewriting techniques for the efficient evaluation of LTL formulas on finite execution traces, both online and offline.
In [8], an efficient method for the online evaluation of past LTL properties is presented.
This method exploits that past LTL can be recursively defined using only values in the previous state of the computation.
Our efficiently monitorable specifications generalize this idea, and apply it uniformly to both verification and data collection.
The system that most closely resembles L OLA is Eagle [17].
Eagle allows the description of monitors based on greatest and least fixed points of recursive definitions.
Many logical formalisms used to describe properties, including past and future LTL formulas, can be translated to Eagle specifications.
These are then compiled into a set of rules that implements the monitor.
L OLA differs from Eagle in the descriptive nature of the language, and in that L OLA is not restricted to checking logical formulas, but can also express numerical queries.
II.
L OLA OVERVIEW In this section we describe the specification language.
The monitoring algorithms will be presented in Section III.
A. Specification Language: Syntax A L OLA specification describes the computation of output streams from a given set of input streams.
A  stream D of type T is a finite sequence of values from T .
We let D(i), i aL 0 denote the value of the stream at time step i.
Definition 1 (L OLA specification) A L OLA specification is a set of equations over typed stream variables, of the form s1 = e1 (t1 , .
.
.
, tm , s1 , .
.
.
, sn ) .. .. .
.
sn = en (t1 , .
.
.
, tm , s1 , .
.
.
, sn ),  where s1 , .
.
.
, sn are called the dependent variables and t1 , .
.
.
, tm are called the independent variables, and e1 , .
.
.
, en are stream expressions over s1 , .
.
.
, sn and t1 , .
.
.
, tm .
Independent variables refer to input streams and dependent variables refer to output streams a .
A L OLA specification can also declare certain output boolean variables as triggers.
Triggers generate notifications at instants when their corresponding values become true .
Triggers are specified in L OLA as trigger D  where D is a boolan expression over streams.
A stream expression is constructed as follows: aV If c is a constant of type T , then c is an atomic stream expression of type T ; aV If s is a stream variable of type T , then s is an atomic stream expression of type T ; aV Let f : T1 AT2 AAV AV AVATk 7a T be a k -ary operator.
If for 1 a$?
i a$?
k , ei is an expression of type Ti , then f (e1 , .
.
.
, ek ) is a stream expression of type T ; aV If b is a boolean stream expression and e1 , e2 are stream expressions of type T , then ite(b, e1 , e2 ) is a stream expression of type T ; note that ite abbreviates if-then-else.
aV If e is a stream expression of type T , c is a constant of type T , and i is an integer, then e[i, c] is a stream expression of type T .
Informally, e[i, c] refers to the value of the expression e offset i positions from the current position.
The constant c indicates the default value to be provided, in case an offset of i takes us past the end or before the beginning of the stream.
a In our implementation we partition the dependent variables into output variables and intermediate variables to distinguish streams that are of interest to the user and those that are used only to facilitate the computation of other streams.
However, for the description of the semantics and the algorithm this distinction is not important, and hence we will ignore it in this paper.
Example 1 Let t1 , t2 be stream variables of type boolean and t3 be a stream variable of type integer.
The following is an example of a L OLA specification with t1 , t2 and t3 as independent variables: s1 s2 s3 s4 s5 s6 s7 s8 s9 s10  = = = = = = = = = =  true t3 t1 a" (t3 a$?
1) ((t3 )2 + 7) mod 15 ite(s3 , s4 , s4 + 1) ite(t1 , t3 a$?
s4 , AZs3 ) t1 [+1, false] t1 [a1, true] s9 [a1, 0] + (t3 mod 2) t2 a" (t1 aSS s10 [1, true])  Stream variable s1 denotes a stream whose value is true at all positions, while s2 denotes a stream whose values are the same at all positions as those in t3 .
The values of the streams corresponding to s3 , .
.
.
, s6 are obtained by evaluating their defining expressions placewise at each position.
The stream corresponding to s7 is obtained by taking at each position i the value of the stream corresponding to t1 at position i + 1, except at the last position, which assumes the default value false.
Similarly for the stream for s8 , whose values are equal to the values of the stream for t1 shifted by one position, except that the value at the first position is the default value true.
The stream specified by s9 counts the number of odd entries in the stream assigned to t3 by accumulating (t3 mod 2).
Finally, s10 denotes the stream that gives at each position the value of the temporal formula t1 Until t2 with the stipulation that unresolved eventualities be regarded as satisfied at the end of the trace.
B. Specification Language: Semantics The semantics of L OLA specifications is defined in terms of evaluation models, which describe the relation between input streams and output streams.
Definition 2 (Evaluation Models) Let D be a L OLA specification over independent variables t1 , .
.
.
, tm with types T1 , .
.
.
, Tm , and dependent variables s1 , .
.
.
, sn with types Tm+1 , .
.
.
, Tm+n .
Let D1 , .
.
.
, Dm be streams of length N +1, with Di of type Ti .
The tuple hD1 , .
.
.
, Dn i of streams of length N + 1 with appropriate types is called an evaluation model, if for each equation in D si = ei (t1 , .
.
.
, tm , s1 , .
.
.
, sn ), hD1 , .
.
.
, Dn i satisfies the following associated equations: Di (j) = val (ei )(j)  for 0 a$?
j a$?
N  where val (e)(j) is defined as follows.
For the base cases: val (c)(j) = c .
val (ti )(j) = Di (j) .
val (si )(j) = Di (j) .
For the inductive cases: val (f (e1 , .
.
.
, ek )(j) = f (val (e1 )(j), .
.
.
, val (ek )(j)) .
val (ite(b, e1 , e2 ))(j) = if val (b)(j) then val (e1 )(j) else val (e2 )(j) .
val (e[k, ( c])(j) = val (e)(j + k) if 0 a$?
j + k a$?
N, c otherwise .
The set of all equations associated with D is denoted by DD .
Example 2 Consider the L OLA specification  is not well-defined, but for this specification the reason is that it has no evaluation models.
To avoid ill-defined specifications we define a syntactic restriction on L OLA specifications guaranteeing that any well-formed L OLA expression is also well-defined.
Definition 3 (Dependency Graph) Let D be a L OLA specification.
A dependency graph for D is a weighted and directed multi-graph G = hV, Ei, with vertex set V = {s1 , .
.
.
, sn , t1 , .
.
.
, tm }.
An edge e : hsi , sk , wi labeled with a weight w is in E iff the equation for Di (j) in DD contains Dk (j + w) as a subexpression of the RHS, for some j (or e : hsi , tk , wi for subexpression Dk (j +w)).
Intuitively, the edge records the fact that si at a particular position depends on the value of sk , offset by w positions.
Note that there can be multiple edges between si and sk with different weights on each edge.
Vertices labeled by ti do not have outgoing edges.
D : s = t1 [1, 0] + ite(t2 [a1, true], t3 , t4 + t5 ).
The associated equations DD are dLs dLT dLa D (j a 1), dL' 2 dL' dL' dLV dLZ dL' dL' D1 (j + 1) + ite dL D3 (j), dL, j a [1, N ), dL' dL' dL' dL' dL' D4 (j) + D5 (j) dL, dLT dLs D(j) = D2 (N a 1), dL' dLZ dLV dL' dL' ite dL D3 (N ), j = N, dL, dL' dL' dL' dL' dL' D (N ) + D (N ) dL' 4 5 dL' dLl D1 (1) + D3 (0) j = 0.
A L OLA specification is well-defined if for any set of appropriately typed input streams, all of the same length, it has exactly one evaluation model.
Example 3 Consider the L OLA specification D1 : s1 = (t1 a$?
10).
For the stream D1 : 0, .
.
.
, 100, the associated equations are D1 (j) = (D1 (j) a$?
10).
The only evaluation model of D1 is the stream D1 (i) = true iff i a$?
10.
In fact, this L OLA specification is well-defined, since it defines a unique output for each possible input.
However, the specification D2 : s2 = s2 aSS (t1 a$?
10)  is not well-defined, because there are many streams D2 that satisfy D2,D for some input stream.
Similarly, the specification D3 : s3 = AZs3  Example 4 Consider the L OLA specification over independent integer variables t1 , t2 : dLs dLT s2 [a1, 7] a$?
t1 [1, 0], dL,.
s1 = s2 [1, 0] + ite dL s2 [a1, 0], s2 s2 = (s1 + t2 [a2, 1]).
Its dependency graph, shown in Figure 1, has three edges from s1 to s2 , with weights 1, 0, a1, and one zero weighted edge from s2 back to s1 .
There is one edge from s1 to t1 , and one from s2 to t2 .
A walk of a graph is a sequence v1 , .
.
.
, vk+1 of vertices, for k aL 1, and edges e1 , .
.
.
, ek , such that ei : hvi , vi+1 , wi i.
The walk is closed iff v1 = vk+1 .
The total weight of a walk is the sum of weights of its edges.
Definition 4 (Well-Formed Specifications) A L OLA specification is well-formed if there is no closed-walk with total weight zero in its dependency graph.
Theorem 1 Every well-formed L OLA specification is well-defined.
All proofs will be available in an extended version of this document.
The following alternative characterization of well-formedness is useful for algorithmic purposes and for the offline monitoring algorithm.
@ABC GFED t1 o  1  @ABC GFED s1 l  1,0,a1  @ABC GFED  , s 2 0  a2  @ABC GFED  / t2  Fig.
1: Dependency graph for the specification of Example 4.
Theorem 2 A L OLA specification is well-formed iff no strongly connected component in G has both a positive and a negative weighted cycle.
The converse of Theorem 1 is not true: not every welldefined L OLA specification need be well-formed.
For instance, the specification s = s aSS AZs is well-defined, but not well-formed.
C. Statistics and Context-free Properties We shall now demonstrate the use of our specification language for computing statistical properties over trace data.
Numerical properties over traces are essential as (1) components of correctness properties that involve counts, maxima or minima over trace data, and (2) estimating performance and coverage metrics in the form of averages.
L OLA can be used to compute incremental statistics, i.e., measures that are defined using an update function fIa (v, u) where u represents the measure thus far, and v represents the new incoming data.
Given a sequence of values v1 , .
.
.
, vn , with a special default value d, the statistic over the data is defined in the reverse sense as v = fIa (v1 , fIa (v2 , .
.
.
, fIa (vn , d)))  or in the forward sense as v = fIa (vn , fIa (vna1 , .
.
.
, fIa (v1 , d)))  Examples of such statistical measures include count with fcount (v, u) = u+1, sum with fsum (v, u) = v +u, max with fmax (u, v) = max (u, v), among many others; the statistical average can be incrementally defined as a pair consisting of the sum and the count.
Given an update function fIa and a data-stream v , the following L OLA queries compute the statistic in the forward and reverse senses respectively: stat f = fIa (stat f [a1, d], v) , stat r = fIa (stat r [1, d], v) .
For most common incremental statistical measures, either of these L OLA queries compute the same result.
The choice of a monitoring strategy can dictate the use of one over another as will be evident in the subsequent section.
The use of numeric data also increases the expressiveness of the language; it enables the expression of context-free properties.
Commonly encountered contextfree properties include properties such as aevery request has a matching grant.a In programs, we may use such properties to verify that every lock acquired has been released, or that every memory cell allocated is eventually freed exactly once.
Example 5 Consider the property: athe number of aas must always be no less than the number of bas.a This property can be expressed in L OLA as s = s[a1, 0] + ite((a aSS AZb), 1, 0) + ite((b aSS AZa), a1, 0) trigger(s a$?
0)  Integer streams in a L OLA specification enable the expression of context-free properties by being used as counters to model stacks.
For instance, a two alphabet stack with alphabet symbols 0 and 1 can be modelled by a counter.
Each pop is implemented by dividing the counter by 2, thereby eliminating the least significant bit.
Each push is modelled by a multiplication by 2 followed by addition, thereby setting the least significant bit.
Thus, with one (unbounded) counter, a L OLA specification can express context-free properties.
It can be shown that L OLA specifications with only boolean streams cannot express context-free properties.
III.
M ONITORING A LGORITHM In this section, we first describe the setting for the monitoring problem considered in the paper.
We then describe our monitoring algorithm using partial evaluation of the equational semantics.
A.
Monitoring Setup We distinguish two situations for monitoring a online and offline monitoring.
With online monitoring, system behaviors are observed as the system is run under a test/real-life setting.
In a simulation setting, we can assume that the monitor is working in tandem with the simulator, with the monitor processing a few trace positions while the simulator waits, and then the monitor waiting while the simulation proceeds to produce the  next few positions.
On the other hand, offline monitoring assumes that the system has been run to completion, and the trace data was dumped to a storage device.
This leads to the following restriction for online monitoring: the traces are available a few points at a time starting from time 0 onwards, and need to be processed online to make way for more incoming data.
In particular, random access to the traces is not available.
B. Online Monitoring Algorithm In online monitoring we assume that the trace is available one position at a time, starting from time 0.
The length of the trace is assumed to be unknown and large.
Let t1 , .
.
.
, tm be independent (input) stream variables, and s1 , .
.
.
, sn be dependent (output) stream variables.
Let j aL 0 be the current position where the latest trace data is available from all the input streams.
Evaluation Algorithm: The evaluation algorithm maintains two stores of equations: aV Resolved equations R of the form Di (j) = c, or Di (j) = c, for constant c. aV Unresolved equations U of the form Di (j) = ei for all other stream expressions ei .
Initially both stores are empty.
At the arrival of input stream data for a particular position j , 0 a$?
j a$?
N , that is, when D1 (j), .
.
.
, Dm (j) become available, the following steps are carried out: 1) The equations D1 (j) = c1 , .
.
.
, Dm (j) = cm are added to R, 2) The associated equations for D1 (j), .
.
.
, Dn (j) are added to U , 3) The equations in U are simplified as much as possible; if an equation becomes of the form Di (j) = c, it is removed from U and added to R. If c is true and the corresponding output variable si is marked as a trigger, then a violation is reported.
4) For each stream ti ( also si ), there is a non-negative constant ki such that Di (j aki ), if present in R can be safely removed.
The constant ki aL 0 is defined as     k is non-negative and  .
ki = max k  ti [ak, d] is a subexpression.
Intuitively, for any position j , j + ki is the latest value in the future whose computation requires the value of Di (j).
Example 6 To illustrate the last point, consider the specification, s = s[a3, 0] + t.  Let D be the input stream.
The value of ki for s is 3 and for t is zero.
This indicates that for any input stream D , the value D (j) can be removed from R at position j itself.
Similarly any D(j) a R may be removed from R at (or after) position j + 3.
Equations in U are simplified using the following rules: 1) Partial evaluation rules for function applications such as, true aSS e a e, 0 + x a x AV AV AV  2) Rewrite rules for if-then, ite(true, e1 , e2 ) a e1 AV AV AV  3) Substitution of resolved positions from R. If Di (j) = c a R, then every occurrence of Di (j) in U is substituted by c and possibly simplified further.
We illustrate the operation of the algorithm on a simple example.
Example 7 Let t1 , t2 be two input boolean stream variables.
Consider the specification D : s = t2 a" (t1 aSS s[1, false]),  which computes t1 Until t2 .
The associated equations for D are: ( D2 (j) a" (D1 (j) aSS D(j + 1)) j + 1 a$?
N D(j) = D2 (j) otherwise.
Let the input streams, D1 and D2 be given by D1 D2  false true  false false  true false  true false  true false  true false  true false  At position 0, we encounter hfalse, truei.
The equation for D(0) is D(0)  = D2 (0) a" (D1 (0) aSS D(1)) a true a" (false aSS D(1)) a true  and thus D(0) = true is added to the resolved store R. At position 1, we encounter hfalse, falsei and thus we can set D(1) = false , which is also added to R. From j = 2 until j = 5, we encounter htrue, falsei.
At each of these positions the equations D(j) = D(j + 1) are added to U .
The equation store U now has the equations D(2) = D(3), D(3) = D(4), .
.
.
, D(5) = D(6).
At position 6, we encounter htrue, falsei with the added information that the trace has ended.
We set D(6) = false and add it to R. This lets us resolve the equations in U and set all the positions from 2 to 6 to false .
Note that the equation associated with Di (j) on the LHS is added only after the current position reaches j , even if the term Di (j) appears on the RHS of some equation before position j is reached.
The algorithm above works in time and space that is linear in the length of the trace and the size of the specification.
Since the memory usage can be as large as the length of the trace in the worst-case, the method may not work for long simulations and large traces.
Example 8 Consider the following L OLA specification: ended = false[1, true] s = ite(ended , t, s[1, true])  in which the output stream D takes the same value everywhere that the input stream D takes at the end of the trace.
The partial evaluation algorithm maintains the unresolved D(0), .
.
.
, D(N ).
Such specifications cannot be monitored efficiently.
Furthermore, if the variable s appears in other expressions, the evaluation of the corresponding streams need to be delayed until D can be resolved.
In the next section we characterize an efficiently monitorable set of L OLA specifications based on the properties of their dependency graphs.
The partial evaluation algorithm will be shown to work efficiently for such specifications.
C. Efficiently Monitorable Specifications We present a class of specifications that are efficiently monitorable.
These specifications are guaranteed to limit the number of unresolved equations in the memory to a pre-determined constant that depends only on the size of the specification and not on the size of the trace.
Definition 5 (Efficiently Monitorable Specifications) A L OLA specification is efficiently monitorable (EM) if its worst case memory requirement under our online monitoring algorithm is constant in the size of the trace.
Example 9 Consider the specification aEvery request must be eventually followed by a grant before the trace endsa, which can be expressed as follows: reqgrant = ite(request, evgrant, true) evgrant = grant a" evgrant[1, false] trigger (AZ reqgrant)  The specification encodes the temporal assertion (request a aS(grant)).
Another way that produces the same result is waitgrant =    AZgrant aSS    request a" waitgrant[a1, false]  trigger ended aSS waitgrant     The stream waitgrant records if the monitor is currently waiting for a grant.
The monitor waits for a grant whenever it encounters a request and stops waiting if there is a grant .
If the trace ends while the monitor is still waiting, it triggers an error.
The latter formulation is efficiently monitorable, while the former is not.
For instance, at every time instance, waitgrant(i) is instantly resolved given its previous value, and those of the input streams.
Thus, the simple partial evaluation algorithm monitors the latter with very little, constant, buffering.
The following theorem characterizes efficiently monitorable L OLA specifications.
Theorem 3 If the dependency graph of a L OLA query has no positive cycles then it is efficiently monitorable.
The converse of the theorem above does not hold in general.
However, in the absence of an alternative syntactic characterization of EM specification, we shall henceforth use the term EM specification to denote queries whose dependency graphs do not contain positive cycles.
Given graph G, that does not have any positive weight cycles, we construct a graph G+ , obtained by removing all negative weight edges from G. Furthermore, among all the edges in G between two nodes si and sj , we choose to add only that edge to G+ which has the maximum positive weight.
The graph G+ has no self loops or multiple edges, and hence is a weighted directed acyclic graph (DAG).
For each node si a G+ , we define ai as follows: dLa edge from si , dL' dL,0, if( there is no outgoing  ) w(ej )  ai =  e : s a a a a s j i j dL' , ow .
dLlmax aj + w(ej )  is an edge in G+  Example tion: s1 s2 s3  10 Consider the following L OLA specifica= t1 [1, false] aSS s3 [a7, false] = ite(s1 [2, true], t2 [2, 0], t2 [a1, 2]) = (s2 [4, true] a$?
5)  The dependency graph G is shown in Figure 2.
The values of the a function are as follows: a(t1 ) = a(t2 ) = 0, a(s1 ) = 1, a(s2 ) = 3, a(s3 ) = 7.  a7  v @ABC GFED s3  4  @ABC GFED  / s2  2  @ABC GFED  / s1  2,a1  1    @ABC GFED t2    @ABC GFED t1  (a) Dependency graph G.  @ABC GFED s3  4  @ABC GFED  / s2  2  @ABC GFED  / s1  2  1    @ABC GFED t2    @ABC GFED t1  (b) Derived graph G+ .
Fig.
2: The dependency graph G for Example 10 and its derived graph G+ .
The significance of the a function is clear through the following theorem.
Theorem 4 The partial evaluation algorithm resolves any trace position Di (j) before time j + ai .
The memory requirement is therefore constant in N for an efficient specification.
This number of unresolved positions in U is upper-bounded by O(a1 + AV AV AV + an ).
For instance, computing the a values for the queries in Example 9, we find that a(waitgrant) = 0.
This shows that the value of waitgrant resolves immediately, given its previous value and the inputs.
Our experimental results in the subsequent section show that requiring specifications to be efficiently monitorable is not unreasonable in practice.
Furthermore, streams involved in positive cycles can be discarded or even rewritten (as shown in Example 9) for the purposes of online monitoring.
The framework developed generalizes naturally to an offline monitoring algorithm.
Please refer to the full version of this paper available online.
IV.
A PPLICATIONS There are numerous applications of this formalism.
In this section, we describe two such applications obtained directly from the industry.
Synopsys, Inc. provided some circuit simulation dumps, along with specifications written in the industry standard System Verilog Assertions (SVA)[18].
We were able to hand-translate the SVA queries directly into L OLA specifications, a process that is potentially mechanizable.
Our OC AML-based implementation of L OLA reads a trace file and the specification file.
It implements the online monitoring algorithm described in Section III with some direct optimizations.
We have incorporated facilities for displaying dependency graphs of specifications.
The following two case studies were considered:  a) Memory Controller: A Verilog model for a memory controller was simulated yielding 13 input streams.
The corresponding SVA assertions were handtranslated into a L OLA specification.
The specification had 21 intermediate streams and 15 output streams, all of which were declared triggers.
Properties enforced included mutual exclusion of signals, correct transfers of address and data, and timing specifications (e.g.
signal stability for 3 or 4 cycles).
The specifications were not EM : the dependency graph had three positive-sum cycles, each encoding a temporal until operator.
Figure 3 shows the performance of L OLA on these traces.
b) PCI: We hand translated SVA assertions describing the PCI 2.2 specifications for the master.
A circuit implementing the master was simulated for varying times to produce a set of traces to plot the performance.
The specification had 15 input streams, 161 output streams and 87 trigger streams.
Our initial implementation contained three positive weight cycles.
We were able to remove these by rewriting the queries carefully.
Running times can also be found in Figure 3.
Bugs were deliberately introduced into the circuit in order to evaluate the effectiveness of runtime verification.
L OLA reports numerous useful trigger violations for the longest trace.
V. C ONCLUSIONS We have presented L OLA, a formalism for runtime verification based on a functional language over finite streams equipped with a partial evaluation-based strategy for online evaluation.
Our formalism combines runtime verification of boolean temporal specifications with statistical measures to estimate coverage and specify complex temporal patterns.
By evaluating our system on industrial strength specifications, we have demonstrated that L OLA can express relevant properties.
Using dependency graphs, we have characterized efficiently monitorable queries that can be monitored online efficiently in terms of space.
Based on our case-studies so far, the restriction to efficiently monitorable specifications seems  # simulation steps 5000 10000 20000 50000 100000 200000 500000 1000000  Controller example # clock pos.
edges time (sec) 250 0.18 500 0.35 1000 0.71 2500 1.78 5000 3.47 10000 6.83 25000 17.02 50000 33.70  PCI example # clock pos.
edges 834 1667 3334 8334 16667 33334 83334 166667  time 4.62 8.87 19.04 29.47 52.53 99.17 236.96 467.98  Fig.
3: Running times for both examples.
All timings were measured on an Intel Xeon Processor running Linux 2.4 with 2Gb RAM.
practical.
In the future, we intend to study automatic techniques for rewriting non-EM specifications into efficiently monitorable ones where possible, and in further collaboration with industry study the applicability of these techniques for larger case studies.
We expect that for such use some syntactic sugar needs to be added to L OLA to facilitate specification of common constructs.
Also the error reporting needs to be improved by synthesizing explanations for each violation.
Extensions to handle synchronous systems with many clocks, asynchronous systems, and distributed systems are also under consideration.
R EFERENCES [1] K. Havelund and G. RosESSu, Eds., Runtime Verification 2001 (RVa01), ser.
ENTCS, vol.
55.
Elsevier, 2001.
[2] aa, Runtime Verification 2002 (RVa02), ser.
ENTCS, vol.
70, no.
4.
Elsevier, 2001.
[3] O. Sokolsky and M. Viswanathan, Eds., Runtime Verification 2002 (RVa03), ser.
ENTCS, vol.
89, no.
2.
Elsevier, 2003.
[4] N. Halbwachs, P. Caspi, P. Raymond, and D. Pilaud, aThe synchronous data-flow programming language LUSTRE,a Proc.
of IEEE, vol.
79, no.
9, pp.
1305a1320, 1991.
[5] G. Berry, Proof, language, and interaction: essays in honour of Robin Milner.
MIT Press, 2000, ch.
The foundations of Esterel, pp.
425a454.
[6] I. Lee, S. Kannan, M. Kim, O. Sokolsky, and M. Viswanathan, aRuntime Assurance Based on Formal Specifications,a in Proc.
of the International Conference on Parallel and Distributed Processing Techniques and Applications, 1999.
[7] D. Drusinsky, aThe temporal rover and the ATG rover,a in SPIN Model Cheking and Software Verification, 2000, pp.
323a330.
[8] K. Havelund and G. RosESSu, aSynthesizing monitors for safety properties,a in Proc.
of TACASa02.
Springer, 2002, pp.
342a 356.
[9] Z.
Manna and A. Pnueli, Temporal Verification of Reactive Systems: Safety.
New York: Springer, 1995.
[10] O. Kupferman and M. Y. Vardi, aModel checking of safety properties,a Formal Methods in System Design, vol.
19, no.
3, pp.
291a314, 2001.
[11] K. Havelund and G. RosESSu, aAn overview of the runtime verification tool java pathexplorer,a Formal Methods for Systems Design, vol.
24, no.
2, pp.
189a215, 2004.
[12] C. Eisner, D. Fisman, J. Havlicek, Y. Lustig, A. McIsaac, and D. V. Campenhout, aReasoning with temporal logic on truncated paths,a in Proc.
of CAVa03, ser.
LNCS, vol.
2725.
Springer, 2003, pp.
27a39.
[13] B. Finkbeiner, S. Sankaranarayanan, and H. B. Sipma, aCollecting statistics over runtime executions,a in [2].
[14] T. Gautier, P. Le Guernic, and L. Besnard, aSIGNAL: A declarative language for synchronous programming of realtime systems,a in Proc.
Conference on Functional Programming Languages and Computer Architecture.
Springer, 1987, pp.
257a277.
[15] K. Sen and G. RosESSu, aGenerating optimal monitors for extended regular expressions,a in [3].
[16] G. RosESSu and K. Havelund, aRewriting-based techniques for runtime verification,a Journal of Automated Software Engineering (to appear).
[17] H. Barringer, A. Goldberg, K. Havelund, and K. Sen, aRulebased runtime verification,a in Proc.
of 5th International Conference VMCAIa04, ser.
LNCS, vol.
2937.
Springer, 2004, pp.
44a57.
[18] aSystem verilog assertion homepage,a 2003, [Online] Available: http://www.eda.org/sv-ac.
A temporal structure that distinguishes between the past, present, and future Andre Trudel  Jodrey School of Computer Science Acadia University Wolfville, Nova Scotia, Canada, B0P 1X0  Abstract  We present a two dimensional temporal structure that has an ever changing present.
Relative to each present, there is a past and future.
The main representational advantage our two dimensional structure has over traditional linear temporal structures is the ability to record when knowledge is added or updated.
We dene a rst order logic that has this structure as its temporal ontology.
1 Introduction  Most temporal rst order logics in Articial Intelligence have a linear (i.e., non-branching) temporal ontology.
Examples of logics with a linear structure are those of Allen 1], Kowalski 4], and Shoham 6].
Even the logic of McDermott 5] uses linear time: Note that, contrary to what is often stated, McDermott's system does not use branching time: time itself is represented by the linear ordering of the real numbers branching only occurs with respect to the totality of possible states ordered by date.
(2], p. 1178) Linear time has its drawbacks.
There is no distinguised element in the ontology to represent the present.
Consequently, there is no concept of a past or future.
Another drawback is that a linear time based logic represents the current state of aairs.
There is no record of when knowledge is obtained or updated.
Humans do not view time as being linear.
Instead, we neatly compartmentalize time into the past, present, and future.
As the present changes, so does the past and future.
For example, we are continually learning things about our past and revising our future plans.
We present a two dimensional temporal structure that captures some of our intuitions about the past, present and future.
It has an ever changing present, and a past and future relative to each present.
We then formally dene a rst order logic that has this structure as its temporal ontology.
2 Proposed logic  Each predicate has two temporal arguments.
For example, red(1,1,house) and alive(5,10).
The two temporal arguments do not specify an interval.
For example, alive(5,10) is not used to represent the fact that alive is true over the interval (5,10).
Instead, the two temporal arguments are cartesian coordinates.
The relation alive(5,10) species that alive is true at the point (5,10) on the cartesian plane.
The temporal ontology consists of a cartesian plane.
The line y = x is used to represent the actual state of the world.
Relative to any point (p p) on the line y = x the line segment fy = x x > pg represents the actual future, fy = x x < pg represents the actual past, fy = p x > pg represents the expected future, and fy = p x < pg represents the perceived past (see gure 1).
What an agent observes or experiences at time p is recorded at the point (p p): Any plans or expectations the agent may have about the future at time p is recorded on the line fy = p x > pg: Similarly, any knowledge the agent learns or is given about the past at time p is recorded on the line fy = p x < pg: On the diagonal line y = x we record what actually happens in the world.
For example, in gure 2 the house is red at time 10 (i.e., red(10,10)).
At time 10, we plan to paint the house white at time 20 (i.e., white(20,10)).
But for some unforeseen reason, the house gets painted earlier at time 15 (i.e., white(15,15)).
We also know that at time 2, the house is white (i.e., white(2,2)).
At time 10, we learn that the house was blue at time 5 (i.e., blue(5,10)).
Note that blue(5,10) records two items of information.
The rst is that the house is blue at time 5, and the second is that this fact was recorded (learned) at time 10.
Formulas along a vertical line need not be consistent.
Figure 3 shows a situation where at time 10 we plan to go to the movies at time 15 (i.e., movies(15,10)).
But at time 15, something comes up that prevents us from going to the movies (i.e., not movies(15,15)).
Also, at time 5 we thought the house had been painted red at time 2 (i.e., red(2,5)).
We later learn at time 10 that the house was not red at time 2 (i.e., not red(2,10)).
The x and y axes of the cartesian plane must be linear and of the same type.
No further restrictions  6 y perceived past        past     x      future y=x   expected   (p,p)  future  x  -  Figure 1: The dierent pasts and futures relative to (p,p)  6  white(15,15)x   y blue(5,10)  x    x     x       x   red(10,10)   white(20,10)   white(2,2) x  -  Figure 2: Colors of a house over time  6  not movies(15,15)  y not red(2,10)  x  red(2,5)  x                    x      x  movies(15,10)  x Figure 3: Inconsistent information  -  19  6  15                       .h    .
.
nancing(19,(15,19)) .
.
.h .
.
.
.h  university((19,23),15) 19  23  -  Figure 4: Intervals are placed on the axes.
They can be discrete, dense, points, intervals, points-intervals, etc.
If intervals are allowed, they appear as one of the temporal parameters.
For example, in gure 4 a 15 year old student plans to attend university between the ages of 19 and 23 (i.e., university( (19,23), 15)).
Also, between the ages of 15 and 19 the student believes that nancing will be in place when entering university at age 19 (i.e., nancing(19, (15,19) )).
Although the examples in this paper only use the north-east corner of the cartesian plane, the whole plane can be used to represent information.
We conclude with an outline of the syntax and semantics for the proposed logic.
2.1 Syntax The logic has two disjoint sorts called temporal and non-temporal.
All terms are sorted.
Predicates have 2 temporal arguments followed by m  0 non-temporal arguments.
Terms and well formed formulas are dened in the standard fashion.
2.2 Semantics An interpretation is a tuple hT U i where T is a non-empty temporal universe, U is a non-empty nontemporal universe, and  is an interpretation function which maps each temporal constant to an element of T, each non-temporal constant to an element of U, each n-ary temporal function to an n-ary function from T n to T , each n-ary nontemporal function to an n-ary function from U n to U, and each (2 m)ary predicate to an (2 m)-ary predicate on T 2  U m .
Quantied variables range over the appropriate universe.
Well formed formulas are interpreted in the usual fashion.
3 Examples  3.1 Leave lights on  Information recorded on the line y = x may later be discovered to be false.
For example in gure 5, the driver of the car believes that he shut o the headlights when he left the car at time 5.
Upon returning to the car at time 20, he discovers the battery is dead.
He then checks the light switch and it is in the \on" position.
Therefore, the lights were not shut o at time 5.
3.2 Course  The proposed logic can be used to model an agent's changing expectations or beliefs over time.
For example, assume a course starts at time 5 and ends at the end of the term at time 25.
At the start of the course, the student believes he will pass (see gure 6).
At time 10, the student does very poorly on the rst assignment and thinks he will not pass the course.
The student does very well on the midterm at time 15 and now believes that he has a chance of passing.
But, the student does poorly on the second assignment at time 20 and once again believes he will fail.
The story has a happy ending.
The student aces the nal exam and passes the course.
3.3 Planning  Assume that at time 5, an agent constructs a plan to enter a room.
The plan consists of going to the door over the interval (5,10) (i.e., gtd( (5,10), 5)), opening the door over the interval (10,15) (i.e., od( (10,15), 5)), and then entering the room over the interval (15,20) (i.e., er( (15,20), 5)).
Note that we represent the plan along with the time that it was constructed.
The plan is shown in gure 7.
Over the interval (5,10), the agent excutes the rst action of the plan which is to go to the door.
Once at the  6 y    not lights-o(5,20)  x          x lights-o(5,5)          x  battery(20,20,dead)  x  -  Figure 5: Lights  6 y   nal(25,25) assgn2(20,20) x  midterm(15,15) x assgn1(10,10)  x begin(5,5) x             x pass(25,25) x x x x  not pass(25,20) pass(25,15) not pass(25,10) pass(25,5) x  -  Figure 6: Expectations of passing the course changes over time      6 10 5  .
gtd((5,10),(5,10)) .
.
.
   x door-locked(10,10) .  .x .
.
.
.
x .
.
.
.
x. .
.
.
.x  gtd((5,10),5) od((10,15),5) er((15,20),5)   -  5  10  15  Figure 7: Remembering a plan and re-planning  20  door, the agent observes that the door is locked which is unexpected.
The agent cannot execute the next action which is to open the door.
At this point, the agent must construct another plan which would be stored on the line fy = 10 x > 10g: The old plan constructed at time 5 remains untouched.
It can be used as a guide while re-planning at time 10.
It can also be used to answer queries.
For example, if we ask the agent why he is at the door at time 10 without a key, the agent can examine the old plan and reply that he expected the door to be unlocked at time 10.
3.4 Multi-agents  The proposed temporal structure is two dimensional.
Additional dimensions can be added to the structure to represent and reason about multi-agent problems.
The addition of a third temporal parameter (i.e., (x y z)) allows us to represent individual knowledge of an agent and common knowledge.
Each agent is assigned a plane.
Information about the n'th agent is stored on the plane (x y n): Information that is common to all agents is stored on the plane (x y 0): For example, assume there are three agents, and all three know that block A is on block B at time 5: on(5,5,0, A,B).
Agent 1 also knows that A is on B at time 6: on(6,6,1, A,B).
At time 10, agent 2 plans to move block C on top of A over the interval (15,20): move((15,20),10,2, C,A).
Agent 3 knows that block A is red at time 7: red(7,7,3, A).
We could also have the situation where all three agents know a fact, but don't realize it is common knowledge (i.e., not contained on the 0'th plane).
For example, each agent has local knowledge that block B is blue at time 10: blue(10,10,1, B), blue(10,10,2, B), blue(10,10,3, B).
Each agent does not know that the other 2 agents also have the information that B is blue at time 10.
Instead of assigning a plane to each agent, we can add a fourth temporal parameter to the structure and assign a cube to each agent.
In agent i's cube (i.e., (x y z i)), information agent i has about agent n is stored on the n'th plane (i.e., (x y n i)), and i's personal information is stored on plane i (i.e., (x y i i)).
For example, agent 1 knows that block B is blue at time 10, and also believes that agent 2 has this information: blue(10,10,1,1, B), blue(10,10,2,1, B).
Information common to all agents is stored on the plane (x y 0 0): A fth dimension can be used to represent groups of agents.
Each group consists of one or more agents.
Information about group n is stored using (x y z a n): The rst four parameters are used to store information about a particular agent in group n: For example, information about the third agent in group 2 is stored in (x y z 3 2):  Other dimensions can be added as needed.
4 Persistence  If the house is blue at time 10, is it also blue at time 15?
Given no knowledge of the house changing color, it seems reasonable to assume that the color of the house persists from time 10 to 15, and we conclude the house is blue at time 15.
This is called the persistence problem.
Traditional linear temporal structures only need to deal with persistence along a single axis.
Here, we must consider two dimensional persistence.
In gure 8, the house is blue at time 10 (i.e., blue(10,10)).
As discussed above, persistence should be allowed into the future (i.e., along the line fy = 10 x > 10g).
Using a similar argument, persistence into the past should also be allowed (i.e., along the line fy = 10 x < 10g).
For example, if the house is blue at time 10, it was probably also blue at time 9.
We also need persistence in the upward direction (i.e., along the line fy > 10 x = 10g).
For example, at the point (11,11), we should remember that the house was blue at time 10 (i.e., blue(10,11)).
Upward persistence models the agent's memory.
We do not allow persistence in the downward direction.
The relation blue(10,10) also records the fact that the color of the house was learnt at time 10.
Therefore at time 9, we have no informationabout the color of the house (i.e., the truth value of blue(10,9) is unknown).
To summarize, we have horizontal bi-directional persistence and vertical upward persistence.
Persistence is not allowed in the vertical downward direction.
In either of the three directions where persistence is allowed, standard algorithms can be used.
Problems arise when vertical and horizontal persistence are inconsistent.
For example in gure 9, at time 20 we know the house was not blue at time 5, and at time 15 we know the house was blue at time 10.
At time 20, was the house blue at time 10 (i.e., is blue(10,20) true)?
Using horizontal persistence and not blue(5,20) we can conclude not blue(10,20).
We can also conclude the opposite using vertical persistence and blue(10,15).
Which answer do we prefer?
The preference between vertical and horizontal persistence depends on the particular situation.
In this case, either answer is reasonable.
In the future, we will investigate algorithms for resolving persistence conicts.
5 Conclusions  We presented a general rst order logic that has a unique two dimensional temporal structure.
The structure consists of a cartesian plane.
The present moves along the line y = x: At any point on the line y = x we can record plans or expectations about the future, and information about the past or present.
The proposed temporal structure has the appearance of being a branching one.
But, it is not.
Time  yes  6  6  y yes fi  x  - yes  no x  -  Figure 8: The persistence of blue(10,10)  6 y  not blue(5,20)  x  - x(10,20) 6 x  blue(10,15) x Figure 9: Vertical and horizontal persistence are inconsistent  -  moves along the single line y = x: The branches emanating from each point on the line y = x are used to store information about the past or future obtained at that point in time.
The main representational advantage our two dimensional structure has over traditional linear temporal structures is the ability to record when knowledge is added or updated.
For example, simple English sentences like \Last night I planned to go to the movies tonight, but now I don't feel like going" cannot be represented using a linear structure.
A linear structure can either represent the fact that the person is going to the movies or not.
It cannot represent the fact that going to the movies tonight was true yesterday and false today.
The sentence is easily represented in the proposed logic: movies(tonight yesterday) ^ not movies(tonight tonight): Instead of using the proposed logic, it is possible to extend the syntax and semantics of traditional linear time logics so that they use a two dimensional structure.
For example, RGCH 3] uses the real numbers.
We can easily add another temporal argument to the logic.
Acknowledgements  Thanks to Denis Gagne for discussing the material contained in this paper.
Research supported by Natural Sciences and Engineering Research Council of Canada grant OGP0046773.
References  1] J.F.
Allen.
Towards a general theory of action and time.
Articial Intelligence, 23(2):123{154, 1984.
2] A. Galton.
Reied temporal theories and how to unreify them.
In 12th International Joint Conference on Articial Intelligence, pages 1177{1182, Sydney, Australia, 1991.
3] S.D.
Goodwin, E. Neufeld, and A. Trudel.
Temporal reasoning with real valued functions.
In  Pacic Rim International Conference on Articial Intelligence (PRICAI'92), pages 1266{1271,  Seoul, Korea, Sept 1992.
4] R.A. Kowalski and M. Sergot.
A logic-based calculus of events.
New Generation Computing, 4:67{95, 1986.
5] D.V.
McDermott.
A temporal logic for reasoning about processes and plans.
Cognitive Science, 6:101{155, 1982.
6] Y. Shoham.
Temporal logics in AI: Semantical and ontological considerations.
Articial Intelligence, 33:89{104, 1987.
Configuration Logic: A multi-site Modal Logic Roger Villemaire, Sylvain Halle, Omar Cherkaoui Universite du Quebec a Montreal C. P. 8888, Succ.
Centre-Ville Montreal, Canada H3C 3P8 villemaire.roger@uqam.ca  Track 3: Temporal Logic in Computer Science Specification and verification of systems Temporal logics for distributed systems  Abstract We introduce a logical formalism for describing properties of configurations of computing systems.
This logic of trees allows quantification on nodes labels which are modalities containing variables.
We explain the motivation behind our formalism and give a classical semantics and also a new equivalent one based on partial functions on variables.
1 Introduction Managing computing equipment configurations is a central task in computer science.
The increase in computational power and in number of computing devices makes this task error prone.
It is important to be able to describe properties in a abstract way to help automatize the management of devices configuration.
We propose in this paper a simple extension to modal logic, aimed at describing properties of computing equipment.
Even if our motivation is towards applications in computer networks, our logical system is applicable to any type of device.
Our goal is to develop a formalism to check, analyze and help debug complex configurations.
In this paper we make a first step in this direction presenting the logical system, a classical semantics and also a new formal semantics in terms of partial functions on variables.
We have tried to keep our logical system as simple as possible and as close as possible to the goal of analyzing configurations.
The paper is structured as follows.
We define our logic in section 2, we explain the motivation in the field of computer networks in section 3, we relate our logic to others computational logic in section 4.
Finally we develop the new formal semantics in section 5 and we conclude with section 6.
2 Configuration logic 2.1 Syntax A Configuration Logic language CL is formed of a set of names N ames = {p, q, r, p1 , q1 , r1 , .
.
.
}, a set of variables V ariables = {x, y, z, x 1 , y1 , z1 , .
.
.}
and a set of relations R1 (x), R2 (x), .
.
.
(respectively of arity arity(R1 ), arity(R2 ), .
.
.).
Formulas are built using the usual boolean connectives ([?
], [?
], !)
and the following quantifiers: Existential quantifiers: There are two forms of existential quantifiers: hp = xiph and hp = x; p = xiph, where p is a name, p a finite sequence of names, x a variable and x a finite sequence of variables of the same length as p. Only the last variable x is considered bound as it will become clear in the classical semantics described further.
Universal quantifiers: There are also two forms of universal quantifiers: [p = x]ph and [p = x; p = x]ph, where p, p, x and x are the same as for existential quantifiers.
Here again only the last variable x is considered bound.
If it is necessary to explicitly write the elements  p1 , .
.
.
, pn of p and those x1 , .
.
.
, xn of x, we will write the quantifiers as hp1 = x1 , .
.
.
, pn = xn ; p = xiph and [p1 = x1 , .
.
.
, pn = xn ; p = x]ph.
To simplify proofs and definitions, we will consider that hp = xiph and [p = x]ph are special cases of [p = x; p = x]ph and hp = x; p = xiph, where p and x are empty.
Without loss of generality, we will restrict ourselves to sentences in which every variable is bound only once.
By renaming, every sentence can be put into this form.
In fact, we want to limit the use of quantifiers in such a way that they are extensions of previous ones.
To make this notion clear, let us introduce the following definition.
Definition 1 A sentence is a formula such that every variable is bound and furthermore any sub-formula hp1 = x1 , .
.
.
, pn = xn ; p = xips is contained in a sub-formula that does not bind any of the x1 , .
.
.
, xn-1 and is of the form hp1 = x1 , .
.
.
, pn-1 = xn-1 ; pn = xn iph  A configuration is a forest (set of trees) where every node is labeled by a name and a value.
Furthermore, no two roots (top level nodes) can have the same name and value.
Similarly, every node has no more than one child having the same name and value.
Formally, we introduce the following definition.
Definition 4 A Configuration is a structure of the form hV, N, R1 , ..., Rn i where: * V is the set of values; * N the set of nodes, is a set of words closed under prefix, on the alphabet formed of (p = v), with p a name and v [?]
V ; * R1 , .
.
.
, Rn are relations on V (i.e.
subsets of V arity(R1 ) , .
.
.
, V arity(Rn ) respectively).
A configuration represents a hierarchical set of parameters configuring some computing equipment.
The nodes representing the parameters have a name and a value.
To introduce the classical semantics we need the following definition.
Definition 5 A Valuation for a configuration is a function r : V ariables - V .
or of the form [p1 = x1 , .
.
.
, pn-1 = xn-1 ; pn = xn ]ph The same must be true of sub-formulas [p 1 x1 , .
.
.
, pn = xn ; p = x]ps.
2.2 Configurations  =  We will introduce some definitions before giving the classical semantics.
Definition 2 A Path is a finite non-empty word on the alphabet formed of all (p = x) where p is a name and x a variable.
Definition 3 A Name-Path is a finite non-empty word on the alphabet formed of all names.
If p = p1 .
.
.
pn and x = x1 .
.
.
xn we will usually write (p = x) for the path (p1 = x1 ) * * * (pn = xn ).
We denote by r[x/v] the valuation that agrees with r on every variable but x, in which case it returns v. We will also write r(x) for r(x1 ) * * * r(xn ) where x = x1 * * * x n .
We can now give the classical semantics for the configuration logic.
Definition 6 Let C be a configuration and r be a valuation for this configuration.
We say that C satisfies a configuration logic formula ph under valuation r (in notation C, r |= ph), if recursively: * C, r |= Ri (x) if Ri (r(x)) holds; * C, r |= ph [?]
ps if C, r |= ph and C, r |= ps; * C, r |= ph [?]
ps if C, r |= ph or C, r |= ps; * C, r |= !ph if C, r 6|= ph;  its siblings.
In our example the node int=eth0 represents the unique interface with name eth0.
3.1 Network Management  Figure 1.
A simple configuration for IP address  * C, r |= hp = x; p = xiph if there exists a v [?]
V such that (p = r(x))(p = v) [?]
N and C, r[x/v] |= ph; * C, r |= [p = x; p = x]ph if for all v [?]
V such that (p = r(x))(p = v) [?]
N it holds that C, r[x/v] |= ph.
Remind that as stated before only the last variable of a quantifier is bound.
3 Motivation The goal behind the development of CL is to describe and verify properties on configurations of computing equipment.
In particular, we are interested in validating the configuration of network routers, which are the equipment responsible for forwarding packets towards their destination.
The configuration of a router is the set of parametervalue pairs that describe the state of the device at a given moment.
These parameter-value pairs are organized in a hierarchical fashion: for example, each router may have multiple interfaces, and each interface has its IP address.
Figure 1 shows a portion of the configuration of a router containing two interfaces, called eth0 and eth1, whose IP addresses are respectively 192.168.1.13 and 192.168.1.14.
In a parameter-value pair, the parameter is a static name, while the value is configurable.
It is important to note that the parameter-value pair is unique among  The global configuration of a network is formed of the configuration of its routers.
To ensure proper functioning of the network, specific relations must be satisfied on the values of the parameters, which may span multiple devices.
When new network services are added, some parameters of the configuration must be changed.
In order to assure that all services still function properly, these changes must be made in such a way that existing relations are still fulfilled.
Due to the size of present networks and the complexity of services, it is of prime importance to develop formalisms and methods to help manage complex configurations in order to ensure that the network stays in a consistent state.
To illustrate the kind of applications we have in mind, we will give two simple, but still representative examples.
3.2 Example 1: IP addresses As has been explained earlier, the parameters of the configuration affected by a service must verify some specific relations.
The simplest example of such relation can be seen in an IP address following the Classless InterDomain Routing (CIDR) scheme [5], [11], whose two components, the value and the subnet mask, are linked by a simple relationship: an address like 206.13.01.48/25, having a network prefix of 25 bits, must carry a mask of at least 255.255.255.128, while the same address with a network prefix of 27 bits must not have a subnet mask under 255.255.255.224.
Figure 2 depicts a portion of a configuration representing an IP address (ip) with its subnet mask (mask) and network prefix (pref).
Let R(m, p) be a relation which holds if m is an acceptable mask for the prefix p. The previous property can be expressed by the CL formula of Figure 3, stating that all addresses a must have a mask m and a prefix p satisfying R(m, p).
Figure 2.
A simple configuration for IP address  [ip = a]hip = a; mask = mi hip = a; pref = pi R(m, p) Figure 3.
A formula for the correct specification of IP addresses  3.3 Example 2: Virtual Private Networks More complex situations can be encountered, in which the parameters of several devices supporting the same service are interdependent.
An example is provided by the configuration of a Virtual Private Network (VPN) service [10], [12], [13].
A VPN is a private network constructed within a public network such as a service provider's network.
A customer might have several sites geographically dispersed, and would like to link them together by a protected communication.
Most of the configuration of a VPN is realized in routers placed at the border between the client's and the provider's networks.
On the client side, these routers are called customer edge (CE) routers, and on the provider side, they are called provider edges (PE).
Many techniques have been developed to ensure the transmission of routing information inside a VPN without making this information accessible from the outside.
One frequently used method consists in using the Border Gateway Protocol (BGP).
This method involves the configuration of each PE to make it a "BGP neighbor" of the other PE's [10].
Without getting into the details, it suffices to know that one interface in each PE router must have its IP address present as a BGP neighbor of each other PE router.
Let P E(r) be a relation satisfied by the PE routers, and N eighbor(a, r) be a relation which holds when  Figure 4.
An excerpt from a configuration of a VPN  [router = r1 ][r = r2 ] (P E(r1 ) [?]
P E(r2 ) [?]
r1 6= r2 - hrouter = r1 ; int = iiN eighbor(i, r2 ))  Figure 5.
A formula for the configuration of a Virtual Private Network  the address a is a BGP neighbor of router r. This property can be expressed by the CL formula of figure 5, stating that for each pair of different routers r 1 and r2 that are both PE's, some interface of r 1 must be a BGP neighbor of r2 .
4 Related Logics In this section, we provide a comparison of CL to other related logics.
4.1 Modal Logics Modal (, 2) and multi-modal (hai, [a]) modalities trace a path and allow to refer to properties of nodes in the future.
While in modal and multi-modal logic one refers to properties of individual future states, in CL the quantifiers allow to reach different nodes and then refer to a property involving many nodes.
For instance the following CL sentence  4.3 Guarded Logics hp = xihp = yix 6= y could at best be expressible in multi-modal logic by _  hp = aiT [?]
hp = biT  a6=b  where a, b range over the domain of x and y.
Hence classical modal and multi-modal logics can be seen as mono-site: basic relations are on the contents of nodes.
On the other hand CL can be seen as a multi-site modal logic: basic relations can involve many nodes.
Of course the presence of variables in modalities will come at a price as we will show below.
4.2 TQL The logic that mainly inspired us is the Tree Query Logic (TQL) [2, 3].
TQL has been developed as the spatial fragment of a more general logic called the ambient logic.
It is a logic which not only allows formulation of sentences that can be model checked against a given tree, but also queries that extract data from those same trees.
The main application of TQL is targeted towards the extraction of data from databases modeled by XML files.
Using TQL prefix operator and its quantification on arbitrary labels of nodes, one gets the CL quantifiers.
CL is therefore a fragment of TQL.
Moreover, TQL provides fix-point operators for expressing recursive properties.
Hence, TQL is much more expressive than CL: It allows more flexible quantifications and recursion by fix-points.
It has been shown that TQL is an undecidable logic: there is no algorithm to decide if there exists a finite structure satisfying a TQL sentence [4].
We have used TQL as a tool for the validation of device configurations [7, 8].
This motivates us to investigate fragments of a logic which would be suitable for describing configurations.
Our goal is to tailor a logic for configuration purpose and to avoid non necessary constructs like fix-points.
Even if our logic is still undecidable as we show below, its simplicity makes easier its integration in a tool.
Our team is actually working on its integration in a network configuration tool.
Guarded logic is a fragment of first-order logic which generalize modal logic.
In guarded logic all quantifiers must be relativized by atomic formulas.
Therefore, quantifiers in the guarded fragment of firstorder logic appear only in the form [?
]y(a(x, y, z) [?]
ps(x, y)) or [?
]y(a(x, y, z) - ps(x, y)) The atom a, called the guard, must contain all free variables of ps [6].
The loosely guarded logic is a generalization of guarded logic where the condition on the guard is relaxed.
In this case the guard must be a conjunction of atomic formulas such that if x is a free variable of a, and y is a variable from y, then there is a conjunct in the guard where x and y both occur [9].
These fragments of first-order logic have a number of interesting properties.
It has been shown [1] that the satisfiability problem for the guarded fragment is decidable, and, moreover, that it has the finite model property (every satisfiable formula in the guarded fragment has a finite model).
The loosely guarded fragment has been shown to have the small model property [9].
Unfortunately, CL configuration properties are neither guarded nor loosely guarded.
For instance, let us consider the sub-formula hrouter = r1 ; int = iiN eighbor(i, r2 ) of the sentence of figure 5.
It can be translated in firstorder terms to [?
]i I(r1 , i) [?]
N eighbor(i, r2 ) where I(r, i) holds if i is a interface of router r. Since r2 is a free variable of N eighbor(i, r2 ) which is not in the guard I(r1 , i), this formula is not guarded.
Furthermore, in general, there is no guarded or loosely guarded equivalent to a CL sentence.
This follows from the fact that one can define an infinite total order in CL by the following sentences on one binary relation R. The conjunction of these sentences is consistent, but it has no finite model, hence the finite model property does not hold for configuration logic.
[p = x]!R(x, x) [p = x][p = y][p = z]R(x, y) [?]
R(y, z) - R(x, z) [p = x]hp = yiR(x, y)  4.4 From classical first-order logic to CL In fact, classical first-order logic can be interpreted in CL by replacing existential quantifiers [?
]x by hp = xi and universal quantifiers [?
]x by [p = x], for some fixed name p. By Trakhtenbrot's result [14] which states that for a first-order language including a relation symbol that is not unary, satisfiability over finite structures is undecidable, we have the analog for CL.
Therefore there can be no effective way to find a bound on the size of the smallest finite model of a CL formula, since enumerating the structures of this size would give decidability for the existence of a finite structure satisfying the sentence.
5 Adapted Valuations As shown above CL does not have nice computational properties like decidability and small model property.
Nevertheless the simplicity of CL allows the investigation of fragments which would be expressible enough for the applications we have in mind, but would still satisfy these properties.
Our investigation of fragments of CL has shown us that the above classical semantic is not appropriate for CL.
CL being a logic about path in trees, it is difficult to work with valuations which are not constrained by the tree structure.
For instance, our existential quantifiers mean the existence of a value on a path an not merely of some value.
Therefore we now give a semantics in terms of partial functions on variables.
The idea is that in order to check a sentence one has to recursively check sub-formulas.
In turn, to check a sub-formula, one has to consider valuations.
We show in this section that instead of considering general valuations, it is sufficient to restrict ourselves to functions sending variables to values that satisfy the hierarchical structure of the variables in the sentence.
This allows to integrate the hierarchical condition on the values of  variables into the definition of these new kinds of valuations.
We propose this new semantics in this section and show that it is equivalent to the previous classical semantics.
Definition 7 The Path of a sub-formula of the form hp = xips or hp = xips is (p = x).
Similarly, the Path of a sub-formula hp = x; q = yips and hp = x; q = yips is (p = x)(q = y).
Since in a specific sentence a variable is bound only once, we will speak of the Path of a bound variable which is the path of the quantifier binding this variable in the sentence.
From definition 1 one can show by induction that the following result holds.
Proposition 1 Let ph be a sentence and x a variable of ph of path (pn = x1 ), .
.
.
, (pn , xn )(p, x).
For all i = 1, .
.
.
, n we have that the path of xi is (pn = x1 ), .
.
.
, (pi , xi ).
We will say that f : A - B is a partial function if f is a function sending elements of its domain dom(f ) to elements of B.
Let ph be a CL formula.
We denote by V ariables(ph) the set of variables (bound or free) of ph.
We can now give the definition of our restricted form of valuation.
Definition 8 Let C = hV, N, R1 , ..., Rn i be a configuration and ph be a sentence.
A partial function r : V ariables(ph) - V is said to be adapted (or phadapted) for C if for every variable y [?]
dom(ph) of ph of path (p1 = y1 ) * * * (pn = yn )(p = y), the following conditions holds: 1.
{y1 , .
.
.
, yn } [?]
dom(r) 2.
(p1 = r(y1 )) * * * (pn = r(yn ))(p = r(y)) [?]
N .
We now have the following fact.
Proposition 2 Let C = hV, N, R1 , ..., Rn i be a configuration, ph be a sentence, and r be a valuation for C adapted to ph.
Let also (p1 = x1 ) * * * (pr = xr )(q = y) be the path of y in ph for some y 6[?]
dom(r).
We have that if {x1 , .
.
.
, xn } [?]
dom(r) and if v [?]
V is such that (p1 = r(x1 )) * * * (pr = r(xr ))(q = v) [?]
N then r0 = r[y/v] is adapted to ph.
Proof 1 To prove that r0 = r[y/v] is adapted, we must show that for y 0 [?]
dom(r0 ) of path (q1 = y1 ) * * * (qm = ym )(q = y 0 ) it holds that 1.
{y1 , .
.
.
, yn } [?]
dom(r0 ) 2.
(q1 = r0 (y1 )) * * * (qm = r0 (ym ))(q = r0 (y 0 )) [?]
N As y 6[?]
dom(r), y cannot appears in any path of a variable of dom(r0 ) except its own.
Therefore the claim must be shown only for y 0 = y.
For y = y 0 the claim follows from the hypothesis.
By the previous result we have that if r is a valuation satisfying definition 6 is adapted and if its domain contains all free variables of the formula under consideration but not y, then the valuations r[y/v] considered in this definition are again adapted.
This fact makes it possible to precise the relationship between adapted and "regular" valuations.
Lemma 1 Let ph be a sub-formula of some sentence ph0 , let C = hV, N, R1 , ..., Rn i be a configuration and r be a valuation for C whose domain contains all free variables of ph.
If F is a set of variables containing all free variables of ph but none of its bound variables, and if r| F , the restriction of r to the domain F , is ph 0 -adapted, then the following conditions are equivalent: 1.
C, r |= ph 2.
C, r|F |= ph Proof 2 The proof goes by induction on the structure of ph.
The cases of an atomic formula, conjunction, disjunction and negation are clear.
All cases of existential and universal quantifiers are similar, so we give details only for ph = hp = x; q = yips.
If C, r |= hp = x; q = yips, then by definition 6, we have that there exists a v [?]
V such that (p = r(x))(q = v) [?]
N and C, r[y/v] |= ps.
Now since y 6[?]
F it follows that y 6[?]
dom(r| F ).
Therefore by Proposition 2 it follows that (r| F )[y/v] is ph0 -adapted.
Let F 0 = F [?]
{y}.
We have that (r|F )[y/v] = r[y/v]|F 0 since they both agree on F and on y.
So r[y/v]|F 0 is ph0 -adapted.
Since F 0 contains all free and no bound variable of ps, it follows by induction hypothesis that C, r[y/v]|F 0 |= ps holds.
Again by equality r|F [y/v] = r[y/v]|F 0 and by definition 6 we have that C, r|F |= hp = x; q = yips holds.
Conversely if C, r|F |= hp = x; q = yips, by definition 6, we have that there exists a v [?]
V such that (p = r(x))(q = v) [?]
N and C, r|F [y/v] |= ps.
As before, it follows from Proposition 2 that (r|F )[y/v] is ph0 -adapted.
Furthermore (r|F )[y/v] = r[y/v]|F 0 holds.
Therefore we have by induction hypothesis that C, r[y/v] |= ps and hence C, r |= hp = x; q = yips.
It now follows that: Theorem 1 Let C = hV, N, R1 , ..., Rn i be a configuration, ph be a sentence and r be a valuation for C. Let [?]
be the empty ph-adapted valuation (its domain is the empty set).
We have that C, r |= ph if and only if C, [?]
|= ph.
From the previous result we get the following equivalence.
Theorem 2 Let C =< V, N, R1 , ..., Rn > be a configuration and ph be a sentence.
The following condition are equivalent.
1.
C, r |= ph for all valuations r; 2.
C, r |= ph for some valuation r; 3.
C, r |= ph for some ph-adapted valuation r; 4.
C, r |= ph for all ph-adapted valuations r. Proof 3 The result follows directly from Theorem 1, since to check that C, r |= ph for some valuation r, it suffices to check that C, [?]
|= ph holds.
Remark 1 It is important to note that the hierarchical structure of variables constrains the possible adapted valuations.
Therefore even if the empty valuation is always an adapted valuation, there is not always an adapted valuation whose domain contains all free variables, as the following example shows.
Example 1 If C = hV, N, R1 , ..., Rn i is such that N contains no (p = v) for some name p then there is no ph-adapted valuation on C for ph = hp = xix = x, whose domain contains x.
6 Conclusion We proposed a new logic for describing the configuration of computing equipment and motivated it with examples from the field of network configuration.
We also gave a classical and a new equivalent semantics.
Since we are interested in applications, we are working at integrating CL in a network configuration tool.
We are also working on using our new semantics to investigate a fragment of CL which would be sufficient to express the properties needed in practice, while having better theoretical properties like decidability and small model property.
References [1] H. Andreka, J. van Benthem, I. Nemeti: Modal Languages and Bounded Fragment of Predicate Logic, ILLC Research Report ML-96-03 (1996), 59 pages.
[2] Cardelli, L.: Describing semistructured data.
SIGMOD Record, 30(4), 80-85.
(2001) [3] Cardelli, L., Ghelli, G.: TQL: A query language for semistructured data based on the ambient logic.
To appear in Mathematical Structures in Computer Science.
[4] Giovanni Conforti, Giorgio Ghelli: Decidability of Freshness, Undecidability of Revelation.
FoSSaCS 2004: 105-120 [5] Fuller, V., Li, T., Yu, J., Varadhan, K.: Classless Inter-Domain Routing (CIDR): an Address Assignment and Aggregation Strategy.
RFC 1519 (1993)  [6] Gradel, E.: On The Restraining Power of Guards.
J. Symb.
Log.
64(4): 1719-1742 (1999) [7] Sylvain Halle, Rudy Deca, Omar Cherkaoui, Roger Villemaire: Automated Validation of Service Configuration on Network Devices.
MMNS 2004: 176-188 [8] Sylvain Halle, Rudy Deca, Omar Cherkaoui, Roger Villemaire, Daniel Puche: A Formal Validation Model for the Netconf Protocol.
DSOM 2004: 147-158 [9] Hodkinson, I. M.: Loosely Guarded Fragment of First-Order Logic has the Finite Model Property.
Studia Logica 70(2): 205-240 (2002) [10] Pepelnjak, I., Guichard, J.: MPLS VPN Architectures, Cisco Press (2001) [11] Rekhter, Y., Li, T.: An Architecture for IP Address Allocation with CIDR.
RFC 1518 (1993) [12] Rosen, E., Rekhter, Y.: BGP/MPLS VPNs.
RFC 2547 (1999) [13] Scott, C., Wolfe, P. Erwin, M.: Virtual Private Networks, O'Reilly (1998) [14] B.A.
Trakhtenbrot, Impossibility of an algorithm for the decision problem in finite classes, Dok.
Akad.
Nauk SSSR 70 (1950) 569-572.
Propagating Possibilistic Temporal Constraints Rasiah Loganantharaj  1 Introduction  Automated Reasoning laboratory The Center for Advanced Computer Studies University of SouthWestern Louisiana Lafayette, LA - 70504  The notion of time plays an important role in any intelligent activities.
Time is represented either implicitly or explicitly.
We are interested in explicit representation of time.
The popular approaches for such representation are based on points or intervals or a hybrid of both.
Propositional temporal assertions are represented as relations among the points, or among the intervals.
The indenite information among either the points or the intervals are represented as disjunctions.
In real world, information is often incomplete, imprecise, uncertain and approximate.
Temporal knowledge is not an exception to this reality.
For example, consider the following information: John often drinks coee during his breakfast.
Sometimes, he drinks his coee before the breakfast and drinks orange juice during his breakfast.
There were few occasions he drank water during his breakfast and drank coee after the breakfast.
Mike talked to John over the phone while John was having coee.
Suppose, we are interested in nding out how Mik's telephone conversation was related to John's breakfast.
From the given information, we have denite relation between the telephone call and John's coee, but there is no information about the relationship between the coee and the breakfast on that particular day.
In the absence of such information, we can use John's habitual pattern to infer plausible relations.
Suppose, Ic and Ib respectively represent the interval over which John was having coee, and John was having breakfast.
Let It represents the interval over which Mike was having telephone conversation with John.
In interval logic, the information is represented as It is during Ic , and Ic is before or during or after Ib .
In such representation, the disjunctive relations do not provide any clue about which relation is highly probable than the others.
Instead, the representation may let us believe that all the relations of a disjunction have equal probability, for example, having coee before, during or after breakfast has equal probability.
This is not what the original information tells us.
Based on John's habit, having coee during breakfast is much more probable than having coee either before or after the breakfast.
This issues have not been studied in temporal reasoning.
In  this paper we will provide a representation to specify uncertain information and to propagate them over temporal constraint network.
This paper is organized as the following.
We introduce interval-based logic in Section 2.
In Section 3, we describe the representation of uncertain temporal knowledge and its propagation.
This paper is concluded by a summary and discussion in section 4.
2 Background on Interval Based System Allen 1] has proposed an interval logic that uses time intervals as primitives.
In this logic, the following seven relations and their inverses are dened to express the temporal relations between two intervals: before(after), meets(metby), overlaps(overlapped-by), starts(started-by), during(contains), ends(ended-by), and equals.
Here, the inverse relations are indicated within parentheses.
Since the inverse of equal is same as itself, there are, in fact, only thirteen relations.
Temporal inferencing is performed by manipulating the network corresponding to the intervals.
Each interval maps onto a node of a network called temporal constraint network (TCN).
A temporal relation, say R, from an interval, say Ii , to another interval, say Ij , is indicated by the label Rij on the directed arc from Ii to Ij .
Obviously, the label Rji of the directed arc from Ij to Ii is the inverse relation of Rij .
If we have denite information about the relation Ii to Ij then Rij will be a primitive interval relation, otherwise it will be disjunctions of two or more primitive interval relations.
Suppose the relation Ii to Ij (Rij ) and the relation Ij to Ik (Rjk ) are given.
The relation between Ii and Ik , constrained by Rij and Rjk , is given by composing Rij and Rjk .
In general, Rij and Rjk can be disjunctions of primitive relations, they are represented as: 1  r2  : : : rn g Rij = frij1  rij2  : : : rijm g and Rjk = frjk jk jk where rij is one of the primitive relations dened in the system.
The interval Ii is related to the interval Ik by the temporal relation given by the following expression:  Rij  Rjk =  (  p) (rijl  rjk  )  l=1:::m p=1:::n p l where (rij  rjk) is a composition (transitive relap , and is obtained from the entry tion) of rijl and rjk j of the transitivity table 1] at the riji row and the rjk  column.
Alternatively this could be written as Rij  Rjk = fT(rij  rjk)jrij 2 Rij ^ rjk 2 Rjkg where T(rij  rjk) is the value of Allen's look up composition table of row rij and column rjk .
Temporal constraints are propagated to the rest of the network to obtain the minimal temporal network in which each label between a pair of intervals is minimal with respect to the given constraints.
Vilain et al.
7] have shown that the problem of obtaining minimal labels for an interval-based temporal constraint network is NP-complete.
Approximation algorithms, however, are available for temporal constraint propagation.
Allen proposed an approximate algorithm that has an asymptotic time complexity of O(N 3 ) where N is the number of intervals.
His algorithm is an approximate one in the sense that it is not guaranteed to obtain the minimum relations, but it always nd the superset of the minimal label.
Since any set is a super set of a null set it is not very comforting because it is possible that global inconsistency may be hiding under 3-consistency.
3 Representation of Uncertainty  The problem of uncertainty is not new to AI problem solving.
In many expert system applications, uncertainty have been studied under approximate reasoning.
Mycin system 6, 2] has used certainty factor whose value varies from -1 to 1 through 0 to represent the condence on an evidence or on a rule.
The value 1 indicates the assertion is true while the value -1 indicates the evidence is false.
0 indicates no opinion on the evidence.
The other values correspond to some mapping of the belief on the evidence onto the scale of -1 to 1.
Prospector model 4, 5] uses probabilistic theory with Bayes' theorem and other approximation techniques to propagate evidences over causal network.
Fuzzy logic 8] has also been used in expert systems to capture knowledge with fuzzy quantiers such as `very much', `somewhat' etc.
Other techniques have also been used to handle uncertainty in expert systems.
Let us look back at the same example presented in the introduction of this paper.
John often drinks coee during his breakfast.
Sometimes he drinks his coee before the breakfast and drinks orange juice during his breakfast.
There were few occasions he drank water during his breakfast and drank coee after the breakfast.
The statement has fuzzy quanties indicating that the frequency of John having coee during his breakfast is much higher than he is having coee either before or after his breakfast.
We should capture the fuzzy quantiers into probabilistic measures in our temporal constraint representation such that the summation of the probabilities of the relations between a pair of intervals is equal to 1.
Let us represent this idea more formally.
The relation Rij , the relation Ii to Ij , is represented as frij1 (w1) rij2 (w2) ::: rijm(wm )g where rijl (wl ) is a primitive relation with its probability or the relative weight of wl .
Since each primitive relation between a pair of intervals is associated with a weight to represent the probability or the relative strength, the summation of the weights must be equal to 1 which we P call probabilistic condition for the weights.
That is, mi=1 wi = 1.
The boundary value of the weight 1 indicates that the relation is true while the value 0 indicates that the relation is false.
Further, the inverse relation of Rij will be the inverse of all the primitive relations of Rij with the same weights.
In the presence of new evidences, the probability values of the relations are modied to take account of the new evidences.
That is, when the relations between a pair of intervals are modied or rened because of other constraining relations, the probability or the weights of the relations are adjusted to satisfy the probabilistic condition.
This process is called normalization.
Let us explain this with an example: Suppose (1) R12 = fb(0:6) O(0:3) d(0:1)g, (2) R13  R32 = fb(0:4) o(0:5) m(0:1)g. The relation R12 is rened to fb(w1) o(w2)g. The weights w1 and w2 are computed using the intersection operation and then the weights are normalized such that w1 + w2 = 1.
When propagating constraints with probabilistic weights, we may need to dene such as union, intersection, composition and normalization operations which are used in propagation.
0  0  Union operation:  Rij p Rij = fr(w)jr(wij ) 2 Ri ^ r(wij ) 2 Rj ^ w = max(wij  wij )g 0  0  0  Intersection Operation:  Rij \p Rij = fr(w)jr(wij ) 2 Ri ^ r(wij ) 2 Rj ^ w = min(wij  wij )g 0  0  0  Composition Operation: Rij  Rjk =  l (wl ) 2 Rij ^ rm (wm ) 2 Rjk fp r(w)jrij jk l  rm ) ^ w = min(wl  wm )g ^ r = T(rij jk  Normalization operation: Suppose the label Rij takes the following form after renement 1 (w ) r2 (w ) ::: rm(w )g. Let w = Pm w .
frij 1 ij 2 ij m i=1 i After normalization operation the label becomes frij1 (w1) rij2 (w2) :::Prijm(wm )g where wi = wi=w which ensures that mi=1 wi = 1 In this approach we use possibilistic ways of combin0  0  0  0  0  ing the constraints as has been used in many expert systems under uncertainty.
3.1 Temporal Propagation  A temporal constraint network (TCN) is constructed from the given temporal assertion such that each node of the constraint network represents each interval of the temporal assertions.
The labels on each arc of a TCN corresponds to the relations between the corresponding intervals.
Further the summation of the weights of each component in each arc should be equal to 1.
If no constraint is specied between a pair of intervals it will take a universal constraint1 as label in the TCN and it will not be used for the purpose of propagation.
When propagating a constraint, other label of the network may get updated to a subset of its label.
The process of updating of a label as a result of propagating a constraint is called label renement.
The label renement takes the following forms: (1) The weights of the labels of an arc get changed, or (2) the primitive relations of a label get reduced.
In either case normalization operation is performed to ensure the summation of the weights adds to 1.
Suppose we are propagating the label Rij of the arc < I J > to the arc < I K > of the triangle IJK.
Let the labels of the arcs < J K > and < I K > are respectively Rjk and Rik.
The new label of the arc < I K > is computed as Rik \ fRij  Rjkg.
When the new relation is not null the weights on the label are normalized.
3.2 Temporal Constraints Propagation Algorithm for uncertain constraints  This is an extension of Allen's propagation algorithm.
The algorithm uses a rst in rst out (FIFO) queue to maintain the constraints that need to be propagated.
Initially all the pairs of constrained intervals are placed into the queue.
The propagation of constraints is initiated by removing an arc, say < Ii  Ij >, from the queue and checking whether the relation between Ii to Ij can constrain the relations on all the arcs incident to either Ii or Ij except the arc < Ii Ij >.
When a new relation is constrained, that is, the old label is modied, the arc (the pair of intervals related by this relation) is placed in the queue.
The main propagation algorithm is described in Figure 1.
In this algorithm, we use the notation Rij to denote the label of the arc i to j.
When we omit the weights on the labels and use the union, the intersection operations of set, and the composition operation of temporal logic, the algorithm becomes identical to the one of Allen's 1].
One may expect the asymptotic complexity to be O(N 3 ) where N is the number of nodes of the TCN.
Intuitively one may make a conclusion that this algorithm 1 a universal constraint is the weakest constraint and thus it is a disjunctions of all the primitive relations of the time model  will also converge in O(N 3) time.
This may be misleading because we have not yet considered the instability eect of the algorithm due to it normalization operation.
An arc is placed back in the queue when either the number of primitive relations of the label is reduced or the weight of the primitive relation of the label is modied even when the label remains unchanged.
An arc may be placed in the queue at most 12 times as a result of the disjunctive relations being rened one at a time till it becomes a singleton relation.
On the other hand, the number of times an arc is placed in the queue due to the change of weight depends on the following parameters: (1) the resolution of the weights (the number of decimal places that counts) and (2) the error bound we are prepared to tolerate.
A complete study on these issues can be found in one of our report 3].
Let us consider an example.
The probability of `having coee' before breakfast is 0.15, during breakfast is 0.8 and after breakfast is 0.05.
The probability of `having a coee' overlapping `reading morning newspaper' is 0.8 and `having coee' meets `reading the newspaper' is 0.2.
Suppose we want to nd the relation between having breakfast and reading newspaper.
Let us propagate the constraints and nd out how the labels gets rened.
Suppose Ib  Ic  Ir respectively represent the intervals of having breakfast, coffee and reading newspaper.
Using the notations dened in this paper we can dene the labels of the initial TCN as following.
Initial TCN Rcb = fb(0:15) d(0:8) a(0:05)g Rcr = fo(0:8) m(0:2)g Rbr = to be computed TCN after propagating Rbc to < Ib  Ir > Rcb = fb(0:15) d(0:8) a(0:05)g Rcr = fo(0:8) m(0:2)g Rbr = fo(:8) oi(:15) d(:15)di(:8)f(:15) fi(:8) b(:05) a(:15)g After normalizing Rbr Rbr = fo(:26) oi(:049) d(:049) di(:26)f(:049) fi(:26) b(:024) a(:049)g This is also the 3-consistent TCN labels.
4 Summary and Discussion  In many real world applications we are faced with the information that is incomplete, indenite, imprecise and uncertain.
When explicit time was used for temporal reasoning, indenite information is accommodated as disjunctions.
For example, drinking coffee is either before, during or after the breakfast.
The disjunctive information implicitly assume equal probability of occurrence even though exactly one can be true between a pair of intervals (points).
In such representation we fail to distinguish or dierentiate the highly probable one from the remotely possible one.
According to our example, having coee during breakfast is highly probable than having coee before  procedure propagate1() 1 While queue is not empty Do 2  f  3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  g  get next < i j > from the queue /* propagation begins here */ For ( ( k 2 set of intervals ) ^k 6= i ^ k 6= j )Do f temp 	 Rik \p (Rij  Rjk) If temp is null Then signal contradiction and Exit Normalize temp If Rik 6= temp Then f place < i k > on queue Rik 	 temp g temp 	 Rkj \p (Rki  Rij ) If temp is null Then signal contradiction and Exit Normalize temp If Rkj 6= temp Then f place < k j > on queue Rkj 	 temp g  g  Figure 1: propagation algorithm or after a breakfast.
In this paper we have proposed a formalism to represent temporal constraints with the associated probabilistic weights and use them to propagate to the rest of the network to obtain 3-consistency.
We have extended Allen's algorithm to handle probabilistic relations among intervals.
The operations we have dened for the labels with weights are applicable to both the points and the intervals.
Therefore, our formalism will be applicable to both the points and the intervals.
Acknowledgement  This research was partially supported by a grant from Louisiana Education Quality Support Fund (LEQSF).
References  1] J. F. Allen.
Maintaining knowledge about temporal intervals.
Communications of ACM, 26(11):832{843, 1983.
2] B. G. Buchanan and E. H. Shortlie Eds.
RuleBased Expert Systems.
Addision-Wesley, 1984.
3] R. Loganantharaj.
Complexity issues of possibilistic temporal reasoning.
preperation, 1994.
4] J. Gaschnig R. O. Duda and P. E. Hart.
Model design in the prospector consultant system for mineral exploration.
In D. Michie, editor, Expert Systems in the Microelectronics Age.
Edinburgh University Press, 1979.
5] P. E. Hart R. O. Duda and N. J. Nilsson.
Subjective bayesian methods for rule-based inference  systems.
In Proceedings of the AFIPS National Computer Conference, volume 45, 1976.
6] E. H. Shortlie.
Computer Based Medical Consultations:MYCIN.
Elsvier, 1976.
7] M. Vilain and H. Kautz.
Constraint propagation algorithm for temporal reasoning.
In Proceedings of AAAI-86, pages 377{382, 1986.
8] L. A. Zadeh.
Making computers think like people.
IEEE Spectrum, pages 26{32, August 1984.
On the freeze quantifier in Constraint LTL: decidability and complexity [?]
arXiv:cs/0609008v2 [cs.LO] 29 Sep 2006  Stephane Demri a,1, Ranko Lazic b,2, David Nowak c,3 a LSV,  CNRS & ENS Cachan & INRIA Futurs (projet SECSI), France  b Department c Research  of Computer Science, University of Warwick, United Kingdom  Center for Information Security, National Institute of Advanced Industrial Science and Technology, Japan  Abstract Constraint LTL, a generalisation of LTL over Presburger constraints, is often used as a formal language to specify the behavior of operational models with constraints.
The freeze quantifier can be part of the language, as in some real-time logics, but this variable-binding mechanism is quite general and ubiquitous in many logical languages (first-order temporal logics, hybrid logics, logics for sequence diagrams, navigation logics, logics with l-abstraction etc.).
We show that Constraint LTL over the simple domain hN, =i augmented with the freeze quantifier is undecidable which is a surprising result in view of the poor language for constraints (only equality tests).
Many versions of freeze-free Constraint LTL are decidable over domains with qualitative predicates and our undecidability result actually establishes S11 completeness.
On the positive side, we provide complexity results when the domain is finite (ExpSpace-completeness) or when the formulae are flat in a sense introduced in the paper.
Our undecidability results are sharp (i.e.
with restrictions on the number of variables) and all our complexity characterisations ensure completeness with respect to some complexity class (mainly PSpace and ExpSpace).
Key words: Linear-time temporal logic, Constraints, Freeze quantifier, Decidability, Computational complexity  [?]
This paper is an extended version of [1].
1 Supported by the ACI "Securite et Informatique" Cortos.
2 Supported by an invited professorship from ENS Cachan, and by grants from the EPSRC (GR/S52759/01) and the Intel Corporation.
Also affiliated to the Mathematical Institute, Serbian Academy of Sciences and Arts, Belgrade.
3 Supported by the ACI "Securite et Informatique" Persee and by the e-Society project of MEXT.
A part of this work was done while affiliated to LSV, CNRS & ENS Cachan, and Department of Information Science, University of Tokyo.
Preprint submitted to Information and Computation  14 January 2014  1  Introduction  Model-checking for infinite-state systems.
Temporal logics are wellstudied formalisms to specify the behavior of finite-state systems and the computational complexity of the model-checking problems is nowadays wellknown, see e.g.
a survey in [2].
However, many systems such as communication protocols have infinitely many configurations and usually the techniques for the finite case cannot be applied directly.
For numerous infinite-state systems, the model-checking problem for the linear-time temporal logic LTL can be easily shown to be undecidable (counter automata, hybrid automata and more general constraint automata [3, Chapter 6]).
Actually, simpler problems such as reachability are already undecidable.
However, remarkable classes of infinite-state systems admit decidable model-checking problems, such as timed automata [4] and subclasses of counter automata [5,6,7,8,9].
For instance, fragments of LTL with Presburger constraints have been shown decidable over appropriate counter automata [10,11].
In order to push further the decidability border, one way consists in considering larger classes of operational models, see e.g.
[5].
Alternatively, enriching the specification language is another possibility.
In the paper, we are interested in studying systematically the extensions of versions of LTL over concrete domains by the so-called freeze quantifier, and in analysing the consequences in terms of decidability and computational complexity.
A variable-binding mechanism.
The freeze quantifier in real-time logics has been introduced by Alur and Henzinger in the logic TPTL, see e.g.
[12].
The formula x * ph(x) binds the variable x to the time t of the current state: x * ph(x) is semantically equivalent to ph(t).
Alternatively, in the explicit clock approach [13], there is an explicit clock variable t and even though in this approach the freeze variable-binding mechanism is possible, the logical formalisms from [12] and [13] are incomparable.
In this paper, we want to extend some of the decidable logics from [10,11,14] to admit the freeze quantifier: |y=x ph(y) holds true at a state iff ph(y) holds true at the same state with y taking the value of x.
Here, y can be in the scope of temporal operators.
A crucial difference with the logics in [12,13] rests on the fact that the variable x may not be monotonic.
We focus on decidability and complexity issues when the language of constraints (at the atomic level of the logics) is very simple in order to isolate the effects of the freeze quantifier.
We know for instance that LTL over integer periodicity constraints augmented with the freeze quantifier is ExpSpace-complete [14].
The above-mentioned variable-binding mechanism that allows the binding of logical variables to objects is very general and it has been used in the literature for various purposes.
Details will be provided along the paper (see e.g.
Sections 2  2.2 and 5).
In particular, one can see flexible variables as processes, values of the domain as resources, and the freeze quantifier and rigid variables as ways to extract and store the current resource used by a process.
This view is nicely illustrated in [15] by the specification of a communication protocol.
In Section 2.2, we consider the case of a process requesting memory blocks.
Our contribution.
In the paper, we analyse decidability and complexity issues of Constraint LTL augmented with the freeze quantifier.
The temporal operators we consider are restricted to the standard future-time operators 'until' and 'next' (no past-time operators).
CLTL| (D) denotes such a logic over the concrete domain D. A concrete domain is composed of a non-empty set equipped with a family of relations.
The atomic formulae of CLTL| (D) are based on constraints over D with the ability to compare values of variables at states of bounded distance (see details in the body of the paper) as done in [16,17,11,18].
First, we show that when the underlying domain D is finite, CLTL| (D) satisfiability is in ExpSpace.
If moreover D has at least two elements with the equality predicate, then CLTL| (D) is ExpSpace-hard.
As a corollary, CLTL| (D, =) satisfiability is ExpSpace-complete when |D| >= 2 and D is finite (Section 3.2).
This witnesses an exponential blow-up since satisfiability for the freeze-free fragment CLTL(D) when D is finite can be easily shown in PSpace as plain LTL [19].
When the domain D is infinite, we show that CLTL| (D, =) is undecidable which is the main result of the paper (Section 4).
This is quite surprising since the language of constraints is poor (only equality tests) and only futuretime operators are used unlike what is shown in [14, Section 7] with past-time operators.
Our proof, based on a reduction from the Recurrence Problem for 2-counter machines, refines this result: CLTL| (D, =) is S11 -complete even if only one flexible variable and two rigid variables (used to record the values of flexible variables) are involved.
Hence, in spite of the very basic Presburger constraints in CLTL| (N, =), satisfiability is S11 -complete.
Decidability of CLTL| (D) can be obtained either at the cost of syntactic restrictions or by assuming semantical constraints (as in the logic TPTL [12] where the freeze quantifier can only record the value of a monotonic variable, namely time).
In order to regain decidability, we introduce the flat fragment of CLTL| (D) which contains the freeze-free fragment CLTL(D) and we show that there is a logarithmic-space reduction from the flat fragment of CLTL| (D) into CLTL(D) assuming that the equality predicate belongs to D. As a corollary, we obtain that the flat fragments of CLTL| (Z, <, =) and CLTL| (R, <, =) are PSpace-complete (Section 3.2).
Flat fragments of plain LTL versions have been studied in [20,10] (see also in [21, Section 5] the design of a flat logical 3  temporal language for model-checking pushdown machines) and our definition of flatness takes advantage in a non-trivial way of the polarity of 'until' subformulae occurring in a formula.
This is a standard way to restrict the interplay between modalities and quantifiers, see e.g.
[22,10,23].
Although we do not claim that flat formulae are especially interesting in practice, they cover nontrivial uses of the freeze quantifier.
However, they cannot express the property that a variable at distinct points takes distinct values.
Along the paper, we consider the satisfiability problem, but as shown in Section 2.3, our results extend to the model-checking problem.
CLTL| (D) extends naturally the freeze-free fragment CLTL(D), and we show that it increases strictly the expressive power (Proposition 1).
However, we prove that significant fragments of CLTL| (D) are as expressive as the full language, for instance by recording only values of flexible variables at the current state or by allowing only rigid variables in atomic formulae (see Section 2.4).
Apart from the technical contributions of the paper, we provide comparisons with several works which involve freeze-like operators, such as in first-order quantification, in timed LTL, in hybrid logics with reference pointers, to quote a few examples.
Structure of the paper.
In Section 2, we present Constraint LTL with the freeze quantifier, satisfiability and model-checking problems of interest, and consider relative expressivity.
Section 3 contains decidability and complexity results when the underlying concrete domain is finite or with restricting to the flat fragment.
In Section 4, we show that CLTL| (N, =) is S11 -complete.
Related work is discussed in Section 5.
In Section 6, we conclude and enumerate a few open problems.
2  Constraint LTL with the freeze quantifier  2.1 Syntax and semantics A constraint system is a set, called the domain, with a countable family of relations on this set.
Let D = (D, (Ri )i[?
]I ) be a constraint system.
We define the logic CLTL| (D) by giving its syntax and semantics.
Syntax.
Let FleVarSet and RigVarSet be countable sets of variables which are respectively called flexible variables and rigid variables.
Terms are given 4  by the grammar: t ::= X * * X} x | y | *{z n times  where x is in FleVarSet and y is in RigVarSet.
We use Xn as an abbreviation for |X *{z* * X}.
Formulae are given by the grammar: n times  ph ::= R(t1 , .
.
.
, tn ) | !ph | ph1 [?]
ph2 | Xph | ph1 Uph2 ||y=Xn x ph  where R ranges over the predicate symbols associated to the relations in (Ri )i[?
]I , x over FleVarSet, and y over RigVarSet.
Note that we use X for denoting either the nth next value Xn x of the variable x or the formula Xph.
We define the Boolean constants, and the temporal operators 'sometimes' and def 'always', as the following abbreviations: [?]
= R(t1 , .
.
.
, tn ) [?]
!R(t1 , .
.
.
, tn ), def def def Fph = [?
]Uph, [?]
= R(t1 , .
.
.
, tn ) [?]
!R(t1 , .
.
.
, tn ), and Gph = !F!ph.
Let FleVars(ph) and RigVars(ph) denote the sets of all flexible and rigid (respectively) variables which occur in ph.
Freeze-free fragment.
CLTL(D) is the fragment of CLTL| (D) with no rigid variables and hence without freeze quantifier.
Flat fragment.
We say that the occurrence of a subformula in a formula is positive if it occurs under an even number of negations, otherwise it is negative.
The flat fragment of CLTL| (D) is the restriction of CLTL| (D) where, for any subformula ph1 Uph2 , if it is positive then | does not occur in ph1 , and if it is negative then | does not occur in ph2 .
More precisely, the flat fragment consists of the following formulae ph.
Subformulae ph are positive, whereas subformulae ph- are negative.
ph ::= R(t1 , .
.
.
, tn ) | !ph- | ph1 [?]
ph2 | Xph | psUph ||y=Xn x ph - - - - ph- ::= R(t1 , .
.
.
, tn ) | !ph | ph- 1 [?]
ph2 | Xph | ph Ups ||y=Xn x ph  ps ::= R(t1 , .
.
.
, tn ) | !ps | ps1 [?]
ps2 | Xps | ps1 Ups2  Semantics.
A model s : N - (FleVarSet - D) is a sequence of mappings from FleVarSet to D. For any i [?]
N, we write s i for the model defined by s i (j) = s(i+j) for every j >= 0.
An environment r is a mapping from RigVarSet to D. We write r[x 7- v] for the environment mapping x to v [?]
D, and any 5  other variable y to r(y).
The semantics of terms is given by: JXn xKs,r = s(n)(x) if x is in FleVarSet JyKs,r = r(y)  if y is in RigVarSet  The semantics of formulae is given by the following satisfaction relation.
(Note that we use R for both a relation symbol and the relation it denotes.)
* * * * * *  s s s s s s  |=r R(t1 , .
.
.
, tn ) iff (Jt1 Ks,r , .
.
.
, Jtn Ks,r ) [?]
R, |=r !ph iff s 6|=r ph, |=r ph1 [?]
ph2 iff s |=r ph1 and s |=r ph2 , |=r Xph iff s 1 |=r ph, |=r ph1 Uph2 iff there exists i such that s i |=r ph2 and for all j < i, s j |=r ph1 , |=r |y=Xn x ph iff s |=r[y7-s(n)(x)] ph.
2.2 Examples  As a first example, consider the formula phx[?]
= G |y=x XG x 6= y def  which states that the values of the variable x at different points in time are mutually distinct.
This is interesting for the verification of cryptographic protocols, where nonces are variables which have to be fresh, i.e.
they cannot take twice the same value.
As a second example, we consider a process requesting memory blocks.
Let us assume two flexible variables o (for operator) and a (for argument) such that o takes its values in the finite domain {Malloc, Access, Free} and a takes its values in an infinite set of memory locations.
We use Malloc(x), Access(x) and Free(x) as respective abbreviations for o = Malloc [?]
a = x, o = Access [?]
a = x, and o = Free [?]
a = x (x is a rigid variable).
We can easily express the following properties in CLTL| (D).
* As soon as a memory location is freed, either it is never accessed again, or it is not accessed until it is allocated again: G(o = Free = |x=a (G!Access(x) [?]
!Access(x)UMalloc(x)))  6  * When a memory location is allocated, it will either be freed in the future or will always be eventually accessed (so that we do not waste memory): G(o = Malloc = |x=a (FFree(x) [?]
GFAccess(x)))  2.3 Satisfiability and model-checking problems  We recall below the problems we are interested in.
Satisfiability problem for CLTL| (D): instance: a CLTL| (D) formula ph; question: is there a model s and an environment r such that s |=r ph?
Without loss of generality we can assume that no rigid variable occurs free in ph, which means that r is not essential above.
The model-checking problem rests on D-automata which are constraints automata.
A D-automaton is simply a Buchi automaton with alphabet a finite set of Boolean combinations of atomic CLTL| (D) formulae with terms of the form x and Xx (x [?]
FleVarSet).
In a D-automaton, letters on transitions induce constraints between the variables of the current state and the variables of the next state as done in [10].
Alternatively, labelling the transitions by CLTL| (D) formulae (as done in [24]) would not modify essentially the decidability status of model-checking problems considered in this paper.
Model-checking problem for CLTL| (D): instance: a D-automaton A and a CLTL| (D) formula ph; question: are there a symbolic o-word v = ph0 , ph1 , .
.
.
accepted by A, a model s (a realisation of v) and an environment r such that s |=r ph and for every i >= 0, s i |=r phi ?
It is not difficult to show that as soon as D is non-trivial the satisfiability problem and the model-checking problem are reducible to each other in logarithmic space following techniques from [19].
In the sequel, we prove results for the satisfiability problem but one has to keep in mind that our results extend to the model-checking problem.
2.4 Expressive power  The freeze quantifier strictly increases expressive power.
In order to show formally that the freeze quantifier is powerful, we show that CLTL| (N, =) is strictly more expressive than its freeze-free fragment CLTL(N, =).
In fact, 7  phx[?]
is an example of a formula ph in CLTL| (N, =) with no free rigid variable for which there is no equivalent formula ps in CLTL(N, =).
The result will follow from the following property.
Lemma 1 Every satisfiable formula ph in CLTL(N, =) has a model which contains only finitely many distinct values.
Moreover, the number of distinct values is polynomial in |ph|.
Proof.
Let ph be a formula in CLTL(N, =) with variables in {x1 , .
.
.
, xn } and k be equal to 1 plus the maximal j such that Xj xi occurs in ph for some flexible variable xi .
Let C be the finite set of constraints of the form Xj1 xi1 = Xj2 xi2 with 0 <= j1 , j2 <= k - 1 and i1 , i2 [?]
{1, .
.
.
, n}.
We define a total ordering on {1, .
.
.
, n} xN as follows: hi, ji < hi' , ji iff j < j ' or (j = j ' and i < i' ).
Given a model s : N - (FleVarSet - N), we build a model s ' : N - (FleVarSet - {1, .
.
.
, k x n}) such that s |= ph iff s ' |= ph.
If x is a flexible variable not occurring in ph, s ' (i)(x) = 1 for every i >= 0.
Otherwise s ' (0)(x1 ) = 1 (h1, 0i is minimal wrt <).
Now suppose that for every hi' , j ' i < hi, ji, s ' (j ' )(xi' ) has been already defined.
We shall define s ' (j)(xi ).
If for some hi' , j ' i in {hi'' , j '' i : 0 <= j -j '' <= k -1, 1 <= i'' <= n, hi'' , j '' i < hi, ji}, s(j ' )(xi' ) = s(j)(xi ) then s ' (j)(xi ) takes the value s ' (j ' )(xi' ).
Otherwise, s ' (j)(xi ) takes an arbitrary value from the set {1, .
.
.
, k x n} \ {s(j '' )(xi'' ) : 0 <= j - j '' <= k - 1, 1 <= i'' <= n, hi'' , j '' i < hi, ji} which is always possible since the second set has strictly less that k x n ele' ments.
One can show that for all c [?]
C and i >= 0, s i |= c iff s i |= c. Hence, s |= ph iff s ' |= ph.
Proposition 1 No formula of CLTL(N, =) is equivalent to the formula phx[?]
of CLTL| (N, =).
The flatness concept is only related to occurrences of the freeze quantifier and for instance the formulae of the form phx[?]
do not belong to the flat fragment.
By contrast, !phx[?]
belongs to the flat fragment of CLTL| (N, =).
By Proposition 1, the flat fragment of CLTL| (N, =) is therefore strictly more expressive than CLTL(N, =) since CLTL(N, =) is closed under negation.
Equivalent syntactic restrictions.
We now show that expressiveness of CLTL| (D) does not change if we restrict the freeze quantifier to refer only to flexible variables in the current state, or if we restrict atomic formulae to contain only rigid variables, or with both restrictions.
Therefore, those restrictions could have been incorporated into the definition of the logic.
However, 8  we chose to allow terms of the form Xn x with flexible x in atomic formulae in order to have CLTL(D) as the freeze-free fragment, and to allow the freeze quantifier to refer to the future so that formulae would be closed under substitution of terms.
Proposition 2 For any formula ph of CLTL| (D), there exists an equivalent formula ph' such that: (I) any occurence of | in ph' is of the form |y=x ; (II) FleVars(ph' ) = FleVars(ph); (III) RigVars(ph' ) = RigVars(ph).
Proof.
By structural induction on ph, it suffices to prove the statement for formulae of the form |y=Xn x ph' where ph' satisfies (I).
This can be done by induction on n. The base case n = 0 is trivial.
For the inductive step, we use structural induction on ph' .
The most difficult case is ph' = ph'1 Uph'2 .
We then have |y=Xn+1 x ph' [?]
|y=Xn+1 x ph'2 [?]
(ph'1 [?]
Xph' ) [?]
(|y=Xn+1 x ph'2 ) [?]
((|y=Xn+1 x ph'1 ) [?]
X |y=Xn x ph' )  and the induction hypotheses apply to each of the three freeze subformulae.
It is worth observing that in the worst case, in the proof of Proposition 2, ph' can be exponentially larger than ph.
Proposition 3 For any formula ph of CLTL| (D), there exists an equivalent formula ph' such that: * * * *  atomic formulae in ph' contain only rigid variables; if any occurence of | in ph is of the form |y=x , then the same is true of ph' ; FleVars(ph' ) = FleVars(ph); |RigVars(ph' )| = max{|RigVars(ph)|, k}, where k is the maximum number of distinct terms in any atomic subformula of ph.
Proof.
ph' is constructed from ph by translating only atomic subformulae of ph.
For example, R(X2 x1 , y1, X3 x2 , X2 x3 , x4 , y2 , x4 ), where xi [?]
FleVarSet and yi [?]
RigVarSet, is translated to |y3 =x4 X2 |y4 =x1 |y5 =x3 X1 |y6 =x2 R(y4 , y1 , y6 , y5, y3 , y2 , y3 ) 9  where y3 , .
.
.
, y6 are drawn from RigVars(ph) \ {y1 , y2 }.
If that set does not have enough elements, new rigid variable names are used.
The latter can then be reused in translations of other atomic subformulae.
Flexible and finitary variables.
If the domain D has at least two elements, and if the equality predicate is present, then formulae and models of CLTL| (D) with n >= 2 flexible variables can be translated to the fragment with only one flexible variable.
Proposition 4 Let D be a constraint system with at least two elements and equality.
For any formula ph of CLTL| (D), one can compute in logarithmic space a formula ph' of CLTL| (D) with a unique flexible variable and the same set of rigid variables as ph, such that ph is satisfiable iff ph' is satisfiable.
Proof.
Let ph be a formula of CLTL| (D) with flexible variables x1 , .
.
.
, xn .
We shall build in logspace a formula ph' of CLTL| (D) with only one flexible variable x' and the same set of rigid variables as ph, such that s ' |=r ph' iff there exists s with s |=r ph and s ' is an encoding of s in the following sense.
A valuation s(i) : {x1 , .
.
.
, xn } - D is encoded by 2n + 4 consecutive values of x' in s ' which form a sequence di1 , di0 , di0 , di0 , di1 , s(i)(x1 ), di2 , s(i)(x2 ), .
.
.
, din , s(i)(xn ) Using the equality predicate, the values dij are constrained in ph' so that three consecutive equal values occur in s ' only at the beginnings of sequences which encode valuations in s. The formula ph' is a conjunction phenc [?
]T (ph) where phenc enforces that models are sequences of length 2n+4 of the above form (details are omitted here).
Formula T (ph) is inductively defined as follows where start = X(x' = Xx' [?]
x' = XXx' ): * T (R(t1 , .
.
.
, tm )) = R(T (t1 ), .
.
.
, T (tm )) where T (y) = y if y is rigid and T (Xk xi ) = Xkx(2n+4)+3+2i x' , * T is homomorphic for Boolean connectives, * T (|y=Xk xi ph1 ) =|y=T (Xk xi ) T (ph1 ), * T (ph1Uph2 ) = (start = T (ph1 ))U(start [?]
T (ph2 )), * T (Xph1) = X2n+4 T (ph1 ).
The logics CLTL| (D) as defined in Section 2.1 do not in general have propositional variables.
If D has at least two elements and equality, then propositional flexible variables, or a flexible variable ranging over a finite alphabet, can be encoded using additional flexible variables over D and equality.
A translation as above can then be employed to reduce the number of flexible variables.
10  For ease of expression, to avoid unnecessary constructs, and because equality on the domain is not necessarily present, arbitrarily many flexible variables and no special finitary variables are considered in the rest of the paper.
3  Decidability results  3.1 Finite domain case  In this section, we basically show that, when D is finite (with at least two elements) and contains the equality predicate, CLTL| (D) is ExpSpace-complete.
In Theorem 1 below, we establish that ExpSpace-hardness is very common when the freeze quantifier is present.
Theorem 1 Let D be a constraint system with equality such that the underlying domain D contains at least two elements.
The satisfiability problem for CLTL| (D) is ExpSpace-hard.
Proof.
We prove this result by a reduction from an ExpSpace-complete tiling problem (see e.g.
[25]).
A tile is a unit square of one of several types and the tiling problem we consider is specified by means of a finite set T of tile types (say T = {t1 , .
.
.
, tl }), two binary relations H (horizontal matching relation) and V (vertical matching relation) over T and two distinguished tile types tinit , tf inal [?]
T .
The problem consists in determining whether, for a given number n in unary, the region [0, .
.
.
, 2n - 1] x [0, .
.
.
, k - 1] of the integer plane for some k can be tiled consistently with H and V , tinit is the left bottom tile, and tf inal is the right upper tile.
Given an instance I = hT, tinit , tf inal , ni of the tiling problem, we build a CLTL| (D) formula phI such that I = hT, tinit , tf inal , ni has a solution iff phI is CLTL| (D) satisfiable.
We consider the following flexible variables: * c1 , .
.
.
, cn are variables that allow to count until 2n and x0 , x1 are variables that will play the role of 0 and 1, respectively; there are corresponding rigid variables c'1 , .
.
.
, c'n ; each element ha, ii of a row [0, .
.
.
, 2n - 1] x {i} such that the binary representation of a is b1 .
.
.
bn , satisfies cj = x0 iff bj = 0 for every j [?]
{1, .
.
.
, n}; * for t [?]
T , zt1 , zt2 are variables such that Dt := zt1 = zt2 is the formula encoding the fact that at a certain position of the integer plane the tile t is ' ' ' ' present.
There are also rigid variables zt1 , zt2 and Dt' := zt1 = zt2 ; * end1 , end2 such that END := end1 = end2 ; 11  The formula phI is the conjunction of the following formulae: * The region of the integer plane for the solution is finite: !END [?]
(!ENDU(c1 = * * * = cn = x0 [?]
G END)) * x0 and x1 behave as different constants: !
(x0 = x1 ) [?]
G(x0 = Xx0 [?]
x1 = Xx1 ) * There is exactly one tile per element of the plane region: G(!END =  _  (Dt [?]
^  !Dt' ))  t' 6=t  t[?
]T  * Constraint on the right upper tile: ^  F(  (ci = x1 ) [?]
!END [?]
Dtf inal [?]
XEND)  1<=i<=n  * Constraint on the left bottom tile: ^  (ci = x0 ) [?]
Dtinit  1<=i<=n  * Incrementation of the counters c1 , .
.
.
, cn : G(  _  ((  2<=i<=n+1  =(  ^  ^  cj = x1 ) [?]
ci-1 = x0 [?]
!END)  i<=j<=n  (cj = Xcj ) [?]
Xci-1 = x1 [?]
1<=j<=i-2  ^  (Xcj = x0 ))))  i<=j<=n  * Limit condition for the incrementation of the counters c1 , .
.
.
, cn : G((!XEND [?]
c1 = * * * = cn = x1 ) = X(c1 = * * * = cn = x0 )) * Horizontal consistency: not the last element of a row  }|  z  {  G( !
(c1 = * * * = cn = x1 ) [?
]!END =  ^  _  (Dt =  t[?
]T  XDt' ))  ht,t' i[?
]H  * Vertical consistency: not on the last row  }|  z  {  G(!END [?]
F(X!END [?]
c1 = .
.
.
= cn = x1 ) = |c'1 =c1 * * * |c'n=cn |z 1' =z 1 |z 2' =z 2 .
.
.
|z 1' =z 1 |z 2' =z 2 X((!
^  1<=i<=n  c'i  = ci )U(  t1  t1  ^  c'i  t1  = ci [?]
1<=i<=n  tk  t1  ^  t[?
]T  12  (Dt'  =  tk  tk  _  ht,t' i[?
]V  tk  XDt' )))  It is not difficult to show that the instance I = hT, tinit , tf inal , ni has a solution iff phI is CLTL| (D) satisfiable.
This is reminiscent to the ExpSpace-hardness of Timed Propositional Temporal Logic (TPTL) [12, Theorem 2], PLTL+Now (NLTL) [26, Proposition 4.7] and a variant of the guarded fragment with transitivity [27, Theorem 2].
Our ExpSpace-hardness proof is in the same vein since basically in CLTL| (D) we are able to count till 2n using only a number of resources polynomial in n and we can compare the truth value of atomic formulae in states of "temporal distance" exactly 2n .
Our proof is a slight variant of the proof of [14, Theorem 6]: instead of using integer periodicity constraints to count till 2n , n binary counters are used.
Observe also that the resulting formula is not flat because of the encoding of vertical consistency.
If we replace U by F, then NExpTime-hardness can be shown by reducing from the n x n tiling problem with n encoded in binary.
Finiteness of D allows us to show the decidability of CLTL| (D).
Theorem 2 Let D be a finite constraint system.
The satisfiability problem for CLTL| (D) is in ExpSpace.
Proof.
Assume that D = {d1 , .
.
.
, dl }.
We introduce an auxiliary constraint system D ' = hD, P1, .
.
.
, Pl i such that Pi = {di}.
For convenience, we write x = di instead of Pi (x).
We shall show how to reduce the satisfiability problem for CLTL| (D) into the satisfiability problem for CLTL(D ' ).
PSpace-membership of CLTL(D ' ) is not very difficult to show and it is a direct consequence of [14, Theorem 4].
We introduce a translation T from CLTL| (D) formulae into CLTL(D ' ) formulae defined as follows: * T is homomorphic for the Boolean operators and the temporal operators, def W * T(R(a1 , .
.
.
, an )) = ( R(di1 ,...,din ) (a1 = di1 [?]
* * * [?]
an = din )).
So far, the translation can be done in polynomial time and logarithmic space since |D|m is a constant of CLTL| (D) where m is the maximal arity of relations in D. The last clause of T is related to the freeze quantifier: def  T(|x' =a ps) =  ^  '  (a = di ) = T(ps)x =di ,  di [?
]D '  where T(ps)x =di is obtained from T(ps) by replacing every occurrence of x' = dj with j 6= i by [?]
and every occurrence of x' = di by [?].
This step requires an exponential blow up and therefore |T(ph)| is exponential in |ph|.
It is easy to 13  show that ph is CLTL| (D) satisfiable iff T(ph) is CLTL(D ' ) satisfiable.
Since T may cause at most an exponential blow up and CLTL(D ' ) is in PSpace, we obtain that CLTL| (D) satisfiability is in ExpSpace.
Our proof can be easily adapted if the freeze quantifier is replaced by the full existential quantifier [?].
Corollary 1 Let D be a finite constraint system with equality such that the underlying domain D contains at least two elements.
The satisfiability problem for CLTL| (D) is ExpSpace-complete.
A formula ph [?]
CLTL| (D) is of |-height k, for some k >= 0, whenever every branch of the formula tree of ph has at most k freeze quantifiers.
For example, the formula |x' =x (y = x' )U |x' =z y = x' is of |-height 2.
Corollary 2 Let D be a finite constraint system.
For every k >= 0, the satisfiability problem for CLTL| (D) restricted to formulae of |-height k is in PSpace.
The complexity of CLTL| (D) with finite D and restricted to the 'sometimes' operator F is still open.
(NExpTime-hardness and ExpSpace upper bound are known.)
3.2 Flat fragment between CLTL(D) and CLTL| (D) The main result of this section is to show that the freeze quantifier in the flat fragment of CLTL| (D) can be encoded faithfully into CLTL(D) even though flat CLTL| (D) can be more expressive than CLTL(D), see for instance the case with D = hN, =i in Section 2.4.
However, as shown below, satisfiability for flat CLTL| (N, =) can be reduced in logarithmic space to satisfiability for CLTL(N, =).
By analogy, CTL* model-checking can be reduced to LTL modelchecking [28] even though CTL* is more expressive than LTL.
It is worth observing that our concept of flatness restricts the interplay between future-time operators and the freeze quantifier as done in [22,10,23] to limit the interaction between modalities and freeze-like quantifiers.
In order to understand why flat formulae are more manageable, in a formula like |y=x Fph that is flat, only the current value of x needs to be stored.
By contrast, in a formula like G |y=x ph that is not flat, one needs to store as many values of x as there are positions.
We assume that the flexible variables of CLTL| (D) are {x0 , x1 , .
.
.}
and the rigid variables of CLTL| (D) are {y0, y1 , .
.
.}.
For ease of presentation, we assume that the flexible variables of CLTL(D) are composed of the following two 14  disjoint sets: {x0 , x1 , .
.
.}
and {y0new , y1new , .
.
.}.
We define a map u from the flat fragment CLTL| (D) into CLTL(D) as follows: u replaces each yj by yjnew in atomic formulae, it is homomorphic for Boolean and temporal operators, and u(|y=Xn x ps) = y new = Xn x [?]
G(y new = Xy new ) [?]
u(ps) def  It is easy to show that u(ph) can be computed in logarithmic space in |ph|.
Proposition 5 Let D be a constraint system with equality.
For any formula ph of the flat fragment of CLTL| (D), ph is CLTL| (D) satisfiable iff u(ph) is CLTL(D) satisfiable.
Proof.
Given a model s of CLTL| (D), an environment r and a formula ph we say that the model s ' of CLTL(D) agrees with s, r and ph iff for all i, j >= 0, s(i)(xj ) = s ' (i)(xj ) and for all free rigid variable yj in ph and i >= 0, s ' (i)(yjnew ) = r(yj ).
We shall use the following basic properties: * u(ps) = ps if ps belongs to CLTL(D).
* If s ' agrees with s, r and ps then (s ' )i agrees with s i , r and ps for every i >= 0.
Given the occurrence of a subformula ps in ph with positive [resp.
negative] polarity, we write the sign sps to denote the empty string [resp.
!].
By abusing notation, we do not distinguish subformulae from occurrences.
We shall show by structural induction that for any occurrence of a subformula ps in ph, for all models s of CLTL| (D) and environment r, s |=r sps ps iff there is s ' that agrees with s, r and ps such that s ' |= sps u(ps).
Statement of the lemma is then immediate.
The base case with atomic formulae and the cases in the induction step with !, [?]
and X are by an easy verification.
By way of example, we treat the case with ps = !ps ' with negative polarity.
So ps ' occurs with positive polarity.
Let s be a model and r be an environment such that s |=r !
!ps ' .
The statements below are equivalent: * s |=r !
!ps ' , * s |=r ps ' , * there is s ' that agrees with s, r and ps ' such that s ' |= u(ps ' ) (by (IH) and change of polarity), * there is s ' that agrees with s, r and ps ' such that s ' |= !u(!ps ' ) (by definition of u).
Let us treat the remaining cases.
15  Case 1 : ps = ps1 Ups2 with positive polarity.
Since ph belongs to the flat fragment, we have ps1 = u(ps1 ).
Let s be a model and r be an environment such that s |=r ps.
The statements below are equivalent: * s |=r ps, * there is i >= 0 such that s i |=r ps2 and for every j < i, s j |=r ps1 , * there is s ' that agrees with s, r and ps2 such that (s ' )i |= u(ps2 ) and for every j < i, (s ' )j |= u(ps1 ) (by (IH), ps1 = u(ps1 ) and, s and s ' agree on flexible variables of ps1 ), * there is s ' that agrees with s, r and ps such that s ' |= u(ps1 )Uu(ps2 ) (ps1 has no free rigid variable).
Case 2 : ps = ps1 Ups2 with negative polarity.
Since ph belongs to the flat fragment, we have ps2 = u(ps2 ) and both ps1 and ps2 have negative polarity.
Let s be a model and r be an environment such that s |=r ps.
The statements below are equivalent: * s |=r !ps, * either there is j >= 0 such that s j |=r !ps1 and for every j <= i, s i |=r !ps2 or for every i >= 0, s i |=r !ps2 , * either there is s ' that agrees with s, r and ps1 such that there is j >= 0 such that (s ' )j |= !u(ps1 ) and for every j <= i, (s ' )i |= !u(ps2 ) (by (IH) and ps2 = u(ps2 )) or there is s ' that agrees with s, r and ps2 such that for every i >= 0, (s ' )i |= !u(ps2 ) (by (IH)), * there is s ' that agrees with s, r and ps1 Ups2 such that either there is j >= 0 such that (s ' )j |= !u(ps1 ) and for every j <= i, (s ' )i |= !u(ps2 ) or for every i >= 0, (s ' )i |= !u(ps2 ) (ps2 has no free rigid variables), * there is s ' that agrees with s, r and ps1 Ups2 such that s ' |= !
(u(ps1 )Uu(ps2 )).
Case 3 : ps =|y=Xn x ps ' .
Let s be a model and r be an environment for sps and ps.
The statements below are equivalent: * s |=r sps ps, * s |=r[y7-s(n)(x)] sps ps ' , * there is s ' that agrees with s, r[y 7- s(n)(x)] and ps ' such that s ' |= sps u(ps ' ) (by (IH)), * there is s ' that agrees with s, r[y 7- s(n)(x)] and ps ' such that s ' |= sps u(ps ' ) and s ' |= G(y new = Xy new ) [?]
y new = Xn x (y free in ps ' ).
* there is s ' that agrees with s, r and ps such that s ' |= sps u(ps ') [?]
G(y new = Xy new ) [?]
y new = Xn x (ps has less free rigid variable than ps ' ).
Corollary 3 For every constraint system D which contains equality, decidability of CLTL(D) implies decidability of the flat fragment of CLTL| (D).
Since CLTL(hZ, <, =i), CLTL(hN, <, =i) and CLTL(hR, <, =i) are PSpace16  complete [11], we can establish the following corollary.
Corollary 4 Flat fragments of each of CLTL| (hZ, <, =i), CLTL| (hN, <, =i), CLTL| (hR, <, =i), and CLTL| (D) with D finite are PSpace-complete.
Corollary 4 can be also adapted to the PSpace-complete constrained version of LTL introduced in [29].
4  Undecidability results  In this section, we shall prove that, if the domain is infinite, and if we do not restrict to flat formulae, the satisfiability problem for CLTL| (D) is undecidable even if we only have the equality predicate.
More precisely, Theorem 3 below is a stronger result, stating that satisfiability is S11 -hard, even restricted to formulae with 1 flexible variable and at most 2 rigid variables.
(An exposition of the analytical hierarchy can be found in [30].)
A corollary of S11 -hardness is that the logic cannot be recursively axiomatised.
The following proposition complements the main result in this section, and states that, for countable and computable constraint systems D, satisfiability for CLTL| (D) is in S11 .
Hence, for a countably infinite domain, the problem in Theorem 3 is S11 -complete.
Proposition 6 If D is countable, and (Ri )i[?
]I is a countable family of computable relations on D, then the satisfiability problem for CLTL| (D, (Ri )i[?
]I ) is in S11 .
Proof.
Let ph be a formula of CLTL| (D, (Ri )i[?
]I ).
We can assume FleVarSet = FleVars(ph) and RigVarSet = RigVars(ph).
Let n = |FleVarSet|, m = |RigVarSet|.
Any model s : N - (FleVarSet - D) can be encoded by functions f1 , .
.
.
, fn : N - N, and any environment r : RigVarSet - D as an m-tuple a1 , .
.
.
, am : N. A first-order predicate on f1 , .
.
.
, fn and a1 , .
.
.
, am which expresses that s |=r ph is routine to construct by structural recursion on ph.
We conclude that satisfiability of ph can be expressed by a S11 -sentence.
We shall prove that the satisfiability problem for a fragment of CLTL| (D, =) is S11 -hard by reducing from the Recurrence Problem for nondeterministic 2-counter machines, which was shown to be S11 -hard in [12, Section 4.1].
A nondeterministic 2-counter machine M consists of two counters C1 and C2 , and a sequence of n >= 1 instructions, each of which may increment or decrement one of the counters, or jump conditionally upon of the counters being zero.
After the execution of a non-jump instruction, M proceeds nondeterministically to one of two specified instructions.
Therefore, the lth instruction is 17  written as one of the following: l : Ci := Ci + 1; goto l' or goto l'' l : Ci := Ci - 1; goto l' or goto l'' l : if Ci = 0 then goto l' else goto l'' We represent the configurations of M by triples hl, c1 , c2 i, where 1 <= l <= n, c1 >= 0, and c2 >= 0 are the current values of the location counter and the two counters C1 and C2 , respectively.
The consecution relation on configurations is defined in the obvious way, where decrementing 0 yields 0.
A computation of M is an o-sequence of related configurations, starting with the initial configuration h1, 0, 0i.
The computation is recurring if it contains infinitely many configurations with the value of the location counter being 1.
The Recurrence Problem is to decide, given a nondeterministic 2-counter machine M, whether M has a recurring computation.
This problem is S11 -hard.
Theorem 3 If D is infinite, then the satisfiability problem for CLTL| (D, =) with |FleVarSet| = 1 and |RigVarSet| = 2 is S11 -hard.
Proof.
Suppose M is a nondeterministic 2-counter machine.
We construct a formula phM of CLTL| (D, =) such that |FleVars(ph)| = 1, |RigVars(ph)| = 2, and phM is satisfiable iff M has a recurring computation.
The basis of the construction is an encoding of computations of nondeterministic 2-counter machines by models of CLTL| (D, =) with one flexible variable, i.e.
by osequences of elements of D. As in the proofs of [12, Theorems 6 and 7], which show S11 -hardness of satisfiability of formulae of TPTL extended with either multiplication by 2 or dense time, we shall encode the value of a counter by a sequence of that length.
However, much further work is needed in this proof because the only operation we have on elements of D is equality.
Let n be the number of instructions in M. We encode a configuration hl, c1 , c2 i by a sequence of elements of D of the form ddd' d |.
.
.
d{z' .
.
}.
f11 .
.
.
fc11 eee' e'' f12 .
.
.
fc22 n  where: (i) the only two pairs of equal consecutive elements are dd and ee, and also fc22 is distinct from the first element in the encoding of the next configuration, (ii) e 6= e'' , (iii) after the first 4 elements, there is a sequence of n elements, and only the lth equals d' , 18  2 4 n+4 phinit (starte [?]
X4 (startd[?
]e )) n = startd [?]
X x = X x [?]
X def  phglob = G(startd = psn1 [?]
starte = psn2 ) n def  in dd' d...d' ... any two consecutive values are distinct  z  psn1 =  def  z   n _  l=1  [?
]X psn2 =  def  i=1  ps  dist  def  !
{  Xi x 6= Xi+1 x  [?]
in ...d' ... exactly one value equals d'  X2 x = Xl+3 x [?]
l-1 ^  }|  X2 x 6= Xj+3 x [?]
j=1  n+4  3 ^  i=1  }|    f11 ...fc11  z  n+3 ^  n ^  j=l+1  mutually distinct  (ps  }|  dist  {  {  X2 x 6= Xj+3 x  U starte ) !
f12 ...fc22 mutually distinct  z  }|  {  Xi x 6= Xi+1 x [?]
X4 (ps dist U startd )  = !startd[?
]e [?]
|y=x X((!startd[?
]e [?]
x 6= y)U startd[?
]e ) Fig.
1.
(iv) f1i , .
.
.
, fcii are mutually distinct, for each i.
We write startd[?
]e to denote the formula x = X1 x stating that the current state is an occurrence of either dd or ee.
We write startd [resp.
starte ] to denote the formula startd[?
]e [?]
x = X3 x [resp.
startd[?
]e [?]
x 6= X3 x] stating the current state is a first occurrence of d [resp.
e] in dd [ee].
The formula phM is defined as a conjunction glob phinit [?]
ph1M [?]
* * * [?]
phnM [?]
phrec n [?]
phn  where the first two conjuncts state that the model is a concatenation of configuration encodings which satisfy (i)-(iv) above, and that it begins with an encoding of the initial configuration h1, 0, 0i.
Their definitions are given in Figure 1.
For any l [?]
{1, .
.
.
, n}, phlM states that, whenever the model contains an encoding of a configuration hl, c1 , c2 i, then the next encoding is of a configuration which is obtained by executing the lth instruction.
Consider the most complex case: l : C2 := C2 - 1; goto l' or goto l'' .
The formula phlM needs to state that, whenever the location counter is l, C1 remains the same, C2 either remains 0 or is decremented, and the next value of the location counter is either l' or l'' : 19  0<=C2 <=1 and the next value of C2 equals 0  z  }|  {  kh2dec = ((x = X1 x [?]
X1 x = X2 x) [?]
(!starte U(starte [?]
X4 (x = X1 x))) [?]
def  C2 >1  }|  z  {  (!
(x = X1 x [?]
X1 x = X2 x) [?]
z  (A)  }|  {  (|y=x !starte U(starte [?]
X4 (!startd [?]
x = y))) [?]
z  (B)  }|  2  {  ((!X startd[?
]e [?]
(|y=x X | (!starte U(starte [?]
4 1 X (x 6= yU(x = y [?]
X x = y ' ))))))UX2 startd[?
]e ) [?]
y ' =x  (C)  }|  z  {  ((X2 !startd )U(X2 startd [?]
|y=x !starte U(starte [?]
X4 (x 6= yU (x = y [?]
!startd [?]
X2 startd ))))) Fig.
2.  phlM = G((startd [?]
X2 x = Xl+3 x) = Xn+4 (kh1eq [?]
(!startd[?
]e U(starte [?]
def  X4 (kh2dec [?]
(!startd[?
]e U(startd [?]
'  (X2 x = Xl +3 x [?]
X2 x = Xl  '' +3  x))))))))  The formula kh2dec given in Figure 2 specifies that, if the current value of C2 is either 0 or 1, then the next value of C2 is 0; and if neither, then the next encoding of the value of C2 equals the current encoding with the last element removed.
The latter is specified as the following conjunction: (A) the first element of the current encoding equals the first element of the next encoding, and (B) for any consecutive pair y and y ' of elements in the current encoding such that y ' is not the last element, the first occurence of y in the next encoding is followed by y ', and (C) the element before the last in the current encoding is the last element in the next encoding.
The formula kh1eq , which specifies that the value of C1 remains the same, is defined similarly.
Definitions of phlM for other forms of instruction use the same machinery.
For incrementing a counter, it is not necessary to specify that the additional element in the next encoding is distinct from the rest, because that is ensured 20  by phglob n .
def  Finally, phrec = GF(startd [?]
X2 x = X4 x) states that the model encodes a recurring computation.
By Propositions 2 and 3, we have that Theorem 3 can be strengthened by restricting to the fragment of CLTL| (D, =) with |FleVarSet| = 1, |RigVarSet| = 2 and such that the flexible variable occurs only in freeze quantifiers of the form |y=x .
By adapting the proof of Theorem 3, the variant of CLTL| (D, =) over models which are finite words is also undecidable, more precisely S01 -hard through encoding the Halting Problem for 2-counter machines.
This should be compared with the undecidability of universality of 1-way nondeterministic register automata [31, Theorem 5.1].
The proof of Theorem 3 can also be modified to yield, for CLTL| (D, =) augmented with the past-time operator U-1 ('since') but restricted to 1 rigid variable, S11 -hardness over infinite models and S01 -hardness over finite models.
The sets of values from D which are used to encode counter values do not have to be enumerated in the same order for consecutive configurations, and simpler logical formulae suffice.
These results are related to the undecidability of emptiness of 2-way deterministic register automata: see [32, Section 7], [31, Theorem 5.3].
5  Related work  In this section, we compare the logic CLTL| (N, =) and the results in this paper with a number of related works in the literature.
We show that there is a surprising variety of formalisms which involve the freeze quantifier or related constructs, revealing links among several works which appear unconnected.
This confirms that the binding mechanism of the freeze quantifier is fundamental.
LTL over concrete domains.
Complexity results for Constraint LTL over concrete domains can be found in [16,17,11,18,14] (see also related results for description logics over concrete domains in [33]).
Decidability and complexity issues for LTL over Presburger constraints have been studied for instance in [34,22,10,14].
Most decision procedures in the above-mentioned works are automata-based whereas undecidability proofs often rely on an easy encoding of the Halting Problem for 2-counter machines.
21  LTL over integer periodicity constraints augmented with the freeze quantifier is shown ExpSpace-complete [14] but CLTL(N, <, =) with past-time operator F-1 and | is undecidable [14].
Real-time logics.
Similar issues for real-time and modal logics equipped with the freeze quantifier have been considered in [12,35,13,36].
In spite of its rich language of constraints, TPTL model-checking is decidable [12] (discrete version).
In this case, decidability is due to the subtle combination of the constraint system and the semantical restrictions (see also versions of metric temporal logics in [37,38]).
The class of logics CLTL| (D) defined in this paper is quite general and it is not difficult to show that discrete-time TPTL [12] is exactly the fragment of CLTL| (D) where * D = N and the only flexible variable is t (time), * the predicates of D are (x <= c)c[?
]Z , (x <= y + c)c[?
]Z , (x [?
]d c)c,d[?
]N , (x [?
]d y + c)c,d[?
]N where [?
]d is equality modulo d, and * the formulae are of the form G(t <= Xt) [?]
GF(t < Xt) [?]
ph with any use of the freeze quantifier being of the form |x=t .
In [12, Theorem 5], S11 -hardness of satisfiability for TPTL without the monotonicity condition on time sequences is established.
By Propositions 2 and 3, CLTL| (N, =) restricted to one flexible variable can be seen as the fragment of TPTL where there are no atomic propositions, and where the only operation on time is equality.
Moreover, it is straightforward to see that Theorem 3 in this paper still holds when satisfiability is restricted to models which contain infinitely many values, which is equivalent to the progress condition when the domain is N. Therefore, a corollary of Theorem 3 is the following strengthening of [12, Theorem 5]: satisfiability for TPTL without the monotonicity condition remains S11 -complete even without atomic propositions and with only equality constraints.
(The proof of [12, Theorem 5] uses arithmetic on time values.)
Hybrid, navigation, spatio-temporal, and similar logics.
Hybrid logics (see e.g.
[39,40,41]) contain a variable-binding mechanism similar to the freeze quantifier: |x ph(x) holds true iff ph(x) holds true when the propositional variable x is interpreted as a singleton containing the current state.
The downarrow binder in such hybrid logics records the value of the current state.
Similarly, in temporal logic with forgettable past [26], the effect of the Now operator is that the origin of time takes the value of the current state: the 22  states before the current state are forgotten.
Identical mechanisms are used in navigation logics for object structures, see e.g.
[42] and in half-order dynamic temporal logics interpreted over traces from sequence diagrams [43].
In the context of spatio-temporal logics, Wolter and Zakharyaschev [16, Section 7] advocate the need to consider operators expressing constraints of W V the form i[?
]N R(x, Xi y) and i[?
]N R(x, Xi y).
They are simple to express in CLTL| (D), as |x' =x GR(x' , y) and |x' =x FR(x' , y).
These formulae are in the flat fragment: see Section 3.2.
Quantified propositional temporal logic with repeating.
The models of Quantified Propositional Temporal Logic with Repeating (also known as RQPTL) introduced in [44] can be encoded by CLTL| (N, =) formulae, unlike the second-order quantification in the language.
Such models are pairs of maps hu : N - S, p : S - 2AP i where S is an arbitrary set (of states).
A possible encoding is by treating u as the interpretation of a distinguished flexible variable, and using the freeze quantifier to specify that, whenever u(i) = u(j), any propositional variable has the same values at time points i and j.
(See Section 2.4 regarding encodings of propositional variables.)
On the other hand, the variant logic RHLTLn [44, Section 4] can be shown equivalent to CLTL| (N, =) with one flexible variable and n rigid variables, except that RHLTLn does not have the U operator but has F and the pasttime operators F-1 and X-1 .
Theorem 3 in this paper and S11 -hardness of RHLTL2 [44, Corollary 1] are therefore complementary results.
Predicate l-abstraction.
A number of decidability and undecidability results for half-order modal logics (to be compared with [35]) are presented in [45].
The half-order aspect of such logics is due to a predicate l-abstraction mechanism, which solves the famous problem of interpreting constants in modal logic.
Even though this construct is essentially the same as the freeze quantifier, apparently there have been no cross-references between the literature dealing with predicate l-abstraction (e.g.
[45,15]) and that dealing with the freeze quantifier (e.g.
[35,12,14,1]).
However, several undecidability results for LTL-like logics with predicate l-abstraction have recently been obtained in [15], independently and concurrently with [1].
The most related to Theorem 3 in this paper are S11 -hardness results for the following logics: (I) LTLl= with temporal operators X and U, and with 3 rigid variables; (II) LTLl with temporal operators X and U, and with countably infinitely many unary predicate symbols (but no equality).
23  Remarkably, LTLl= is essentially the same as CLTL| (N, =).
The proofs of (I) in [15] and of Theorem 3 above reduce from the same S11 -hard problem.
However, the encodings are different, enabling Theorem 3 to be sharper by restricting to 1 flexible and 2 rigid variables.
An interesting discussion of applications to dynamic systems with resources, like communication protocols for mobile agents, can also be found in [15].
Monodic first-order temporal logics.
Since freeze quantification is firstorder quantification over a singleton set, the freeze quantifier can be expressed in first-order temporal logics [46,47,48,49].
Indeed, CLTL| (N, =) satisfiability can be reduced to first-order temporal logic T L satisfiability over the linear structure hN, <i (the latter logic was introduced in [49, Chapter 11]).
To each flexible variable x one associates a monadic predicate symbol Px in such a way that Px is interpreted as the singleton set containing the value of x.
A formula of the form |x' =Xx ph is then translated to [?
]x' XPx (x' ) [?]
ph' where ph' is the translation of ph.
The translation is homomorphic for Boolean and temporal operators, whereas for instance y = Xz with y, z [?]
FleVarSet is translated into [?
]x Py (x) [?]
XPz (x).
One needs also to be able to express that at every state Px is interpreted by a singleton, which can be encoded by the formula G([?
]z Px (z) [?]
[?
]z, z ' (Px (z) [?]
Px (z ' ) = z = z ' )).
Consider the fragment of CLTL| (N, =) with |RigVarSet| = 1.
It is easy to check that its translation is contained in the monodic fragment of T L with equality, and with only two individual variables and monadic predicate symbols.
We recall that in the monodic fragment, any temporal subformula (i.e.
whose outermost construct is a temporal operator) must have at most one free individual variable.
Even though monodic T L over hN, <i is decidable [50], its extension with equality is not [47], even with the above restrictions [46].
Logics and automata for data languages.
In [51,52], data languages are defined as sets of finite data words in (S x D)* where S is a finite alphabet and D is an infinite domain (generalising the concept of timed languages), and automata which recognise data languages are introduced.
The latter are related to register and pebble automata for strings over infinite alphabets (e.g.
[31]).
First-order logic over finite data word models is considered in [53], with motivations stemming from query languages for semistructured data.
More precisely, the carrier of a model is the set of positions in a data word, there are no function symbols, the unary predicates correspond to elements of S, and there are binary predicates <, +1, as well as ~ which is interpreted as equality of elements of D at given positions.
FOk (~, <, +1) denotes such a logic with 24  k variables.
The main result of [53] is that satisfiability of FO2 (~, <, +1) is decidable, by a doubly exponential-time reduction to nonemptiness of multicounter automata.
(The latter problem is decidable, but there is no known elementary upper bound.)
The following variant of CLTL| (D, =) has models which are words over SxD: there is one flexible variable x which takes values in D, plus one flexible variable l which takes values in S and on which freeze quantification cannot be used, but to which unary predicates Pa for equality testing with a [?]
S can be applied.
Interestingly, that logic with infinite D and 1 rigid variable is incomparable with FO2 (~, <, +1).
In one direction, FO2 (~, <, +1) cannot express the U operator, and also not formulae of the form |y=x ph where y occurs in ph under two or more temporal operators.
In the other direction, FO2 (~, <, +1) can express past-time operators such as F-1 .
6  Conclusion  We have shown that adding the freeze quantifier to CLTL(D) leads to undecidability as soon as the underlying domain is infinite and the equality predicate is part of D. As illustrated in the paper, in most related work dealing with undecidable logics having a binding mechanism similar to freeze quantification, either past-time operators can be encoded or constraints richer than equality are available.
The logic CLTL| (D) is ExpSpace-complete for most of finite domains D. In order to design a specification language over infinite domains with LTL temporal operators and the freeze quantifier that admits a decidable modelchecking problem, syntactic restrictions could be a reasonable solution.
The existence of a logarithmic-space reduction from the flat fragment of CLTL| (D) into CLTL(D) when the equality predicate is present leads us to believe that the flatness criterion is most relevant here.
As we have seen, the following fragments/variants of CLTL| (D, =) with infinite D and |FleVarSet| = 1 are S11 -hard: * the temporal operators are X and U, and |RigVarSet| = 2; * the temporal operators are X, U and U-1 , and |RigVarSet| = 1; * the temporal operators are X, X-1 , F and F-1 , and |RigVarSet| = 2; It is open whether the intersections of these fragments are decidable.
Other open problems include: * decidability in the presence of semantic restrictions such as reversal bound25  edness [5] of a flexible variable; * decidability over infinite domains without equality (and where equality is not definable by other predicates), such as h{0, 1}*, <i with < being either the strict prefix relation or the strict subword relation.
Acknowledgements.
We are grateful to Deepak D'Souza, Claire David, Anca Muscholl and Luc Segoufin for helpful discussions, and to Frank Wolter for having directed us to related work.
References [1] S. Demri, R. Lazic, D. Nowak, On the freeze quantifier in constraint LTL: decidability and complexity, in: 12th Int.
Symp.
Temporal Representation and Reasoning (TIME), IEEE, 2005, pp.
113-121.
[2] P. Schnoebelen, The complexity of temporal logic model checking, in: AiML'02, Vol.
4 of Advances in Modal Logic, King's College, 2003, pp.
393-436.
[3] P. Revesz, Introduction to Constraint Databases, Springer, 2002.
[4] R. Alur, D. Dill, A theory of timed automata, Theoretical Comput.
Sci.
126 (1994) 183-235.
[5] O. Ibarra, Reversal-bounded multicounter machines and their decision problems, J. ACM 25 (1) (1978) 116-133.
[6] B. Boigelot, Symbolic methods for exploring infinite state spaces, Ph.D. thesis, Universite de Liege (1998).
[7] B. Boigelot, P. Wolper, Representing arithmetic constraints with finite automata: an overview, in: 18th Int.
Conf.
Logic Prog.
(ICLP), Vol.
2401 of Lect.
Notes Comput.
Sci., Springer, 2002, pp.
1-19.
[8] A. Finkel, J. Leroux, How to compose Presburger accelerations: Applications to broadcast protocols, in: 22nd Conf.
Foundations of Software Tech.
and Theoretical Comput.
Sci.
(FSTTCS), Vol.
2256 of Lect.
Notes Comput.
Sci., Springer, 2002, pp.
145-156.
[9] J. Leroux, G. Sutre, Flat counter systems are everywhere!, in: 3rd Int.
Symp.
Automated Tech.
for Verification and Analysis (ATVA), Vol.
3707 of Lect.
Notes Comput.
Sci., Springer, 2005, pp.
489-503.
[10] H. Comon, V. Cortier, Flatness is not a weakness, in: 14th Int.
Works.
Comput.
Sci.
Logic (CSL), Vol.
1862 of Lect.
Notes Comput.
Sci., Springer, 2000, pp.
262-276.
[11] S. Demri, D. D'Souza, An automata-theoretic approach to constraint LTL, Tech.
Rep. 03-11, LSV, an extended abstract appeared in FSTTCS'02 (2003).
26  [12] R. Alur, T. Henzinger, A really temporal logic, J. ACM 41 (1) (1994) 181-204.
[13] E. Harel, O. Lichtenstein, A. Pnueli, Explicit clock temporal logic, in: 5th Symp.
Logic in Comput.
Sci.
(LICS), IEEE, 1990, pp.
400-413.
[14] S. Demri, LTL over integer periodicity constraints, Tech.
Rep. 04-6, LSV, an extended abstract appeared in FOSSACS'04 (2004).
[15] A. Lisitsa, I. Potapov, Temporal logic with predicate l-abstraction, in: 12th Int.
Symp.
Temporal Representation and Reasoning (TIME), IEEE, 2005, pp.
147-155.
[16] F. Wolter, M. Zakharyaschev, Spatio-temporal representation and reasoning based on RCC-8, in: 7th Int.
Conf.
Principles of Knowledge Representation and Reasoning (KR), Morgan Kaufmann, 2000, pp.
3-14.
[17] P. Balbiani, J. Condotta, Computational complexity of propositional linear temporal logics based on qualitative spatial or temporal reasoning, in: 4th Int.
Works.
Frontiers of Combining Systems (FroCoS), Vol.
2309 of Lect.
Notes Artif.
Int., Springer, 2002, pp.
162-173.
[18] D. Gabelaia, R. Kontchakov, A. Kurucz, F. Wolter, M. Zakharyaschev, On the computational complexity of spatio-temporal logics, in: 16th Int.
Florida AI Research Soc.
Conf.
(FLAIRS), AAAI, 2003, pp.
460-464.
[19] A. Sistla, E. Clarke, The complexity of propositional linear temporal logic, J. ACM 32 (3) (1985) 733-749.
[20] D. Dams, Flat fragments of CTL and CTL*: separating the expressive and distinguishing powers, Logic J. IGPL 7 (1) (1999) 55-78.
[21] O. Ibarra, Z. Dang, On removing the stack from reachability constructions, in: Int.
Symp.
Algorithms and Computation (ISAAC), Vol.
2223 of Lect.
Notes Comput.
Sci., Springer, 2001, pp.
244-256.
[22] A. Bouajjani, P. Habermehl, Constrained properties, semilinear sets, and Petri nets, in: 7th Int.
Conf.
Concurrency Theory (CONCUR), Vol.
1119 of Lect.
Notes Comput.
Sci., Springer, 1996, pp.
481-497.
[23] B. ten Cate, M. Franceschet, On the complexity of hybrid logics with binders, in: 19th Int.
Works.
Comput.
Sci.
Logic (CSL), Vol.
3634 of Lect.
Notes Comput.
Sci., Springer, 2005, pp.
339-354.
[24] P. Wolper, Temporal logic can be more expressive, Inf.
and Comput.
56 (1983) 72-99.
[25] P. van Emde Boas, The convenience of tilings, in: Complexity, Logic, and Recursion Theory, Vol.
187 of Lecture Notes in Pure and Applied Logic, Marcel Dekker, 1997, pp.
331-363.
[26] F. Laroussinie, N. Markey, P. Schnoebelen, Temporal logic with forgettable past, in: 17th Symp.
Logic in Comput.
Sci.
(LICS), IEEE, 2002, pp.
383-392.
27  [27] E. Kieronski, EXPSPACE-complete variant of guarded fragment with transitivity, in: 19th Ann.
Symp.
Theoretical Aspects of Comput.
Sci.
(STACS), Vol.
2285 of Lect.
Notes Comput.
Sci., Springer, 2002, pp.
608-619.
[28] A. Emerson, J. Halpern, "Sometimes" and "Not Never" revisited: on branching versus linear time temporal logic, J. ACM 33 (1986) 151-178.
[29] S. Demri, R. Gascon, Verification of qualitative Z-constraints, in: 16th Int.
Conf.
Concurrency Theory (CONCUR), Vol.
3653 of Lect.
Notes Comput.
Sci., Springer, 2005, pp.
518-532.
[30] H. Rogers, Jr, Theory of Recursive Functions and Effective Computability, McGraw-Hill, 1967.
[31] F. Neven, T. Schwentick, V. Vianu, Finite state machines for strings over infinite alphabets, ACM Trans.
Comput.
Logic 5 (3) (2004) 403-435.
[32] C. David, Mots et donnees infinies, Master's thesis, LIAFA (2004).
[33] C. Lutz, NEXPTIME-complete description logics with concrete domains, ACM Trans.
Comput.
Logic 5 (4) (2004) 669-705.
[34] A. Bouajjani, R. Echahed, P. Habermehl, On the verification problem of nonregular properties for nonregular processes, in: 10th Symp.
Logic in Comput.
Sci.
(LICS), IEEE, 1995, pp.
123-133.
[35] T. Henzinger, Half-order modal logic: how to prove real-time properties, in: 9th Ann.
Symp.
Principles of Distr.
Comput.
(PODC), ACM, 1990, pp.
281-296.
[36] T. Brihaye, V. Bruyere, J. Raskin, Model-checking for weighted timed automata, in: 2nd Int.
Conf.
Formal Modelling and Analysis of Timed Systems (FORMATS), Vol.
3253 of Lect.
Notes Comput.
Sci., Springer, 2004, pp.
277- 292.
[37] R. Alur, T. Henzinger, A really temporal logic, in: 30th Ann.
Symp.
Foundations Comput.
Sci.
(FOCS), IEEE, 1989, pp.
164-169.
[38] R. Alur, T. Feder, T. Henzinger, The benefits of relaxing punctuality, J. ACM 43 (1996) 116-146.
[39] V. Goranko, Hierarchies of modal and temporal logics with references pointers, J.
Logic, Lang.
and Inf.
5 (1996) 1-24.
[40] C. Areces, P. Blackburn, M. Marx, A road-map on complexity for hybrid logics, in: 13th Int.
Works.
Comput.
Sci.
Logic (CSL), Vol.
1683 of Lect.
Notes Comput.
Sci., Springer, 1999, pp.
307-321.
[41] M. Franceschet, M. de Rijke, B.-H. Schlingloff, Hybrid logics on linear structures: Expressivity and complexity, in: 10th Int.
Symp.
Temporal Representation and Reasoning and 4th Int.
Conf.
Temporal Logic (TIMEICTL), IEEE, 2003, pp.
164-171.
28  [42] F. de Boer, R. van Eijk, Decidable navigation logics for object structures, in: 15th Int.
Works.
Comput.
Sci.
Logic (CSL), Vol.
2142 of Lect.
Notes Comput.
Sci., Springer, 2001, pp.
324-338.
[43] S. M. Cho, H. H. Kim, S. D. Cha, D. H. Bae, A semantics of sequence diagrams, Inf.
Process.
Lett.
84 (2002) 125-130.
[44] T. French, Quantified propositional temporal logic with repeating states, in: 10th Int.
Symp.
Temporal Representation and Reasoning and 4th Int.
Conf.
Temporal Logic (TIME-ICTL), IEEE, 2003, pp.
155-165.
[45] M. Fitting, Modal logic between propositional and first-order, J.
Logic and Comput.
12 (6) (2002) 1017-1026.
[46] A. Degtyarev, M. Fisher, A. Lisitsa, Equality and monodic first-order temporal logic, Studia Logica 72 (2002) 147-156.
[47] F. Wolter, M. Zakharyaschev, Axiomatizing the monodic fragment of first-order temporal logic, Ann.
Pure and Applied Logic 118 (2002) 133-145.
[48] I. Hodkinson, R. Kontchakov, A. Kurucz, F. Wolter, M. Zakharyaschev, On the computational complexity of decidable fragments of first-order linear temporal logics, in: 10th Int.
Symp.
Temporal Representation and Reasoning and 4th Int.
Conf.
Temporal Logic (TIME-ICTL), IEEE, 2003, pp.
91-98.
[49] D. Gabbay, A. Kurucz, F. Wolter, M. Zakharyaschev, Many-dimensional modal logics: theory and practice, Cambridge University Press, 2003.
[50] I. Hodkinson, F. Wolter, M. Zakharyaschev, Decidable fragments of first-order temporal logics, Ann.
Pure and Applied Logic 106 (2000) 85-134.
[51] P. Bouyer, A. Petit, D. Therien, An algebraic approach to data languages and timed languages, Inf.
and Comput.
182 (2) (2003) 137-162.
[52] P. Bouyer, A logical characterization of data languages, Inf.
Process.
Lett.
84 (2) (2002) 75-85.
[53] M. Bojanczyk, C. David, A. Muscholl, T. Schwentick, L. Segoufin, Two-variable logic on words with data, Tech.
Rep. 2005-004, LIAFA (2005).
29

A Multilevel Distance-based Index Structure for Multivariate Time Series Kiyoung Yang Computer Science Department University of Southern California Los Angeles, CA 90089-0781 kiyoungy@usc.edu  Abstract Multivariate time series (MTS) datasets are common in various multimedia, medical and financial applications.
In previous work, we introduced a similarity measure for MTS datasets, termed Eros (Extended Frobenius norm), which is based on the Frobenius Norm and Principal Component Analysis (PCA).
Eros computes the similarity between two MTS items by measuring how close the corresponding principal components (PCs) are using the eigenvalues as weights.
Since the weights are based on the data items in the database, they change whenever data are inserted into or removed from the database.
In this paper, we propose a distance-based index structure, Muse (Multilevel distancebased index structure for Eros), for efficient retrieval of MTS items using Eros.
Muse constructs each level as a distance-based index structure without using the weights, up to z levels.
At the query time, Muse combines the z levels with the weights, which enables the weights to change without the need to rebuild the index structure.
In order to show the efficiency of Muse, we performed several experiments on a set of synthetically generated clustered datasets.
The results show the superiority of Muse as compared to Sequential Scan and M-tree in performance.
1 INTRODUCTION A time series is a series of observations, xi (t); [i = 1, * * * , n; t = 1, * * * , m], made sequentially over time where i indexes the measurements made at each time point t [13].
It is called a univariate time series when n is equal to 1, and a multivariate time series (MTS) when n is equal to, or greater than 2.
MTS datasets are common in various fields, such as in multimedia, medicine and finance.
For example, in multimedia, Cybergloves used in the Human and Computer Interface applications have about 20 sensors, each of which generates 50~100 values in a second [11].
In medicine,  Cyrus Shahabi Computer Science Department University of Southern California Los Angeles, CA 90089-0781 shahabi@usc.edu  Electro Encephalogram (EEG) from 64 electrodes placed on the scalp are measured to examine the correlation of genetic predisposition to alcoholism [16].
Functional Magnetic Resonance Imaging (fMRI) from 696 voxels out of 4391 has been used to detect similarities in activation between voxels in [5].
In our previous work [14], we proposed a similarity measure Eros (Extended Frobenius norm) for efficient similarity searches in MTS databases.
Eros is based on the Frobenius norm that is used to compute the matrix norm [9], and Principal Component Analysis (PCA) [6].
Eros computes the similarity between two MTS items by measuring how close their corresponding principal components (PCs), i.e., the eigenvectors from their covariance matrices, are using the eigenvalues as weights.
The weights are aggregated from the eigenvalues of all the MTS items in the database.
Hence, the weights change whenever data are inserted into or removed from the database.
Empirically, we showed that Eros outperforms Euclidean Distance (ED), Weighted Sum SVD (WSSVD) [12], Dynamic Time Warping (DTW) [10] and PCA similarity factor (SP CA ) [8] in terms of precision/recall.
In this paper, we propose an index structure termed Muse (Multilevel distance-based index structure for Eros) for efficient retrieval of MTS items using Eros.
Distance-based index structures, such as iDistance [15] and M-tree [3], have been shown to outperform feature-based index structures, such as R-tree and its variants, for high dimensional datasets.
Hence, we extend a distance-based index structure so that the similarity search using Eros whose lower bound is obtained using the weighted Euclidean distance, can be performed efficiently.
To explain Muse, let us denote the principal component (PC) of an MTS item whose corresponding eigenvalue is the largest as the first PC of an MTS item, and that whose corresponding eigenvalue is the second largest as the second PC of an MTS item.
We subsequently call the first PCs of all the MTS items as the first PC group, and the second PCs of all the MTS items as the second PC group.
For Muse, we thusly construct a number  of levels, each of which is a distance-based index structure corresponding to one PC group without using the weights.
In order to build a z-level Muse, we utilize the first z PC groups of all the data items.
At the query time when a similarity search is performed, Muse combines the z levels of the distance-based index structures with the weights to yield the lower bounds of the similarities between the query MTS item and the MTS items in the database.
Since the weights are employed not when constructing the index structure, but when performing a similarity search, Muse also allows the weights to change without the need to re-construct the index structure, even when data are added into or removed from the database.
In order to show the efficiency of Muse, we compare the performance of Muse to those of M-tree [3] and sequential scan in terms of efficiency and scalability.
The remainder of this paper is organized as follows.
Section 2 discusses the background.
Our proposed method is described in Section 3.
This is followed by the experiments and results in Section 4.
Conclusions and future work are presented in Section 5.
n dataset (see Section 3.3), i=1 wi = 1 and cos thi is the angle between ai and2 bi .
The range of Eros is between 0 and 1, with 1 being the most similar.
Eros does not satisfy the triangle inequality [4].
Hence, we obtained the lower and upper bounds of Eros as follows, so that efficient retrievals of MTS items can be performed using an index structure.
We first defined DEros which reversely preserves the similarity relation of Eros, and obtained the lower and upper bounds of DEros , i.e., Dmin and Dmax , respectively.
For notations used in the remainder of this paper, please refer to Table 1.
DEros (A,B,w)= =  Muse is the extension of a distance-based index structure designed for our previously introduced similarity measure Eros [14].
In this section, we briefly describe Eros and the distance-based index structures.
2.1  [?]
[?]
Dmax (A,B,w)= = Dmin (A,B,w)=  2-2  n i=1  wi |<viA ,viB >|  2-2  n i=1  wi |  n i=1  2-2  [?]
[?]
=  2 BACKGROUND  [?]
[?]
n i=1  2-2  n j=1 n i=1 n j=1 n i=1  n j=1  A xv B | vij ij  A -v B )2 wi (vij ij  wi  n j=1  A vB vij ij  A |-|v B |)2 wi (|vij ij  wi  n j=1  A vB | |vij ij  Note that Dmax and Dmin are distance metrics that satisfy the triangle inequality.
Muse utilizes the lower bound of Eros, i.e., Dmin to filter out those MTS items that are not to be in the result set.
2.2  Distance-Based Index Structures  Eros  In [14], we proposed Eros as a similarity measure for multivariate time series.
Intuitively, Eros computes the similarity between two matrices using the principal components (PCs), i.e., the eigenvectors from the covariance matrices, and the eigenvalues as weights.
The weights are aggregated from the eigenvalues of all the MTS items in the database.
Hence, the weights change whenever data are inserted into or removed from the database.
Definition 1 Eros (Extended Frobenius norm).
Let A and B be two MTS items of size mA x n and mB x n, respectively1 .
Let VA and VB be two right eigenvector matrices obtained by applying SVD to the covariance matrices, MA and MB , respectively.
Let VA = [a1 , * * * , an ] and VB = [b1 , * * * , bn ], where ai and bi are column orthonormal vectors of size n. The Eros similarity of A and B is then defined as Eros(A,B,w)=  n i=1  wi |<ai ,bi >|=  n i=1  wi | cos thi |  (1)  where < ai , bi > is the inner product of ai and bi , w is a weight vector which is based on the eigenvalues of the MTS 1 MTS  items have the same number of columns (e.g., sensors), but may have different number of rows (e.g., time samples).
Intuitively, the distance-based index techniques, such as iDistance [15] and M-tree [3], work as in Algorithms 1 and 2.
The distance-based index structures have been shown to dominate feature-based index structures, such as R-tree and its variants, for high dimensional datasets.
iDistance and M-tree utilize the Euclidean distance.
Though the weighted Euclidean distance can be employed for iDistance and M-tree, the index structures should then be reconstructed whenever there is a change in the weight, i.e., in the database.
This reconstruction would be very costly when there are a lot of items in the dataset, and the items are continuously generated.
Algorithm 1 Distance-based Index : Construction 1: Partition the data or the data space.
2: Choose one reference point for each partition.
3: Compute the distance between all the data within the partition and its reference point.
4: Compute the max and min radii of each partition, i.e., the distances between the reference point and the farthest data item and the closest data item in the partition, respectively.
2 For simplicity, it is assumed that the covariance matrices are of full rank.
In general, the summations in Equation (1) should be from 1 to min(rA , rB ), where rA is the rank of MA and rB the rank of MB .
Algorithm 2 Distance-based Index : kNN Search 1: Given a query item Q for which the kNN search is performed, compute the distances between the query item and all the reference points.
2: Sort the partitions based on the distances to Q in non-decreasing order.
3: Search for kNNs of Q using the triangle inequality from within the closest partition.
Table 1.
Notations used in this paper Symbol A AT MA VA SA viA A vij A v*j w  n mA  Definition an m x n matrix representing an MTS item the transpose of A the covariance matrix of size n x n for A the right eigenvector matrix of size n x n A ] for MA , i.e., VA = [ v1A , v2A , * * * , vn an n x n diagonal matrix that has all the eigenvalues for MA obtained by SVD a column orthonormal eigenvector of size n for VA jth value of viA , i.e., a value at the ith column and the jth row of VA all the values at the jth row of VA a weight vector of size n, such that ri=1 wi = 1, [?
]i wi >= 0 number of variables, i.e., number of columns of a matrix number of samples, i.e., number of rows of a matrix A    3 Muse: Multilevel distance-based index structure for Eros In this section, we describe Muse (Multilevel distancebased index structure for Eros).
Muse constructs one level of distance-based index structure using each PC group, up to z levels.
Note that the weights are not utilized when Muse is constructed.
When performing a similarity search for a given query item Q, Muse combines the z levels constructed a priori with the weights to yield the lower bounds of the similarities between Q and all the MTS items in the database.
That is, the weights are applied not when constructing a z-level Muse, but when performing a similarity search, which allows the weights to get updated without the need to reconstruct the index structure.
Since Eros does not satisfy the triangle inequality, when performing a similarity search, Muse utilizes the lower bound of Eros, i.e., Dmin , to filter out those MTS items that are not to be in the result set.
For the MTS items that are not filtered out, Muse employs DEros to obtain the final result set.
We start by first describing how Muse is constructed, and how the similarity search for a given MTS item Q using Muse is performed in Sections 3.1 and 3.2, respectively.
Updating Muse is presented in Section 3.3, followed by the discussions on the selection of reference points for Muse in Section 3.4.
The discussion on the number of levels is presented in Section 3.5.
Table 1 lists the notations used in the remainder of this paper, if not specified otherwise.
3.1  Construction  Recall that for Eros, each principal component (PC) is assigned a weight which may change whenever items are inserted into or removed from the database.
The distancebased index structures, such as iDistance [15] and Mtree [3], utilize the Euclidean distance when building the index structures.
Though the weighted Euclidean distance metric, e.g., Dmin , may be employed for iDistance and Mtree, the index structure should be rebuilt when the weights change.
This is due to the fact that the weight is applied when constructing the index structure.
If an index structure is constructed without using the weights, and the weights can later be incorporated into the index structure constructed a priori, then the weights can change without having to rebuild the index structure.
That is, the index construction and the weight application should be independent.
This is exactly how Muse works for Eros, which will be subsequently described.
Let us denote the PC of an MTS item whose corresponding eigenvalue is the largest as the first PC of an MTS item, and that whose corresponding eigenvalue is the second largest as the second PC of an MTS item.
Let us subsequently call the first PCs of all the MTS items as the first PC group, and the second PCs of all the MTS items as the second PC group.
For Muse, we thusly construct one distancebased index structure, which is called a level, for each PC group of the dataset without using the weights.
In order to build a z-level Muse, we utilize the first z PC groups of all the data items.
Let us first define two more distance metrics to be used for building Muse.
Definition 2 A distance metric D|*|,k between two MTS items, A and B, using the kth PC is defined as follows: [?]
n A B 2 D|*|,k (A,B)=  j=1  (|vkj |-|vkj |)  and a distance metric D|*| between two n dimensional vectors, a and b, is defined as follows: [?]
n 2 D|*| (a,b)=  j=1  (|aj |-|bj |)  Note that Eros assigns a weight to each PC.
Hence, Muse constructs one level of a distance-based index structure using each PC group.
For each PC group, we first partition the PCs and assign one reference point for each partition.
We then compute the distances between each of the reference points and all the PCs which belong to that partition using D|*| .
The weights are computed separately by Algorithms 6 or 7 (Refer to [14] for details).
Algorithm 3 describes how to construct a z-level Muse, where Rjl represents the lth reference point at the jth level; rjl is the farthest distance from the lth reference point at the jth level; n is the number of the PCs of an MTS item and nR is the number of reference points.
dist[i, j] stores  the distance between the jth PC of the ith MTS item and its reference point.
partitionID[i, j] stores the ID of the partition to which the jth PC of the ith MTS item belongs in the jth level.
Intuitively, we store the ID of the partition to which each PC of an MTS item belongs and compute the distance between each PC and the reference point of its partition.
Algorithm 3 Muse : Construction Require: the number of all the MTS items in the dataset N , the number of levels to be built for Muse, z  n, reference points Rjl where 1 <= l <= nR and 1 <= j <= z; 1: for j=1 to z do 2: for i=1 to N do 3: E - the jth PC of the ith MTS item in the database; 4: l - the ID of the partition closest to E; 5: dist[i, j] - D|*| (Rjl , E); 6: partitionID[i, j] - l; 7: if rjl <= dist[i, j] then 8: rjl - dist[i, j]; 9: end if 10: end for 11: end for 12: Compute the weight vector w using Algorithm 6 or 7;  Figure 1(a) represents the conceptual diagram of Muse, where uj is the number of reference points at the jth level.
In this figure, the lth partition at the jth level is represented by a reference point Rjl and its radius rjl .
At the jth level (1 <= j <= z), the jth PC group is divided into uj partitions.
The lth partition (1 <= l <= uj ) at the jth level contains all the IDs of the MTS items whose jth PCs belong to this partition as well as the distances between Rjl and all the jth PCs of the MTS items in this partition.
When a similarity search is issued for a given MTS item Q, the pre-computed distances in the z-level Muse are then combined together with the weights to find the lower bounds, i.e., Dmin of the similarities between Q and all the MTS items in the database.
Note that in Algorithm 3, it is assumed for simplicity that the number of reference points, i.e., nR , is the same for all the levels.
3.2  k NN search using Muse  Given a query object Q, a set of objects D, and an integer k >= 1, k Nearest Neighbor (kNN) search is to find k objects in D which have the shortest distance from Q [3].
Range queries, which retrieve all the items within a fixed distance from a query object, would be simpler than kNN searches [15].
Hence, we concentrate on kNN searches in this paper.
Muse stores the ID of the partition to which each PC of an MTS item belongs and the distance between each PC and its partition's reference point.
When a kNN search is issued given a query item Q, we need to combine all the pre-computed distances between the PCs of an MTS item A  and the reference points as well as the weights w to obtain the lower bound of Dmin (A, Q, w).
There are two ways to compute the lower bound of Dmin (A, Q, w) using a zlevel Muse.
We first describe the naive way followed by the Muse way which generates a tighter lower bound.
To discuss the computation of the naive lower bound, assume a 2-level Muse with two partitions for each level as in Figure 1(b).
That is, we pre-compute all the distances between the first 2 PCs of all the MTS items and their reference points.
Using the first 2 PCs, we then have the following inequalities:    =  Dmin (A,Q,w)  >= >=  n i=1  n j=1  A |-|v Q |)2 wi (|vij ij  1-2 Dmin (A,Q,w)  (2)  1-2 1-2 Dmin (C,A,w)-Dmin (Q,C,w)  (triangle inequality)  where C = [c1 , c2 ], ci is the reference point of viA , i.e., the 1-2 ith PC of A, and Dmin is to use the first 2 PCs to com1-2 1-2 pute Dmin .
Dmin (C, A, w) and Dmin (Q, C, w) can be expanded and represented as follows: 1-2 Dmin (C,A,w)=    =  and 1-2 Dmin (Q,C,w)=    n j=1    n j=1  w1    w1 =  A 2 (|c1j |-|v1j |) +w2  n j=1  A 2 (|c2j |-|v2j |)  w1 (D|*|,1 (C,A))2 +w2 (D|*|,2 (C,A))2  Q (|v1j |-|c1j |)2 +w2  n j=1  Q (|v2j |-|c2j |)2  w1 (D|*|,1 (Q,C))2 +w2 (D|*|,2 (Q,C))2  D|*|,i (C, A) is computed a priori at the time of a z-level Muse construction and D|*|,i (Q, C) is computed once online when Q is provided.
Hence, we can immediately obtain the lower bound of Dmin (A, Q, w) using Muse when performing a kNN search.
Note that the pre-computed distance D|*|,i (C, A) is not affected even when the weight wi changes.
Consequently, the weight vector w can change without the need to rebuild the index.
Let us formally define the lower bound described above, termed LBN aive .
Definition 3 The lower bound LBN aive between two MTS items, A and Q, using a z-level Muse is defined as follows: [?]
z 2 LBN aive,z (A,Q,w)= i=1 wi (D|*|,i (C,A)) (3) [?]
z 2 -  i=1  wi (D|*|,i (Q,C))  where C = [c1 , * * * , cz ] and ci is the reference points of viA .
Yet, using Muse, we can obtain a tighter lower bound of 1-2 (A, Q, w) Dmin (A, Q, w) than LBN aive .
Consider Dmin in Equation (2), which can be expanded as follows: 1-2 Dmin  fifi fiw (A,Q,w)= +w 1  2  n j=1  Q 2 A (|v1j |-|v1j |)  n j=1  Q 2 A (|v2j |-|v2j |)  (4)  r11  Level 1  R 11  1  R 12  R 1u  1  r2u  r22  r  21  Level 2  r1u  r12  R 21  2  r11  R 2u  R 22  Level 1  2  r12  R 11  R 12 Pb  Pa  r22  r rz2  rz1  Level z  R zu  R z2  R z1  Level 2  rzuz z  21  R 21  R 22  Pc  (a)  Pd  (b)  Figure 1.
(a) Conceptual Diagram of Muse (b) 2-level Muse Then at the first level of Muse, i.e., using the first PCs, we have the following inequality: [?]
w1  w  1  n j=1  n j=1  w  A |-|v Q |)2 >= (|v1j 1j  A 2 (|c1j |-|v1j |) -  n j=1  1   wi (D|*|,i (C, Q))2 with Ai and Proof 1 Substitute  wi (D|*|,i (C, A))2 with Bi .
Then, [?]
z 2 LBM use,z (A,Q,w)=  (5)  Q (|v1j |-|c1j |)2  [?]
w2  2  n j=1  n j=1  w  A |-|v Q |)2 >= (|v2j 2j  A |)2 - (|c2j |-|v2j  n j=1  2  {Ai -Bi }  and  and at the second level, using the second PCs, we have  w  i=1  LBN aive,z (A,Q,w)=  (6)  Q (|v2j |-|c2j |)2  [?]
z i=1  A2i -  [?]
z i=1  Bi2  Square LBMuse,z (A, Q, w) and LBN aive,z (A, Q, w) and we get LBM use,z (A,Q,w)2 =  z i=1  A2i +  z i=1  Bi2 -2  z i=1  Ai Bi  (7)  By squaring and summing up Equations (5) and (6) and then taking its square root, we obtain the lower bound of Equation (4).
The lower bound described above is defined as follows:  and  Definition 4 The lower bound LBMuse between two MTS items, A and Q, using a z-level Muse is defined as follows:  Hence, the inequality between LBMuse,z and LBN aive,z depends on the last term of Equations (7) and (8).
Recall that Holder's Inequality [9] states that  LBM use,z  fifi fi (A,Q,w)=  z i=1   [?
]w (D  -[?
]w (D  where C = [c1 , * * * , cz ] and ci is i.e., the ith PC of A.
|*|,i (C,A))  i  i  2  2 |*|,i (Q,C))  	    LBN aive,z (A,Q,w)2 =  z i=1  A2i +  z i=1  [?]
Bi2 -2  z i=1  A2i  [?]
z i=1  Bi2  2  the reference points of viA ,  That is, LBMuse computes the lower bounds at each level, and sums up the lower bounds to yield the lower bound of Dmin between the MTS items and the query item.
LBMuse,z yields tighter lower bound of Dmin than LBN aive,z meaning that the former generates less false hits.
Hence, we use LBMuse,z when performing kNN searches with Muse.
Lemma 1 For the lower bounds LBN aive and LBMuse , the following inequality holds : LBMuse,z (A, Q, w) >= LBN aive,z (A, Q, w).
n k=1  |ak bk |<=(  n k=1  |ak |p )1/p (  n k=1  |bk |q )1/q  By Holder's Inequality where p = q = 2, we find the following inequality z i=1  |Ai Bi |<=(  z i=1  |Ai |2 )1/2 (  z i=1  |Bi |2 )1/2  Hence, by Equations (7), (8) and Holder's Inequality3, LBM use,z (A,Q,w)>=LBN aive,z (A,Q,w)  A kNN search using Muse is performed as in Algorithm 4.
Intuitively, we first sort the MTS items so that those in the partitions closer to the given query item Q will be examined first (Line 9).
This can be done as follows: See Figure 1(b), which is a 2-level Muse with two reference points 3 Note that A >= 0 and B >= 0.
Hence, |A | = A , |B | = B and i i i i i i |Ai Bi | = Ai Bi .
(8)  for each level.
Given a query item Q, assume that v1Q is closer to partition Pa than Pb , and v2Q is closer to partition Pd than Pc .
First, all the MTS items whose first PCs belong to partition Pa are identified.
Among these items, the MTS items whose second PCs belong to partition Pd are examined first, and then the items whose second PCs belong to partition Pc .
Similarly, the MTS items whose first PCs belong to partition Pb are examined.
This process is repeated for all the z levels of Muse.
The rest of the Algorithm 4 is described as follows: In Lines 1~3, knnDistance array is initialized.
The distances between the first z PCs of Q and the reference points are computed (Lines 4~8), and kNNs of Q are searched from the closest partition to Q (Line 9).
LBMuse,z is used to filter out the MTS items that are not to be in the candidate set (Line 12) and the refinement phase using DEros is performed in Lines 14~18.
The data structures knnDistance and knnID are updated so that they are sorted in non-decreasing order of DEros in knnDistance.
Algorithm 4 Muse : kNN search Require: the number of levels built for Muse, z  n, reference points Rjl where 1 <= l <= nR and 1 <= j <= z, a query MTS item Q and k; 1: for i=1 to k do 2: knnDistance[i] - [?
]; 3: end for 4: for t=1 to z do 5: for i=1 to nR do 6: distRQ[t, i] - D|*| (Rt1 , vtQ ); 7: end for 8: end for 9: Sort MTS items using distRQ so that the MTS items which belong to the partition closest to Q be checked first; 10: for i=1 to N do 11: A - the ith sorted MTS item; 12: if LBM use,z (A, Q) >= knnDistance[k] then 13: continue; 14: end if 15: if DEros (A, Q) <= knnDistance[k] then 16: knnDistance[k] - DEros(A, Q); 17: knnID[k] - id of A; 18: Update knnDistance and knnID; 19: end if 20: end for  3.3  Updating Muse  Recall that Muse considers the computation of distances between reference points and the MTS items, and the computation of weights separately.
As described in Section 3.2 of [14], all the eigenvalues from the dataset are stored in an n x N matrix S, where n is the number of variables and N is the number of MTS items in the dataset.
When a new MTS item is added into the database, the weights for Eros can thusly be updated as in Algorithm 5.
The MTS item can be inserted into Muse following Lines 3~9 of Algorithm 3.
When MTS items are removed from the database, Muse can be updated similarly.
Algorithm 5 Update weights for Muse 1: function updateWeight(S,S') Require: an n x N matrix S, where n is the number of variables for the dataset and N is the number of MTS items in the dataset.
An n x N  matrix S', where n is the number of variables for the dataset and N  is the number MTS items newly added to the dataset.
2: Snew - concatenate S and S'; 3: computeWeightRaw(Snew ) or computeWeightRatio(Snew );  Algorithm 6 Computing a weight vector w based on the distribution of raw eigenvalues 1: function computeWeightRaw(S) Require: an n x N matrix S, where n is the number of variables for the dataset and N is the number of MTS items in the dataset.
Each column vector si in S represents all the eigenvalues for the ith MTS item in the dataset.
sij is a value at column i and row j in S. s*i is the ith row in S. si* is the ith column, i.e, si .
2: for i=1 to n do 3: wi - f (s*i ); 4: end for 5: for i=1 to n do 6: wi - wi / n j=1 wj ; 7: end for    Algorithm 7 Computing a weight vector w based on the distribution of normalized eigenvalues 1: function computeWeightRatio(S) Require: the same as Algorithm 6.
2: for i=1 to N do 3: si - si / n j=1 sij ; 4: end for 5: computeWeightRaw(S);    3.4  Reference Points  The choice of reference points affects the performance of the distance-based index structures [15] and the index should be re-built when the reference points change.
Therefore, the reference points should be chosen with care.
First, let us consider the data, i.e., the PCs, for which an index structure is built.
The PCs are normal vectors, whose norms are 1s.
Hence, they can be represented as points in a hypersphere whose radius is 1.
If we take the absolute values of the PCs in order to compute D|*| , the resultant PCs are represented as points on the hypersphere in the first quadrant (see Figure 2 for an example in 3D space).
Based on this observation, for our experiments, the edge points where the hypersphere meets each axis are chosen as the reference points at each level.
And the reference points are the same for all the levels.
There are two advantages to this heuristic: 1) As observed in [15], this would in general  1 0.9 0.8  Pruning Ratio  0.7 0.6 0.5 0.4 0.3 0.2 LB  Muse  0.1  LB  Naive  0  2  3  4  5 6 7 # of items (x 10000)  8  9  10  Figure 2.
The data space of the first PC group for Muse in 3D  Figure 3.
Pruning ratios of LBMuse with LBN aive  reduce the amount of overlap among partitions, and 2) It is easy to find the partition to which a PC belongs.
The A |, i.e., the dimension partition ID of a PC viA is argmax |vij  small random variation to each of the AUSLAN MTS item.
The Australian Sign Language (AUSLAN) dataset 4 uses 22 sensors on the hands to gather the datasets generated by signing of a native AUSLAN speaker.
It contains 95 distinct signs, each of which has 27 examples.
In total, the number of signs gathered is 2565.
The average length is around 60.
The variation added to each of the AUSLAN MTS item is a vector whose values are chosen from the interval [0, 1] and then processed so that its mean is 0 and its values are between [-, ].
For experiments,  is chosen to be 0.05.
We use each AUSLAN data as seed and generate approximately 30,000 to 100,000 items.
j  whose absolute value is the maximum.
Hence, we do not have to compute the distances between a PC and reference points to find out to which partition a PC belongs.
3.5  Levels of Muse  Muse utilizes the first z PC groups to construct a z-level index structure.
The computation of distances and weights are separately considered.
When a kNN search is performed, the distances computed a priori are combined with the weights to yield a lower bound, i.e., LBMuse .
On one hand, the greater the number of levels of Muse is, the tighter the LBMuse is.
One the other hand, the greater the number of levels, the longer it would take to compute LBMuse and to perform Line 9 of Algorithm 4.
Hence, the number of levels of Muse should be decided with discretion.
Recall that the weights used for Muse are based on the distribution of the eigenvalues obtained from all the MTS items in the database.
Hence, the weights will have similar characteristics as the eigenvalues, i.e., the first few weights have large values while the rest have small values close to zero.
For our experiments, we employed similar heuristics used for SP CA to choose the number of principal components.
The number of levels, z, is chosen such that the sum of the first z weights is greater than 0.99.
4 PERFORMANCE EVALUATION 4.1  Datasets  In order to show the efficiency of Muse, we create a set of clustered synthetic dataset, SY N T H, as in [1], by adding a  4.2  Methods  We compare Muse to M-tree in terms of pruning ratio and processing time.
Pruning Ratio is the ratio of the number of items pruned to the number of items in the database [7].
The processing time of Muse is also compared to that of sequential scan.
Recall that Eros, in itself, is not a distance metric; Eros utilizes its lower bound, Dmin , to perform similarity search efficiently.
Hence, M-tree cannot be used with Eros.
Moreover, M-tree cannot be used with weighted distance metrics whose weights may change frequently; M-tree should be reconstructed each time the weights change.
Therefore, in order to compare Muse and M-tree, we modified both of them to compute the distance between two MTS items using Dmin which is a distance metric, and assumed there would be no change in the database once the index structures are constructed.
Page sizes of 8KB and 16KB are employed for M-tree.
We only show the result of 16KB M-tree, which is better than that of 8KB M-tree.
For the reference points of Muse, as described in Section 3.4, the edge points where the hypersphere meets each axis are chosen at each level.
Muse is implemented in 4 http://kdd.ics.uci.edu/  1  5  5  0.9  4.5 4  0.6  3  0.5 0.4  2  0.3 0.2  1 Pruning Ratio Processing Time Sequential Scan  0.1 0  4  1  2  3  4  5  3.5 3 second  Pruning Ratio  0.7  Processing Time (second)  0.8  2.5 2 1.5 1 0.5  6  0  # of levels  (a) Trade-offs of Muse  0  Sequential Scan Muse 2  3  4  5 6 7 # of items (x 10000)  8  9  10  (b) Processing Time  Figure 4.
(a) shows the trade-offs of Muse : Pruning Ratio vs Processing Time vs Level.
(b) depicts the Processing times of Muse and Sequential Scan using Eros as the similarity measure.
both C++ and Matlab.
The experiments are performed on a Pentium IV 3.2GHz machine with 3GB of main memory.
4.3  RESULTS  The pruning ratios of LBMuse and LBN aive using the SYNTH dataset is presented in Figure 3.
This figure confirms Lemma 1 as LBMuse yields tighter lower bound than LBN aive resulting in higher pruning ratio than LBN aive by more than 10%.
For Muse, even though the number of levels increases, the pruning ratio does not improve much after level 3, while the processing time more than doubles when the number of level changes from 5 to 6, as Figure 4(a) shows using the SYNTH dataset with 30K items.
Also, the performance is worse than sequential scan when the number of levels is greater than 5, where the overhead of computing LBMuse and performing Line 9 of Algorithm 4 begin to overwhelm.
Moreover, the number of partitions would be nz for a z-level Muse.
For the AUSLAN dataset, there would be 234256 (224 ) partitions for a 4-level Muse.
Hence, we suggest no more than 4 levels for Muse in general.
Figure 4(b) demonstrates that Muse is almost 4 times faster than sequential scan when the number of data items is 100,000.
The length of a PC for each MTS item in the SYNTH dataset is 22.
3-level Muse, which utilizes the first 3 PCs, has 66 dimensions.
Figure 4(b) shows that Muse works well for high dimensional datasets, while the featurebased index techniques, such as R-tree and its variants, become inefficient when the dimension is greater than 20 [1].
The pruning ratios of M-tree and 3-level Muse when Dmin distance metric is employed are shown in Figure 5(a), for 1 NN searches (K1), 5 NN searches (K5) and 10 NN searches (K10).
Recall that, unlike Muse, M-tree cannot be  utilized, e.g., for Eros, when the weighted Euclidean distance is the distance metric to be used for the index structures, and the weight keeps changing whenever new data items are inserted into and/or removed from the database, which would probably be the usual case when dealing with real-world time series datasets.
Though 3-level Muse utilizes only the first three PC groups (i.e., 66 dimensions), and M-tree considers all the PC groups (i.e., 22 x 22 = 484 dimensions), Figure 5(a) represents that the pruning ratio of 3-level Muse is comparable to that of M-tree.
When performing 1 NN searches, the pruning ratio of Muse is even higher than that of M-tree.
This is due to the fact that the weights for Eros are based on the distributions of the eigenvalues, the first few of which represent more than 99% of the total variance.
Another reason of M-tree's poor performance in processing time would be that M-tree does not utilize all the pre-computed distances immediately; M-tree can only utilize the distances of MTS items in the visited nodes.
Similar result has also been observed in [2].
Figure 5(b) shows that 3-level Muse outperforms M-tree in processing time, which may reconfirm the aforementioned limitation of M-tree.
5 CONCLUSIONS AND FUTURE WORK We proposed a multilevel distance-based index structure, termed Muse (Multilevel distance-based index structure for Eros) for efficient retrieval of MTS items using Eros [14].
Muse builds a number of levels, each of which is a distancebased index structure corresponding to one PC group.
Hence, for a z-level Muse, the first z PC groups are utilized.
Since Eros assigns a weight to each PC group, each level of Muse can be constructed without considering the weight.
When performing a similarity search, we combine  1 0.9  0.9  0.8 Processing Time (secconds)  1 0.95  Pruning Ratio  0.85 0.8 0.75 0.7 0.65  3-level Muse(Dmin, K1) 3-level Muse(Dmin, K5) MTree(Dmin, 16KB, K1) MTree(Dmin, 16KB, K5) MTree(Dmin, 16KB, K10) 3-level Muse(Dmin, K10)  0.6 0.55 0.5  2  3  4  5 6 7 # of items (x 10000)  8  9  MTree(Dmin, 16KB, K10) MTree(Dmin, 16KB, K5) MTree(Dmin, 16KB, K1) 3-level Muse(Dmin, K10) 3-level Muse(Dmin, K5) 3-level Muse(Dmin, K1)  0.7 0.6 0.5 0.4 0.3 0.2 0.1  10  (a) Pruning Ratio  0  2  3  4  5 6 7 # of items (x 10000)  8  9  10  (b) Processing time  Figure 5.
For the SYNTH dataset, (a) shows the pruning ratios of 3-level Muse utilizing 66 dimensions and M-tree with page size of 16KB employing 484 dimensions, and (b) depicts the processing times of both methods.
The distance metric Dmin is employed for both Muse and M-tree for the sake of comparison.
the z levels with the weights to compute the lower bounds between the query item and the items in the database, and filter out those that are not to be in the result sets.
For items that are not filtered out, the refinement is performed using DEros to obtain the result.
Using a set of synthetically generated clustered datasets, we showed that Muse outperforms the sequential scan and M-tree in terms of pruning ratio and processing time.
We intend to extend this work to obtain possibly higher pruning ratio by utilizing the upper bound of DEros , i.e., Dmax , as well as the lower bound of DEros , i.e., Dmin .
In addition, we also plan to figure out if some simpler structures, such as B+tree utilized in iDistance [15], can be employed for Muse.
If this is plausible, Muse may be easily plugged into commercial database systems.
Acknowledgment This research has been funded in part by NSF grants EEC-9529152 (IMSC ERC), IIS-0238560 (PECASE) and IIS-0307908, and unrestricted cash gifts from Microsoft.
Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.
The authors would also like to thank the anonymous reviewers for their valuable comments.
References [1] T. Bozkaya and M. Ozsoyoglu.
Indexing large metric spaces for similarity search queries.
ACM TODS, 24(3), 1999.
[2] L. Chen and R. Ng.
On the marriage of lp-norms and edit distance.
In Proc.
of the VLDB Conference, 2004.
[3] P. Ciaccia, M. Patella, and P. Zezula.
M-tree: An efficient access method for similarity search in metric spaces.
In VLDB, 1997.
[4] R. O. Duda, P. E. Hart, and D. G. Stork.
Pattern Classification.
Wiley Interscience, second edition, 2001.
[5] C. Goutte, P. Toft, E. Rostrup, F. A. Nielsen, and L. K. Hansen.
On clustering fMRI time series.
NeuroImage, 9(3):298-310, 1999.
[6] I. T. Jolliffe.
Principal Component Analysis.
Springer, 2002.
[7] E. Keogh.
Exact indexing of dynamic time warping.
In Proc.
of the VLDB Conference, 2002.
[8] W. Krzanowski.
Between-groups comparison of principal components.
JASA, 74(367), 1979.
[9] T. K. Moon and W. C. Stirling.
Mathematical Methods and Algorithms for Signal Processing.
Prentice Hall, 2000.
[10] H. Sakoe and S. Chiba.
Dynamic programming algorithm optimization for spoken word recognition.
IEEE Trans.
Acoust., Speech, Signal Processing, 26(1), 1978.
[11] C. Shahabi.
AIMS: An immersidata management system.
In VLDB CIDR, January 2003.
[12] C. Shahabi and D. Yan.
Real-time pattern isolation and recognition over immersive sensor data streams.
In Proc.
of the 9th Int.
Conference On Multi-Media Modeling, 2003.
[13] A. Tucker, S. Swift, and X. Liu.
Variable grouping in multivariate time series via correlation.
IEEE Trans.
Syst., Man, Cybern.
B, 31(2):235-245, 2001.
[14] K. Yang and C. Shahabi.
A PCA-based similarity measure for multivariate time series.
In Proc.
of the Second ACM International Workshop on Multimedia Databases, 2004.
[15] C. Yu, B. C. Ooi, K.-L. Tan, and H. V. Jagadish.
Indexing the distance: An efficient method to KNN processing.
In VLDB, pages 421-430, September 2001.
[16] X. L. Zhang, H. Begleiter, B. Porjesz, W. Wang, and A. Litke.
Event related potentials during object recognition tasks.
Brain Research Bulletin, 38(6):531-538, 1995.
Hypothetical Reasoning From Situation Calculus to Event Calculus Alessandro Provetti  CIRFID - Universita di Bologna Via Galliera 3/a, Bologna I-40121 ITALY provetti @cird unibo it :  Abstract  Pinto and Reiter have argued that the Situation Calculus, improved with time handling axioms, subsumes the features of linear time temporal formalisms such as Event Calculus and Interval Logic.
In this note we nd answers to some of their remarks by showing a modied version of Event Calculus that seems to match Situation Calculus handling of hypothetical reasoning and projection.
Further consideration on semantics and expressive power of Event Calculus put forward by Pinto and Reiter are discussed in the light of recent proposal for an unifying semantics for languages for time and actions.
1 Introduction  In their very recent production, Reiter and Pinto7, 8] have introduced an upgraded version of Situation Calculus (SC) which makes it possible: to represent dates and time-stamp actions and situations which actually occurred in the world to represent actual situations as a branch of the tree of possible developments of things that Situation Calculus handles.
This new features are obtained by adding new predicate denitions and introducing a new sort of constants for representing dates, a convenient ordering, and functions such as Start(action) or End(action), linking actions to their dates.
Pinto and Reiter argue that the improved version matches the so-called linear time formalisms, viz.
Allen's Interval Logic and the Calculus of Events(EC) of Kowalski and Sergot3], on their own ground: representing actions and change over time.
Nonetheless, the resulting Situation Calculus maintains intact its native characteristics(set out in 5]) of dealing with alternative, hypothetical Work done during author's stay at Computer Science Department of University of Texas at El Paso, which is gratefully acknowledged.
:  plans/sequences of actions and projecting their effects.
Another point raised by Pinto and Reiter is on semantics: they present a logic programming implementation of a subset of the formalism which enjoys a clear completion-based semantics, in contrast with EC relying on Negation as Failure.
In this paper it will be counter-argued that Situation Calculus specic -and indeed desirable- features are easily implementable in a linear-time formalism like Event Calculus.
In chapter 2 a simple version of EC is presented which departs from the original version criticized by Pinto et al.
but can be taken as representative of current versions of EC.
In chapter 3 new predicates are introduced for allowing reasoning about a ctional sequence of actions and projecting the value of uents.
This simulation can be either performed in the future, for exploring the result of alternative plans or starting from a date in the past, which allows for counterfactual reasoning.
In chapter 4 the declarative semantics aspect is discussed if an EC axiomatization is seen as a logic program, then the most common declarative semantics agree, yielding what is believed a clear semantics.
Indeed, a new semantics is proposed by translating EC axiomatizations to the language A of Gelfond and Lifschitz 1], which enjoys a semantics conceived for actions and change.
A translation from a domain description EC-style to one in A is proposed which maps also the closed-world assumption into the target axiomatization.
This technical result is kept for a full version of the paper, while it would be necessary to dene a similar translation from Pinto and Reiter's formalisms to A itself it will then result very interesting to compare the two axiomatizations and their models within the same language.
This approach is specular to that of Kartha in 2] on translating A to chosen nonmonotonic formalisms In the end, the author argues for a substantial equivalence of the two (improved) formalisms.
In the rest of the paper acquaintance with Situation Calculus and the semantics of Logic Programming is assumed.
2 The Event Calculus of the 90s  The Event Calculus has been proposed by Kowalski and Sergot 3] as a system for reasoning about time and actions in the framework of Logic Programming.
Event Calculus is based on an ontology of events, assumed as primitive.
These events are represented by means of constants that uniquely identify them.
The second ontology is that of uents 1, which represents descriptions of the reality being modeled.
A uent holds over time from the moment when an event initiates it, i.e.
the event makes it true in the world.
Events may also terminate, i.e.
make false in the world, uents.
The Event Calculus is based on forward default persistence: a uent holds over time until a terminating event is recorded.
Since the rst proposal, a number of improved formalization have steamed, in order to adapt the calculus to dierent tasks.
Hence, the reduced version of Shanahan in11] in presented, since it can be taken as a common-core denition embedded in the latest applications2.
Events are represented by sets of instantiations like the following: Happens(E1) Date(E1  T1) Act(E1 Unstack(B )) Notice that there are both event-tokens, labeled with the constants E1  E2 : : : and events-types named by Unstack, Stack etc.
The eect of an actiontype(its meaning) is understood by looking at the Initiates=Terminates axioms where it appears.
The denitions of Initiates and Terminates are for expressing domain knowledge.
A convenient example is the Block World, as both Shanahan and Pinto et al.
use it: Initiates(e On(x y))  Act(e Move(x y))  Initiates(e Clear(z ))  Act(e Move(x y)) Date(e t) HoldsAt(On(x z ) t) z 6= y T erminates(e Clear(y))  Act(e Move(x y)) T erminates(e On(x y))  Act(e Move(x z )) z 6= y Elsewhere called properties or relationships.
This version is even more simplied, as it assumes events are recorded in the database in the same order as they happened in reality.
For discussing a fuller formalization, the reader is invited to consult late works of Sergot10] and Sripada13].
1 2  Starting from a database of events and a domain description by Initiates=Terminates the axioms of EC makes it possible to derive atoms:  Holds(F T ) which are understood as "uent F is true at time T".
Axiom ECI means that a uent holds at a certain time if an event happened earlier initiated the uent itself and there is no evidence in the database of the uent stopping to hold in the meantime.
In other words, in the interval between the initiation of the uent and the time the query is about, no terminating events must happen.
This is made sure by axiom ECII .
The forward default persistence rule is implemented by using Negation as Failure on Clipped in ECI.
(ECI ) HoldsAt(f t)  Happens(e) Initiates(e f ) Date(e ts ) ts < t not Clipped(ts  p t) (ECII ) Clipped(ts  f t)  Happens(e ) Terminates(e  f ) Date(e  t ) ts < t  t fit The predicates < and fi establish an ordering on events.
We stipulate that temporal constants T1  T2 T3 : : : are mapped on naturals, and that the ordering relations are also mapped on the same relations on naturals, thus inheriting their properties.
In chapter 3, an improved version of the axioms will be presented in order to deal with hypothetic events.
The hypothetic events have no timestamping, so that the problem of integrating the linear order of actual events and the order on those hypothetical is not addressed directly.
2.1 The Assumption Underlying Event Calculus  EC is a formalism based on negation-as-failure.
This device implements the implicit assumptions on the knowledge of the domain that are used by EC.
Techniques are available, viz.
explicit negation, for making these closure assumptions explicit.
Let us list these assumptions, taking advantage of the discussions in 9, 11]: It is assumed that no events occur other than those which are known to occur.
It is assumed that all the events are timestamped.
These two assumptions seems too strong for real applications such as database updates in fact, they are lifted in enriched versions of EC.
It is assumed that no types of events can aect a given uent other than those which are known to do so This assumption can be made explicit by resorting to classical negation with these axioms: :Initiates(e f )  not Initiates(e f )  :Terminates(e f )   not Terminates(e f ) This approach is semantically founded on the Answer Sets semantics of Gelfond and Lifschitz and, for matter or generality, won't be used in the rest of the paper.
It is assumed that uents persist until an event happen that inuence them.
Conversely, It is assumed that every uent has an explanation in terms of events.
That is, at least one initiating event is necessary for making a uent true.
This is particularly interesting for generating explanations of uents by abducing events11].
If observations on the value of uents can be introduced in the formalization, i.e.
HoldsAt updates are allowed, a transformation of the axioms is necessary for giving consistent answers, at cost of a loss of elegance Sripada13] presents a version of the calculus for accommodating such updates.
3 Hypothetical Reasoning in EC  In this section we dene new predicates (on top of those already existing) for performing projection of hypothetical sequences of actions.
The purpose is that eectively illustrated by Pinto and Reiter7]: By preserving the branching state property of the Situation Calculus, we can express and answer a variety of hypothetical queries, although counterfactuals cannot be expressed.
For example "At time Tp in the past, when you put A on B, could A have been put on C instead?"
can be simply expressed as: during(Tp  s) ^ actual(s)  possible(put(A C ) s): "If I had performed put(A C ), would F have been true?"
V holds(F do(put(A C ) Sp)) possible(put(A C ) Sp ): None of these features is possible in linear temporal logics.
We need the branching structure of the situation calculus, coupled with a linear time line in that branching structure.
In the following, the new axioms and a modied and enriched version of the old ones will be illustrated, so that to deal with the sample queries proposed.
3.1 The new predicates  The ideas motivating the new predicates denition are the following: to rewrite situation calculus axioms within EC, in order to carry out projection to provide a link between the point in time t where the simulation begins and the value of uents in the simulation.
That is, uents that are true at t are still true during the simulation as long as an event does not terminate them.
To this extent, the eect of the simulation depends from the time it starts to make it possible both to project in the future and to reason hypothetically about a sequence of actions to this extent, the eect of a simulation does not depend from the time it starts.
HypHolds  The new predicate HypHolds is the counterpart of Situation Calculus Holds and it is understood as follows: a) HypHolds(F E type T ) is true if -has E type been performed at time T - F would be true thereafter.
(EC 1) HypHolds(f Res(e type t))  MayHappen(e type t) Initiates(e type f t) (EC 2) HypHolds(f Res(e type t))  MayHappen(e type t) not Terminates(e type f t) HoldsAt(f t) Now the predicate is dened for an arbitrary sequence of actions performed starting from T : b) HypHolds(F Res(An  Res(: : : Res(A1  T ) : : :))) is true if -has the sequence of actions A1 : : :An been performed starting from T - then F would be true thereafter.
In practice T replaces S0 , thus linking the chain of actions to the starting point of the simulation.
(EC 3) HypHolds(f Res(e type s))  HypMayHappen(e type s) HypInitiates(e type f s) (EC 4) HypHolds(f Res(e type s))  HypMayHappen(e type s) HypHolds(f s) not HypT erminates(e type f s)  Starting the simulation with t = 0, where each uent is false (by NAF) is a way to study in insulation the net eect of a plan.
MayHappen  In order to ensure that an action(i.e.
a type of event) can be performed at a certain time or in a certain state of aairs, the predicate MayHappen and HypMayHappen are introduced: MayHappen(E type t)  HoldsAt(C1  t) ::: HoldsAt(Cn  t) For instance: MayHappen(Move(a b) t)  HoldsAt(Clear(b) t) For each MayHappen instantiation, a relative instantiation of HypMayHappen is made For instance: HypMayHappen(Move(a b) s)  HypHolds(Clear(b) s)  HoldsAt and Clipped  The modications to these predicates are not substantial, some folding operation has been carried out and the arity of Initiates and Terminates has been increased to accommodate the parameter time.
As far as it goes, this version is expected to give the same results as Shanahan's in terms of success of HoldsAt queries.
Initiates and Terminates  Also for these predicates duplication is necessary in order to handle both dates and situations.
The new denition of Initiates and HypInitiates are like in this example: Initiates(e On(x y) t)  Act(e Move(x y)) Date(e t)  Initiates(e Clear(z ) t)  Act(e Move(x y)) HoldsAt(On(x z ) t) Date(e t) z 6= y HypInitiates(Move(x y) On(x y) s) HypInitiates(Move(x y) Clear(z ) s)  HypHolds(On(x z ) s) z 6= y A similar transformation must be applied to the denition of Terminates.
3.1.1 The new predicates at work  The rst question addressed by Pinto and Reiter: "At time Tp in the past, when you put A on B, could A have been put on C instead?"
translates into the following: ?
; MayHappen(Put(A C ) Tp ) Conversely, the second example: "At time Tp in the past, when you put A on B, could A have been put on C instead?"
translates into: ?
; HypHolds(On(A C ) Res(P ut(A C ) Tp))  4 Comparing the Semantics  Pinto and Reiter7] have compared the standard "rst-order + circumscription" semantics with that of EC: One advantage of this is the clean semantics provided by our axiomatization, in contrasts to the event calculus reliance on the Negation as failure feature of logic programming, whose semantics is not well understood.
The argument is rather appropriate, EC has been natively dened within Logic Programming 3, 10, 4] and the use of negation as failure for implementing default persistence is somehow intrinsic to EC.
It is nonetheless the case to notice that the set of axioms described in this paper (PEC ) form together a stratied logic program in the sense of Apt et al.6], under the following stratication 3: <p = fHoldsAt HypHolds MayHappen HypMayHappen Initiates HypInitiatesg < fClipped T erminates HypTerminatesg < f< fig < fHappens Act Dateg On stratied programs the semantics common in literature hold a unique minimal model.
This is the case for Przymusinki's perfects models semantics6] by taking the partition as an ordering over predicates the same goes for Apt et al.
6] iterated Fixpoint technique and for Gelfond and Lifschitz's Stable Models semantics.
The resulting, minimal and unique model of these semantics should carry an unambiguous meaning for EC4 .
Taking <p as a circumscribing policy, the perfect model results in a model of prioritized circumscription CIRC (PEC  <p ) for the theory PEC  it may be rewarding to compare the respective circumscriptive 3 This stratication is in fact redundant, but ts better intuition on layers of predicates.
To the extent of dening the declarative semantics predicates < and can be dened as a set of ground instances on time constants.
4 Notice in passing that Conjecture 1 of Apt et al.
in 6] ascribes to stratied programs the completeness of SLDNF resolution.
models of two intuitively equivalent theories in EC and SC.
This has not yet been carried out to author's knowledge.
4.1 Alternative Semantics  Beside the stratication-based semantics discussed above, there have been eorts to provide alternative semantics for event calculi a rst attempt is probably that of Shanahan12], who discussed a characterization in terms of circumscription.
In this section it is proposed an alternative approach by translation of Event Calculus formalizations to the language A of Gelfond and Lifschitz1], which enjoys a declarative semantics purported to actions and uents..
The translation  transforms a set of event descriptions in terms of Happens, Date etc.
into a correspondent set of A axioms.
The result sought after is soundness and completeness of the translation of an EC domain description D and of a query ?
; HoldsAt(F T ) into an domain description  (D) and a v-proposition F after CD (T ) such that:  D `EC HoldsAt(F T ) ()  (D) j=A F after CD (T ) where the chronicle CD (T ) is the list of actions happened before T in D and ordered by means of their dates.
The proof of this proposition will be included in the full version of paper.
The advantages of the translation are twofold: EC is given a new semantics and, in principle, at least a signicant class of A axiomatizations might be eectively computed in Prolog by dening a reverse translation to EC programs.
As soon as a similar translation from extended SC to A will become available, it will be possible to compare the two languages within the same semantical framework.
5 Conclusion  Similarities and dierences between Event Calculus and Situation Calculus have been subject of much attention in the latest literature4, 7, 8].
On the one hand, Pinto and Reiter have successfully implemented the treatment of time into SC thus matching the results obtainable with EC.
This work, on the other hand, has shown an improved version of EC which performs hypothetical reasoning on the eect of actions, one of the features that motivated Situation Calculus at its birth5].
Far this undertake from being nished, the author argues for a substantial equivalence of the two formalisms on the ground of expressive power, clear semantics and computational properties.
As for exibility, extended versions of Event Calculus existing in the literature for dealing with compound events, temporal granularities and continuous processes are quite encouraging, as well as applications to abductive planning, deductive databases and process modeling in areas such as engineering and Law.
As for elegance, tastes probably matter.
The present author feels easier at Event Calculus because of a more intuitive ontology of events and dates rater than actions, situations and dates5 , because of a plain computational value of the axiomatization and because the closed-world based semantics need not careful metatheoretical specications(circumscription) to yield the expected results.
This is not to say that all the aws of EC Pinto and Reiter point to can be easily xed.
As an instance, the aim to provide names for intervals of time bounded by events partially known has resulted in the rst formalization of EC allowing unintended models, as shown in 7].
The quest for improving EC is helped by such criticisms, as long as they recognize the long way EC has gone since 1986.
Acknowledgments  My thanks to Michael Gelfond, Chitta Baral, Stefania Costantini, Gaetano Lanzarone, Paulo Azevedo and Angelo Montanari.
References  1] Michael Gelfond and Vladimir Lifschitz.
Representing Actions and Change by Logic Programs.
In The Journal of Logic Programming., Vol.
17(2,3,4),november 1993. pages 301-355.
2] G. Neelakantan Kartha.
Soundness and Completeness Theorems for Three Formalizations of Action.
Proc.
of IJCAI'93 Conference, 1993. pages 724{729.
3] Robert Kowalski and Marek Sergot.
A Logicbased Calculus of Events.
New Generation Computing, volume 4 pages 67{95.
Ohmsha Ltd and Springer Verlag, 1986 4] Robert Kowalski.
Database Updates in the Event Calculus.
Journal of Logic Programming, volume 12, June 1992, pages 121{146.
5] John McCarthy and Patrick Hayes.
Some philosophical problems from the standpoint of articial intelligence.
In B. Meltzer and D. Michie, editors, Machine Intelligence, volume 4, pages 463{ 502.
Edinburgh University Press, Edinburgh, 1969.
6] Jack Minker, editor.
Foundations of Deductive databases and Logic Programming.
Morgan Kaufmann Publ., 1988.
7] Javier Pinto and Raymond Reiter.
Adding a Time Line to the Situation Calculus.
Working Papers of Common Sense '93, The second AAAI symposium on logical formalizations of common sense reasoning.
Austin(Tx), January 1993.
See 9] for a discussion on the ontologies of such formalisms 5  8] Javier Pinto and Raymond Reiter.
Temporal Reasoning in Logic Programming: A Case for the Situation Calculus.
Proceedings of ICLP'93 Conference.
Budapest, June 1993.
9] Alessandro Provetti.
Action and Change in Logic Programming: Event Calculus, Situation Calculus and A .
Manuscript.
Spring 1993.
10] Marek J. Sergot.
(Some topics in) Logic Programming in AI.
Lecture notes of the GULP advanced school on Logic Programming.
Alghero, Italy, 1990.
11] Murray P. Shanahan.
Prediction is Deduction but Explanation is Abduction.
Proc.
of IJCAI'89 Conference.
Detroit, 1989. pages 1055{1050.
12] Murray P. Shanahan.
A Circumscriptive Calculus of Events.
Imperial College Dept.
of Computing Technical Report.London, 1992.
13] Sury Sripada.
Temporal Reasoning in Deductive Databases.
PhD Thesis in Computing.
Imperial College, London, 1991.
A presentation of this work can be found in the Proc.
of IJCAI'93, pages.
860{865.
Constraint-Based Qualitative Simulation  arXiv:cs/0504024v1 [cs.AI] 7 Apr 2005  Krzysztof R. Apt National University of Singapore, Singapore, and CWI and UvA, Amsterdam, The Netherlands apt@comp.nus.edu.sg  Abstract We consider qualitative simulation involving a finite set of qualitative relations in presence of complete knowledge about their interrelationship.
We show how it can be naturally captured by means of constraints expressed in temporal logic and constraint satisfaction problems.
The constraints relate at each stage the 'past' of a simulation with its 'future'.
The benefit of this approach is that it readily leads to an implementation based on constraint technology that can be used to generate simulations and to answer queries about them.
1 Introduction Qualitative reasoning was introduced in AI to abstract from numeric quantities, such as the precise time of an event or the location or trajectory of an object in space, and to reason instead on the level of appropriate abstractions.
Two different forms of qualitative reasoning were studied in the literature.
The first one is concerned with reasoning about continuous change in physical systems, monitoring streams of observations and simulating behaviours, to name a few applications.
The main techniques used are qualitative differential equations, constraint propagation and discrete state graphs.
For a thorough introduction see [14].
The second form of qualitative aims at reasoning about contingencies such as time, space, shape, size, directions, through an abstraction of the quantitative information into a finite set of qualitative relations.
One then relies on complete knowledge about the interrelationship of these qualitative relations.
This approach is exemplified by temporal reasoning due to [1], spatial reasoning introduced in [10] and [20], reasoning about cardinal directions (such as North, 0 2005 c IEEE.
Personal use of this material is permitted.
However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from the IEEE.
Sebastian Brand National University of Singapore, Singapore brand@comp.nus.edu.sg  Northwest), see, e. g., [16], etc.
For a recent overview of this approach to spatial reasoning, see [8].
Qualitative simulation deals with reasoning about possible evolutions in time of models capturing qualitative information.
One assumes that time is discrete and that only changes adhering to some desired format occur at each stage.
[15] discusses qualitative simulation in the first framework, while qualitative spatial simulation is considered in [9].
Our aim here is to show how qualitative simulation in the second approach to qualitative reasoning (exemplified by qualitative temporal and spatial reasoning) can be naturally captured by means of temporal logic and constraint satisfaction problems.
The resulting framework allows us to concisely describe various complex forms of behaviour, such as a simulation of a naval navigation problem or a solution to a version of a piano movers problem.
The domain knowledge is formulated using a variant of linear temporal logic with both past and future temporal operators.
Such temporal formulas are then translated into constraints.
The usual constraint-oriented representation of the second approach to qualitative reasoning is based on modelling qualitative relations as constraints.
See, for example, [11] for an application of this modelling approach.
In contrast, we represent qualitative relations as variables.
This way of modelling has important advantages.
In particular, it is more declarative since model and solver are kept separate; see the study of the relation variable model in [6].
In our case it allows us to express all domain knowledge on the same conceptual level, namely as constraints on the relation variables.
Standard techniques of constraint programming can then be used to generate the simulations and to answer queries about them.
To support this claim, we implemented this approach in the generic constraint programming system ECLi PSe [22] and discuss here several case studies.
coveredby  2 Simulation Constraints  a b  2.1 Constraint Satisfaction Problems  a b  a We begin by briefly introducing Constraint Programming.
Consider a sequence X = x1 , .
.
.
, xm of variables with respective domains D1 , .
.
.
, Dm .
By a constraint C on X, written C(X), we mean a subset of D1 x * * * x Dm .
A constraint satisfaction problem (CSP) consists of a finite sequence of variables X with respective domains and a finite set C of constraints, each on a subsequence of X.
A solution to a CSP is an assignment to its variables respecting their domains and constraints.
We study here CSPs with finite domains.
They can be solved by a top-down search interleaved with constraint propagation.
The top-down search is determined by a branching strategy that controls the splitting of a given CSP into two or more CSPs, the 'union' of which is equivalent to (i. e., has the same solutions as) the initial CSP.
In turn, constraint propagation transforms a given CSP into one that is equivalent but simpler.
We use here heuristicscontrolled domain partitioning as the branching strategy and hyper-arc consistency of [19] as the constraint propagation.
Hyper-arc consistency is enforced by removing from each variable domain the elements not used in a constraint.
inside  a  a a  b  b  b  disjoint  meet  overlap  b equal a b  a b  contains  covers Figure 1.
The eight RCC8 relations  Example.
Take the qualitative spatial reasoning with topology introduced in [10] and [20].
The set of qualitative relations is the set RCC8, i. e., Q = {disjoint, meet, equal, covers, coveredby, contains, inside, overlap}; see Fig.
1, which also shows the neighbourhood relation between these relations.
 We fix now a sequence O of objects of interest.
By a qualitative array we mean a two-dimensional array Q on O x O such that  2.2 Intra-state Constraints * for each pair of objects A, B [?]
O, the expression Q[A, B] is a variable denoting the (basic) relation between A, B.
So its initial domain is a subset of Q.
To describe qualitative simulations formally, we define first intra-state and inter-state constraints.
A qualitative simulation corresponds then to a CSP consisting of stages that all satisfy the intra-state constraints.
Moreover, this CSP satisfies the inter-state constraints that link the variables appearing in various stages.
For presentational reasons, we restrict ourselves here to binary qualitative relations (e. g., topology, relative size).
This is no fundamental limitation; our approach extends directly to higher-arity relations (e. g., ternary orientation).
* the consistency conditions hold on Q, so for each triple of objects A, B, C the following intra-state constraints are satisfied:  We assume that we have at our disposal  reflexivity:  Q[A, A] = equal,  converse:  conv(Q[A, B], Q[B, A]),  composition:  comp(Q[A, B], Q[B, C], Q[A, C]).
Each qualitative array determines a unique CSP.
Its variables are Q[A, B], with A and B ranging over the sequence of the assumed objects O.
The domains of these variables are appropriate subsets of Q.
An instantiation of the variables to elements of Q corresponds to a consistent Q-scenario.
In what follows we represent each stage t of a simulation by a CSP Pt uniquely determined by a qualitative array Qt .
Here t is a variable ranging over the set of natural numbers that represents discrete time.
Instead of Qt [A, B] we also write Q[A, B, t], reflecting that, in fact, we deal with a single ternary array.
* a finite set of qualitative relations Q, with a special element denoting the relation of an object to itself; * consistency conditions on Q-scenarios; we assume the usual case that they can be expressed as relations over Q, specifically as a binary converse relation conv and a ternary composition relation comp, * a conceptual neighbourhood relation between the elements of Q that describes which atomic changes in the qualitative relations are admissible.
2  2.3 Inter-state Constraints  7  To describe the inter-state constraints, we use as atomic formulas statements of the form  b  6  Q[A, B] ?
q  (eventually), (from now on), U  c  (next time), (until),  3  2  1  12 13  Figure 2.
Navigation path  Past formulas.
Here the evaluation starts at the upper bound and moves backward.
|=[s..t] Q[A, B] ?
c |=[s..t]  -1  ph  |=[s..t] -1 ph |=[s..t] -1 ph |=[s..t] kh S ph  if if if if if and  Q[A, B, t] ?
c where ?
[?]
{=, 6=}; |=[s..r] ph and r = t - 1 and s 6 r; |=[s..r] ph for all r [?]
[s..t]; |=[s..r] ph for some r [?]
[s..t]; |=[s..r] ph for some r [?]
[s..t] |=[u..t] kh for all u [?]
[r + 1 .. t].
Furthermore, we write Q[A, B] [?]
{q1 , .
.
.
, qk } as an abbreviation of (Q[A, B] = q1 ) [?]
.
.
.
[?]
(Q[A, B] = qk ).
The meaning of Q[A, B] [?]
/ {q1 , .
.
.
, qk } is analogous.
The bounded quantification [?
]A [?]
{o1 , .
.
.
, ok }.
ph(A) represents the disjunction ph(o1 ) [?]
.
.
.
[?]
ph(ok ).
Universal quantification [?
]A [?]
{o1 , .
.
.
, ok }.
ph(A) is interpreted analogously.
As usual, A in ph(A) denotes a placeholder (free variable), and ph(oi ) is obtained by replacing A in all its occurrences by oi .
Propositional connectives.
These are defined as expected, in particular independently of the 'past' or 'future' aspect of the formula.
For example, if not if  11  4  and their 'past' counterparts, -1 , -1 , -1 , and S (since).
While it is known that past time operators can be eliminated, their use results in more succinct (and in our case more intuitive) specifications; see, e. g., [18].
Inter-state constraints are formulas that have the form ph - ps.
Both ph and ps are built out of atomic formulas using propositional connectives, but ph contains only past time temporal operators and ps uses only future time operators.
Intuitively, at each time instance t,Seach inter-state cont straint ph - ps links the 'past' CSP i=0 Pi with the 'fuStmax ture' CSP i=t+1 Pi .
So we interpret ph in the interval [0..t], and ps in the interval [t + 1 .. tmax ].
We now explain the meaning of a past or future temporal formula ph with respect to the underlying qualitative array Q in an interval [s..t], for which we stipulate s 6 t. We write |=[s..t] ph to express that ph holds in the interval [s..t].
|=[s..t] !ph |=[s..t] ph1 [?]
ph2  10  a  5 where ?
[?]
{=, 6=} and q [?]
Q, or 'true', and employ a temporal logic with four temporal operators,  9  8  |=[s..t] ph, |=[s..t] ph1 or |=[s..t] ph2 .
Conjunction ph1 [?]
ph2 and implication ph1 - ph2 are defined analogously.
2.4 An Example: Navigation Future formulas.
Intuitively, the evaluation starts at the lower bound of the time interval and moves only forward in time.
|=[s..t] Q[A, B] ?
c  if  |=[s..t]  if  ph  |=[s..t] ph |=[s..t] ph |=[s..t] kh U ph  if if if and  A ship navigates around three buoys along a specified course.
The position of the buoys is fixed; see Fig.
2.
We reason qualitatively about the cardinal directions  Q[A, B, s] ?
c where ?
[?]
{=, 6=};  Q = {N, NE, .
.
.
, W, NW, EQ}  |=[r..t] ph and r = s + 1 and r 6 t; |=[r..t] ph for all r [?]
[s..t]; |=[r..t] ph for some r [?]
[s..t]; |=[r..t] ph for some r [?]
[s..t] |=[u..t] kh for all u [?]
[s .. r - 1].
with the obvious meaning (EQ is the identity relation).
Ligozat [16] provides the composition table for this form of qualitative reasoning and shows that it captures consistency.
The buoy positions are given by the following global 3  3.1 Unfolding Translation  intra-state constraints: Q[buoy a , buoy c ] = NW,  We translate the propositional connectives into appropriate Boolean constraints.
The temporal operators are unfolded over the simulation stages.
For example, the 'future' formula (Q[A, B] = q) in the interval [1..3] translates to  Q[buoy a , buoy b ] = SW, Q[buoy b , buoy c ] = NW.
All objects occupy different positions: [?
]A, B [?]
O.
A 6= B - Q[A, B] 6= EQ.
(Q[A, B, 1] = q) [?]
b1 , (Q[A, B, 2] = q) [?]
b2 , (Q[A, B, 3] = q) [?]
b3 , b1 [?]
b2 [?]
b3 = 1,  The initial position of the ship is south of buoy c, so we have Q[ship , buoy c ] = S. The ship is required to follow a path around the buoys.
In Fig.
2, the positions required to be visited are marked with bold circles.
We stipulate  and  with fresh Boolean variables b1 , b2 , b3 .
(Q[ship , buoy a ] = W [?]
(Q[ship , buoy b ] = N [?]
Translation for 'future' formulas.
(Q[ship , buoy c ] = E [?]
(Q[ship , buoy c ] = S )))) to hold in the interval [0 .. tmax ].
A tour of 13 steps exists (and is found by our program); it is indicated in Fig.
2.  cons + ([s..t], true) [?]
b cons + ([s..t], !ph) [?]
b  is is  cons + ([s..t], ph1 [?]
ph2 ) [?]
b  is  b = 1; b' = !b, cons + ([s..t], ph) [?]
b' ; (b1 [?]
b2 ) [?]
b, cons + ([s..t], ph1 ) [?]
b1 , cons + ([s..t], ph2 ) [?]
b2 ;  cons + ([s..t], Q[A, B] ?
c) [?]
b is (Q[A, B, s] ?
c) [?]
b where ?
[?]
{=, 6=};  3 Temporal Formulas as Constraints  cons + ([s..t], ph) [?]
b  (b1 [?]
b2 ) [?]
b, cons + ([r..t], ph) [?]
b2 , (s + 1 6 t) [?]
b1 , (s + 1 = r) [?]
b1 ; V cons + ([s..t], ph) [?]
b is ( r[?
]s..t br ) [?]
b, + cons ([r..t], ph) [?]
br for all r [?]
[s..t]; W cons + ([s..t], ph) [?]
b is ( r[?
]s..t br ) [?]
b, cons + ([r..t], ph) [?]
br for all r [?]
[s..t];  We explain now how a temporal formula (an inter-state constraint) is imposed on the sequence of CSPs representing the spatial arrays at consecutive times.
Such a formula is reduced to a sequence of constraints by eliminating the temporal operators.
We provide two alternative translations.
The first simply unfolds the temporal operators into primitive constraints, while the second retains more structure and avoids duplication of subformulas by relying on array constraints.
Consider a temporal formula ph - ps where ph uses only 'past' time operators S and ps uses only 'future' time operators.
Given a CSP ti=s Pi , we show how the past temporal logic formula ph translates to a constraint cons - ([s..t], ph) and how the future temporal logic formula ps translates to a constraint cons + ([s..t], ps), both on the variables of S t i=s Pi .
We assume that the target constraint language has Boolean constraints and reified versions of simple comparison and arithmetic constraints.
Reifying a constraint means associating a Boolean variable with it that reflects the truth of the constraint.
For example, (x = y) [?]
b is a reified equality constraint: b is a Boolean variable reflecting the truth of the constraint x = y.
We denote by cons([s..t], ph) [?]
b the sequence of constraints representing the fact that the formula ph has the truth value b in the interval [s..t].
The 'past' or 'future' aspect of a formula is indicated by a marker - or + , resp., when relevant.
The translation of ph proceeds by induction and is initiated with cons([s..t], ph) [?]
1 (where s 6 t).
is  cons + ([s..t], kh U ph) [?]
b is cons + ([s..t], ph [?]
kh [?]
(kh U ph)) [?]
b.
Translation for 'past' formulas.
This case is symmetric to the 'future' case except for the 'backward' perspective.
So we have cons - ([s..t], Q[A, B] ?
c) [?]
b is (Q[A, B, t] ?
c) [?]
b where ?
[?]
{=, 6=}, for example.
The remaining cases are defined analogously.
Observe that the interval bounds s, t in cons([s..t], ph) are treated as constants such that s 6 t.  3.2 Array Translation This alternative translation avoids the potentially large disjunctive constraints caused by unfolding the and U operators.
The idea is to push disjunctive information inside variable domains, with the help of array constraints.
4  Reconsider the formula (Q[A, B] = q) in the interval [1..3].
It is translated into a single array constraint, with the help of a fresh variable x ranging over time points:  Simulate spatial array Q, state constraints, tmax 7-- solution PS := hi; t := 0 while t < tmax do Pt := create CSP from Qt and impose intra-state constraints PS := append Pt to PS and impose inter-state constraints  Q[A, B, x] = q, 1 6 x, x 6 3.
Array constraints generalise the better-known element constraint.
Constraint propagation for array constraints is studied in [5] and used in our implementation.
When negation occurs in the formula, a complication arises with this translation approach, however.
Just negating the associated truth value, as in the unfolding translation, is now incorrect.
We therefore first transform a formula into negation normal form (NNF).
The array translation of NNF formulas follows.
We give it only for 'future' formulas and where different from the unfolding translation.
The case of negation does not apply anymore.
cons + ([s..t], ph) [?]
b is cons + ([s..t], ph [?]
( true - cons + ([s..t], ph) [?]
b  is  cons + ([s..t], kh U ph) [?]
b is  hPS, failurei := prop(PS) if not failure then PS ' := PS with final state constraints imposed on Pt hsolution, successi := solve(PS ' ) if success then return solution t := t + 1 return failure Figure 3.
The simulation algorithm  ph) [?]
b;  The array translation results in just two array constraints, namely Q[ship , buoy , r1 ] = E and Q[ship , buoy , r2 ] = S, The four ordering constraints 1 6 r1 , r1 6 n r1 6 r2 , and r2 6 n control the fresh variables r1 , r2 .
  s 6 r, r 6 t, cons + ([r..t], ph) [?]
b; (b1 [?]
(b2 [?]
b3 )) [?]
b, s 6 r, r 6 t, cons + ([r..t], ph) [?]
b1 , (s = r) [?]
b2 , s 6 u, u 6 r, (u = r - 1) [?]
b3 , cons + ([s..u], kh) [?]
b3 .
4 Simulations By a qualitative simulation we mean a finite or infinite sequence PS = hP0 , P1 , .
.
.i of CSPs such that for each chosen inter-state constraint ph - ps we have that the constraint  The interval end points s, t in cons([s..t], ph) can now be variables with domains, in contrast to the case of the unfolding translation where s, t are constants.
We are careful to maintain the invariant s 6 t and state appropriate constraints to this end.
Therefore, for example, we unfold ph into a conjunction only step-wise, as the formula ph).
ph [?]
( true -  cons([0 .. t0 ], ph) - cons([t0 + 1 .. t], ps) S is satisfied by the CSP ti=0 Pi , * if PS is finite with u elements, for all t0 [?]
[0 .. u - 1], t = tmax , * if PS is infinite, for all t0 > 0, t > t0 + 1.
Example.
Let us contrast the two alternative translations for a formula from the navigation domain.
Consider ph[?]
(ph1 [?]
Thus, at each stage of the qualitative simulation, we relate its past (and presence) to its future using the chosen interstate constraints.
Consider an initial situation I = P0 and a final situation Fx determined by a qualitative array of the form Qx , where x is a variable ranging over the set of integers (possible time instances).
We would like to determine whether a simulation exists that starts in I and reaches Ft , where t is the number of steps.
If one exists, we may also be interested in computing a shortest one, or in computing all of them.
ph2 ),  ph1 [?]
(Q[ship , buoy ] = E) and ph2 [?]
(Q[ship , buoy ] = S), in the interval [1..n] for a constant n, as a 'future' formula.
So we consider the sequence of constraints cons + ([1..n], ph) for each translation.
The unfolding translation generates many reified equality constraints of the form (Q[ship , buoy , k] = D) Pn[?]
bi,k , where D is E or S. More specifically, n + i=1 i = n(n + 3)/2 such constraints and as many new Boolean variables are created.
Many of the constraints are variants of each other differing only in their Boolean variable bi,k .
Simulation algorithm.
The algorithm given in Figure 3 provides a solution to the first two problems in presence of a non-circularity constraint.
5  The sequence PS of CSPs is initially empty and subsequently step-wise extended; so it remains finite.
We view PS as a single CSP, which consists of regular finite domain variables and constraints and which thus fits into the problem format solvable by a standard constraint programming techniques.
We employ the auxiliary procedures prop and solve.
The call to prop performs constraint propagation of the intrastate and inter-state constraints.
In our implementation, the hyper-arc consistency notion is used.
As a result, the variable domains are pruned and less backtracks arise when solve is called.
If the outcome is an inconsistent CSP, the value false is returned in the failure flag.
The call to solve checks if a solution to the CSP corresponding to the given sequence of CSPs exists.
If so, a solution and true is returned, otherwise h[?
], falsei.
In our implementation, solve is a standard backtrack search (based on variable domain splitting) combined with constraint propagation as in the prop procedure.
We use the constant tmax equal to the number of different qualitative arrays, i. e., tmax = |O| * (|O| - 1) * 2|Q|-1 .
If the desired simulation exists, the above algorithm finds a shortest one and outputs it in the variable solution.
T  L  C  P  B  S Figure 4.
A piano movers problem  believe it is worth doing so, and it is not difficult to modify our implementation (the solve procedure) accordingly.
5.3 Heuristics Our implementation also incorporates the specialised reasoning techniques for RCC8 [21] and the cardinal directions [16].
In these studies, maximal tractable subclasses of the respective calculi are identified, and corresponding polynomial decision procedures are discussed.
Our context requires that these techniques are treated as heuristics, due to the presence of side constraints (notably the inter-state constraints).
With a relation variable model for qualitative spatial reasoning, these heuristics fall into the customary class of variable and value ordering heuristics for guiding search in constraint programming.
In our implementation, the search heuristic splits the relation variable domains appropriately so that one of the new domains belongs to a maximal tractable subclass of the respective calculus.
5 Implementation We implemented the simulation algorithm of Fig.
3 and both alternative translations of temporal formulas to constraints in the ECLi PSe constraint programming system [22].
The total program size is roughly 1500 lines of code.
5.1 Propagation Support for enforcing hyper-arc consistency for Boolean and many reified constraints, as well as for extensionally defined constraints such as conv, comp and the conceptual neighbourhood constraint, is directly available in ECLi PSe (by its fd/ic and propia libraries).
For array constraints, we use the ECLi PSe implementation discussed in [5].
The availability of these (generic) implementations of propagation mechanisms explains why we chose hyper-arc consistency.
We emphasise, however, that in a relation variable model, constraint propagation is relevant only for efficiency.
6 Case Studies We now report on two case studies.
In both of them, the solutions were found by our implementation within a few seconds.
6.1 Piano Movers Problem Consider the following version of the piano movers problem.
There are three rooms, the living room (L), the study room (S) and the bedroom (B), and the corridor (C).
Inside the study room there is a piano (P) and inside the living room a table (T); see Figure 4.
Move the piano to the living room and the table to the study room assuming that none of the rooms and the corridor are large enough to contain the piano and the table at the same time.
Additionally, ensure that the piano and the table at no time will touch each other.
5.2 Search We use the basic backtracking algorithm provided by ECLi PSe , but we control it with the heuristics described in the following section.
Various other, advanced search strategies are available in ECLi PSe , for example Limited Discrepancy Search [13].
Although we did not experiment with these techniques, we 6  * if the piano or the table overlaps with one space s, then it also overlaps with some other space s' , such that s and s' touch each other:  To formalise this problem, we describe the initial situation by means of the following formulas: ph0 [?]
ph1 [?]
ph2 [?]
Q[B,L] = disjoint [?]
Q[B,S] = disjoint [?]
Q[L,S] = disjoint,  [?
]s [?]
S.
[?
]o [?]
{P, T}.
(Q[s, o] = overlap - [?
]s' [?]
S. (Q[s' , o] = overlap [?]
Q[s, s' ] = meet)),  Q[C,B] = meet [?]
Q[C,L] = meet [?]
Q[C,S] = meet,  * if the piano overlaps with one space, then it does not touch any space, and equally the table:  Q[P,S] = inside [?]
Q[T,L] = inside.
[?
]s [?]
S.
[?
]o [?]
{P, T}.
(Q[s, o] = overlap - [?
]s' [?]
S. Q[s' , o] 6= meet),  We assume that initially ph0 , ph1 , ph2 hold, i. e., the constraints cons - ([0..0], ph0 ), cons - ([0..0], ph1 ) and cons - ([0..0], ph2 ) are present in the initial situation I.
Below, given a formula ph, by an invariant built out of ph we mean the formula ph - ph.
Further, we call a room or a corridor a 'space' and abbreviate the subset of objects {B, C, L, S} by S. We now stipulate as the inter-state constraints the invariants built out of the following formulas:  * both the piano and the table can touch at most one space at a time: [?
]s, s' [?]
S.
[?
]o [?]
{P, T}.
(Q[s, o] = meet [?]
Q[s' , o] = meet - Q[s, s' ] = equal).
After these additions, our program generated the shortest solution in the form of a simulation of length 12.
In this solution the bedroom is used as a temporary storage for the table.
Interestingly, the table is not moved completely into the bedroom: at a certain moment it only overlaps with the bedroom.
* the relations between the rooms, and between the rooms and the corridor, do not change: ph0 [?]
ph1 , * at no time do the piano and the table fill completely any space: [?
]s [?]
S. (Q[P, s] 6= equal [?]
Q[T, s] 6= equal) ,  6.2 Phagocytosis  * together, the piano and the table do not fit into any space.
More precisely, at each time, at most one of these two objects can be within any space:  The second example deals with a simulation of phagocytosis: an amoeba absorbing a food particle.
This problem is discussed in [9].
We quote:  [?
]s [?]
S.
!
(Q[P, s] [?]
{inside, coveredby} [?]
Q[T, s] [?]
{inside, coveredby}),  "Each amoeba is credited with vacuoles (being fluid spaces) containing either enzymes or food which the animal has digested.
The enzymes are used by the amoeba to break down the food into nutrient and waste.
This is done by routing the enzymes to the food vacuole.
Upon contact the enzyme and food vacuoles fuse together and the enzymes merge into the fluid containing the food.
After breaking down the food into nutrient and waste, the nutrient is absorbed into the amoeba's protoplasm, leaving the waste material in the vacuole ready to be expelled.
The waste vacuole passes to the exterior of the protozoan's (i. e., amoeba's) body, which opens up, letting the waste material pass out of the amoeba and into its environment."
* at no time instance do the piano and the table touch each other: Q[P, T] = disjoint.
The final situation is captured by the constraints Q[P, L] = inside  and Q[T, S] = inside.
Remarkably, the interaction with our program revealed in the first place that our initial formalisation was incomplete.
For example, the program also generated solutions in which the piano is moved not through the corridor but 'through the walls', as it were.
To avoid such solutions we added the following intrastate constraints.
* each space is too small to be 'touched' (met) or 'overlapped' by the piano and the table at the same time:  To fit it into our present framework, we slightly simplified the problem representation by not allowing for objects to be added or removed dynamically.
[?
]s [?]
S.
!
(Q[s, P] [?]
{overlap, meet} [?]
Q[s, T] [?]
{overlap, meet}), 7  7 Final Remarks  In this problem, we have six objects, amoeba , nucleus , enzyme , vacuole , nutrient and waste .
The initial situation is described by means of the three following constraints:  The most common approach to qualitative simulation is the one discussed in [14, chapter 5].
For a recent overview see [15].
It is based on a qualitative differential equation model (QDE) in which one abstracts from the usual differential equations by reasoning about a finite set of symbolic values (called landmark values).
The resulting algorithm, called QSIM, constructs the tree of possible evolutions by repeatedly constructing the successor states.
During this process, CSPs are generated and solved.
This approach is best suited to simulate evolution of physical systems.
A standard example is a simulation of the behaviour of a bath tub with an open drain and constant input flow.
The resulting constraints are usually equations between the relevant variables and lend themselves naturally to a formalisation using CLP(FD), see [7, chapter 20] and [3].
The limited expressiveness of this approach was overcome in [4], where branching time temporal logic was used to describe the relevant constraints on the possible evolutions (called 'trajectories' there).
This leads to a modified version of the QSIM algorithm in which model checking is repeatedly used.
Our approach is inspired by the qualitative spatial simulation studied in [9], the main features of which are captured by the composition table and the neighbourhood relation discussed in Example 2.2.
The distinction between the intra-state and inter-state constraints is introduced there, however the latter only link the consecutive states in the simulation.
The simulation algorithm of [9] generates a complete tree of all 'evolutions', usually called an envisionment.
In contrast to [9], our approach is constraint-based.
This allows us to repeatedly use constraint propagation to prune the search space in the simulation algorithm.
Further, by using more complex inter-state constraints, defined by means of temporal logic, we can express substantially more sophisticated forms of behaviour.
While the prevalent approach to constraint-based modelling of qualitative spatial knowledge maps qualitative relations to constraints, we use variables to express qualitative relations.
The relation variable approach is much more declarative, separating the model from the solver.
The advantage of a relation variable model for qualitative simulations is that the knowledge of the spatial domain as well as of the application domain can be expressed on the same conceptual level, by intra-state and inter-state constraints.
This leads to a model that can easily be realised within a typical constraint programming system using generic propagation and search techniques, and is also immediately open to advances in these systems.
Simulation in our approach subsumes a form of planning.
In this context, we mention the related work [17] in  Q[amoeba , nutrient ] = disjoint, Q[amoeba , waste ] = disjoint, Q[nutrient , waste ] = equal.
We have the intra-state constraints Q[enzyme , amoeba ] = inside, Q[vacuole , amoeba ] [?]
{inside, coveredby}, Q[vacuole , enzyme ] [?]
{disjoint, meet, overlap, covers}, and, concerning the nucleus, Q[nucleus , vacuole ] [?]
{disjoint, meet}, Q[nucleus , enzyme ] [?]
{disjoint, meet}, Q[nucleus , amoeba ] = inside.
The inter-state constraints are Q[nutrient , amoeba ] = meet - Q[nutrient, amoeba ] = overlap, Q[nutr., amoeba ] [?]
{inside, coveredby, overlap - Q[nutr., amoeba ] [?]
{inside, coveredby}.
We model the splitting up of the food into nutrient and waste material by Q[nutrient , waste ] = equal - .
(ph1 - .
ph2 [?].
ph3 ) [?].
Q[nutrient , waste ] 6= equal; with ph1  [?]
Q[nutrient , vacuole ] = inside [?]
Q[enzyme , nutrient ] = overlap [?]
Q[enzyme , waste ] = overlap  ph2  [?]
Q[nutrient , waste ] = overlap  ph3  [?]
Q[nutrient , waste ] = equal  The dotted operators express if-then-else, that is, a- .
b [?].
c  [?]
(a - b) [?]
(!a - c).
The final situation is described by means of the constraints Q[amoeba , waste ] = disjoint, Q[amoeba , nutrient ] [?]
{contains, covers}.
Our program generated a simulation consisting of 9 steps.
8  the area of planning which shows the benefits of encoding planning problems as CSPs and the potential with respect to solving efficiency.
Also related is the TL PLAN system where planning domain knowledge is described in temporal logic [2].
The planning system is based on incremental forward-search, so temporal formulas are just unfolded one step at a time, in contrast to the translation into constraints in our constraint-based system.
Finally, [12] discusses how a qualitative version of the piano movers problem can be solved using an approach to qualitative reasoning based on topological inference and graph-theoretic algorithms.
Our approach is substantially simpler in that it does not rely on any results on topology apart of a justification of the composition table.
of 10th National Conference on Artificial Intelligence (AAAI'92), pages 679-684.
AAAI Press, 1992.
[10] M. J. Egenhofer.
Reasoning about binary topological relations.
In O. Gunther and H.-J.
Schek, editors, Proc.
of 2nd International Symposium on Large Spatial Databases (SSD'91), volume 525 of LNCS, pages 143-160.
Springer, 1991.
[11] M. T. Escrig and F. Toledo.
Qualitative Spatial Reasoning: Theory and Practice.
Application to Robot Navigation, volume 47 of Frontiers in Artificial Intelligence and Applications.
IOS Press, 1998.
[12] B. Faltings.
Using topology for spatial reasoning.
In Proc.
of 8th International Symposium on Artificial Intelligence and Mathematics (AI&M'00), 2000.
References [1] J. F. Allen.
Maintaining knowledge about temporal intervals.
Communications of the ACM, 26(11):832- 843, 1983.
[13] W. D. Harvey and M. L. Ginsberg.
Limited discrepancy search.
In Proc.
of 14th International Joint Conference on Artificial Intelligence (IJCAI'95), volume 1, pages 607-615.
Morgan Kaufmann, 1995.
[2] F. Bacchus and F. Kabanza.
Using temporal logics to express search control knowledge for planning.
Artificial Intelligence, 116, 2000.
[14] B. Kuipers.
Qualitative reasoning: modeling and simulation with incomplete knowledge.
MIT Press, 1994.
[3] A. Bandelj, I. Bratko, and D. Suc.
Qualitative simulation with CLP.
In Proc.
of 16th International Workshop on Qualitative Reasoning (QR'02), 2002.
[15] B. Kuipers.
Encyclopedia of Physical Science and Technology, chapter Qualitative simulation, pages 287-300.
Academic Press, third edition, 2001.
[4] G. Brajnik and D. Clancy.
Focusing qualitative simulation using temporal logic: theoretical foundations.
Annals of Mathematics and Artificial Intelligence, 22:59-86, 1998.
[16] G. Ligozat.
Reasoning about cardinal directions.
Journal of Visual Languages and Computing, 9(1):23-44, 1998.
[5] S. Brand.
Constraint propagation in presence of arrays.
In K. R. Apt, R. Bartak, E. Monfroy, and F. Rossi, editors, Proc.
of 6th Workshop of the ERCIM Working Group on Constraints, 2001.
[17] A. Lopez and F. Bacchus.
Generalizing GraphPlan by formulating planning as a CSP.
In Proc.
of International Joint Conference on Artificial Intelligence (IJCAI'03), 2003.
[6] S. Brand.
Relation variables in qualitative spatial reasoning.
In S. Biundo, T. Fruhwirth, and G. Palm, editors, Proc.
of 27th German Annual Conference on Artificial Intelligence (KI'04), volume 3238 of LNAI, pages 337-350.
Springer, 2004.
[18] N. Markey, F. Laroussinie, and Ph.
Schnoebelen.
Temporal logic with forgettable past.
In Proc.
of 17th IEEE Symposium on Logic in Computer Science (LICS'02), pages 383-392, 2002.
[7] I. Bratko.
PROLOG Programming for Artificial Intelligence.
International Computer Science Series.
Addison-Wesley, third edition, 2001.
[19] R. Mohr and G. Masini.
Good old discrete relaxation.
In Y. Kodratoff, editor, Proc.
of European Conference on Artificial Intelligence (ECAI'88), pages 651-656.
Pitman publishers, 1988.
[8] A. G. Cohn and S. M. Hazarika.
Qualitative spatial representation and reasoning: An overview.
Fundamenta Informaticae, 46(1-2):1-29, 2001.
[20] D. A. Randell, Z. Cui, and A. G. Cohn.
A spatial logic based on regions and connection.
In B. Nebel, C. Rich, and W. R. Swartout, editors, Proc.
of 2nd International Conference on Principles of Knowledge Representation and Reasoning (KR'92), pages 165-176.
Morgan Kaufmann, 1992.
[9] Z. Cui, A. G. Cohn, and D. A. Randell.
Qualitative simulation based on a logical formalism of space and time.
In P. Rosenbloom and P. Szolovits, editors, Proc.
9  [21] J. Renz and B. Nebel.
Efficient methods for qualitative spatial reasoning.
Journal of Artificial Intelligence Research, 15:289-318, 2001.
[22] M. G. Wallace, S. Novello, and J. Schimpf.
ECLiPSe: A platform for constraint logic programming.
ICL Systems Journal, 12(1):159-200, 1997.
10
Using Constrained Resolution for Abductive Temporal Reasoning Nicolas Chleq  INRIA Sophia-Antipolis BP 93 { 06902 Sophia Antipolis Cedex { France chleq@sophia.inria.fr  Abstract  We describe in this article an abductive procedure based on a constrained resolution principle.
The choice of constrained resolution is motivated by the whish to gain full advantage of using reified temporal logics.
For this purpose, it is interesting to deal eciently with temporal ordering and equality relation between instants.
The constrained resolution principle described here is a solution to this point.
It is an instance of the more general constrained resolution principle of H.J.
Burckert.
It also relies on the work done in the area of temporal constraint propagation.
For the purpose of temporal reasoning it is also necessary to cope with temporal persistency of known and deduced facts.
This point is solved by handling persistency in an abductive fashion.
1 Introduction  This article describes a resolution-based abductive procedure.
This procedure is based on works done in the area of abductive logic programming and uses a constrained resolution principle.
Such a resolution principle is necessary in order to be able to deal with any reified temporal logic based on instants.
For the purpose of this paper, we use it on a simple temporal logic based on a simple and nave ontology.
It is also suciently expressive for practical use, but this gain on expressiveness is due to an increased complexity of the language itself (more axioms), hence a more complex reasoning task.
The use of abduction in temporal reasoning has been motivated by 12].
This reasoning method is complementary to prediction as it allows to deal with persistency and produce explanation, while retaining a very intuitive form for causal rules eect if causes.
Abduction has also been applied to planning with temporal formalisms such as the Event Calculus 5, 10].
Most abductive procedure are based on resolution, and practical use of abduction relies on the feasibility of resolution based reasoning for temporal reasoning.
Though it is not necessary to argue again on the usefulness of a resolution principle, practical use of this method is not straightforward.
In particular, some features of the formul on which this method is applied can suer from great eciency problems.
Equality relation and self-resolving clauses are some examples of these problematic features.
Most of these problems are to be solved by adapted strategies and specialized inference rules.
A lot of work has been done on the combination of the resolution principle with some particular \algorithmic" theories: for example the Theory Resolution of Stickel 14], and the Constrained Resolution of Burckert 2].
In the next section, we begin by an informal presentation of a temporal logic.
This logic is used throughout this paper, and its features are representative of most reified temporal logics.
It also illustrates the need of a specialized resolution principle, which I describe as an instance of Burckert's one.
The rest of the paper is devoted to abductive reasoning, and focuses on the abductive procedure we developed as an extension of the abductive logic programming procedure described in 7].
2 Reified Temporal Logics  Reified temporal logics are sorted predicate calculi: one of the sorts is used for time points or time intervals.
A formal description of these logics is given in 13].
They are usually based on two primitive entities: instants as in McDermott's logic 9], or intervals as in Allen's one 1].
The basic construct h  ti of these logics associates a formula with a temporal entity t, this last one being either an instant or an interval depending on the kind of logic.
The intuitive meaning of this expression is that the formula is true at the instant denoted by the term t, or true throughout the interval denoted by t. These languages can express the truth of such-and-such proposition over time, they are hence good candidates for the expression of temporal knowledge.
2.1 A temporal logic  The logic we use is a two-sorted predicate calculus, where time is the sort of all expressions denoting time instants, and proposition is the sort of all terms associated with temporal entities.
It is based on in-  f]t1 t2] pg !
t1 < t2 f]t1 t2] pg !
begin(t1  p) f]t1 t2] pg !
end(t2  p) f]t1 t2] pg !
persist(t1  t2 p) persist(t1  t4 p) ^ (t1 fi t2 < t3 fi t4) !
persist(t2  t3 p) persist(t1  t3 p) ^ (t1 < t2 fi t3) !
true(t2  p) persist(t1  t2 p) ^ begin(t3  p) !
(t3 fi t1 ) _ (t2 < t3 ) persist(t1  t2 p) ^ end(t3 p) !
(t3 < t1) _ (t2 fi t3) begin(t1  p) !
(8t2 > t1 persist(t1  t2 p) _ (9t3 (t1 < t3 < t2 ) ^ end(t3  p))) begin(t1  p) ^ persist(t1  t2 p) ^ end(t2 p) !
f]t1 t2] pg  ( A1 ) (A2 ) (A3 ) (A4 ) (A5 ) (A6 ) (A7 ) (A8 ) (Ap ) (A9 )  Figure 1: Some axioms of the temporal logic.
stants as the primitive temporal entity, and intervals are written with two instants as their lower and upper bounds.
The simplest expression associates a proposition with an instant or an interval: ft P g means that P holds at time t, and f]t t ] P g means that P holds throughout the interval ]t t ].
Instants are taken from a set that we want to be dense, so that we are able to speak of an instant between any two other instants: the set of rational numbers suits our need.
We divide the set of propositions into the ephemeral ones, used to describe \instantaneous" phenomena, and durable ones.
Propositions of the first class are always associated with instants by expressions of the form ft pg, while propositions of the second type are associated with intervals by expressions of the form f]t t ] pg.
The set of durable propositions can be refined similarly to the classification established by Shoham 13].
For our purpose, we are only interested in the liquid propositions in this taxonomy, or homogeneous in ETL 11]: we call these propositions stable and this means that their truth throughout an interval implies their truth at all instants within the interval.
The truth of p at one instant t, as a consequence of the truth of p over an interval comprising t, is expressed by true(t p).
The set of stable propositions is a subset of the set of durable ones.
An instant can be represented by five means: a number, a variable, a symbolic constant, an arithmetic expression such as t + n where t is an instant and n a number, and a functional term f(t1  : : : tn).
We use two relation symbols for the ordering of instants: < and fi.
Between these ordering relations and expressions of the form fI P g, we propose axiom A1 in figure 1.
Another useful feature for exibility of a temporal logic as a knowledge expression language, is the ability to give partial information about truth periods.
For this purpose, we introduce the following expressions: begin(t, p) means that one of the truth period of proposition p begins at the instant t. When the 0  0  0  beginning of the interval is known by this way, the term e(t p) refers to the end (the upper bound) of this interval.
The axiom A2 establishes the relationship with the expression fI pg where I is an interval.
end(t, p) means that one of the truth period of p ends at the instant t. In the same way as above, the term b(t p) refers to the lower bound of this interval.
This expression is formally defined by axiom A3 .
persist(t1, t2 , p) means that the interval ]t1 t2] is included in one of the truth period of p. This expression is defined by the two axioms A4 and A5 .
The maximality of truth period expressed by formulae like f]t t ] pg entails that the lower bound of these intervals are really the time when the proposition becomes true, and that the upper bounds are the instants when the proposition ceases to be true.
This entails that overlapping truth periods of the same proposition lead to a contradiction between the truth inside one of the intervals, and the non-truth outside the other one.
We suggest axioms A7 and A8 to express that overlapping of distinct truth periods is not allowed for a durable proposition.
Axiom A9 completes the definition of the logic and enables to deduce a complete truth period from partial information.
0  2.2 Example  We propose to illustrate the use of this logic by the well known \Yale Shooting Problem" which is written in figure 2.
We can deduce from this example that the gun will become unloaded at time 4 because of the firing.
We express it by end(4 loaded) and the proof of it involves reasoning about the temporal persistency of loaded.
The truth period of this proposition begins at time T1 because of the loading action on the gun.
Then, thanks to axiom Ap , it will last as long as needed, provided it is not interrupted.
At this moment, the upper bound of the persistence can not be fixed, but we can assume it to be at least equal to  fT1  loadingg true(2 alive) f4 pull;triggerg T1 < 2 R1 : 8t ft loadingg !
begin(t loaded) R2 : 8t ft unloadingg ^ true(t loaded) !
end(t loaded) R3 : 8t ft pull;triggerg ^ true(t loaded) !
end(t loaded) R4 : 8t ft pull;triggerg ^ true(t loaded) ^ true(t alive) !
end(t alive) Figure 2: The Yale Shooting Problem.
loading and pull-trigger are ephemeral propositions, and loaded and alive are stable.
4.
It can not be greater than 4 because of axiom A7 .
Thus, the only solution is that this upper bound is equal to 4.
3 The constrained resolution principle  In this section, we focus on the ability to do resolution-based reasoning with some temporal logic similar to the one described in the previous section.
The main problem of these languages comes from the use of equality and ordering relation symbols.
The usual axiomatization of the ordering relation fi, which describes the transitivity, reexivity and antisymmetry involves some self-resolving clauses.
This feature entails that for some queries the resolution process may spend a lot of time with repeated use of these clauses, without any way to know whether or not these inferences are relevant for the original query.
Our solution is an instance of the more general resolution principle proposed by H.J.
Burckert 2].
This constrained resolution principle is introduced in the framework of a particular logic, called logic with restricted quantifiers.
In this logic, quantifiers are associated with formul interpreted as restriction on the variables of the overall formula.
Clausal formul with restriction are noted C k R , which means 8X R !
C, where X is the vector of variables in C. T denotes the theory of restriction formul: it is given in such a way that (un)satisfiability and validity of these formul can be decided by an algorithmic mean.
Burckert simply assumes given a class of models for the restriction theory T .
The RQ-resolution principle is given by: fP (x1 	 	 	  xn)g   C k R f:P (y1 	 	 	  yn)g   D k S R ^ S ^ ; is T -satisfiable C  D k R^S ^; (1) where ; is the conjunction of equations x1 = y1 : : : xn = yn .
One of the completeness result from 2] says that given an unsatisfiable set of clauses, it is possible to derive by RQ-resolution an empty clause 2 k R such that T j= 9(R).
For the purpose of temporal reasoning, we consider that the restriction theory T is the theory where fi is interpreted as an ordering relation between time  instants and = means that two instants are at the same position on the time line.
To enable the use of constrained resolution, one first needs to have a constrained clausal form of the input formul.
For example, axiom A1 in Figure 1 produces the following constrained clause:  f]t1 t2] pg k :(t1 < t2 ) while axiom A6 is transformed in: true(t2  p)  f]t1 t3] pg k (t1 < t2 fi t3 ) The constrained forms of the axioms have a restriction which is a conjunction of temporal constraints.
However, some of these constraints are negative literals.
To simplify the use of constrained resolution, we choose to assume that fi is a total ordering relation.
This allows to use rewriting rules such as :(t < t ) !
t fi t to eliminate negative constraints.
We also split clauses with disjunctive restriction: C k R1 _ R2 !
f C k R1  C k R2 g 0  0  3.1 Deciding satisability  Provided that the restrictions of clauses are conjunction of positive litterals, it is possible to use temporal constraint propagation techniques to decide satisfiability.
Thus, the satisfiability of a conjunction of temporal expressions is equivalent to the global consistency of the constraints network built from these expressions.
For our problem, we are interested in both the symbolic and numeric relationships between instants.
We choose to rely on the formalism of Simple Temporal Problem (STP) studied by Dechter 3].
In this formalism, a constraint between two instants is represented by an edge between two nodes representing the instants, the label of the edge being a numeric interval.
Such a constraint is written x : a b] : y where x and y are two instants, a and b are two numbers belonginq to R   f;1 +1g.
This constraint means that a fi y ; x fi b.
A set of these constraints gives a network of binary constraints, such that an O(n3) path consistency algorithm is a complete decision procedure for the global consistency of the constraint set.
All expressions comparing instants denoted with numbers, variables, constants and arithmetic terms can be expressed within this constraint formalism.
Some simplification rules for unification 8], especially the ones for decomposition of functional terms,  are used inside the constraint solver when an equality is encountered.
The purpose is to: (1) handle non-arithmetic functional terms involved in equations by simplifying these equations (2) identify equations that involve variables so that they are used to instantiate the resolvent clause.
This keeps the set of constraints as small as possible.
Thus, given the set of constraints R ^ S ^ ; of rule (1), the satisfiability test produces a pair h C i where  is a substitution and C is a set of constraints such that (R ^ S ^ ;)  C. Then, the resolution principle is formulated as a variant of Burckert's one.
This gives: fP (x1 	 	 	  xn)g   C k R f:P (y1 	 	 	  yn)g   D k S R ^ S ^ ; is satisfiable (C   D) k ; (2) where ; is the set fx1 = y1  : : : xn = yn g, and h ; i is the pair resulting from the satisfiability test of R ^ S ^ ;.
0  0  4 Abductive temporal reasoning  This section describes an extension of the abductive logic programming procedure described by Kakas et Mancarella in 7].
This extension handles constrained resolution and, contrary to the original one, can handle non ground abducible litterals.
The original abductive procedure is an extension of SLD-resolution, and is inspired from the first one described by Eshghi and Kowalski in 4] to handle negation as failure in a abductive fashion.
The definition assumes a logic program P (a set of clauses of the form C  L1 : : :Ln , where C is the head and L1 : : :Ln the body), a set H of predicate symbols called abducible predicate, and a set IC of integrity constraints (clauses with empty head).
The purpose of the original procedure is to find, for a query Q, a set " of hypotheses (ground instances of abducible predicates) such that there exists a stable model M 6] of P   " such that M j= Q and M j= IC.
4.1 Denition of the procedure  The particular features of the procedure are the following:  we use the ability for a refutation using constrained resolution to produce \conditional answers", as it is done in Constraint Logic Programming.
For this purpose, we consider that, at the end of an abductive refutation, the ground temporal constraints in the restriction of the derived empty clause represent, if they are not satisfied, some additional ordering hypotheses which can be assumed if they are consistent  in the same way, it is possible to force a failure in a derivation by assuming some additional constraints.
When an empty clause is derived,  the constraint part of this empty clause can be made unsatisfiable.
The simplest possibility is to add a new temporal constraint to the current set of hypothesis such that the constraints set of the empty clause becomes inconsistent The procedure builds interleaved sequences of states.
The first sequence form is called an abductive refutation where each state has the form hGi  "ti "i $i  Iii.
At the beginning, G0 is the original query.
Gi is a goal clause, "i is a set of constraints, and "i is the current set of hypotheses.
The set Ii is initialized with the integrity constraints in IC and is used to collect new integrity constraints from the failure in the consistency check part of the procedure.
Denition 1 Let G be a goal clause of the form B .
An abductive refutation of G is a finite sequence of tuple:      G1  "t1 "1 $1 I1 : : : Gn "tn "n $n In where Gi is a goal clause, "ti is a set of ground constraints, "i is a set of ground literals, $i is a substitution, Ii is a set of integrity constraints, G1 = G $1 =  I1 = IC Gn = 2 k R such that either T  "tn j= R or "tn ^ R is T satisfiable and for each i = 1 : : : n, Gi has the form L L k R where L is the selected literal, and the  next state Gi+1  "ti+1 "i+1 $i+1  Ii+1 is obtained 0  according to one of the following rules: (A1 ) L is positive, C is the resolvent of Gi and of a variant of some clause in P on the selected literal L with the pair h ;i, then:  Gi+1 = C "ti+1 = "ti $i+1 =   "i+1 = "i Ii+1 = Ii  (A2 ) L is either positive and abducible or negative, L unifies with an element of "i with the pair h ;i, (R ^ ;) is T -satisfiable, then:  Gi+1 = L k (R ^ ;) "ti+1 = "ti "i+1 = "i $i+1 =  Ii+1 = Ii 0  (A3 ) L is either positive and abducible or negative, neither L nor its negation unifies with an element of "i, and there exists a consistency derivation from hF0 "ti "i   fLg Ii i to hfg "t "  I i then: 0  0  0  Gi+1 = L k R "ti+1 = "t "i+1 = " $i+1 =  Ii+1 = I where  is a substitution which maps each variable of L to a new skolem constant, and F0 is the set of all resolvents of the clause L  with clauses of Ii .
0  0  0  0  A consistency derivation implements the test of consistency of an hypothesis.
It is very similar in essence to a negation as failure call in logic programming.
The aim is to check whether an assumption is consistent with the program P and the current set of hypotheses " and of temporal constraints "t. A consistency derivation is a sequence of states of the form hFi  Dit Di  Ii i, where Fi is a set of goal clauses, Dit is the set of temporal constraints, and Di the set of current hypotheses.
The set Ii collects the failed goals during the test, so that they will be used with further hypotheses.
Denition 2 A consistency derivation is a finite sequence of tuple      F1 D1t  D1  I1 : : : Fm  Dmt  Dm  Im such that for each i 2 1 m], Fi is a set of goal clauses and has the form f L L k R g  Fi , Fm is the empty set, Dit is a set of ground temporal constraints, Di is a set of ground literals, and Ii is a set of integrity constraints, and L is selected  in the body of L L k R .
Fi+1  Dit+1 Di+1  Ii+1 is obtained according to one 0  0  0  of the following rules: (C1) there exists in Fi an empty clause C = 2 k R , then: 0  Fi+1 = Fi ; fC g Di+1 = Di Dit+1 = Dit   fD g Ii+1 = Ii where D is a ground constraint such that R ^ D is inconsistent, and Dit+1 is T 0  0  0  0  satisfiable.
(C2) L is positive, C is the set of all resolvents of clauses in P with the clause L L k R on the literal L, then: 0  Fi+1 = C   Fi Dit+1 =Dit Di+1 = Di I  i Ii+1 = I   f L L k R g ifif CC 6= =  i 0  0  (C3) L is either positive and abducible or negative, C is the set of all resolvents of L L k R with elements of Di on the literal L, then: 0  Fi+1 = C   Fi Dit+1 = Dit Di+1 = Di Ii+1 = Ii   f L L k R g 0  0  (C4) L is either positive and abducible or negative, L is ground, the opposite of L is in Di , then:  Fi+1 = Fi Di+1 = D  Dit+1 = Dt Ii+1 = Ii  0  0  0  (C5) L is either positive and abducible or negative, L is ground, and theret exists an abductive refutation from h:L Di  Di  Iii to the state h 2 k R  Dt D  $  I i then: 0  0  0  0  Fi+1 = Fi Ii+1 = I  0  0  0  Di+1 = D  0  and either Dit+1 = Dt if T  Dt j= R , or Dit+1 = Dt ^ R if T  Dt 6j= R and Dt ^ R is T -satisfiable.
0  0  0  0  0  0  0  0  0  For our purpose, the logic program P is made of the constrained clause form of the axioms of Figure 1 together with rules describing domain relationships, such as the clauses of Figure 2 for the YSP example.
Axioms A1 , A7, and A8 are integrity constraints in the set IC.
The predicate symbol persist is abducible: this means that whenever a new persistency hypothesis is needed, the consistency check will try to refute goals of the form begin(t p) and end(t p) where t falls within the period of the persistency assumption.
Of course, the aim is that these refutations fail so that the assumption does not violate integrity constraints.
4.2 Example  Recall the YSP example of Figure 2.
The query Q =end(t p), means that we are interested in finding when the gun will cease to be loaded.
Rule R3 yields the goal true(4 loaded).
Axiom A6 produces the goal persist(t1  t2 loaded) k t1 < 4 fi t2 where the literal persist(t1  t2 loaded) is abducible.
We begin a consistency derivation with a set of temporal ordering assumptions "t = fS1 < 4 fi S2 g, and a set of hypotheses " = fpersist(S1  S2  loaded)g, where S1 and S2 are new temporal constants.
The set of goals that we want to fail is: 8 :persist(S1  S2 loaded) k >  9 > > < = 2 k S 2 fi S1  F1 = > end(t loaded) k S1 fi t < S2  > : begin(t loaded) k S1 < t fi S2   The first clause in F1 disappears because the opposite of the literal :persist(S1  S2  loaded) is in ", and the second clause also disappears when we add the ordering constraints S1 < S2 to the set "t. At the end of the consistency derivation the set of clauses is empty, and the temporal ordering constraints in "t force S2 to be equal to 4 and S1 to T1 .
The primary abductive refutation ends and yields the substitution ft 7!
4g as an answer to the query Q.
One limitation of the procedure lies in the ability to handle repeated events: although the logic is able to describe those situations, the refutation procedure must be protected for infinite queries by a bound on the depth of the refutation.
It should be noted that those queries do not lead to subsumption between the current goal and one of its ancestors: it appears to be a \translation" on the time line.
At this moment, we do not have any mean to identify this relationship, nor can we characterize \translated" goals with respect to their utility in the refutation process.
5 Conclusions  In this paper, we describe an abductive procedure using a constrained resolution principle.
Such a resolution principle is very useful in the area of  temporal reasoning.
Constrained resolution allows an important gain on eciency by reducing nondeterminism, which is otherwise too much a trouble in pure resolution-based reasoning methods.
The abductive procedure is based on work done by kakas and Mancarella on abductive logic programming.
This procedure can be used to handle persistency as an assumption, and for planning problems where the set of computed hypotheses and temporal constraints describes a plan to achieve the requested goal.
References  1] James F. Allen.
Towards a general theory of action and time.
Artificial Intelligence, 23(2):123{ 154, 1984.
2] Hans-Jurgen Burckert.
A Resolution Principle for a Logic with Restricted Quantifiers, volume 568 of Lecture Notes in Artificial Intelligence.
Springer-Verlag, Berlin Heidelberg, 1991.
3] Rina Dechter, Itay Meiri, and Judea Pearl.
Temporal constraints networks.
Artificial Intelligence, 49:61{95, 1991.
4] K. Eshghi and R. A. Kowalski.
Abduction compared with negation by failure.
In G. Levi and M. Martelli, editors, Logic Programming: Proc.
of the Sixth International Conference, pages 234{254.
MIT Press, Cambridge, MA, 1989.
5] Kave Eshghi.
Abductive planning with event calculus.
In Proc.
of the 5th Int.
Conf.
on Logic Programming, pages 562{579, 1988.
6] Michael Gelfond and Vladimir Lifschitz.
The stable model semantics for logic programming.
In Proc.
of ICLP'88, pages 1070{1080, 1988.
7] A. C. Kakas and P. Mancarella.
On the relation between truth maintenance and abduction.
In Proc.
of PRICAI'90, pages 438{443, 1990.
8] A. Martelli and U. Montanari.
An ecient unification algorithm.
ACM Trans.
Programming Languages and Systems, 4(2):258{282, 1982.
9] Drew V. McDermott.
A temporal logic for reasoning about processes and plans.
Cognitive Science, 6:101{155, 1982.
10] Lode Missiaen.
Localized Abductive Planning with the Event Calculus.
PhD thesis, K.U.
Leuven, September 1991.
11] Erik Sandewall.
Non-monotonic entailment for reasoning about time and action Part I : Sequential actions.
Research Report LiTH-IDA-R-8827, Linkoping University, September 1988.
12] Murray Shanahan.
Prediction is deduction but explanation is abduction.
In Proc.
of the 11 th Int.
Joint Conference on Artificial Intelligence (IJCAI), pages 1055{1060, 1989.
13] Yoav Shoham.
Temporal logics in AI: Semantical and ontological considerations.
Artificial Intelligence, 33:89{104, 1987.
14] Mark E. Stickel.
Automated deduction by theory resolution.
Journal of Automated Reasoning, 1:333{355, 1985.
Using Inference for Evaluating Models of Temporal Discourse Philippe Muller IRIT, Universite Paul Sabatier, Toulouse, France muller@irit.fr Axel Reymonet IRIT, Universite Paul Sabatier, Toulouse, France reymonet@irit.fr  Abstract This paper addresses the problem of building and evaluating models of the temporal interpretation of a discourse in Natural Language.
The extraction of temporal information is a complicated task as it is not limited to finding pieces of information at specific places in a text.
A lot of temporal data is made of relations between events, or relations between events and dates.
Building such information is highly context-dependent, taking into account information more than a sentence at a time.
Moreover it is not clear what the target representation should be: the way it is done by human beings is still a subject of study in itself.
It seems to require some sort of reasoning, either purely temporal or involving complex world knowledge.
This is the reason why evaluating this task is also problematic when trying to design a system for it.
We present a method for enriching the detection of event-to-event relations with a basic reasoning model, that can be also used for helping to compare the extraction of temporal information by a system and by a human being.
We have experimented with this method on a set of texts, comparing a very basic model of tense interpretation with a more complex model inspired by the Reichenbach's well-known theory of narrative discourse.
1.
Introduction This paper focuses on the extraction of temporal informations in texts, and on the issue of evaluating a system automating the task.
While the semantics of temporal markers and the temporal structure of discourse are welldeveloped subjects in formal linguistics [20], the investigation of quantifiable systematic annotations of unrestricted texts is a somewhat recent topic.
The issue has started to generate some interest in computational linguistics [7], as it is potentially an important component in information ex-  traction, automatic summarization or question-answer systems, generating some international effort towards a standard mark-up scheme for temporal information, TimeML (www.timeml.org ).
This effort has been only partially related to the vast literature on temporal representation and reasoning in Artificial Intelligence (AI) and Knowledge Representation (KR).
While detecting dates and temporal expressions (such as after a few days, last year, at two o'clock,...) is not a very difficult problem, relating such expressions with events introduced by verbs require some syntactic analysis [21, 18].
Stamping events with a precise date is even more difficult, as this kind of information is not always available or is highly contextual [5].
Finding events denoted by nominal phrases (e.g.
World War I, the destruction of Troy) is not an easy task in general either, if they are not present in a typical prepositional phrase such as after World War I, and requires a specific lexicon.
Then there is an amount of information that is expressed only with relations between temporal entities (something happens before/during/after something else), and this level of vagueness raises new problems.
First, the relations best suited to that task must be chosen among many propositions, (linguistically oriented or more concerned with knowledge representation issues) Then, the target representation must be compared and evaluated with respect to some standard.
But the way it is done by human beings is still a subject of study in itself [15], and while it seems to require some sort of reasoning, either purely temporal or involving complex world knowledge, it is still unclear what representations humans have of the temporal ordering of events in a text they read.
The main proposal made for human annotation by [17], and imported in the TimeML recommendations, is to have a set of relations associated with a few rules of inference that are supposed to give a transitive closure of an annotation that can be the target of comparisons.
This used a specific  model that was somewhat unaware of the large KR/AI tradition of temporal representation and reasoning.
The use of a more well-studied inference model is advocated in [13], also as an arguably cleaner way of separating the problem of the intended representation of information from the process of handling and evaluating it.
We use it here to compare a few strategies in extracting the temporal ordering of events in natural language texts.
The paper is organised as follows: we present the different linguistic levels at which some temporal information can be expressed, then we discuss the problem of comparing temporal annotations and the need for an inference model.
We also present a few procedures to extract temporal relations between events and how they compare according to the methodology we proposed.
2.
Structuring temporal information expressed in NL The kind of temporal information that can be found in texts involves three classical levels: lexico-syntactic, semantic and pragmatic, and we deal with them separately.
At the lexico-syntactic level, we have specific temporal markers and patterns that group together expressions corresponding to similar semantic types of temporal adjuncts as follows (translated from their French counterparts): * non absolute dates ("March 25th", "in june"), * absolute dates "July 14th, 1789", * dates, relative to utterance time ("two years ago"), * dates, relative to some temporal focus ("3 days later"), * absolute dates, with imprecise reference ("in the beginning of the 80s"), * basic durations ("during 3 years"), * durations with two dates (from February, 11 to October, 27. .
.
), * absolute durations ("starting July 14"), * relative durations, w.r.t utterance time ("for a year"), * relative durations, w.r.t temporal focus ("since"), * temporal atoms (three days, four years, .
.
.
).
At the semantic level, each type of adjunct (date or duration) gets values for each instantiated attribute among: starting time, ending time, duration, type (absolute/relative).
This uses only parts of the TimeML coding standard for dates and durations that are relevant for the following treatments.
Then, according to each type of temporal adjunct, we try to establish a link between an event and any temporal adjunct present in the same syntactic clause (the event is before, after, or during a date, or receive a duration that can be used later on).
At the pragmatic level, which is the level we want to investigate more precisely and the one in need of a methodology, we handle the semantic of verb tenses with respect to the temporal structure of a discourse (how tenses are chained, and what this means for relations between events for instance), how temporal references evolve through the interpretation and the role of the structure of discourse1 .
More details are given sections 4, 5.
3.
Evaluating annotations 3.1.
The problem of comparing temporal models of the same text What we want to annotate is something close to the temporal model built by a human reader of a text; as such, it may involve some form of reasoning, based on various cues (lexical or discursive), and that may be expressed in several ways.
As was noticed by [17], it is difficult to reach a good agreement between human annotators, as they can express relations between events in different, yet equivalent, ways.
For instance, they can say that an event e1 happens during another one e2 , and that e2 happens before e3 , leaving implicit that e1 too is before e3 , while another might list explicitely all relations.
One option could be to ask for a relation between all pairs of events in a given text, but this would be demanding a lot from human subjects, since they would be asked for n x (n - 1)/2 judgments, most of which would be hard to make explicit.
Another option, followed by [17] is to use a few rules of inference (similar to the example seen above), and to compare the closures (with respect to these rules) of the human annotations.
Such rules are of the form "if r1 holds between x and y, and r2 holds between y and z, then r3 holds between x and z".
Then one can measure the agreement between annotations with classical precision and recall on the set of triplets (event x,event y,relation).
This is certainly an improvement, but [17] points out that humans still forget available information, so that it is necessary to help them spell out completely the information they should have annotated.
Setzer estimates that an hour is needed on average for a text with a number of 15 to 40 events, and subjects get tired of it very quickly.
1 This aspect has not been studied here as it raises many more theoretical questions, see [3], and might not be ready yet for the kind of evaluation we have in mind; but see a preliminary report in [10].
3.2.
Separating the inference model from the annotation scheme The previously introduced method has two shortcomings.
First, the choice of temporal relations proposed to annotators, i.e.
"before", "after", "during", and "simultaneously", is arbitrary and somewhat ill defined (the latter is defined as "roughly at the same time", [17], p.81).
The second  before  X Y X during  Y  X meets  Y starts  X Y  X overlaps Y  X equals  X finishes  Y  Y  Figure 1.
Allen Relations between two intervals X and Y (Time flows from left to right)  problem is related to the inferential model considered, as it is only partial.
Even though the exact mental processing of such information is still beyond reach, and thus any claim to cognitive plausibility is questionable, there are more precise frameworks for reasoning about temporal information.
For instance the well-studied Allen's relation algebra (see Figure 1).
Here, relations between two time intervals are derived from all the possibilities for the respective position of those interval endpoints (before, after or same), yielding 13 relations2 .
These relations are now the possible relations between events in the TimeML scheme.
What this framework can also express are more general relations between events, such as disjunctive relations (relation between event 1 and event 2 is relation A or relation B), and reasoning on such knowledge.
We think it is important at least to relate annotation relations to a clear temporal model, even if this model is not directly used.
Besides, we believe that measuring agreement on the basis of a more complete "event calculus" will be more precise, if we accept to infer disjunctions of atomic relations.
Then we want to give a better score to the annotation "A or B" when A is true, than to an annotation where nothing is said.
Section 3.4 gives more details about this problem.
In order to validate the method, we have to compare the results given by the system with a "manual" annotation.
It is not really realistic to ask humans (experts or not) for Allen relations between events.
They are too numerous and some 2 In the following (and Table 2) they will be abbreviated with their first letters, adding an "i" for their inverse relations.
So, for instance, "before" is "b" and "after" is "bi" (b(x,y)[?]
bi(y,x)).
are too precise to be useful alone, and it is probably dangerous to ask for disjunctive information (it remains to be seen what the TimeML conventions will yield in that respect, as they force the annotator to choose one and only one of Allen's relations).
But we still want to have annotation relations with a clear semantics, that we could link to Allen's algebra to infer and compare information about temporal situations.
Here we have chosen relations similar to that of [4] (as in [9]), who inspired Allen; these relations are equivalent to certain sets of Allen relations, as shown in Table 1.
We thought they were rather intuitive, seem to have an appropriate level of granularity, and since three of them are enough to describe situations (the other 3 being the converse relations), they are not to hard to use by naive annotators.
This would have to be confirmed by an empirical study on a set of annotators.
Table 1 give their definition from Allen relations.
Relations "includes" and "is_included" can be applied when two relations have the same temporal extent, for simplicity (since this is very rare), and it is also possible to indicate that two expressions refer to the same event (they are then treated as only one node).
3.3.
The inference model We have argued in favor of the use of Allen relations for defining annotating temporal relations, not only because they have a clear semantics, but also because a lot of work has been done on inference procedures over constraints expressed with these relations.
We therefore believe that a good way of avoiding the pitfalls of choosing relations for human annotation and of defining inference patterns for these relations is to define them from Allen relations and use relational algebra computation to infer all possible relations between events of a text (i.e.
saturate the constraint graph, see below), both from a human annotation and an annotation given by a system, and then to compare the two.
In this perspective, any event is considered to correspond to a convex time interval.
The set of all relations between pairs of events is then seen as a graph of constraints, which can be completed with inference rules.
The saturation of the graph of relations is not done with a few handcrafted rules of the form (relation between e1 and e2) + (relation between e2 and e3) gives (a simple relation between e1 and e3) but with the use of the full algebra of Allen relations.
This will reach a more complete description of temporal information, and also gives a way to detect inconsistencies in an annotation (which can be useful for a human annotator).
An algebra of relations can be defined on any set of relations that are mutually exclusive (two relations cannot hold at the same time between two entities) and exhaustive (at least one relation must hold between two given en-  BEFORE AFTER OVERLAPS IS _ OVERLAPPED INCLUDES IS _ INCLUDED  [?]i[?
]j [?]i[?
]j [?]i[?
]j [?]i[?
]j [?]i[?
]j [?]i[?
]j  (i before j = ((i b j) [?]
(i m j))) (i after j = ((i bi j) [?]
(i mi j))) (i overlaps j = ((i o j))) (i is_overlapped j = ((i oi j))) (i includes j = ((i di j) [?]
(i si j) [?]
(i fi j) [?]
(i e j))) (i is_included j = ((i d j) [?]
(i s j) [?]
(i f j) [?]
(i e j)))  Table 1.
Relations proposed for annotation tities).
The algebra starts from a set of atomic relations U= {r1 , r2 , ...}, and a general relation is a subset of U, interpreted as a disjunction of the relations it contains.
From there we can define union and intersection of relations as classical set union and intersection of the base relations they consist of.
Moreover, one can define a composition of relations as follows: (r1 * r2 )(x, z) - [?
]y r1 (x, y) [?]
r2 (y, z) By computing beforehand the 13 x 13 compositions of base relations of U, we can compute the composition of any two general relations (because r [?]
r 0 =O when r, r 0 are basic and r 6= r0 ): [ {r1 , r2 , ...rk } * {s1 , s2 , ...sm } = (ri * sj ) i,j  Saturating the graph of temporal constraints means applying these rules to all compatible pairs of constraints in the graph and iterating until a fixpoint is reached.
The following, so-called "path-consistency" algorithm [2] ensures this fixpoint is reached: Let A = the set of all edges of the graph, N = the set of vertices of the graph, U = the disjunction of all 13 Allen relations, Rm,n = the current relation between nodes m and n 1. changed = F alse 2. for all pair of nodes (i, j) [?]
N x N and for all k [?]
N such that ((i, k) [?]
A [?]
(k, j) [?]
A) (a) R1i,j = (Ri,k * Rk,j ) (b) if no edge (a relation R2i,j ) existed before between i and j, then R2i,j = U (c) intersect: Ri,j = R1i,j [?]
R2i,j (d) if Ri,j = [?]
(inconsistency detected) then : error (e) if Ri,j = U (=no information) do nothing else update edge; changed = T rue 3. if changed, then go back to 1.
This algorithm is proven to be correct: if it detects an inconsistency then there is really one, but incomplete in general (it does not necessarily detect an inconsistent situation).
Allen's original algorithm can be improved in various ways (using reference intervals to build clusters on which local consistency is enforced for instance), but this was not really necessary here since the graphs are rather small.
There are sub-algebras for which this procedure is also complete, and it would be interesting to see if annotations are part of such a sub-algebras (see [16] for more details about temporal constraints and algorithms).
3.4.
Comparing two temporal graphs To abstract away from particulars of a given annotation for some text, and thus to be able to compare the underlying temporal model described by an annotation, we try to measure a similarity between annotations given by a system and human ones, from the saturated graphs of detected temporal relations.
We do not want to limit the comparison to "simple" (atomic) relations, as in [17], because it makes the evaluation very dependent on the choice of relations, and we think it can give a misleading impression of how good the system performs (consider for instance the extreme case of a non-consistent annotation that could not be detected when looking only for basic relations).
We also want to have a gradual measure of the imprecision of the system annotation.
For instance, finding there is a "before or during" relation between two events is better than proposing "after" if the human put down "before", and it is less good than the correct answer "before".
Actually we are after two different notions.
The first one is the consistency of the system's annotation with the human's: the information in the text can never contradict the system's annotation, i.e.
the former implies the latter.
The second notion is how precise the information given by the system is.
A very disjunctive information is less precise than a simple one, for instance (a or b or c) is less precise than (a or b) if a correct answer is (a).
In order to measure these, we use two elementary comparison functions between two sets of relations S and H (each set has as members the basic relations that constitutes the disjunction), where S is the annotation proposed by the system and H is the annotation inferred from what was proposed by the human: finesse =  |S[?
]H| |S|  coherence =  |S[?
]H| |H|  The global score of an annotation is the average of a measure on all edges that have information according to the human annotation (this excludes edges with the universal disjunction U) once the graph is saturated.
Finesse is intended to measure the quantity of accurate information the system gets, while coherence gives an estimate of errors the system makes with respect to information in the text.
Finesse and coherence thus are somewhat similar respectively to recall and precision, but we decided to use new terms to avoid confusion ("precision" being an ambiguous term when dealing with gradual measures, as it could mean how close the measure is to the maximum 1).
Obviously if S=H on all edges, all measures are equal to 1.
If the system gives no information at all, S is a disjunction of all relations so H [?]
S, H [?]
S = H and coherence=1, but then finesse is very low.
These measures can of course be used to estimate agreement between annotators.
4.
Stages for the extraction of temporal relations We will now present our method to achieve the task of annotating automatically event relations, before going through a small example.
The starting point was raw text plus its broadcast date.
We then applied the following steps prior to discourse interpretation: * part of speech tagging, with some post-processing to locate some lexicalised prepositional phrases and mark specific lexical items (days, months,...);  these events and dates (noted t1 , t2 , ...) recognized in the text: input = h(e1 , {(e1 R1,j1 tj1 ), ...}), (e2 , {...}), ...i Then we consider the following steps: 1. filtering of events according to their lexical type (to exclude e.g.
states, reports, or aspectual constructions and focus on "occurrences", in the TimeML terminology).
2. chaining constraints: a set of discourse rules is used to establish possible relations between two events appearing consecutively in the text, according to the tenses of the verbs introducing the events.
These rules for French are similar to rules for English proposed in [6, 19, 8], but are expressed with Allen relations instead of a set of ad hoc relations.
Let tense(ej ) be the tense of the verbe denoting event i in the text.
Then V (tense(ei ), tense(ei-1 )) denotes the possible relations between two successive events having the same tense as ei and ei-1 , and it is taken from Table 2.
For instance, if both event are simple past events, Vsp,sp = {e, b, m, s, d, f, o} These rules are only applied when no temporal marker (such as "when", "after that", "then",...) indicates a specific relation between the two events or when computing dates does not conflict with the result (leading to an inconsistent temporal graph).
* for each event associated to a temporal adjunct, a temporal relation is established, with a date when possible.
3. temporal perspective handling: introducing state variables following Reichenbach [14] : E: the event reference point (the current narrative location, which is an event), S: speech time, R: the temporal perspective point (another event).
Rules are used to determine how these variables change during the interpretation, according to various cues, the main one being a change of tense.
For instance, a series of past tense verbs have as reference point the last event introduced (so E=R and E < ei for each new simple past event ei , which then becomes R) while the use of the pluperfect shifts the temporal perspective point (if ej has tense pluperfect, ej < R, and for each subsequent pp event, E < ej < R).
The relations due to Reichenbach are summed up Table 3.
Actually we observed that the ordering relation he assumed is too strong in practice, so the one we used is less constrained, we replace precedence with {b, m, s, f, d, o, e} (before or included or overlap).
The only tenses that depend on a temporal perspective are the pluperfect and the future perfect.
Details and an example are given below.
At this point we have a sequence of events (recording also their types and tenses) with some relations between  The basic algorithm only applies the second step (see example below).
The more elaborate one derived from  * partial parsing with a cascade of regular expressions analysers (cf.
[1]; we also used Abney's Cass software to apply the rules).
This was done to extract dates, temporal adjuncts, various temporal markers, and to achieve a somewhat coarse clause-splitting (one finite verb in each clause) and to attach temporal adjuncts to the appropriate clause (this is of course a potentially large source of errors).
Relative clauses are extracted and put at the end of their sentence of origin, in a way similar to [5].
* date computation to precise temporal locations of events associated with explicit, yet imprecise, temporal information, such as dates relative to the time of the text (e.g.
last monday).
Reichenbach does some filtering and then handles variables as follows (mixing chaining constraints with temporal perspective):i - 1 is the index of the previously handled event, i is the current event index.
At the beginning, E=R=the first event.
1. get all possible relations relVi-1,i = V (tense(ei ), tense(ei-1 )) between tenses of events i - 1 and i from Table 2 2. if a relation (ei relDi-1,i ei-1 ) can be inferred from input (by date computation), (a) rel = relDi-1,i [?]
relVi-1,i (intersect the relations) (b) if (rel 6= [?
]), (relate current event i with E, R and S, according to its tense, following table 3, and to i - 1, if it's not already E or R) i. add to graph: (E relE,i ei ), (R relR,i ei ), (S relS,i ei ), (ei-1 rel ei ) ii.
if (there is change of tense to a new one not dependant on R), then (reinitialise R and E).
iii.
else if (rel [?]
{ b , m , o }), then (E = ei ) iv.
else if (rel [?]
{ bi , mi , oi }), (R = E); (E = ei ) (c) else (rel = [?
]), record (ei-1 relDi-1,i ei ) (just keep the date constraint if any was found) and reinitialise R et E (we found an inconsistency so there must have been a narrative shift) 3. else [!
( [?]
relDi-1,i )], update S, R, E 3 as in 2)b) and the temporal graph.
5.
Example of the processing pipeline We will use the following text, extracted (and slightly modified for simplicity, since most sentences in the texts are very long) from a text of our corpus, dated Monday, June 2nd, 2003.
Les chances de percee au sommet d'Aqaba ont diminue (lundi).
Le Premier Ministre israelien Ariel Sharon, son homologue palestinien Mahmoud Abbas et M. Bush se sont reunis, et ont echoue a trouver un accord sur le calendrier a suivre.
La possibilite d'un cessez-le-feu avait ete evoquee mais avait ete rejetee.
Translation: Chances of a breakthrough at the Aqaba summit decreased (Monday).
PM A. Sharon, PM M. Abbas and P. G. Bush met, and failed to agree on a schedule.
The possibility of a cease-fire had been considered but had been rejected.
3 As  before, update follows table 3.
The preprocessing consists in tokenizing (separating words) lemmatizing and morphosyntactically tagging the text, yielding for the beginning of the second sentence (tags used are from the well-known Penn Treebank format, added with specific tags for verbs and time related words): le/dta chance/nn de/of percee/nn a/a le/dta sommet/nn de/of Aqaba/nnp avoir/aux_pres diminuer/ver_ppas lundi/day... Then the shallow parser can be applied.
It is made of pattern matching rule of the form : rewrite a sequence of "dta nn" as "nx" (noun chunk), divided in stages where the output of each stage is the input of the next.
The previous sentence would then be analysed as (brackets indicates the hierarchical structure; some simplifications in the structure are made for clarity, as there are many levels of noun phrases) [c0 [np2 [np [nx [dta le] [nn chances]]] [of de] [np [nx [nn percee]]] [a a] [np [nx [dta le][nn sommet] [name [of de][nnp Aqaba]]]]]] [vx [ver_pc [aux_pres avoir] [ver_ppas diminuer]]] [cct [daterelST [day lundi]]] ] ...
Here c0 indicates a basic clause, whith only one finite verb in it.
The semantic interpretation of the sentence can thus be done, where any temporal adverbial (cct structures) is given a value if possible and is related to the event in the same clause, according to its type.
Here "lundi" belongs to the category of date-relative-to-speech-time, and since it is used with a past-tensed verb it is computed as the monday before speech time, so is given the value (2003-5-26).
Besides, as the adverbial phrase is a direct adjunct of the verb, it is assumed to be a simple localisation (so the event in included in the date).
Every time another date is added to the interpretation, we compute any qualitative relation we can find with every date already introduced, using their computed value (including duration calculus is there is any).
For the example text, the dates detected are (t0=date of the publication): t3 : 2003-5-26 t0 : 2003-6-2  (lundi)  So we also have that t0 < t3 Here the system considered that a past tense used with "lundi" (monday) in the first sentence meant last monday before the publication of the text (while it is a (unsual) way of referring to the day of the publication in AFP news texts).
e1/e2 imp pqp pres sp  imp o, e, s, d, f, si, di, fi b, m,o, e, s, d, f U b, m, o, e, s, d, f  pqp bi, mi, oi b, m, o, e, s, d, f, bi, mi U e, s, d, f, bi, mi  pres e, b e, b b, m, o, si, di, fi, e e, b  sp o, d, s, f, e, si, di, fi b, m, o U e, b, m, s, d, f, o  Table 2.
Possible temporal relations between two successive events according to their tenses, for the main relevant tenses, sp=simple past and perfect, imp=French imparfait, pqp=pluperfect, pres=present; U stands for the universal relation (no information) Tense of current event Plus-que-parfait (pluperfect) Passe simple (simple past)/Imparfait Present Simple Future Futur anterieur (future perfect)  relations between event i and E,R,S E<i<R<S E=R<i<S i [?]
S, E = R S<E=R<i S<E<i<R  Table 3.
Grammatical tense and Reichenbach perspective: relations between current event and E (last one), R (reference point) and S (speech time)  The last stage consists in determining the possible relations between successive events.
We will first have a look at the basic algorithm.
In the example, we have a sequence of clauses as follows (begin=1 stands for a fictitious event corresponding to the speech time), with number of event, tense of the verb (pp=past perfect, pqp=pluperfect), lemma of the verb.
1 2 4 5 8 9  begin ver_pp ver_pp ver_pp ver_pqp ver_pqp  'diminuer' 'reunir' 'echouer' 'evoquer' 'rejeter'  (decrease) (meet) (fail) (consider) (reject)  The algorithm will then introduce a relation between speech time and each event according to its tense (past or future, so here 2 < 1, 4 < 1, 5 < 1,...), add that e1 is during t0 , and use table 2 to introduce relations between successive events.
Event 4 follows event 2 so we look at the table for (pp,pp) which the same as (sp,sp) in current french, and find 2{e, b, m, s, d, f, o}4.
The same is found between 4 and 5.
Then (pqp,pp) yields a relation between 5 and 8: {e, b}.
Finally we have between 8 and 9, from (pqp,pqp): {b, m, o, e, s, d, f, bi, mi}.
At each stage, the graph is saturated and if an inconsistency appears, we come back to the state before the last introduced event.
Looking at the graph we see that nothing is inferred between 5 and 9, while the use of the pluperfect indicates that 9 have occcured before the sequence 2-5.
This is the problem addressed in part by the method inspired by Reichenbach's work.
The interpretation following Reichenbach would consider two variables: E, the last event introduced, R the ref-  erence point and S, the speech time (=event 1).
For the explanation, only the sequence of tenses is relevant since there is only one date and no temporal connector between clauses.
This is how E and R evolve, and the relation between the current event and E, R and S, using Table 3 (for readability we left the symbol "<", but it should be seen as {b, m, o, e, s, d, f }): + event E R relations new E new R 2(pp) 2<S 2 2 4(pp) 2 2 E<4<S 4 4 5(pp) 4 4 E<5<S 5 5 8(pqp) - 5 8<R<S 8 5 9(pqp) 8 5 E<9<R 9 5 There are special rules for updating E and R when there is a change of tense depending whether the new tense needs a temporal perspective (plupefect, future perfect) (e.g.
pp(5) to pqp(8), then R becomes the previous E, and E is reset to nothing), or not (all others tenses, then E and R are reset when there is a change).
6.
Comparing theories of temporal interpretation Our methodology has been used to compare a few strategies for the pragmatic level of discourse temporal interpretation.
For each text we have made two series of measures: one on annotation relations (thus disjunctions of Allen relations are re-expressed as disjunctions of annotation relations that contains them), and one on equivalent Allen relations (which arguably reflects more the underlying computation,  while deteriorating the measure of the actual task).
We then used finesse and coherence to estimate our annotation made according to the method described in the previous sections.
We tried it on a limited set of newswire texts (from AFP), for a total of about 200 events.
Each one of these texts has between 10 and 40 events.
The human generated graphs have between 12 and 266 edges for an average of 115.
The system average size was about 170 (for the last two methods combined, with more edges for the last one).
We averaged this on the number of texts.
Results are shown Table 4.
The first strategy is a "random" annotation made in the following way: to each pair of successive events in a text, we choose a random annotation relation.
Then we saturate the graph of constraints and we compare with the human annotation.
This strategy has poor results except for the coherence on Allen relations, which is very high maybe because random unrelated annotations do not produce a lot of coherent additional information and re-expressed as Allen relations they result in very disjunctive information (thus trivially coherent)4.
The second one is a baseline using a similar strategy but assuming every event is in the order it has in the text (so instead of having a random relation between consecutive events, we always generate "before").
Again, coherence in Allen relations is surprisingly high, but the rest is very low.
The third one is the strategy based on simple tense chaining constraints.
It was the first strategy we tried to check the feasability of this kind of study.
The last one is based on a filtered, localised Reichenbach model that performs better on coherence while damaging finesse a lot.
This one takes less risk since it may "reset" the interpretation to record more local constraints and is perhaps more precise about the information it gives.
Nonetheless, when it is converted back to the simplest annotation relations, there is not much difference with the more basic model, and it's even slightly worse.
At first glance, it seems that the venerable ideas of Reichenbach are tricky to apply to real texts, where local coherence may not be ideal.
We still have to analyse precisely what really happens here, but it means at least that we have to keep the measure on Allen relations to study more finely the methods used.
To the best of our knowledge, only [9] and [12] mention having tried this kind of annotation 5 .
The former considers only relations between events in a same sentence, and the latter did not evaluate their method until they enrich it with a learning stage [11].
They indicate good results (about 75%) in both recall and precision on unambiguous (atomic) relations, but this is only on a partial ordering, and includes relations between dates and both events or dates (a much easier task, since dates are almost always explicitely, if not 4 We  have not investigated that unexpected effect yet.
lot of other proposals have not been evaluated beyond a set of fabricated examples, see among others [19, 8, 6].
5A  unambiguously, related to an event).
It seems to us that the measures we propose reflects more accuratly the difficulty of the task.
Finally, it is worth remembering that human annotation itself is a difficult task, with potentially a lot of disagreement between annotators.
For now, our texts have been annotated by two experts, with an a posteriori resolution of conflicts.
We therefore have no measure of inter-annotator agreement which could serve as an upper bound of the performance of the system.
But we also did an experiment to see how human can agree on this task without too much training.
We took 7 subjects to whom we explained the notion of relations between events and how to decide them and gave them a short newswire text to annotate (the text had 12 events, which were underlined in the text so only relations were to be added).
The results were, as expected, not very good: on average on 42 comparisons6, finesse = 0.51 and coherence is .49 for Allen relations, and finesse= .58 and coherence = .55 for annotation relations.
We are in the process of building a larger corpus of texts with precise annotation from experts, with an evaluation of the agreement between them7 .
Last, one could argue that trying to reach that level of precision in finding temporal relations in a text is irrealistic, and that humans don't do it and focus on the important temporal relations.
This would have to be investigated but it appears difficult to implement something similar from a methodological point of view.
Establishing a standard annotation for a text would still be an issue; moreover one runs the risk of getting only the obvious, explicit relations, while it has been shown that humans can agree on relations they had not seen by themselves, if they are asked a posteriori.
7.
Conclusion In this paper we have presented a method to handle temporal information in natural language texts, which tries to bridge the gap between natural language processing methods and knowledge representation techniques.
Using wellstudied inference processes is used for two tasks: improving the extraction of information, and helping the evaluation and comparison of the extracted information.
Our pilot study shows the possibilities of combining techniques and leaves a lot of room for improvements and experimentation.
We have chosen the simplest form of reasoning to stay within reasonable computational bounds but different models could be tried, as long as the target representation is linked but separated from the inference model.
A lot of 6 The measures are not symmetric because we consider only the events correctly found by one annotator against the other one, regarded as the correct annotation; however, note that precision(a,b)=coherence(b,a).
7 Preliminary tests let us believe that measures around 0.7-0.8 should be reached, although not without effort.
Method "Random" baseline "Before" Baseline Basic constraints Modified Reichenbach  F (Allen) 0.04 0.03 0.49 0.195  C (Allen) 0.78 0.84 0.55 0.75  F (Ann.
rel.)
0.05 0.03 0.23 0.215  C (Ann.
rel.)
0.02 0.015 0.31 0.225  Table 4.
Comparing methods tuning still has to be made to reach good results, but we also hope to help the task of the human in specifying temporal knowledge (as this is far from obvious from the agreement we observe between humans) and by bootstrapping the annotation either in a semi-automated way, or as a starting point for learning algorithm, as in [11].
References [1] Steven Abney.
Corpus-Based Methods in Language and Speech, chapter Part-of-Speech Tagging and Partial Parsing.
Kluwer Academic Publisher, 1996.
[2] J. Allen.
Towards a general theory of action and time.
Artificial Intelligence, 23:123-154, 1984.
[3] N. Asher and A. Lascarides.
Temporal interpretation, discourse relations, and commonsense entailment.
Linguistics and Philosophy, 16:437-493, 1993.
[4] B. Bruce.
A model for temporal references and its application in a question answering program.
Artificial Intelligence, 3(1-3):1-25, 1972.
[5] Elena Filatova and Eduard Hovy.
Assigning timestamps to event-clauses.
In Harper et al.
[7].
[6] Claire Grover, Janet Hitzeman, and Marc Moens.
Algorithms for analysing the temporal structure of discourse.
In Sixth International Conference of the European Chapter of the Association for Computational Linguistics.
ACL, 1995.
[7] Lisa Harper, Inderjeet Mani, and Beth Sundheim, editors.
ACL Workshop on Temporal and Spatial Information Processing, 39th Annual Meeting and 10th Conference of the European Chapter.
Association for Computational Linguistics, 2001.
[8] M. Kameyama, R. Passonneau, and M. Poesio.
Temporal centering.
In Proceedings of ACL 1993, pages 70-77, 1993.
[9] W. Li, K-F. Wong, and C. Yuan.
A model for processing temporal reference in chinese.
In Harper et al.
[7].
[10] I. Mani and J. Pustejovsky.
Temporal discourse models for narrative structure.
In ACL Workshop on Discourse Annotation, Barcelona, Spain, 2004.
[11] I. Mani, B. Schiffman, and J. Zhang.
Inferring temporal ordering of events in news.
In Proceedings of the Human Language Technology Conference (HLTNAACL'03), 2003.
(short paper).
[12] I. Mani and G. Wilson.
Robust temporal processing of news.
In Proceedings of ACL 2000, 2000.
[13] P. Muller and X. Tannier.
Annotating and measuring temporal relations in texts.
In Proceedings of Coling 2004, volume I, pages 50-56, Geneve, 2004.
[14] H. Reichenbach.
Elements of Symbolic Logic.
McMillan, New York, 1947.
[15] W. Schaeken and P. N. Johnson-Laird.
Strategies in temporal reasoning.
Thinking and Reasoning, 6:193- 219, 2000.
[16] Eddie Schwalb and Lluis Vila.
Temporal constraints: A survey.
Constraints, 3(2/3):129-149, 1998.
[17] Andrea Setzer.
Temporal Information in Newswire Articles: an Annotation Scheme and Corpus Study.
PhD thesis, University of Sheffield, UK, September 2001.
[18] Franck Shilder and Christopher Habel.
From temporal expressions to temporal information: Semantic tagging of news messages.
In Harper et al.
[7], pages 65-72.
[19] F. Song and R. Cohen.
Tense interpretation in the context of narrative.
In Proceedings of AAAI'91, pages 131-136, 1991.
[20] Mark Steedman.
Temporality.
In J.
Van Benthem and A. ter Meulen, editors, Handbook of Logic and Language.
Elsevier Science B.V., 1997.
[21] Nikolai Vazov.
A system for extraction of temporal expressions from french texts based on syntactic and semantic constraints.
In Harper et al.
[7].
A Proper Ontology for Reasoning About Knowledge and Planning Leora Morgenstern  IBM T.J. Watson Research Center P.O.
Box 704, Yorktown Heights, N.Y. 10598 leora@watson.ibm.com will be successful.
Abstract:  Research on the knowledge preconditions problems for actions and plans has sought to answer the following questions: (1) When does an agent know enough to perform an action?
(2) When can an agent execute a multi-agent plan?
It has been assumed that the choice of temporal ontology is not crucial.
This paper shows that this assumption is wrong and that it is very di cult to develop within existing ontologies theories that can answer both questions (1) and (2).
A theory of linear time does not support a solution to the knowledge preconditions problem for action sequences.
A theory of branching time solves this problem, but does not support a solution to the knowledge preconditions problem for multi-agent plan sequences.
Linear time supports prediction, but does not support hypothetical reasoning branching time supports hypothetical reasoning, but does not support prediction.
Since both prediction and hypothetical reasoning are essential components of the solution to the knowledge preconditions problems, no comprehensive solution has yet been proposed.
To solve this problem, we introduce a new temporal ontology, based on the concept of an occurrence that is real relative to a particular action.
We show that this ontology supports both hypothetical reasoning and prediction.
Using this ontology, we dene the predicates needed for the proper axiomatization for both knowledge preconditions problems.
1 Introduction  Intelligent agents not only possess knowledge, but they reason about the knowledge that they possess.
This sort of introspection is particularly crucial for planning.
Agents are not capable of performing every action, so an agent who constructs a plan must reason about his ability to perform the actions in his plan.
Since the ability to perform many actions rests directly upon an agent's knowledge, he must reason about whether he has that knowledge, or how he can get that knowledge.
For example, an agent who plans to perform the sequence of actions: (open up safe, remove money) must know that he knows the combination of the safe in order to predict that his plan  There has been a fair amount of research in the eld of knowledge and planning in the last 15 years.
Most of this work (Moore, 1980], Konolige, 1982]) has focussed on the knowledge preconditions problem for actions: what does an agent need to know in order to perform an action?
This question is only part of the story, however: if an agent does not know enough to perform an action, he will presumably not just drop his goal.
Instead, he will either plan to get the information, possibly by asking another agent, or by delegating the task to another more knowledgeable agent.
In either case, he will have to construct a more complex multi-agent plan.
This gives rise to the knowledge preconditions problem for plans: what does an agent have to know in order to successfully execute a plan?
For example, if I don't know the combination of the safe, I may ask Bob to tell me the combination.
To predict that this plan will work, I must know that Bob knows the combination, that he will tell it to me, and so on.
Presumably, the knowledge preconditions for this sort of plan are weaker than for my plan to open the safe { but they are di cult to make explicit.
In Morgenstern, 1988], we studied the knowledge preconditions problem for plans in detail, and furnished axioms giving su cient knowledge preconditions for various sorts of plans, including sequences, conditionals, and loops.
However, as noted there, these axioms are overly strong they entail that an agent has su cient knowledge to execute a plan even when intuition tells us otherwise.
For example, what seems to be a straightforward or \natural" way of axiomatizing the knowledge preconditions for plan sequences entails that an agent could always do a sequence of actions as long as he could perform the rst action, but (and this is the crucial point) he never actually did.
This was true even if the second action was impossible to perform.
The theory is still valid for forward reasoning planning however, it is clearly undesirable to have a theory that legitimates degenerate plans.
This paper addresses and solves this problem.
We had previously suggested that the problem was most probably due to the use of linear time, and claimed that using a more sophisticated temporal ontology such as branching time would solve the prob-  lem.
As we will show in this paper, branching time is also not su cient.
We need to construct a new and richer underlying temporal ontology.
This paper is structured as follows: We briey describe the logical language used, and give a natural language characterization of the solution to the knowledge preconditions problems.
Next we show that formalizing these axioms in a linear theory of time will not work.
The following section shows that the seemingly obvious solution { recasting these axioms using a branching theory of time { does not work either.
Finally, we introduce a new temporal ontology, called relativized branching time, which takes elements of both branching and linear times and is based on the notion of the \most real" world, relative to a particular action.
We show that this temporal ontology can be used to construct a correct theory of knowledge preconditions for actions and plans.
2 The Logical Language  We will be working in a logical language L, an instance of the rst order predicate calculus.
(What follows is terse and incomplete, due to space considerations.
L is modeled on the logic used in Morgenstern, 1988] ) L is distinguished by the following features: 1] L contains a 3-place predicate Know.
Know(a,p,s) means that agent a knows the sentence represented by the term p in the situation (\time-point") s. When we say that the term p represents a sentence, we are indicating that a quotation construct is present in L. Thus: 2] L allows quotation.
We can use a term or a w in L and talk about that term or w in L. We do this by associating with each term or w of L the quoted form of that term or w.
In general, we will denote the term representing a w or term as that w or term surrounded by quotation marks.
Some notes on quotation: unrestricted use of quotation can lead to paradox Montague, 1963] some sort of resolution is necessary.
Here we choose: 3] L is interpreted by a three-valued logic, which is transparent to the user and ignored in the remainder of this paper.
4] Quantication into quoted contexts is a somewhat messy enterprise, involving some sort of quasi-quotes.
We use the notation of Davis, 1990] : The delimiters ^^ and ## are used when the variables that are quantied into quoted contexts range over strings @ is used for variables that range over objects other than strings.
The partial function h maps a string onto the term it represents it is abbreviated as the .
(the period).
Those unfamiliar with quasi-quotation should just ignore these symbols.
As we have indicated, and will be arguing at greater length, the choice of a temporal ontology will be crucial for our endeavor.
Nevertheless, there are some elements that will be present in any choice.
They  are: 5] The basic building block is the situation, or time point.
(How these points are organized is the crux of the dierences between approaches).
Intervals of time are indicated by a pair of time-points, the starting time and the ending time.
An action or event is a collection of intervals { intuitively those intervals in which the action takes place.
An event is an action restricted to a particular agent (the performing agent).
The function Do maps an agent and an action onto an event.
Actions and events can be structured using standard programming language constructs.
A plan is any structure of events, e.g., Sequence(Do(Susan(ask(Bob,combination)), Do(Bob(tell(Susan, combination)))) A restricted subset of actions are primitive: - they cannot be further decomposed.
Other actions are complex and are built  up out of primitive actions using our programming language structures.
In all formulas of the theory and metatheory, all variables are assumed to be universally quantied unless otherwise indicated.
3 What We Want to Say  In English, the solution to the knowledge preconditions problem for actions can be stated as follows: it is assumed that all agents know how to perform the basic action types of primitive actions.
In order to know enough to perform a primitive action, then, one must only know what the parameter of the action is.
That is, one must know of a constant equivalent to the parameter.
Thus, for example, suppose that dial is a primitive action.
Then one knows how to dial the combination of a safe if one knows of a sequence of digits equivalent to the combination of the safe.
The knowledge preconditions for complex actions are given recursively in terms of the knowledge preconditions for primitive actions.
If an action is complex, an agent must explicitly know its decomposition into primitive actions, and know how to perform the decomposition.
Moreover, if one cannot perform an action, one generally constructs some multiple agent plan whose end result is the achievement of the original goal.
The solution to the knowledge preconditions problem for plans can therefore be stated as follows: An agent knows how to execute a plan if he knows how to perform all of the actions of the plan for which he is the performing agent, and can predict that the other agents in the plan will perform their actions when their time allows.
For example, Susan can execute the plan sequence sequence(do(Susan, ask(Bob, combination)), do(Bob, tell(Susan, combination))) if Susan can ask Bob for the combination, and she knows that as a result of her asking him for the combination, he will tell it to her.
Note that in order for Susan to predict that Bob will tell her the combination, she must know that Bob in fact knows it, and that he is willing to  share the information.
The above natural language description is a succinct summary of the observations of Moore, 1980] (for primitive actions) and Morgenstern, 1988] (for complex actions and multi-agent plans).
The di culty now is in formalizing this { correctly { within a formal logic.
It is necessary to formalize prediction { knowing that an event will happen in the future { and the notion of vicarious control { controlling a plan even if you are not involved in it.
The problem addressed in this paper arises in the characterization of the knowledge preconditions for complex plans in terms of primitive plans.
We focus here on sequences of plans.
We would like to say that an agent knows how to perform a sequence of actions if he knows how to perform the rst action, and as a result of performing the rst action, he will be able to perform the second action.
Similarly, an agent knows how to execute a sequence of plans if he can execute the rst, and as a result of the rst plan's occurrence, he can execute the second.
We turn to the formalization of these principles in the next section.
4 Diculties With Linear Time  One of the simplest ways to view time is as a straight line - i.e., the standard time line of school history books.
There is a total ordering on time points or situations.
We call this representation of time \linear time."
An interval of time is a segment of the time line as mentioned in Section 2, intervals are denoted by their start and end points.
An action is a collection of intervals Occurs(act1,s1,s2) is true i (s1,s2) is an element of act1.
The knowledge preconditions for primitive actions are omitted here.
They can be found in Morgenstern, 1988].
The axiom for one simple case can be found in this paper's appendix.
We focus here on complex actions.
Recall that we would like to say that an agent knows how to perform a sequence of act1, act2 if he knows how to perform act1 and knows that as a result of performing act1, he will know how to perform act2.
A reasonable try at the knowledge preconditions axiom for action sequences might thus be: Axiom 1: (Knows-how-to-perform(a,act1,s1) & (Occurs(do(a,.act1),s1,s2) ) Knows-how-to-perform(a,act2,s))) ) Knows-how-toperform(a,`sequence(^act^ ^act2^)',s1)  Despite this axiom's plausibility, it does not say what we want.
It allows agents to know how to perform some very odd action sequences.
In particular, it entails that an agent knows how to perform a sequence of two actions if (s)he knows how to perform the rst act but does not perform this act { even if (s)he doesn't know how to perform the second act!
For example, consider the agent Nancy Kerrigan, the  Figure 1: McDermott's branching time.
Real chronicle in bold  action sequence (ice skate, build atom bomb) , and the situation S1 representing January 7, 1994.
It is clear that on January 7, Nancy Kerrigan knew how to ice skate.
We know, however, that due to injuries, she did not skate on that day.
Then the statement  Knows-how-to-perform(Kerrigan,`sequence(ice skate, build atom bomb)', S1 is true, since the second con-  junct of the left-hand side of the axiom is vacuously true.
The problem, when we examine this anomaly more closely, seems to be that material implication is being used to capture the notion of \as a result of performing action 1."
The truth is that material implication is quite dierent from, and much stronger than, the notion of result.
This is the reason it is so much more di cult to modify Axiom 1 than one might suppose.
It is not merely that we have somehow missed something in the formalization.
The problems inherent in material implication have appeared in many suggested modications of this axiom as well, since material implication plays a central role in these axioms as well.
This problem strikes a familiar chord.
In fact, there are many types of reasoning, such as counterfactual reasoning, and concepts in temporal reasoning, such as prevention and causality, that would seem to be straightforward to implement, but which fail due to the very strong nature of material implication.
One approach to solving such problems has been to examine these concepts within the framework of a richer ontology.
Often, the ontology chosen has been branching time McDermott, 1982].
We examine the knowledge precondition problems in the context of branching time in the next section.
5 Diculties With Branching Time  In branching time, time points are ordered by a partial order as opposed to a total order.
There is a unique least point, and one cannot have s1 s2 and s3 s2 unless either s1 s3 or s3 s1 (that is, every child has at most one parent).
Thus, while one could visualize linear time as a straight line, the best way to visualize branching time is as a sideways tree (See Figure 1).
Conceptually, the branch points correspond to action choice points each branch represents a dierent action performed.
Following Mc<  <  <  <  Dermott 1982], any linearly ordered set of points (or path), beginning with the least point, and without gaps, is called a chronicle.
There ia s one chronicle that is designated as the \real chronicle" this corresponds to the way the world is.
A time point is called real if it lies on the real chronicle.
An interval is called real if it contains only real time points.
We introduce the predicate Real-occurs:  Definition:  Real-occurs(act,s1,s2) Occurs(act1,s1,s2) & Real( (s1, s2) ).
,  Since we used linear time in the last section, the  Occurs predicate used there corresponds to the Realoccurs predicate of this section.
Axiom 1 is now cor-  rect.
The left-hand conjunct is not vacuously true in the Nancy Kerrigan example, above the axiom now says: if Nancy Kerrigan knew how to ice skate on January 7, and in any possible world resulting from her skating on January 7, she knew how to build an atom bomb, then she knows how to perform the sequence of actions.
In fact, it is safe that assume that in no possible world resulting from Nancy Kerrigan's skating did she know how to build an atom bomb thus she does not know how to perform the sequence of actions.
This is just what we would anticipate.
Indeed, the fact that the axiom now works is to be expected Moore 1980] used branching time (his temporal ontology was a variation of the situation calculus) and was able to correctly formalize knowledge preconditions for action sequences.
The problem now is that branching time cannot be used for formalizing knowledge preconditions for plans.
The reason, briey, is that in order for an agent to reason that he can execute a multiagent plan, the agent must be able to predict that other agents will perform certain actions.
Predicting means knowing that an event will actually occur - i.e., that the occurrence will be part of the real chronicle.
But suppose, now, that an agent, Susan, is reasoning about her ability to execute sequence(pln1, pln2).
E.g., assume that Susan is reasoning about her ability to execute the plan sequence(Do(Susan,ask(Bob,combination)),Do(Bob,tell (Susan,combination))).
We assume that pln1 is a  single action where Susan is the performing agent pln2 is a single action where Bob is the performing agent.
Then Susan must know that she can perform pln1 1 and that as a result of performing pln1, Bob will perform pln2.
That is, she must know that in any possible world resulting from the event Do(Susan,ask(Bob,combination)), Bob will perform Do(Bob,tell(Susan,combination)).
But this is impossible by nature of the denitions: Bob can only really perform pln2 in the one real chronicle, not in every branch in which Susan performs pln1.
Moreover if In order to reason about plan execution, one must reason not only about knowledge preconditions, but also physical and social feasibility.
When all three are satisfied, an agent can-perform an action.
See Appendix.
1  Figure 2: Branching time doesn't support hypothetical  reasoning: Bob doesn't \really" tell Susan the number when Susan asks for it (non-bold segments)  Susan doesn't perform pln1, then Bob's performance of pln2 will only occur in non-real chronicles!
This situation is shown in Figure 2.
Thus, we are now in a situation that is precisely the opposite of the situation that occurred in linear time.
The theory based on linear time is too liberal it entails that agents know how to perform sequences of two actions even if they do not know how to perform the second action.
The theory based on branching time, on the other hand, is too restrictive.
It is virtually impossible to prove, under reasonable assumptions, that an agent can execute a standard sequence of plans, such as asking a friend for a piece of information, and receiving that information.
More formally, consider the following axioms:  Axiom 2:  Can-execute-plan(a,`sequence( ^pln1^,^pln2^)',s1) , Know(a,`Vicarious-control(@a, #pln1#,@s1)',s1) & Know(a,`Occurs(^pln1^,@s1,s2))Vicariouscontrol(@a,# pln2 #,s2)',s1)  Axiom 3:  actors(pln) = f a g & Can-perform(a,`action(@a, ^pln ^ )',s) ) Vicarious-control(a,pln,s)  Axiom 4:  actors(pln) 6= f a g & 9 s2 Real-occurs(.pln,s,s2) ) Vicarious-control(a,pln,s) .
Vicarious-control, in the axioms above, can be  thought of meaning \one of the following: I can do it or it will happen."
That is, one vicariously controls a plan if one can count on it happening.
I can count on my xing myself a scrambled egg in the morning because I know how to perform the action thus, by Axiom 3, I vicariously control it.
I can count on the sun rising this morning because I can predict that it will happen thus, by Axiom 4, I vicariously control it.
Axiom 2 states that I can count on a sequence of  plans if I can count on the rst plan, and as a result of the rst plan's occurrence, I can count on the second.
Now consider the plan sequence  sequence(do(Susan,ask(Bob,comb)), do(Bob, tell(Susan,comb))).
It can easily be seen that under most normal sets of assumptions, Can-executeplan(the above plan) cannot be proven using Axioms  1 through 4.
This is just one anomalous case.
Similar problems occur with conditional plans, and in cases where agents are not directly involved in any aspect of their plan { i.e., when the entire plan consists of actions that have been delegated.
The problem arises whenever one must predict that an action will take place if a piece of a plan has occurred.
6 A Solution That Works: Branching Time With Relativized Real States  Thus far, we have demonstrated that linear time is di cult to use to formalize knowledge preconditions because it does not allow for generalized hypothetical reasoning that branching time is likewise di cult because it emphasizes hypotheticals too strongly and does not allow for generalized prediction.
What we want is a theory that supports both hypothetical reasoning and prediction.
2 That is, we would like to develop a theory in which we can say: given that act1 has occurred, act2 will surely occur.
This \sureness" or \realness" is relative to the action that has occurred.
We call this relativized branching time.
To capture this concept, we modify the ontology of branching time as follows.
We introduce a collec(to be read as \more real tion of partial orders than" ) on branch segments of our tree.
There is a partial order at each branching point  is <r  <ri  i  <r  2 Other, less satisfactory approaches are possible.
We could use linear time, but introduce an explicit predicate Causes and thus eliminate the problems of material implication.
Our axiom on knowledge preconditions for action sequences would then read: (Know-how-to-perform(a,seq(act1,act2),s) & Causes(act1,Know-how-to-perform(a,act2))) ) Know-how-to-perform(a,seq(act1,act2),s) .
But there are several problems with this strategy: We need to give a semantics to Causes.
If we cannot, the theory is somewhat bogus if we reduce Causes to material implication, the problems return through the back door.
Moreover, sometimes the fact that one knows how to perform an action act2 after performing an action act1 does not mean that performing act1 caused the agent to know how to perform act2.
One can imagine a situation in which I know that I will be told the combination of the safe at some point late in the day.
In the meantime, I spend my day chopping wood.
Now, it is perfectly plausible that I will know how to open the safe after I chop wood { but I would not want to say that the wood chopping caused me to know how to open the safe.
Another approach would be to develop an ontology using only \axiomatically possible worlds."
The disadvantages here would be that it would be non-intuitive and hard to modify.
the collection of for all .
Where no confusion will result, we will simply write for .
has the following properties: For each branch point i with n branch segments 1    , 9!
3 1.
1  ;1 +1    <ri  i  <r  b  bn  <ri  <r  bj  bj <r b  bj <r bj  bj <r bj  bj <r bn  (existence and uniqueness of least element under <r ) 2.
8k l = 6 j :bk <r bl :bl <r bk (Other than the least element, branches are incomparable.)
This bj is the \most real branch " at point i.
Intuitively, it is the branch most likely to occur at time i. i  is the unique minimal element in the partial order induced by .
3 Note also that condition (2) may be dropped if we wish to model a world in which there are dierent levels of preferred occurrences relative to some action.
For example, condition (2) would most likely be dropped in a theory that allowed for defeasible reasoning.
If one originally inferred that some action would happen because it was on the most preferred branch, and then had to retract that conclusion, it would be helpful to know which of the remaining branches was most likely to occur, and make new predictions based on this information.
We can use the notion of a most real branch to dene the concept of a most real path at a point s. Specically, dene a path in a tree as a sequence of branches 1 ,    where for each , 2 (1 ; 1), the endpoint of is the starting point of +1 .
Definition: ( 1,    ) is the most real path i for all 2 (1,j) is the most real branch segment relative to 's starting point.
Thus, for example, in Figure 3, the path ( 0 , 2 , 6 , 11, 13, 14) is the most real path at the point 0 because all the branch segments are the most real at their starting points.
On the other hand, the path ( 0 , 2 , 6 , 7, 10) is not most real at 0 because ( 6 , 7) is not the most real branch segment at 6 .
Let 0 be the root of a branching tree structure.
Note that the most real path at 0 corresponds precisely to McDermott's real chronicle.
Our move to a richer temporal ontology has thus lost us nothing in expressivity.
We now extend the relation to range over subtrees in the obvious way.
We thus have the following: Denition of for subtrees: Assume 1 2, where 1 has the endpoints ( 1 ) and 2 has the endpoints ( 2 ).
Let 1 be the subtree rooted at 1 and 2 be the subtree rooted at 2 .
Then 1 2. bj  <ri  b  bj  bi  i  j  bi  b  i  bi  bj  bi  bi  s  s  s  s  s  s  s  s  s  x  s  s  s  s  s  <rs  b  s  s  s  <r  <r  b  b  s s  t  s s  b  s  t  t  s  <rs t  We have imposed the condition of uniqueness for ease and simplicity of presentation but this condition is not strictly necessary.
It is likely that in complex domains with varying degrees of granularity of representation, there can be several most preferred branches.
For example, if Susan asks Bob for the combination, the branch in which he answers her orally and the branch in which he answers her in writing could both be most preferred branches.
We deal with this in the longer version of this paper.
3  Goal(a,act,s) & Can-perform(a,act,s) ) 9 s2 Real-wrt(s,s2) & Occurs(do(a,act),s,s2)  We can now formalize the concept of relativized prediction as follows:  Axiom 4'  actors(pln) 6= f a g& 9 s2 Real-wrt(s,(s,s2)) & Occurs(pln,s,s2) ) Vicarious-control(a,pln,s).
Using this axiomatization of the solution to the knowledge preconditions problem, we can build a theory of commonsense reasoning in which benchmark planning problems can be solved.
As an example, we consider the example of section 3, in which Susan plans to learn the combination of a safe by asking a cooperative agent Bob.
Consider a situation 1.
Assume that Bob in 1 knows the combination of some safe and that Susan knows this fact in 1.
Consider, further, a common set of social protocols governing agents' behavior, as discussed in Morgenstern, 1988] or Shoham, 1993].
Examples of such protocols are: that cooperative agents will accept one another's goals if possible, and that cooperative agents are constrained to tell the truth to one another.
Assume that these protocols hold for Susan and Bob in 1, that Susan and Bob are aware of these facts, and that both obey the S4 axioms of knowledge.
Then we have the following theorem: S  Figure 3: relativized branching time: at each branch-  ing point, there exists a unique preferred branch (in bold).
Note that since (so,s2) is more real than (s0,s19), the tree rooted at s2 is more real than the tree rooted at s19.
See Figure 3 for examples of these denitions.
Using this ontology, we can now introduce the concept of a state that is real relevant to some point in time.
Specically, we introduce the predicate Realwrt(s1,s2), which is given by the following metatheoretic denition: Denition: j= Real-wrt(s1,s2) i s2 is a point on where is the most real branch point originating from s1.
We extend Real-wrt to range over intervals in the obvious way.
Specically: bj  bj  Definition: Real-wrt(s1,(si,sj)) , 8 s 2 (si,sj) Real-  wrt(s1,s)  Those causal rules which have action occurrences in their consequent must now be written in terms of this predicate.
In general, where before we would have: Holds(uent,s1) ) 9 s2 Occurs(act,s1,s2)  we would now have:  Holds(uent,s1) ) 9 s2 Real-wrt(s1,s2) & Occurs(act,s1,s2)  and where before we would have: Occurs(act1,s1,s2) ) 9 s3 Occurs(act2,s2,s3)  we would now have:  Occurs(act1,s1,s2) ) 9 s3 Real-wrt(s2,s3) & Occurs(act2,s2,s3).
In the above transformation rules, the term  Holds(uent,s1) is really just syntactic sugar in fact,  in our notation, the situation is just another argument to the predicate.
Here is an example of a transformation: Where before we had Goal(a,act,s) & Can-perform(a,act,s) ) 9 s2 Occurs(do(a,act),s,s2)  we would now have:  S  S  S  Theorem: Can-execute-plan(Susan, sequence( do(Susan,ask(Bob,comb)), tell(Susan,comb)))  do(Bob,  We sketch the main points of the proof.
Axiom numbers refer to the axioms listed in the appendix.
We rst prove the following lemmas: Lemma1: If A and B are cooperative agents, then A can tell P to B i A knows P. Proof: By Axiom 5, an agent A can perform the ac-  tion of telling P to B i the knowledge preconditions, the physical preconditions, and the social protocols are all satised.
We assume for simplicity that the physical preconditions are satised (Axiom 6).
Moreover, all agents always know how to perform the simple act of uttering a string.
(Axioms 7 and 8).
It remains to satisfy the social protocol.
By Axiom 9, the social protocols are satised i agent A tells the truth { i.e., if he knows P. Thus, if A knows P, the social protocols are satised, and since the knowledge and physical preconditions are satised, he can tell P to B. Conversely, if he can tell P to B, the social protocols must be satised, and thus he must know P. 2 Lemma2: Assume A and B are cooperative agents.
If A asks B to do Act1, and B can do Act1, then B will subsequently perform Act1  Formally,  Cooperative(a,b,s1) & Occurs(do(a,ask(b,act1)),s1,s2) ) 9s3 Real-wrt(s2,s3) & Occurs(do(b,.act1),s2,s3)  Proof: Axiom 10 tells us that cooperative agents adopt one another's goals.
That is, if A asks B, during some interval (s1,s2) to do some act, it is then B's goal in s2 to do this action.
Moreover, we have from Axiom 11 that if an agent has a goal of performing a certain action, and he can perform that action, he will subsequently perform the action.
2 Note that Axiom 11 explicitly uses the concept of relativized realness.
Neither a stronger nor a weaker concept will su ce.
If Axiom 11 had read: Goal(a,act,s) & Can-perform(a,act,s) ) 9 s2 Real(s2) & Occurs(do(a,.act),s,s2) then it would be false.
On the other hand, if Axiom 11 had read: Goal(a,act,s) & Can-perform(a,act,s) ) 9 s2 Occurs(do(a,.act),s,s2)  it would not be strong enough to prove Lemma 2.
Indeed the proof of Lemma 2 depends on the ontology developed here.
It seems unlikely that it could be proven in a standard McDermott-type branching logic.
The proof of the theorem then goes as follows: By protocol (Axiom 14), agents can ask other cooperative agents for information.
Moreover, the physical preconditions and knowledge preconditions are satised (Axioms 12 and 13).
Thus, Susan can perform the rst part of her plan.
Thus, Susan can vicariously control the rst part of her plan (Axiom 3).
We must now show that if she performs this part, Bob will perform the second part.
First we must show that Bob can perform the action of telling Susan the combination.
By assumption, Bob knows the combination in S1.
Moreover, agents do not forget (Axiom 15).
Thus, Bob knows the combination in any situation subsequent to S1.
Therefore, by Lemma 1, he can perform the action of telling Susan the combination in S1.
Now, using Lemma 2, we can show that if Susan asks Bob the combination, he will subsequently tell it to her.
This means that Susan vicariously controls the second part of the plan (Axiom 4') by Axiom 2, Susan can execute the plan consisting of the sequence (Do(Susan,ask(Bob,combination)),Do(Bob, tell(Susan,combination))).
2 Again, this proof will not hold in a branching temporal logic.
Note, however, that the theory is not too powerful.
In particular, it will not entail degenerate plans like Nancy Kerrigan's plan, above.
Thus, the theory based on relativized branching time avoids both the problems of linear time and of standard branching time.
7 Conclusion and Further Directions  In the late seventies and early eighties, many researchers (Allen, 1984], McDermott, 1982]) argued for the importance of a correct ontology of time.
The pendulum shifted somewhat subsequently, with McDermott 1984] arguing that some ontological distinc-  tions were not all that crucial.
In particular, he argued that the dierence between linear and branching time was not that great, and would probably not make much of a dierence in temporal reasoning.
We have shown that, contrary to McDermott's hopes, this distinction is crucial for theories of knowledge and planning, and that in fact, neither ontology is adequate for such theories.
Linear time does not allow hypothetical reasoning, and thus cannot properly handle knowledge preconditions for action and plan sequences.
Branching time can handle hypothetical reasoning, but it cannot handle prediction properly, especially in hypothetical reasoning contexts.
(Recently, Pinto and Reiter 93] have also noted the problems of using standard branching time.)
Thus, those who ignore the issue of ontology do so at their own peril: all researchers who have used standard ontologies for reasoning about knowledge and planning have developed theories that are inadequate in some respect.
We have developed a dierent ontology for time, relativized branching time, which allows for relativized realness.
This allows prediction in hypothetical contexts, and thus allows the proper axiomatization of knowledge preconditions.
The resultant theory can handle standard benchmark problems correctly, while avoiding the anomalies of previous theories.
Relativized branching time appears promising for other research areas as well.
Because it supports certain types of hypothetical reasoning, it may be a suitable ontology for counterfactual reasoning.
In particular, relativized branching time may help give structure to the rather vague concept of \most similar possible worlds" which has been used (see, e.g.
Lewis, 1963]), to explain the semantics of counterfactuals such as \If I had struck a match (at S1), it would have burst into ames."
In our ontology, such a sentence can be analyzed as follows: it is true if given a (typically non-real) branch segment (S1,S2) during which the match is struck, it is true on the most real branch segment of S2 that the match burst into ames.
Most similar can be understood as the most real subtree of the endpoint of a non-real branch.
Such an analysis is very preliminary but suggests promising directions for future research.
8 Acknowledgements:  The author thanks the anonymous reviewers for comments on an earlier draft of this paper.
9 Bibliography  Allen, 1984] Allen, James: Toward a General Theory of Action and Time, Articial Intelligence, vol.
23, no.
2, 1984, pp.
123-154 Davis, 1984] Davis, Ernest: Representations of Commonsense Knowledge, Morgan Kaufmann, Los Altos, 1990  Konolige, 1982] Konolige, Kurt: A First Order Formalizationof Knowledge and Action for a Multi-agent Planning System, J.E.
Hays and D. Michie, eds.
Machine Intelligence 10, 1982 Lewis, 1963] Lewis, David: Counterfactuals, Oxford, 1963 McDermott, 1984] McDermott, Drew: The Proper Ontology for Time, unpublished, 1984 McDermott, 1982] McDermott, Drew: \A Temporal Logic for Reasoning About Processes and Plans," Cognitive Science, 1982 Montague, 1963] Montague, Richard: Syntactical Treatments of Modality with Corollaries on Reexion Principles and Finite Axiomatizability, in Acta Philosophica Fennica, fasc.
16, pp.
153-167, 1963 Moore, 1980] Reasoning About Knowledge and Action, SRI TR 191, 1980 Morgenstern, 1988] Morgenstern, Leora: Foundations of a Logic of Knowledge, Action, and Communication, NYU Ph.D. Thesis, Courant Institute of  Mathematical Sciences, 1988 Pinto and Reiter, 1993] Pinto, Javier and Raymond Reiter: Adding a Time Line to the Situation Calculus Shoham, 1993] Shoham, Yoav: Agent-Oriented Programming, Articial Intelligence, 1993  10 Appendix:  Below, a list of the axioms and denitions used in the proofs of the lemmas and main theorem of Section 6.
All variables are assumed to be universally quantied unless otherwise noted.
Axioms 1 through 4' are taken from sections 4 through 6 of this paper.
Axiom 1: (Knows-how-to-perform(a,act1,s1) & (Occurs(do(a,.act1),s1,s2) ) Knows-how-to-perform(a,act2,s))) ) Knows-how-toperform(a,`sequence(^act^ ^act2^)',s1) Axiom 2: Can-execute-plan(a,`sequence( ^pln1^,^pln2^)',s1) , Know(a,`Vicarious-control(@a, #pln1#,@s1)',s1) & Know(a,`Occurs(^pln1^,@s1,s2))Vicariouscontrol(@a,# pln2 #,s2)',s1) Axiom 3: actors(pln) = f a g & Can-perform(a,`action(@a,^pln ^ )',s) ) Vicarious-control(a,pln,s) Axiom 4': actors(pln) = 6 f a g & 9 s2 Real-wrt(s,s2) & occurs(pln,s,s2) ) Vicarious-control(a,pln,s) .
Axiom 5: Can-perform(a,act,s) , Know-how-to-perform(a,act,s) & Physsat(a,act,s) & Socialsat(a,act,s)  An agent can perform an action if the knowledge preconditions, the physical preconditions, and the social  protocols are all satised.
Axiom 6: Physsat(a,`tell(@b,#p#)',s)  For the sake of this paper, it is assumed that there are no physical preconditions for communicative actions.
In reality, there are a variety of preconditions, including being at the same place as the hearer (or being connected in some way).
Axiom 7: Primitive-act(`tell')  The simple locutionary action of just uttering a string is considered to be primitive, with correspondingly simpler knowledge preconditions.
Axiom 8: Primitive-act(f) ) Know-how-to-perform(a,^f^(^x1,  ,^xn)',s) where all of x1    xn are constants.
An agent knows how to perform any primitive action if all the arguments are constant.
Axiom 9: Cooperative(a,b,s) ) Socialsat(a,`tell(@b,#p#)',s) , Know(a,p,s)  Cooperative agents are constrained to tell the truth.
Axiom 10: Cooperative(a,b,s1) Occurs(do(a,ask(b,info)),s1,s2) ) Goal(b,tell(a,info),s2)  ^  If one agents asks a cooperative agent for information, the second agent will subsequently have the goal of giving over the information.
The above axiom has quite a bit of syntactic sugar in it.
The term \info" is shorthand for what is really going on: Agent a is asking agent b to tell him a string of the form: `Equal(term,p)', where p is a constant.
Agent b adopts the goal of telling him a string of that form.
In Morgenstern 1988], this axiom is presented without any syntactic sugar.
Axiom 11: Goal(a,act,s) & Can-perform(a,act,s) ) 9 s2 Real-wrt(s,s2) & Occurs(do(a,.act),s,s2))  If an agent has the goal of performing an act, and can perform the act, he will perform the act.
Note the crucial use of the Real-wrt predicate.
Axiom 12: Primitive-act(`ask')  Asking is a primitive action.
Axiom 13: Physsat(a,ask(b,info),s)  The physical preconditions of asking for information are always satised.
Axiom 14: Cooperative(a,b,s) ) Socialsat(a,ask(b,info),s)  If agents are cooperating, it is always all right to ask for information.
Axiom 15: Know(a,#p#,s) ) 8 s2  s Know(a,#p#,s2)  This is the axiom of perfect memory.
Agents never forget.
Max-Count Aggregation Estimation for Moving Points* Yi Chen Peter Revesz Dept.
of Computer Science and Engineering, University of Nebraska-Lincoln, Lincoln, NE 68588, USA  Abstract Many interesting problems regarding moving objects can be reduced to the following question: Given a set S of moving points on a line and two other movings points A and B on the same line, what is the maximum number of points in S that will be simultaneously between A and B within a time interval (t 1 , t2 )?
We propose an algorithm that can estimate the answer for arbitrary A and B and any fixed S in a chosen constant time.
We show that the error rate of the estimation is related to this chosen constant and some other parameters of the input data.
Our experimental results show that high accuracy estimation can be achieved when S has a large number of points and A and B are not too close to each other.
1 Introduction Spatio-temporal databases are increasingly important in various areas including e-commerce, meteorology, telecommunications, and transportation.
In querying such spatio-temporal databases, the common aggregation operations of Count and Max occur frequently.
In addition to these, Revesz and Chen [3] recently introduced a third aggregate operator called Max-Count, which arises only in the case of moving points and has no analogue in relational databases.
The three aggregate operators mentioned can be defined as follows: Let S be a set of N moving points in a database, and let Q be a query rectangle Q.
Count: For a given time, count the number of points of S that are in Q. Max: For a given time and a value for each point of S, find the maximum of the values of the points of S that are in Q.
* This research was supported in part by USA NSF grant EIA0091530.
Authors' email:{ychen, revesz}@cse.unl.edu.
Max-Count: Find the maximum number of points of S that are simultaneously in Q within a time interval (t 1 , t2 ).
(Optional: Return also the earliest time when the maximum is reached.)
In one-dimensional space, instead of a query rectangle, we talk about a query interval, whose endpoints are called query points.
Acharya et al.
[1] gave an algorithm that can estimate the Count of the rectangles in the database that intersect a new query rectangle.
Choi and Chung [2] and Tao et al.
[4] proposed methods that can estimate the Count of the moving points in the plane that intersect a new query rectangle.
Hence while estimation in the case of Count is an old idea, its consideration in the case of Max-Count is new.
In Section 2, we give an algorithm that can estimate the Max-Count aggregate operator on spatio-temporal databases that represent a set of one-dimensional and linearly moving points.
In Section 3, we present experimental results that show that our estimation algorithm provides accurate estimation over various queries.
2 Max-Count Aggregation Estimation We first discuss the special case when the set of moving points in one dimensional space has uniform distribution of initial position (at time t = 0) and the velocity.
(This special case is corresponds to the case of one bucket of a histogram in Definition 2.2.
We generalize later this case to an arbitrary number of buckets.)
Let S be a set of N moving points in one dimensional space.
The position of a point P i [?]
S at time t can be represented by a linear function of time P i (t) = ai t + bi .
In the dual plane, this point can be represented as a static point with the coordinate (a i , bi ).
Suppose that the N points represented in the dual plane are distributed uniformly in a rectangular area R as shown in Figure 1.
Definition 2.1 The dual space of the one-dimensional moving point set is a two dimensional space in which  1 and Reasoning (TIME'04) Proceedings of the 11th International Symposium on Temporal Representation 1530-1311/04 $20.00 (c) 2004 IEEE  position R  l1  l2  A  Q1  Q2  velocity  be represented as A = A 1 - A2 .
If l1 is below l2 , then we have A = A2 - A1 .
It is also clear that A1 and A2 can be calculated in a constant time.
For example, given a time instance t, we have (i) if l1 is above the rectangular area, then A 1 = R; (ii) if l1 is below the rectangular area, then A 1 = 0; (iii) if l1 intersects the rectangular area, we have the following cases as shown in Figure 2: 1.
Only the upper-right vertex is above l 1 .
2.
Only the upper-left vertex is above l 1 .
3.
Upper-left and upper-right vertexes are above l 1 .
Figure 1.
Estimation idea assuming uniformly distributed point sets.
4.
Upper-left and lower-left vertexes are above l 1 .
5.
Upper-right and lower-right vertexes are above l 1 .
6.
Only lower-left vertex is below l 1 .
the x and y-coordinates denote the speed and the initial position, respectively, of the moving points.
Definition 2.2 The spatio-temporal histogram consists of a finite partitioning into a set of rectangular areas, called buckets, the two dimensional dual space of the one-dimensional moving point set.
Each bucket is described by its corner vertices and the total number of points in it.
Definition 2.3 Given two moving query points let Q 1 and Q2 be their duals and let lines l 1 and l2 cross them, respectively, with slopes -t as shown in Figure 1.
Then the query band is the area between the lines l 1 and l2 .
In the above definition, the slope of the lines change with the variable t. Lemma 2.1 Let Q1 and l1 be as in Definition 2.3.
Then at any time t, the moving points whose duals lie below (or above) l 1 in the dual plane are exactly those that are to the left (or right) of Q 1 is the original onedimensional line.
Lemma 2.2 Let S be a set of N moving points which are all mapped with a uniform distribution within a rectangular area R in the dual plane as shown in Figure 1.
Then the number of points in S that lie between Q 2 (t) and Q1 (t) at time t can be estimated to be N * A/R, where A is the intersection of the rectangular area R and the query band.
Hence if we can calculate the area of the intersection, we can efficiently calculate the estimated aggregation result.
Let A1 be the area in the rectangle R below l1 .
Similarly, let A2 be the area in the rectangle R below l2 .
If l1 is above l2 , then area of the intersection A can  7.
Only the lower-right vertex is below l 1 .
Lemma 2.2 and the above imply that we need a constant number of calculations to find the Count aggregate, because we need to consider only one value of t, hence the slopes of l1 and l2 are fixed.
When we pose MaxCount aggregates on moving points, the situation is more complex, because we have to consider all possible t values in the time interval (t 1 , t2 ), meaning that the slopes of l1 and l2 vary.
This looks a daunting task.
However, in the following, we show that we still need only a constant number of calculations to find Max-Count aggregates.
Lemma 2.3 Let S be a set of moving points in one dimensional space, such that in the dual plane they are uniformly distributed in a rectangular area R. Let Q 1 and Q2 be two moving points.
Given a query time interval (t1 , t2 ), we can estimate the Max-Count aggregation by a constant number of calculations.
Lemma 2.4 Let R be a rectangle and l a line.
Then, the area in R that is below (or above) l can always be represented by a function of the form A = a * t + bt + c, where a, b and c are constants.
Now we prove that the area that is the intersection of R and the query band can also be represented by a similar function of time.
Lemma 2.5 Let S be a set of moving points in one dimensional space, and let Q 1 (t) and Q2 (t) be two moving query points.
Then for any given histogram H of S, the estimated number of points that are located between the two query points at time t can be represented by a function of the form count(t) = a * t +  2 and Reasoning (TIME'04) Proceedings of the 11th International Symposium on Temporal Representation 1530-1311/04 $20.00 (c) 2004 IEEE  b +c t  l1  l1  l2  l1  l1 Q1 (A)  l1  (E)  Q2 (B)  (C)  l1 l1 (F)  (G)  (D)  l1 (H)  Figure 2.
Cases with one bucket and one line.
where a, b and c are constants and t = 0.
When t = 0, then count(t) = d where d is a constant.
query points.
Order the lines by their slopes.
Find the Time Partition Order of the time interval (t [ , t] ).
Lemma 2.6 Suppose the dual plane is partitioned into rectangular buckets.
We can calculate the Max-Count of a query band during a query time interval when the query band covers the same set of corner points of the buckets.
2.
For each time interval associated with the Time Partition Order calculate the function of time having the form a * t + bt + c, where a, b and c are constants.
Definition 2.4 Let H be a histogram.
Let Q 1 (t) and Q2 (t) be two query points.
Let (t [ , t] ) be the query time interval.
We define the Time Partition Order to be the set of time instances T P = {t1 , t2 , ..., ti , ..., tk }, such that k is a constant and t 1 = t[ and tk = t] and for each time interval [ti , ti+1 ) the set of bucket corner vertices that lie within the query band remains the same.
Note that a query band changes with t as the slope of the lines l1 and l2 changes.
For the query band to remain in one of the states shown in Figure 2 during a time interval [ti , ti+1 ), it cannot change so much that it either leaves a corner vertex or adds a new corner vertex of a bucket.
Therefore, throughout the time interval [t i , ti+1 ) the number of points within the query band can be estimated by the same function of the form a * t + bt + c, where a,b and c are constants.
All the above lemmas and observations lead to the following algorithm to estimate the Max-Count value.
Algorithm 2.1 Max-Count Algorithm Input: A histogram H, query points Q 1 (t) and Q2 (t) and a query time interval (t [ , t] ).
Output: The estimated Max-Count value.
1.
Find all bucket corner vertices in H. Find the lines between the corner vertices and the dual of the  3.
For each such function of time, calculate the maximum value within the corresponding time interval.
Store the result in a list.
4.
The maximum value in the list is the final result.
Theorem 2.1 Let H be a histogram with B number of buckets.
Let Q1 (t) and Q2 (t) be two moving query points, and let (t[ , t] ) be a time interval.
It takes O(B log B) time to calculate the estimated Max-Count value.
Example 2.1 We show in Figure 3 a histogram which contains three buckets and in which P and Q are the duals of the two moving query points.
There are a total of eight corner vertices for the buckets in the histogram, as shown in the figure.
Figure 3(t i ) shows the query band at time ti .
The query band consists of two parallel lines which have the slope -t i .
The line crossing Q also crosses G. This means that at time t i , F lies in the query band, and G enters the query band.
We sweep the query band clockwise as time increases and slope decreases.
Then we find that at a later time t i+1 , G still lies in the query band, but F is leaving the query band, as shown in Figure 3(t i+1 ).
During the time interval [t i , ti+1 ), the query band intersects with all three buckets.
Moreover, the intersection between each bucket and the query band remains in one of the states shown in Figure 2.
For example, for the intersection of the query band and bucket 1 remains  3 and Reasoning (TIME'04) Proceedings of the 11th International Symposium on Temporal Representation 1530-1311/04 $20.00 (c) 2004 IEEE  G  G  Bucket 1  Bucket 1  F  Bucket 3  F  Q  Bucket 2  Bucket 3 Q  Bucket 2  P  P  H  H  (ti )  (ti+1 )  Figure 3.
The query band at two different times.
in the case shown in Figure 2(5).
Hence, the area of the intersection between the query band and bucket 1 can be represented by the same function of time.
According to Lemmas 2.2 and 2.4, the number of points can be estimated by a function of time f 1 = a1 * t + bt1 + c1 .
Similarly, the number of points in the intersection of the query band and buckets 2 and 3 can be estimated by functions f2 = a2 * t + bt2 + c2 and f3 = a3 * t + bt3 + c3 .
Then, the total number of points during the time interval [ti , ti+1 ) can be estimated by the function of time f = f1 +f2 +f3 = (a1 +a2 +a3 )*t+ b1 +bt2 +b3 +(c1 +c2 +c3 ).
Observe that the Max-Count value during this time interval can be calculated with constant time.
Since the Time Partition Order forms O(B) number of such time intervals, it takes O(B) time to calculate the Max-Count of all such intervals and the final result.
3 Experiments  We study the impact of various parameters for the performance of the algorithm.
We systematically generate several synthetic datasets that consist of a large number of one-dimensional moving points.
Both the initial positions and the speeds of these points are distributed between 0 and 10, 000 according to the Zipf distribution.
In the Zipf distribution we assumed that the higher speed and higher displacement points were denser.
This is similar to the dataset used in [2, 4].
Therefore, in the dual plane the dataset was distributed within a rectangular area with height 10, 000 and width 10, 000 with a greater density of points in the upper and right regions of the histogram.
3.1 Experimental Parameters We consider the estimation accuracy with respect to the following parameters: Number of Buckets: We used the histogram algorithm of [1] that allowed us to specify the number of buckets as an input.
We used either 10 or 20 buckets in our experiments.
Number of Points: This is the number of points in the histogram.
Since we used the same Zipf distribution in all of our experiments, the higher number of points also mean a higher density of the points.
We varied the number of points from 8000 to 40000.
Query Range: This is the distance between the duals of the two moving query points.
We varied the query range from 400 to 2000, that is, from 2% to 20% of the width of the histogram.
Query Type: The position of the dual of the two moving query points can be either in a dense region or a sparse region of the histogram.
We used one dense and one sparse query in the experiments.
Originally we did not consider the query type as a parameter.
However, we added it when we realized that it is actually an important parameter.
Presenting only an average running time of a set of different queries would actually hide an interesting and non-obvious relationship.
3.2 Dense Queries In our first set of experiments we considered queries where the location of the duals of the moving query  4 and Reasoning (TIME'04) Proceedings of the 11th International Symposium on Temporal Representation 1530-1311/04 $20.00 (c) 2004 IEEE  points was in a dense region of the histogram.
We call these cases dense queries.
Error Rate  60  3.2.1 10 Buckets  50  We fixed the number of buckets to be 10 and varied the number of points and the query range.
40 30 20 10 0  Error Rate  5000 10000 15000 20000 Number of points25000 30000 35000 40000 400  60 50 40 30  600  800  1000  1200  1400  1600  1800  2000  Range  20 10 0  Figure 5.
Performance measures for a dense query and 20 buckets.
5000 10000 15000 20000 Number of points25000 30000 35000 40000 400  600  800  1000  1200  1400  1600  1800  2000  Range  Figure 4.
Performance measures for a dense query and 10 buckets.
Figure 4 shows that the error rate (the absolute value of the difference between the estimated and the actual values divided by the actual value) decreases exponentially as the number of points increase.
The error rate also decreases slightly as the query range increases.
Discussion: These findings were as expected.
Obviously, as the number of points increases, the points are more clearly Zipf distributed.
With a clearer Zipf distribution in the entire plane, the bucketing algorithm can find buckets in which the points are more uniformly distributed than before, because it has to consider only the Zipf factor and less random factors.
Hence the accuracy increases.
The query range data is harder to explain.
Intuitively, in general the higher the intersection area between a bucket and the query band the less is the error rate.
When the query range is wider the intersection areas between the buckets and the query band tends to be greater, in fact, the query band may completely cover many buckets.
For those buckets that are completely covered the estimation would be accurate.
also decreases slightly as the query range increases.
This results are similar to the results in Section 3.2.1, with a slightly lower error rate here than in the previous section for most combinations of number of points and query ranges.
Discussion: Intuitively, when we allow more buckets in the histogram, the distribution in each bucket is more uniform, hence the total error rate should be lower.
However, there is no visible decrease of the error rate when the number of buckets increases from 10 to 20.
Apparently most of the extra buckets do not intersect with the query band, hence increasing the number of buckets does not significantly lower the error rate.
For dense queries, with either 10 or 20 buckets, a slight change in time could result in a large change in the estimate of the number of points in the query band.
This explains why the error rate can be high (up to 50%) in the case of a relatively few number of points but remains low in the case of a high number of points.
Apparently the estimate and the actual values change more in tandem with the higher density.
3.3 Sparse Queries By sparse queries we mean queries that are the opposite of dense queries.
In sparse queries the duals of the moving query points are located in a sparse region of the histogram.
3.2.2 20 Buckets  3.3.1 10 Buckets  Figure 5 shows that the error rate decreases exponentially as the number of points increase.
The error rate  Figure 6 shows the relationship of the error rate, number of points and query range when the number of buckets  5 and Reasoning (TIME'04) Proceedings of the 11th International Symposium on Temporal Representation 1530-1311/04 $20.00 (c) 2004 IEEE  Error Rate  Error Rate  10 9 8 7 6 5 4 3 2 1 0  10 9 8 7 6 5 4 3 2 1 0  5000 10000 15000 20000 Number of points25000 30000 35000 40000 400  600  800  1000  1200  1400  1600  1800  2000  Range  5000 10000 15000 20000 Number of points25000 30000 35000 40000 400  600  800  1000  1200  1400  1600  1800  2000  Range  Figure 6.
Performance measures for a sparse query and 10 buckets.
Figure 7.
Performance measures for a sparse query and 20 buckets.
is 10 and we have sparse queries.
The error rate is always relatively small, that is, it is always below 10%.
There is no clear relationship between the error rate and query range.
In fact, the error rate decreases about linearly when the number of points is 24,000, but it increases linearly when the number of points is 8,000 and 40,000.
Similarly, there is no clear relationship between the error rate and the number of points.
For example, the error rate goes up and down for query range 400 and down and up for query range 2, 000.
Discussion: The lack of a clear relationship between the error rate and the query rate in this case may be due simply to the fact that the error rate remains lower than 7% in most cases.
With such a relatively small error rate the ups and downs in Figure 6 cannot be statistically significant.
with 20 buckets than with 10 buckets.
For sparse queries, with either 10 or 20 buckets, a slight change in time results only in a small change in the estimate of the number of points in the query band.
This explains why the error rate is always small even when we have relatively few number of points.
Our experiments show that the query type is an important, perhaps the most important, parameter in the performance of the Max-Count estimation algorithm.
That is surprising, because it is a less obvious variable than the others.
However, even in the case of dense queries a good performance can be guaranteed if the number of points is high, the query range is not too small, and the number of buckets is 10 or higher.
3.3.2 20 Buckets Figure 7 shows that the error rate is again very small in most cases when we use sparse queries and fix the number of buckets to be 20.
The highest error rate occurs in one corner of the picture when the number of points is 8, 000 and the query range is 2, 000.
There seems to be a decrease in the error rate as we go away from that corner in any direction, either decreasing the query range or increasing the number of points.
Discussion: In many ways these results were similar to those in Section 3.3.1.
The most surprising result again was that the error rate was small, always less than 10%.
It was also noteworthy that in the case of sparse queries the average error rate seems to be slightly lower  References [1] S. Acharya, V. Poosala, and S. Ramaswamy.
Selectivity estimation in spatial databases.
In Proc.
ACM SIGMOD, pages 13-24, 1999.
[2] Y.-J.
Choi and C.-W. Chung.
Selectivity estimation for spatio-temporal queries to moving objects.
In Proc.
ACM SIGMOD, 2002.
[3] P. Revesz and Y. Chen.
Efficient aggregation on moving objects.
In TIME-ICTL, 2003.
[4] Y. Tao, J.
Sun, and D. Papadias.
Selectivity estimation for predictive spatio-temporal queries.
In ICDE, 2003.
6 and Reasoning (TIME'04) Proceedings of the 11th International Symposium on Temporal Representation 1530-1311/04 $20.00 (c) 2004 IEEE
titative temporal information 7].
Moreover, we are also applying LaTeR to model-based diagnosis of dynamic systems.
In both cases, LaTeR high-level language provide a useful interface for obtaining a loosely coupled integration, and LaTeR's e	cient treatment of queries (and updates) provides crucial advantages.
A discussion on such applications can be found in 5].
A prototype of LaTeR has been implemented in C on Sun workstations, under the UNIX operating system.
References  1] J. Allen.
Maintaining knowledge about temporal intervals.
Communications of the ACM, 26:832{ 843, 1983.
2] J. Allen.
Time and time again: the many ways to represent time.
Int.
J.
Intelligent Systems, 6(4):341{355, 1991.
3] R. Arthur and J. Stillman.
Temporal reasoning for planning and scheduling.
Technical report, AI Lab, General Elettric Research Center, 1992.
4] V. Brusoni, L. Console, B. Pernici, and P. Terenziani.
LaTeR: a general purpose manager of temporal information.
In Methodologies for Intelligent Systems 8, pages 255{264.
Lecture Notes in Computer Science 869, Springer Verlag, 1994.
5] V. Brusoni, L. Console, B. Pernici, and P. Terenziani.
Dealing with time in knowledge based systems: a loosely coupled approach.
In Proc.
FLAIRS '95, Melbourne, FL, 1995.
6] V. Brusoni, L. Console, and P. Terenziani.
On the computational complexity of querying bounds on dierences constraints.
Articial Intelligence (to appear), 1995.
7] L. Console, B. Pernici, and P. Terenziani.
Towards the development of a general temporal manager for temporal databases: a layered and modular approach.
In Proc.
of the Int.
Work.
on an Infrastructure for Temporal Databases, Arlington, Texas, 1993.
8] E. Davis.
Constraint propagation with interval labels.
Articial Intelligence, 32:281{331, 1987.
9] T. Dean and D. McDermott.
Temporal data base management.
Articial Intelligence, 32:1{ 56, 1987.
10] R. Dechter, I. Meiri, and J. Pearl.
Temporal constraint networks.
Articial Intelligence, 49:61{ 95, 1991.
11] A. Gerevini and L. Schubert.
E	cient temporal reasoning through timegraphs.
In Proc.
13th IJCAI, pages 648{654, Chambery, 1993.
12] H. Kautz and P. Ladkin.
Integrating metric and qualitative temporal reasoning.
In Proc.
AAAI 91, pages 241{246, 1991.
13] J. Koomen.
The TIMELOGIC temporal reasoning system.
Technical Report 231, Computer Science Department, University of Rochester, Rochester, NY, March 1989.
14] L. McKenzie and R. Snodgrass.
Evaluation of relational algebras incorporating the time dimension in databases.
ACM Computing Surveys, 23(4):501{543, 1991.
15] I. Meiri.
Combining qualitative and quantitative constraints in temporal reasoning.
In Proc.
AAAI 91, pages 260{267, 1991.
16] R. Snodgrass, editor.
Proc.
of the Int.
Work.
on an infrastructure for Temporal Databases.
1993.
17] A. Tansell, R. Snodgrass, J. Cliord, S. Gadia, and A. Segev.
Temporal Databases: Theory, design and implementation.
Benjamin Cummings, 1993.
18] P. VanBeek.
Approximation algorithms for temporal reasoning.
In Proc.
11th IJCAI, pages 1291{1297, 1989.
19] P. VanBeek.
Temporal query processing with indenite information.
Articial Intelligence in Medicine, 3:325{339, 1991.
20] M. Vilain.
A system for reasoning about time.
In Proc.
AAAI 82, pages 197{201, 1982.
21] M. Vilain and H. Kautz.
Constraint propagation algorithms for temporal reasoning.
In Proc.
AAAI 86, pages 377{382, 1986.
22] M. Vilain, H. Kautz, and P. VanBeek.
Constraint propagation algorithms for temporal reasoning: a revised report.
In D.S.
Weld and J. de Kleer, editors, Readings in Qualitative Reasoning about physical systems, pages 373{381.
Morgan Kaufmann, 1989.
23] Ed Yampratoom and J. Allen.
Performance of temporal reasoning systems.
SIGART Bulletin, pages 26{29, 1993.  long less than 100 and more than 75% for sequences long from 100 to around 200.
A more detailed evaluation of the results can be found in 6].
5 Comparisons with Related Work  Dierent criteria can be considered in order to compare the temporal managers developed in the articial intelligence literature.
A rst important criteria concerns completeness.
In LaTeR, as in many articial intelligence approaches, we choose to retain completeness.
since it seems important to us in order to provide users and applications with uncontestable and reliable results.
This rises a trade-o between expressive power and computational complexity of complete temporal reasoning.
As e.g.
in Timegraph 11] and in Tachyon 3] we chose to limit the expressive power in order to retain tractability.
In particular, the expressive power of LaTeR is comparable to that of Tachyon, which deals with temporal constraints that can be mapped onto conjunctions of bounds on differences, too.
In 23], Allen distinguishes between two dierent class of temporal managers: (i) managers that use a constraint satisfaction technique at assertion time, building an all-to-all graph with the constraints between each pair of temporal entities in the knowledge base (ii) managers that build partial graph structures, which need further processing at query time.
For instance, Allen classied TimeLogic 13], MATS 12] and Tachyon 3] as systems of the rst type, and Timegraph 11] and TMM 9] as systems of the second type.
In 23], Allen, considering only atomic queries (i.e., queries for extracting the constraints between two entities in the graph, or yes/no queries without conjunction) pointed out that the approaches computing the all-to-all graph are more e	cient than those computing only partial graphs when dealing with queries.
In fact, in these approaches, queries can be answered in constant time, by reading the values from the graph, while in the approaches in (ii) some further reasoning may be needed.
On the other hand, the approaches in (i) are less e	cient when dealing with assertions (updates), since the whole all-to-all graph has to be computed after each assertion.
LaTeR is a system computing the all-to-all graph (which is the minimal network in the case of LaTeR) that reconciles the advantages of both types of approaches, thanks to its e	cient treatment of complex queries and of assertions as hypothetical queries.
This  result has been obtained via the treatment of complex types of queries.
As shown in van Beek's work 19], as soon as one considers non-atomic queries (even only conjunctions of yes/no queries), two problems arise: on the one hand, the distinction between queries about necessity and queries about consistency is needed on the other hand, constraint propagation may be required.
Van Beek's work has two major limitations with respect to the work in this paper: (i) it deals with qualitative information only and (ii) it performs constraint propagation on the whole network (global propagation) both for queries about necessity and queries about consistency (notice, however, that Van Beek allows the use of all logical connectives in the query language, although answering queries becomes exponential).
On the other hand, we showed that propagation is needed only for queries about consistency (and hypothetical queries, which are not considered in 19]) and, even in such a case, local propagation is su	cient.
Thus, LaTeR retains the e	cient query processing typical of approaches computing the all-to-all graph also in case complex queries.
Furthermore, since in LaTeR assertions followed by queries can be simulated by hypothetical queries (which are answered by local temporal reasoning), LaTeR does not have to recompute the whole all-to-all graph at each assertion, so that also assertions are managed e	ciently.
Besides providing the computational advantages above, LaTeR treatment of dierent (and complex) types of queries seems to us a main feature of the system in itself, since queries (and assertions) constitute the main way of interacting with temporal managers.
Thus, we believe that the expressive query language (and manipulation language) constitutes an advantage of LaTeR with respect to the other systems in the literature.
For instance, high-level interface languages are widely used in the temporal databases community.
However, most of the approaches to temporal databases only deal with time stamps associated with information and do not consider temporal relations between entities (see, e.g., 14]), so that temporal constraint propagation is not needed.
6 Conclusions  In the paper we showed how queries on an heterogeneous temporal knowledge base can be answered e	ciently, independently of the dimension of the knowledge base Currently, LaTeR is being loosely coupled with Oracle, in order to extend relational databases to deal also with (possibly imprecise) qualitative and quan-  the consistency of the knowledge base and thus consistency must be checked after each update and before answering the queries following the update itself.
Since answering queries in an inconsistent knowledge base is meaningless, the consistency check must be performed anyway.
Moreover, the minimal network of the updated knowledge base can be produced by the same algorithms that check consistency.
This means that the presence of updates does not aect the e	ciency of our approach: consistency has to be checked anyway but this produces the minimal network and queries can be answered e	ciently given the minimal network (see the previous section).
Our approach, on the other hand, suggests an e	cient way for dealing with a class of updates, specifically updates that add new constraints (which are the most common in many applications, see the discussion in 5]).
In fact, in such a case one can answer the queries following an update as hypothetical ones.
More specically, a query Q following an update U can be answered as the hypothetical query: Q if U which only involves local propagation.
If a query Q follows a sequence of updates U1  : : : Uh , this can be simulated as the query Q if U1  : : :Uh .
The advantage of such an approach is that during a session of interleaved queries and updates all the operations can be performed with local propagation and the actual update of the knowledge base (which can be very costly) can be delayed with respect to the query process (e.g., performed once and o-line at the end of the session).
Dealing with updates as hypothetical queries can provide signicant computational advantages.
However, when the sequence of updates and queries becomes very long and the updates involve signicant parts of the knowledge base, such advantages may be lost.
A detailed evaluation of the such computational advantages and trade-os can be found in 6] where we compare: the case where the minimal network is recomputed after each update (and then queries are answered with local propagation as discussed in the previous section) the case where queries are dealt with as hypothetical ones.
The evaluation is performed by taking into account three dierent parameters: The length of the sequences (\k" in (7)) The average dimension of updates/queries (we assume that updates and queries have the same  average dimension), i.e., the average ratio between the dimension of queries/updates and the dimension of the knowledge base How extensive the updates are, that is: how many entities involved in the i ; th update were not involved in the previous ones.
At one extreme, all the updates may involve the same set of variables (i.e., the same part of the knowledge base is repeatedly changed) at the other extreme, each update may involve a part of the knowledge base that was not involved by any previous update and thus the updates in the sequence tend to involve larger and larger parts of the knowledge base as the length of the sequence increases (in general, both extreme cases are unlikely taking this as a parameter allows us to consider all possibilities).
Two dierent evaluations are then performed: First of all we evaluated the break-even point between the two approaches that is: the maximum length of the sequence for which dealing with updates as hypothetical queries provides advantages, given the average dimension and extension of the updates or, conversely, which is the maximum dimension for the updates for which there are advantages, given the length of the sequence.
For example, it turned out that for a sequence of 40 updates and queries in which one half of the variables involved in each update was not involved by previous ones (so that the updates tend to extend to signicant parts of the knowledge base), dealing with updates as hypothetical queries provides advantages when the average dimension of each update/query is less that 7% of the knowledge base.
Conversely, when the average dimension of each query/update is 1% of the knowledge base, the approach is advantageous when the length of the sequence is less than 320.
From our experience in the practical application of LaTeR (see 5]), these dimensions are realistic in the sense that it is common that the dimension of updates/queries is around 1% of the dimension of the knowledge base and in any case never more than 5%.
We evaluated how big the computational advantage is.
For example, when the average dimension of update/queries is 1% of the knowledge base (and one half of the variables involved in each update were not involved in previous ones), the advantage is around 90% if the sequence is  Each one of the constraints in (6), taken in isolation, is consistent with (5), but the conjunction in (6) is inconsistent with (5).
Thus constraint propagation is needed in order to check whether a set of constraints is consistent with a given knowledge base.
However, we proved that global propagation of the constraints in the query to the whole knowledge base is not needed for computing the answer.
In fact, since the minimal network is available and since the goal is not to update the whole knowledge base but just to answer the query, local propagation is su	cient (local propagation concerns only the variables in the query).
More formally, we proved the following theorem (the proof can be found in 6]):  Theorem 1 Let S be a set of variables, K:B: a set  of bounds of dierences on such variables and NS the consistent minimal network computed by the (complete) propagation algorithm.
Let us consider a query MAY (Q) on K:B: (where Q is a conjunction of atomic tests) referring to a set G  S of variables (i.e., all the constraints in the query involve only variables in G).
Let NS be the minimal network obtained by propagating the constraints in Q to NS (i.e., to all the variables - global propagation in S ) and NG the minimal network obtained by propagating the constraints in Q to NG , where NG is the restriction of NS to the variables in G (local propagation) then NS is consistent if and only if NG is consistent.
0  0  0  0  The theorem guarantees that in order to answer MAY queries of the form: MAY (C1 AND C2 : : : AND Cn) it is su	cient to propagate the constraints Ci in the query to the part of the minimal network whose nodes are the variables in the query (i.e., occurring in C1 C2 : : : Cn ).
Therefore conjunctive MAY queries can be answered in a time that is cubic in the number of variables in the query and that is independent of the dimension of the knowledge base.
3.2.3 Hypothetical Queries.
Hypothetical queries are queries of the form Q if C where Q is a query of one of the types discussed in the previous subsections and C is a conjunction of temporal constraints, expressed in LaTeR's high-level language.
For example, given the knowledge base in gure 1, the following queries could be asked:  HowLong John work If Mary work Lasting 4h 50min?
Answer : 5h MUST ( Tom work During Mary work) If Mary work Lasting 4h 50min?
Answer : Y es In principle, an hypothetical query should be answered in 3 steps: (i) adding the constraints C to the temporal knowledge base (ii) computing the minimal network N for the new knowledge base (iii) answering the query Q given N .
However, we proved the following theorem (see 6] for more details): Theorem 2 Given S, NS , G, NG , Q, NS and NG as in Theorem 1, then for each pair of variables hX Y i in G, the maximal admissibility range for X ; Y provided by NG (minimal network computed with local 0  0  0  0  0  propagation) is the same as the maximal admissibility range for X ; Y provided by NS (minimal network computed with global propagation).
0  In other words, as regards the variables in G, local propagation to the part of the minimal network concerning the variables in G produces the same results as global propagation to the whole minimal network.
This means that, for any query Q If C, it is su	cient to proceed as follows: perform local propagation of the constraints in C to the part of the minimal network involving the variables in C fi Q Answer Q as discussed in the previous subsections.
The theorem guarantees that this procedure provides the same result that would be obtained by propagating the constraints in C to the whole knowledge base before answering the query Q.
Thus, also hypothetical queries are answered in LaTeR in a time which is independent of the dimension of the knowledge base (more specically, in a time that is cubic in the number of variables in C fi Q).
4 Dealing with updates  In the practical applications of temporal reasoning queries are interleaved with updates.
In other words, a typical session with a temporal knowledge server could have the form of a sequence: U1  Q1 U2 Q2 : : : Uk  Qk (7) of alternated updates (Ui ) and queries (Qi ).
An update corresponds to the the addition or removal of some temporal assertion.
Each update may aect  MAY ( Tom work During Mary work AND start(John work) After 1620) which involves checking that the conjunction of the two assertions is consistent with the knowledge base.
Given the example in gure 1 the answer to such a query is negative.
Queries about consistency/necessity are mapped into conjunctions of atomic tests each one of which is a check on the distance between two time points (and thus the dierence between two variables in the minimal network).
The mapping is the same used for translating assertions into bounds on dierences sketched in section 2.
For example, the conjunction of atomic tests corresponding to the queries (1) and (2) above are respectively: MUST( 0 < STW ; SMW AND 0 < EMW ; ET W) MAY ( 0 < ST W ; SMW AND 0 < EMW ; ET W ) (STW and ET W are as above SMW and EMW are the starting and ending points of \Mary work").
In other words, high level queries about consistency (necessity) are answered by checking that a conjunction of bounds on dierences (atomic tests) is consistent (follows necessarily) from the constraints in the knowledge base.
Given the minimal network, atomic tests can be performed as local checks on such a network.
However, dierent checks are performed in case of MUST and MAY queries as a result the computational complexity of the cases is dierent, as it will be discussed in the two following subsections.
3.2.1 MUST queries  Let us consider a query about necessity of the form MUST(C1 AND C2 : : : AND C ), where each C is an atomic test of the form c  X ; Y  d .
We distinguish two cases:  (n = 1), i.e., the query involves only one atomic test and thus has the form: MUST(c  X ; Y  d) Let afi b] be the maximal admissibility range for the dierence X ; Y (read from the minimal network).
The query is satised i all the values for X ; Y which satisfy the constraints are in cfi d], that is: MUST (c  X ; Y  d) , cfi d]  afi b] (3) Intuitively, since the maximal admissibility range afi b] includes all the values for X ; Y satisfying the constraints, then any interval cfi d] such that cfi d]  afi b] includes all the values for n  i  i  i  i  i  X ; Y satisfying all the constraints.
A query involving one constraint can thus be answered in constant time with a simple lookup in the minimal network and a containment check.
 (n > 1), i.e., the query involves a conjunction of atomic tests and has the form MUST (C1 AND C2 : : : AND C ).
In this case each one of the C can be checked independently of the others since the following property holds: MUST(C1 AND C2 : : : AND C ) , MUST(C1 ) AND : : :AND MUST(C ) n  i  n  n  Thus a query about necessity can be answered in time linear in the number of constraints (and thus in the number of variables) in the query.
3.2.2 MAY queries  Let us consider a query about possibility, i.e., of the form MAY (C1 AND C2 : : : AND C ), where each C is an atomic test of the form c  X ;Y  d .
This case is more complex than the one of MUST queries since the MAY operator does not distribute over a conjunction.
The base case, however, is similar, in the sense that when n = 1, the answer to a query of the form: MAY (c  X ; Y  d) can be provided with a local check on the minimal network.
Let afi b] be the maximaladmissibility range for the dierence X ; Y (read from the minimal network).
The query is satised i there is (at least) a value p 2 cfi d] for X ; Y which satises all the constraints, that is: MAY (c  X ; Y  d) , cfi d] \ afi b] 6= 	 (4) Intuitively, since the maximal admissibility range afi b] includes only values for X ; Y satisfying the constraints in the knowledge base, then any interval cfi d] intersecting afi b] contains at least one value for X ; Y satisfying all the constraints.
The case where the consistency of a conjunction of constraints has to be checked is more complex since atomic tests are not independent of each other.
For instance, consider the knowledge base formed by the following constraints: f0  X ; Z  30fi 5  Z ; W  25fi 10  Y ; X  20fi 15  Y ; Z  30g (5) and the query: MAY (15  Y ; Z  20 AND 15  X ; Z  20) (6) n  i  i  i  i  i  where STW and ET W (SJW and EJW) are the variables associated with the starting and ending points of \Tom work" (\John work") respectively.
Given a knowledge base of temporal information (expressed as bounds on dierences), its consistency must be checked before answering queries (or performing updates), since query processing is not interesting in an inconsistent knowledge base.
LaTeR checks the consistency of a set of bounds on dierences constraints using the complete algorithm discussed in 10], whose complexity is O(N 3), where N is the number of variables.
This algorithm produces the minimal network of the set of constraints, i.e., a compact representation of all the solutions.
More specically, for each pair hXfi Y i of variables, the minimal network provides the maximal admissibility range afi b] for the dierence X ; Y .
In other words afi b] is the set of all and only the values for X ; Y consistent with the knowledge base.
LaTeR keeps track of such a network since, as we shall discuss in the following section, this provides interesting computational advantages during query processing.
3 Ecient Query Answering in LaTeR  At least three dierent types of high-level queries are important for querying a temporal knowledge base: queries for extracting some piece of information from the knowledge base (e.g., the duration of an event or the relation between two events), queries for checking whether a set of temporal constraints is consistent with or follows necessarily from the knowledge base and hypothetical queries.
LaTeR provides a high-level language for expressing all these types of queries.
Queries in the high-level language are then translated into the corresponding low-level queries on bounds on dierences constraints, which are answered eciently, in a time that is independent of the dimension of the knowledge base.
Let us consider the types of queries listed above one at a time.
3.1 Queries for extracting temporal information.
Dierent high-level primitives are provided: When, HowLong, Delay and Relation, which give as answer respectively (1) the temporal location of temporal entities (points or intervals), (2) the duration of time intervals, (3) the delay between two time points and (4) the temporal relations between two temporal entities.
These queries can be answered by a simple lookup in the minimal network.
For example, given the knowledge base in gure 1, the following query could be asked:  HowLong John work?
Answer : 4hfi 50min ; 5h This query can be answered by simply reading in the minimal network the maximal admissibility range of the dierence between the variables corresponding to the end and start of \John work".
As a further example, the following query could be asked: Relation Mary workfi John work Answer : start(Mary work) After start(John work) end(Mary work) non strict Before end(John work) Also in such a case the answer can be read directly from the minimal network (and is then translated in the output format above).
Notice that the answer corresponds to the following relation in Allen's interval algebra: Mary work (During OR Finishes) John work  3.2 Queries about consistency/necessity.
A second important type of query is that of Yes/No queries for asking whether a set (conjunction) of constraints is true in the given knowledge base.
Since in LaTeR temporal information may be imprecise, it is necessary to distinguish whether some conclusion must necessarily hold (i.e., it is entailed by the knowledge base) or whether it may hold (i.e., it is consistent with the knowledge base).
This distinction is similar, e.g., to the one in 19].
Therefore, modal operators must be introduced in the query language in order to distinguish between queries asking whether a set of constraints is possible (consistent) given the knowledge base or whether it follows from the knowledge base.
In LaTeR queries about necessity/consistency are expressed by prexing the MUST or MAY operator to the primitives of the high level manipulation language.
For instance, given the knowledge base in gure 1, one could ask: MUST (T om work During Mary work) (1) MAY (T om work During Mary work) (2) (1) corresponds to asking whether the relation Tom work During Mary work is entailed by the knowledge base (2) asks whether it is consistent with the knowledge base.
Given the knowledge base in gure 1, the answer to (1) is negative while the answer to (2) is positive.
Conjunction is also provided, so that one can ask for the necessity/consistency of a conjunction of temporal constraints.
For example, one could ask the following query:  the approaches that maintain the minimal network and those that perform reasoning at query time 23] discussing how our approach strongly supports the former alternative (since we deal eciently with complex queries and with a class of updates).
2 Representing time in LaTeR  LaTeR is a general purpose manager of temporal  information conceived as a \knowledge server" that can be loosely-coupled with dierent Articial Intelligence and database applications 4, 5].
We believe that a knowledge server must have a predictable behavior.
This has at least two main consequences: (i) from the inferential point of view, complete temporal reasoning must be performed (ii) from the computational point of view, reasoning must be performed in polynomial time.
Moreover, a friendly interface language for interacting with the system must be available in particular, a powerful query language must be provided and query processing must be performed very eciently.
LaTeR is a two-level architecture: the higher level provides the manipulation and query interface language (to which we shall return in the following) the lower level is based on the use of a constraint framework.
LaTeR assumes that time is linear, totally ordered, continuous and metric.
Time points are the basic entities an interval I is dened as a convex set of time points with a starting and an ending point, denoted respectively as start(I) and end(I) (with start(I) < end(I)).
The distance between time points is the basic primitive in our approach and is dened as follows: Given two time points P1 and P2, the assertion distance(P1,P2,afi b]) is true i the distance between P1 and P2 is between a and b, where afi b 2 Rfi a  b:1 The notion of distance is isomorphic to the notion of dierence between reals.
Thus, standard and well-known constraint propagation techniques (see 8] or frameworks such as tcsp and stp 10]) can be used to implement such a notion: the variables correspond to the time points and each assertion distance(P1,P2,afi b]) can be represented as a bound on the dierence between the variables X1 and X2 corresponding to P1 and P2, i.e., as a linear inequality of the form: a  X2 ; X1  b In order to achieve the goal of tractable complete reasoning we limited the expressive power to deal 1 We consider also the case where one of the extremes and b or both of them are not included, i.e.
the range is partially or completely open.
a  only with conjunctions of bounds of dierences, in which complete constraint propagation is performed in O(N 3 ) (where N is the number of variables).
The expressive power of LaTeR's lower level is thus the one of stp 10].
LaTeR provides a high-level interface language for manipulating and querying a temporal knowledge base.
Each assertion in such a language is translated (in constant time) into a set of lower-level constraints (bounds on dierences).
Given the restrictions above on the lower level, we have some restrictions on the expressive power of the interface language.
In particular, the following types of information can be expressed: precise or imprecise location of time points and intervals, precise or imprecise duration of time intervals, precise or imprecise delay between time points, qualitative relations between points, intervals or points and intervals, limiting to the continuous pointisable relations 22] (as discussed in 19] this is not too restrictive in practice since many commonly used relations are indeed continuous pointisable).
Figure 1 provides examples of assertions in LaTeR's high level language (see 4] for a denition of the language).
John work Since 1400 ; 1430 Until 1800 ; 1900 start(Mary work) 10 ; 40 min After start(John work) Mary work Lasting AtLeast 4 hfi 40 min end(Mary work) non strict Before end(John work) Tom work Since 1415 Until 1830 Tom work During John work Figure 1: A simple knowledge base.
For example, the rst assertion localizes (in an imprecise way) the interval of time corresponding to \John work" the second denes a delay between the starting point of \Mary work" and the starting point of \John work" the third denes the duration of \Mary work".
The non strict operator can be used in conjunction with the precedence (and containment) relations to express that the relation itself is not strict (in the example the meaning is that the end of \Mary work" is before or equal the end of \John work").
As an example of the translation of high-level assertions into bounds on dierences, the last assertion in gure 1 is translated into bounds on dierences as follows: (0 < STW ; SJW ) ^ (0 < EJW ; ET W)  Ecient query answering in LaTeR  V. Brusoni and L. Console and P. Terenziani Dip.
Informatica, Universitfia di Torino, Corso Svizzera 185, 10149 Torino, Italy E-mail: fbrusoni,lconsole,terenzg@di.unito.it  Abstract  In the paper we address the problem of answering queries eciently in heterogeneous temporal knowledge bases (in which qualitative and quantitative pieces of information are amalgamated).
In particular, we rst outline a powerful high-level language for querying a temporal knowledge base.
We then show that, in our language, if the minimal network computed during consistency checking is maintained, then queries can be answered eciently in time that depends only on the dimension of the query and is independent of the dimension of the knowledge base.
Finally, we discuss how our approach can deal eciently also with updates and, specically, with sequences of interleaved updates of the knowledge base and queries.
1 Introduction  A lot of attention has been paid in the Articial Intelligence community to the problem of dealing with time 2, 22].
In particular, most articial intelligence approaches focus on reasoning issues 1, 15, 18, 20, 21].
On the other hand, the problems of (i) designing a high level language for manipulating and querying temporal knowledge bases and (ii) answering (complex) queries eciently have been often disregarded.
Some of these problems have been faced in the database community 14, 16, 17] where, however, reasoning and complexity issues received only a limited attention.
The aim of this paper is to reconcile these two complementary tendencies in a general-purpose manager of temporal information: LaTeR (Layered Temporal Reasoner).
In LaTeR heterogeneous temporal information (that is, qualitative and quantitative information) is amalgamated in a principled way startThis work was partially supported by CNR under grant no.
94.01878.CT07.
ing from the notion of distance between time points.
LaTeR, moreover, provides a high-level language for manipulating temporal information the expressive power of the language has been limited in such a way that complete constraint propagation can be performed in polynomial time (section 2 sketches those aspects of LaTeR that are relevant in this paper, see 4, 5] for more details).
The paper denes a powerful query language including modal operators for asking whether a set of assertions follows necessarily from a knowledge base or it is only possibly true and supporting yes/no queries, queries for extracting temporal information and hypothetical queries.
We believe that having a powerful language for querying temporal knowledge bases is fundamental for the practical applicability of managers of temporal information.
The main goal of the paper is to propose an approach for answering queries eciently in a temporal knowledge base (section 3).
Notice that the problem is interesting only in case the knowledge base is consistent since answering queries such as those mentioned above in an inconsistent knowledge base is banal.
We show that in our language, if we maintain the propagated knowledge base (\minimal network") obtained as a result of checking consistency of the knowledge base, then the complexity of answering queries is independent of the dimension of the knowledge base and depends only on the dimension of the query, where the dimension of a knowledge base (query) corresponds to the number of temporal entities involved in the knowledge base (query).
A critical aspect when the minimal network is maintained is that of updating such a network each time the knowledge base is updated (since, in principle, the whole network has to be recomputed after every update).
In the paper we discuss how updates to the temporal knowledge base and interleaved sequences of updates and queries can be dealt with efciently in our approach (section 4).
In section 5 we compare our approach to related ones.
In particular, we consider the trade-o between
Probabilistic Temporal Interval Networks Vladimir Ryabov University of Jyvaskyla P.O.
Box 35 Jyvaskyla, FIN-40351, Finland vlad@it.jyu.fi  Abstract A Probabilistic Temporal Interval Network is a constraint satisfaction problem where the nodes are temporal intervals and the edges are uncertain interval relations.
We attach a probability to each of Allen's basic interval relations.
An uncertain relation between two temporal intervals is represented as a disjunction of Allen's probabilistic basic relations.
Using the operations of inversion, composition, and addition, defined for this probabilistic representation, we present a path consistency algorithm.
1.
Introduction A Constraint Satisfaction Problem (CSP) [9] can be represented by a finite set of variables (or nodes), their associated domains, and a set of constraints on these variables.
The domain of a variable is the set over which the variable takes its values.
Each element of the domain is called a label.
Solving a CSP consists of finding assignments of labels to variables that satisfy the given constraints.
A Probabilistic Temporal Interval (PTI) network is a special type of CSP.
A PTI network consists of a set of nodes (temporal intervals) and the edges represent the uncertain relations between them.
An uncertain relation between two intervals is a set of Allen's [1] basic relations, where a probability is attached to each basic relation.
We re-define three reasoning operations: inversion, composition, and addition.
A standard pathconsistency algorithm is modified to deal with uncertain interval relations.
Due to a lack of space, we omit our algorithm for finding consistent scenarios in PTI networks using a backtracking algorithm, heuristic methods for optimizing the algorithm's performance, and methods for computing the probability of a consistent scenario using the probabilities associated with Allen's basic relations on each edge.
A complete  Andre Trudel Jodrey School of Computer Science Acadia University Wolfville, Nova Scotia, B4P 2R6, Canada Andre.Trudel@AcadiaU.ca  version of this paper which includes omitted algorithms appears as a technical report [8].
Our path-consistency algorithm, backtracking algorithm, and ordering heuristics are partially based on van Beek and Manchak's work [3].
Our probabilistic representation is more general than their standard temporal representation.
Our paper can be viewed as an extension of the work presented in [3].
Other formalisms for handling uncertainty, such as possibility theory, have been applied to temporal representation and reasoning.
A fuzzy extension of Allen's Interval Algebra was proposed in [2].
In that paper, a possibility theory was utilized to model uncertain temporal relations by assigning a preference degree to every basic Allen's relation within an uncertain interval relation.
Three reasoning operations, namely, inverse, conjunctive combination (analogous to our operation of addition), and composition were defined for that representation.
A path-consistency algorithm for interval-based networks with fuzzy temporal constraints has been proposed, and a tractable sub-algebra has been identified.
In addition to the above, other related work is: the paper of Ladkin and Reinefeld [5] describes algebraic methods for interval constraint problems; a theoretical evaluation of selected backtracking algorithm was presented in [4]; a description of backtracking algorithms for disjunctions of temporal constraints in [10], and analysis of symbolic and numeric constraint satisfaction techniques for temporal reasoning [6].
In Section 2 we define uncertain interval relations and present reasoning operations.
In Section 3 we define PTI networks, give an example of such a network, and describe the types of input that are accepted by the algorithms presented in this paper.
In Section 4 we define a path consistency algorithm.
Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIME'04) 1530-1311/04 $20.00 (c) 2004 IEEE  2.
Uncertain Interval Relations In this section, we define uncertain interval relations and the reasoning operations inversion, composition, and addition.
We denote temporal intervals with capital non-bold letters, i.e.
A, B.
The relation between two intervals is denoted with a subscripted capital letter R. For example, the relation between intervals A and B is written as RA,B.
There are thirteen basic mutually exclusive relations [1] that can hold between two temporal intervals.
The set of these relations is denoted as X={eq, b, bi, d, di, o, oi, m, mi, s, si, f, fi}.
We refer to an element of this set as kh[?]X.
An uncertain relation between two temporal intervals is represented as a set of probabilities of all the basic relations that can hold between them.
The probability of a basic temporal relation between two intervals is further denoted using the letter "e" with a superscript indicating the basic relation and possibly a subscript indicating the intervals, e.g., eAeq, B .
The uncertain relation between intervals A and B is written as RA,B={ekh"kh[?]X}.
The set RA,B has a cardinality of 13, one entry for each of Allen's basic temporal relations.
The probabilities in RA,B sum to 1.
For example, RA,B={eeq=0.5, eb=0.2,ebi=0.3} means that the relationship between intervals A and B is "eq [?]
b [?]
bi" and, "eq" is the sole relationship between intervals A and B with probability 0.5.
Similarly for "b" with probability 0.2 and "bi" with 0.3.
Note that in this and all subsequent examples, zero entries are omitted.
For example, "m" has a probability of 0 of being the relationship between A and B.
The operation of inversion (~) derives the relation ~ RB,A when the relation RA,B is defined, and RB,A = RA , B .
Given the probability values eAkh , B , the probability values eBkh, A are calculated according to the inversion table for Allen's interval relations [1], i.e.
eBoi, A = eAo , B .
For example, the inverted relation for RA,B={eeq=0.05, eb=0.2, ebi=0.1, ed=0.35, edi=0.01, eo=0.2, eoi=0.09}is RB,A={eeq=0.05, eb=0.1, ebi=0.2, ed=0.01, edi=0.35, eo=0.09, eoi=0.2}.
The operation of composition ([?])
derives the relation RA,C, when the relations RA,B and RB,C are defined, and RA,C=RA,B[?]RB,C.
We assume that the probability values  eA,kh B and eB,kh C , where kh[?
]X, are  known.
The probability values  kh  eA ,C are calculated  according to the algorithm for composition (Figure 1) presented in [7].
1. e khA,C =0, where kh[?
]X; 2. for i=1 to 13 do 3. for j=1 to 13 do 4. begin 5.
X'={kh1,kh1,...,khm}, where X' is a set of all Allen's relations which are possible between A and C when e khAi,B and kh  e B,j C are combined; 6.  for k=1 to m do  7.  e khA,kC = e khA,kC +  1 khi khj e A, B e B,C m  //khk[?
]X'; khi,khj [?
]X; 8.  end.
Figure 1.
Composition algorithm [7]  The algorithm in Figure 1 considers all possible combinations of the probability values from RA,B and RB,C.
For example, the result of the standard nonprobabilistic composition of "b" and "d" is {b,d,o,m,s}.
We need to distribute the probability eAb , BeBd ,C between the values from the set {b,d,o,m,s}.
For example, the composition of the two uncertain relations RA,B={eeq=0.3,eb=0.7} and RB,C={ed=0.5,eo=0.5} b d results in RA,C={e =0.42, e =0.22, eo=0.22, em=0.07, es=0.07}.
The operation of addition ([?])
combines the relations RA' ,B and RA'' ,B into a single relation RA,B.
In this case, we write RA,B =  RA' ,B [?]
RA'' ,B .
We use the  algorithm from [7] for performing addition (Figure 2).
For example, the addition of two uncertain relations RA' ,B ={eeq=0.3,eb=0.5,eo=0.2} and RA'' ,B ={eeq=0.05, ed=0.2, eo=0.75} is RA,B={eeq=0.214, eo=0.786}.
1. e khA, B =0, where kh[?
]X; 2. for i=1 to 13 do 3.  e  khi A, B  e'Akh,iB e'A' kh, Bi , //where e'A,khiB from = khi khi e'A, B + e'A' , B // R 'A ,B and e'A' kh, Bi from R 'A' ,B ;  4. for i=1 to 13 do 5.  e  khi A ,B  e khAi ,B = .
| e khA,B kh[?
]X  Figure 2.
Addition algorithm [7]  Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIME'04) 1530-1311/04 $20.00 (c) 2004 IEEE  3.
PTI Networks A PTI network N is a directed graph where the nodes represent intervals and the arcs represent the uncertain temporal relations between these intervals.
We represent such a graph as a set of n variables (intervals) V={v1,v2,...,vn} and the relations between them as R v , v ={ekh"kh[?
]X}, where vi,vj[?]V.
The set of all i  j  uncertain temporal relations for the network N is denoted as Ps.
For example, the PTI network N shown in Figure 3 has 4 intervals A, B, C, D, and 5 uncertain relations between them RA,B={eeq=0.3, eb=0.7}, RB,C={eb=0.5, ebi=0.5}, RA,C={eb=1}, RB,D={eb=1}, and RC,D= {eb=0.2, ebi=0.8}.
Note that there is no edge between A and D. In this case, we assume it is a totally uncertain relation with all possible entries having equal probability values.
In later sections, we define a path consistency and a backtracking algorithm for PTI networks.
The algorithms accept three types of input: A PTI network: The interval relations within this network include probabilistic values for Allen's relations (e.g., the network shown in Figure 3).
{eeq=0.3,eb=0.7}  B  {eb=0.5,ebi=0.5}  {eb=1}  A  C  {eb=1}  {eb=0.2,ebi=0.8}  D Figure 3.
The PTI network N A standard qualitative temporal CSP: In this case, the network does not include probability values and needs to be converted to a PTI network.
We assume that the Allen relations contained in the label on an edge are equally likely.
For example, if we have the label {eq,b} on an edge, we convert this to a PTI with {eeq=0.5,eb=0.5}.
In general, each of the n entries in a label are assigned the probability 1/n.
A qualitative temporal CSP with preferences: Assume we have a standard qualitative temporal CSP.
In addition, we are given relation preferences for each edge.
For example, if we have the label {b,m,o} and we also know that b is preferred over m and o.
There is no preference between m and o.
The PTI network label becomes {eb=2/(2+1+1), em=eo=1/(2+1+1)}, i.e.
{eb=0.5, em=0.25, eo=0.25}.
In general, we create a  partial order and rank each element in the order.
The smallest ranked element is assigned a rank of 1.
The probability assigned to an element is its rank over the sum of the ranks.
4.
Path Consistency Algorithm The path consistency algorithm (PC-algorithm) can be used to test an Interval Algebra (IA) network for consistency as was proposed by Allen [1], as well as a part of the backtracking search algorithm ([5] and [3]).
In this section we present a PC-algorithm (shown in Figure 4) adapted to PTI networks.
Note that our PCalgorithm is almost identical to the one in [3].
We use probabilistic versions of inversion, addition, composition, and the test conditions in lines 7 and 14 in Figure 4.
As the name implies, the PC-algorithm repeatedly checks for 3-consistency for every possible three nodes i, j, and k. The values of the uncertain relations Ri,j and Rj,k potentially constrain the value of the relation Ri,k.
Using the operations of composition and addition we compute a possible value for the relation Ri,k using the triangle t=Ri,k[?](Ri,j[?]Rjk).
Analogous to standard qualitative temporal CSPs, Ri,k and t have the following properties: If ekh is zero in Ri,k then the same entry is also zero in t. If ekh is non-zero in t then the same entry is also non-zero in Ri,k.
From the above, t has the same number or fewer zero entries as Ri,k.
Also, all non-zero entries in t are also non-zero in Ri,k.
If the derived relation t is more certain than the initial value of Rik, we update Rik with t. The derived relation is more certain than the initial one if: It has more zero entries (probability values for the basic relations).
Or, the relations are not equal and have the same number of zero entries, but the initial relation is lexicographically smaller than the derived one, when the entries are ordered in descending order.
To illustrate the latter case, let us consider an example: Ri,k = {eeq=0.2,eb=0.5,em=0.3} and t = {eeq=0.25, eb=0.25, em=0.5}.
Ordering the entries in descending order we obtain Ri,k={eb=0.5,em=0.3,eeq=0.2} and t={em=0.5, eeq=0.25, eb=0.25}.
The maximum entries are equal to 0.5; therefore we need to compare the next ones.
The second entry of 0.3 for Ri,k is bigger than 0.25 for t, so we conclude that the relation Ri,k is more certain than t. In this case, Ri,k would not be updated.
Let us underline, that such a lexicographical comparison is utilized only when the two relations are not equal and have the same number of zero entries.
The described procedure is also performed to tighten the relation Rj,k in a similar way.
The motivation for the  Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIME'04) 1530-1311/04 $20.00 (c) 2004 IEEE  lexicographic comparison is to favor relations with comparatively larger probabilities.
The computational complexity of the path consistency algorithm is O(n3) when counting composition operations as taking unit time.
As it was pointed out by many authors (e.g., [3] and [5]), for an implementation of the path consistency algorithm to be efficient, the reasoning operations used must be efficient.
Particularly, the time performance of the algorithm in Figure 4 strongly depends on the method of calculating the composition of relations.
1.
L - {(i,j) | 1 <= i < j <= n} 2. while (L [?]
{[?]})
do 3. select and delete an (i,j) from L 4. for k = 1 to n do 5. if (k [?]
i) AND (k [?]
j) do 6. t = Ri,k [?]
(Ri,j [?]
Rjk); 7. if (t has more zero entries than Ri,k) OR ((t [?]
Ri,k) AND (t has the same number of 8. zero entries as Ri,k) AND (Ri,k is lexicographically smaller than t, when entries in Ri,k and t are ordered in descending order)) 9. then do 10.
Ri,k = t; 11.
Rk,i = Inverse (t); 12.
L = L [?]
{(i,k)}; 13. t = Rk,j [?]
(Rk,i [?]
Ri,j); 14 if (t has more zero entries than Rk,j) OR ((t [?]
Rk,j) AND (t has the same number of 15. zero entries as Rk,j) AND (Rk,j is lexicographically smaller than t, when entries in Rk,j and t are ordered in descending order)) 16. then do 17.
Rk,j = t; 18.
Rj,k = Inverse (t); 19.
L = L [?]
{(k,j)}; Figure 4.
Path-consistency algorithm for PTI networks  Our results are theoretical, and experiments will be carried out in the near future.
After implementing the algorithms, we will test them on PTI networks.
The relative speed and the order that the scenarios are generated will be studied.
6.
References [1] Allen, J.: Maintaining Knowledge about Temporal Intervals.
Communications of the ACM 26(11) (1983) 832843.
[2] Badaloni, S., Giacomin, M.: A Fuzzy Extension of Allen's Interval Algebra, In Proceedings of the 6-th Congress of the Italian Association for AI, Lecture Notes in Artificial Intelligence 1792 (2000) 155-165.
[3] van Beek, P., Manchak, D.: The Design and Experimental Analysis of Algorithms for Temporal Reasoning, Journal of Artificial Intelligence Research 4 (1996) 1-18.
[4] Kondrack, G., van Beek, P.: A Theoretical Evaluation of Selected Backtracking Algorithms, Artificial Intelligence 89 (1997) 365-387.
[5] Ladkin, P., Reinefeld, A.: Fast Algebraic Methods for Interval Constraint Problems, Annals of Mathematics and Artificial Intelligence 19(3-4) (1997) 383-411.
[6] Mouhoub, M., Charpillet, F., Haton, J.-P.: Experimental Analysis of Numeric and Symbolic Constraint Satisfaction Techniques for Temporal Reasoning, Constraints: An International Journal 3(2-3) (1998) 151-164.
[7] Ryabov, V.: Handling Uncertain Interval Relations, In Proceedings of the 2-nd IASTED International Conference on AI and Applications, ACTA Press (2002) 291-296.
[8] Ryabov, V., Trudel, A.: Probabilistic Temporal Interval Networks: Extended version, Technical Report TR-2004001, Acadia University (2004).
[9] Schwalb, E., Vila, L.: Temporal Constraints: A Survey, Constraints: An International Journal 3(2-3) (1998) 129-149.
[10] Stergiou, K., Koubarakis, M.: Backtracking Algorithms for Disjunctions of Temporal Constraints, Artificial Intelligence 120 (2000) 81-117.
5.
Conclusions We defined a PTI network whose nodes represent temporal intervals and edges represent the uncertain relations between them.
We then proposed a PTI path consistency algorithm.
Other related algorithms can be found in [8].
Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIME'04) 1530-1311/04 $20.00 (c) 2004 IEEE
Time and Uncertainty in Reasoning about Order Robert A. Morris and Dan Tamir Florida Institute of Technology Melbourne, FL 32901 email:morris@cs.
t.edu  Abstract  The ability to intelligently order events is important for planning and scheduling in the presence of uncertainty about the expected duration of those events.
This paper presents a time-based theory of an agent in a dynamic environment, and a framework for reasoning for the purpose of generating eective orderings of events.
1 Setting the Stage In this study, the interest is in the role of time in the ability of intelligent agents to plan or schedule events, especially actions, events of which they are the agent.
Researchers in AI have, for a number of years, oered analyses and computational models of the temporal reasoning underlying these abilities.
These models have explained the intuitive complexity of reasoning about time in terms of proving the consistency of a set of temporal constraints, an inherently intractable problem.
This study adds further complexity by folding into the framework a dynamically changing environment, wherein temporal knowledge becomes outdated, as well as being partial and incomplete.
How, we ask, can an agent utilize the information found in such an environment in order to eectively solve planning and scheduling problems?
The impetus for this investigation is a system which provides a solution to the dicult problem of scheduling telescope observations  Kenneth M. Ford  University of West Florida Pensacola FL 32514 email: kford@ai.uwf.edu 1].
This solution required attention be given to the fact that the duration of a repeating event may be dierent on dierent occasions.
This made any generated schedule \fragile", which means that there was a tendency for it to \break" during execution.
The novelty of the approach of the researchers was the integration of statistical information about past occurrences of events in order to predict how well a schedule will stand up against a contrary night sky.
This allowed for sensitive locations in the schedule to be identied, and made it feasible to maintain a library of contingency schedules.
We feel this approach to solving planning and scheduling problems in a changing world can be extended and generalized to other problems with similar, dynamic environments.
One objective of this study is to perform these transformations.
The objectives of this paper consist of Constructing an abstract representation of the intelligent behavior which is manifested in the telescope scheduling example, as well as others Proposing a formal representation of the knowledge required to realize this behavior and Presenting a computational model of learning the requisite knowledge based on statistical evidence inferred from the experience of temporal duration and order.
2 Abstract Representation of Behavior The interest here is in systems embedded in a dynamic environment with feedback in the form of rewards.
It is desirable for the system to learn from these rewards in order to maximize its rewards over the long run.
Traditionally, such a system is modeled in terms of state transition networks, consisting of states, actions and state-transition functions.
Here, an alternative model is presented with an underlying temporal ontology.
Specically, there are events represented by their durations, and a single atomic temporal ordering relation, immediately precedes (<).
Given a set E = fA B C g of events, if an agent prefers a certain ordering of their occurrences, say, A < B < C (\A immediately before B immediately before C ") to another, the reason may have to do with constraints which lead to a preference for that order.
There are many varieties of constraints possibly underlying this preference.
Here the interest is in criteria for orderings that are based on temporal constraints.
One example of such a constraint involves minimizing the overall extent of the performance of all the tasks.
By overall extent is meant the interval of time it takes all the events in E to complete.
On this criterion, an ordering of E which is expected to minimize the overall extent of E will be the most preferred ordering.
Another criterion for ordering will be in terms of minimizing the overall duration uncertainty of the set of tasks.
Intuitively, duration uncertainty is manifested in terms of a relative lack of condence concerning how long an event, or a set of events, will take.
If it is possible to predict that one ordering of the tasks will exhibit less duration uncertainty than another, then choosing the ordering with less uncertainty will be preferred.
This is analogous to taking a \sure bet", even if the payo is less than another choice which  is less likely.
The inability to predict how long an event will last on a given occasion (duration uncertainty) is a pervasive feature of common sense experience.
Things that happen in a given day, e.g., breakfast, driving to work, faculty meetings, going to the dentist, exhibit varying amounts of duration uncertainty.
Duration uncertainty is undesirable to a rational agent because it leads to failure in the completion of plans and schedules, and the need for time-consuming repair and revision.
To satisfy one or the other of these constraints, an agent can choose to order the occurrences of the events in such a way that events in close temporal proximity share one or more stages.
Informally, a stage of an action or event E is an action or event which occurs as part of the occurrence of E .
For example, \preparing the cleaning utensils" can be viewed as a stage in most or all cleaning actions.
Often, an event can be \sliced" in dierent ways to uncover its stages.
Suppose two cleaning room actions, clean kitchen (K ) and clean bath (B ) are performed together, say K < B .
There will be a tendency for the preparation stage of B to not be required (or be simplied) hence the overall duration of performing both should be reduced.
Furthermore, since the duration uncertainty of the whole will be a function of the duration uncertainty of the dierent stages, there's a chance that duration uncertainty can also be reduced as a result of this pairing.
This situation is illustrated in Figure 1.
In this gure, stage S1 of K is shared with B .
The temporal eect of sharing stages is that the events can be viewed as overlapping in time.
Notice that when speaking of such relations, there is no assumption of convexity (no interruption) with respect to the intervals making up the durations of the events.
It follows that an agent should be able to more accurately predict how long the bathroom cleaning will take when preceded by the  K B  S1 S1  Figure 1: Eect of Pairing Similar Events in Close Temporal Proximity kitchen cleaning action than it could predict its duration in isolation, or when preceded by a event sharing no stages with it.
The point of the examples, then, is that events that share stages will tend to be mutually inuencing with respect to duration, especially when paired in close temporal proximity.
This sort of information would be useful for an agent who is either lacking the requisite knowledge about the events for which it needs to nd an intelligent ordering, or in which the environment is constantly changing, making its knowledge outdated.
Consider, for example, a robot assigned the task of delivering mail in a dynamically changing environment.
Oces may move, for example, or construction to dierent parts of the complex may require dynamically revising the routes, and hence possibly the order, in which mail is delivered.
Similarly, it may be equipped with only a crude or outdated map of its environment.
We proceed to formalize a model of an agent in a dynamically changing environment.
The model is based on the familiar idea of using a network to store temporal information.
Here, the nodes, or variables represent events in terms of their durations, and the arcs store values which represent the eect of orderings of events on the durations of events that follow them in close temporal proximity.
Denition 1 (Duration Network )A Duration Network N is a set of variables V = V1 : : : Vn , and a set of labeled edges E =  '$ &% '$ &% -2  '$ &% '$ &% V =6  2 ( ( ( ( ( V1 = 4 ((( 0 ;; C C QQ CC QQ ;; CC 0 Q -1 C ; C ;; QQQ 0 CC CC ; QQ C Q  V3 = 5  -1  V4 = 2  Figure 2: Instantiated Duration Network  instantiation of  fhVi  Vj i : 8Vi Vj 2 V g. An N is a function I : V fi E !
Z , such that, all Vi  Vj 2 V , Eij 2 E :  for  I (Vi) > 0 I (Eij )  0 and Let Eij = hVi  Vj i.
Then jI (Eij )j < I (Vi) and jI (Eij )j < I (Vj ).
A duration network is a complete network in which the variables stand for events, and their values are durations of these events.
The labels on the arcs represent the eect of sharing stages on durations.
A negative value for I (hVi  Vj i) represents the advantage of performing Vi and Vj together by virtue of their sharing a stage the negative value is the \reward" for doing them in close temporal proximity.
Figure 2 depicts an instantiated duration graph.
To illustrate the meaning of the graph, consider the nodes V1 and V2 .
The order V1 < V2 < V3 would yield a \reward" of 2 time units.
This means that the overall duration of performing this sequence would be 4 ; 2 + 6 + 0 + 5 = 13 time units.
Compared with performing V1 < V3 < V2, which has overall duration 4 ; 1 + 5 + 0 + 6 = 14 time units, the rst ordering would have the smaller overall extent.
It is useful to distinguish what we will call legs of a tour of a duration network N .
Intuitively, if a tour is a complete path through the network, a leg of the tour is any subpath of that path.
More formally, we use the notion of sub-sequence of a sequence (using the notation t v t) to characterize tour legs.
If t = hVt1  Vt2  : : : Vtn i is a tour through N , then, for example, hVt3  Vt4  Vt5 i is a leg.
To relate a leg to its tour, we use t=hVti  : : :Vti +mi to mean \the part of t consisting of the indicated leg".
Denition 2 (Process/Tour of a Duration Network)A k-process of a duration network N = (V E ) is a sequence P = hI1 I2 : : :  Iki of instantiations of N .
A tour t of a duration network N with variables V = fV1 : : : Vn g is a permutation of V .
We write t = hVt1  : : :  Vtn i to enumerate the elements of t. Denition 3 (Cost of a Tour/Tour Series)Given an Instantiation I and tour t = hVt1  : : :  Vtn i, the cost of a tour in I (c(t I )) is c(t I ) = I (Vt1 )+I (hVt1  Vt2 i)+: : :+I (hVtn;1  Vtn i)+I (Vtn ) Given a k-process P = hI1 : : :Ik i and a sequence of corresponding tours T = ht1 : : :  tk i, called a tour series, the cost of the series T in process P (C (T P )) is X C (T P ) = c(ti Ii) 0  1ik  More generally, we can introduce the notion of \cost of a leg L = hVti  : : :Vti+m i of a tour t in I " as follows: ( 6v t c(t=L I ) = c(L I0) :: Lotherwise Finally, we can speak of the cost of a leg in a tour series T and a process P :  C (T=hVti  : : :Vti+m i P )  as the sum of the costs of this leg in all the tours in the series containing this leg.
The interest now is to dene a set of oneperson games involving tours of the duration graph.
The specic goal of interest is to nd a tour series Tmest of length k which is an agent's estimate of the minimal tour series Tmin .
The latter is the tour series which, given a duration network N and k-process P , incurs the minimum cost over all possible tour series.
Other goals are of course possible.
One is to minimize the standard deviation from the mean of tour durations in the series.
Another goal is to complete as many of the events (i.e., visit as many of the variables) as possible, given rigorous time constraints (i.e.
cost).
Other versions of the game dier on assumptions concerning either the agent's initial knowledge of N , its abilities to update the knowledge based on experience in the form of tours it has made, or on the properties of P .
The interest is in nding denitions of P which characterize properties and relations of the abstract world which are homomorphic to those properties and relations which occur in real world planning and scheduling domains.
First, let us say that an instantiation I is totally repeating in P = hI1 I2 : : : Imi if 9i ji 6= j I = Ii = Ij 2 P .
We can rene this to \repeats n times" in an obvious way.
Two total repetitions of I , say Im and Ip are n units apart if km ; pk = n. If n = 1, then the repetitions will be said to be consecutive.
I will be said to be p-periodic in P if any pair of occurrences of I in P repeat r units apart, where r is a factor of p. Similarly, I is almost periodic in P if there exists a p such that any pair of occurrences of I in P occur a distance apart which is \close" to being a factor of p. We assume this notion of being almost periodic is intuitive enough to remain qualitative, although obviously it can be made more precise.
Finally, we can dene a notion of a partially repeating instantiation  in P , and derivative notions, in terms of instantiations that share some of their values.
Secondly, a process will be said to be invariant if the values of the dierent instantiations do not dier a great deal.
We distinguish two kinds of invariance, duration and path invariance.
First, consider total duration invariance.
We can draw an even ner distinction between weak and strong total duration invariance.
We can express strong invariance in terms of mean, or average duration, and standard deviation.
Thus, let the mean duration of an event represented by Vi in a process P be the average duration of Vi over all instantiations in P .
Let VPi be a variable denoting the standard deviation from the mean.
We say that a process P is -invariant if for each Vi , the value of VPi is less than .
Finally, we say that a process P is totally invariant if there exists a  which is close to 0 such that P is -invariant.
Path invariance means that there is never a large dierence in the cost among dierent paths through N throughout a process P .
More precisely, let c(t1 I ) and c(t2 I ) be the costs of any two of the n!
tours through a duration network N with n variables, given I .
Then path invariance implies that the the dierence between these values is not greater than some small value .
Strong invariance is a global property of a process: intuitively, it says that the duration of any variable or edge of a duration network never strays excessively from the mean.
This does not allow a \real good" path ever to become \real bad", although it may become less good.
Weak invariance is a strictly local phenomenon: it constrains every pair Ii Ii+1 of consecutive instantiations in P to be \close in their assignments" to all elements of N .
(This notion can be made precise in an obvious manner.)
Thus, weak invariance allows for a good path to become bad over the long run.
We can generalize any of these notion of invariance to (partial) invariance, in which a  subset of I exhibits invariance.
Again, for our purposes, it is enough to leave this intuitive notion qualitative.
We view the world as exhibiting varying degrees of invariance and periodicity.
An intelligent agent can learn and apply knowledge about invariance and periodicity in order to make plans which are intelligent.
This is the case although the knowledge the agent has is incomplete, and partial, and the world is in constant ux.
In the next section, we consider this capability in the context of constructing the tour series Tmest.
3 Computational Theory As noted, the ability of an agent to eectively solve the class of problems abstractly characterized as a traversal of a duration network depends on The goal of the game The properties of P  and Assumptions about the agent's knowledge of N and P .
For example, if the agent is given the requisite knowledge to determine P , then it does not matter whether P exhibits any invariance or periodicity: the agent will be able to \precompute" an optimal Tmest based on an exhaustive search of each instantiation.
The case to be examined here is the one in which the knowledge the agent has of P is, at best, partial.
In this section, we describe a version of the game in which 1.
The agent has, initially, an \abstract map" of N  2.
The agent has no quantitative knowledge about P  3.
P exhibits total strong duration and path invariance.
4.
The goal of the game is for the agent to construct Tmest .
We next present a computational theory which explains and realizes this behavior.
To solve for a goal, given the initial constraints, the agent needs to have a means to learn and apply knowledge it discovers about P to select a tour tj , given t1 : : : tj 1, as part of a series.
To make use of the rewards aorded by certain paths, we introduce the notion of relative mean duration: Denition 4 (Relative Duration) Let N = (V E ) be a duration network, T = ht1 : : :tk i be a tour series and P = hI1 : : : Ik i be a process.
The relative duration of an event Vi with respect to an event Vj in an instantiation In and corresponding tour tn (rd(hVi  Vj i tn In)) is c(tn=hVi  Vj i In) + c(tn=hVj  Vii In ).
Furthermore, the relative mean duration of an event Vi with respect to an event Vj over a set of k occurrences of Vi and Vj (rmdViVj (I T ))is ;  C (T=hVi Vj i I ) + C (T=hVj  Vii I ) k Let rmdViVj (I T ) denote the standard deviation of rmdViVj (I T ).
If I and T are given, the notation for these values is simplied to rmdViVj and rmdVi Vj .
Intuitively, relative duration is the cost of the leg Vi < Vj or Vj < Vi in a tour, given an instantiation of the variables and the edge connecting them.
Since the relation of \sharing a stage" is symmetrical, these costs are assumed to be identical e.g., any reward for pairing cleaning actions K and B in immediate temporal proximity will be collected, whether the order be K < B or B < K .
Relative mean duration, then, consists of the average relative duration of pairs of events over a set of tours in a series.
Assuming P exhibits total duration and path invariance, an agent can incrementally  '$ &% '$ &%  '$ &% '$ &%  8 (((( V2 ((( ( V1 11 ;; CC QQ CC QQ ;; CC 8 Q ; 8 CC CC ; QQQ 6  CC ;; V3  6  QQ C Q  V4  Figure 3: A Possible -Graph Associated with Figure 2  learn the requisite knowledge for constructing Tmest on the basis of computing and storing relative mean durations.
This information will be stored in what will be called a -network: Denition 5 Given a duration network N and a process P , a -network for N = (V E ) is a weighted undirected network with the following characteristics.
Each vertex is labeled by one of the elements in a set V .
Each edge (Vi  Vj ) is labeled.
The value of the label represents rmdViVj .
Figure 3 is an example of a -network corresponding to the duration network in the previous gure.
The labels on each edge represent values for rmdViVj .
These values would be accurate, for example, at the end of a kprocess P consisting of k repetitions of the instantiation depicted in the previous gure.
With the information in the -network, an agent can determine the next best tour in Tmest .
Let us assume that the process P exhibits strong duration and path invariance, but incorporates no assumptions about periodicity.
The method TS for constructing Tmin is summarized in Figure 4.
For the sake of simplicity, there is an assumption of a \learn-  UpdateMean(var :  ; graph t : tour I :  instantiation k : index)  Algorithm TS Input:  A process P = hI1 : : :  Iki A Duration Network N = (V E ), V = fV1  V2  : : :  Vn g, initialized by an instantiation Iinit A -network = (V  E ), where V = V and E = E .
For each edge Eij in , let v(Eij ) represent the value of the label on that edge.
Initially, this value is 0.
Output: The updated -network , which now contains statistical information about P based on its having executed a tour series Tmest = htmest1  : : :tmestk i and C (Tmest I ).
For each edge Eij in do v(Eij )   rd(hVi  Vj i Iinit) k   1 Tmest   hi  loop  tk   HamiltonPath( ) UpdateMean(  tk  Ik k) Tmest   Tmest + htk i /* hui + hvi = hu vi */ k  k+1 until k = p Return and C (Tmest P ) Figure 4: Algorithm for Constructing a Tour Series which Minimizes Overall Extent  begin For each edge Eij = hVi  Vj i in do rmdViVj   c(tk =hVi  Vj i I ) + c(tk =hVj  Vii I ) if rmdViVj > 0 then v(Eij )   (k 1)v(Eijk)]+rmdViVj end ;  Figure 5: Updating Algorithm for -graph ing phase" in which the agent is supplied values for one instantiation Iinit of N .
This can be viewed as, e.g., a robot being supplied a \map" of the world it needs to navigate repeatedly.
The main loop iteratively generates tk , the next tour in the series, from by performing a Hamilton Tour of this network, and updates based on information it has acquired about Ik as the result of its tour tk .
The Hamilton tour gives the best estimate of the tour with the lowest overall extent.
The \nal score" of the game is the overall cost of the tour series Tmest.
The UpdateMean Algorithm records the cost of tk as the result of Ik , by updating the -network accordingly.
The procedure is summarized in Figure 5.
This procedure simply updates the mean relative duration rmdViVj for each edge in , based on the result of the cost of traversing this edge in Ik by tour tk (if this tour contains this leg if not, then this cost is 0 and no updates are made).
This algorithm, we claim, realizes behavior which, under the constraints posed by this version of the game, is useful in the generation of intelligent orderings of a set of events.
As a variation on the game, suppose the agent is interested, not in reducing overall extent, but rather in reducing the duration uncertainty associated with a set of events.
This would be the case, e.g., if the agent has no constraints on the time of the completion  of a set of tasks, but wanted to be reasonably sure, at each moment in every tour, on which leg of the tour it is located.
A minor modication of the game in the preceding section will allow the agent to estimate a tour series Tdu, which approximates the tour series which is minimal with respect to duration uncertainty.
Again, let rmdViVj be the standard deviation of the relative mean duration of a set of occurrences of events Vi and Vj in immediate temporal succession.
Imagine modifying the -network so that the labels on each edge Eij stores values of rmdViVj .
We can replace UpdateMean with a procedure, call it UpdateSD, for updating standard deviations, based again on the result of the most recent tour.
Then, applying TS with UpdateSD computes Tdu , which estimates the tour series with the minimal duration uncertainty.
Numerous other enhancements to the representation are possible.
For example, incorporating duration uncertainty as a constraint would lead to a variation of the one-person game in which the agent's goal is to minimize the duration uncertainty associated with a tour.
Another enhancement to the game involves incorporating assumptions regarding periodicity to I would make such information useful to store in a -network.
Relative durations would be further relativized to time periods, which are represented by the index k on the instantiation Ik .
Relative mean durations, and their standard deviations, would be required to reect this relativization.
For this information, it is possible that a quantitative model for probabilistic temporal reasoning such as found in 2], could be applied alternatively, a qualitative model of recurrence, such as 3], might serve the same purpose.
4 Conclusion This paper has provided a framework for developing planning and scheduling systems in a dynamic world.
One primary assumption  motivating this framework is that events tend to exhibit varying degrees of duration uncertainty, and that an intelligent agent needs to confront this uncertainty in planning situations.
One aid in reducing duration uncertainty exploits the fact that events share stages with other events.
References  1] Drummond, M. Bresina, J. Swanson, K., 1994.
Just-In-Case Scheduling.
In Proceedings of the Twelfth National Conference on Articial Intelligence (AAAI94).
AAAI Press/MIT Press, Menlo Park, 1994:1098-1104.
2] Goodwin, Scott D., Hamilton, H. J., Neufeld, E., Sattar, A., Trudel, A.
Belief Revision in a Discrete Temporal Probability-Logic.
Proceedings of Workshop on Temporal Reasoning, FLAIRS94, 3] Morris, R., Shoa, W., and Al-Khatib, L., (1994) Domain Independent Reasoning About Recurring Events.
Forthcoming in The Journal of Computational Intelligence.
A Possibility Theory-Based Approach to the Handling of Uncertain Relations between Temporal Points Allel HADJALI IRISA/ENSSAT 6, rue de Kerampont - BP 447 22305 Lannion Cedex (France) email: hadjali@enssat.fr  Didier DUBOIS Henri PRADE IRIT/CNRS 118, route de Narbonne 31062 Toulouse Cedex 4 (France) e-mail: {dubois, prade}@irit.fr  Abstract Uncertain relations between temporal points are represented by means of possibility distributions over the three basic relations "smaller than", "equal to", and "greater than".
Operations for computing inverse relations, for composing relations, for combining relations coming from different sources and pertaining to the same temporal points, or for representing negative information, are defined.
An illustrative example of representing and reasoning with uncertain temporal relations is given.
This paper shows how possibilistic temporal uncertainty can be handled in the setting of point algebra.
Moreover, the paper emphasizes the advantages of the possibilistic approach over a probabilistic approach previously proposed.
This work does for the temporal point algebra what the authors previously did for the temporal interval algebra.
1.
Introduction Representing and reasoning about time is an essential part of many Artificial Intelligence (AI) tasks (natural language understanding, planning, medical diagnosis and causal explanation, etc).
Since the late eighties, temporal reasoning has been attracting the attention of many AI researchers.
Several approaches have been proposed in this research area [2][17]: logical formalisms for time, ontological primitives, and algorithms for temporal reasoning and their complexity.
Nevertheless, few of these works were concerned with the practical fact that our knowledge about time may be pervaded with vagueness and uncertainty.
Dealing with uncertainty in temporal knowledge is considered as one of the major emerging trends in temporal representation and reasoning, as stressed by Chittaro and Montanari [2].
Attempts along this line are not numerous.
Dubois and Prade [8] propose an approach for the representation of imprecise or uncertain temporal knowledge in the framework of possibility theory.
In this work, fuzzily-  known dates, time intervals with ill-known bounds, and uncertain precedence relations between events can be handled.
Guesgen et al.
[14] introduce fuzzy Allen relations as fuzzy sets of ordinary Allen relations agreeing with a neighborhood structure.
Dubois et al.
[5] have proposed a possibilistic temporal logic where each classical logic formula is associated with the fuzzy set of time points where the formula is certainly true to some extent.
Let us also mention the work done by Freksa [13] who proposes a generalization of Allen's interval-based approach to temporal reasoning, based on semi-intervals, for processing coarse and incomplete information.
In a more recent paper [4], we have studied different types of problems raised by the fuzziness of the categories used for expressing information in temporal reasoning, when time intervals are used as ontological primitives.
Especially, we have provided a fuzzy setbased extension of Allen's approach to interval-based representation of temporal relations.
This extension allows for a gradual description of possible relations between time intervals.
It is based on a fuzzy partition made of three possible fuzzy relations between dates (clearly smaller, approximately equal, and clearly greater).
Allen's calculus is then extended to the case of fuzzy temporal relations in a convenient and expressive way.
Moreover, we have shown that indices for expressing the uncertainty pervading Allen relations between two time intervals (or their fuzzified versions), can be estimated in terms of necessity measures, and used as a basis in deductive reasoning.
Different primitives can be considered for expressing temporal elements.
The main candidates are points [18] and intervals [1].
In this paper, we use temporal points as ontological primitives for representing temporal information.
We propose an approach for handling uncertainty on temporal relations between points in the framework of possibility theory.
Possibility theory [7] is an uncertainty theory devoted to the handling of incomplete information.
It is different from probability theory.
From the point of view of  Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIME'04) 1530-1311/04 $20.00 (c) 2004 IEEE  uncertainty modeling, classical probability theory (assuming a single distribution) is unable to model ignorance (even partial ignorance) in a natural way.
Uniform probability distributions on finite sets of elementary events better express randomness than ignorance, namely, the equal chance of occurrence of elementary events.
Ignorance rather means that each possible occurrence is viewed as equally plausible by an agent, because there is no available evidence that supports any of them.
No probability measure can account for such a state of lacking knowledge (since even a uniform probability distribution leads to assigning nonequal probabilities to at least two possible contingent events in general).
In contrast, possibility theory copes with the situation of complete ignorance in a non-biased way (without making any prior assumption).
Another difference between probability theory and possibility theory is that the probabilistic encoding of knowledge is numerical and relies on an additivity assumption while possibilistic encoding can be purely qualitative, hence less demanding in data.
Lastly, possibilistic reasoning is computationally less difficult than probabilistic reasoning (but the obtained results may also be less informative).
Two set-functions P and N, called possibility and necessity measures, are used in order to model available information in possibilistic framework.
Statements like "A is possible" and "A is certain" are clearly distinguished via the two dual measures P and N respectively.
Only an ordering of elementary events is requested in possibility theory, and it is enough to reconstruct the two orderings of events.
Possibility theory can thus be interpreted either as a representation of ordinal uncertainty based on linear ordering, or as a numerical theory of uncertainty handling special types of probability bounds.
As mentioned above, there are very few works trying to handle uncertainty in temporal reasoning.
Recent work done by Ryabov and Puuronen [15] proposed a probabilistic model for dealing with uncertain relations between temporal points.
Starting with the three basic relations that can hold between two dates a and b: "<" (before), "=" (at the same time), and ">" (after), Ryabov and Puuronen define an uncertain relation between a and b as any possible disjunction of these basic relations, i.e., "<=" ( "<" or "="), ">=" ( "=" or ">"), "[?]"
("<" or ">"), and total ignorance ("<" or "=" or ">").
The uncertainty is then represented by a vector (e<, e=, e>)a,b, where e<a,b  (respectively ea=,b , ea>,b ) is the probability of a < b (respectively a = b, a > b).
Then formulas, which supposedly preserve the probabilistic semantics, are given for propagating uncertainty when composing relations, or  when fusing pieces of temporal information about the same dates.
However, the major flaw of this approach is the way to cope with the state of complete ignorance (this is not surprising since it relies on a probabilistic model).
When nothing is known about the relation between any two temporal points, Rayabov and Puuronen suggest the use of so-called domain probability values, denoted e<D , e =D and e >D , for representing the probabilities of the basic relations between two temporal points in this situation.
This proposal makes sense only if a prior probability distribution is available.
However such a prior probability is never made explicit.
In the present paper, a possibilistic approach for the representation and management of uncertainty in temporal relations between two points, is proposed.
Uncertainty is represented as a vector involving three possibility values expressing the plausibility of the three basic relations ("<", "=", and ">") that can hold between these points.
Reasoning about these uncertain temporal relations is considered through a set of operations including inversion, composition, combination, and negation, like in [15].
These different operations govern the uncertainty propagating in the inference process.
We show that the whole reasoning process can be handled in possibilistic logic [6].
The paper is organized as follows.
Section 2 presents the possibilistic representation of uncertain relational knowledge about dates and compares it to the probabilistic approach.
Section 3 discusses the inference rules that form the basis of the reasoning method.
The illustrative example of [15] for reasoning with uncertain temporal relations is considered in section 4.
To conclude, the main interesting points of the approach are briefly recalled and some future working directions are outlined.
2.
Representation issue Three basic relations can hold between two temporal points: "<" (before), "=" (at the same time), and ">" (after).
These relations are fully certain temporal relations between points.
When knowledge about temporal relations is lacking, the number of alternative options is finite.
For instance, we may only know that date a does not take place after date b, that is, either date a takes place before date b, or a is at the same time than b, but these options exclude the remaining alternative.
More generally, an uncertain relation between temporal points is any possible disjunction of basic relations, i.e., "<=" ("<" or "="), ">=" (">" or "="), "[?]"
("<" or ">"), and "?"
("<", "=", or ">").
The last case represents total ignorance, i.e., any of the three relations is possible.
Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIME'04) 1530-1311/04 $20.00 (c) 2004 IEEE  In the following, we extend this representation by assuming that each of the three basic relations are more or less plausible.
2.1.
Background on possibility theory In the last decade, there has been a major trend in uncertainty modeling (especially regarding partial belief) emphasizing the idea that the degree of confidence in an event is not totally determined by the confidence in the opposite event, as assumed in probability theory.
Possibility theory belongs to this trend.
It was coined by L.A. Zadeh in the late seventies [20] as an approach where uncertainty is induced by pieces of vague linguistic information, described by means of fuzzy sets [19].
Possibility theory offers a simple, non-additive modeling of partial belief, which contrasts with probability theory.
It provides a potentially more qualitative treatment of partial belief since the operations "max" and "min" play a role in possibility theory somewhat analogous to the sum and the product in probability calculus.
In the possibilistic framework, the estimation of an agent's confidence about the occurrence of related events, is based on the two set-functions P and N, called possibility and necessity measures respectively [7].
Let U be a reference set and A and B subsets of U.
- A possibility measure is a mapping P from P(U) = 2U to [0, 1] which satisfies the following axioms: i) P([?])
= 0, ii) Normalization: P(U) = 1, iii) Maxitivity, which in the finite case reads P(A[?
]B) = max(P(A), P(B)).
The weak relationship between the possibility of an event A and that of its complement Ac ("not A") can be expressed by max(P(A), P(Ac)) = 1 due to A[?
]Ac = U and P(U) = 1.
In case of total ignorance, both A and Ac are fully possible P(A) = P( Ac) = 1.
Note that this leads to a representation of ignorance ([?]
A, P(A) = 1) which presupposes nothing about the number of elements in the reference set U (elementary events), while the latter aspect plays a crucial role in probabilistic modeling.
Indeed the normalization constraint in probability theory enforces the constraint Su[?
]U p(u) = 1.
In case of no information, there is no reason to assign a probability to an elementary event higher than to another.
Hence p(u) = 1/card(U), which depends on the way elementary events are defined.
Worse, except if card(U) = 2, there will be two contingent events A, B with P(A) [?]
P(B), which questions the idea that this uniform probability represents ignorance (see Dubois, Prade and Smets [11]).
The case when 0 < min(P(A), P(Ac)) < 1 corresponds to partial belief about A or its complement.
The interpretation of the endpoints of the [0, 1] scale for a possibility measure are clearly different from the probabilistic use: P(A) = 0 means that A is impossible (i.e., A is certainly false), while P(A) = 1 only expresses that A is completely possible, which leaves P(Ac) completely unconstrained.
The weak relationship between P(A) and P(Ac) forces us to use both quantities for the description of uncertainty about the occurrence of A. P(Ac) estimates the possibility of "not A", which is related to the certainty (or necessity) of occurrence of A, since when "not A" is impossible then A is certain.
It is thus natural to use this duality when defining the degree of necessity of A.
- A necessity measure is the dual of the possibility measure and is defined as follows N(A) = 1 - P(Ac).
It estimates the certainty of event "A" as the degree of impossibility of the event "not A".
Note that N(A) = 1 means that A is certainly true, while N(A) = 0 only says that A is not certain at all (however A might still be possible).
It is easy to verify that N(A) > 0 implies that P(A) = 1 (an event is completely possible before being somewhat certain).
Necessity measures satisfy an axiom dual of the one of possibility measures, namely: N(A[?
]B) = min(N(A), N(B)).
For more details about possibility theory, see also [9] and [10].
For convenience, we have presented possibility and necessity measures valued on [0, 1] scale.
However, it can be straightforwardly generalized to any linearly ordered scale.
Then, 1 - ([?])
is replaced by the orderreversing map on the scale.
This remark emphasizes the qualitative nature of the possibilistic setting.
2.2.
Modeling temporal uncertainty Let us consider the basic granule of uncertain temporal information that can be naturally expressed by an agent, in the form of uncertain statements concerning pairs of dates: Definition 1.
Let a et b be two temporal points.
An uncertain relation ra,b between a et b is represented by a vector Pab = (P<, P=, P>)a,b, where P<a,b (respectively  Pa=,b , P>a,b ) is the possibility of a < b (respectively a = b, a > b).
Obviously, the normalization property of the measure of possibility holds, i.e., max( P<a,b , Pa=,b , P>a,b ) = 1.
This  property means that at least one of the three basic relations holds with a possibility equals to 1.
For instance, if we know that the temporal relations between points a and b is ">" with possibility 0.8, "=" with possibility 0.4, hence ">" with a possibility 1 then, the uncertainty vector is (0.8, 0.4, 1)a,b.
It is worth noticing  Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIME'04) 1530-1311/04 $20.00 (c) 2004 IEEE  that from the uncertainty vector ( P<a,b , Pa=,b , Pa>,b ) we can derive the necessity that a >= b (respectively a <= b, a [?]
b) using the usual duality between possibility and necessity.
Namely, N >=a,b = N(a >= b) = 1 - Pa<,b , Na<=,b = N(a <= b) = 1 - Pa>,b , Na[?
],b = N(a [?]
b) = 1 - Pa=,b .
In a similar way, we can also obtain N(a < b) = 1 - max( Pa=,b , Pa>,b ), N(a > b) = 1 - max( P<a,b , Pa=,b ), N(a = b) = 1 - max( P<a,b , Pa>,b ).
Note that an agent positively asserting a relation between a and b should use necessity degrees rather than possibility degrees.
For instance, declaring a >= b is more faithfully modeled by N(a >= b) = 1 than by P(a >= b) = 1, the latter being very weak and uninformative.
Possibility degrees can only express information negatively ("it is more or less impossible that...").
Hence although the uncertainty vector Pab= ( Pa<,b , Pa=,b , Pa>,b ) is the primitive representation in terms of a possibility distribution on elementary mutually exclusive situations involving a and b, it is more convenient in practice to use a vector of necessities Nab= ( N >=a,b , Na[?
],b , Na<=,b ) on the complements of elementary events.
An important principle in possibility theory is the principle of minimal specificity which assumes that any elementary event is possible unless ruled out by an available piece of information.
In view of this principle any piece of information can be expressed by means of a single possibility distribution that is the least committed one in agreement with this piece of information, obtained by maximizing possibility degrees.
This choice is tentative and can be questioned upon the arrival of a new piece of information that will lead to a more specific possibility distribution.
A piece of information like "a >= b is sure to level a" expressed as N(a >= b) >= a, corresponds to P<a,b <= 1 - a, hence ( P<a,b , Pa=,b , Pa>,b ) = (1- a, 1, 1), or equivalently ( Na>=,b , Na[?
],b , Na<=,b ) = (a, 0, 0).
This possibility distribution is the least upper bound of all such distribution that respect the constraint N(a >= b) >= a..
If another piece of information of the form N(a [?]
b) >= b, is obtained, it corresponds to revising (actually expanding) ( P<a,b , Pa=,b , Pa>,b ) into Pab= (1- a, 1- b, 1) and so on.
Note that it is obtained as the (fuzzy set) intersection of (1- a, 1, 1) and (1, 1- b, 1).
Information of the form N(a > b) > g is expressed likewise as Pab= (1- g, 1- g, 1).
Added to the two other items yields Pab =  (min(1- a,1- g) min(1- b,1- g), 1).
In case of complete ignorance, namely, when nothing is known about the relations between two temporal points, each of the three alternatives receives possibility value 1.
Pab= (1, 1, 1).
Hence any information pertaining to the relative position of a and b can be expressed by a unique Pab triplet.
The above setting is very similar to the one in [15] where the basic information granule is of the form of probability vectors Pab = ( e<a,b , ea=,b , e>a,b ) whose components are summing to 1.
However these authors assume that at least two such probabilities are available, while this is not requested in possibility theory.
The standard probabilistic framework cannot express the knowledge of one piece of information such as e<a,b .
In case of plain ignorance, resorting to Pab = (1/3, 1/3, 1/3) is hardly convincing since it entails P(a >= b) > P(a < b) for instance, which is a non trivial piece of knowledge.
3.
Reasoning relations  about  uncertain  temporal  To reason on the basis of uncertain temporal relations, we revisit the set of inference rules considered in [15].
They enable us to infer new temporal information and to propagate uncertainty in a possibilistic way.
This reasoning tool relies on four operations expressing: inversion, composition, combination, and negation, which are presented in the following subsections.
We show that the probabilistic rules used in [15] are sometimes debatable, and that their possibilistic counterparts are more sound (if sometimes providing less information).
3.1.
Inversion operation Let ra,b be a known relation between two temporal points a and b, the inversion operation (~) enables to produce the relation rb,a between the points b and a as pictured in Figure 1. ra,b a  b rb,a = ~r a,b Figure 1.
Operation of inversion  The basic relations "<" and ">" are mutually exchanged, and the inversion of relation "=" is the relation "=" itself.
To derive the uncertainty vector of an inverted temporal relation, we only need to exchange the possibility values  Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIME'04) 1530-1311/04 $20.00 (c) 2004 IEEE  of the basic relations "<" and ">".
Now, if the uncertainty vector of ra,b is represented by Pab and rb,a = ~r a,b then, the uncertainty vector Pba of the relation rb,a is such that P<b,a = Pa>,b , P =b,a = Pa=,b and P >b,a = P<a,b .
It is obvious that the normalization property for the derived uncertain relation rb,a holds (i.e., max( P<b,a , P =b,a , P >b,a ) = 1), knowing that it holds for the initial uncertain relation ra,b.
The same inversion process is valid for probability triplets [15] or necessity triplets.
Basically, it comes down to exchanging a and b.
3.2.
Composition operation Knowing the uncertain relation ra,b between two temporal points a and b, and the uncertain relation rb,c between the points b and c, the operation of composition ([?])
enables us to derive the temporal relation ra,c that may hold between the points a and c. Figure 2 illustrates this operation.
ra,b  b  rb,c  c  a ra,c = ra,b [?]
rb,c Figure 2.
Operation of composition  This operation expresses somewhat the transitive closure of the three basic relations that can hold between two time points.
Instead of assuming that the uncertain relations ra,b and rb,c are defined by the vectors Pab and Pbc respectively, we shall use necessity triplets Nab and Nbc.
The reason is that we can straightforwardly use possibilistic logic [6] to compute the uncertainty vector for the derived relation ra,c, i.e.
Nac.
Indeed possibilistic logic is sound and complete with respect to the computation of possibility distributions exploiting the principle of minimal specificity.
The basic inference rule we shall need here is possibilistic resolution, in the form N(a th1 c) >= min (N(a th2 b), N(b th3 c)) provided that a th2 b and b th3 c imply a th1 c, where th1, th2 and th3 are any relations among dates.
- Computing N(a >= c).
The only existing sufficient condition for deriving a >= c with certainty from information concerning a th2 b and b th3 c where th2 and th3 are of the form >=, <= or [?]
is when both a >= b and b >= c hold.
Hence it follows: N(a >= c) >= min (N(a >= b), N(b >= c)), where the latter two values are obtained from Pab and Pbc.
Of course, N(c >= a) is computed likewise.
- Computing N(a [?]
c).
The only existing sufficient conditions for deriving a [?]
c with certainty from information concerning a th2 b and b th3 c where th2 and th3 are of the form >=, <= or [?]
are when at least one of a [?]
b or b [?]
c hold together with a >= b >= c or a <= b <= c. Indeed, suppose not, then either a = b = c, or a >= b, for instance, and c > b which is not enough to conclude a [?]
c (and likewise for any other similar conditions).
Hence N(a [?]
c) >= N((a [?]
b or b [?]
c) and (a >= b >= c or a <= b <= c)) = min(N(a [?]
b or b [?]
c), N(a >= b >= c or a <= b <= c) >= min(max(N(a [?]
b), N(b [?]
c), max(N(a >= b >= c), N(a <= b <= c)), since N(p or q) >= max(N(p), N(q)) = min(max(N(a [?]
b), N(b [?]
c), max(min(N(a >= b), N(b >= c)), min(N(a <= b),N(b <= c)).
Hence we can reconstruct Nac from necessity triplets Nab and Nbc.
The uncertainty vector Pac can be obtained from the necessity triplet Nac as follows: P<a,c = 1 - N>=a,c , Pa=,c = 1 - Na[?
],c and Pa>,c = 1 - N<=a,c .
The above derivations make no assumption such as e.g.
independence and the like.
It contrasts with the work in < [15].
These authors compute for instance probability ea,c as e<a,c = e<a,b e<b,c + e<a,b e=b,c + e a=,b e<b,c + e<a,b e>b,c e<U + ea>,b e<b,c e<U , the three first terms stem from sufficient conditions for a < c. Moreover, identities such as P(a < b < c) = P(a < b)[?
]P(b < c) are taken for granted.
Lastly, the two last terms in the right hand side of the equality involve socalled domain probabilities, and account for cases where a < c is not forbidden by relations on (a, b) and (b, c).
They are added with the hope to derive P(a < c) exactly.
Unfortunately, this derivation looks debatable in several respects.
The precise meaning of domain probabilities remains obscure, let alone their practical assessment.
The authors never clarify what is the probability space underlying their calculations.
Insofar as several (at least 3) dates are involved, probabilities like P(a < b) and P(b < c) are marginal probabilities stemming from an unknown underlying joint distribution.
Hence it is not likely that the triplet Pac can be exactly derived from marginal probabilities such as triplets Pab and Pbc.
Maximal sufficient conditions for deriving a < c in terms of a th2 b and b th3 c where th2 and th3 are of the form >, < or = are a < b < c, a = b < c, and a < b = c. Hence, only the inequality P(a < c) >= P(a < b < c) + P(a = b < c) + P(a < b = c) holds without any additional assumptions.
It seems that P(a < c) can be exactly computed only if one can define a unique joint probability function on the proper space S. Suppose only variables a, b, c are involved.
Then the probability space S is made of  Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIME'04) 1530-1311/04 $20.00 (c) 2004 IEEE  sequences x th2 y th3 z where (x, y, z) is a permutation of the dates a, b, c and th2, th3 [?
]{>, =}.
There are 13 such sequences corresponding to elementary events whose probability must be specified.
Then for instance P(a < c) = P(a < b < c) + P(a = b < c) + P(a < b = c) + P( b< a < c) + P( a < c < b).
But these terms are not derivable from the only knowledge of Pab and Pbc.
In particular this is especially true for P(b < a < c) and P(a < c < b).
Besides P(a < b < c) will generally differ from P(a < b)[?
]P(b < c).
Only Max(0, P(a < b)+ P(b < c) - 1) <= P(a < b < c) <= min(P(a < b), P(b < c)) is valid.
So a valid local inference on P(a < c) in terms of a th2 b and b th3 c where th2 and th3 are of the form >, < or = without any assumption is e<a,c >= Max(0, e<a,b + e<b,c - 1) + Max(0, e<a,b + e=b,c - 1) + Max(0, e a=,b + e<b,c - 1), but this inference is not clearly complete.
So the probabilistic scheme proposed in [15] looks debatable, and making it sound leaves us with only (weak) probability bounds.
Possibilistic inference rules look simpler and stronger, even if more qualitative.
The possibility distribution on S is defined by applying the principle of minimal specificity to all information items considered as projections.
3.3.
Operation of combination This operation deals with the situation where the information about a relation is coming from two or more distinct sources or experts.
For instance, assume that the uncertain relation that holds between two temporal points a and b is provided by two experts E1 and E2.
The former (respectively the latter) suggests the relation r1a,b (respectively r2a,b) defined by the uncertainty vector P1<, P1=, P1> a,b (respectively P<2, P=2, P>2 a,b ).
Our goal is  (  )  (  )  how to combine these two uncertain temporal relations into a single uncertain relation ra,b.
Let us denote the combination operation by the symbol [?].
Then, we write ra,b = r1a,b [?]
r2a,b.
The idea here is that all raw information pertaining to the pair of dates (a, b) should be merged so as to produce a unique possibility triplet Pab.
In the possibilistic framework, the general ideas that govern the fusion of information issued from several distinct sources are, first, that there is no unique combination mode, and, the choice of the combination mode depends on an assumption about the reliability of sources [12].
Then, the uncertainty vector Pab for the relation ra,b can be computed as follows.
i) if all the sources agree and are considered as equally and fully reliable, then Pab is such that P<a,b = min( P1< , P<2 ),  Pa=,b = min( P1= , P2= ), Pa>,b = min( P1> , P>2 ).
This means that the source that assigns the least possibility degree to a given relation is considered as the best-informed with respect to this relation.
There is no reinforcement with the operator "min".
The idempotence of this operator copes with the problem of possible redundancy of sources and duplicated information.
However, the obtained result may be a subnormalized possibility distribution if the sources are partially conflicting, i.e.
max( P<a,b , Pa=,b , Pa>,b ) < 1.
In that case, a quantitative renormalization exists which consists in dividing all the possibility degrees by < = > max ( Pa,b , Pa,b , Pa,b ).
This is a combination rule commonly used in possibility theory [12].
A qualitative renormalization exists also, which is defined by setting to one the highest possibility degree(s), other degree(s) remaining unchanged.
ii) if the sources disagree and at least one of them is wrong, then (P<, P=, P>)a,b is such that P<a,b = max( P1< , P<2 ), Pa=,b = max( P1= , P2= ), Pa>,b = max( P1> , P>2 ).
In this case, the source that assigns the greatest possibility degree to a given relation is considered as the bestinformed with respect to this relation.
But then there is no need for renormalization.
Note that in the absence of conflict the combination rule is just an application of the principle of minimal specificity.
In contrast, when the local pieces of information are modeled by probability distributions, Ryabov and Puuronen [15] proposed an ad hoc combination rule similar to Dempster rule of combination [3], but invariant with respect to negation.
It is basically a renormalized harmonic mean of the components of the probability vectors.
It clearly presupposes independence between sources.
arious other options are available as for instance a weighted average of the uncertainty triplets, or a geometric mean (see Cooke, [3]).
3.4.
Operation of negation It may happen that we have no information about the relations which are possible between two temporal points, but information about relations that cannot hold between these points.
The aim of the operation of negation is to infer information about the possible uncertain temporal relation ra,b between two points a and b when it is known that an uncertain temporal relation between a and b is not possible.
For instance, it might be known that the relation  Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIME'04) 1530-1311/04 $20.00 (c) 2004 IEEE  ra,b is definitely not "=".
In other terms, it is certain with a degree equals to 1 that ra,b is not "=".
In this case, ra,b can be still "<" or ">".
The pieces of information, like it is certain at level a that the relation ra,b is "not th" where th [?]
{<, =, >}, are represented by assigning possibility degree 1 - a to th, while the two other relations have possibility degrees equal to 1.
This is in agreement with the fact that N(a th b) = 1 - P(a r b) where r is the set of relations representing "not th".
For instance, if it is certain at level 0.6 that ra,b is not "=" then, the uncertainty vector (P<, P=, P>)a,b = (1, 0.4, 1).
Moreover, if it is a-certain that the relation ra,b is not ">" and b-certain is not "=" then, this is represented as the min-combination of the two possibility distributions.
Namely, (P<, P=, P>)a,b = min[(1, 1, 1 - a), (1, 1 - b, 1)] = (1, 1 - b, 1 - a), applying the combination rule.
If we followed the above reasoning in the probabilistic case, the negation of Pab would consist in assigning probability mass e<a,b to the event a >= b (the disjunction of > and =), ea>,b to the event a <= b, and ea=,b to the event a [?]
b.
Clearly, what is obtained is no longer a probability measure, but a random set describing body of uncertain evidence in the sense of belief functions [16].
However Ryabov and Puuronen [15] never envisage this possibility because they insist on ever getting a unique probability measure as the result of any inference operation.
They propose to systematically share the probability carried from th [?]
{>, =, <} over to the complement of th between the two elements of the disjunction (e.g.
e<a,b is shared between > and =).
This process is debatable since the negation of a probability triplet corresponds to a weaker form of information.
4.
An illustrative example Consider a slightly modified version of the example already discussed in [15].
Let a, b and c be three temporal points where the relations that could hold between them are uncertain.
We know that the relation between the points a and b is provided by two information sources (supposedly reliable).
According to the first source, the possibility degrees of the basic relations "<", "=", and ">" between a and b are 1, 0.2, and 0.3 respectively.
The second source suggests that the possibility values of the basic relations between these two points are 1, 0.4, and 0.25.
We know also that it is certain at level 0.6 that the relation between b and c is not ">".
The problem is to estimate the uncertainty vector (P<, P=, P>)a,c of the temporal relation that could hold between the points a and c.  The available pieces of information can be summarized as follows: r1a,b = (1, 0.2, 0.3), r2a,b = (1, 0.4, 0.25), rb,c = (1, 1, 0.4), using the negation operation.
b  ra,b = r1a,b [?]
r2a,b  rb,c  a  c ra,c = ra,b [?]
rb,c Figure 3.
The structure of the example  The combination operation produces ra,b = r1a,b [?]
r2a,b= (1, 0.2, 0.25).
In terms of possibilistic logic, this information reads N(a [?]
b) >= 0.8, N(a <= b) >= 0.75.
The possibility distribution rb,c expresses that N(b <= c) >= 0.6.
Then we get N(a <= c) >= min(N(a <= b), N(b <= c)) = 0.6.
Similarly, N(a [?]
c) >= min(N(a [?]
b), N(a <= b <= c)) >= min (0.8, 0.6) = 0.6.
Hence (P<, P=, P>)a,c = (1, 0.4, 0.4) which means that the basic relation "<" is completely possible between the temporal points a and c, and both the basic relations "=" and ">" are possible between these points with a degree 0.4.
5.
Conclusion Possibility theory provides an expressive tool for the representation and the treatment of uncertainty pervading pieces of local information in temporal reasoning.
There are two interesting features of this theory in uncertainty modeling that make it different from standard probabilistic modeling.
First, it can be purely qualitative, thus avoiding the necessity of quantifying uncertainty if information is poor.
Second, it is capable of modeling ignorance in a non-biased way.
In [4], we had discussed how to use possibility and necessity measures to estimate to what extent it is possible, or certain, that some Allen relations (or their fuzzified versions) hold between two time intervals, when knowledge is uncertain.
In this work, we have shown, for the case of point algebra, how possibility theory can be directly used for handling uncertainty about the relative position of time points.
The difficulty of proposing a sound probabilistic approach lies in the fact that relational temporal information is sparse, granular so that it is not possible to define a unique probability distribution over the sequences of time points without making debatable assumptions or using ad hoc inference rules.
For instance it is not clear how to define a Bayesian net from the available probabilistic information.
An unbiased probabilistic approach inevitably leads to  Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIME'04) 1530-1311/04 $20.00 (c) 2004 IEEE  handle a family of probability, and cannot rule out the case where the available information is inconsistent.
The use of possibility theory naturally copes with such incomplete information.
Several developments of this preliminary work can be envisaged.
First it seems that a direct encoding of possibilistic relational time information into possibilistic logic would provide a sound and complete reasoning method, attaching degrees of necessity to relational statements of the formal a th b.
There is no need to resort to ad hoc inference rules.
It would provide for an automated reasoning tool.
Since the strict equality rarely holds between two temporal points in practice, the proposed approach could also be extended in order to allow for handling a fuzzy partition made by the three basic relations "smaller than (<)", "approximately equal ([?
])", and "greater than (>)".
Lastly, it may be possible to reconsider the probabilistic approach in the light of evidence theory of Shafer, so as to more simply account for the incompleteness of information that seems to pervade the declarative approach to temporal reasoning.
6.
References [1] J.F.
Allen, "Maintaining Knowledge about Temporal Intervals", Communication of the ACM, ol.
26, 1983, pp.
832-843.
[2] L. Chittaro and A. Montanari , "Temporal Representation and Reasoning in Artificial Intelligence: Issues and Approaches", Annals of Mathematics and Artificial Intelligence, ol.
28, 2000, pp.
47-106.
[3] R. Cooke, "Experts in Uncertainty", Oxford University Press, U.K, 1991.
[4] D. Dubois, A. HadjAli, and H. Prade, "Fuzziness and Uncertainty in Temporal Reasoning", Journal of Universal Computer Science, Special Issue on Spatial and Temporal Reasoning (Guesgen H., Anger F., Ligozat G., Rodriguez R., Edts.
), ol.
9(9), 2003, pp.
1168-1194.
[5] D. Dubois, J. Lang, and H. Prade, "Timed Possibilistic Logic", Fundamentae Informaticae, ol.
15, 1991, pp.
211234.
[6] D. Dubois, J. Lang and H. Prade, "Automated Reasoning using Possibilistic Logic: Semantics, Belief Revision and ariable Certainty Weights", IEEE Trans.
on Data and Knowledge Engineering, 6(1), 1994, pp.
64-71.
[7] D. Dubois and H. Prade, "Possibility Theory", Plenum Press, 1988.
[8] D. Dubois and H. Prade, "Processing Fuzzy Temporal Knowledge", IEEE Trans.
on Systems, Man and Cybernetics, ol.
19, 1989, pp.
729-744.
[9] D. Dubois and H. Prade, "Possibility Theory: Qualitative and Quantitative Aspects", andbook of Defeasible Reasoning and uncertainty Management Systems - ol.
1 (Gabbay D.M.
and Smets P., Eds.
), Kluwer Academic Publishers, Dordrech, 1998, pp.
169-226.
[10] D. Dubois and H. Prade, "Fundamentals of Fuzzy Sets", andbooks of Fuzzy Sets Series (Dubois D., Prade H., Eds.
), Kluwer Academic Publishers, Dordrecht, Netherlands, 1999, pp.
343-438.
[11] D. Dubois, H. Prade and P. Smets, "Representing Partial Ignorance", IEEE Trans.
on Systems, Man and Cybernetics, 26(3), 1996, pp.
361-377.
[12] D. Dubois, H. Prade, and G. Yager, "Merging Fuzzy Information", In: Fuzzy Sets in Approximate Reasoning and Information Systems (Bezdek J., Dubois D., Prade H., Eds.
), Kluwer Academic Publishers, Dordrecht, 1999, pp.
335-401.
[13] C. Freksa, "Temporal Reasoning Based on SemiIntervals", Artificial Intelligence, vol.
54, 1992, pp.
199227.
[14] H. W. Guesgen, J. Hertzberg, and A. Philpott, "Towards Implementing Fuzzy Allen Relations", Proc.
ECAI-94 Workshop on Spatial and Temporal Reasoning, Amsterdam, 1994, pp.
49-55.
[15] .
Ryabov and S. Puuronen, "Probabilistic Reasoning about Uncertain Relations between Temporal Points", Proc.
of the 8th International Symposium on Temporal Representation and Reasoning (TIME'01), IEEE Computer Society, 2001, pp.
1530-1511.
[16] G. Shafer, "A Mathematical Theory of Evidence", Princeton University Press, Princeton, N.J, 1976.
[17] L. ila, "A Survey on Temporal Reasoning in Artificial Intelligence", Artificial Intelligence Communications, ol.
7(1), 1994, pp.
4-28.
[18] N. ilain and H. Kautz, "Constraint Propagation Algorithms for Temporal Reasoning", Proc.
of 5th National Conference of the American Association for Artificial Intelligence, Morgan Kaufmann, 1986, pp.
377382.
[19] L.A. Zadeh, "Fuzzy Sets", Information and Control, ol.
8, 1965, pp.
338-353.
[20] L.A. Zadeh, "Fuzzy Sets as a Basis for a Theory of Possibility", Fuzzy Sets and Systems, 1, 1978, pp.
3-28.
Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIME'04) 1530-1311/04 $20.00 (c) 2004 IEEE
An algebraic approach to granularity in time representation Jerome Euzenat INRIA Rhone-Alpes, IMAG-LIFIA 46, avenue Felix Viallet, 38031 Grenoble cedex, France Jerome.Euzenat@imag.fr  Abstract Any phenomenon can be seen under a more or less precise granularity, depending on the kind of details which are perceivable.
This can be applied to time.
A characteristic of abstract spaces such as the one used for representing time is their granularity independence, i.e.
the fact that they have the same structure at different granularities.
So, time "places" and their relationship can be seen under different granularities and they still behave like time places and relationship under each granularity.
However, they do not remain exactly the same time places and relationship.
Here is presented a pair of operators for converting (upward and downward) qualitative time relationship from one granularity to another.
These operators are the only ones to satisfy a set of six constraints which characterize granularity changes.
1 .
Introduction "Imagine, you are biking in a flat countryside.
At some distance ahead of you there is something still.
You are just able to say (a) that a truck (T) is aside a house (H), it seems that they meet.
When you come closer to them (b) you are able to distinguish a bumper (B) between them, and even closer (c), you can perceive the space between the bumper and the house."
This little story shows the description of the same reality perceived at several resolution levels: this is called granularity.
Granularity would not be a problem if different individuals, institutions, etc.
would use the same granularity.
This is not the case and, moreover, these individuals communicate data expressed under different granularities.
There could be a problem if, for instance, someone at position (a), asked "how would you call that which is between H and T?"
because at that granularity, the description of the scene would assume that there is nothing between H and T. The study of granular knowledge representation thus tries to express  how the same phenomenon can, in some sense, be consistently expressed in different manners under different granularities.
This is achieved through operators which, for a situation expressed under a particular granularity, can predict how it is perceivable under another granularity.
coarser (a)  (b) upward  downward  T  H B  (c) finer  Figure 1.
The same scene under three different granularities.
This is taken as a spatial metaphor for time granularity and is used throughout the paper.
Granularity can be applied to the fusion of knowledge provided by sources of different resolution (for instance, agents -- human or computers -- communicating about the same situation) and to the structuring of reasoning by drawing inference at the right level of resolution (in the example of figure 1, the first granularity is informative enough for deciding that the truck driving wheel is on the left of the house -- from the standpoint of the observer).
On one hand, in [10], granularity is expressed granularity between two, more or less detailed, logical theories.
On the other hand, the physical time-space and its representation have been well-studied because many applications require them.
A very popular way to deal with time is the representation of relationships between time intervals [2].
To our knowledge, qualitative time granularity has never been studied before.
Jerry Hobbs [10] introduced granularity in an abstract way  (i.e.
not connected to time) and [11, 4] introduced operators for quantitative time granularity which share a common ground with ours (see SS7).
The paper first recalls some basics about time representation (SS2).
This section can be skipped by those who already know the subject.
Then, the usual interpretations of time and granularity in this context are introduced.
Afterwards, required properties for granularity change operators in the classical time algebra are presented (SS3).
This part is very important since, once accepted the remainder is directly deduced.
The only set of operators (for instant and interval algebra) satisfying the required properties are thus deduced in SS4.
The results concerning the relationship between granularity and inference are then briefly presented (SS5).
The proofs of all the propositions, but the "only" part of the first one, can be found in [6].
The main results (but those of SS5) are from [7].
2 .
Background Classical notions about temporal algebras, neighborhood structures and instant-interval conversions are presented here.
2.1.
Temporal algebra  reciprocal: x2 r-1x1 after (>)  x1/x2  simultaneously (=)  =  Table 1.
The 3 relationships between instants x1 and x2.
x3 > = <  > > > <=>  relation: x1 r x2 before (b) during (d) overlaps (o) starts (s) (and finishes before) finishes (f) (and start after) meets (m)  x1/x2  = > = <  < <=> < <  Table 2.
Composition table between instant relationships.
It is sometimes possible to deduce the relationship between two instants x and z, even if it has not been provided, by propagating the otherwise known relationships.
For instance, if x is simultaneous ({=}) to  reciprocal: x2 r-1x1 after contains overlapped by started by (and finishes after) finished by (and starts before) met by e  equals (e)  There has been considerable work carried out on qualitative time representation.
We recall here several notions about the algebra of topological and vectorial relationships holding between time entities.
An instant is a durationless temporal entity (also called time point by analogy with a point on a line).
It can be numerically represented by a date.
Qualitatively representing these instants requires identifying them and putting them in relation.
There are three possible mutually exclusive relationships between instants.
They are called <<before>> (<), <<after>> (>) and <<simultaneously>> (=).
The set {<, =, >} is called A3.
relation (r): x1 r x2 before (<)  y which is anterior ({<}) to z, then x is anterior to z; this is called composition of temporal relations.
The composition operator x3 is represented by a composition table (table 2) which indeed indicates that =x 3< gives {<}.
A (continuous) period is a temporal entity with duration.
It can be thought of as a segment on a straight line.
A numerical representation of a period is an interval: a couple of bounds (beginning instant, ending instant) or a beginning instant and a duration.
Intervals can be manipulated through a set of 13 mutually exclusive temporal relationships between two intervals (see table 3); this set is called A13.
Table 3.
The 13 relationships between two intervals x1 and x2.
The composition operator x 13 is represented by a composition table [2], similar to the table 2, which allows to deduce, from a set of intervals and constraints between these intervals, the possible relations between any two of these intervals.
2.2.
Extensions of notations Let G be either A1 3 or A3 , [?]
be the logical disjunction and x be the composition operator on G, the following notations are used (in a general manner, <2G [?]
x> is an algebra of binary relationships).
The lack of knowledge concerning the actual position of some temporal entity x with regard to the temporal entity y is expressed by a sub-set r of G which is interpreted as the disjunction of the relations in r: xry =  [?]
xry  r[?
]r  Thus, x{b m}y signifies that the temporal entity x is anterior to or meets the temporal entity y.
The following conventions are used below: * When a result is valid for both algebras, no distinction is made between the temporal entities concerned.
The base sets (A13, A3, and maybe others), as well as the composition x and reciprocity -1 operators are not distinguished;  *  * *  The letter r represents a sub-set of the corresponding base set of relations (r[?
]G); the letter <<r>> represents a relationship.
r -1 represents the set of relations reciprocal of those contained in r: {r-1; r[?]r}.
r1xr2 represents the distribution of x on [?
]:  x+> and y=<y- y+> is expressed by a quadruple (r1, r2, r3, r4) of relationships between the extremities defined as so:  U  considering that x - < x + and y - < y + , each possible relationship between the bounding instants are expressible with such a quadruple (see table 4).
The symbol = is used such that =x is the expression of an interval as a couple of extremities and =r a relationship between intervals expressed as a quadruple.
= is extended towards sets of relations such that =r is a set of quadruples.
Thus:  r1 x r 2 =  r1 x r2 r1 [?
]r1 ,r 2 [?
]r 2  2.3.
Neighborhood structure Two qualitative relations between two entities are called conceptual neighbors if they can be transformed into one another through continuous deformation of the entities [9].
A conceptual neighborhood is a set of relations whose elements constitute a connected subgraph of the neighborhood graph.
DEFINITION (conceptual neighborhood): A conceptual X neighbor relationship is a binary relation NG on a set G X of relations such that NG (r 1 ,r 2 ) if and only if the continuous transformation of an entity o1 in relationship r 1 with another entity o2 can put them in relation r2 without transition through another relation.
>  =  <  d f  (a) b  m (b)  o  s  e si  di  oi  mi  bi  fi  Figure 2.
Neighborhood graphs for (a) instant-to-instant relations, (b) interval-to-interval relations.
The neighborhood graph is made of relations as nodes and conceptual neighborhood as edges (reciprocal relationships are denoted with an "i" added at the end for the sake of readability).
The graph of figure 2a represents the graph of A conceptual neighborhood N3 between instants (the only continuous deformation is translation).
The graph of A Figure 2b represents the conceptual neighborhood N13 for the deformation corresponding to the move of an extremity of an interval (more generally, the deformation corresponds to moving a limit).
Throughout the paper, the only considered transformation A is the continuous move of a limit (called A-neighborhood in [9]).
The influence of this choice is acknowledged when it matters.
2.4.
Conversion from interval to instant formalisms Relationships between intervals can be expressed in function of the relationships between their bounding instants (see table 4): any relationship between x=<x -  <x- x+ > (r1, r2, r3, r4) <y- y+ >  %0 x - r1 y - [?]
x - r2 y + [?]
x + r3 y - [?]
x + r4 y +  [?
]n [?]
< x - x + > [?]
U ( ri1 , ri2 , ri3 , ri 4 ) [?]
< y - y + > [?
]i =1 [?]
{  [?]
}  n  [?]
x - ri1 y - [?]
x - ri2 y + [?]
x + ri3 y - [?]
x + ri 4 y + i =1  xr y  x -r1y -  x -r2y +  x + r3y -  x + r4y +  b d o s f m e m-1 f-1 s-1 o-1 d-1 b-1  < > < = > < = > < = > < >  < < < < < < < = < < < < >  < > > > > = > > > > > > >  < < < < = < = > = > > > >  Table 4.
The 13 relationships between intervals expressed through relationships between interval extremities.
Since any formula representing relationship between four instants x-, x+, y- and y+ respecting the properties of intervals (x-<x+ and y-<y+ ) can be expressed under that form, the inverse operation = is defined.
It converts such an expression between bounding instants of two intervals into a set of relations expressing the disjunction of relations holding between the intervals.
Of course, both operators (= and =) are inverse.
3 .
Requirements for granularity change operators We aim at defining operators for transforming the representation of a temporal situation from one granularity to another so that the resulting representation should be compatible with what can be observed under  that granularity.
The requirements for building such operators are considered here.
The first section concerns what happens to classical models of time and to temporal entities when they are seen through granularity.
The second one provides a set of properties that any system of granularity conversion operators should enjoy.
These properties are expressed in a sufficiently abstract way for being meaningful for instants and periods, time and space.
3.1.
Granularity change operators Time is usually represented under a particular granularity.
Thus, the time representation system presented so far is an adequate representation for time at any granularity (as far as only qualitative properties are considered).
For instance, the three situations of figure 1 can be expressed in the same formalism with objects and qualitative relations between them.
If we only consider the position of the objects along the horizontal line, the three elements (T, B and H) are related to each other in the way of figure 1c by T{m}B (the truck meets its bumper) and B{b}H (the bumper is before the house).
We aim at elucidating the relationship between two representations of the same reality under two different granularities.
As a matter of fact, the situations of figure 1 cannot be merged into one consistent situation: figures 1b and c together are inconsistent since, in (b), B{m}H and, in (c), B{b}H which, when put together, gives B{}H. First, the reasons for these problems are examined before providing a set of properties that granularity change operators must satisfy.
Time is usually interpreted as a straight line, instants as points and intervals as segments.
Under a numerical light, granularity can be defined as scaling plus filtering what is relevant and what is not (discretizing).
However, granularity is a special filter since, as the name indicates, it filters on size.
For the time concern, the granularity of a system can be defined as the duration of the smallest relevant event (relevance being defined independently beforehand).
But what happens to non relevant events?
There are two solutions: * they can vanish; * they can remain with size 0, i.e.
as instants.
In both cases, these solutions share additional consequences (for symbolic representations): if, under a coarse granularity, one observes that some event is connected to another this can be wrong under a finer granularity since a non relevant laps of time could be relevant here.
In another way, when communicating the same observation, it must be taken into account that the short laps of time may be non relevant (and thus that the relationship between the event can be disconnected).
This  is what happened for the relationship between B and H, which is {b}, in Figure 1c, and becomes {m}, in 1b.
In order to account for this situation, which appears to be regular, we need a downward (resp.
upward) operator which, from a relationship observed a some particular granularity, is able to provide a set of relationships at a finer (resp.
coarser) granularity which represents what can be perceived under that last granularity.
The purpose here, is not to design granularity conversion operators which can make events vanish or turn into instants (see [5]) but rather operators which can account for the possibility of having, under a finer granularity, new space between two entities, and, vice-versa, that a space can become non relevant under a coarser granularity.
These operators are called upward and downward granularity conversion operators and noted by the infix g | g' and g'| g operators (where g and g' are granularities such that g is finer -- more precise -- than g', i.e.
that the size of relevant events is smallest in g than in g').
The following g- g' operator will be used for any of them when the property holds for both (then there is no constraint upon g and g').
As usual, the notation g-g' introduced for the conversion of a single relationship is extended towards sets: g-g' r =  U g- g'  r.
r[?
]r  3.2.
Properties for granularity change operators Anyone can think about a particular set of such operators by imagining the effects of coarseness.
But here are provided a set of properties which should be satisfied by any system of granularity conversion operators.
In fact, the set of properties is very small.
But next section shows that they are sufficient for constraining the possibility for such operators to only one (plus the expected operators corresponding to identity and conversion to everything).
Self-conservation Self-conservation states that whatever be the conversion, a relationship must belong to its own conversion.
It is quite a sensible and minimal property: the knowledge about the relationship can be less precise but it must have a chance to be correct.
(1) r [?]
g-g'r (self-conservation) Neighborhood compatibility A property considered earlier is the order preservation property [10] which states (a part of this):  x > y = !
(g - g' x < g - g' y) (order preservation) However, this property has the shortcoming of being vectorial rather than purely topological.
Its topological generalization, is reciprocal avoidance: x r y = !
(g - g' x r-1 g - g' y) (reciprocal avoidance) Reciprocal avoidance, was over-generalized and caused problems with auto-reciprocal relationship (i.e.
such that r=r -1 ).
The neighborhood compatibility, while not expressed in [5] has been taken into account informally: it constrains the conversion of a relation to form a conceptual neighborhood (and hence the conversion of a conceptual neighborhood to form a conceptual neighborhood).
(2) [?
]r, [?]r',r"[?
]g-g'r , [?]r1,...rn[?
]g-g'r such that X  r1=r', rn=r" and [?]i[?
][1,n-1] NG (ri,ri+1) (neighborhood compatibility) This property has already been reported by Christian Freksa [9] who considers that a set of relationships must be a conceptual neighborhood for pretending being a coarse representation of the actual relationship.
(2) is weaker than the two former proposals because it does not forbid the opposite to be part of the conversion, but, in such a case, it constrains whatever be in between the opposite to be in the conversion too.
Neighborhood compatibility seems to be the right property, partly because, instead of the former ones, it does not forbid a very coarse grain under which any relationship is converted in the whole set of relations.
It also seems natural because granularity can hardly be imagined as discontinuous (at least in continuous spaces).
Conversion-reciprocity distributivity An obvious property for such an operator is symmetry.
It is clear that the relationships between two temporal occurrences are symmetric and thus granularity conversion must respect this.
(3) (g-g' r-1) = (g-g' r)-1 (distributivity of g-g' on -1) Inverse compatibility Inverse compatibility states that the conversion operators are consistent with each other, i.e.
that, if the relationship between two occurrences can be seen as another relationship under some granularity, then the inverse operation from the latter to the former can be achieved through the inverse operator.
(4)  r[?]
I g ' | g r ' and r [?]
I g ' | g r '  r '[?
]g | g' r  r '[?
]g | g ' r  (inverse compatibility)  For instance, if someone in situation (b) of figure 1 is able to imagine that, under a finer granularity (say situation c), there is some space between the bumper and the house, then (s)he must be whiling to accept that if (s)he were in situation (c), (s)he could imagine that there is no space between them under a coarser granularity (as in situation b).
Cumulated transitivity A property which is usually considered first is the full transitivity: g- g'*g'- g" r = g- g" r This property is too strong; it would for instance imply that: g- g'*g'- g r = r Of course, it cannot be achieved because this would mean that there is no loss of information through granularity change: this is obviously false.
If it were true anyway, there would be no need for granularity operators: everything would be the same under each granularity.
We can expect to have the cumulated transitivity: g' g" = g" g" g' g" g| *g'| r g| r and |g'* |g r = |g r  However, in a purely qualitative calculus, the amounts of granularity (g) are not relevant and this property becomes a property of idempotency of operators: (5)  |*|=| and |*| = |  (idempotency)  At first sight, it could be clever to have non idempotent operators which are less and less precise with granularity change.
However, if this applies very well to quantitative data, it does not apply for qualitative: the qualitative conversion applies equally for a big granularity conversion and for a small one which is ten times less.
If there were no idempotency, converting a relationship directly would give a different result than doing it through ten successive conversions.
Representation independence Representation independence states that the conversion must not be dependent upon the representation of the temporal entity (as an interval or as a set of bounding instants).
Again, this property must be required: (6)g-g' r = = g-g' =r and g-g' r= = g-g' = r (representation independence) Note that since = requires that the relationships between bounding instants allows the result to be an interval, there could be some restrictions on the results (however, these restrictions correspond exactly to the vanishing of an interval that which is out of scope here).
4 .
The granular system for time relations Once these six properties have been defined one can start generating candidate upward and downward conversion operators.
However, these requirements are so precise that they leave no place for choice.
We are showing below by starting with the instant algebra that there is only one possible couple of operators.
Afterwards, this easily transfers to interval algebra.
4.1.
Conversion operators for the instant algebra The 64(=2 3 .2 3 ) a priori possible operators for converting < and = can be easily reduced to six: the constraint (1) restricts the conversion of < to be {<}, {<=}, {<>} or {<=>} and that of {=} to be in {=}, {<=}, {=>} or {<=>}.
The constraint (2) suppresses the possibility for < to become {<>}.
The constraint (3) has been used in a peculiar but correct way for eliminating the {<=} (resp.
{=>}) solutions for =.
As a matter of fact, this would cause the conversion of =-1 to be {=>} (resp.
{<=}), but =-1 is = and thus its conversion should be that of =.
<\= {<} {<=} {<=>}  {=} Id b d  {<=>} a g no info  Table 5.
The six possible conversion operators for = and <.
There are still six possible conversion operators left (Id, a, b, g, d and NI).
Since the above table does not consider whether the operators are for downward or upward conversion, this leaves, a priori, 36 upwarddownward couples.
But the use of property (4) -- the putative operators must be compatible with their inverse operator (and vice-versa) -- reduces them to 5: Id-Id, ab, g-g, d-d and NI-NI.
The solution Id-Id cannot be considered as granularity since it does not provide any change in the representation.
The solution NI-NI is such that it is useless.
The d-d pair has the major flaw of not being idempotent (i.e.
d *d [?]
d ): as a matter of fact, the composition of d with itself is NI, this is not a good qualitative granularity converter (this violates property 5).
There are two candidates left: the g-g has no general flaw, it seems just odd to have an auto-inverse operator (i.e.
which is its own inverse) since we all know the asymmetry between upward and downward conversion: it could be a candidate for upward conversion (it preserves the equality of equals and weakens the assertions of difference) but it does not fit intuition as a downward  conversion operator (for the same reasons).
Moreover, g does not respect vectorial properties such as orderpreservation (g is just b plus the non distinction between < and >).
Thus the a -b pair is chosen as downward/upward operators.
The main argument in favor of a-b is that they fit intuition very well.
For instance, if the example of figure 1 is modeled through bounding instants (x - for the beginning and x + for the end) of intervals T+ , B-, B+ and H-, it is represented in (c) by T+=B- (the truck ends where the bumper begins), B-<B+ (the beginning of the bumper is before its end), B+<H(the end of the bumper is before the beginning of the house) in (b) by B+ =H - (the bumper ends where the house begins) and in (a) by B-=B+ (the bumper does not exist anymore).
This is possible by converting with the couple a-b which allows to convert B+<H- into B+=H(= [?]
b<) and B-=B+ into B-<B+ (< [?]
a=), but not with the use of g as a downward operator.
Thus the following result is established: PROPOSITION: The table 6 defines the only possible non auto-inverse upward/downward operators for A3.
relation: r < = >  g|  g'r  <= = >=  g|  g'r  < <=> >  Table 6.
Upward and downward granularity conversions between instants.
The operators of table 6 also satisfy the properties of granularity operators.
PROPOSITION: The upward/downward operators for A3 of table 6 satisfy the properties (1) through (5).
4.2.
Conversion operators for the interval algebra By constraint (6) the only possible operators for A13 are now given.
They enjoy the same properties as the operators for A3.
PROPOSITION: The upward/downward operators for A13 of table 7 are the only one to satisfy the property (6) with regard to the operators of A3 of table 6.
PROPOSITION: The upward/downward operators for A13 of table 7 satisfy the properties (1) through (5).
The reader is invited to check on the example of figure 1, that what has been said about instant operators is still valid.
The upward operator does not satisfy the condition (2) for B-neighborhood (violated by d, s and f) and C-neighborhood (o, s and f).
This result holds since the corresponding neighborhoods are not based upon  independent limit translations while this independence has been used for translating the results from A3 to A13.
relation: r b d o s f m e m-1 f-1 s-1 o-1 d-1 b-1  g|  g'r  bm dfse o f-1 s m e se fe m e m-1 f-1 e s-1 e -1 -1 o s f e m-1 d-1 s-1 f-1 e b-1 m-1  g|  g'r  b d o osd d f o-1 bmo o f-1 d-1 s e s-1 d f o-1 o-1 m-1 b-1 d-1 f-1 o d-1 s-1 o-1 o-1 d-1 b-1  Table 7.
Upward and downward conversion operators between intervals.
5.
Granularity and inference The composition of symbolic relationship is a favored inference mean for symbolic representation systems.
One of the properties which would be interesting to obtain is the independence of the results of the inferences from the granularity level (property 7).
The distributivity of g - g ' on x denotes the independence of the inferences from the granularity under which they are worked out.
(7) g- g' (r1 x r2) = (g- g' r1) x (g- g' r2) (distributivity of g-g' over x) This property is only satisfied for upward conversion in A3.
P ROPOSITION : The upward operator for A3 satisfies property (7).
It does not hold true for A13 : let consider three intervals x , y and z such that x b y and y d z , the application of composition of relations gives x{b o m d s}z which, once upward converted, gives x{b m e d f s o f-1}z.
By opposition, if the conversion is first applied, it returns x{b m}y and y{d f s e}z which, once composed, gives x{b o m d s}z.
The interpretation of this result is the following: by first converting, the information that there exists an interval y forbidding x to finish z is lost: if, however, the relationships linking y to x and z are kept, then the propagation will take this into account and recover the lost precision: {b m e d f s o f-1}[?
]{b o m d s}={b o m d s}.
However, this cannot be enforced since, if the length of y is so small that the conversion makes it vanishing, the correct information at that  granularity is the one provided by applying first the composition: x can meet z under such a granularity.
However, if (7) cannot be achieved for upward conversion in A13, we proved that upward conversion is super-distributive over composition.
PROPOSITION: The upward operator for A13 satisfies the following property: (8)  (g|g' r1) x (g|g' r2) [?]
g|g' (r1 x r2) (super-distributivity of g|g' over x)  A similar phenomenon appears with the downward conversion operators (it appears both for instants and intervals).
So let consider three instants x, y and z such that x>y and y=z, on one hand, the composition of relations gives x>z, which is converted to x>z under the finer granularity.
On the other hand, the conversion gives x>y and y{<=>}z because, under a more precise granularity y could be close but not really equal to z.
The composition then provides no more information about the relationship between x and z (x{<=>}z).
This is the reverse situation as before: it takes into account the fact that the indicernability of two instants cannot be ensured under a finer grain.
Of course, if everything is converted first, then the result is as precise as possible: downward conversion is sub-distributive over composition.
PROPOSITION: The downward operators for A13 and A3 satisfy the following property: (9)  g|  g g g' (r1 x r2) [?]
( |g' r1) x ( |g' r2) (sub-distributivity of g|g' over x)  These two latter properties can be useful for propagating constraints in order to get out of them the maximum of information quickly.
For instance, in the case of upward conversion, if no interval vanishes, every relationship must be first converted and then composed.
6 .
Further and ongoing works Category theory which is widely used in programming language semantics has been introduced in knowledge representation [1] in order to account for the relation of approximation between, on the one hand, a knowledge base and the modeled domain, and on the other hand, the many achievements of a knowledge base.
Ongoing works tackle the problem of such a categorical semantics for time representation.
It meets the intuition: granular representation is approximation.
This will provide the advantage of allowing the integration of a specialized time representation into a wider context (e.g.
for adding temporal extension to objects represented as  Ps-terms).
Category theory allows to do so in a general way.
works have been done for extending qualitative granularity from time to space and are reported in [6, 7].
7 .
Related works  Acknowledgments  Jerry Hobbs introduced the concept of granularity from the non discernability of particular terms with regard to a given set of predicates (these terms can be substituted in the range of any of the given predicates without changing their validity).
The main difference here is that the granularity is given a priori in the structure of time and the scaling notion while Hobbs defines a granularity with regard to relevant predicates.
To our knowledge there is no other proposal for integrating granularity into qualitative time representation.
There has been tremendous work on granularity in metric spaces.
One of the more elaborate model is that of [11, 4].
It proposes a quantitative temporal granularity based on a hierarchy of granularities strictly constrained (to be convertible, divisible...) which offers upward and downward conversion operators for instants and intervals (instead of their relationships).
[5] offers a more general (i.e.
less constrained) framework for quantitative relationships and thus achieves weaker properties.
Hence, the properties obtained here for qualitative representation are compatible with the quantitative representation of [11, 4].
Others works [3] considered granularity in a hybrid qualitative/quantitative system.
The same effect as presented here could certainly been achieved through the computation of qualitative relationships from quantitative ones (using [11, 4] but not in a pure qualitative fashion.
[8] presents a systems which shares a great deal with ours: they treat granularity changes between several representations expressed in the same classical temporal logic (just like here, we used the classical A3 and A13) and they map these representations to natural numbers instead of real numbers [5].
However, temporal logics and algebra of relations are not immediately comparable so the results are quite different in nature.
It is expected that the categorical framework sketched in SS6 allows to compare the two approaches in depth.
Many thanks to Hany Tolba who carefully commented a longer version of this paper and to one of the reviewers who pointed out interesting connections.
8 .
Conclusion In order to understand the relationships between several granularities, a set of requirements have been established for conversion operators.
The only possible operators filling these requirements have been defined.
Moreover other properties of the operators have been established (preservation of the relationship between points and interval).
These operators can be used for combining information coming from different sources and overcoming their contradictory appearance.
Further  References 1.
2.
3.
4.
5.
6.
Hassan Ait-Kaci, Andreas Podelski, Towards a meaning of LIFE, Journal of logic programming16(3-4):195-234, 1993 James Allen, Maintaining knowledge about temporal intervals, Communication of the ACM 26(11):832-843 (rep. in Ronald Brachman, Hector Levesque (eds.
), Readings in knowledge representation, Morgan Kaufmann, Los Altos (CA US), pp509-521, 1985), 1983 Silvana Badaloni, Marina Berati, Dealing with time granularity in a temporal planning system, Lecture notes in computer science (lecture notes in artificial intelligence) 827:101-116, 1994 Emanuele Ciapessoni, Edoardo Corsetti, Angelo Montanari, P. San Pietro, Embedding time granularity in a logical specification language for sychronous real-time systems, Science of computer programming 20(1):141-171, 1993 Jerome Euzenat, Representation granulaire du temps, Revue d'intelligence artificielle 7(3):329361, 1993 Jerome Euzenat, Granularite dans les representations spatio-temporelles, Research report 2242, INRIA, Grenoble (FR), 1994 available by ftp from ftp.imag.fr as /pub/SHERPA/rapports/rrinria-2242-?.ps.gz  7.
Jerome Euzenat, An algebraic approach to granularity in qualitative time and space representations, Proc.
14th IJCAI, Montreal (CA), 1995 to appear 8.
Jose Luis Fiadeiro, Tom Maibaum, Sometimes "tomorrow" is "sometime": action refinement in a temporal logic of objects, Lecture notes in computer science (lecture notes in artificial intelligence) 827:48-66, 1994 9.
Christian Freksa, Temporal reasoning based on semi-intervals, Artificial intelligence 54(1):199227, 1992 10.
Jerry Hobbs, Granularity, Proc.
9th IJCAI, Los Angeles (CA US), pp432-435, 1985 11.
Angelo Montanari, Enrico Maim, Emanuele Ciapessoni, Elena Ratto, Dealing with time and granularity in the event calculus, Proc.
4th FGCS, Tokyo (JP), pp702-712, 1992


Decidability of the Theory of the Totally Unbounded o-Layered Structure Angelo Montanari and Gabriele Puppis Dipartimento di Matematica e Informatica, Universita di Udine Via delle Scienze 206, 33100 Udine, Italy {montana, puppis}@dimi.uniud.it Abstract In this paper, we address the decision problem for a system of monadic second-order logic interpreted over an olayered temporal structure devoid of both a finest layer and a coarsest one (we call such a structure totally unbounded).
We propose an automaton-theoretic method that solves the problem in two steps: first, we reduce the considered problem to the problem of determining, for any given Rabin tree automaton, whether it accepts a fixed vertex-colored tree; then, we exploit a suitable notion of tree equivalence to reduce the latter problem to the decidable case of regular trees.
1.
Introduction This paper addresses the decision problem for a system of monadic second-order (MSO for short) logic interpreted over an o-layered temporal structure devoid of both a finest layer and a coarsest one (we call such a structure totally unbounded and we denote it by TULS).
Layered structures have been originally proposed by Montanari et al.
in [4, 5, 6] to model finite and infinite hierarchies of time granularities.
They focus their attention on three distinct layered structures: the n-layered k-refinable structure, denoted by n-LS, which consists of a fixed finite number n of temporal layers such that each time point can be refined into k time points of the immediately finer layer, if any, and the downward (resp.
upward) unbounded k-refinable o-layered structure, denoted by DULS (resp.
UULS), which consists of an infinite number of arbitrarily fine (resp.
coarse) layers.
The MSO theories of these structures have been shown to be expressive enough to capture meaningful temporal properties of reactive systems (such as "P holds at all time points k n , with n >= 0") and decidable.
The decidability of the theories of the k-refinable n-LS, DULS, and UULS has been proved by reducing them to the MSO theories of one successor (S1S), of k successors (SkS), and of the k-ary systolic  tree (S1S k ), respectively.
Both the DULS and the UULS can naturally be viewed as tree structures.
The DULS can be viewed as an infinite sequence of infinite k-ary trees, while the UULS can be seen as a complete k-ary infinite tree generated from the leaves or, equivalently, as an infinite sequence of finite increasing k-ary trees [3].
The totally unbounded k-refinable o-layered structure (TULS) can be viewed as the composition of the DULS and the UULS.
In this paper we deal with the decision problem for the theory of the TULS.
To some extent, the solution we propose can be viewed as an extension of Carton and Thomas' solution to the decision problem for the MSO theories of residually ultimately periodic words [1].
We first provide a tree-like characterization of the TULS.
Taking advantage of it, we define a non-trivial encoding of the TULS into a vertex-colored tree that allows us to reduce the decision problem for the TULS to the problem of determining, for any given Rabin tree automaton, whether it accepts such a vertex-colored tree.
Finally, this latter problem is reduced to the decidable case of regular trees by exploiting a suitable notion of tree equivalence [7] (we call residually regular trees those vertex-colored trees for which such a reduction works).
Notice that, since both the DULS and the UULS can be easily embedded into the TULS, we obtain, as a by-product, a new uniform decidability proof for the theories of the two structures.
2.
Basic notions MSO logics.
MSO logics over graph structures are defined as follows.
A graph structure is defined as a tuple S = (S, E1 , .
.
.
, Ek ), where S (also denoted Dom(S)) is a countable set of vertices and E1 , .
.
.
, Ek are binary relations defining the edge labels.
A vertex-colored graph is a graph S endowed with a partition P1 , .
.
.
, Pm of Dom(S) that defines the colors of the vertices.
MSO formulas are built up from a set of atoms of the form Xi (xj ), Pi (xj ), Pi [?]
Pj , Ei (xj , xk ), by means of the standard connectives [?]
, [?]
, and !
and quantifications over  first-order variables (which are denoted by lowercase letters, e.g., xj , xk , and interpreted as single vertices) and second-order variables (which are denoted by uppercase letters, e.g., Xi , and interpreted as sets of vertices).
The semantics of an MSO formula is defined in the standard way [9].
For a given formula ph(x1 , .
.
.
, xn , X1 , .
.
.
, Xl ), with free variables x1 , .
.
.
, xn , X1 , .
.
.
, Xl , we write S 2 ph[v1 , .
.
.
, vn , V1 , .
.
.
, Vl ] whenever the MSO formula ph holds in the structure S with the interpretation v1 /x1 , .
.
.
, vn /xn , V1 /X1 , .
.
.
, Vl /Xl .
The MSO theory of a structure S, denoted by MTh(S), is the set of all sentences that hold in S. Thus MTh(S) is said to be decidable iff there is an effective way to test whether a given sentence holds in S or not.
Furthermore, we say that an n-ary relation R is MSO-definable in S if there is a formula ph(x1 , .
.
.
, xn ) such that (v1 , .
.
.
, vn ) [?]
R iff S 2 ph[v1 , .
.
.
, vn ].
Colored trees.
K-ary m-colored trees are vertex-colored (deterministic) graphs whose domain is a prefix-closed language over [k], with [k] = {1, .
.
.
, k}, and whose edge relations are such that (u, v) [?]
Ei iff v = ui.
Given a colored tree S, we denote by S(v) the color of the vertex v. The frontier Fr (S) of the tree S is the prefix-free language {u [?]
Dom(S) : [?]
i [?]
[k].
ui 6[?]
Dom(S)}.
In this paper, we mainly deal with full trees, namely, those trees for which, whenever there exists l [?]
[k] such that (u, ul) [?]
El , then (u, ui) [?]
Ei , for every i [?]
[k].
Though the standard notion of full tree includes both empty trees and singleton trees, it is convenient to exclude such cases.
A path of the tree S is a (finite or infinite) word u such that every finite prefix of u belongs to Dom(S).
Given a path u of S, we denote by S|u the sequence of colors associated with the vertices of u.
A branch is a maximal path, namely, a path which is not a proper prefix of any element of S. We denote the set of all (finite or infinite) branches by Bch(S).
Tree automata.
According to [8], a k-ary Rabin tree automaton over the alphabet [m] is a quadruple M = ([n], I, E, AP ), where [n] is a finite set of states, I [?]
[n] is a set of initial states, E [?]
[n] x [m] x [n]k is a transition relation, and AP is a finite collection of accepting pairs (Li , Ui ) with Li , Ui [?]
S. Given an infinite complete k-ary m-colored tree S, a run of the automaton M on S is any infinite complete k-ary n-colored tree r such that (r(u), S(u), r(u1), .
.
.
, r(uk)) [?]
E for every u [?]
Dom(r).
r is said to be successful, and S is said to be accepted by M , if r(e) [?]
I and, for every branch u and every accepting pair (Li , Ui ), we have Inf (r|u) [?]
Li = [?]
and Inf (r|u) [?]
Ui 6= [?
], where Inf (a) denotes the set of elements that occur infinitely often in a sequence a.
The language L (M ) is the set of all trees accepted by M .
We further denote by Img(a) the set of elements that occur in a sequence a.
3.
Layered structures In this section we define (o-)layered structures and we explore the relationships among them.
In particular, we show that the theories of the (k-refinable) DULS and UULS can be easily embedded into the theory of the (k-refinable) TULS.
Definition 1.
AS k-refinable layered structure is a graph Sk = ( i[?
]I Li , <, (|l )l[?
][k] ), where I S [?]
Z, Li = {(i, n) : n [?]
N}, < is a totalSorder over i[?
]I Li , and, for all l [?]
[k] and all (i, n) [?]
i[?
]I Li , |l is a function that maps (i, n) to (i + 1, kn + l - 1) (if there exists such an element).
For all i [?]
I, Li is called a layer of the structure and, for all l [?]
[k], |l is called the l-th projection function, since it maps elements of a given layer to elements of the immediately finer layer (if any).
Both n-layered and o-layered structures have been studied in the literature; in the following, we restrict our attention to o-layered ones.
In [5], Montanari et al.
investigate two meaningful o-layered structures, namely, the k-refinable DULS (abbreviated Dk ) and the krefinable UULS (Uk ).
As already pointed out, Dk can be seen as an infinite sequence of infinite complete k-ary trees, while Uk can be seen either as an infinite k-branching tree generated from the leaves or as an infinite sequence of finite increasing k-trees.
Formally, Dk is obtained by setting I = N and defining < as the total order given by the preorder visit (for elements of the same tree) and by the linear order of the trees (for elements belonging to different trees), while Uk is obtained by setting I = -N and defining < as the total order given by the in-order visit of the nodes of the tree.
In this paper, we focus our attention on a new class of o-layered structures, namely, the k-refinable TULS (Tk ).
For any k, Tk can be seen as the composition of Dk and Uk .
Formally, Tk is obtained by setting I = Z and defining < as a suitable total order over Dom(Tk ) (e.g., the total order induced by the pre-order visit or the in-order one).
In order to systematically analyze the relationships between Tk and Dk (resp.
Uk ), we define a couple of auxiliary relations.
With a little abuse of notation, we use the unary relational symbol L0 to identify the elements of the layer L0 = {(0, n) : n [?]
N} (L0 is the top layer of Dk , the bottom layer of Uk , and a distinguished intermediate layer of Tk ).
Furthermore, we denote by <0 (resp.
+0 ) the order relation (resp.
the successor function) over L0 , which is defined as follows: for every n, n0 [?]
N, (0, n) <0 (0, n0 ) iff n < n0 (resp.
+0 ((0, n)) = (0, n0 ) iff n0 = n + 1).
(It is well-know that <0 is MSO-definable in terms of +0 .)
In Figure 1, we depict a little part of T2 , pointing out the elements of L0 by black-colored vertices and the successor function +0 by bold arrows.
It is not difficult to show that the total order < is MSOdefinable in terms of (|l )l[?
][k] both in Uk and in Tk .
This is  1 3 1 1  2  2  1  +0 1  1  +0  2  2  1  1  2  1  1  2  2  1  1  2  3  2 +0  1  +0  2  2  1  1  2  1  1  3  2  2  1 3  1  2  2  2  2  1  1  3 1  3 1  2 3  2  3  2  1  2  3 1  2  2 3  1  2  3 1  3 1  2  2 3  1  2  3 1  2  3 1  2  Figure 1.
The layered structure (T2 , L0 , +0 ).
Figure 2.
The colored tree ([?
]3 , DT2 , LT2 ).
not the case with Dk , where < is MSO-definable in terms of (|l )l[?
][k] and <0 , or, equivalently, +0 (but not L0 ).
Moreover, the addition of one relation among L0 , <0 , and +0 to Tk (it is easy to show that they are inter-definable in Tk ) allows us to prove that the MSO logic over both Dk and Uk can be embedded into the MSO logic over Tk (it is worth emphasizing that the additional predicate is needed for dealing with both Dk and Uk , not only for Dk ).
The details of the proofs can be found in [7].
The following theorem states that MSO formulas interpreted over (Tk , L0 ) can be turned into equivalent MSO formulas interpreted over the colored tree ([?
]k+1 , DTk , LTk ).
In particular, we have (i) (Tk , L0 ) 2 L0 [v] iff ([?
]k+1 , DTk , LTk ) 2 LTk [fTk (v)], and (ii) (Tk , L0 ) 2 |1 (u, v) iff ([?
]k+1 , DTk , LTk ) 2 E1 (fTk (u), fTk (v)) [?]
Ek+1 (fTk (v), fTk (u)).
4.
The decision problem for (Tk , L0 ) In [3], the decision problems for the MSO theories of the DULS and the UULS have been solved by reducing them to the decision problems for theories of suitable (different) tree structures.
In the following, we deal with the decision problem for the MSO theory of Tk extended with L0 , namely, for the theory of the relational structure (Tk , L0 ).
As a preliminary step, we show that the MSO logic over (Tk , L0 ) can be embedded into the MSO logic over a suitable (k + 1)-ary vertex-colored tree, thus reducing the original problem to the problem of deciding the theory of such a colored tree.
Notice that this embedding allows us to move from the setting of layered structures to the more standard framework of colored tree structures.
The correspondence between the two structures is established by means of a suitable injective function fTk , which maps vertices of Tk to vertices of the infinite complete (k + 1)-ary tree, henceforth denoted by [?
]k+1 : fTk ((i, n)) = (k + 1) * .
.
.
* (k + 1) * | {z } A times  n * ( kA+i-1 mod k + 1) * .
.
.
* ( kn0 mod k + 1) | {z } A+i times  where A is equal to dlog k (n+1)e-i, whenever n+1 >= k i , and to 0 otherwise.
Let DTk = fTk (Dom(Tk )) and LTk = fTk (L0 ).
Both DTk and LTk are (proper) subsets of [k +1]* .
Let us assign them the status of vertex-coloring relations over [?
]k+1 .
In Figure 2, we represent the relations DT2 and LT2 for the (portion of the) layered structure depicted in Figure 1.
Shaded nodes represent the vertices of the domain of (T2 , L0 ) and black-colored nodes represent the vertices belonging to the layer L0 .
Theorem 1.
For any MSO formula ph(x, X), there is an MSO formula ph0 such that (Tk , L0 ) 2 ph[v, V ] iff ([?
]k+1 , DTk , LTk ) 2 ph0 [fTk (v), fTk (V )].
It is easy to see that DTk = [k]* [?]
{k + 1} * {k + 1}* * {k} * [k]* is a regular language of finite words over [k + 1], and thus there exists an MSO formula ph(x) such that [?
]k+1 2 ph[v] iff v [?]
DTk , that is, DTk is MSOdefinable in [?
]k+1 .
As for LTk , assume, by contradiction, that there exists an MSO formula ph defining LTk in [?
]k+1 .
This would imply that ps(X) = [?]
x. X(x) - ph(x) holds in [?
]k+1 iff X is interpreted as LTk .
By Rabin's Theorem [8], there would be a Rabin automaton M such that L (M ) is the singleton consisting of the tree [?
]k+1 colored by LTk .
Such a tree would be non-regular since it would have infinitely many non-isomorphic subtrees.
Because any non-empty Rabin-recognizable set of trees contains a regular tree, L (M ) could not be a singleton.
This is a contradiction and hence LTk is not MSO-definable in [?
]k+1 .
It follows that the MSO logic of ([?
]k+1 , LTk ) is (strictly) more expressive than the MSO logic of [?
]k+1 .
In the following, we develop an automaton-based approach to the decision problem for MTh([?
]k+1 , LTk ).
5.
The automaton-based approach In this section, we outline an automaton-based proof method that generalizes the method advocated by Carton and Thomas in [1], where noticeable properties of residually ultimately periodic words are exploited to prove the decidability of the MSO theories of labelled linear orderings (a detailed presentation of the proposed method can be found in [7]).
As a first step, we show how to reduce the decision problem for MSO theories of colored trees to the acceptance problem for Rabin tree automata.
It is well-known that any  tuple V = (V1 , .
.
.
, Vm ), with Vi [?]
[k]* , can be naturally encoded by a suitable infinite complete k-ary 2m -colored tree, called the canonical representation of V .
Let us denote by SV the canonical representation of V .
Rabin's Theorem [8] establishes a strong correspondence between MSO formulas satisfied by ([?
]k , V ) and Rabin tree automata accepting SV : for every formula ph(X), we can compute a Rabin tree automaton M (and, conversely, for every Rabin tree automaton M , we can compute a formula ph(X)) such that [?
]k 2 ph[V ] iff SV [?]
L (M ).
Let us denote by Acc(SV ) the problem of deciding whether a given Rabin tree automaton recognizes SV .
We have that MTh([?
]k , V ) is decidable  iff Acc(SV ) is decidable.
The problem Acc(SV ) is known to be decidable for any regular colored tree SV (this follows from the decidability of the emptiness problem for Rabin tree automata and from their closure under intersection [8]).
In the following, we will extend the class of colored trees for which the acceptance problem turns out to be decidable.
We introduce the class of residually regular trees and we solve the acceptance problem for such a class by reducing residually regular trees to equivalent regular colored trees (according to a suitable notion of tree equivalence).
We preliminarily introduce some tools for manipulating colored trees.
For every pair of full k-ary m-colored trees S1 and S2 and for every color c [?]
[m], the concatenation S1 *c S2 is the tree resulting from the simultaneous substitution of all the c-colored leaves of S1 by S2 .
The operator *c is not associative; thus, we assume that it associates to the left.
We note that by concatenating a full tree to S, we always obtain a tree S 0 that extends S. Hence, we can easily generalize the definition to the case of infinite concatenations.
We call factorization any finite or infinite concatenation of the form Q S0 *c0 S1 *c1 .
.
.
(we denote infinite concatenations by i[?
]N (Si )ci ).
A factorization is said to be regular if each Sn is either a finite or a regular full tree and there are two positive integers p and q (called respectively prefix and period) such that, for every n >= p, cn = cn+q and Sn = Sn+q .
It is easy to prove that a full colored tree is regular iff it enjoys a regular factorization.
This implies that the MSO theory of any infinite complete colored tree generated by a regular factorization is decidable.
Now, given an automaton M , we define a suitable equivalence relation [?
]M between full colored trees such that (i) [?
]M is a congruence (that is, it respects factorizations) and (ii) it groups together those trees which are indistinguishable by M .
We preliminarily introduce the notion of computation of M .
A computation of the automaton M = ([n], I, E, AP ) on a full m-colored tree S is a full n-colored tree r such that (i) Dom(r) = Dom(S) and (ii) (r(u), S(u), r(u1), .
.
.
, r(uk)) [?]
E for every u [?]
Dom(r) \ Fr (r).
Definition 2.
Given an automaton M = ([n], I, E, AP ) over the alphabet [m], and two full m-colored trees S1 and S2 , S1 [?
]M S2 holds iff, for every computation r1 of M on S1 , there is a computation r2 of M on S2 (and vice versa) such that 1.
(S1 (e), r1 (e)) = (S2 (e), r2 (e)), namely, the colors and the states at the roots of the trees are the same; 2.
{Inf (r1 |u) : u [?]
Bch(S1 )} = {Inf (r2 |v) : v [?]
Bch(S2 )}, namely, the states occurring infinitely often in the branches of the trees are the same; 3.
{(S1 (u), r1 (u), Img(r1 |u)) : u [?]
Fr (S1 )} = {(S2 (v), r2 (v), Img(r2 |v)) : v [?]
Fr (S2 )}, namely, for every finite branch u of S1 , there is a finite branch v of S2 (and vice versa) such that S1 (u) = S2 (v), r1 (u) = r2 (v), and Img(r1 |u) = Img(r2 |v).
It is possible to show that [?
]M is a congruence of finite index that groups together those trees which are indistinguishable by the automaton M (namely, S1 [?
]M S2 implies S1 [?]
L (M ) iff S2 [?]
L (M )) [7].
By exploiting the indistinguishability of equivalent trees, we will reduce the acceptance problem for a large class of colored trees (that we will call residually regular trees) to the acceptance problem for regular colored trees.
Intuitively, we say that a sequence of finite full colored trees is 1-residually regular if, for every congruence of finite index, it is congruent to an ultimately periodical sequence of finite trees (and this sequence can effectively be constructed).
We call residually regular trees those trees that are obtained by concatenating the trees in a residually regular sequence.
We further extend the notion to level n by no longer considering finite trees but level n - 1 residually regular trees.
Let us formalize such an idea.
We denote by [i]l,r either i or ((i - l) mod r) + l, depending on whether i < l or not.
Definition 3.
Given n >= 1, a factorization S0 *c0 S1 *c1 .
.
.
is an n-residually regular factorization if 1. for every i, either Si is a finite full tree or n > 1 and we can provide an (n - 1)-residually regular factorization of Si , 2. for any congruence [?]
of finite index, there exist two positive integers p and q (called prefix and pattern of the factorization with respect to [?])
such that ci = c[i]p,q and Si [?]
S[i]p,q .
An n-residually regular tree is a tree enjoying an nresidually regular factorization.
Given a congruence [?]
of finite index, we inductively define regular forms of residually regular factorizations.
The regular factorization Q [?
]-regular form ofQa 1-residually 0 (S ) is the tree (S ) , where Si0 = S[i]p,q , and i[?
]N i ci i[?
]N i ci p and q are respectively a prefix and a period of the factorization with respect to [?].
For n > Q 1, a [?
]-regular form of an n-residually regular factorization i[?
]N (Si )ci is a tree Q 0 0 (S ) , where S is either S or a [?
]-regular form [i]p,q i i[?
]N i ci  of an (n - 1)-residually regular factorization of S[i]p,q , depending on whether Si is finite or not, Qwhere p and q are respectively a prefix and a period of i[?
]N (Si )ci with respect to [?].
It is easy to verify that a [?
]-regular form of a residually regular tree S is a regular tree equivalent to S, and hence the following theorem trivially follows.
The upshot of such a result is that infinite complete residually regular trees enjoy a decidable MSO theory.
Theorem Q 2.
Let M be an automaton over the alphabet [m], i[?
]N (Si )ci be an n-residually regular factorization of an infinite complete m-colored tree S, and S 0 be its [?
]M regular form.
We have that S [?]
L (M ) iff S 0 [?]
L (M ) (and thus Acc(S) is decidable).
6.
Decidability of the MSO theory of (Tk , L0 ) We conclude the paper by exploiting Theorem 2 to decide the MSO theory of (T2 , L0 ) (the proof can be easily generalized to any k > 2).
By Theorem 1, such a problem can be reduced to the decidability of MTh([?
]3 , LT2 ).
Figure 3 shows the corresponding colored tree ST2 , where blackcolored nodes represent the elements of the layer L0 .
Such a tree can be easily shown to enjoy a decidable MSO theory by providing a residually regular factorization.
A possible choice for the components of such a factorization is represented by dashed regions in Figure 3.
For convenience we denote colored trees by terms.
For instance, the term 2hS1 , S2 , S3 i denotes the tree obtained by appending S1 , S2 , and S3 to a vertex colored by 2.
Using a set of Qthree colors {1, 2, 3}, the factorization can be written as i[?
]N (Si )3 , where * S0 = 2hW, W, 3i, * Si+1 = 1hW, 2, 3i *2 Ri , * W is the infinite complete ternary 1-colored tree ([?
]3 , [3]* ), * R0 = 2hW, W, Wi, * Ri+1 = 1h2, 2, Wi *2 Ri .
The elements corresponding to L0 are represented by color 2.
Now, notice that any congruence [?]
of finite index induces an homomorphism from the set of full colored trees endowed with operator *2 to a finite groupoid.
By exploiting the recursive definition of the Ri 's and noticeable properties of finite groupoids [7], one can easily prove that (Si )i[?
]N is an ultimately periodic sequence up to [?]
and hence the above factorization is residually regular.
7.
Conclusions In this paper, we developed an automaton-based method for deciding the MSO theory of the k-ary totally unbounded o-layered structure.
As a by-product we obtained new uniform decidability proofs for the theories of downward and upward unbounded o-layered  Figure 3.
The tree ST2 embedding (T2 , L0 ).
structures.
The proposed method uses well-known results from automata-theory to reduce the decision problem for the considered MSO theory to the acceptance problem for Rabin tree automata.
It further introduces the class of residually regular trees, which extends that of regular trees, and for which the acceptance problem turns out to be solvable by exploiting a suitable notion of tree equivalence.
As a matter of fact, in [7] we exploited the proposed automaton-based approach to solve the decision problem for a large set of meaningful relational structures, including, for instance, the deterministic trees in the Caucal hierarchy [2].
References [1] O. Carton and W. Thomas.
The monadic theory of morphic infinite words and generalizations.
Information and Computation, 176:51-65, 2002.
[2] D. Caucal.
On infinite terms having a decidable monadic theory.
In Proceedings of the 27th International Symposium on Mathematical Foundations of Computer Science, volume 2420 of LNCS, pages 165-176.
Springer, 2002.
[3] M. Franceschet and A. Montanari.
Temporalized logics and automata for time granularity.
Theory and Practice of Logic Programming, Special Issue on Verification and Computational Logic, 4(5), 2004.
To appear.
[4] A. Montanari.
Metric and Layered Temporal Logic for Time Granularity.
ILLC Dissertation Series 1996-02.
Institute for Logic, Language and Computation, University of Amsterdam, 1996.
[5] A. Montanari, A. Peron, and A. Policriti.
Theories of olayered metric temporal structures: Expressiveness and decidability.
Logic Journal of IGPL, 7(1):79-102, 1999.
[6] A. Montanari and A. Policriti.
Decidability results for metric and layered temporal logics.
Notre Dame Journal of Formal Logic, 37(2):260-282, 1996.
[7] A. Montanari and G. Puppis.
Monadic second-order theories of branching structures.
Research Report 01, Dipartimento di Matematica e Informatica, Universita di Udine, Italy, 2004.
[8] M. Rabin.
Decidability of second-order theories and automata on infinite trees.
Transactions of the American Mathematical Society, 141:1-35, 1969.
[9] W. Thomas.
Languages, automata, and logic.
In G. Rozemberg and A. Salomaa, editors, Handbook of Formal Languages, volume 3, pages 389-455.
Springer, 1997.
A language to express time intervals and repetition Diana Cukierman and James Delgrande School of Computing Science Simon Fraser University Burnaby, BC, Canada V5A 1S6 fdiana,jimg@cs.sfu.ca  Abstract  We are investigating a formal representation of time units, calendars, and time unit instances as restricted temporal entities for reasoning about repeated activities.
We examine characteristics of time units, and provide a categorization of the hierarchical relations among them.
Hence we dene an abstract hierarchical unit structure (a calendar structure) that expresses specic relations and properties among the units that compose it.
Specic time objects in the time line are represented based on this formalism, including non-convex intervals corresponding to repeated activities.
A goal of this research is to be able to represent and reason eciently about repeated activities.
1 Introduction  The motivation for this work is to ultimately be able to reason about schedulable, repeated activities, specied using calendars.
Examples of such activities include going to a specic class every Tuesday and Thursday during a semester, attending a seminar every rst day of a month, and going to tness classes every other day.
Dening a precise representation and developing or adapting known ecient algorithms to this domain would provide a valuable framework for scheduling systems, nancial systems, process control systems and in general date-based systems.
We search for a more general, and formalized representation of the temporal entities than in previous work.
We explore further the date concept, building a structure that formalizes dates in calendars.
Schedulable activities are based on conventional systems called calendars.
We use as a departure point \calendar" in the usual sense of the word.
Examples of calendars include the traditional Gregorian calendar, university calendars, and business calendars, the last two groups being dened in terms of the Gregorian.
The framework we dene concerns a generic calendar abstract structure, which subsumes the mentioned calendars, and arguably any system of measures based on discrete units.
Calendars can be considered as repetitive, cyclic temporal objects.
We dene an abstract structure that formalizes calendars as being composed of time units, which are related by a  decomposition relation.
The decomposition relation is  a containment relation involving repetition and other specic characteristics.
Time units decompose into contiguous sequences of other time units in various ways.
A calendar structure is a hierarchical structure based on the decomposition of time units.
This structure expresses relationships that hold between time units in several calendars.
We refer to the concept of time unit instances, and distinguish dierent levels of instantiation.
Whereas \month" refers to a time unit class, \June" is referred to as a named time unit.
June is one of the 12 occurrences of the notion of month with respect to year, and viewed extensionally it represents the set of all possible occurrences of June.
Finally, \June 1994" is one specic instance of a month.
2 Related work  Our formalism deals with time units, which are a special kind of time interval with inherent durations.
Therefore we base our work on time intervals as the basic temporal objects 1].
In 21], the time point algebra is developed, based on the notion of time point in place of interval.
Computation of the closure of pairwise time-interval relations in the full interval algebra is NP-complete, whereas the time point algebra has a polynomial closure algorithm.
However the time point algebra is less expressive than the interval algebra: certain disjunctive combinations of relations are not expressible with the time point algebra.
Nonetheless, some applications do not require the full expressive power of the interval algebra, and can benet from ecient (albeit less expressive) representations.
Hence it is of interest to study restrictions of the interval algebra.
The present framework deals with restricted kinds of intervals within a hierarchical structure.
The intent is that the hierarchy provide a basis for obtaining ecient algorithms for certain operations.
(This may be contrasted with 11] which considers a hierarchical structure in the general interval algebra.)
Nonconvex intervals (intervals with \gaps") are employed in 12, 13] when using time units in a repetitive way or when referring to recurring periods.
The time unit hierarchy proposed in our work generalizes 13], in that temporal objects can be represented by any se-  quence of composed time units, as opposed to xed in Ladkin's approach.
Moreover, we are able to combine systems of measurement, and so talk about the third month of a company's business year as corresponding with June in the Gregorian calendar.
13] also does not take into account the varying duration of specic time instances, a matter addressed and formalized in our work.
In addition we address the varying duration of specic time instances.
18] also elaborate on the notions of non-convex interval relations dened in 12, 13] while 16] proposes a generalization of nonconvex intervals.
Leban et.
al.
15] deals with repetition and time units.
This work relies on sequences of consecutive intervals combined into \collections".
The collection representation makes use of \primitive collections" (essentially circular lists of integers), and two basic operators, slicing and dicing, which subdivide an interval and select a subinterval respectively.
Poessio and Brachman 19] are mainly concerned with the implementation of algorithms to detect overlapping repeated activities.
This work relies on temporal constraint satisfaction results and algorithms 8].
19] also introduces the concept of using dates as reference intervals to make constraint propagation further ecient.
We envision our proposed formalism provides a useful framework to follow this idea.
5] exposes a set theoretic structure for the time domain with a calendar perspective.
It formalizes the temporal domain with sets of \constructed intervallic partitions" which have a certain parallel to our decomposition into contiguous sequences of intervals.
However, this formalization is more restricted than ours and can not handle the Gregorian nor general calendars.
3] and 4] propose formalizations that deal with calendars and time units.
These two papers are interestingly related to our research, even though they have evolved independently.
These works are analyzed and compared with our research in the Section 5.
3 Time units and time unit instances  The central element of our formalism is that of a time unit.
Time units represent classes of time intervals, each with certain commonalties and which interact in a limited number of ways.
For example, year and month are time units.
Something common to every year is that it decomposes into a constant number of months.
A characteristic of month is that it decomposes into a non-constant number of days, which vary according to the instance of the month.
Properties that are common to time units determine the time unit class attributes.
In 6, 7], the identier of a time unit class and the (general) duration are presented.
Relative and general durations are compared and the concept of an atomic time unit is introduced.
Time unit instances and their numbering and naming is described as well.
All attributes are formally dened in a functional way.
We here  present a summary of these concepts with examples from the Gregorian calendar.
Identier of a time unit The rst attribute of the time unit class is a unique identier, a time unit name, for example year or month.
We distinguish time unit names when the time units have the same duration but dier in their origin.
For example, years in the Gregorian calendar start in January, but academic years, in university calendars in the northern hemisphere, start in September.
Year and academic year are two dierent time units, which have several properties in common.
Duration Time units inherently involve durations a time unit expressly represents a standard adopted to measure periods of time.
Dierent time unit instances of the same time unit class can have dierent durations.
For example dierent months have different number of days, from 28 to 31.
Accordingly, the attribute representing a duration of a class is a range of possible values.
We represent this range by a pair of integers, the extremes of the range of possible lengths any time unit instance can have.
Thus, month as a class has a duration of (28 31).
A duration referred to as general will be expressed in a basic unit, common to all the time units in one calendar.
For example, using day as a common or basic unit, month has a (general) duration of (28 31) days.
A specic month, for example, February 1994, has a duration of 28 days.
We also dene another kind of duration a duration relative to another time unit.
An important reason why (general) durations of all time units in a calendar are dened based on the same basic measure unit is to be able to compare them.
This notion plays a fundamental role in the partial order among time units in the same calendar or variants of a calendar.
Time units decompose into smaller units up to a nite level at which a time unit is atomic | a non-divisible interval.
When the time interval is non-decomposable it is called a time moment, following 2].
Time units therefore model time in a discrete fashion.
Time units may be considered atomic in one application and decomposable into smaller time units in other applications, depending on the intended granularity for the application 10].
Instance names and numbers The name or number of a time unit instance can be expressed in several ways.
Similar to durations, the name is relative to another time unit, a reference time unit.
For example, a day instance can be named from Sunday to Saturday or numbered 1 to 7 if week is the reference time unit of day.
(Names can be thought of as synonyms for the numbers).
Instances of time units which do not have any reference time unit are numbered relative to a conventional reference point or zero point of the calendar, for example the Christian Era for the Gregorian calendar.
The ordered set of all the possible names the instances of a time unit can take within a reference can be expressed extensionally, by a sequence of names (if there are names associated to the time unit class), or a pair of numbers representing the range of maximum possible instance numbers of a class.
This sequence (or pair of numbers) is an attribute of the time unit class.
A particular instance name or number of a time unit within a reference reects the relative position of the instance within the reference, given a certain origin where counting of instance values starts.
For example, months are counted from 1 to 12, in a year in the Gregorian calendar, starting from 1.
But, in the case of months counted within an academic year, the origin of numbering is not month number 1, but rather the 9th (or September), so the 3rd month of a academic year is the 11th month of all the possible months name sequence (or November).
Circular counting is assumed.
More detail appears in 6].
3.1 Decomposition of time units  The primary relation among time units is that of decomposition.
When A decomposes into B, A will be referred to as the composed time unit, and B will be referred to as the component unit.
For example, a year decomposes into months and a month into days.
Also a month decomposes into weeks, a week into days, etc.
Clearly there are dierent kinds of decompositions: a year decomposes exactly into 12 months, whereas a month decomposes in a non-exact way into weeks, since the extreme weeks of the month may be complete or incomplete weeks.
We propose that all these variations in the decomposition relation can be captured with two dierent aspects of the relation: alignment and constancy.
A time unit may decompose into another in an aligned or non-aligned fashion.
The decomposition is aligned just when the composed time unit starts exactly with the rst component and nishes exactly with the last component.
Consequently, a certain number of complete components t exactly into the composed time unit.
Examples of aligned decompositions include year into months, month into days, and week into day.
Examples of non-aligned decompositions include year into weeks and month into weeks.
Figure 1 shows a graphical picture of aligned and non-aligned decomposition.
A (a) Bx  By A  A  A (b)  Bx  By  Bx  By  Bx  By  Figure 1: Graphical representation of Alignment A time unit may decompose into another in a constant or non-constant fashion.
The decomposition is constant when the component time unit is repeated  a constant number of times for every time unit instance.
Examples of constant decompositions include year into months and week into days.
Examples of non-constant decompositions include month into days and year into days.
There are four possible combinations resulting from these two aspects of alignment and constancy.
These combinations cover examples from the various calendars analyzed.
(Arguably) all the relationships of interest in such systems are covered by the variants resulting from combining alignment and constancy of decomposition.
We analyze the composition (or product), intersection (or sum) and inverse of decomposition relations.
For example, if two aligned decomposition relations are multiplied, the resulting decomposition relation is also aligned.
For example year decomposes into month, month decomposes into day.
These two (aligned) decomposition relations can be composed (or multiplied) to obtain the (aligned) decomposition relation of year into day.
It should be noticed that these three operations (composition, intersection and inverse) are dened among decomposition between time unit classes.
This is to be contrasted to the relational algebras dened in the literature and constraint propagation algorithms, which deal with relations between time intervals in the time line, i.e., at the instance level (for example 1, 18, 14].)
A detailed study of these operations appears in 6].
3.2 Calendar Structures: time unit hierarchies  A calendar structure is a pair: a set of time units and a decomposition relation.
We obtained the following result:  Theorem 1 (Decomposition) The decomposition  relation is a particular case of containment, and constitutes a partial order on the set of time units in a calendar.
Therefore, a calendar structure is dened as a containment time unit hierarchy.
The structure is dened so that variants of a specic calendar can be dened in terms of a basic one.
Such would be the case of a university or business calendar, based on the Gregorian calendar.
Such calendars have the same time units as the basic one, with possible new time units or a dierent conventional beginning point.
For example, many university calendars would have a semester time unit, where the academic year begins in September.
Since calendar structures are partial orders, they can be represented by a directed acyclic graph.
The set of nodes in a calendar structure represents the set of time units.
Edges represent the decomposition relation.
The intended application of use of the calendar structure determines which level of time units is included.
Chains We develop a categorization based on the  subrelations of decomposition, i.e.
considering only constant/aligned decompositions, or aligned only, or constant only.
Calendar substructures, composed of chains result from these subrelations.
The term \chains" appears in 17], however there chains are dened on time intervals on the time line.
In our case we are referring to chains of time unit classes.
Nonetheless, chains of time unit classes are directly related to expressions that represent time unit instances, and inuence greatly in eciency matters.
To give an example, the operation of converting time unit instances from one time unit to another will be more ecient when the time units intervening in the time unit instance expression decompose in a constant/aligned way divisions and multiplications can be done, whereas it is necessary to have some iterative process of additions or subtractions when the decomposition is not exact.
A chain is a consecutive linear sequence of time units, such that each one decomposes into any other in the chain in the same way.
Hence all time units in an aligned chain decompose in an aligned way, etc.
A calendar structure can be organized according to these chains.
There is a dierent subgraph associated to each type of decomposition.
For example, Figure 2 shows the Gregorian calendar structure characterized by two aligned chains: <28-centuries, 4-centuries, century, year, month, day, hour> and <28-centuries, week, day, hour>.
In this gure we can observe there are three common nodes to both chains: 28-centuries,day and hour.
28-Centuries 4-Centuries  Century  Year  Month  Week  Day  Hour  Figure 2: Aligned Gregorian calendar substructure We refer to the subgraph that results from organizing the calendar structures according to a certain type of chain as a calendar substructures of that particular type of decomposition (constant, aligned or constant/aligned).
Therefore a chain of a certain type is composed by all those time units in the hierarchy that form a path in the corresponding calendar substructure.
3.3 A language of the set of time units  We dene a language of time units based on the fact that calendar structures can be organized in constant/aligned, constant or aligned chains.
Together with the organization of calendar structures in chains, we distinguish special time units as primitive.
The set of primitive time units includes one time unit per chain in a minimal way.
Hence, in case that all chains have at least one common node, the primitive set is a singleton.
It was proved 6] that any calendar structure can be extended (i.e.
added time units) so that there is such unique common time unit to all chains, for any type of chain.
For example, if the Gregorian calendar is organized as in Figure 2, there are three minimal sets of primitive time units: fhourg, fdayg or f28-centuriesg.
The main idea of this language is that all time units in the structure can be recursively constructed starting from a set of primitive time units, decomposing or composing them, with decomposition limited to an aspect (for example only aligned).
The symbols that are used in this language are: A set of primitive time units, PRIM = fP P1 P2 : : :g, a set of special symbols, f = ( )g a set of (possibly innitely many) constants, CONS = fsecond minute hour :::g and a language to express durations, DUR = fd j d 2 N + g fi f(d1 d2) j + d1 d2 2 N and d1 < d2g.
The language is dened: 1.
If P 2 PRIM, then P 2 TUS.
2.
If C 2 CONS, then C 2 TUS.
3.
If T 2 TUS and D 2 DUR then (T  D) 2 TUS.
4.
If T 2 TUS and D 2 DUR then (T=D) 2 TUS.
5.
Those are all the possible elements of TUS.
Briey, T = (S  D) when T is composed of a contiguous sequence of D S's and T = (S=D) when D contiguous T's compose S. For example, if we organize the Gregorian calendar with aligned chains, the following is a possible interpretation of the language: PRIM = fdayg month = day  (28 31) year = day  (365 366) = month  12 hour = day=24 twoday = day  2 trimester = month  3: Generally we consider only one time unit as primitive.
It is convenient to include constants into the language.
We abuse notation in that we name constants, which are part of the alphabet of symbols, by the name of the domain elements.
Thus if  is an interpretation function,  : TUS  Calendar ;!
Time units in the calendar.
For example (month Gregorian) = \month", (month  3 University) = \trimester".
As long as it does not lead to ambiguities, we will not make an explicit use of this interpretation function in this paper.
Using the same strings for constants and domain elements appears elsewhere, for example 20].
3.3.1 Named time units  As discussed in previous sections, there is more than one level of time unit instantiation.
For exam-  ple, a subclass of month in the Gregorian calendar could be the fourth month (synonym of April).
This does not represent a specic month yet.
We call April a named-month, and viewed extensionally, it represents the set of all specic instances of the month \April".
A specic instance would be \April 1995".
To be able to express specic instances we dene calendar expressions in the next section.
Named time units are dependent on the time unit, a reference time unit and a position within the reference.
Names and numbers are functions dened on the sets of time units, and some involving instances as well.
We write name(T R X) to represent the Xth \named-T" within the reference time unit R, such that R decomposes into T .
If X is outside permissible values, or if the time unit has no associated names with that reference name(T R X) represents an inconsistent value (?).
Examples of this function include name(month year 3) = \March" name(month academic year 3) = \November" name(day week 8) = ?.
Numbered-time units are dened in an analogous way.
For example number(month year 3) = \3".
A function related to named and numbered time units is the last possible value.
Some cases, as already explained, will not provide a unique value, but a range of last values.
In those cases there exists a unique last value only at the instance level (and not at the class level).
For example, last name(month year) = \December" last number(day month) = (28 31) last number instance(day F ebruary 1994) = 28.
4 Calendar expressions  A goal of our research is to represent and reason with time unit instances, that is, the temporal counterpart of single or repeated activities occurring in the time line.
We want to express these time entities in terms of days, weeks, hours, etc, relative to certain conventional calendars.
Calendar structures above dened provide a formal apparatus of units on which to represent a date-based temporal counterpart of activities.
Intervals, time points and moments in the time line will be represented in terms of these units.
Specic time objects are expressed based on the time units in a calendar structure, via calendar expressions.
A basic calendar expression is dened by a conventional beginning reference point or zero point associated to the calendar, and a nite sequence of pairs: Z 	 (t1 x1) : : : (tn xn)].
Each pair (ti xi) contains a time unit ti from the calendar structure and a numeric expression xi.
Numeric expressions include numbers and variables ranging over the set of integers.
The pairs in the sequence are ordered so that any time unit in a pair decomposes into the time unit in the following pair in the sequence.
We also dene duration expressions.
These are similar to calendar expressions in that they consist of a list of pairs (time unit,value), but have no beginning refer-  ence point nor included variables.
The operation of adding a duration expression to a calendar expression denes a new calendar expression.
4.1 A language for simple and repetitive time unit instances: calendar expressions  The formal denition of the language of calendar expressions is presented next.
The language of duration expressions is not introduced here for space reasons, it follows a similar style as the calendar expressions language.
The decomposition relation is used in the denitions.
The same kind of decomposition used to dene the time units in the calendar structure (and therefore the time units language, TUS) is used in these languages.
The language of calendar expressions CALXS, uses the following: A conventional zero point, Z, a set of special symbols f	 ( )  ] \ fi = Lastg, a set of time units (TUS), a set of variables (VAR), which will be ranging in the set of positive naturals, a set of duration expressions(DURXS), and can be dened as: 1.
If T 2 TUS X 2 N + fi VAR then (Z 	 (T X)]) 2 CALXS.
2.
If T1 T2 2 TUS, such that T1 decomposes into T2 , X2 2 N + fi VAR fi fLastg and Z 	 CX (T1 X1)] 2 CALXS, then (Z 	 CX (T1 X1 ) (T2 X2 )]) 2 CALXS.
3.
If CX(*v ) 2 CALXS, where *v are the variables * * in CX, then (CX( v ) where Exp( v )) 2 CALXS Exp(*v ) is an expression constraining *v .
4.
If CX1 and CX2 2 CALXS then (CX1 fi CX2 ) (CX1 \ CX2 ) (CX1 =CX2) 2 CALXS.
5.
If CX 2 CALXS and DX 2 DURXS then (CX + DX) 2 CALXS.
6.
These are all the possible elements of CALXS.
4.2 Examples of calendar expressions  The following examples express calendar expressions in the Gregorian calendar.
The zero point is the beginning of the Christian Era (CE).
1.
CE 	 (year X) (month 4)].
The fourth month within any year.
(Viewed extensionally, it represents the set of all specic instances of the month \April").
X is a non-quantied variable.
2.
CE 	 (year 1994) (week 17) (day 5)].
The 5th day within the 17th week of the year 1994.
The last time unit in the sequence provides the precision of the time unit instance.
Thus, in Example 1 above, the precision is month.
It can also be observed that a calendar expression with no variables is a single convex interval a time unit instance of the last time unit in the calendar expression.
Consequently, Z 	(t1 x1) : : : (tn xn)] is a (single) tn-instance.
Example 2 above is a (single) day-instance.
A calendar expression with variables represents a  set of time unit instances.
These sets of intervals are  the temporal counterpart of a date-based repeated activity, a specic case of a non-convex interval, as dened in 12] .
Thus, CE 	 (year X) (day 175)] represents a non-convex interval, and the subintervals are the days numbered 175th.
As well, when there are variables in the calendar expression, these can be constrained with logic/mathematical expressions.
This example can be extended to: 3.
CE 	 (year Y ) (day 175)] where (Y > 1992 and Y < 1996).
The 175th day from the beginning of each year after 1992 and before 1996.
Hence starting and ending times of non-convex intervals can be expressed straightforwardly.
Another extension to the language of calendar expressions consists of applying set operations (union, intersection and dierence) to combine non-convex intervals, viewing non-convex intervals as sets of subintervals.
(limit cases could produce an empty set of intervals, which we consider a limit case of calendar expression).
The following illustrate this possibility: 4.
CE 	 (year 1994) (month 11) (day X)] \ CE 	 (year 1994) (week W) (day Tuesday)].
Tuesdays of November 1994.
5.
CE 	 (year 1994) (month 11) (day X)] = (CE 	 (year 1994) (month W) (day Y )] where (Y = 7 or Y = 1) ).
Days of November 1994 which are not Saturdays nor Sundays, i.e., weekdays of November 1994.
Saturday and Sunday are abbreviations of certain days numbers as numbered within week.
= represents set dierence.
Finally we also allow calendar expressions to be moved (or displaced) by adding a certain (convex) interval.
For example, October 1994 plus one month is November 1994.
We use duration expressions to express moved expressions.
For example: 6.
CE 	 (year 1994) (month 11) (day 5)] + (day 5)].
The 5th day of November is moved 5 days, resulting in the 10th day of November 1994.
7.
CE 	 (year 1994) (month X) (day 5)] + (day 5)] The 10th day of every month in 1994.
It is worth noticing that (month 1)] is a duration expression representing one month.
On the contrary, CE 	 (year Y ) (month 1)], stands for the non-convex interval of all January's.
Named time units are precluded as a valid duration.
The following examples show how to represent slightly more elaborated periodicity patterns.
8.
CE 	 (year 1994) (week  2 W ) (day 1)].
Every other Monday of 1994.
9.
CE 	 (year 1994) (month M) (day Last)].
Last day of every month.
\Last" is based on the last number function above presented.
Examples 1, 2 and 8 above are accounted by rules 1 and 2 of the language.
Rule 3 allows to have calendars with variables that are constrained, as in example 3.
Examples 4 and 5 are covered by rule 4.
Finally examples 6 and 7 correspond to rule 5.
4.3 Semantics of calendar expressions  A set-based semantics is proposed for these languages.
Interpretation of duration expressions and calendar expressions is based on an interpreted calendar structure.
Recall that we conventionally name constants in the language of the time units (TUS) as the elements in the domain, i.e.
as the time units in the calendar.
Therefore, for the sake of a more clear presentation we will omit the application of an interpretation function when referring to interpreted time units whenever this does not lead to ambiguities.
Examples will be made within the Gregorian calendar.
Hence the zero point Z will be interpreted as year zero of the Christian Era (CE).
A duration expression is interpreted as a convex interval in the time line.
Comparison, addition and subtraction of duration expressions are dened.
Addition and subtraction can be viewed as translations or displacements in the time line.
Limit cases, circular counting, dierence in precision, etc, are taken into account.
This is not presented in this paper.
Calendar expressions are interpreted as sets of intervals in the time line.
Let !
be a function interpreting calendar expressions.
(We omit here the specication of the calendar to simplify this presentation, and again will provide examples from the Gregorian Calendar), then !
: CALXS ;!
2time intervals.
1.
!
(Z 	 (T X)]) = a.
If X 2 N + , the singleton with the X th occurrence of the interval T starting from !(Z).
For example, !
(Z 	 (year 1995)]) = fyear 1995g.
b.
If X is a variable, Sthe set of time intervals T starting from !
(Z), i.e.
1 X =1 !
(Z 	 (T X)]).
For example, !
(Z +	 (year Y )]) = set with every year since CE = N .
2.
!
(Z 	 CX (T1 X1 ) (T2 X2 )]) = a.
If X2 2 N + , the set of X2th subintervals T2 within each interval in the set !
(Z 	 CX (T1 X1)]).
For example, !
(Z 	 (year Y ) (month April)]) = fApril 1, April 2, : : : g. b.
If X2 = Last, the set of Lth subintervals T2 within each interval in the set !
(Z 	 CX (T1 X1 )]), where L = last number(T2 T1) as dened in Section 3.3.1 above.
For example !
(Z 	 (year Y ) (month Last)]) = f December 1, December 2, : : : g. In case T1 decomposes into T2 in a non-constant way, L depends on X1 and possibly on CX.
For example !
(Z 	 (year 95) (month M) (day Last)]) = fJanuary 31 1995, February 28 1995, : : : g. c. If X is a variable, the set of all time intervals T2 within eachS interval in the set !
(Z 	 CX (T1 X1 )]), i.e.
LX =1 !
(Z 	 CX (T1 X1)]), where L = last number(T2 T1).
For example, !
(Z 	  (year 1995) (month M)]) = fJanuary 1995, February 1995, : : : , December 1995 g. 3.
!
(CX(*v ) where Exp(*v )) = S* * !
(CX(*v )).
v j Exp( v ) For example, !
(Z 	 (year 95) (month M)] where (M > 3 and M < 6) ) = fApril 1995, May 1995g.
4.
!
(CX1 fi CX2 ) = !
(CX1 ) fi !
(CX2 ) !
(CX1 \ CX2 ) = !
(CX1 ) \ !
(CX2 ) !
(CX1 =CX2 ) = !
(CX1 )=!
(CX2 ) See examples in Section 4.2 above.
5.
!
(CX = S T + DX) (DX ) (Interval) 8 Interval 2 !
(CX) Addition of a duration expression is interpreted as a translation (T) or displacement of all the subintervals denoted by the calendar expression, eventually a single one (!
(CX)), by a distance dened by the duration expression (!(DX)).
See examples in Section 4.2 above.
5 Alternative proposals dealing with calendars  3] by Chandra et.al.
present a language of \calendar expressions" to dene, manipulate and query what they call \calendars".
In terms of terminology used, their and our proposals coincidentally refer to \calendars" and \calendar expressions".
The notions however are of a dierent nature the two researches have evolved completely independently.
We refer to \calendar" in the usual sense, where the Gregorian calendar or a university calendar are possible examples.
On the other hand, their \calendars" are structured collections of intervals, as dened in 15].
The dierence between both \calendar expressions" is more subtle.
Both calendar expressions dene, in dierent ways, lists of points and intervals in the time line.
However, our expressions are of a more declarative nature, whereas their expressions are closer to a procedure to obtain such intervals.
For example, we express the collection of Fridays as: CE 	 (year Y ) (week W ) (day 5)].
The calendar expressions in 3] are meant as a way of creating and manipulating their \calendars" (i.e., collections of intervals).
5]/DAYS:during:WEEKS expresses the Fridays collection in their language.
There are two operations involved here: selection and the strict foreach operation (or dicing, as dened in 15]) with the operator during.
We have continued further a formal denition of the domain and languages.
On the other hand, Chandra et al.
's work includes a description of an algorithm to parse their expressions and generate a procedural plan to produce an evaluation plan, implemented in Postgress.
As well, they outline how to incorporate calendar expressions to temporal rules in a temporal data base.
We have not dealt with temporal  databases however it is perceived as an area in which to apply our framework.
Ciapessoni et al.
4] dene a many sorted rst order logic language, augmented with temporal operators and a metric of time.
The language is an extension of a specication language TRIO 9].
We will focus the present discussion on the semantics of their system, specially in the extension they provide to embed time granularities in the language.
At this stage of our research, this is where we see the two works as being comparable.
The semantics of the language in 4] includes a temporal universe which consists of a nite set of disjoint and dierently grained \temporal domains".
Each temporal domain contains temporal instants expressed in the corresponding granularity.
Time domains relate with a granularity (or coarseness) relation and a disjointedness relation.
Very briey, our \time units classes" are related to their \time domains" our \decomposition relation" to their \granularity relation" our \repetition factor" to their \conversion factor", our \aligned decomposition" to their \disjointedness" relation.
Another similar characteristic in both proposals is the distinction between time moments and durations (or displacements in their terminology).
We deal with a more general concept of repetition factor.
In fact we extensively address non-constant decomposition.
We emphasize how to express temporal entities based on dates with the conventions of the Gregorian and other calendars.
At the present stage we specially address the formal representation and reasoning about repetitive temporal terms.
As well, we believe that our abstraction of decomposition with only two characteristics (constancy and alignment) denes the relations of interest between the time unit classes in a more general way, as compared to the distinction of dierent relations in 4].
6 Discussion  The goal of this work is to be able to represent and reason eciently about repeated activities using the formalism.
We have dened a hierarchical structure of time units, emphasizing on decomposition among the time units.
We take into consideration the distinction between time unit classes, named time units and specic time unit instances, (for example \month", \August", and \August 1994").
A characterization of the hierarchical structure considers subrelations of decomposition, according to the dierent aspects of decomposition: aligned, constant, and constant/aligned.
A path in such a substructure is referred to as a chain, where all time units decompose in the same way.
Chains constitute the basis we use to dene a formal language to characterize time units.
We also introduce a language to represent time unit instances, called calendar expressions.
These ex-  pressions provide for a straightforward way of representing the temporal counterpart of repeated activities.
We envision that reasoning with calendar expressions based on time units in one chain will produce very ecient operations.
Constant/aligned chains are expected to be particularly ecient.
Currently we are working on further formalizing useful operations between calendar expressions.
A set based semantics has been provided for such expressions.
We are investigating alternative semantic characterizations.
We are also considering adding more expressive power to the language to consider for example expressions that add uncertainty, as for example \twice a week".
Important future research includes studying algorithms that would best t with this formalism, so that we may obtain ecient inferences when reasoning about repeated activities.
Algorithms developed for qualitative and/or quantitative temporal constraint satisfaction problems, or variations, could be considered in this matter.
A language which allows one to dene time units of a variant calendar, such as a university calendar, in terms of a basic calendar, such as the Gregorian, is under study also.
Finally, we believe that the formalism dened represents a generic approach, appropriate to ultimately reason eciently with repeated events within any measurement system based on discrete units that relate with a repetitive containment relation, such as the Metric or Imperial systems.
References  1] J. F. Allen.
Maintaining knowledge about temporal intervals.
Communications of the ACM, 26(11):832{843, 1983.
2] J. F. Allen and P. J. Hayes.
Moments and points in an interval-based temporal logic.
Computational Intelligence, 5:225{238, 1989.
3] R. Chandra, A. Segev, and M. Stonebraker.
Implementing calendars and temporal rules in next generation databases.
In Proc.
of the International Conference on Data Engineering, ICDE94, pages 264{273, 1994.
4] E. Ciapessoni, E.Corsetti, A. Montanari, and P. San Pietro.
Embedding time granularity in a logical specication language for synchronous real-time systems.
Science of Computer Programming, 20:141{171, 1993.
5] J. Cliord and A. Rao.
A simple, general structure for temporal domains.
In C. Rolland, F. Bodart, and M. Leonard, editors, Temporal aspects of information systems, pages 17{28.
Elsevier Science Publishers B.V. (North-Holland), 1988.
6] D. Cukierman.
Formalizing the temporal domain with hierarchical structures of time units.
M.Sc.
Thesis.
Simon Fraser University, Vancouver, Canada, 1994.
7] D. Cukierman and J. Delgrande.
Hierarchical decomposition of time units.
In Workshop of Temporal and Spatial reasoning, in conjunction with AAAI-94, pages 11{17, Seattle, USA, 1994.
8] R. Dechter, I. Meiri, and J. Pearl.
Temporal constraint networks.
Articial Intelligence, 49:61{ 95, 1991.
9] C Ghezzi, D. Mandrioli, and A. Morzenti.
Trio, a logic language for executable specications of real-time systems.
Journal of Systems and Software, 12(2), 1990.
10] J. Hobbs.
Granularity.
In Proc.
of the Ninth IJCAI-85, pages 1{2, 1985.
11] J.
A. G. M. Koomen.
Localizing temporal constraint propagation.
In Proc.
of the First Inter-  national Conference on Principles of Knowledge Representation and Reasoning, pages 198{202,  12] 13] 14] 15] 16] 17] 18] 19] 20]  21]  Toronto, 1989.
P. Ladkin.
Time representation: A taxonomy of interval relations.
In Proc.
of the AAAI-86, pages 360{366, 1986.
P. B. Ladkin.
Primitives and units for time specication.
In Proc.
of the AAAI-86, pages 354{359, 1986.
P. B. Ladkin and R. D. Maddux.
On binary constraint problems.
Journal of the ACM, 41(3):435{469, 1994.
B. Leban, D. D. McDonald, and D. R. Forster.
A representation for collections of temporal intervals.
In Proc.
of the AAAI-86, pages 367{371, 1986.
G. Ligozat.
On generalized interval calculi.
In Proc.
of the AAAI-91, pages 234{240, 1991.
S. A. Miller and L. K. Schubert.
Time revisited.
Computational Intelligence, 6(2):108{118, 1990.
R. A. Morris, W. D. Shoa, and L. Khatib.
Path consistency in a network of non-convex intervals.
In Proc.
of the IJCAI-93, pages 655{660, 1993.
M. Poesio and R. J. Brachman.
Metric constraints for maintaining appointments: Dates and repeated activities.
In Proc.
of the AAAI-91, pages 253{259, 1991.
R. Reiter.
Towards a logical reconstruction of relational database theory.
In M. L. Brodie, J. Mylopoulos, and J. W. Schmidt, editors, On Conceptual Modeling, pages 191{238.
SpringerVerlag, 1984.
M. Vilain and H. Kautz.
Constraint propagation algorithms for temporal reasoning.
In Proc.
of the AAAI-86, pages 377{382, 1986.
Using Temporal Logics of Knowledge in the Formal Verification of Security Protocols Clare Dixon, Mari-Carmen FernaEndez Gago, Michael Fisher and Wiebe van der Hoek Department of Computer Science University of Liverpool, Liverpool L69 7ZF, United Kingdom FAX: E MAIL: WWW:  (+44) 151 79 43715  {C.Dixon,M.C.Gago,M.Fisher,wiebe}@csc.liv.ac.uk http://www.csc.liv.ac.uk/{Eclare,Emari,Emichael,Ewiebe}  Abstract Temporal logics of knowledge are useful for reasoning about situations where the knowledge of an agent or component is important, and where change may occur in this knowledge over time.
Here we use temporal logics of knowledge to reason about security protocols.
We show how to specify the NeedhamSchroeder protocol using temporal logics of knowledge and prove various properties using a resolution calculus for this logic.
1 Introduction Improved communication infrastructures encourage parties to interchange more and more sensitive data, such as payment instructions in e-commerce, strategic information between commercial partners, or personal information in, for instance, medical applications.
Issues such as authentication of the partners in a protocol, together with the confidentiality of information therefore become increasingly important: cryptographic protocols are used to distribute keys and authenticate agents and data over hostile networks.
Although the protocols used often look very intricate, many examples are known of sensitive applications that were acrackeda and had to be furnished with new, aimproveda protocols.
It is obvious that in such informationsensitive applications as above, one prefers to formally prove that certain information can not be eavesdropped by unwanted third parties.
The application of logical tools to the analysis of security protocols was pioneered by Burrows, Abadi and Needham.
In [1] and [10] specific epistemic logics, collectively referred to as BAN logics, are proposed to deal with authentication issues.
At the same time, standard epistemic logics ([12, 18]) have been applied successfully to reason about communication protocols, cf.
the derivation of the alternating bit protocol in [14] or, more recently, the analysis of TCP [22]).
In such an analysis, an epistemic language is useful in order to express that some receiver indeed knows some message at a specific state of the protocol, or that a sender knows that the receiver knows the message.
In this setting, contrary to the security framework of BAN, the implicit assumption is always that the network is not hostile.
In spite of the potential, more recent work on epistemic logic has mainly focused on theoretical issues such as variants of modal logic, completeness, and derived notions such as distributed knowledge and common knowledge.
Logics of knowledge are useful for specifying systems where statements such as If John knows his own private key and John receives a message encrypted in his public key then John knows the contents of the message 1  are required.
Statements such as the above represent static situations, in other words they describe the state of the knowledge within the world, but not how this knowledge evolves.
For this, we incorporate temporal logic.
Temporal logics have been shown to be useful for specifying dynamic systems that change over time [16].
By combining temporal and epistemic logics, we provide a logical framework in which systems requiring both dynamic aspects and informational aspects relating to knowledge can be described.
This is particularly important in security protocols, where one wants to ensure that certain knowledge is obtained over time or, at least, that ignorance of potential intruders persists over the whole run of the protocol.
Such temporal logics of knowledge have been used in the specification and verification of distributed and multi-agent systems [8, 13, 18], in analysing security protocols [23, 9], and in characterising knowledge games such as the muddy children problem [4].
The logic, KL(n) , that we consider here is the fusion of a linear time temporal logic (comprising finite past and infinite future) with the multi-modal logic S5 (see, for example, [12] for more details about this logic).
In order to prove that a particular property ' follows from a problem specification , for example a security protocol, where both ' and are formulae of KL(n) we must prove that ` ) '.
Since we carry out proofs using clausal resolution for temporal logics of knowledge [3, 4], which is a refutation method, we actually check that the combination of and :' is unsatisfiable.
This resolution calculus uses a translation to a normal form in order to separate modal and temporal components, a novel resolution method applied to the temporal part and modal resolution rules applied to the modal part.
Information is carried between the two components using clauses containing only literals.
Given the above background, in this paper, we bring together specification using temporal logics of knowledge and verification using clausal resolution, and apply these to the problem of formally analysing security protocols.
In order to show how such protocols can be specified and verified, we consider one very well known protocol, namely the Needham-Schroeder protocol [20].
This protocol has been widely studied with particular problems uncovered via formal analysis, for example [17].
The paper is structured as follows.
In Section 2 we describe the Needham-Schroeder protocol.
In Section 3 we give the syntax and semantics of a linear-time temporal logic of knowledge and in Section 4 we present a clausal resolution method for this logic.
In Section 5 we show how the Needham-Schroeder protocol can be specified in this logic.
2 The Needham-Schroeder protocol with public keys The Needham-Schroeder protocol with public keys [20] intends to establish authentication between an agent A who initiates a protocol and an agent B who responds to A.
The complete protocol consists of seven messages, but we focus on a simplified version consisting of only three messages.
The messages that we omit are those whereby the agents request other agentas public keys from a server.
Note that omitting these steps is equivalent to assuming that each agent always knows the othersa public keys.
The protocol can then be described as the three following steps: Message Direction Contents Message 1 A !
B : fNA ; Agpub key(B) Message 2 B !
A : fNB ; NA gpub key(A) Message 3 A !
B : fNB gpub key(B) Note that message contents of the form fX ; Y gpub key(Z ) represent messages containing both X and Y but then encrypted with Zas public key.
Elements of the form NX are special items of data, called nonces.
Typically, 2  agents in the protocol will generate their own unique nonce (often encrypted) which is, at least initially, unknown to all other agents.
Message 1: A sends B an encrypted nonce together with Aas identity, all encrypted with Bas public key.
Message 2: When B receives Message 1, it decrypts it to obtain NA .
Then B returns to A the nonce NA and generates another nonce of his own, NB , and sends it back, this time encrypted with Aas public key.
Message 3: When A receives Message 2, it returns Bas nonce, this time encrypted with Bas public key in order to prove Aas authenticity.
It would seem that A should be sure he is talking to B, since only B ashoulda be able to decrypt Message 1.
In the same way, B seems to be sure that he is talking to A since only A ashoulda be able to decrypt Message 2.
However, we will see later that this is not always the case.
3 Syntax and Semantics The logic, KL(n) , a temporal logic of knowledge we consider is the fusion of linear-time temporal logic with multi-modal S5.
We first give the syntax and semantics of KL(n) , where each modal relation is restricted to be an equivalence relation [15].
The temporal component is interpreted over a discrete linear model of time with finite past and infinite future; an obvious choice for such a flow of time is (N ; <), i.e., natural numbers ordered by the usual aless thana relation.
This logic has been studied in detail [15] and is the most commonly used temporal logic of knowledge.
3.1 Syntax Formulae are constructed from a set P = fp; q; r; : : : g of primitive propositions.
The language KL(n) contains the standard propositional connectives : (not), _ (or), ^ (and) and ) (implies).
For knowledge we assume a set of agents Ag = f1; : : : ng and introduce a set of unary modal connectives Ki , for i 2 Ag, where a formula Ki  is read as aagent i knows a.
For the temporal dimension we take the usual [11] set (always), U (until) and W of future-time temporal connectives h(next), (sometime or eventually), (unless or weak until).  }
The set of well-formed formulae of KL(n) , WFFK is defined as follows:     false, true and any element of P is in WFFK ; if A and B are in WFFK then so are (where i 2 Ag)  :A  }A  A_B A  A^B AU B  A)B AW B  Ki A hA  We define some particular classes of formulae that will be useful later.
Definition 1 A literal is either p, or :p, where p 2 P .
Definition 2 A modal literal is either Ki l or :Ki l where l is a literal and i 2 Ag.
Notation: in the following, l are literals, m are either literals or modal literals and D are disjunctions of literals or modal literals.
3  3.2 Semantics First, we assume that the world may be in any of a set, S, of states.
Definition 3 A timeline t, is an infinitely long, linear, discrete sequence of states, indexed by the natural numbers.
Let TLines be the set of all timelines.
Definition 4 A point q, is a pair q = (t; u), where t into t. Let Points be the set of all points.
2 TLines is a timeline and u 2 N is a temporal index  Definition 5 A valuation  , is a function  : Points  P Definition 6  !
fT ; Fg.
A model M, is a structure M = hTL; R ; : : : ; Rn ;  i, where: 1     TL  TLines is a set of timelines, with a distinguished timeline t0 ;     is a valuation.
Ri , for all i 2 Ag is the agent accessibility relation over Points, i.e., Ri Ri is an equivalence relation;   Points  Points where each  As usual, we define the semantics of the language via the satisfaction relation aj=a.
For KL(n) , this relation holds between pairs of the form hM ; pi (where M is a model and p is a point in TL  N ), and formulae in WFFK .
The rules defining the satisfaction relation are given below.
hM ; (t; u)i j= true hM ; (t; u)i 6j= false hM ; (t; u)i j= p hM ; (t; u)i j= :A hM ; (t; u)i j= A _ B hM ; (t; u)i j= hA hM ; (t; u)i j= A hM ; (t; u)i j= }A hM ; (t; u)i j= A U B hM ; (t; u)i j= A W B hM ; (t; u)i j= KiA  iff iff iff iff iff iff iff  iff iff  ((t; u); p) = T (where p 2 P ) hM ; (t; u)i 6j= A hM ; (t; u)i j= A or hM ; (t; u)i j= B hM ; (t; u + 1)i j= A 8u0 2 N ; if (u  u0) then hM ; (t; u0 )i j= A 9u0 2 N such that (u  u0 ) and hM ; (t; u0 )i j= A 9u0 2 N such that (u0  u) and hM ; (t; u0 )i j= B; and 8u00 2 N ; if (u  u00 < u0 ) then hM ; (t; u00 )i j= A hM ; (t; u)i j= A U B or hM ; (t; u)i j= A 8t0 2 TL: 8u0 2 N : if ((t; u); (t0 ; u0 )) 2 Ri then hM ; (t0 ; u0 )i j= A  For any formula A, if there is some model M and timeline t such that hM ; (t; 0)i j= A, then A is said to be satisfiable.
If for any formula A, for all models M there exists a timeline t such that hM ; (t; 0)i j= A then A is said to be valid.
Note, this is the anchored version of the (temporal) logic, i.e.
validity and satisfiability are evaluated at the beginning of time (see for example [5]).
As agent accessibility relations in KL(n) models are equivalence relations, the axioms of the normal modal system S5 are valid in KL(n) models.
The system S5 is widely recognised as the logic of idealised knowledge, and for this reason KL(n) is often termed a temporal logic of knowledge.
4  4 Resolution for Temporal Logics of Knowledge The resolution calculus is clausal requiring a translation to a normal form to separate modal and temporal components and to put formulae in a particular form.
A set of temporal resolution rules applied to the temporal part and modal resolution rules are applied to the modal part.
Information is carried between the two components using clauses containing literals.
Full details of resolution based proof methods for temporal logics of knowledge are given in [3, 4].
4.1 Normal Form Formulae in KL(n) can be transformed into a normal form SNF K (Separated Normal Form for temporal logics of knowledge).
For the purposes of the normal form we introduce a symbol start such that hM ; (t0 ; 0)i j= start.
This is not necessary but allows the normal form to be implications.
An alternative would be to let initial clauses (see Figure 1) be a disjunction of literals.
The translation to SNFK removes many of the temporal operators that do not appear in the normal form by rewriting using their fixpoint definitions.
Also the translation uses the renaming technique [21] where complex subformulae are replaced by new propositions and the truth value of these propositions is linked to the formulae they replaced in all states.
 operator, which allows nesting of K and  operators.
The To achieve this we introduce the i operator is defined in terms of the C (or common knowledge) and E (or everybody knows) operators.
We define E by E , i2Ag Ki  : The common knowledge operator, C, is then defined as the maximal  operator is defined as the maximal fixpoint of fixpoint of the formula C , E( ^ C) : Finally, the  ,  ): ( ^ C Thus we reason about reachable points from the initial point in the distinguished timeline t0 (where start is satisfiable), i.e.
the points we require in the proof.
V  4.1.1  Definition of the Normal Form  Formulae in SNFK are of the general form    ^  Tj  j  where each Tj , known as a clause, must be in one of the varieties described in Figure 1 where ka , lb , and l are literals and mib are either literals, or modal literals involving the Ki operator.
Thus a Ki clause (also known as a modal clause) may not contain modal literals Ki l1 and Kj l2 (or Ki l1 and :Kj l2 ) where i 6= j.
Each Ki clause involves literals, or modal literals involving the Ki operator where at least one of the disjuncts is a modal literal.
The outer a  a operator that surrounds the conjunction of clauses is usually omitted.
Similarly, for convenience the conjunction is dropped and we consider just the set of clauses Tj .
To apply the temporal resolution rule (see Section 4.2), one or more step clauses may need to be combined.
Consequently, a variant on SNFK called merged-SNFK ) [6], is also defined.
Given a set of step clauses in SNFK , any step clause in SNFK is also a clause in SNFK .
Any literal clause of the form true ) F is written into a merged- SNFK clause as true ) hF.
Any two merged-SNFK clauses may be combined to produce a merged-SNFK clause as follows A B (A ^ B)  ) ) )  hC hD h(C  ^ D)  where A and B are conjunctions of literals and C and D are conjunctions of disjunctions of literals.
5  start  ^g  )  _r b=1  ka  )  h  a=1  ^g  lb  _r  (an initial clause)  lb  (a step clause)  b=1  ka  ) }l  a=1  true true  ) )  _r  (a sometime clause) mib  (a Ki aclause)  lb  (a literal clause)  b=1 r  _  b=1  Figure 1: Clauses in SNFK 4.1.2  Translation to Normal Form  The translation to SNFK is carried out by renaming complex subformulae with new propositional variables and linking the truth of the subformula to that of the proposition at all moments.
Temporal operators are removed using their fixpoint definitions.
Classical and temporal equivalences (see for example [5]) are also used to get formulae into the correct format.
See [4, 7] for more details.
4.2 Resolution Rules The resolution rules presented are split into four groups: those concerned with initial resolution, modal resolution, step resolution and temporal resolution.
As well as the resolution rules presented, simplification and subsumption also takes place.
So for example the step clause a ) h(b _ b _ c) is automatically rewritten as a ) h(b _ c).
Initial Resolution follows  An initial clause may be resolved with either a literal clause or an initial clause as  [IRES1]  true start start  ) ) )  _ l) (B _ :l) (A _ B) (A  [IRES2]  start start start  ) ) )  _ l) (B _ :l) (A _ B) (A  Modal Resolution During modal resolution we apply the following rules which are based on the modal resolution system introduced by Mints [19].
In the following we may only resolve two Ki clauses together if they relate to the same i, i.e.
we may not resolve a clause containing K1 with a clause containing K2 .
We may resolve a literal or modal literal and its negation or the formulae Ki l and Ki :l as we cannot both know something and know its negation.
[MRES1]  true true true  ) ) )  D_m D0 _ :m D _ D0  [MRES2]  6  true true true  ) ) )  D _ Ki l D0 _ Ki :l D _ D0  Next, as we have the T axiom, ` Ki p ) p, we can resolve formulae such as Ki l with :l (giving MRES3).
The rule MRES4 requires the function mod i (D0 ), defined below, and is justified due to the external Ki  operator), i.e, we distribute K into the second clause and operator surrounding each clause (due to the i resolve :Ki l with Ki l. The amod i a function ensures that during this distribution at most one Ki or :Ki operator applies to each literal due to the equivalences :Ki :Ki ' , Ki ' and :Ki Ki ' , :Ki ' in S5.
[MRES3]  ) ) )  true true true  D _ Ki l D0 _ :l D _ D0  [MRES4]  true true true  ) ) )  D _ :Ki l D0 _ l D _ mod i (D0 )  Definition 7 The function mod i (D), defined on disjunctions of literals or modal literals D, is defined as follows.
mod i (A _ B) = mod i (A) _ mod i (B) mod i (Ki l) = Ki l mod i (l) = :Ki :l mod i (:Ki l) = :Ki l Finally, we require the following rewrite rule to allow us to obtain the most comprehensive set of literal clauses for use during initial, step and temporal resolution [MRES5]  true true  ) )  L _ Ki l1 _ Ki l2 _ : : : L _ l1 _ l2 _ : : :  Here, L is a disjunction of literals.
Step Resolution aStepa resolution consists of the application of standard classical resolution to formulae representing constraints at a particular moment in time, together with simplification rules for transferring contradictions within states to constraints on previous states (standard simplification and subsumption rules are also applied).
The following resolution rules may be applied by resolving two step clauses or a step clause with a literal clause.
P ) h(A _ l) Q ) h(B _ :l) [SRES1] h(A _ B) (P ^ Q) )  [SRES2]  ) ) )  true Q Q  _ l) _ :l) h(A _ B)  (A  h(B  Once a contradiction within a state is found, the following rule can be used to generate additional literal clauses.
P ) hfalse [SRES3] true ) :P This rule states that if, by satisfying P, a contradiction is produced, then P must never be satisfied in any  :P moment.
The new constraint therefore represents Termination Each cycle of initial, modal or step resolution terminates when either no new resolvents are derived, or false is derived in the form of either start ) false or true ) false.
7  W  Temporal Resolution The temporal resolution rule is as follows, where we resolve a sometime clause, Q ) l, with a condition, nk=0 Ak , that implies :l in the next moment (known as a loop formula for :l).
n h :l k=0 Ak ) Q ) l  }  W  }n ^  )  Q  (  :Ai) W l  i=0  W  This resolvent states that once Q is satisfied then none of the Ai should be satisfied unless l is satisfied.
A systematic way of deriving nk=0 Ak from the set of step clauses such that  _n  Ak  )  h  :l  k=0  is described in [2] but is beyond the scope of this paper.
Translating the resolvent into SNFK we obtain the following clauses for each i where wl is a new proposition.
true ) :Q _ l _ :Ai true ) :Q _ l _ wl wl ) h(l _ :Ai ) wl ) h(l _ wl )  4.3 The temporal resolution algorithm Given any temporal formula 1.
Translate  to be shown unsatisfiable the following steps are performed.
into a set of SNFK clauses  s.  2.
Perform modal and step resolution (including simplification and subsumption) until either (a) true ) false is derived - terminate noting  unsatisfiable; or  (b) no new resolvents are generated - continue to step 3.  }
3.
Select an eventuality from the right hand side of a sometime clause within s , for example l. Search for loop formulae in :l and generate the appropriate resolvents.
If no new formulae have been generated try the next sometime clause otherwise (if new formulae have been generated) go to step 4.
If there are no eventualities for which new resolvents can be derived, go to step 5.
4.
Add the new resolvents to the clause-set and perform initial resolution until either (a) start ) false is derived - terminate noting  unsatisfiable; or  (b) no new resolvents are generated - continue at step 2.
5.
Perform initial resolution until either (a) start ) false is derived - terminate noting  unsatisfiable; or  (b) no new resolvents are generated -terminate declaring  8  satisfiable.
4.4 Correctness Firstly we can show that the transformation into SNF K preserves satisfiability.
Theorem 1 A KL(n) formula A is satisfiable if, and only if, 0 [Aa is satisfiable (where 0 is the translation into SNFK ).
Proofs analogous to those in [4, 7] will suffice.
Theorem 2 (Soundness) Let S be a satisfiable set of SNFK clauses and T be the set of clauses obtained from S by an application of one of the resolution rules.
Then T is also satisfiable.
This can be shown by showing an application of each resolution rule preserves satisfiability.
(see [4]) Theorem 3 (Completeness) If a set of SNFK clauses is unsatisfiable then it has a refutation by the temporal resolution procedure given in this paper.
This is carried out by constructing a graph to represent all possible models for the set of clauses.
Deletions in the graph represent the application of of the resolution rules.
An empty graph corresponds with the generation of false (see [4]).
5 Specifying the Needham-Schroeder Protocol in KL(n) In this section, we will use KL(n) to specify the Needham-Schroeder protocol.
In particular, we will provide axioms describing the key aspects of both the system and the protocol.
In order to do this we use the following syntactic conventions.
Let M1 and M2 be variables over messages, Key be a variable over keys and X ; Y ; : : : be variables over agents.
Moreover, for every agent, X, we assume there are keys pub key(X ) and priv key(X ), while in this protocol A and B are constants representing two specific agents.
We identify the following predicates:          send(A; Msg; Key) is satisfied if agent A sends message Msg encrypted by Key; rcv(A; Msg; Key) is satisfied if agent A receives message Msg encrypted by Key; Msg(M1 ) is satisfied if M1 is a message; val pub key(X ; V ) is satisfied if the value public key of X is V val priv key(X ; V ) is satisfied if the value public key of X is V val nonce(NA ; V ) is satisfied if the value of nonce NA is V contains(M1 ; M2 ) is satisfied if the message M2 is contained within M1 .
To simplify the description, we allow quantification and equality over the sets of agents, messages and keys.
As we assume a finite set of agents, messages and keys this logic remains essentially propositional.
There are a few general assumptions:     initially, only agent A knows the content of its own nonce NA and only B knows the content of its own nonce NB ; if agent A sends messages containing its nonce or Bas nonce they are encrypted with Bas public key; 9        if agent B sends messages containing its nonce or Aas nonce they are encrypted with Aas public key; messages sent are not guaranteed to arrive at the required destination; if a message does arrive at an agent, then that message must have been previously sent by an agent; knowledge of messages persists, i.e.
agents do not forget; if a message is received, and the receiver knows the private key required, then the receiver will know the content of the message;  We allow a simple representation of public and private keys in terms of the unary functions apub key(X )a and apriv key(X )a.
All agents know the public keys of all other agents, but each agentas private key is only known by that agent.
There is also an axiom below (Axiom 14) explaining that, if an agent receives a message, and that message is encrypted by a public key whose private key the agent knows, then the agent will then know the contents of the message.
5.1 Specifying Structural Assumptions We begin with various structural assumptions concerning keys and message contents.
1.
8X ; Key; M1 : send(X ; M1 ; Key) ) :contains(M1 ; priv key(X )) a agents will not reveal their private key to others pub key(X ; V1 ) , val pub key(X ; V1)a ^ priv key(X ; V2 ) , val priv key(X ; V2 )a ^ [val nonce(X ; V1 ) , val nonce(X ; V1)a a the public keys, private keys and nonces of all the agents remain the same during the protocol  2.
8X ; V1 ; V2 :  [val  [val  3.
8X ; Y ; V : (val pub key(X ; V ) ^ val pub key(Y ; V )) ) X a no two agents have the same public keys  =  Y)  4.
8Key; M1 : (send (A; M1 ; Key) ^ [contains(M1 ; NA ) _ contains(M1 ; NB )a) ) (Key = pub key(B)) a if agent A sends out messages containing NA or NB the messages must be encrypted with Bas public key.
5.
8Key; M2 : (send(B; M2 ; Key) ^ [contains(M2 ; NA ) _ contains(M2 ; NB )a) ) (Key = pub key(A)) a if agent B sends out messages containing NA or NB the messages must be encrypted with Aas public key.
5.2 Specifying Scenario Assumptions As we will be concerned with one particular scenario, namely the simple interaction between agents given above, we instantiate message contents, keys and names for this scenario.
10  6.
Msg(M1 ) , ((M1 = m1 ) _ (M1 = m2 ) _ (M1 = m3 )) a in this particular scenario, we just use three messages, m1 , m2 and m3 .
7.
8X ; Y ; Z :  , ((X = A) _ (X = NA )))^ , ((Y = NA ) _ (Y = NB )))^ (contains(m3 ; Z ) , (Z = NB )) a message m1 contains only NA and A, message m2 contains only NB and NA and message m3 contains only NB (contains(m1 ; X ) (contains(m2 ; Y )  8. start )  val priv key(A; av) ^ val priv key(B; bv ) ^ val priv key(C; cv )^ :val priv key(A; bv ) ^ :val priv key(A; cv ) ^ :val priv key(B; av)^ :val priv key(B; cv ) ^ :val priv key(C; av ) ^ :val priv key(C; bv )^ val pub key(A; a) ^ val pub key(B; b) ^ val pub key(C; c))^ :val pub key(A; b) ^ :val pub key(A; c) ^ :val pub key(B; a)^ :val pub key(B; c) ^ :val pub key(C; a)) ^ :val pub key(C; b))^ val nonce(NA ; an ) ^ val nonce(NB ; bn ) ^ val nonce(NC ; cn )^ :val nonce(NA ; bn ) ^ :val nonce(NA; cn ) ^ :val nonce(NB; an )^ :val nonce(NB ; cn ) ^ :val nonce(NC ; an ) ^ :val nonce(NC ; bn )^  5.3 Specifying Basic Knowledge Axioms We here specify the basic attributes of an agentas knowledge.
9. start ) 8X :(9V : KX val nonce(NX ; V )) ^ [8Y ; Z : (Y 6= X ) ) :KY val nonce(NX ; Z )a a initially agents only know their own nonces.
10.
8X : (9V :KX val priv key(Y ; V ) , (X  =  Y ))  a agents only know their own private keys  11.
8X : KX val pub key(A; a) ^ KX val pub key(B; b) ^ KX val pub key(C; c) a all agents know all the public keys.
gK  12.
8X ; N ; V : KX val nonce(N ; V ) )  X val  13.
8X ; Y ; V : KX val priv key(Y ; V ) ) they know  gK  nonce(N ; V ) a agents never forget nonces they know  X val  priv key(Y ; V )  a agents never forget private keys  5.4 Specifying Communication Axioms We now specify the communication between agents, and how this affects the agentas knowledge.
For conha, meaning in the previous g f venience, we allow the use of past-time temporal operators, in particular a de moment in time, and a  a, meaning at some time in the past.
These operators have the following semantics.  }
hM ; (t; u)i j= defghA hM ; (t; u)i j= } A  iff iff  u > 0 and hM ; (t; u  i j= A  1)  9 2 N such that (0  u0 < u) u0  11  and hM ; (t; u0 )i j= A  14.
8X ; M1 ; N1 g((Msg(M1 ) ^ contains(M1 ; N1 )) ) (9V1 KX val nonce(N1 ; V1 ) , g[K val nonce(N ; V ) _ (9Y :9V : rcv(X ; M ; pub key(Y )) ^ K val priv key(Y ; V ))a)) f e d  X X 1 1 1 a for all moments except the first moment if M1 is a message which contains N1 an agent knows the content of N1 either if it already knew the content of N1 , or if it received an encrypted version of M1 that it could decode.  }
15.
8X ; Key; M1 : rcv(X ; M1 ; Key) ) 9Y :  send(Y ; M1 ; Key) a if an agent receives a message, then there was some agent that previously sent that message 16.
8X ; Key; M1 ; N1 (send(X ; M1 ; Key) ^ contains(M1 ; N1 )  rcv(X ; M1 ; Key)  )  }  9V1 : KX val nonce(N1 ; V1 ) _  a if an agent sends a message M1 encrypted with Key, then it must either know the contents M1 or just be forwarding the encrypted message as a whole  Note that there is no axiom such as  send(: : : ) )  }rcv(: : :)  as messages cannot be guaranteed to be delivered.
However, we do have a related axiom (Axiom 15) which says that, if a message is received, then some other agent must have sent it previously.
5.5 Axioms in Normal Form We can translate all the above axioms into the normal form.
As an example, we will assume three agents A, B and C and translate axioms 6, 7, 9, 10, and 11, as in Fig.
2.
We can also write axiom 14 into normal form as follows for all agents X.
( h(Msg(M1 )  9V  (  1  ^ contains(M hK  X val  1 ; N1 )) ) nonce(N1 ; V1 )  ; N1 )) ) (9V1 hKX val nonce(N1 ; V1 )  ( h(Msg(M1 )  ^ contains(M  1  , ,  nonce(N ; V ) _ 9Y :9V rcv(X ; M ; pub key(Y )) ^ KX val priv key(Y ; V ))a))  [KX val  1  (  1  1  nonce(N ; V ) _ 9V :(rcv(X ; M ; pub key(A)) ^ KX val priv key(A; V ))_ 9V :(rcv(X ; M ; pub key(B)) ^ KX val priv key(B; V ))_ 9V :(rcv(X ; M ; pub key(C)) ^ KX val priv key(C; V )))a))  [KX val  1  (  1  1  1 1  We provide a version below where X is instantiated as B, M1 is instantiated as m1 , N1 is instantiated as NA and d1 , d2 , d3 , d4 , e1 , e2 , e3 , f1 , f2 ,and f3 are new propositions.
Note due to axiom 14 containing the , operator we must rename formulae below this using , rather than just ).
In the following d1 is a new name for the following d1  , (rcv(B; m ; pub key(A)) ^ (KB val priv key(A; av ) _ KB val priv key(A; bv ) _ KBval priv key(A; cv ))) 1  and similarly for d2 and d3 where A in the above are replaced by B and C respectively.
d4 is a new name for the disjunction of these formulae i.e.
d4 , d1 _ d2 _ d3 f1 is a new name for Bas knowledge that the value of NA is an i.e.
f1  , KB val nonce(NA ; an )  12  6a: 6b: 6c: 7a: 7b: 7c: 7d : 7e: 9a: 9b: 9c: 9d : 9e: 9f : 9g: 9h: 9i: 10a: 10b: 10c: 10d : 10e: 10f : 10g: 10h: 10i: 10j: 10k: 10l: 10m: 10n: 10o: 10p: 10q: 10r: 10s: 10t: 10u: 11a: 11b: 11c: 11d : 11e: 11f : 11g: 11h: 11i:  true ) Msg(m1 ) true ) Msg(m2 ) true ) Msg(m3 ) true ) contains(m1 ; A) true ) contains(m1 ; NA ) true ) contains(m2 ; NA ) true ) contains(m2 ; NB ) true ) contains(m3 ; NB ) start ) (KA val nonce(NA; an ) _ KA val nonce(NA ; bn ) _ KA val nonce(NA ; cn )) start ) (KB val nonce(NB; an ) _ KB val nonce(NB ; bn ) _ KB val nonce(NB ; cn )) start ) (KC val nonce(NC ; an ) _ KC val nonce(NC ; bn ) _ KC val nonce(NC ; cn )) start ) :KA val nonce(NB ; bn ) start ) :KA val nonce(NC ; cn ) start ) :KB val nonce(NA ; an ) start ) :KB val nonce(NC ; cn ) start ) :KC val nonce(NA; an ) start ) :KC val nonce(NB; bn ) true ) KA val priv key(A; av) _ KA val priv key(A; bv) _ KA val priv key(A; cv ) true ) KB val priv key(B; av) _ KB val priv key(B; bv) _ KB val priv key(B; cv ) true ) KC val priv key(C; av ) _ KC val priv key(C; bv ) _ KC val priv key(C; cv ) true ) :KA val priv key(B; av ) true ) :KA val priv key(B; bv ) true ) :KA val priv key(B; cv ) true ) :KA val priv key(C; av ) true ) :KA val priv key(C; bv ) true ) :KA val priv key(C; cv ) true ) :KB val priv key(A; av ) true ) :KB val priv key(A; bv ) true ) :KB val priv key(A; cv ) true ) :KB val priv key(C; av ) true ) :KB val priv key(C; bv ) true ) :KB val priv key(C; cv ) true ) :KC val priv key(A; av ) true ) :KC val priv key(A; bv ) true ) :KC val priv key(A; cv ) true ) :KC val priv key(B; av ) true ) :KC val priv key(B; bv ) true ) :KC val priv key(B; cv ) true ) KA val pub key(A; a) true ) KA val pub key(B; b) true ) KA val pub key(C; c) true ) KB val pub key(A; a) true ) KB val pub key(B; b) true ) KB val pub key(C; c) true ) KC val pub key(A; a) true ) KC val pub key(B; b) true ) KC val pub key(C; c)  Figure 2: Normal form translation of axioms 6, 7, 9, 10, and 11  13  where f2 and f3 are similarly defined except an is replaced by bn and cn respectively.
The right hand side of the implication becomes  9V  (  1  hK  X val  ,  nonce(N1 ; V1 )  (KX val  nonce(N1 ; V1 ) _ d4 ))  Removing the existential quantifier we obtain  ^ contains(M ; N )) ) (e _ e _ e )  ( h(Msg(M1 )  1  where e1  )(  hf  1  1  1  2  3  , (f _ d )) 1  4  and similarly for e2 and e3 where f1 is replaced by f2 and f3 respectively.
14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14  :e ^ :e ^ :e ) ) h(:Msg(m ) _ :contains(m ; NA )) (e ^ :f ^ :d ) ) h:f (e ^ f ) ) hf (e ^ d ) ) hf (e ^ :f ^ :d ) ) h:f (e ^ f ) ) hf (e ^ d ) ) hf (e ^ :f ^ :d ) ) h:f (e ^ f ) ) hf (e ^ d ) ) hf true ) :f _ KB val nonce(NA ; an ) true ) f _ :KB val nonce(NA ; an ) true ) :f _ KB val nonce(NA ; bn ) true ) f _ :KB val nonce(NA ; bn ) true ) :f _ KB val nonce(NA ; cn ) true ) f _ :KB val nonce(NA ; cn ) true ) :d _ d _ d _ d true ) d _ :d true ) d _ :d true ) d _ :d true ) :d _ rcv(B; m ; pub key(A)) true ) :d _ (KB val priv key(A; av ) _ KB val priv key(A; bv ) _ KB val priv key(A; cv ) true ) :rcv(B; m ; pub key(A)) _ d _ :KB val priv key(A; av ) true ) :rcv(B; m ; pub key(A)) _ d _ :KB val priv key(A; bv ) true ) :rcv(B; m ; pub key(A)) _ d _ :KB val priv key(A; cv ) true ) :d _ rcv(B; m ; pub key(B)) true ) :d _ (KB val priv key(B; av ) _ KB val priv key(B; bv ) _ KB val priv key(B; cv ) true ) :rcv(B; m ; pub key(B)) _ d _ :KB val priv key(B; av ) true ) :rcv(B; m ; pub key(B)) _ d _ :KB val priv key(B; bv ) true ) :rcv(B; m ; pub key(B)) _ d _ :KB val priv key(B; cv ) true ) :d _ rcv(B; m ; pub key(C)) true ) :d _ (KB val priv key(C; av ) _ KB val priv key(C; bv ) _ KB val priv key(C; cv ) true ) :rcv(B; m ; pub key(C)) _ d _ :KB val priv key(C; av ) true ) :rcv(B; m ; pub key(C)) _ d _ :KB val priv key(C; bv ) true ) :rcv(B; m ; pub key(C)) _ d _ :KB val priv key(C; cv ) (  1  2  3  1  1  1  1  4  1  4  2  2  1  4  2  2  2  3  1  1  1  2  1  2  4  3  2  4  3  3  3  3  3  4  3  1  1  2  2  3  3  4  1  2  4  1  4  2  4  1  3  1  1  1  1  1  1  1  1  2  1  2  1  2  1  2  1  2  3  1  3  1  3  1  3  1  3  14  3  Note further that the following is a consequence of axiom 15.
15a:  start ) :rcv(B; m1 ; pub key(B))  6 Verifying Properties of the Specification Once we have the above axioms relating to the specific scenario, we can attempt to prove various statements.
Refutations tend to be quite long, so we will only provide detail for the simpler examples.
6.1 Bas Knowledge on Receipt of NA The first example will capture the statement once B receives the nonce of A encoded by Bas public key then B knows the nonce of A This can be translated into KL(n) as (rcv(B; m1; pub  ) h9V : KB val nonce(NA ; V ))  key(B))  or removing the existential quantifier as follows.
(rcv(B; m1; pub  key(B)) )  h(K  B val  nonce(NA ; an ) _ KB val nonce(NA ; bn ) _ KB val nonce(NA ; cn ))  We will now show how to establish this using the resolution method outlined earlier.
To prove the above, we negate the statement and derive the following set of clauses.
p1: start p2: x p3: true p4: y p5: true p6: true p7: true  ) ) ) ) ) ) )  x  }y  :y _ rcv(B; m ; pub key(B)) 1  hz  :z _ :KBval nonce(NA ; an ) :z _ :KBval nonce(NA ; bn ) :z _ :KBval nonce(NA ; cn )  In the following we will need an instantiation of axiom 13 KB val priv key(B; bv ) )  hK  B val  priv key(B; bv )  which is translated into normal form as follows 13 13 13  p1 true true  ) hp ) :p _ KBval priv key(B; bv ) ) p _ :KBval priv key(B; bv ) 1  1  1  The proof takes place as follows.
15  p8: p9:  p10: p11: p12: p13: p14: p15: p16: p17: p18: p19: p20: p21: p22: p23: p24: p25: p26: p27:  ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )  true true true (e1 ^ d4 ) (e2 ^ d4 ) (e3 ^ d4 ) (e1 ^ d4 ^ y) (e2 ^ d4 ^ y) (e3 ^ d4 ^ y) true true true (:e1 ^ :e2 ^ :e3 ) (:e1 ^ :e2 ^ :e3 ) true true true true true true  p28: p29: p30:  :z _ : f :z _ : f :z _ :f h:z h:z h:z  [ax14; p5  1 2 3  hfalse hfalse hfalse  :e _ :d _ :y) :e _ :d _ :y) (:e _ :d _ :y) h:contains(m ; N ) A (  1  4  (  2  4  3  4  1  hfalse  _e _e ) _ e _ :d _ :y) (e _ :d _ :y) (:d _ :y) (:d _ :y) (:rcv(B; m ; pub key(B))_ :KBval priv key(B; bv ) _ :y) ) (:KBval priv key(B; bv ) _ :y) ) (:p _ :y) ) h:y  true true p1  (e1  2  (e2  3  3  3  4  4  4 2  1  1  MRES1a [ax14; p6 MRES1a [ax14; p7 MRES1a [ax14; p8 SRES2a [ax14; p9 SRES2a [ax14; p10 SRES2a [p4; p11 SRES1a [p5; p12 SRES1a [p6; p13 SRES1a [p14 SRES3a [p15 SRES3a [p16 SRES3a [ax6a; ax14 SRES2a [ax7b; p20 SRES2a [p21 SRES3a [p17; p22 MRES1a [p18; p23 MRES1a [p19; p24 MRES1a [ax14; p25 MRES1a [ax14; p26  MRES1a [p3; p27 MRES1a [ax13; p28 MRES1a [ax13; p29 SRES2a  we can apply the temporal resolution rule with clauses p30 and axioms 13 together giving p1  )  h  :y  for resolution with p2: p31: p32: p33: p34: p35: p36: p37: p38:  true true wy wy true true true true  p39: p40: p41: p42:  start start start start  ) ) ) ) ) ) ) )  :x _ y _ :p :x _ y _ wy h(y _ :p ) h(y _ w ) l :x _ y _ :KBval priv key(B; bv ) :x _ y _ KB val priv key(B; av ) _ KBval priv key(B; cv ) :x _ y _ val priv key(B; av ) _ val priv key(B; cv ) :x _ rcv(B; m ; pub key(B)) _ val priv key(B; av ) _val priv key(B; cv ) ) :x _ val priv key(B; av ) _ val priv key(B; cv ) ) :x _ val priv key(B; cv ) ) :x ) false 1  1  1  [p30; ax13; p2  TRESa TRESa [p30; ax13; p2 TRESa [p30; ax13; p2 TRESa [ax13; p31 MRES1a [ax10b; p35 MRES1a [p36 SRES5a [p30; ax13; p2  [p37; p3  MRES1a IRES1a [ax8; p39 IRES2a [ax8; p40 IRES2a [p1; p41 IRES2a [ax15a; p38  6.2 Cas Ignorance A key part of this protocol is that information is transferred between agents A and B without agent C ever being able to intercept sensitive information.
We can verify this by showing that, in the scenario above, C 16  will never know the value of Aas nonce, i.e.
8V : :KC value nonce(NA ; V ) Rather than giving a full refutation, we will indicate how this can be proved.
As axiom 14 states, for an agent (in this case C) to know the value of NA then either it knew it originally, or it received a message that contained it.
In the first case, agent C did not know the value of NA originally.
In the second case, by axioms 3 and 4, agents A and B only ever send out messages containing NA encrypted by pub key(B) or pub key(A), respectively.
Consequently, C can never know the value of NA .
6.3 Confirmation of Bas Knowledge Once A receives m2 (which, in turn, contains NA ) back, then it can infer that B knows NA , i.e.
rcv(A; m2 ; pub key(A)) )  hK  A KB NA )  Recall that axiom 15 states that if an agent receives a message, then there must have been a previous corresponding send.
Since A did not send the message, the only choice is  } send(B; m ; pub key(A)) _ } send(C; m ; pub key(A)) 2  2  However, in order to send a message, an agent must know the contents of that message.
Since we know (from above) that C doesnat know the value of NA , then C can not have sent the message.
Consequently, B must have sent the message and so A can infer that KB NA .
Form this, we can infer KA KB NA .
7 Loweas attack on the Protocol In the following section we describe an attack on the protocol as suggested by Lowe [17].
First we outline some general assumptions about the intruder.
The intruder is able to:-        overhear and intercept messages being passed in the system, decrypt messages that are encrypted with its own public key, then it can learn them, introduce new messages with the nonces it knows, replay any message he has seen, even if he was not able to decrypt it.
if the intruder cannot learn the message, then it can remember the encrypted part and can pass it to other agents.
Lowe considers a situation where A could run the protocol with an enemy C, then C could pretend is he A and could start a new run of the protocol with B [17].
The situation could be described as follows: Message Message 1 Message 2 Message 3 Message 4 Message 5 Message 6  Direction Contents A!C: (NA ; A)pub key(C) C(A) !
B : (NA ; A)pub key(B) B !
C(A) : (NA ; NB )pub key(A) C(A) !
A : (NA ; NB )pub key(A) A!C: (NB )pub key(C) C(A) !
B : (NB )pub key(B) 17  In message 1 A starts running the protocol with C, sending the nonce NA , encrypted with Cas public key.
In message 2 the intruder impersonates A (denoted above as C(A)) to start a run of the protocol with B, sending the same nonce that A sent before but encrypted with Bas public key.
Then B replies by sending a new nonce NB to C, but it encrypts it with Aas public key (message 3).
C cannot decrypt the message, but it forwards it to A in order to obtain NB (message 4).
In message 5 A sends NB to C with Cas public key, therefore he now knows NB .
Then he returns NB to B making him believe that the protocol has been correctly run with A.
However, now A, B and C all know the nonces NA and NB .
In the previous set of axioms this attack is disallowed due to axiom 4, i.e.
A is only allowed to send messages containing NA or NB enctypted in Bas public key.
If axiom 4 is not present, then agent A is permitted to send out a message containing NA with a public key different from that of B. Consequently, we are not then able to prove 8V : :KC value nonce(NA ; V ) and so we cannot be sure that B, and only B, knows NA .
8 Concluding Remarks We have used a combination of non-classical logics, in particular the fusion of linear time temporal logic with the multi-modal logic S5 (representing knowledge) to represent and reason about security protocols.
In particular we have specified the Needham-Schroeder Protocol using temporal logics of knowledge giving axioms relating to communication mechanisms, knowledge etc.
We have proved various properties of this specification by using a resolution calculus for this logic and briefly discussed a well known attack on this protocol.
Future work involves the specification and verification of other protocols in this logic.
The consideration of further protocols would also help us identify other suitable combinations of temporal and modal logics.
We would like to develop tools to carry out the verification of the required properties.
At the moment we have a prototype resolution theorem prover for the single modal version of temporal logics of knowledge but this needs extending to deal with the multi-modal case in order to prove theorems automatically.
Further, we would like to develop programs that animate the reason for proof failure to assist the designers of protocols to detect flaws.
References [1] M. Burrows, M. Abadi and R. Needham, A Logic of Authentication, ACM Transactions on Computer Systems, vol.
8, p. 18a36, 1990.
[2] C. Dixon.
Temporal Resolution using a Breadth-First Search Algorithm.
Annals of Mathematics and Artificial Intelligence, 22:87a115, 1998.
[3] C. Dixon and M. Fisher.
Resolution-Based Proof for Multi-Modal Temporal Logics of Knowledge.
In S. Goodwin and A. Trudel, editors, Proceedings of TIME-00 the Seventh International Workshop on Temporal Representation and Reasoning, Cape Breton, Nova Scotia, Canada, July 2000.
IEEE Press.
[4] C. Dixon, M. Fisher, and M. Wooldridge.
Resolution for Temporal Logics of Knowledge.
Journal of Logic and Computation, 8(3):345a372, 1998.
[5] E. A. Emerson.
Temporal and Modal Logic.
In J. van Leeuwen, editor, Handbook of Theoretical Computer Science, pages 996a1072.
Elsevier Science Publishers B.V.: Amsterdam, The Netherlands, 1990.
18  [6] M. Fisher.
A Resolution Method for Temporal Logic.
In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence (IJCAI), pages 99a104, Sydney, Australia, August 1991.
Morgan Kaufman.
[7] M. Fisher, C. Dixon, and M. Peim.
Clausal Temporal Resolution.
ACM Transactions on Computational Logic, 2(1):12a56, January 2001.
[8] M. Fisher and M. Wooldridge.
On the Formal Specification and Verification of Multi-Agent Systems.
International Journal of Cooperative Information Systems, 6(1), January 1997.
[9] J. Glasgow, G. MacEwen, and P.Panangaden.
A Logic to Reason About Security.
ACM Transactions on Computer Systems, 10(3):226a264, August 1992.
[10] L. Gong, R. Needham and R. Yahalom, Reasoning about Belief in Cryptographic Protocol Analysis, Proc.
IEEE Symp.
on Research in Security and Privacy, p. 234a248, 1990.
[11] D. Gabbay, A. Pnueli, S. Shelah, and J. Stavi.
The Temporal Analysis of Fairness.
In Proceedings of the Seventh ACM Symposium on the Principles of Programming Languages, pages 163a173, Las Vegas, Nevada, January 1980.
[12] R. Fagin, J. Y. Halpern, Y. Moses, and M. Y. Vardi.
Reasoning About Knowledge.
MIT Press, 1995.
[13] J. Y. Halpern.
Using reasoning about knowledge to analyze distributed systems.
Annual Review of Computer Science, 2, 1987.
[14] J.Y.
Halpern and L.D.
Zuck, A Little Knowledge Goes a Long Way: Simple Knowledge-Based Derivations and Correctness Proofs for a Family of Protocols, Proc.
6th ACM Symp.
on Principles of Distributed Computing, 1987, p. 268a280.
[15] J. Y. Halpern and M. Y. Vardi.
The Complexity of Reasoning about Knowledge and Time.
I Lower Bounds.
Journal of Computer and System Sciences, 38:195a237, 1989.
[16] Z.
Manna and A. Pnueli.
The Temporal Logic of Reactive and Concurrent Systems: Specification.
Springer-Verlag, New York, 1992.
[17] G. Lowe.
Breaking and Fixing the Needham-Schroeder Public-key Protocol Using csp and fdr.
In T. Margaria and B. Steffen, editors, Tools and Algorithms for the Construction and Analysis of Systems: second international workshop, TACAS a96, volume 1055 of Lecture Notes in Computer Science, pages 147a166.
Spinger, 1996.
[18] J.-J.
C. Meyer and W. van der Hoek.
Epistemic Logic for Computer Science and Artificial Intelligence, volume 41 of Cambridge Tracts in Theoretical Computer Science.
Cambridge University Press, 1995.
[19] G. Mints.
Gentzen-Type Systems and Resolution Rules, Part I: Propositional Logic.
Lecture Notes in Computer Science, 417:198a231, 1990.
[20] R.M.
Needham and M.D.
Schroeder.
Using Encryption for Authentication in Large Networks of Computers.
Communications of the ACM, 21:993a999, 1978.
[21] D. A. Plaisted and S. A. Greenbaum.
A Structure-Preserving Clause Form Translation.
Journal of Symbolic Computation, 2(3):293a304, September 1986.
19  [22] F. Stulp and R. Verbrugge, A knowledge-based algorithm for the Internet protocol TCP, to appear in the Bulletin of Economic Research, 2001.
Also at http://tcw2.ppsw.rug.nl/prepublications [23] P. Syverson.
Adding Time to a Logic of Authentication.
In Proceedings of the First ACM Conference on Computer and Communications Security, pages 97a101.
ACM Press, 1993.
20
Experimental Evaluation of Search and Reasoning Algorithms, 1994.
11] Debasis Mitra, Padmini Srnivasan, Mark L. Gerard, and Adrian E. Hands.
A probabilistic interval constraint problem: fuzzy temporal reasoning.
In World Congress on Computational Intelligence (FUZZ-IEEE'94), 1994.
12] Raul E. Valdes-Perez.
The satisability of temporal constraint networks.
In Proceedings of AAAI, 1987.
13] Peter van Beek.
Ph.D. dissertation: Exact and Approximate Reasoning about Qualitative Temporal Relations.
University of Alberta, Ed-  monton, Canada, 1990.
5 Conclusion In this article we have described some advantages of having the capability to generate all possible consistent singleton models (CSM), from a given set of temporal intervals and some incomplete constraints between them.
The problem of nding all consistent singleton models is plagued with two sources of expected exponential growth with respect to the number of temporal entities in the data base.
First, the problem of nding one consistent model is itself NP-complete.
Second, intuition suggests that the number of models should increase exponentially with respect to the number of nodes.
The latter apprehension is possibly not appropriate, at least from our preliminary experimental results with randomly generated temporal networks.
Number of models actually reduces as constraining level increases with the number of nodes6].
Ladkin's experiment4] has also allayed the fear about hardness of the problem of nding one consistent model.
An algorithm is described here which systematically searches and prunes search space to achieve not only one CSM, but all CSM's.
The systematic nature of its searching (and pruning) seem to be the key to its eciency.
Signicance of this algorithm in temporal reasoning and its capability to nd all CSM's in reasonable amount of time is the focus of this article.
Theoretical implications involve the capability of studying global consistency problem empirically at much greater depth than has been done so far.
Our mechanism also makes us capable to handle non-monotonicity (of a particular type) in temporal reasoning.
Other theoretical implications are also discussed in this article.
There are quite a few implications of this algorithm from the point of view of applied research.
Some of the most important implications involve temporal data base, critical-domain application areas, prediction/explanation, generating all feasible (interval-based) plans, plan recognition, and image recognition There are many other implications which have not been discussed in this article (for example, parallelizability of the algorithm and associated speed up).
Generating all CSM's were considered to be a very hard problem.
This is the main reason why no one (as far as we know) has looked into the signicance of having all CSM's available.
Higher eciency of our algorithm makes such modeling feasible, and hence studying the signicance  of such modeling necessary.
References 1] J. F. Allen.
Maintaining knowledge about temporal intervals.
Communications of the ACM, 26(11):510{521, 1983.
2] J. F. Allen.
Towards a general theory of action and time.
Arti cial Intelligence, 26(2), 1984.
3] Peter Cheeseman, Bob Kanefsky, and William M. Taylor.
Where the really hard problems are.
In Proccedings of International Joint Conference on Arti cial Intelligence, pages 331{337, 1991.
4] Peter B. Ladkin and Alexander Reineeld.
Eective solution of qualitative interval constraint problems.
Art cial Intelligence, 57:105{124, 1992.
5] David Mitchell, Bart Seleman, and Hector Levesque.
Hard and easy distributions of sat problems.
In Proceedings of the Tenth National Conference of Am.
Asso.
of Arti cial Intelligence, 1992.
6] Debasis Mitra.
Ph.D. dissertation: Eciency issues in temporal reasoning.
University of  Southwestern Louisiana, Lafayette, 1994.
7] Debasis Mitra and Rasiah Loganantharaj.
All feasible plans using temporal reasoning.
In Proceedings of AIAA/NASA conference on intelligent robotics in  eld, factory, services and space, '94, 1994.
8] Debasis Mitra and Rasiah Loganantharaj.
Efcent exact algorithm for nding all consistent singleton labelled models.
In Proceedings of IEEE Robotics and Automation conference, Nice, France, 1991.
9] Debasis Mitra and Rasiah Loganantharaj.
Weak-consistency algorithms for intervalbased temporal constraint propagation.
In AAAI'94 workshop note on Spatial and Temporal reasoning, 1994.
10] Debasis Mitra, Nabendu Pal, and Rasiah Logananthatraj.
Complexity studies of a temporal constraint propagation algorithm: a statistical analysis.
AAAI'94 workshop note on  example, we may study how such models grow in planning domain with respect to the structural parameters mentioned above.
(c) Studying real-life problems: Instead of studying average number of models (or average-case complexity) for randomly generated networks, as suggested in previous paragraphs, one can study networks from classes of real life of temporal consistency-problems, e.g., planning, scheduling and diagnosis.
(d) Topological distance between local consistency problems and global-consistency problem: Local consistency problems and global-consistency problem form a hieararchy9].
An ecient globalconsistent algorithm makes it feasible to empirically study the topological distance6] between global-consistency problem and any other localconsistency problem (e.g.
3-consistency), for which ecient polynomial algorithm already exists.
Such study will expose a quantitative measure of the usefulness of these local consistency algorithms.
(e) Predictability of complexity: In a recent set of experimental studies we are trying to have the capability of predicting run-time of an algorithm based on the structure of a problem instance10].
In addition to predicting time-complexity, now we could predict the number of CSM's a problem instance can have, which is more related to the problem-structure.
In order to gain such statistical prediction power, we need to experiment with our algorithm, in line with such experiments for empirically studying time-complexity.
(f) Non-monotonic temporal reasoning: The algorithm requires maintenance of all CSM's at each stage of updating the network.
This enables one to go back and redo the reasoning from any stage, in case some information on any old arc gets changed in future.
This non-monotonicity is not feasible unless all CSM's are preserved at each stage.
4 Practical implications of the algorithm Practical implication of being able to incrementally developing models, and having all such models available in a data base are immense.
Some of them are discussed in this section.
(a) Temporal data base: A temporal data base will grow incrementally as and when a new temporal assertion comes to the knowledge of the user.
So far temporal reasoning community has not paid much attention to this fact.
The present algorithm is particularly tuned to such application for ecient incremental growth, making it a very strong candidate for utilization in temporal data bases.
(b) Critical applications: In any critical application temporal queries on multiple arcs have to be answered in real-time.
Most of the temporal reasoning scheme of present day are not suitable for handling query involving multiple temporal relations in real time.
Having all CSM's available in data base makes this feasible.
(c) Prediction of possible scenarios: Having all CSM's available again makes one capable of nding out a suitable model for the purpose of prediction, such that the chosen model is consistent with the currently supplied partial model.
(d) Explanation: Similar to prediction, developing scenarios involving past temporal assertions can act as explanations for some phenomena which are occurring at present or which have occurred in the past.
(e) All feasible plans: A special case of predictive capability of temporal reasoning system is that of generating all feasible plans7].
(f) Plan recognition: A special advantage of having a data base of all feasible plans available, is to recognize a plan based on executed operations up to a `present' time.
(g) Image recognition: If images are represented in terms of spatial relations (a two dimensional extension of temporal relations) between objects in the image (as number of possible models derived from incomplete information), then image recognition problem becomes a problem of search within models.
(h) Incorporating uncertainty in temporal reasoning: Some work has been done recently in studying propagation of fuzzy plausibility values of temporal constraints while running 3-consistency algorithm11].
In the temporal reasoning system for nding all CSM's, presented in this article, such uncertainty propagation will result in an ordering on all possible models, and possibly eliminating some weak models, thus gaining further pruning power for the sake of eciency, paying for the overhead incurred in uncertainty propagation.
2 Consistent Singleton Modeling Algorithm The algorithm described here for nding all consistent temporal models is based on forward pruning technique which we have developed sometime back8].
The formalism used is based on constraint propagation technique, which was rst developed for interval based-temporal reasoning by Allen1].
In this formalism propositional temporal entities (on time-intervals) are represented as nodes in a graph, and relations between them are represented as directed arcs between them.
Labels on each arc express possible temporal relations between two end nodes.
There are 13 primitive temporal relations feasible between time intervals1].
Disjunction of them forms an incomplete information between two temporal entities.
Three types of such labels are of special interest.
(1) Disjunction of all 13 primitive relations signies lack of any information (or constraint) between two nodes.
(2) A null relation as any label in the network signies contradiction of constraints, or an inconsistent network.
(3) Single primitive relation on each of the labels signify a singleton model of the temporal data base.
Our algorithm adds each node one by one to the data base of temporal intervals.
Thus the n-th node is added to the database of (n -1) nodes.
The data base of (n -1) nodes consists of a set of consistent singleton models (CSM).
New constraints from nth node to all other previously added nodes are used to nd n-node consistent singleton models.
This is done for all CSM's available in the old data base.
New data base consists of n-node CSM's.
At each stage an approximate algorithm is run rst as preprocessor to prune primitive relations from labels which do not have any possibility of forming a CSM.
After running the preprocessor, the forward pruning (FP) algorithm is run.
FPalgorithm systematically picks up singletons (primitive relations) from the next new arc from an ordered set of new arcs, and updates labels on all remaining new arcs by standard temporal interval{ constraint updating technique2].
If in any updating stage a null relation is generated it is considered to be a contradiction, and the algorithm backtracks over, rst the primitive relations on current arc (we call it sidetrack), and if it is exhausted, then over the previous arc in the ordered set (of new arcs).
If backtracking exhausts all primitive relations from label on the rst picked up arc, then the present  old-CSM can not nd a progeny.
A new n-node CSM is found when a primitive relation is picked up nal updated label of on the last arc of the set.
A new CSM is added for each primitive relation on the last arc to the new data base.
This process is repeated for all old CSM's.
If no model is found for all old CSM's then the new given constraints on n-th node is not consistent with respect to old data base.
Using this dynamic programming technique, the algorithm improves upon the unsystematic approach of backtracking which have been used for complete interval-based reasoning algorithms developed before.
The cost of this improved time eciency is in the additional space requirement of storing list of singletons at each stage.
The data structure design for the implementation becomes quite important in achieving space and time eciency.
The algorithm is already implemented in its preliminary form.
3 Theoretical implications of the algorithm In this section some theoretical signicance of having this algorithm for completely solving the problem of interval-based temporal constraint propagation is discussed.
(a) Empirical studies of temporal constraint satisfaction problem (TCSP): Recently a new approach of empirically studying NP-complete problems3, 5] is taking shape.
Apart from gaining insight into the structure of the problem, such empirical studies also help in developing more efcient algorithms.
Our algorithm should provide an ecient means to empirically study the temporal constraint propagation problem, which also happened to be NP-complete problem.
This algorithm, being more ecient and powerful (in providing all models), should yield better insight into the problem, with respect to some identied problem structures like average number of constrained arcs, average number of primitive relations on those arcs, or higher moments of their distributions10].
(b) Average number of models: Since our algorithm is particularly suitable for generating all CSM's, it gives an opportunity to study average number of models with respect to dierent problem structures.
Such studies could be very valuable in acquiring domain-specic information.
For  Theoretical and practical implications of an algorithm for nding all consistent temporal models Debasis Mitra dmitra@ccaix.jsums.edu Department of Computer Science 1400 Lynch St, P.O.
Box 18839 Jackson State University, Jackson MS 39217  Abstract In this article we have discussed an algorithm for nding all consistent temporal models (with interval-based representation) in an incremental fashion, and signicance of having this algorithm.
Reasoning with time in interval-based representation is NP-complete problem.
This has deterred researchers from studying the signicance of having a data base of all consistent models.
Advantage of having such models is multi-fold, as we have tried to show here.
Our algorithm is ecient enough so that obtaining all consistent models become practicable.
This makes such study necessary.
Signicance elaborated here spreads over both theoretical as well as practical aspects.
1 Introduction Reasoning with time is an important part of most cognitive activities.
Detecting consistency for a given set of temporal constraints is an NP-complete problem, when temporal entities are considered as intervals in time.
So far there exist only four algorithms for completely detecting temporal consistency.
First one, based on dependency-directed backtracking was proposed by Valdes-Perez12].
It was not complete, and there is no reported implementation of it.
Second one, forwarded by Peter van Beek13] is based on rst isolating a polynomially solvable (pointizable) subset of the problem, next solving it using polynomial algorithm, then converting the result back to original domain, and on failure, backtracking to nd dierent subset of the original problem.
Third algorithm, based on an unsystematic approach of backtrack, was developed and implemented by Ladkin et al4].
Experimentation with their algorithm has shown that, on an average, the problem is not a very hard one.
Almost at the same time, we have developed another algorithm which not only detects consistency, but also nds all possible consistent (temporal) models.
Ladkin et al's algorithm nds one consistent model as a side eect.
Although, if sucient time is given, their depthrst search algorithm also can be extended to nd all consistent models, our dynamic programmingbased algorithm is particularly geared towards efciently nding all consistent models.
Its forwardpruning heuristic prunes the search space in each stage, thus gaining eciency on average case.
Another aspect of our algorithm is that it adds each temporal entity individually, thus developing the data base of the temporal entities incrementally.
Other algorithms are not particularly geared towards such incremental development of data base.
On-the-fly Automata Construction for Dynamic Linear Time Temporal Logic Laura Giordano Universita del Piemonte Orientale Alessandria, Italy laura@mfn.unipmn.it Abstract We present a tableau-based algorithm for obtaining a Buchi automaton from a formula in Dynamic Linear Time Temporal Logic (DLT L), a logic which extends LTL by indexing the until operator with regular programs.
The construction of the states of the automaton is similar to the standard construction for LT L, but a different technique must be used to verify the fulfillment of until formulas.
The resulting automaton is a Buchi automaton rather than a generalized one.
The construction can be done on-the-fly, while checking for the emptiness of the automaton.
1.
Introduction The problem of constructing automata from Linear-Time Temporal (LTL) formulas has been deeply studied [11].
The interest on this problem comes from the wide use temporal logic for the verification of properties of concurrent systems.
The standard approach to LTL model checking consists of translating the negation of a given LTL formula (property) into a Buchi automaton, and checking the product of the property automaton and the model for language emptiness.
Therefore it is essential to keep the size of the automaton as small as possible.
A tableau-based algorithm for efficiently constructing a Buchi automaton is presented in [2].
This algorithm allows to build the graph "on the fly" and in most cases builds quite small automata, although the problem is intrinsically exponential.
Further improvements have been presented in [1, 8].
Dynamic Linear Time Temporal Logic (DLT L) [6] extends LTL by indexing the until operator with programs in Propositional Dynamic Logic, and has been shown to be strictly more expressive than LTL [6].
In [3, 4] we have developed an action theory based on DLTL and of its product version [5], and we have shown how to use it to model multi-agent systems and to verify their properties, in particular by using model checking techniques.
In [6] it is shown  Alberto Martelli Universita di Torino Torino, Italy mrt@di.unito.it  that the satisfiability problem for DLTL can be solved in exponential time, by reducing it to the emptiness problem for Buchi automata.
This motivates the interest in developing efficient techniques for translating formulas into automata.
In this paper we present an efficient tableau-based algorithm for constructing a Buchi automaton from a DLTL formula.
The construction of the states of the automaton is similar to the standard construction for LT L [2], but the possibility of indexing until formulas with regular programs puts stronger constraints on the fulfillment of until formulas than in LTL, requiring more complex acceptance conditions.
Thus we extend the structure of graph nodes and the acceptance conditions by adapting a technique proposed in [6].
The resulting automaton will be a Buchi automaton instead of a generalized Buchi automaton as in [2].
2.
Dynamic Linear Time Temporal Logic In this section we shortly define the syntax and semantics of DLTL as introduced in [6].
In such a linear time temporal logic the next state modality is indexed by actions.
Moreover, (and this is the extension to LTL) the until operator is indexed by programs in Propositional Dynamic Logic (PDL).
Let S be a finite non-empty alphabet.
The members of S are actions.
Let S* and So be the set of finite and infinite words on S, where o = {0, 1, 2, .
.
.}.
Let S[?]
=S* [?]
So .
We denote by s, s 0 the words over So and by t, t 0 the words over S* .
Moreover, we denote by <= the usual prefix ordering over S* and, for u [?]
S[?]
, we denote by prf(u) the set of finite prefixes of u.
We define the set of programs (regular expressions) P rg(S) generated by S as follows: P rg(S) ::= a | p1 + p2 | p1 ; p2 | p * where a [?]
S and p1 , p2 , p range over P rg(S).
A set of finite words is associated with each program by the mapping * [[]] : P rg(S) - 2S , which is defined as follows: * [[a]] = {a};  * [[p1 + p2 ]] = [[p1 ]] [?]
[[p2 ]]; * [[p1 ; p2 ]] = {t1 t2 | t1 [?]
[[p1 ]] and t2 [?]
[[p2 ]]}; S * [[p * ]] = [[p i ]], where  A Buchi automaton over an alphabet S is a tuple B = (Q, -, Qin , F ) where: * Q is a finite nonempty set of states; * -[?]
Q x S x Q is a transition relation; * Qin [?]
Q is the set of initial states; * F [?]
Q is a set of accepting states.
- [[p 0 ]] = {e} - [[p i+1 ]] = {t1 t2 | t1 [?]
[[p]] and t2 [?]
[[p i ]]}, for every i [?]
o.
Let P = {p1 , p2 , .
.
.}
be a countable set of atomic propositions.
The set of formulas of DLTL(S) is defined as follows: DLTL(S) ::= p | !a | a [?]
b | aU p b where p [?]
P and a, b range over DLTL(S).
A model of DLTL(S) is a pair M = (s, V ) where s [?]
So and V : prf (s) - 2P is a valuation function.
Given a model M = (s, V ), a finite word t [?]
prf (s) and a formula a, the satisfiability of a formula a at t in M , written M, t |= a, is defined as follows: * M, t |= p iff p [?]
V (t ); * M, t |= !a iff M, t 6|= a; * M, t |= a [?]
b iff M, t |= a or M, t |= b; * M, t |= aU p b iff there exists t 0 [?]
[[p]] such that t t 0 [?]
prf (s) and M, t t 0 |= b.
Moreover, for every t 00 such that e <= t 00 < t 01 , M, t t 00 |= a.
A formula a is satisfiable iff there is a model M = (s, V ) and a finite word t [?]
prf (s) such that M, t |= a.
The formula aU p b is true at t if "a until b" is true on a finite stretch of behavior which is in the linear time behavior of the program p. The derived modalities hpi and [p] can be defined as follows: hpia [?]
>U p a and [p]a [?]
!hpi!a.
Furthermore, if we let S = {a1 , .
.
.
, an }, the U, O (next), 3 and 2 of LTL can be defined as follows: Oa [?]
W * haia, aUb [?]
aU S b, 3a [?]
>Ua, 2a [?]
!3!a, a[?
]S * where, in U S , S is taken to be a shorthand for the program a1 + .
.
.
+ an .
Hence both LTL(S) and PDL are fragments of DLTL(S).
As shown in [6], DLTL(S) is strictly more expressive than LTL(S).
In fact, as the logic ETL [10] to which DLTL is inspired, DLTL has the full expressive power of the monadic second order theory of o-sequences.
3.
Automaton Construction In this section we show how to build a Buchi automaton for a given DLTL formula ph using a tableau-like procedure.
The automaton generates all the infinite sequences (models) satisfying the formula ph.
First we recall the definition of Buchi automata.
1  We define t <= t 0 iff [?
]t 00 such that t t 00 = t 0 .
Moreover, t < t 0 iff t <= t 0 and t 6= t 0 .
Let s [?]
So .
Then a run of B over s is a map r : prf (s) - Q such that: * r(e) [?]
Qin a * r(t ) - r(t a) for each t a [?]
prf (s) The run r is accepting iff inf(r) [?]
F 6= [?
], where inf(r) [?]
Q is given by: q [?]
inf (r) iff r(t ) = q for infinitely many t [?]
prf (s).
Finally L(B), the language of o-words accepted by B, is: L(B) = {s|[?]
an accepting run of B over s}.
Our aim is now to construct a Buchi automaton for a given DLTL formula ph.
We build a graph defining the states and transitions of the automaton.
A tableau-like procedure allows a node to be expanded by applying propositional rules as well as by expanding the temporal operators.
It will make use of a reformulation of the following axioms of DLTL in [6]2 : W a[?
]S hai> W W 0 aU p b [?]
(b [?]
(a [?]
a[?
]S hai p0 [?
]da (p) aU p b)), for e [?]
[[p]], W W 0 aU p b [?]
a [?]
a[?
]S hai p0 [?
]da (p) aU p b, for e 6[?]
[[p]], where a a da (p) = {p 0 |p -- p 0 } and -- is a transition relation (defined in [6]) such that the program p 0 is obtained from the program p by executing action a.
In our construction, we exploit the equivalence results between regular expressions and finite automata and we make use of an equivalent formulation of DLTL formulas in which "until" formulas are indexed with finite automata rather than regular expressions.
Thus we have aU A b instead of aU p b, where L(A) = [[p]].
In fact, for each regular expression p there is an (2-free) nondeterministic finite automaton A, accepting the language [[p]] generated by p. Moreover the size of the automaton is linear in the size of p [7].
Satisfiability of until formulas aU A b must be modified accordingly by replacing [[p]] with L(A) in the definition above3 .
More precisely, in the construction we will make use of the following notation for automata.
Let A = (Q, d, QF ) be an 2-free nondeterministic finite automaton over the alphabet S without an initial state, where Q is a finite set of 2 3  Remember that haia [?]
>U a a.
The idea of using finite state automata to label "until" formulas is inspired both to the automata construction for DLTL in [6] and to the automata construction for ETL in [9].
states, d : Q x S - 2Q is the transition function, and QF is the set of final states.
Given a state q [?]
Q, we denote with A(q) an automaton A with initial state q.
The two axioms above will thus be restated as follows: W W 0 aU A(q) b [?]
(b[?](a[?]
a[?
]S hai q0 [?
]d(q,a) aU A(q ) b)) (q is a final state) W W 0 aU A(q) b [?]
a [?]
a[?
]S hai q0 [?
]d(q,a) aU A(q ) b (q is not a final state) These formulas can to be valid.
Observe W be easily proved 0 that the disjunction q0 [?
]d(q,a) aU A(q ) b is a finite disjunction, as the set of states q 0 in d(q, a) is finite.
The main procedure to construct the Buchi automaton for a formula ph builds a graph G(ph) whose nodes are labelled by sets of formulas, and which defines the states and the transitions of the Buchi automaton.
The procedure makes use of an auxiliary tableau-based function which is described in the next section.
3.1.
Tableau computation The tableau procedure we introduce makes use of signed formulas, i.e.
formulas prefixed with the symbol T or F. This procedure takes as input a set of formulas4 and returns a set of sets of formulas, obtained by expanding the input set according to a set of tableau rules, formulated as follows: ph = ps1 , ps2 , if ph belongs to the set of formulas, then add ps1 and ps2 to the set ph = ps1 |ps2 , if ph belongs to the set of formulas, then make two copies of the set and add ps1 to one of them and ps2 to the other one.
The rules are the following: T(a [?]
b) = Ta, Tb F(a [?]
b) = Fa, Fb F(a [?]
b) = Fa|Fb T(a [?]
b) = Ta|Tb T!a = Fa F!a = Ta W W 0 R1 TaU A(q) b = T(b[?](a[?]
a[?
]S hai q0 [?
]d(q,a) aU A(q ) b)) (q is a final state) W W 0 R2 TaU A(q) b = T(a [?]
a[?
]S hai q0 [?
]d(q,a) aU A(q ) b) (q is not a final state) W W 0 FaU A(q) b = F(b[?](a[?]
a[?
]S hai q0 [?
]d(q,a) aU A(q ) b)) (q is a final state) W W 0 FaU A(q) b = F(a [?]
a[?
]S hai q0 [?
]d(q,a) aU A(q ) b) (q is not a final state) Given a set of formulas s, function tableau(s) works as follows: 4  In this section we will always refer to signed formulas  * add T  W  a[?
]S hai>  to s,  * expand the set of formulas of s according to the above rules (by possibly creating new sets) until all formulas in all sets have been expanded, * return the resulting set of sets.
W Formula T a[?
]S hai> makes explicit that in DLTL each state must be followed by a next state (O> is an axiom in DLTL).
If the expansion of a set of formulas produces an inconsistent set, then this set is deleted (consistency constraint).
A set is inconsistent if it contains either "T[?]"
or "F>" or "Ta and Fa" or "Thaia and Thbib with a 6= b".
Observe that the expansion of an until formula aU A(q) b only requires a finite number of steps, namely a number of steps linear in the size of the automaton.
It is easy to see that for each set of formulas returned by tableau there is exactly one symbol a [?]
S such that the set contains formulas of the form Thaia.
In fact, because W of T a[?
]S hai>, there must be at least one formula of that kind, whereas the consistency constraint prevents from having more than one formula of the form Thaia for different symbols a [?]
S.  3.2.
Building the graph To build the graph we will consider each set of formulas obtained through the tableau construction as a node of the graph.
The above tableau rules do not expand formulas of the kind haia.
Since the operator hai is a next state operator, expanding this kind of formulas from a node n means to create a new node containing a connected to n through an edge labelled with a.
Given a node n containing a formula Thaia, then the set of nodes connected to n through an edge labelled a is given by tableau({Ta|Thaia [?]
n} [?]
{Fa|Fhaia [?]
n}).
States and transitions of the Buchi automaton are obtained directly from the nodes and edges of the graph.
While we will give later the details of the construction of the automaton, we want now to address the problem of defining accepting conditions.
Intuitively this has to do with until formulas, i.e.
formulas of the form TaU A(q) b.
If a node n of the graph contains the formula TaU A(q) b, then we will accept an infinite path containing this node if it is followed in the path by a node n0 containing Tb and TaU A(qF ) b, where qF is a final state of A.
Furthermore if t is the sequence of labels in the path from n to n0 , then t must belong to L(A(q)), and all nodes between n and n0 must contain Ta.
This problem has been solved in LTL by imposing generalized Buchi acceptance conditions.
In our formulation they could be stated as follows: For each subformula aU b of the  Figure 1.
(a) automaton A and (b) graph for 2hA(s1 )ip  initial formula there is a set F of accepting states including all the nodes q [?]
Q such that either TaUb is not contained in the node or Tb holds.
Unfortunately a similar solution does not apply in the case of DLTL, because acceptance of until formulas is more constrained than in LTL.
Let us consider for instance the formula 2hA(s1 )ip, with S = {a}.
The automaton A is given in Figure 1(a).
By eliminating the derived modalities, this formula can be rewritten as the signed formula F(>U A1 (s0 ) !
(>U A(s1 ) p)), where the automaton A1 has only one (final) state s0 connected to itself through a transition labelled a.
By applying the above construction starting from this formula, we obtain the graph in Figure 1(b), where for simplicity we have kept only the most significant formulas.
Every node of this graph contains a formula T(>U A(s1 ) p), and the only node which might fulfill the until formulas is node n3 , since it contains T(>U A(s3 ) p), with s3 final, and Tp.
However it is easy to see that not all infinite paths through n3 will be accepting.
For instance, in the path n1 , n2 , n3 , n4 , n3 , n4 , n3 , n4 , .
.
.
no occurrence of n3 fulfills the formula T(>U A(s1 ) p) in n2 , since the distance in this path between node n2 and any occurrence of n3 is odd, while all strings in L(A(s1 )) have even length.
We present now a different solution, derived from [6], where some of the nodes will be duplicated to avoid the above problem.
Before describing the construction of the graph, we make the following observation.
Let us assume that a node n contains the until formula TaU A(q) b, such that q is not a final state.
Since this formula has been W expanded with0 (R2), node n will also contain Thai q0 [?
]d(q,a) aU A(q ) b for some a.
Therefore, according to the construction of the successor nodes, 0 each successor node will contain a formula TaU A(q ) b, where q 0 [?]
d(q, a).
We say that this until formula is derived from formula TaU A(q) b in node n. On the other  hand, if q is a final state, then TaU A(q) b has been expanded with (R1), and two cases are possible: either n contains Tb or all successor nodes contain a derived until formula as described above.
If a node contains an until formula which is not derived from a predecessor node, we will say that the formula is new.
New until formulas are obtained during the expansion of the tableau procedure.
It is easy to see that if TaU A(q) b is a new formula, then aU A(q) b must be a subformula of the initial formula.
For instance, the formula T(>U A(s1 ) p) is new in each of the nodes in Figure 1.
Note that an until formula in a node might be both a derived and a new formula.
In that case we will consider it as a derived formula.
We can now show how the graph can be built and how the accepting conditions are formulated.
Each node of the graph is a triple (F, x, f ), where [?]
F is an expanded set of formulas, x [?]
{0, 1}, and f [?]
{|, }.
In order to formulate the accepting condition, we must be able to trace the until formulas along the paths of the graph to make sure that they satisfy the until condition.
Therefore we extend signed formulas so that all until formulas have a label 0 or 1, i.e.
they have the form Tl aU A(q) b where l [?]
{0, 1}.
For each node (F, x, f ), the label of an until formula in F will be assigned as follows.
If it is a derived until formula, then its label is the same as that of the until formula in the predecessor node it derives from.
Otherwise, if the formula is new, it is given the label 1 - x.
Of course function tableau must be suitably modified in order to propagate the label from an until formula to its derived formulas in the successor nodes, and to give the right label to new formulas.
To do this we assume that it has two parameters: a set of formulas and the value of x.
Function create graph in Figure 2 builds a graph G(ph), given an initial formula ph, by returning the triple hQ, I, [?
]i, where Q is the set of nodes, I the set of initial nodes and [?]
: Q x S x Q the set of labelled edges.
Note that two formulas T0 aU A(q) b and T1 aU A(q) b are considered to be different.
For instance, by applying create graph to the formula of Figure 1, we get two nodes ({T0 (>U A(s1 ) p), T0 (>U A(s2 ) p), T1 (>U A(s4 ) p)}, |, 1) and ({T0 (>U A(s1 ) p), T0 (>U A(s2 ) p), T0 (>U A(s4 ) p), T1 (>U A(s2 ) p)}, |, 1).
These two nodes correspond to node n4 in Figure 1.
States and transitions of the Buchi automaton B(ph) are obtained directly from the nodes and edges of the graph.
The set of accepting states[?
]consists of all states whose associated node contains f = .
Let r be a run of B(ph).
Since we identify states of the automaton with nodes of the graph, r can also be considered as an infinite path of G(ph), and r(t ) will denote a node of such a graph.
According to the construction of the graph, the  function create graph(ph) I := [?]
for all F [?]
tableau({Tph}, 0) [?]
I := I [?]
{(F, 0, )} end-for U := Q := I [?]
:= [?]
while U 6= [?]
do remove[?
]n = (F, x, f ) from U if f = then x0 := 1 - x else x0 := x end-if for all F 0 [?]
tableau({Ta|Thaia [?]
F}[?]
{Fa|Fhaia [?]
F}, x0 ) [?]
if f = then f 0 :=| 0 else if there exists no Tx aU A(q) b [?]
F 0 then [?]
f 0 := else f 0 :=| end-if end-if n0 := (F 0 , x0 , f 0 ) if [?
]n00 [?]
Q such that n00 = n0 then [?]
:= [?]
[?]
{(n, a, n00 )} else Q := Q [?]
{n0 } [?]
:= [?]
[?]
{(n, a, n0 )} U := U [?]
{n0 } end-if end-for end-while return hQ, I, [?
]i  [?]
[?]
(0, ), (1, |), .
.
.
, (1, |), (1, ), (0, |), .
.
.
, (0, |), [?]
(0, ), * * * Let us call 0-sequences or 1-sequences the sequences of nodes of r with x = 0 or x = 1 respectively.
If r is an accepting [?]
run, then it must contain infinitely many nodes containing , and thus all 0-sequences and 1-sequences must be finite.
Intuitively, every until formula contained in a node of a 0-sequence must be fulfilled within the end of the next 1sequence, and vice versa.
In fact, assuming that the formula has label 1, the label will be propagated to all derived formulas in the following nodes until a node is found fulfilling the until formula.
But, on the other hand, the 1-sequence terminates only when there are no more until formulas with label 1, and thus that node must be met before the end of the next 1-sequence.
3.3.
Correctness of the procedure The next proposition summarizes what we have already pointed out in the previous section.
Proposition 1 Assume that a node n of the graph contains Tl aU A(q) b, and let a be the label of the outgoing edges (remember that all outgoing edges from a node have the same label).
Then the following holds: if q is not a final state of A node n contains Ta and each outgoing edge leads to a node containing an until formula 0 Tl aU A(q ) b derived from Tl aU A(q) b in n, such 0 that q [?]
d(q, a) else, if q is a final state of A, either (a) node n contains Tb and no successor node contains a formula derived from Tl aU A(q) b, or (b) node n contains Ta and each outgoing edge leads to a node containing a derived until formula 0 Tl aU A(q ) b, such that q 0 [?]
d(q, a)  Figure 2.
Function create graph x and f values of the nodes of r have the following properties: [?]
* if a node contains (0, ) then its successor node contains (1, |) [?]
* if a node contains (1, ) then its successor node contains (0, |)  Given a run r, we will denote with r(t ).F the F field of the node r(t ), and similarly for the x and f fields.
* if a node contains (0, |)[?
]then its successor node contains either (0, |) or (0, )  1.
[?
]t 0 s.t.
t t 0 [?]
prf (s) : Tl aU A(q ) b [?]
r(t t 0 ).F and * q 0 [?]
dA (q, t 0 ) 5  * if a node contains (1, |)[?
]then its successor node contains either (1, |) or (1, )  2.
[?
]t 0 s.t.
t t 0 [?]
prf (s) : Tl aU A(q ) b [?]
r(t t 0 ).F, q 0 * is a final state, q 0 [?]
dA (q, t 0 ), Tb [?]
r(t t 0 ).F and no  Therefore the sequence of the x and f values in r will be as follows:  Proposition 2 Let s [?]
So and r : prf (s) -- Q be a (non necessarily accepting) run.
For each t [?]
prf (s), let r(t ) = (F, x, f ).
Then for each Tl aU A(q) b [?]
F one of the following holds: 0  0  5  * is the obvious extension of d to sequences dA A  successor node of r(t t 0 ) contains an until formula de0 rived from Tl aU A(q ) b.
Moreover, for every t 00 such that e <= t 00 < t 0 , Ta [?]
r(t t 00 ).F.
For each FaU A(q) b [?]
F the following holds: 3.
[?
]t 0 s.t.
t t 0 [?]
prf (s): if t 0 [?]
L(A(q)) then either Fb [?]
r(t t 0 ).F or there is t 00 such that e <= t 00 < t 0 , Fa [?]
r(t t 00 ).F.
Proof It follows from Proposition 1 and procedure create graph.
In an accepting run, case (2) must hold for all until formulas and all nodes.
This is proved in the following theorem, together with its converse.
Theorem 1 Let s [?]
So and r : prf (s) -- Q be a run.
Then, for each t [?]
prf (s) and for each Tl aU A(q) b [?]
r(t ).F, condition (2) of Proposition 2 holds if and only if r is an accepting run.
Proof If part: r is an accepting run.
As pointed out before the nodes of r are arranged in alternating 0-sequences and 1-sequences of finite length.
Then we can have the following cases: a) l = 0 and r(t ).x = 0.
Let us assume that condition (1) of Proposition 2 holds.
Then each node r(t t 0 ) following r(t ) in the same 0-sequence, will contain a derived formula T0 aU A(q) b (remember that the label of a derived formula cannot change).
On the other hand, the 0-sequence containing r(t ) is finite, and, by construction, the last node of this sequence does not contain any until formula labelled with 0.
Therefore the assumption is wrong, and condition (2) must hold.
b) l = 1 and r(t ).x = 1.
As case (a).
c) l = 1 and r(t ).x = 0.
Let us assume again that condition (1) of Proposition 2 holds.
Then each node r(t t 0 ) following r(t ) will contain an until formula derived from T1 aU A(q) b in r(t ).
All derived formulas will be labelled 1 up to the last node of the 0-sequence.
This label will necessarily propagate to the first node of the following 1-sequence, and we fall in case (b).
d) l = 0 and r(t ).x = 1.
As case (c).
Only if: condition (2) holds.
We show that all 0 and 1-sequences of r are finite.
This is true for the initial 0sequence, which consists only of the first node.
Let us assume now that a 0-sequence is finite.
We show that the following 1-sequence is also finite.
According to the construction, the last node of the 0-sequence can contain only until formulas with label 1.
The following 1-sequence goes on until its nodes contain some until formula with label 1.
Since condition (2) holds, for each of these until formulas there is a t 0 such that the successor node of r(t t 0 ) does not contain an until formula derived from it.
On the other hand  all new until formulas created in this 1-sequence will have label 0.
Therefore, if t max is the longest among all t 0 , after node r(t t max ) there will be no until formula labelled with 1, and the 1-sequence will terminate.
The same holds by replacing 0 with 1 and vice versa.
Lemma 1 Let s beVa set of W formulas V and tableau(s) = {s1 , .
.
.
, sn }.
Then s - 1<=i<=n si .
Proof All rules used by the function tableau correspond to equivalence formulas.
Lemma 2 Let M = (s, V ) be a model, t [?]
prf (s), and let such that M, t |= V n = (F, x, f ) be a node of the graph 0 F. Then there exists a successor n = (F 0 , x0 , f 0 ) of n V 0 such that M, t a |= F , where t a [?]
prf (s).
Moreover, if TaU A(q) b [?]
F 0 where q is a final state and M, t a |= Tb, then Tb [?]
F 0 .
Proof The proof comes from the construction and the previous lemma.
In particular the last part holds if, when expanding TaU A(q) b in F 0 with rule (R1), we choose the set containing Tb.
Theorem 2 Let M = (s, V ) and M, e |= ph.
Then s [?]
L(B(ph)).
Proof We show how to build an accepting run r of B(ph) over s. The first node of r is chosen by taking an initial V node n = (F, x, f ) of the graph such that M, e |= F. The following nodes of r are chosen by repeatedly applying Lemma 2.
To prove that the run is an accepting run, we have to show that all the until formulas are fulfilled.
Assume that TaU A(q) b occurs on the run at r(t ).
Then, for the choice of the run r, it must be that M, t |= aU A(q) b.
By definition of satisfiability we have that there exists t 0 [?]
L(A(q)) such that t t 0 [?]
prf (s) and M, t t 0 |= b.
Moreover, for every t 00 such that e <= t 00 < t 0 , M, t t 00 |= a.
As t 0 [?]
L(A(q)), by the choice of run r and the construction * of the automaton, there must be a final state q 0 [?]
dA (q, t 0 ) A(q 0 ) 0 such that TaU b belongs to r(t t ).F.
Moreover for all t 00 such that e <= t 00 < t 0 , Ta belongs to r(t t 00 ).F.
By Lemma 2, Tb also belongs to r(t t 0 ).F.
Hence, condition (2) of Proposition 2 holds and we can conclude, by Theorem 1, that r is an accepting run.
Given a set F of signed formulas, we define the sets P os(F) and N eg(F) respectively as the sets of positive and negative propositions in F, i.e.
P os(F) = {p [?]
P|Tp [?]
F}, and N eg(F) = {p [?]
P|Fp [?]
F}.
Theorem 3 Let s [?]
L(B(ph)).
Then there is a model M = (s, V ) such that M, e |= ph.
Proof Let r be an accepting run.
for each t [?]
prf (s) let r(t ) = (Ft , xt , ft ).
The model M = (s, V ) can be obtained by defining V (t ) [?]
2P such that V (t ) [?]
P os(Ft ) and V (t ) [?]
N eg(Ft ) = [?].
It is easy to prove by induction on the structure of formulas that, for each t and for each formula a, if Ta [?]
Ft then M, t |= a, and if Fa [?]
Ft then M, t 6|= a.
In particular, for until formulas labelled T we make use of Theorem 1 and of Proposition 2, case 2, while for until formulas labelled F we make use of Proposition 2, case 3.
From Tph [?]
F2 , it follows that M, 2 |= ph.
3.4.
Complexity It is known that for p [?]
P rg(S), we can construct in polynomial time a non-deterministic finite state automaton A with L(A) = [[p]] such that the number of states of A is linear in the size of p [7].
The expansion of each until formula aU A(q) b in the initial formula ph introduces at most a number of formulas which is linear in the size of A and, hence, is linear in the size of p. In fact, observe that the expansion of the until formula aU A(q) b (and its descendants) 0 introduces at most |QA | subformulas of the form aU A(q ) b, with q 0 [?]
QA .
Let a1 U p1 b1 ,....,an U pn bn be all the until formulas occurring in the initial formula ph.
It must be that |p1 | + .
.
.
+ |pn | <= |ph|.
Hence, the number of until formulas which are introduced in the construction of the automaton is linear in the size of the initial formula ph.
Therefore, in the worst case, the number of states of the Buchi automaton is exponential in the size of |ph|.
4.
Conclusions In this paper we have presented a tableau-based algorithm for constructing a Buchi automaton from a DLT L formula.
The formula is satisfiable if the language recognized by the automaton is nonempty.
The construction of the states of the automaton can be done on-the-fly during the search that checks for emptiness.
As in [6] we make use of finite automata to verify the fulfillment of until formulas.
However, the construction of the automaton given in [6] is based on the idea of generating all the (maximally consistent) sets of the subformulas of the initial formula.
Moreover, rather then introducing the states of the finite automata in the global states of the Buchi automaton, we stay closer to the standard construction for LTL [2] and we detect the point of fulfillment of the until formulas by associating a finite automaton with each until formula (rather than a regular expression) and by keeping track of the evolution of the state of these (finite) automata during the expansion of temporal formulas.
This construction could be improved in various ways, in particular by adopting the techniques presented in [1].
5.
Acknowledgements This research has been partially supported by the project MIUR PRIN 2003 "Logic-based development and verification of multi-agent systems".
References [1] M. Daniele, F. Giunchiglia and M.Y.
Vardi.
Improved automata generation for linear temporal logic.
In Proc.
11th CAV, Springer LNCS vol.
1633, pp.
249-260, July 1999.
[2] R. Gerth, D. Peled, M.Y.
Vardi and P. Wolper.
Simple on-thefly automatic verification of linear temporal logic.
In Proc.
15th work.
Protocol Specification, Testing and Verification, Warsaw, June 1995.
[3] L.Giordano, A.Martelli, and C.Schwind.
Reasoning about actions in dynamic linear time temporal logic.
Logic Journal of the IGPL, 9(2):289-303, 2001.
[4] L. Giordano, A. Martelli, and C. Schwind.
Specifying and Verifying Systems of Communicating Agents in a Temporal Action Logic.
In Proc.
AI*IA'03, Pisa, Springer LNCS vol.
2829, pp.
262-274, September 2003.
[5] J.G.
Henriksen and P.S.
Thiagarajan.
A Product Version of Dynamic Linear Time Temporal Logic.
In CONCUR'97, 1997.
[6] J.G.
Henriksen and P.S.
Thiagarajan.
Dynamic Linear Time Temporal Logic.
In Annals of Pure and Applied logic, vol.96, n.1-3, pp.187-207, 1999.
[7] J. Hromkovic, S. Seibert and T. Wilke.
Translating Regular Expressions into Small e-Free Nondeterministic Finite Automata.
In Proc.
STACS'97, Springer LNCS vol.
1200, pp.
55-66, 1997.
[8] F. Somenzi and R. Bloem.
Efficient Buchi automata from LTL formulae.
In Proc.
12th CAV, Springer LNCS vol.
1855, pp.
247-263, 2000.
[9] M. Vardi and P. Wolper.
Reasoning about infinite computations.
In Information and Computation 115,1-37 (1994).
[10] P. Wolper.
Temporal logic can be more expressive.
In Information and Control 56,72-99 (1983).
[11] P. Wolper.
Constructing Automata from Temporal Logic Formulas: A Tutorial.
In Proc.
FMPA 2000, Springer LNCS vol.
2090, pp.
261-277, July 2000.
Using Temporal Logic to Control Search in a Forward Chaining Planner Fahiem Bacchus Dept.
Of Computer Science University Of Waterloo Waterloo, Ontario Canada, N2L 3G1 fbacchus@logo.uwaterloo.ca Abstract: Over the years increasingly sophisticated planning algorithms have been developed.
These have made for more efficient planners, but unfortunately these planners still suffer from combinatorial explosion.
Indeed, recent theoretical results demonstrate that such an explosion is inevitable.
It has long been acknowledged that domain independent planners need domain dependent information to help them plan effectively.
In this work we describe how natural domain information, of a "strategic" nature, can be expressed in a temporal logic, and then utilized to effectively control a forwardchaining planner.
There are numerous advantages to our approach, including a declarative semantics for the search control knowledge; a high degree of modularity (the more search control knowledge utilized the more efficient search becomes); and an independence of this knowledge from the details of the planning algorithm.
We have implemented our ideas in the TLP LAN system, and have been able to demonstrate its remarkable effectiveness in a wide range of planning domains.
1  Introduction  Planners generally employ search to find plans, and planning research has identified a number of different spaces in which search can be performed.
Of these, three of the most common are (1) the forward-chaining search space, (2) the backwardchaining search space, and (3) the space of partially ordered plans.
The forward-chaining space is generated by applying all applicable actions to every state starting with the initial state; the backward-chaining space by regressing the goal conditions back through actions that achieve at least one of the subgoals; and the space of partially ordered plans by applying a collection of plan modification operators to an initial "dummy" plan.
Planners that explore the backward-chaining space or the space of partially ordered plans have an advantage over those that explore the forward-chaining space in that the latter spaces are generated in a "goal directed" manner.
Hence, such This research was supported by the Canadian Government through their IRIS project and NSERC programs.
Fahiem Bacchus is currently on sabbatical leave from the University of Waterloo, Canada.
Froduald Kabanza Dept.
De Math Et Informatique Universite De Sherbrooke Sherbrooke, Quebec Canada, J1K 2R1 kabanza@dmi.usherb.ca planners are intrinsically goal directed: they need never consider actions that are not syntactically relevant to the goal because the spaces they explore do not include such actions.
Partial-order planners have an additional advantage over simple backward chaining planners in that the objects in their search space are partially ordered plans.
This allows these planners to delay ordering actions until they detect an interaction between them.
Linear backward or forward-chaining planners, on the other hand, might be forced into backtracking because they have prematurely committed to an ordering between the actions.
However, both backward-chaining and partial-order planners search in spaces in which knowledge of the state of the world is far less complete than in the forward-chaining space.
For example, even if a backward-chaining planner starts with a completely described initial world and actions that preserve the completeness of this description, it will still have only incomplete knowledge of the world state at the various points of its search space.
Partial order planners also suffer from this problem.
The points of their search space are incomplete partially ordered plans, and at the various stages of an incomplete plan we have only limited knowledge of the state of the world.
On the other hand, the points in the forward-chaining space are world descriptions.
Such descriptions provide a lot of information about the world state, even if the description is incomplete.
As we will demonstrate in this paper, such knowledge can be effectively utilized to control search in this space.
The choice between the various search spaces has been the subject of much recent inquiry [BW94; MDBP92], with current consensus seemingly converging on the space of partially ordered plans,1 mainly because of its goal-directness and least commitment attitude towards action ordering.
However, these studies have only investigated simple heuristic search over these spaces, where domain independent heuristics, like counting the number of unsatisfied sub-goals, are utilized.
Domain independent heuristics cannot take advantage of structural features that might be present in a particular domain.
Theoretical work [ENS92; Sel94] indicates that for the traditional S TRIPS actions used by almost all current planners, 1  Although, see [VB94] for an refreshing counterpoint.
finding a plan is, in general, intractable.
This means that no domain independent planning algorithm can succeed except in very simple (and probably artificial) domains.
More importantly, however, is that there may be many domains where it is feasible to find plans, but where domain structure must be exploited to do so.
This can be verified empirically; e.g., the partial order planner implemented by Soderland et al.
[SBW90] cannot effectively generate plans for reconfiguring more than 5 blocks in the blocks world using domain independent heuristic search.
Nevertheless, the blocks world does have sufficient structure to make it easy to generate good plans in this domain [GN92].
One way of exploiting domain structure during planning is to use domain information to control search.
Hence, a more practical evaluation of the relative merit of various planning algorithms and search spaces would also take into account how easy it is to exploit domain knowledge to control search in that space.
The idea of search control is not new, e.g., it is a prominent part of the P RODIGY planing system [CBE+ 92].
Our work, however, makes a number of new contributions to the notion of search control.
In particular, we demonstrate how search control information can be expressed in a first-order temporal logic, and we develop a method for utilizing this information to control search during planning.
By using a logic we gain the advantage of providing a formal semantics for the search control information.
Furthermore, we would claim that this semantics is quite natural and intuitive.
This differentiates our mechanism for search control from classical state-based heuristics and from the control rules employed by the P RODIGY system.
P RODIGY control rules are implemented as a rule-based system.
Various rules are activated dependent on the properties of the node in the search space that is currently being expanded.
These rules are activated in a particular order and have various effects.
This means that any attempt to give a semantics to these rules would require an operational semantics that makes reference to way in which the rules are utilized.
By using the forward-chaining search space we end up searching in the space of world descriptions.
This allows us to utilize search control knowledge that only makes reference to the properties of these worlds, i.e., to properties of the domain.
Thus the search control knowledge used can be considered to be no different from the description of the domain actions: it is part of our knowledge of the dynamics of the domain.
In contrast, P RODIGY control rules include things like binding selection rules that guide the planner in deciding how to instantiate actions.
Such rules have to do with particular operations of the P RODIGY planning algorithm, and to compose such rules the user must not only possess domain knowledge but also knowledge of the planning algorithm.
Finally, the language in which we express search control knowledge is richer than previous approaches.
Hence, it can capture much more complex control knowledge.
In particular, the control strategies are not restricted to considering only the current state, as are P RODIGY control rules and statebased heuristics.
They can, e.g., consider past states and pass  information forward into future states.
All of these features make the control information employed by our system not only easier to express and understand, but also more effective, sometimes amazingly effective, as we will demonstrate in Section 4.
Using the forward-chaining search space is not, of course, a panacea.
It does, however, seem to better support effective search control.
We have already mentioned two reasons for this: we have access to more information about the world state in this space, and it allows us to express search control information that is independent of the planning algorithm.
We have also found a third advantage during our experiments.
In many domains, humans seem to possess strategies for achieving various kinds of goals.
Such strategies seem to be most often expressed in a "forward-direction".
This makes their use in controlling forward-chaining search straightforward, but exploiting them in the other search spaces not always so.
Due to its support of effective search control, we have found that forward-chaining can in many domains yield planners that are more effective than those based on partial order planning or backwards-chaining regression.
The forward-chaining search space still suffers from the problem that it is not goal directed, and in many of our test domains we have found that some of the search control information we added was designed to recapture goal-directedness.
Much of this kind of search control knowledge can be automatically inferred from the operator descriptions, using ideas like those of [Etz93].
Inferring and learning search control in the form we utilize is an area of research we are currently pursuing.
One of the key advantages of using a logic to express search control knowledge is that it opens the door to reasoning with this knowledge to, e.g., generate further control knowledge or to verify and prove properties of the search control knowledge.
But again this avenue is a topic for future research.
In the rest of the paper we will first describe the temporal logic we use to express domain strategies.
We then describe how knowledge expressed in this language can be used to control forward chaining search.
We have implemented our approach in a system we call TLP LAN, and we describe some of our empirical results with this system next.
We close with a summary of our contributions and a description of some of the extensions to our approach we are currently working on.
2 First-order Linear Temporal Logic We use as our language for expressing strategic knowledge a first-order version of linear temporal logic (LTL) [Eme90].
The language starts with a standard first-order language, L, containing some collection of constant, function, and predicate symbols.
LTL adds to L the following temporal modalities: U (until), 2 (always), 3 (eventually), and (next).
The standard formula formation rules for first-order logic are augmented by the following rules: if f 1 and f2 are formulas then so are f1 U f2 , 2f1 , 3f1 , and f1 .
Note that the first-order and temporal formula formation rules can be applied in any order, so, e.g., quantifiers can scope temporal modalities al-  lowing quantifying into modal contexts.
Our planner works with standard S TRIPS operators and world descriptions, and it takes advantage of the fact that these world descriptions support the efficient testing of various conditions.
In particular, worlds described as lists of positive literals support the efficient evaluation of complex first-order formulas via model-checking [HV91].
Hence, we can express complex conditions as first-order formulas and evaluate their truth in the worlds generated by forward-chaining.
Part of our TLP LAN implementation is a first-order formula evaluator, and TLP LAN allows the user to define predicates by firstorder formulas.
These predicates can in turn be used in temporal control formulas, where they act to detect various conditions in the sequence of worlds explored by the planner.
To ensure that it is computationally effective to evaluate these first-order formulas and at the same time not limit ourselves to finite domains (e.g., we may want to use the integers in our domain axiomatization), we use bounded instead of standard quantification.
In particular, instead of the quantifiers 8x or 9x, we have 8 x: ] and 9 x: ] , where  is an atomic formula2 whose free variables include x.
It is easiest to think about bounded quantifiers semantically: 8 x: ]  for some formula  holds iff  is true for all x such that  (x) holds, and 9 x: ]  holds iff  is true for some x such that  (x) holds.
Computational effectiveness is attained by requiring that in any world the set of satisfying instances of  be finite.
3 The formulas of LTL are interpreted over models of the form M = hs0 fi s1 fi : : :i, i.e., a sequence of states.
Every state si is a model (a first-order interpretation) for the base language L. In addition to the standard rules for the first-order connectives and quantifiers, we have that for a state si in a model M and formulas f1 and f2 :  fi hMfi sii j= f1 U f2 iff there exists j  i such that hMfi sj i j= f2 and for all k, i  k < j we have hMfi sk i j= f1 : f1 is true until f 2 is achieved.
fi hMfi sii j= state.
f1 iff hMfi si+1 i j f1 : f1 is true in the next =  fi hMfi sii j= 3f1 iff there exists j  i such that hMfi sj i j= f1 : f1 is eventually true.
fi hMfi sii j= 2f1 iff for all j  i we have hMfi sj i j= f1 : f1 is always true.
Finally, we say that the model M satisfies a formula f if hMfi s0i j= f .
First-order LTL allows us to express various claims about the sequence of states M. For example, on(Afi B ) asserts that in state s2 we have that A is on B .
Similarly, 2  We also allow to be an atomic formula within the scope of a (described below).
That is, instead of allowing x to range over the entire domain, we only allow it to range over the elements satisfying .
Thus, the underlying domain may be infinite, but any particular quantification over it is finite.
Also we allow formulas of the form 9 x: ] where  is implicitly taken to be TRUE.
GOAL modality 3  2:holding C asserts that we are never in a state where we are holding C , and 2 on Bfi C ) on Bfi C U on Afi B asserts that whenever we enter a state in which B is on C it remains on C until A is on B , i.e., on Bfi C is preserved until we achieve on Afi B .
Quantification allows even greater expressiveness, e.g., 8 x clear x clear x asserts that ev(  )  (  (  )  (  (  (  (  )  (  )))  )  )  :  (  )]  ( )  ery object that is clear in the current state remains clear in the next state.
We are going to use LTL formulas to express search control information (domain strategies).
Search control generally needs to take into account properties of the goal, and we have found a need to make reference to requirements of the goal in our LTL formulas.
To accomplish this we augment the base language L with a goal modality.
In particular, to the base language L we add the following formula formation rule: if f is a formula of L then so is GOAL(f ).
If the agent's goal is expressed by the first order formula , then semantically this modality is a modality of entailment from  and any state constraints.
That is, GOAL (f ) is true iff  ^  j= f , where  are the set of conditions true in every state, i.e., the set of state constraints.
However, testing if GOAL (f ) is true given an arbitrary goal formula  is intractable (in general, it requires theorem proving).
Our implemented planning system TLP LAN allows the goal modality to be used only when goals are sets of positive literals, and it computes goal formulas by assuming that these literals describe a "goal world" using a closed world assumption.
For example, if the goal is the set of literals fon(Afi B ), on(Bfi C )g then these are assumed to be the only positive literals that are true in the goal world, every other atomic formula is false by the closed world assumption.
For example clear(A) is false in the goal world.
Intuitively, it is not a necessary requirement of the goal.
TLP LAN can then use its firstorder formula evaluator over this goal world, so the formula GOAL (9 y:on(Afi y )] ) will also evaluate to true.
If, however, we had as our goal fP (A)g and the state constraint that in all states P (A) ) Q(A), then, in its current implementation, TLP LAN will incorrectly (according to the above semantics) conclude that GOAL (Q(A)) is false.
That is, TLP LAN cannot currently handle state constraints over the goal world.
3 Expressing Search Control Information Any LTL formula specifies a property of a sequence of states: it is satisfied by some sequences and falsified by others.
In planning we are dealing with sequences of executable actions, but to each such sequence there corresponds a sequence of worlds: the worlds we pass through as we execute the actions.
These worlds act as models for the language L. Hence, we can check the truth of an LTL formula given a plan, by checking its truth in the sequence of world visited by that plan using standard model checking techniques developed in the program verification area (see, e.g., [CG87]).4 Hence, if 4 LTL formulas  actually require an infinite sequence of worlds as their model.
In the context of standard planning languages, where a plan consists of a finite sequence of actions, we can terminate every finite sequence of actions with an infinitely replicated "do nothing"  we have a domain strategy for the goal fon(Bfi A)fi on(Cfi B )g like "if we achieve on(Bfi A) then preserve it until on(Cfi B ) is achieved", we could express this information as the LTL formula 2(on(Bfi A) ) on(Bfi A) U on(Cfi B )) and check its truth against candidate plans, rejecting any plans that violate this condition.
What we need is an incremental way of checking our control strategies against the partial plans generated as we search for a correct plan.
If one of our partial plans violates our control strategy we can reject it, thus pruning all of its extensions from the search space.
We have developed a mechanism for doing incremental checking of an LTL formula.
The key to this method is the progression algorithm given in Table 1.
In the algorithm quantified formulas are progressed by progressing all of their instances.
This algorithm is characterized by the following theorem: Theorem 3.1 Let M = hs0 fi s1 fi : : :i be any LTL model.
Then, we have for any LTL formula f , hMfi si i j= f if and only if hMfi si+1i j= f + .
The progression algorithm admits the following implementation strategy, used in TLP LAN.
Every world generated during our search of the forward-chaining space is labeled with an LTL formula f , with the initial world being labeled with a user supplied LTL control formula that expresses a control strategy for this domain.
When we expand a world w we progress its formula f through w using the given algorithm, generating a new formula f + .
This new formula becomes the label of all of w's successor worlds (the worlds generated by applying all applicable actions to w).
If f progresses to FALSE , (i.e., f + is FALSE ), then Theorem 3.1 shows that none of the sequences of worlds emanating from w can satisfy our LTL formula.
Hence, we can mark w as a dead-end in the search space and prune all of its successors.
The complexity of evaluating Clause 2 of the progression algorithm depends on the form of the world descriptions.
It requires us to test an atemporal formula in the current world.
Hence, its complexity depends on two things, the complexity of the atemporal formula and the form of the world description.
If the worlds are incompletely described and represented simply as a first-order formula that characterizes some of its properties, then this clause will require theorem proving to evaluate.
If the world is described as a set of atomic formulas and a collection of horn clauses, and if the formula is quantifier free then evaluating the formula in the world might still be tractable.
The efficiency of this step is important however, as we must use this algorithm at every world expanded during plan search.
In our current implementation of TLP LAN we use worlds that are described as sets of positive literals, and we employ a closed world assumption: every positive literal not in this set is assumed to be false.
In this way we need make no restrictions on the atemporal formulas that can appear in our LTL control formula.
In particular, we can use arbitrary first-order action.
This corresponds to infinitely replicating the final world in the sequence of worlds visited by the plan.
Inputs: An LTL formula f and a world w (generated by forward-chaining).
Output: A new formula f + , also expressed as an LTL formula, representing the progression of f through the world w.  Algorithm Progress(f ,w) 1.
Case 2. f =  2 L (i.e.,  contains no temporal modalities): f + := TRUE if w j= ffi FALSE otherwise.
3 f = f1 ^ f2 : f + := Progress(f1 fi w) ^ Progress(f2 fi w) 4. f = :f1: f + := :Progress(f1 fi w) 5. f = f1 : f + := f1 6. f = f1 U f2 : f + := Progress(f2 fi w) _ (Progress(f1 fi w) ^ f ) 7. f = 3f1: f + := Progress(f1 fi w) _ f 8. f = 2f1 : f + := V Progress(f1 fi w) ^ f 9. f = 8 x: ] f1 : f + := fc:wj=(c)g Progress(f1 (x=c)fi w) W 10. f = 9 x: ] f1 : f + := fc:wj=(c)g Progress(f1 (x=c)fi w) Table 1: The progression algorithm.
formulas in our LTL control.
With worlds described by (assumed to be) complete sets of positive literals, we can employ model-checking instead of theorem proving to determine the truth of these formulas in any world, and thus evaluate clause 2 efficiently.
4 Empirical Results Blocks World.
Our first empirical results come from the blocks world, which we describe using the four operators given in table 2.
If we run our planner with the vacuous search control formula 2TRUE 5 , and exploring candidate plans in a depth-first manner, rejecting plans with state cycles, we obtain the performance given in Figure 1.
Each data point represents the average time required to solve 5 randomly generated blocks world problems (in CPU seconds on a SUN1000), where the initial state and the goal state were independently randomly generated.
The same problems were also run using S NLP, a partial order planner [MR91; SBW90], using domain independent heuristic search.
The graph demonstrates that both of these planners hit a computational wall at or before 6 blocks.6 S NLP failed to solve 4 of the six block problems posed; the times shown on the graph include the time taken by the runs that failed.7 5 This formula is satisfied by every sequence, and hence it provides no pruning of the search space.
6 We can note that with 5 blocks and a holding predicate there are only 866 different configurations of the blocks world.
This number jumps to 7057 when we have 6 blocks, and to 65990 when we have 7 blocks.
7 That is, S NLP exceeded the resource bounds we set (on the number of nodes in the search tree).
Note that by including these times in the data we are making S NLP's performance seem better than it really is: S NLP would have taken strictly more time to solve these problems than the numbers indicate.
The same comment applies to the tests described below.
Operator pickup (x) putdown (x) stack (xfi y) unstack (xfi y)  Preconditions and Deletes ontable(x), clear(x), handempty.
holding(x).
holding(x), clear(y).
on(xfi y), clear(x), handempty.
Adds holding(x).
ontable(x), clear(x), handempty.
on(xfi y), clear(x), handempty.
holding(x), clear(y).
Table 2: Blocks World operators.
400 SNLP TLPlan  Time (CPU sec.)
350 300 250 200 150 100 50 0 0  1  2  3  4  5  6  Number of Blocks  Figure 1: Performance of blind search in the blocks world This shows that domain independent heuristic search does not work well in this domain, even for the sophisticated S NLP algorithm.
Domain independent heuristics have difficult exploiting the special structure of blocks world.
Nevertheless, the blocks world does have a special structure that makes planning in this domain easy [GN92], and it is easy to come up with effective control strategies.
A basic one is that towers in the blocks world can be build from the bottom up.
That is, if we have built a good base we need never disassemble that base to achieve the goal.
We can write a firstorder formula that defines when a block x is a good tower, i.e., a good base that need not be disassembled.
4  goodtower(x) = clear(x) ^ goodtowerbelow(x)  4  goodtowerbelow(x) = (ontable(x) ^ : GOAL (9 y :on(xfi y )] _ holding(x))) _ 9 y:on(xfi y)] :GOAL(ontable(x) _ holding(x)) ^ :GOAL(clear(y)) ^ 8 z :GOAL(on(xfi z ))] z = y ^ 8 z :GOAL(on(zfi y))] z = x ^ goodtowerbelow(y) A block x satisfies the predicate goodtower(x) if it is on top of a tower, i.e., it is clear, and the tower below it does not violate any goal conditions.
The various tests for the violation of a goal condition are given in the definition of goodtowerbelow.
If x is on the table, the goal cannot require that it be on another block y nor can it require that the robot be holding x.
On the other hand, if x is on another block y, then x should not be required to be on the table, nor should the robot be required to hold it, nor should y be required to be clear, any block that is  required to be below x should be y, any block that is required to be on y should be x, and finally the tower below y cannot violate any goal conditions.
Our planner can take this first-order definition of a predicate (rewritten in Lisp syntax) as input.
And we can then use this predicate in an LTL control formula where during the operation of the progression algorithm (Table 1) its first-order definition will be evaluated in the current world for various instantiations of its "parameter" x.
Hence, we can use a strategy of preserving good towers by setting our LTL control formula to  2 8 x clear x (  :  (  )]  goodtower(x) )  goodtowerabove(x))fi  (1)  where the predicate goodtowerabove is defined in a manner that is symmetric to goodtowerbelow.
In any world the formula will prune all successor worlds in which a good tower x is destroyed, either by picking up x or by stacking a new block y on x that results in a violation of a goal condition.
Note also that by our definition of goodtower, a tower will be a good tower if none of its blocks are mentioned in the goal: such a tower of irrelevant blocks cannot violate any goal conditions.
Hence, this control rule also stops the planner from considering actions that unstack towers of irrelevant blocks.
What about towers that are not good towers?
Clearly they violate some goal condition.
Hence, there is no point in stacking more blocks on top of them as eventually we must disassemble these towers.
We can define: badtower(x)  4 clear x  =  (  )  ^ :goodtower(x)  And we can augment our control strategy to prevent growing  bad towers, by using the formula:    2 8 x clear x :  ( )]  goodtower(x) ) goodtowerabove(x) ^ badtower(x) ) (:9 y:on(yfi x)] )  (2)  This control formula stops the placement of additional blocks onto a bad tower.
With this control formula only blocks on top of bad towers can be picked up.
This is what we want, as bad towers must be disassembled.
However, a single block on the table that is not intended to be on the table is also a bad tower, and there is no point in picking up such a block unless its final position is ready.
Adding this insight we arrive at our final control strategy for the blocks world:    2 8 x clear x :  ( )]  goodtower(x) ) goodtowerabove(x) ^ ;badtower(x) ) (:9 y:on(yfi x)] ) ^ ontable(x)  ^ 9 y:GOAL(on(xfi y))]:goodtower(y) ) (:holding(x))  (3)  The performance of our planner with these three different control formulas is shown in Figure 2.
As in Figure 1 each data point represents the average time taken to solve 5 randomly generated blocks world problems.
This figure also shows the performance of the P RODIGY planner on these problems.
This planner, like TLP LAN, was run with a collection of hand written search control rules.
In this domain our method proved to be more effective than that of P RODIGY.
In fact, it is not difficult to show that the final control rule yields an O(n2) blocks world planner (where n is the number of blocks in the world).
In particular, the planner can find a near optimal plan (at most twice the length of the optimal) using depth-first search without ever having to backtrack.
Furthermore, there is always an optimal plan admitted by this control formula.
Hence, if we employ breadth-first search our planner will find an optimal plan.
However, finding an optimal plan is known to be NP-hard [GN92].
P RODIGY employed 11 different control rules some of which have similar intuitive content to our control formulas, but with others that required an understanding the P RODIGY planning algorithm.
We would claim that our final control formula is easily understood by anyone familiar with the blocks world.
P RODIGY does end-means analysis, so at every node in its search space it has available a world description.
It would be possible to use our control formulas on this world description.
However, most of the search performed by P RODIGY is a goal regressive search to find an action applicable to the current world.
It is this part of the search that seems to be hard to control.
Bounded Blocks World.
We have also implemented a bounded blocks world in which the table has a limited amount of space.
Dealing with resource constraints of this kind is  fairly easy for a forward-chaining planner, but much more difficult for partial order planners.8 A strategy capable of generating good plans in polynomial time can be written for this domain also, but it is more complex than our strategy for the unconstrained blocks world.
But even very simple strategies can be amazingly effective.
With no search control at all TLP LAN took an average of 470 CPU seconds to solve 10 different randomly generated six block problems when the table had space for 3 blocks.
When we changed the strategy to a simple "trigger" rule (trigger rules are rules that take advantage of fortuitous situations but do not attempt to achieve these situations) the average dropped to 0.48 seconds!
The particular rule we added was: whenever a block is clear and its final position is ready, move it there immediately.
This rule is easily expressed as an LTL formula.
Figure 3 plots the average time taken by TLP LAN to solve 10 random n block reconfiguration problems in the bounded blocks world where the table has space for 3 blocks.
The graph shows TLP LAN's performance using no control knowledge, the simple trigger rule given above, and a complete backtrack-free strategy.
Schedule World.
Another domain we have tested is the P RODIGY scheduling domain.
This domain has a large number of actions and the branching factor of the forward chaining space is large.
Nevertheless, we were able to write a conjunction of natural control formulas that allowed TLP LAN to generate good schedules without backtracking.
In this domain the task is to schedule a collection of objects on various machines to achieve various machining effects.
One example of the control formulas we used is, 8 x:object(x)] 2(polish(x) ^ :polish(x)) ) 2:polish(x), where polish is true of an object x if x is being polished in the current world.
This formula prohibits action sequences where an object is polished twice (basically the transition from being polished to when polishing stops prohibits future polishing).
Another use of the control formulas was to detect impossible goals.
For example, 8 x:object(x)] 8 s:GOAL(shape(xfi s))] s = cylindrical _ 2shape(xfi s).
In the domain the only shape we can create are cylinders.
This formula, when progressed through the initial state, checks every object for which the goal mentions a desired shape to ensure that the desired shape is cylindrical.
If it is not then the object must have that shape in the current state (i.e., in the initial state) and in all subsequent states.
Hence, when we pose a goal such as shape(Afi cone), the planner will immediately detect the impossibility of achieving this goal unless A starts off being cone shaped.
The performance of TLP LAN and S NLP (using domain independent heuristics) is shown in Figure 4.9 The data points in the figure show the average time taken to solve 10 random problems.
The x-axis plots the number of new features the goal requires (i.e., the 8  Resource constraints are beyond the capabilities of S NLP, but see [YC94] for a partial order planner with some ability to deal with resource constraints.
9 We were unable to obtain the P RODIGY control rules for this domain; hence, we omitted P RODIGY from this comparison.
400  Time (CPU sec.)
350 300 250 200 150 Prodigy 4.0 Control Formula 1 Control Formula 2 Control Formula 3  100 50 0 0  5  10  15  20  25  30  35  40  45  50  Number of Blocks  Figure 2: Performance of search control in the blocks world 400  Time (CPU sec.)
350 300 250 200 150 100  No Control Tigger Rule Complete Strategy  50 0 0  5  10  15  20  25  30  35  40  Number of Blocks  Figure 3: Performance of TLP LAN in the bounded blocks World number of machining operations that must be scheduled).10 In the experiments the number of objects are just sufficient to allow goals involving that number of new features (in this domain we can only add a limited number of new features to each object).
TLP LAN was able to solve all of the problems at each data point.
However, S NLP failed to solve 4 of the 3 new feature problems.
It is important to note that we are not using our experiments to claim that TLP LAN is a superior planner to S NLP (or P RODIGY).
After all, in the experiments described above, TLP LAN is being run with extensive control knowledge while S NLP was not.
Hence, it should be expected to outperform S NLP.
What we are claiming is that (1) domain independent heuristic search in real domains is totally inadequate, (2) planning can be effective in these domains with the addition of natural domain dependent search control knowledge, and (3) the TLP LAN approach to search control knowledge in particular is an effective way to utilize such knowledge.
These points are borne out by the data we have presented.
We have only incomplete evidence, given from the blocks world, that our approach to specifying search control knowledge is superior to 10 In this test we did not pose any impossible goals; so did not take advantage of the TLP LAN control formulas designed to detect impyossible goals.
P RODIGY's.
However, we are currently engaging in a more systematic comparison.
What is very clear at this point however, is that our LTL control formulas are much easier to understand, and their behavior easier to predict, than P RODIGY control rules.
In conclusion, we have demonstrated that search control knowledge for forward chaining planners can be expressed in a logical formalism and utilized effectively to produce efficient planners for a number of domains.
We are currently working on extending our planner so that it can plan for temporally extended goals (e.g., maintenance goals) and quantified goals.
TLP LAN is not yet capable of generating plans that satisfy all such goals.
This is due to its handling of eventuality goals like 3p.
If we pose the goal q, then our current implementation will search for a plan whose final state satisfies q.
The temporal formula 3p will be used only as a search control formula, and since it involves only an eventuality that can be postponed indefinitely, this will have no pruning effect.
Hence, our implementation could return a plan that achieves q in the final state but never passes through a state satisfying p. A more sophisticated implementation is required to ensure that eventualities are eventually satisfied and not postponed indefinitely.
In addition, we are working on applying our planner to some practical problem domains.
Finally,  400  Time (CPU sec.)
350 300 250 200 150 100  SNLP  50  TLPlan  0 0  5  10  15  20  25  30  Number of new features  Figure 4: Performance in the Schedule world.
as mentioned above, we are working on automatically generating search control knowledge from operator descriptions along the lines of [Etz93], and on a more systematic comparison with the P RODIGY system.
Acknowledgments: thanks to Adam Grove for extensive discussions; David McAllister for insights into inductive definitions; John McCarthy for the idea of trigger rules; Manuela Veloso and Jim Blythe for help with P RODIGY; and Hector Levesque, Nir Friedman and Jeff Siskind, David Etherington, Oren Ertzioni, and Dan Weld for various useful insights and comments on previous versions.
References [BW94]  A. Barrett and D.S.
Weld.
Partial-order planning: evaluating possible efficiency gains.
Artificial Intelligence, 67(1):71-112, 1994.
[CBE+ 92] J.G.
Carbonell, J. Blythe, O. Etzioni, Y. Gill, R. Joseph, D. Khan, C. Knoblock, S. Minton, A. Perez, S. Reilly, M. Veloso, and X. Wang.
Prodigy 4.0: The manual and turorial.
Technical Report CMU-CS-92-150, School of Computer Science, Carnegie Mellon University, 1992.
[CG87]  E. M. Clarke and O. Grumberg.
Research on automatic verification of finite-state concurrent systems.
In Joe F. Traub, Nils J. Nilsson, and Barbara J. Grozf, editors, Annual Review of Computing Science.
Annual Reviews Inc., 1987.
[Eme90]  E. A. Emerson.
Temporal and modal logic.
In J. van Leeuwen, editor, Handbook of Theoretical Computer Science, Volume B, chapter 16, pages 997-1072.
MIT, 1990.
[ENS92]  K. Erol, D.S.
Nau, and V.S.
Subrahmanian.
On the complexity of domain-independent planning.
In Proceedings of the AAAI National Conference, pages 381-386, 1992.
[Etz93]  Oren Etzioni.
Acquiring search-control knowledge via static analysis.
Artificial Intelligence, 62(2):255-302, 1993.
[GN92]  N. Gupta and D.S.
Nau.
On the complexity of blocksworld planning.
Artificial Intelligence, 56:223-254, 1992.
[HV91]  J. Y. Halpern and M. Y. Vardi.
Model checking vs. theorem proving: a manifesto.
In Proceedings of the International Conference on Principles of Knowledge Representation and Reasoning, pages 325-334, 1991.
[MDBP92] S. Minton, M. Drummond, J. Bresina, and A. Phillips.
Total order vs. partial order planning: Factors influencing performance.
In Proceedings of the International Conference on Principles of Knowledge Representation and Reasoning, pages 83-82, 1992.
[MR91]  D. McAllester and D. Rosenblitt.
Systematic nonlinear planning.
In Proceedings of the AAAI National Conference, pages 634-639, 1991.
[SBW90]  S. Soderland, T. Barrett, and D. Weld.
The SNLP planner implementation.
Contact bug-snlp@cs.washington.edu, 1990.
[Sel94]  B. Selman.
Near-optimal plans, tractability and reactivity.
In Proceedings of the International Conference on Principles of Knowledge Representation and Reasoning, pages 521-529, 1994.
[VB94]  M. Veloso and J. Blythe.
Linkability: Examining causal link commitments in partial-order planning.
In Proceedings of the Second International Conference on AI Planning Systems, 1994.
[YC94]  Q. Yang and A. Chan.
Delaying variable binding committments in planning.
In Proceedings of the Second International Conference on AI Planning Systems, 1994.
MIT Sloan School of Management MIT Sloan Working Paper 4480-04 CISL Working Paper No.
2004-02 February 2004  Effective Data Integration in the Presence of Temporal Semantic Conflicts  Hongwei Zhu, Stuart E. Madnick, Michael D. Siegel  (c) 2004 by Hongwei Zhu, Stuart E. Madnick, Michael D. Siegel.
All rights reserved.
Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission, provided that full credit including (c) notice is given to the source.
This paper also can be downloaded without charge from the Social Science Research Network Electronic Paper Collection: http://ssrn.com/abstract=529722  Effective Data Integration in the Presence of Temporal Semantic Conflicts Hongwei Zhu Stuart E. Madnick Michael D. Siegel  MIT Sloan School of Management 30 Wadsworth Street, Cambridge, MA 02142, USA {mrzhu, smadnick, msiegel}@mit.edu  Working Paper CISL# 2004-02 February 2004  Composite Information Systems Laboratory (CISL) Sloan School of Management Massachusetts Institute of Technology Cambridge, MA 02142  1  This page is blank  2  Effective Data Integration in the Presence of Temporal Semantic Conflicts Hongwei Zhu, Stuart E. Madnick, Michael D. Siegel MIT Sloan School of Management 30 Wadsworth Street, Cambridge, MA 02142 {mrzhu, smadnick, msiegel}@mit.edu  Abstract The change in meaning of data over time poses significant challenges for the use of that data.
These challenges exist in the use of an individual data source and are further compounded with the integration of multiple sources.
In this paper, we identify three types of temporal semantic heterogeneities, which have not been addressed by existing research.
We propose a solution that is based on extensions to the Context Interchange framework.
This approach provides mechanisms for capturing semantics using ontology and temporal context.
It also provides a mediation service that automatically resolves semantic conflicts.
We show the feasibility of this approach by demonstrating a prototype that implements a subset of the proposed extensions.
1 Introduction Effective management of temporal data has become increasingly important in application domains from day-to-day record keeping to counterterrorism efforts.
In some cases, it is even required by law for organizations to store historical data and make sure it is accurate and easy to retrieve1.
While temporal databases can be used to manage the data, ensuring that the retrieved data is meaningful to the users is still an unsolved problem when data semantics changes over time.
As an example, suppose an arbitrage analyst in New York needs to compare DaimlerChrysler's stock prices at New York and Frankfurt exchanges.
He retrieved the data from Yahoo's online historical database, see Figure 1.
Two strange things caught his eyes at a quick glance at the data.
First, prices at two exchanges differ substantially; and second, the price at 1  See Robert Sheier on "Regulated storage" in Computer World, 37(46), November 17, 2003.
Health Insurance Portability Act requires healthcare providers keep records till two years after death of patients; Sarbanes-Oxley Act requires auditing firms retain records of financial statements.
Frankfurt dropped by almost 50% at the turn from 1998 to 1999!
Imagine what conclusions the analyst would draw from this data.
Figure 1.
Historical stock prices for DaimlerChrysler.
Top: New York; Bottom: Frankfurt These unusual observations result from unresolved semantic conflicts between the data sources and the data receiver.
In this case, not only are the currencies for stock prices different at the two exchanges, but the currency at Frankfurt also changed from German Marks to Euros at the beginning of 1999.
Once the data is normalized using this knowledge, it can be seen there is neither significant arbitraging opportunity nor abrupt price plunge for this stock.
We call metadata knowledge such as the currency for price context knowledge, and the history of time varying metadata temporal context.
1  To allow data receivers like the analyst to effectively use data from temporally heterogeneous sources, we need to represent temporal context knowledge and incorporate it into data integration and query answering systems.
This is a challenging problem not addressed by previous research.
Temporal database research has focused on management of temporal data in a homogeneous environment, providing no mechanism for semantic annotation.
Semantic data integration techniques developed so far are based on snapshot data models that ignore the time dimension.
The objective of this research is to fill this gap by developing semantic integration techniques to effectively resolve temporal semantic conflicts between data sources and receivers.
Specifically, we extend the Context Interchange (COIN) framework [6, 10] with temporal contextual knowledge representation and reasoning capability to allow for effective retrieval of data from heterogeneous sources.
We begin in the next section with an example to illustrate various temporal semantic heterogeneities.
In Section 3 we give a brief review of related research.
We outline our approach to these challenges in section 4 and present some preliminary results in Section 5.
In the final section, we summarize and briefly discuss future research.
2 Challenges of temporal data integration 2.1 A simple integration example A temporal database is one that supports some aspect of time, not counting user-defined time such as birthday and hiring date [13].
This rather informal definition is due to the fact that the temporal dimensions are often application specific, therefore it is either difficult or unnecessary to support all aspects of time.
Nevertheless, most temporal data can be viewed as time-stamped propositions and represented as relational tuples with timestamps.
Table 1 gives an example of some time series data for a company.
Intuitively, the example describes how the values of several attributes change over time.
Each tuple represents a fact that can be viewed as a predicate with a timestamp argument and other non-temporal arguments.
However, there are other unspecified metadata attributes, such as currency type and scale factor, that critically determine the truth value of each predicate.
We call the specification of metadata attributes context knowledge.
For metadata attributes whose value changes over time, a specification of their history is termed temporal context.
Table 2 gives examples of the context knowledge in a simple integration scenario involving the source in Table 1 and a possible receiver.
The receiver context can be time varying as well.
Semantic conflicts arise because the source and the receiver have different contexts, which need to be reconciled for the receiver to meaningfully use the data.
Imagine the complexity of integration scenarios that involve tens or even hundreds of sources and receivers with time varying heterogeneous contexts.
We need integration technologies that can effectively manage this complexity.
Table 1.
Company time series data Year ... 1999 2000 2001 2002 ...  Num_Employee  Profit  Tax  5100 12000 25.3 30.6  4.2 13000 20000 35.3  1.1 2500 4800 7.97  Table 2.
Examples of temporal context knowledge Currency Scale factor for profit and tax Scale factor for Num_Employee Profit  Source Francs(FRF), year <= 2000 Euros, year>=2001 1M , year =< 1999 1K, 2000<=year<=2001 1M, year>=2002 1, year<=2001 1K, year>=2002 Exclude tax, year<=2000 Include tax, year>=2001  Receiver USD, always 1K, always 1K, always Include tax, always  2.2 Temporal semantic heterogeneities We see at least three categories of issues in the integration of temporal data.
In the following, we informally describe each using examples.
Representational heterogeneity - the same relational attribute may be represented differently in the time span of a data source.
In addition to currency changes for monetary concepts like profit and tax, there are also scale factor changes, as described in Table 2.
Ontological heterogeneity - the ontological concept represented by an attribute may change over time.
There are many examples of this kind.
In Table 2, profit on and before 2000 excludes taxes, afterwards it includes taxes.
There are also cases where the entity referred to by an identifier changes over time.
For example, stock symbol  2  "C" at New York Stock Exchange (NYSC) used to refer to Chrysler but changed to refer to Citigroup on December 4, 1998 after Chrysler merged with Daimler-Benz and the merged company chose to use symbol "DCX".
Similarly, country code "YUG" for Yugoslavia have different geographic boundaries at different times of the Balkans war.
The derivation method or composition of complex concepts often changes over time.
Many government published time series data sets often come with a "Change of Definitions" that explains changes to terminologies.
For example, the national unemployment data may include undocumented immigrants in the workforce at one time and exclude them at another.
Heterogeneity in temporal entity - the abstraction and representation of time domain differs across systems and time.
Although a temporal entity is just another data type, it has special properties and operations that warrant a category of its own.
The example in Table 1 uses point representation for the timestamp attribute year.
Another system may choose to use intervals, e.g., [1/1/1999, 12/31/1999] for the year 1999.
Differences in calendar systems, time zones, and granularities present many challenges for integration.
The semantics of the association between propositions described by the non-temporal attributes in a tuple and the temporal entity may differ across attributes.
How the truth of a proposition over an interval is related to its truth over subintervals is described by the proposition's heredity properties [23].
Recognizing this property is useful for temporal query language design.
For example, if in a bank account database the balance over an interval is known and the user queries the balance at a time within the interval, the query language should use the liquidity property of balance attribute to infer the result [3].
We observe that heredity is often attribute dependent and does not change over time or across data sources.
Thus we need not consider heterogeneity of this property in the data integration setting.
In an effective integration framework, data receivers should not be burdened by these context heterogeneities; rather, it should be a service of the system to record and use context knowledge to reconcile context differences before delivering  data to the receiver.
Our temporal extension to COIN framework will provide such a solution.
3 Review of Related Research Related research can be found in the areas of temporal database, temporal reasoning, and data integration.
Although it provides useful concepts and techniques, research from each area alone does not address all temporal semantic heterogeneity problems identified in this paper.
The following brief review is not intended to summarize or criticize the findings in each area; rather, it is to identify the most relevant results and show what is missing from a temporal semantic data integration point of view.
3.1 Temporal databases In temporal database research, the time domain is often represented as time points with certain granularities.
An interval is a set of contiguous points and can be represented as a pair of begin and end points.
A time point represents a segment on the continuous time line, thus it has a duration and is not an instant in time ontology [11].
Although both are indivisible, point type has a successor function while instant type does not.
Over 40 temporal data models have been proposed [20], yet only a few have been implemented with very limited commercial impact.
Many of the models let the system manage timestamps, which effectively hide the timestamp attribute from the user.
This approach is inconsistent with relational theory, as articulated in [4], where they explicitly make the timestamp an attribute of the relational schema.
As commonly practiced, databases that store temporal data often have a schema with explicit timestamp attribute(s); standard SQL is used to retrieve data and temporal operations are selectively implemented in the application layer.
Our framework will target the common situation where data sources have limited temporal support.
As in the case of conventional databases, temporal databases also fail to facilitate context knowledge management.
As a result, context is often hard-coded into data transformations in multi-dimensional data warehouses.
This ad-hoc approach lacks flexibility and scalability.
3.2 Temporal reasoning While a restricted set of temporal logics can be executed using logic programming, there  3  seems to be a trend where temporal logics are transformed into temporal constraints to take advantage of the efficiency of constraint solvers.
The framework provided in [17] combines qualitative and quantitative (metric) temporal relations over both time points and time intervals.
These relationships can be considered as temporal constraints in constraint logic programming.
Therefore, temporal reasoning can be treated as a constraint solving problem, to which a number of constraint solving techniques [12] can be applied.
A solver implemented using constraint handling rules (CHR) [9] is demonstrated in [7].
We will exploit this general approach in this research.
Temporal granularity research, both logic [18] and algebraic [3] based, has developed techniques for representing and reasoning about granularities and user-defined calendars.
Conversions between granularities [2] will be useful in dealing with heterogeneities in temporal entity.
3.3 Data integration Approaches to achieving data integration largely fall into tight-coupling and loose-coupling categories depending on whether a global schema is used [6, 10].
In these approaches, intended data semantics in sources and receivers are explicitly incorporated in either the view definitions or the user queries.
The computation complexity in rewriting user queries for the former approach [15] and the user's burden of writing complex queries for the latter significantly limit the flexibility and scalability of these approaches.
A middle ground approach, as articulated in [5] and demonstrated in [6, 10], avoids these shortcomings by encapsulating data semantics into a context theory and maintaining accessibility of source schema by users.
In the COIN approach, the user issues queries as if all sources are in the user's context and a mediator is used to rewrite the queries to resolve semantic differences.
Unfortunately, existing approaches, including COIN, assume static metadata and ignore the temporal aspect of context knowledge.
Consequently, temporal concepts are missing in the ontologies used in these systems.
Research in medical information systems concerns what diagnostic conclusions can be drawn from a set of temporal data (e.g., a series of high temperatures lead to "having a fever" conclusion).
In a seemingly relevant research [19, 22], interpretation contexts are  dynamically constructed using temporal data as input (e.g., from doses of a certain drug infer a certain treatment protocol, which has a side effect of causing high temperatures.
In this interpretation context, high temperatures do not imply a fever).
However, it also assumes that interpretation contexts do not vary in time.
This research will focus on the representation and reasoning of temporal context.
We will develop a framework that incorporates context knowledge into query evaluation process to automatically detect and reconcile temporal semantic conflicts.
By combining the concepts and techniques from the three relevant research areas, we develop a scalable solution to temporal heterogeneity problems.
4.
Temporal COIN approach A recent extension to the COIN framework demonstrates its new capability in solving ontological heterogeneity problems [6].
With various temporal representation and reasoning techniques at our disposal, we feel that COIN can be further extended to handle temporal semantic heterogeneities.
4.1 The COIN framework The COIN framework [6] uses an ontology as a formal mechanism for representing context knowledge and a mediation service to dynamically detect and reconcile semantic conflicts in user queries.
Implemented in abductive constraint logic programming (ACLP) [14], the mediator generates mediated queries (MQs) that serve as intensional answers to the user.
A distributed query executioner generates a query plan and brings extensional answers to the user.
Conversion functions defined internally or externally are called during query execution stage to convert extensional dataset from its source context into receiver's context.
The existing COIN uses a snapshot data model that does not allow temporal context representation; the mediator also lacks temporal reasoning capability.
4.2 Temporal extensions to representation The extended framework admits temporal data sources, which are assumed to be relational with an explicit timestamp attribute in their schema.
They accept SQL queries with usual  4  comparison operators (=, >, <, etc.)
on timestamp domain.
Definition An ontology defines a common semantic domain that consists of semantic data types and their relationships, including 1) is-a relation, indicating a type is a subtype of another; and 2) named attribute relation, indicating the domain and range types of the attribute.
The ontology is augmented with temporal concepts as defined in the time ontology from an early DAML effort [11].
The most general one is called Temporal Entity, which can be further specialized as Instant or Interval.
Each element in the source schema is mapped to a corresponding semantic data type in the ontology by an elevation axiom.
A timestamp can be elevated to Temporal Entity or a finer type in the ontology.
For types whose values are time dependent, we relate them to a temporal entity type via temporal attribute.
There are two kinds of attributes in the ontology.
The range of a regular attribute usually is defined in source schema and obtains its value from the extensional database (EDB).
A contextual attribute, also called a modifier, is an implicit attribute whose value functionally determines the interpretation and the value of the regular attribute [21].
For example, currency and scale factor in Table 1 are contextual attributes for profit and tax.
Definition The temporal context of a data source or a receiver is a specification of the history of all contextual attributes in the ontology.
Mathematically, it is set of <contextual_attribute, history> pairs, where history is a set of <value, valid_interval> pairs.
In existing COIN, a context is simply a set of <contextual_attribute, value> pairs.
The temporal extension allows us to represent the entire history of each contextual attribute.
If the value does not change over time, the history set is simply a singleton with the valid_interval covering entire time span of interest.
Our implementation achieves backward compatibility by treating <contextual_attribute, value> as the shorthand for <contextual_attribute, {<value, entire_time_span>}>.
Temporal entity type may also have contextual attributes, e.g., granularity, time zone, etc., to account for various contexts.
4.3 Temporal extensions to mediation Given a user query expressed as if everything were in user's context, the mediator detects and reconciles semantic differences by examining the assignments to all contextual attributes and applying appropriate conversion functions if the assignment differs between any source and the receiver.
With temporal extensions, contextual attributes are no longer singly valued.
However, at any point in time, there is only one valid value for each attribute.
The mediator needs to find the maximum time interval over which all involved contextual attributes are singly valued.
Over this interval, a mediated query can be generated as in the case of existing COIN; the derived interval appears in the MQ as additional constraints over the attribute of temporal entity type.
The mediator translates history pairs for contextual attributes into temporal constraints, which are posted into a constraint store concurrently solved by solvers written in CHR.
Through back tracking, all intervals over which contextual attributes are singly valued are found.
In our framework, contexts are declaratively defined using First Order Logic rules.
The mediator is a general abductive reasoner.
When new sources are added, we only need to add context rules for them.
External functions can also be called to convert between contexts.
These features lend COIN great flexibility and scalability.
5 Prototype and preliminary results We are able to solve a range of temporal heterogeneity problems exemplified in Section 2 by implementing a fraction of the suggested extensions.
5.1 Representation Figure 2 shows the ontology we constructed for the example.
Here, we use the most general concept temporal entity, which can represent both instants and intervals.
Using elevation axioms, we create the mappings between attributes in each data source and the types in the ontology, as shown in Figure 2.
5  constraints are always true, we will not include this predicate for this case.
basic scale sem_number  currency tempAttr  temporalEntity  tempAttr  monetaryValue  scale  type profit Year ... 1999 2000 2001 2002 ...  Num_Employee  Profit  Tax  5100 12000 25.3 30.6  4.2 13000 20000 35.3  1.1 2500 4800 7.97  Legend Semantic type Subtype rel.
Attribute Contextual attr.
Elevation  Figure 2.
Example ontology and elevation We model the time line as discrete and unbounded with both points and intervals as primitives.
The past and future infinities are represented by constants bottom and top.
Intervals are represented with begin and end points, which can be accessed by functions begins and ends.
We implement the <= relation between points as a tle constraint.
The contains relation between an interval and a point is translated into tle constraints; the overlaps relation between intervals are also translated into tle constraints.
This simple model has all the expressive power to represent the kinds of temporal knowledge in Table 2.
For example, internally we use the following Prolog statements to represent the source context for currency: modifier(monetaryValue,O,currency,c_src,M):containObj([bottom, 2000], O), cste(basic, M, c_src, "FRF").
modifier(monetaryValue,O,currency,c_src,M):containObj([2001, top],O), cste(basic, M, c_src, "EUR").
The head of the statement reads: O of type monetaryValue has a contextual attribute (i.e., modifier) currency, whose value in source context c_src is M. Its body has two predicates.
containObject(I, O) will use the tempAttr of O to obtain its temporal attribute T (which corresponds to Year attribute in the EDB) of type temporalEntity and add constraint contains(I, T).
The helper predicate cste specifies the primitive value of M in c_src context.
Thus, the history of each contextual attribute is now a set of pairs <Vi, Ii>, where UiI i = [bottom, top].
For context knowledge that does not change over time, we could have used [bottom, top] interval in containObj predicate.
Since the translated  5.2 Mediation As described earlier, the mediation service needs to find the maximum interval over which all contextual attributes are singly valued.
Figure 3 helps visualize this task by graphically representing the context knowledge in Table 2.
For example, [bottom, 1999] is such an interval where the source context can be described with a set of singly valued contextual attributes: {<monetaryValue.currency, "FRF">, <monetaryValue.scale, "1000000">, <profit.type, "taxExluded">, <sem_num.scale, "1">}.
c_src: source context monetaryValue currency scale  1M 1999  FRF  profit type  Excl Tax  c_target: receiver context  sem_num scale  monetaryValue currency scale  1K  USD  2001 EUR 1M  Incl Tax  sem_num scale  1  2000  2002  profit type  1K  Incl Tax  1K  1K  Figure 3.
Visualization of temporal context  Recall that we translate all temporal relations into tle constraint over points.
Each contextual attribute will generate two tle constraints for the temporal variable.
The above problem is thus turned into to a problem of solving the constraints generated by all the contextual attributes, which is solved concurrently using a solver implemented in CHR.
Constraints over bottom and top can be removed using simplification rules so that these two literals do not appear in the list of abducted predicates.
Constraints over other time points can be pair-wise simplified.
We also implement overlaps to simplify tle constraints over four points at a time.
These rules tighten the bounds of the temporal variable or signify a failure if inconsistencies are found.
Along with rules that handle equality constraint, this point based temporal constraint solver covers the 13 relations for temporal intervals in Allen [1].
Relations before, after, meets, and met_by  6  generate a failure, all the rest relations are subsumed into overlaps.
Through backtracking, the recursive algorithm finds all intervals over which contextual attributes are singly valued.
Conversions are applied as in the case of existing COIN implementation.
This simple temporal constraint based extension transforms a temporal context problem into a set of non-temporal problems, thereby allows us to reuse the non-temporal implementation of the COIN mediator.
5.3 Preliminary results.
These temporal extensions to COIN framework allow us to achieve semantic interoperability for the integration example.
The prototype can generate MQs that reconcile temporal context differences.
As an example, suppose a user in the receiver context wants to retrieve data from the company time series relation named Financials in Table 1.
So he issues the following SQL query: Select Year, Num_Employee, Profit From Financials;  and expects the returned data to be in his context.
The query is translated into a well formed Datalog query in our prototype system.
The extended COIN mediator takes this query and the representation of the integration as input, and produces the following mediated query in Datalog (which COIN eventually converts to SQL): %1.
=<1999: currency,scale,type;scale answer('V32', 'V31', 'V30') :'V29' is 'V28' * 1000.0, 'V31' is 'V27' * 0.001, olsen("FRF", "USD", 'V26', 'V32'), 'V28' is 'V25' * 'V26', financials('V32', 'V27', 'V25', 'V24'), 'V32' =< 1999, 'V23' is 'V24' * 'V26', 'V22' is 'V23' * 1000.0, 'V30' is 'V29' + 'V22'.
%2.
2000: currency and type;scale answer(2000, 'V21', 'V20') :'V21' is 'V19' * 0.001, financials(2000, 'V19', 'V18', 'V17'), olsen("FRF", "USD", 'V16', 2000), 'V15' is 'V18' * 'V16', 'V14' is 'V17' * 'V16', 'V20' is 'V15' + 'V14'.
%3.
2001: currency;scale answer(2001, 'V13', 'V12') :'V13' is 'V11' * 0.001, financials(2001, 'V11', 'V10', 'V9'), olsen("EUR", "USD", 'V8', 2001), 'V12' is 'V10' * 'V8'.
%4.
>=2002: currency,scale;none answer('V7', 'V6', 'V5') :olsen("EUR", "USD", 'V4', 'V7'),  financials('V7', 'V6', 'V3', 'V2'), 2002 =< 'V7', 'V1' is 'V3' * 'V4', 'V5' is 'V1' * 1000.0.
The mediated query has four subqueries, each resolves a set of semantic conflicts that exist in the time specified by the timestamp attribute.
Note that olsen predicate corresponds to a currency conversion data source introduced by the conversion function for currency contextual attribute.
Readers are encouraged to walkthrough the subqueries to verify that they resolve all the semantic conflicts in Table 2 or in Figure 3.
6.
Discussion and future plan We have identified three types of semantic heterogeneity in the integration of temporal data.
There is an ever increasing need to efficiently handle these heterogeneities as more historical data is used for auditing, mining, forecasting, investigation, and many other purposes.
We proposed temporal extensions to the COIN framework.
A prototype of the extensions shows that our approach is capable of solving temporal context problems.
With its declarative knowledge rule base and its capability of calling external functions, this approach is flexible and extensible.
Our future research aims to develop this approach in several aspects.
Current representation of temporal context explicitly compares an interval with the temporal attribute of an object.
The representation may be made cleaner by using an annotated temporal constraint logic [8].
We need to investigate how this logic can be integrated with the ACLP implementation of COIN mediator.
The most important part of future research will be focused on the heterogeneities of temporal entities.
Intuitively, we can imagine a solution that adds various contextual attributes to the temporal entity type in the ontology and relies on external functions to convert between contexts.
If this is not expressive enough to represent the diversity of time, a richer time ontology may be necessary.
We also need to incorporate metric temporal reasoning, which often involves computations of one or more calendars.
We will investigate the feasibility of leveraging web services like those in [2].
This is a challenging and important research area because misunderstanding date and time can have serious consequences, as history has shown in an 1805 event [16] where the Austrian troops were  7  forced to surrender largely because of the misunderstanding of a date in two different calendar systems.
Acknowledgements: The study has been supported, in part, by MITRE Corp., MIT-MUST project, the Singapore-MIT Alliance, and Suruga Bank.
References [1] J. F. Allen, "Maintaining Knowledge about Temporal Intervals," Communications of the ACM, vol.
26, pp.
832-843, 1983.
[2] C. Bettini, "Web services for time granularity reasoning," presented at TIME-ICTL'03, 2003.
[3] C. Bettini, S. Jajodia, and X. S. Wang, Time Granularities in Databases, Data Mining, and Temporal Reasoning: Springer, 2000.
[4] C. J.
Date, H. Darwen, and N. A. Lorentzos, Temporal Data and the Relational Model: A Detailed Investigation into the Application of Interval Relation Theory to the Problem of Temporal Database Management: Morgan Kaufmann Publishers, 2003.
[5] A. Farquhar, A. Dappert , R. Fikes, and W. Pratt, "Integrating Information Sources Using Context Logic," presented at AAAI Spring Symposium on Information Gathering from Distributed Heterogeneous Environments, 1995.
[6] A. Firat, "Information Integration using Contextual Knowledge and Ontology Merging," PhD Thesis, MIT, 2003.
[7] T. Fruhwirth, "Temporal Reasoning with Constraint Handling Rules," ECRC-94-5, 1994.
[8] T. Fruhwirth, "Temporal Annotated Constraint Logic Programming," Journal of Symbolic Computation, vol.
22, pp.
555-583, 1996.
[9] T. Fruhwirth, "Theory and Practice of Constraint Handling Rules," Journal of Logic Programming, vol.
37, pp.
95-138, 1998.
[10] C. H. Goh, S. Bressan, S. Madnick, and M. Siegel, "Context Interchange: New Features and Formalisms for the Intelligent Integration of Information," ACM TOIS, vol.
17, pp.
270-293, 1999.
[11] J. R. Hobbs, "A DAML Ontology of Time," 2002.
[12] J. Jaffar and M. J. Maher, "Constraint Logic Programming: A Survey," Journal of Logic Programming, vol.
19/20, pp.
503-581, 1999.
[13] C. S. Jensen and e.
al., "The Consensus Glossary of Temporal Database Concepts---February 1998 Version," 1998.
[14] A. C. Kakas, A. Michael, and C. Mourlas, "ACLP: Integrating Abduction and Constraint Solving," Journal of Logic Programming, vol.
44, pp.
129177, 2000.
[15] M. Lenzerini, "Data integration: a theoretical perspective," presented at 21st ACM SIGMODSIGACT-SIGART symposium on Principles of database systems, 2002.
[16] S. E. Madnick, "Metadata Jones and the Tower of Babel: The Challenge of Large-Scale Semantic Heterogeneity," presented at 1999 IEEE MetaData Conference, 1999.
[17] I. Meiri, "Combining Qualitative and Quantitative Constraints in Temporal Reasoning," presented at AAAI 91, 1996.
[18] A. Montanari, "Metric and Layered Temporal Logic for Time Granularity," PhD Thesis, University of Amsterdam, 1996.
[19] J. H. Nguyen, Y. Shahar, S. W. Tu, A. K. Das, and M. A. Musen, "Integration of Temporal Reasoning and Temporal-Data Maintenance Into A Reusable Database Mediator to Answer Abstract, TimeOriented Queries: The Tzolkin System," Journal of Intelligent Information Systems, vol.
13, pp.
121-145, 1999.
[20] G. Ozsoyoglu and R. T. Snodgrass, "Temporal and Real-Time Databases: A Survey," IEEE Transactions on Knowledge and Data Engineering, vol.
7, pp.
513-532, 1995.
[21] E. Sciore, M. Siegel, and A. Rosenthal, "Using Semantic Values to Facilitate Interoperability Among Heterogeneous Information Systems," ACM TODS, vol.
19, pp.
254-290, 1994.
[22] Y. Shahar, "A Knowledge-Based Method for TemporalL Abstraction of Clinical Data," Stanford University, 1994.
[23] Y. Shoham, "Temporal Logics in AI: Semantical and Ontological Considerations," Artificial Intelligence, vol.
33, pp.
89-104, 1987.
8
Speeding up temporal reasoning by exploiting the notion of kernel of an ordering relation Luca Chittaro, Angelo Montanari Dipartimento di Matematica e Informatica Universita di Udine, Via delle Scienze, 206 33100 Udine - ITALY {chittaro|montana}@dimi.uniud.it  Iliano Cervesato Dipartimento di Informatica Universita di Torino, Corso Svizzera, 185 10149 Torino - ITALY iliano@di.unito.it  1 Introduction In this paper, we consider the problem of expediting temporal reasoning about partially ordered events in Kowalski and Sergot's Event Calculus (EC).
EC is a formalism for representing and reasoning about events and their effects in a logic programming framework [7].
Given a set of events occurring in the real world, EC is able to infer the set of maximal validity intervals (MVIs, hereinafter) over which the properties initiated and/or terminated by the events maximally hold.
Event occurrences can be provided with different temporal qualifications [1].
In this paper, we suppose that for each event we either specify its relative position with respect to some other events (e.g., event e 1 occurs before event e2) or leave it temporally unqualified (the only thing we know is that it occurred).
Database updates in EC provide information about the occurrences of events and their times [6] and are of additive nature only.
We assume here that the set of events is fixed, and the input process consists in the addition of ordering information.
We will show how the introduction of partial ordering heavily increases the computational complexity of deriving MVIs.
Then, we will provide a precise characterization of what EC actually does to compute MVIs, and propose a solution to do it efficiently when only incomplete information about event ordering is available.
The paper is organized as follows.
In Section 2, we introduce the basic features of EC with relative times and partial ordering.
In Section 3, we analyze its computational complexity, that turns out to be exponential.
Moreover, we show how complexity can be reduced to polynomial (O(n5)) by adopting a graph marking technique that speeds up search.
In Section 4, we provide a deeper analysis of how EC derives MVIs, and we formally introduce the notion of kernel of an ordering relation.
Then, we show how the notion of kernel can be usefully applied to further reduce the complexity of computing MVIs.
Section 5 discusses an example, also contrasting the set of MVIs with the sets of necessarily and possibly true MVIs.
Section 6 concludes the paper.
2 The Event Calculus with relative times and partial ordering EC takes the notions of event, property, time-point and time-interval as primitives and defines a model of change in which events happen at time-points and initiate and/or terminate time-intervals over which some property holds.
Time-points are unique points in time at which events take place instantaneously.
Time-intervals are represented as pairs  of time-points.
EC embodies a notion of default persistence according to which properties are assumed to persist until an event occurs which terminates them.
In this paper, we focus our attention on situations where precise temporal information for event occurrences is not available.
We represent the occurrence of an event e of type tye by means of the clause: happens(e,tye).
The relation between types of events and properties is defined by means of initiates and terminates clauses: initiates(tye, p1).
terminates(tye, p2).
The initiates (terminates ) clause relates each type of event tye to the property p it initiates (terminates).
The plain EC model of time and change is defined by means of the axioms: holds(period(Ei,P,Et)):happens(Ei,TyEi), initiates(TyEi,P), happens(Et,TyEt), terminates(TyEt,P), before(Ei,Et), not broken(Ei,P,Et).
(1.1)  broken(Ei,P,Et):(1.2) happens(E,TyE), before(Ei,E), before(E,Et), (initiates(TyE,Q);terminates(TyE,Q)), (exclusive(P,Q);P=Q).
The holds axiom states that a property P maximally holds between events Ei and Et if Ei initiates P and occurs before Et that terminates P, provided there is no known interruption in between.
The negation involving the predicate broken is interpreted using negation-as-failure.
The broken axiom states that a given property P ceases to hold if there is an event E that happens between Ei and Et and initiates or terminates a property Q that is exclusive with P .
The exclusive( P , Q ) predicate is a constraint to force the derivation of P to fail when it is possible to conclude that Q holds at the same time.
Finally, the condition P = Q constrains interferences due to incomplete sequences of events relating to the same property.
It indeed guarantees that broken succeeds also when an initiating or terminating event for property P is found between the pair of events Ei and Et starting and ending P respectively.
The exclusive facts have obviously to be defined for each specific application (e.g.
exclusive(p,q).
).
Finally, knowledge about the relative ordering of events is expressed by means of facts of the form beforeFact( e 1 , e 2 ) .
The predicate b e f o r e used in h o l d s and b r o k e n is defined as the transitive closure of beforeFact :  before(E1,E2):beforeFact(E1,E2).
(1.3)  before(E1,E2):beforeFact(E1,E3),before(E3,E2).
(1.4)  The ordering information is entered through the predicate updateOrder( e 1 , e 2 ) : updateOrder(E1, E2) :assert(beforeFact(E1, E2)).
(1.5)  We assume that the set of ordered pairs is always consistent as it grows.
This means that before is supposed to represent a relation that is irreflexive, anti-symmetric and transitive.
The axioms of EC, shown as clauses (1.1-5), will be referred as program 1 in the following.
3  A complexity analysis  In the case of EC with absolute times and total ordering, the worst case complexity of deriving all the MVIs for a given property has been proven to be O(n3), where n is the number of recorded events [3].
In this section, a worst case complexity analysis will be carried out for EC with relative times and partial ordering.
We consider an EC database consisting of a set of events E={e1,...,en} and a set w of elements (ei,ej) whose transitive closure w + is a strict ordering relation on ExE.
The cost is measured as the number of accesses to the database to unify facts during the computation.
Some hashing mechanism is assumed so that fully-instantiated atomic goals are matched in one single access to a sequence of variable-free facts in case of success, and do not need any access in case of failure.
The complexity is given as a function of the number n of recorded events.
3.1  The complexity of EC with relative times and partial ordering  Queries have the form h o l d s ( p e r i o d ( E i , p , E t ) ) , where Ei and Et are variables and p can be either a variable or a constant.
The update predicate is always called with ground arguments.
For each predicate, we now analyze the cost of finding all its solutions.
u p d a t e O r d e r ( e 1 , e 2 ) : a call to this predicate has unitary cost since it only results in asserting a new fact in the database.
happens(Ei,TyEi) and happens(Et,TyEt) : each of this goal succeeds n times, since n events are recorded in the database.
So, the cost of each is O(n).
i n i t i a t e s ( T y E i , P ) and t e r m i n a t e s ( T y E t , P ) : the cost of these predicates is constant for a ground TyEi (or TyEt) even when they are called with P uninstantiated (as in clause 1.2).
exclusive(P,Q) : this predicate is always called ground and it thus can be matched against at most one fact in the database.
The cost is therefore constant.
beforeFact(E1,E2) : When called ground, as in clause (1.3), the query cost is constant.
In clause (1.4) instead, the call results in instantiating a variable.
The complexity is given as the maximum number of matching facts in the  database.
Having n nodes, at most n-1 edges can start from a given node.
Thus, when called with one variable argument, this predicate has cost O(n).
before(Ei,Et) : we will show that the standard twoclauses definition of before (clauses 1.3-4) has a worst case complexity that is at least exponential.
Consider n>=4 events arranged as shown in figure 1: all the n events but two (en-1 and en) participate in a total order between e1 and en2 .
All the transitive pairs, but the pair ( e 1 , e n ) are explicitly specified by means of beforeFact .
We assume that beforeFact(e 1 ,e n-1 ) textually follows any other fact of the form beforeFact(e 1 ,e i ) , for i = 2 ..n-2 .
en-2  Due to the operational behavior of PROLOG, EC thus tries to prove before(e1,en)  m = n-3  looking for a path that passes through e n - 2 e 1 e n-1 before attempting the en (only possible) path Figure 1 that includes e n - 1 .
Backtracking due to the failure in proving before(e n-2 ,e n ) causes every path from e 1 to e n-2 to be unsuccessfully attempted.
The resulting cost is computed as follows.
Let m=n-3 be the length of the longest path between e 1 and e n-2 .
We have the following recursive relation, where Cbf(m) represents the cost of finding all the solutions to the goal before(e',e") in case a total order of length m exists between e' and e" (the value of m highlighted in Figure 1 concerns the pair (e1,en-2)): Cbf (1) = 1;  m= 1 m -1  Cbf ( m ) = m + [?]
Cbf (i )  m >1  i =1  Indeed, there are m edges (represented by beforeFact ) starting from e' , which we can traverse to go towards e" .
One of these edges directly reaches e" (clause 1.3).
If m>1, each of the remaining m-1 edges leads to a node e (clause 1.4), reducing the problem to size i, where i is the length of the longest path between e and e".
In order to give an analytical form for Cbf(m), let us unfold the expression for Cbf(m): Cbf(m)=m+Cbf(m-1)+ Cbf(m-2) + Cbf(m-3) +...  +Cbf(1)=  =m +  (m-1)+2Cbf(m-2) +2Cbf(m-3) +... +2Cbf(1)=  =m+  (m-1) +  =m+ 20(m-1)+  2(m-2) +4Cbf(m-3) +... +4Cbf(1)= 21(m-2) +  22(m-3) +... +2m-2(1)  We can summarize this formula as: m- 1  Cbf (m ) = m +  m- 1  m -2  [?]
2i-1 ( m - i ) >= [?]
2i -1 = [?]
2 j = 2 m-1 - 1  i =1  i =1  j =0  Therefore the cost is at least O(2 m ).
Since we set m=n-3, before with both arguments instantiated turns out to be at least exponential in the worst case.
broken(ei,p,et) : since the definition of this predicate contains calls to before , this goal has an exponential  complexity.
holds(period(Ei,p,Et)) : reasoning as for broken , we obtain an exponential cost too.
The major results of the preceding analysis are the constant cost of updating ordering information (updateOrder ) and the exponential query complexity (holds).
3.2  The addition of marking The exponential cost resulting from the previous analysis makes EC with relative times not appealing for practical computations.
It is interesting to note that this very high cost origins from the calls to b e f o r e .
Can this predicate be re-implemented with a lower cost?
We use before to check whether a pair of nodes belongs to the transitive closure of a cycle-free relation.
Well-known algorithms for this kind of operations have polynomial complexity in the number of nodes.
Unfortunately, these algorithms are more suited to be implemented using traditional programming languages rather than logic programming.
We will now present a PROLOG program implementing a marking algorithm.
We foresee that it is written using extra-logical features of PROLOG.
Therefore, it will hardly be classified as a logic program.
Nevertheless, this program can be considered acceptable: once we have proven that the two versions of before behave coherently, we can see the non-declarative procedure as an actual implementation of the logical one.
before(E1, E2) :markingBefore(E1, E2), !, unmarkAll.
before(E1, E2) :unmarkAll, fail.
markingBefore(E1, E2) :beforeFact(E1, E2), !.
markingBefore(E1, E2) :beforeFact(E1, E3), happens(E3, _, unmarked), mark(E3), markingBefore(E3, E2).
(2.1)  unmarkAll :happens(E, _, marked), !, unmark(E), unmarkAll.
unmarkAll.
unmark(E) :retract(happens(E, TyE, marked)), assert(happens(E, TyE, unmarked)).
mark(E) :retract(happens(E, TyE, unmarked)), assert(happens(E, TyE, marked)).
happens(E,TyE):happens(E,TyE,_).
(2.5)  (2.2) (2.3) (2.4)  (2.6) (2.7) (2.8) (2.9)  Program 2 The idea is very simple: during the search, nodes are marked as they are visited, and only edges leading to not yet marked nodes are analyzed.
We need to change the arity of the predicate happens to support marking: the third argument contains either marked or unmarked with the obvious meaning.
Initially all the nodes are unmarked.
The purpose  of clause (2.9) is to maintain the one parameter interface to happens.
Let us now prove that the two versions of before compute the same relation, given the same factual database.
The declarative program consisting of clauses (1.3-4) yields before(e',e") if and only if the database contains a path leading from e' to e" , where the edges are represented by beforeFact (a simple inductive proof can be found in [2]).
We will now prove that the database contains that path if and only if program 2 derives m a r k i n g B e f o r e ( e ' , e " ) , and consequently before(e',e").
First suppose that the database contains at least a path p=( e' ,e 1 ),(e 1 ,e 2 ),...,(e k-1 ,e k ), (e k ,e" ) of length k+1 that links nodes e' and e" passing through k intermediate nodes and that all nodes are initially unmarked (the validity of this condition after each execution of before is guaranteed by the execution of the unmarkAll predicate occuring in clauses (2.1) and (2.2)).
If there is more than one path from e' to e" , let p be that path whose first edge comes first in the listing of beforeFact (if there is more than one path from e' to e" with this edge as its first edge, then let p be the path whose second edge comes first in the listing of beforeFact ; and so on).
In other words, this selected path is the first path from e' to e" considered by the PROLOG control strategy.
Let us prove that markingBefore(e',e") is derivable from program 2, and only nodes e 1,e 2,..,e k of the selected path are marked during this process.
The proof is inductive in the length of the selected path.
The case where the length of the path is 1, i.e.
there are 0 intermediate nodes and p=(e',e"), is caught by clause (2.3).
We assume, as inductive hypothesis, that the statement holds for length k (i.e.
k-1 intermediate nodes), and we prove that it holds for length k+1 (i.e.
k intermediate nodes).
Let us consider an instance of clause (2.4) where E 1 = e ' and E 2 = e " .
The first subgoal matches beforeFact(e',e 1 ) in the database, instantiating E3 to e 1 .
By hypothesis, all nodes are initially unmarked and e 1 can not have been marked up to now: if e1 were marked, then it would have been already reached by traversing another path, but this would contradict the hypothesis that p (to which e1 belongs) is the first path from e' to e" considered by the PROLOG control strategy.
Therefore, the second subgoal, happens(e 1 ,_,unmarked) , is immediately provable.
Moreover, the presence of this fact in the database causes the subgoal mark(e1) to mark e1 by clause (2.8).
By inductive hypothesis, markingBefore(e 1 ,e") is derivable and nodes e2,..,ek of the selected path are marked as a by-product of the process.
Thus, the overall clause succeeds and proves the goal markingBefore(e',e") .
Conversely, suppose that m a r k i n g B e f o r e ( e ' , e") is derivable from program 2.
We proceed by induction on the height h of the resolution tree for markingBefore(e',e") .
If h=1, then clause (2.3) has been applied, and the database contains the fact  beforeFact(e',e").
Otherwise, we assume, as inductive hypothesis, that the statement holds for every tree of height lower than h and prove its validity for trees of height h. Since h>1, clause (2.4) must have been selected at the first step.
Thus, b e f o r e F a c t ( e ' , e 1 ) a n d markingBefore(e 1 ,e") have successful derivations for some node e1.
By inductive hypothesis, the derivability of the former goal implies that there is a path p'=(e1,e2),...,(ek,e") between nodes e1 and e".
Considering (e',e1) together with p', we obtain the desired path.
We will now evaluate the cost of b e f o r e as implemented by program 2, and show the impact on EC of substituting clauses (1.3-4) of program 1 with program 2.
Let us first compute the cost of m a r k i n g B e f o r e and unmarkAll .
It is easy to show that the latter is always called.
Thus, the complexity of before is equal to the sum of the costs of the two.
The predicate m a r k i n g B e f o r e is designed to be called with both arguments instantiated.
Since clause (2.4) marks a node before the recursive call and since the number of nodes (initially all unmarked) is n, this predicate is called at most n times.
Moreover, each execution of m a r k i n g B e f o r e involves at most n -1 accesses to a b e f o r e F a c t fact.
Therefore, the overall cost for this predicate is O(n2).
Since at most n nodes have been marked by m a r k i n g B e f o r e , u n m a r k A l l has cost O ( n ) .
Therefore, before costs at most O(n2).
This upper bound is reached in the situation of Figure 1.
After integrating this version of before into EC, the cost of holds becomes polynomial, dropping from O(2n) to O(n5).
Indeed, if there are k events initiating p and h events terminating p in the database, the call to h a p p e n s ( E i , T y E i ) in the body of h o l d s , with E i unbound, can succeed n times but initiates(TyEi,P) will succeed only for k of the identified events.
Analogously, h a p p e n s ( E t , T y E t ) succeeds n times but terminates(TyEt,P) retains only h events.
As a result, k*h pairs of events are allowed to reach before(Ei,Et) .
Since k+h <= n, the product k*h is maximum for k=h=n/2, resulting in a quadratic number of pairs.
For each pair, both b e f o r e ( E i , E t ) and not broken(Ei,P,Et) will be considered in the worst case.
The cost of the former has been proved to be O(n 2 ) above, while the cost of the latter is evaluated as follows: the first goal in broken is called with an uninstantiated argument and can succeed n times; for each success, at worst two calls to before (2*O(n 2 )) and three constant cost calls (initiates and terminates with the first argument bound, and exclusive ) are performed.
Therefore, the cost of b r o k e n turns out to be cubic (O(n)*O(n2)).
Returning to holds, we obtain a cost equal to O(n2)*O(n3)=O(n5).
4 What the Event Calculus actually does and how to do it efficiently In this section, we aim at getting a better understanding  of what EC actually does when computing MVIs.
This insight allows to design more efficient versions of EC.
The representation of the ordering of events is of primary importance.
Let E = { e 1 ,...,e n } be the set of events, represented in EC by the predicate happens .
The ordered pairs beforeFact( e i , e j ) contained into the database constitute a set w[?
]ExE .
Notice however that the main axioms of EC never access w (i.e.
beforeFact facts) directly.
Instead, they rely heavily on the predicate before that models the transitive closure of w. Let us denote as w+ the transitive closure of w. w + is a strict order on E, i.e.
a relation that is irreflexive, asymmetric and transitive.
w is a subset of w+ and can indeed be viewed as a specification of it.
It is easy to prove that w+ can contain a quadratic number of edges; indeed the maximum number of edges n*(n-1)/2 is reached when w + is a total order.
From a graph-theoretic point of view, w corresponds to a directed acyclic graph G on E , whose nodes are event occurrences and such that there exists an edge from node ei to ej if and only if the pair (ei,ej) belongs to w., while w+ corresponds to its completion G+.
In order to compute the set of MVIs for a property p, the predicate holds in program 1 considers every pair of events e i , e j such that e i initiates p and e j terminates p, checks whether (ei,ej) belongs to w +, i.e.
if G + contains an edge from ei to ej, and ascertains that no interrupting event e occurs in between, i.e.
that G + does not contain any node e associated to a property q exclusive with p (or associated to the property p itself) such that (ei,e)[?
]G+ and (e,ej)[?]G+.
This approach presents two drawbacks.
First, EC blindly picks up every pair consisting of an event initiating p and an event terminating p, and only later looks for possible interruptions.
We would like instead to include the determination of possibly interrupting events within the search of candidate MVIs for p (i.e.
pairs (ei,ej) such that ei initiates p and ej terminates it).
Second, since G+ is implicit in G, we showed in the previous section that the cost of checking whether an edge belongs to G + by means of the marking version of before is proportional to the number of edges in G, i.e.
to the number of beforeFact in the database.
Both problems can be solved by shifting the emphasis from the transitive closure w + of w to its antitransitive closure, or kernel, w -.
w - is the least subset of w such that (w-)+=w+ and it can be obtained by removing every pair (e i,e j) from w such that (e i,e)[?
]w + and (e,e j)[?
]w + for some event e. The notion of kernel induces a subgraph G- of G that does not contain any transitive edge.
The number of edges in G - is strictly lower than n*(n-1)/2 and is indeed linear in most cases (as in the example reported in Section 5).
The results of Section 3 call for optimization in those cases (we expect them to be the most frequent) where the critical operation is querying for the validity of a certain property.
In these circumstances, the upper bound that comes out from the previous analysis is still not acceptable.
The solution we propose in the following operates on the representation of ordering information in the database.
We  will first introduce the proposed technique in the case where a single property is involved.
Then, the solution will be generalized in order to account for multiple properties.
4.1  Storing and updating the kernel In order to store and update the kernel of the ordering relation, clause (1.5) of program 1 must be replaced with the following program: updateOrder(E1, E2) :(3.1) not before(E1, E2), assertOP(E1,E2).
assertOP(E1, E2) :beforeFact(EA, EB), retractInBetween(E1,EA,EB,E2).
(3.2)  assertOP(E1, E2) :assert(beforeFact(E1, E2)), !.
(3.3)  retractInBetween(E1,EA,EB,E2) :(E1=EA; before(EA, E1)), !, (EB=E2; before(E2, EB)), !, retract(beforeFact(EA, EB)), fail.
(3.4)  Program 3 When the edge (e1,e2) is already entailed by the current ordering relation, it is not added to the representation, in order to maintain minimality.
This case is caught by clause (3.1) through the negative call to before.
Co-operating clauses (3.2-4) deal with the complementary case, i.e.
edge (e1,e2) is not subsumed by the current ordering.
Edge (e1,e2) is added to the representation by clause (3.3).
Before doing this, edges becoming redundant because of transitivity must be located and retracted from the database.
As shown by figure 2 (where the thin lines represent sequences of zero or more chained instances of beforeFact ), adding the edge (e1,e2) can close a transitive relation between nodes eA (possibly e1 ) and eB (possibly e2).
This is problematic when there exists already a direct link between these two nodes: this previously e1 e2 inserted link has now become redundant and eB eA must be removed.
This is done by Figure 2 clauses (3.2) and (3.4).
Notice that there may exist several pairs of the above kind, and all of the corresponding edges must be retracted.
This is achieved through backtracking by forcing the failure of clause (3.4).
When every possibility has been examined, the execution finally backtracks to clause (3.3) that succeeds asserting (only once) the added edge.
The cost of these operations is the following: retractInBetween(E1,EA,EB,E2) : this predicate is called ground and its cost thus corresponds to the cost of two ground calls to before - 2O(n2) - plus the constant cost of performing the retraction.
Notice that no backtracking is allowed within this clause.
The resulting cost is therefore quadratic.
assertOP(E1,E2): this predicate calls retractInBetween for each element of the kernel of the ordering relation (clause 3.2) and then asserts the ordered pair of interest (clause 3.3).
We showed that an upper bound for  the number of edges of the kernel is at most O (n 2 ) .
Therefore, a call to this predicate can cost at most O(n4).
updateOrder(E1,E2) : this predicate is called ground and its cost is given by the cost of one negative call to b e f o r e , and one to a s s e r t O P , resulting in O (n 4 ) complexity.
4.2  Single property In the case of a single property p, once only the kernel of the ordering relation is retained in the database, clause (1.1) can be simplified as follows: holds(period(Ei, p, Et)) :happens(Ei,TyEi), initiates(TyEi, p), happens(Et,TyEt), terminates(TyEt, p), beforeFact(Ei, Et).
Indeed, whenever p is the only property represented in the database, an event e interrupting a candidate MVI (e i,e j), where ei initiates p and ej terminates it, must either initiate or terminate p. Therefore (ei,ej) is an MVI for p if and only if ei is an immediate predecessor of ej in the current ordering, in the sense that no other event is recorded between the two.
It is worth noting that the predicate broken is not needed in this case.
Computing the complexity of this restricted version of EC is trivial.
In fact, the cost of holds(period(Ei,p,Et)) consists in the two calls to happens .
Since there are n events in the database, a call to h o l d s costs O (n 2 ).
Finally, notice that a further improvement in efficiency (at least in the average case) can be obtained by eliminating the calls to happens and by rearranging the atomic goals in its body as follows: holds(period(Ei, p, Et)) :beforeFact(Ei, Et), happens(Ei,TyEi), initiates(TyEi, p), happens(Et,TyEt), terminates(TyEt, p).
The complexity becomes equal to the number of beforeFact in the database, i.e.
the cardinality of the kernel.
4.3  Multiple properties We now generalize the technique to the case of multiple properties.
We maintain the minimality of the ordering information, as in the single property case, and implement a graph search algorithm for the query predicate holds.
This algorithm is implemented by the program given in the Appendix, where holds is clause (4.1).
The search is started from a specific initiating event.
Let ei be this event and p the corresponding property.
If ei is not instantiated in the query, all events in the graph will be processed.
The idea is to examine all the successors of e i searching for events terminating p. The search starts from its immediate successors, and proceeds breadth-first.
Exhausting a layer before examining nodes in the next is indeed crucial for the soundness of the algorithm.
As depicted in Figure 3, the nodes in the first layer after ei, are partitioned in three categories: terminating events for p, events interfering with p (i.e.
other initiating events for p, or  for properties incompatible with p) and independent events.
The nodes in the first category are terminating events in the MVIs returned to the user, and their successors are marked since there is no need to keep them into consideration during further processing.
<p] Nodes in the second termP category are simply marked as well as their successors since [p> initP [p> they cannot be ei contained in a successful path for Others otherP the user query.
The nodes in the third category are used to determine the next Figure 3 layer to explore, which is obtained by collecting all of their unmarked immediate successors.
The procedure repeats recursively until the most distant layer from ei is examined.
With reference to the code in the Appendix, findTerm (clause 4.2) finds the elements in the first layer after a given node, f i n d T e r m i n a n t s (clause 4.3) is the predicate which processes a layer and partitions the nodes, termP (clauses 4.4-6) is used to identify the events in the first category, nodes in the second category are processed by means of the predicates initP together with the auxiliary predicate initPorEx (clauses 4.7-11), and the remaining nodes are processed by the predicate otherP (clauses 4.12-13).  }
} }  5  Analyzing example  the  beverage  dispenser  We will now introduce an example, comment on the results of executing the basic and enhanced versions of EC on it, and further characterize the set of MVIs computed on this example by contrasting it with the set of possibly true and the set of necessarily true MVIs, respectively.
Consider the functioning of the simple beverage dispenser depicted in Figure 4: by setting the selector to the apple or to the orange position, apple juice or orange juice is obtained, respectively.
Choosing the stop position terminates the output of juice.
We model this knowledge as follows: initiates(selectApple,supplyApple).
initiates(selectOrange,supplyOrange).
terminates(selectStop, supplyApple).
terminates(selectStop, supplyOrange).
exclusive(supplyApple,supplyOrange).
exclusive(supplyOrange,supplyApple).
We consider an actual situation where six events happened: e1, e2, e3, e4, e5, and e6.
The following PROLOG factual knowledge associates events to their types: happens(e1,selectApple,unmarked).
happens(e2,selectStop,unmarked).
happens(e3,selectOrange,unmarked).
happens(e4,selectStop,unmarked).
happens(e5,selectApple,unmarked).
happens(e6,selectStop,unmarked).
These facts are intended for the marking implementation of EC, as it can be seen from the arity of the predicate happens .
The PROLOG code for the standard case is analogous and differs only for the absence of the third argument.
In the intended final ordering, events are ordered according to their indices.
Therefore, the final situation is represented in figure 5.
In our example, we will consider the following sequence of ordered pairs, which arrive one at a time: (e 1 ,e 4 ) - (e 1 ,e 6 ) - (e 2 ,e 4 ) - (e 1 ,e 2 ) - (e 3 ,e 4 ) - (e 4 ,e 5 ) (e2,e3) - (e2,e6) - (e5,e6).
e  1  supplyApple  e  2  e  e  3  supplyOrange  4  e  e  5  6  supplyApple  Figure 5 This sequence has been devised so that the complete situation shown in Figure 5 can be fully derived only after the last update.
The 9 ordered pairs are entered into the PROLOG database by running the following goals, in sequence.
????
?-  updateOrder(e1,e4).
updateOrder(e1,e6).
updateOrder(e2,e4).
updateOrder(e1,e2).
updateOrder(e3,e4).
???
?-  updateOrder(e4,e5).
updateOrder(e2,e3).
updateOrder(e2,e6).
updateOrder(e5,e6).
Table 1 shows the evolution of the Apple STOP Orange computation: each row corresponds to the addition of one of these ordered pairs to the database.
The first column shows which update is being performed.
The second column gives a visual account of the content of the database.
In particular, the kernel is represented in solid lines while deleted edges are drawn as dotted lines.
The third column contains the list of the MVIs derived by EC, i.e.
the result of running a generic query of the form ?h o l d s ( X ) .
For conciseness, we represented p e r i o d ( e i , s u p p l y A p p l e , e t ) as Figure 4 the more compact a ( e i , e t ) and period(ei,supplyOrange,et) as the more compact o(ei,et).
The experimental data (number of nodes visited in the resolution tree) obtained with this example show a more efficient behavior for the enhanced version of EC in the query phase.
This fact becomes more and more evident as the number of ordered pairs into the knowledge base grows: if the enhanced EC is only slightly faster (40 nodes analyzed versus 47) when there is no ordering information, after adding the last ordered pair the first answer is retrieved 4.2 times faster (86 nodes examined instead of 363) and the basic implementation explores 5 times more nodes (889 versus 178) to find all the MVIs.
Of course, the update operation is more expensive in the enhanced case, since just one node is explored in the basic implementation.
Anyway, this extracost is acceptable considering the benefits produced in the query phase and the fact that the overall cost (update and query) of the enhanced version is lower.
w  w visually  X:  w  w visually  X:  holds(X)  e1  e5  e3  e2  holds(X)  e4  e3  + (e 3 ,e 4 )  e6  a(e 1 ,e 2 ) a(e 1 ,e 6 ) o(e 3 ,e 4 )  e4  e2 e1 e6  + (e 1 ,e 4 )  e1  e4  e3  e5  e2  e6  e5  e3  + (e 4 ,e 5 )  e5  e4  e2  a(e 1 ,e 2 ) a(e 1 ,e 6 ) o(e 3 ,e 4 )  e1 e6  + (e 1 ,e 6 )  e4 e1  e3  e6  e5  e5  e2  a(e 1 ,e 6 )  + (e 2 ,e 3 )  e4  e3  a(e 1 ,e 2 ) a(e 1 ,e 6 ) o(e 3 ,e 4 )  e2 e1 e6  e2  + (e 2 ,e 4 )  e4  e3  e1 e6  a(e 1 ,e 6 )  e5  + (e 2 ,e 6 )  e3  e4 e2  e3  e6  e5  e1  a(e 1 ,e 2 ) a(e 1 ,e 6 )  + (e 5 ,e 6 )  a(e 1 ,e 2 ) o(e 3 ,e 4 )  e2 e1  + (e 1 ,e 2 )  e5  e4  e1  e6  e2  e3  e4  e5  e 6 a(e 1 ,e 2 ) o(e 3 ,e 4 ) a(e 5 ,e 6 )  Table 1 MVIs derived by EC  Necessary MVIs  Possible  MVIs  [?]
[?]
a(e1,e2),a(e1,e6),o(e3,e4),a(e5,e2),a(e5,e6)  ?- updateOrder(e1,e4).
[?]
[?]
a(e1,e2),a(e1,e6),o(e3,e4),a(e5,e2),a(e5,e6)  ?- updateOrder(e1,e6).
a(e1,e6)  [?]
a(e1,e2),a(e1,e6),o(e3,e4),a(e5,e2),a(e5,e6)  ...  ...  ...  ...  ?- updateOrder(e2,e3).
a(e1,e2),a(e1,e6),o(e3,e4)  [?]
a(e1,e2),a(e1,e6),o(e3,e4),a(e5,e6)  ?- updateOrder(e2,e6).
a(e1,e2),o(e3,e4)  a(e1,e2)  a(e1,e2),o(e3,e4),a(e5,e6)  ?- updateOrder(e5,e6).
a(e1,e2),o(e3,e4),a(e5,e6)  a(e1,e2),o(e3,e4),a(e5,e6)  a(e1,e2),o(e3,e4),a(e5,e6)  Table 2 Further insights on the behavior of EC with partially ordered events can be obtained by comparing it with the behavior of the Skeptical EC and Credulous EC, two variants of EC we proposed [4] to compute the MVIs which are derivable in all the total orders consistent with the given partial order (Skeptical EC), and those derivable in at least one of the total orders (Credulous EC).
As an example, Table 2 considers  again the beverage dispenser example and provides a comparison of the MVIs computed by the calculus presented in this paper, contrasted with those computed by the Skeptical and the Credulous calculus.
Dean and Boddy [5] studied the task of deriving which facts must be or can possibly be true over certain intervals of time in presence of partially ordered events (in our context, which MVIs must  necessarily hold and which possibly hold), focusing on the computational complexity of the task and showing that it is intractable in the general case.
In [2,4], we focused on providing the task with a modal logic formulation.
While [5] develop polynomial algorithms to compute supersets of the set of possible facts and subsets of the set of necessary facts, the calculus presented in this paper polynomially computes a set in between, mimicking some behaviors of human reasoners.
6 Conclusions The paper analyzed in detail the process of computing MVIs for properties in EC, and proposed a revision of the calculus that strongly increases its efficiency when dealing with partial ordering information.
The resulting calculus models events and their ordering relations in terms of a directed acyclic graph, and incorporates a marking technique to speed up the visit of the graph during the computation of validity intervals.
Moreover, it provides an alternative solution to the problem of supporting default persistence that further improves its performance.
Instead of the expensive generateand-test approach of the original calculus, it restricts a priori the search space by exploiting the kernel of an ordering relation.
Since we did not determine a lower bound for the complexity of the problem of deriving MVIs with partially information about event ordering, the possibility of further improving the achieved results can not be excluded.
References [1] I. Cervesato, A. Montanari, A. Provetti 1993.
"On the Non-monotonic Behavior of Event Calculus for Deriving Maximal Time Intervals", I n t e r v a l Computations 2, 83-119.
[2] I. Cervesato, L. Chittaro, A. Montanari 1995.
"A Modal Calculus of Partially Ordered Events in a Logic Programming Framework".
Proc.
ICLP '95, Tokyo, Japan, MIT Press.
[3] L. Chittaro, A. Montanari 1996.
"Efficient temporal reasoning in the Cached Event Calculus", to appear in Computational Intelligence Journal.
[4] L. Chittaro, A. Montanari, A. Provetti 1994.
"Skeptical and Credulous Event Calculi for Supporting Modal Queries", in Proc.
ECAI '94, Amsterdam, The Netherlands, Wiley and Sons Publishers, 361-365.
[5] T. Dean, M. Boddy 1988.
"Reasoning about Partially Ordered Events", Artificial Intelligence 36, 375-399.
[6] R. Kowalski 1992.
"Database Updates in the Event Calculus", in Journal of Logic Programming 12, 121146.
[7] R. Kowalski, M. Sergot 1986.
"A Logic-based Calculus of Events", in New Generation Computing,  4, Ohmsha Ltd and Springer-Verlag, 67-95.
Appendix holds(period(Ei, P, Et)) :(4.1) happens(Ei, TyEi, unmarked), initiates(TyEi, P), findTerm(Ei, P, Et).
findTerm(Ei, P, Et) :(4.2) findSucc(Ei, Es), findTerminants(Es, P, Res),unmarkAll,!, member(Et, Res).
findTerminants(Es, P, Res) :termP(P, Es, LessEs, ResTerm), initP(P, LessEs, FewerEs), otherP(P, FewerEs, ResOther), append(ResTerm, ResOther,Res).
(4.3)  termP(P,[E|Tail], NonTerm,[E|Term]) :(4.4) happens(E,TyE),terminates(TyE, P), markAll(E),termP(P, Tail, NonTerm, Term).
termP(P,[E|Tail],[E|NonTerm],Term) :(4.5) happens(E,TyE), not terminates(TyE, P), termP(P, Tail, NonTerm, Term).
termP(P, [], [], []).
(4.6) initP(P, [E|Tail], NonP) :initPorEx(E, P), markAll(E), initP(P, Tail, NonP).
initP(P, [E|Tail], [E|NonP]) :not initPorEx(E, P), initP(P, Tail, NonP).
initP(P, [], []).
(4.7)  (4.8) (4.9)  initPorEx(E, P) :(4.10) happens(E,TyE), initiates(TyE, P).
initPorEx(E, P) :(4.11) happens(E,TyE), (initiates(TyE,Q);terminates(TyE,Q)), exclusive(P, Q).
otherP(P, [E|Es], Res) :listSucc([E|Es], SuccEs), findTerminants(SuccEs, P, Res).
otherP(P, [], []).
(4.12)  findSucc(E, Es) :setof(NextE,(beforeFact(E, NextE), happens(NextE, _, unmarked)),Es), !.
findSucc(E, []).
(4.14)  listSucc([E|Es], SuccEEs) :findSucc(E, SuccE), listSucc(Es, SuccEs), listUnion(SuccE, SuccEs, SuccEEs).
listSucc([], []).
(4.13)  (4.15) (4.16)  (4.17)  markAll(E) :mark(E), findSucc(E, SuccE), markAllIn(SuccE).
(4.18)  markAllIn([E|Es]) :markAll(E), markAllIn(Es).
markAllIn([]).
(4.19)  listUnion([E|L1], L2, L3) :member(E, L2), !, listUnion(L1, L2, L3).
listUnion([E|L1], L2, [E|L3]) :listUnion(L1, L2, L3).
listUnion([], L, L).
(4.20) (4.21) (4.22) (4.23)
Pre-Processing Time Constraints for Efficiently Mining Generalized Sequential Patterns Florent Masseglia INRIA Sophia Antipolis AxIS Research Team 2004 rte des lucioles 06902 Sophia Antipolis, France Florent.Masseglia@inria.fr  Pascal Poncelet Laboratoire LGI2P Ecole des Mines d'Ales Parc Sc.
Georges Besse 30035 Nimes, France Pascal.Poncelet@ema.fr  Abstract In this paper we consider the problem of discovering sequential patterns by handling time constraints.
While sequential patterns could be seen as temporal relationships between facts embedded in the database, generalized sequential patterns aim at providing the end user with a more flexible handling of the transactions embedded in the database.
We propose a new efficient algorithm, called GTC (Graph for Time Constraints) for mining such patterns in very large databases.
It is based on the idea that handling time constraints in the earlier stage of the algorithm can be highly beneficial since it minimizes computational costs by preprocessing data sequences.
Our test shows that the proposed algorithm performs significantly faster than a stateof-the-art sequence mining algorithm.
Maguelonne Teisseire LIRMM UMR CNRS 5506 161 Rue Ada 34392 Montpellier, France Teisseire@lirmm.fr  algorithm, called GSP was proposed.
In this paper, we propose a new efficient algorithm, called GTC (Graph for Time Constraints) for mining generalized sequential patterns in large databases.
GTC minimizes computational costs by using a data-sequence preprocessing operation that takes time constraints into account.
The main new feature of GTC is that time constraints are handled prior to and separately from the counting step of a data sequence.
The rest of this paper is organized as follows.
In Section 2, the problem of mining generalized sequential patterns is stated and illustrated.
A review of related work is presented in Section 3.
The reasons for our choices are discussed in Section 4.
The GTC algorithm for efficiently discovering all generalized sequential patterns is given in Section 5.
Section 6 presents the detailed experiments using both synthetic and real datasets, and performance results obtained are interpreted.
Section 7 concludes the paper.
1.
Introduction Although sequential patterns are of great practical importance (e.g.
alarms in telecommunications networks, identifying plan failures, analysis of Web access databases, etc.
), in the literature, they have received relatively little attention [1, 6, 4, 5, 2].
They could be seen as temporal relationships between facts embedded in the database.
A sequential pattern could be "5% of customers bought 'Foundation', then 'Foundation and Empire', and then 'Second Foundation"'.
The problem of discovering sequential patterns in databases, introduced in [1], is to find all patterns verifying user-specified minimum support, where the support of a sequential pattern is the percentage of data-sequences that contain the pattern.
Such patterns are called frequent patterns.
In [6], the definition of the problem is extended by handling time constraints and taxonomies (is-a hierarchies) and a new  2.
From Sequential Patterns to Generalized Sequential Patterns First of all, we assume that we are given a database DB of customers' transactions, each of which has the following characteristics: sequence-id or customer-id, transactiontime and the items involved in the transaction.
Such a database is called a base of data sequences (Cf.
Figure 4).
Definition 1 Let I = {i1 , i2 , ..., im } be a set of literals called items.
An itemset is a non-empty set of items.
A sequence s is a set of itemsets ordered according to their timestamp.
It is denoted by < s1 s2 ...sn > where sj is an itemset.
A k-sequence is a sequence of k-items (or of length k).
A sequence < s1 s2 ...sn > is a sub-sequence of another sequence < s01 s02 ...s0m > if there exist integers i1 < i2 < ... < in such that s1 [?]
s0i1 , s2 [?]
s0i2 , ...sn [?]
s0in .
Example 1 Let us consider that a given customer purchased items 10, 20, 30, 40, 50, according to the following sequence: s =< (10) (20 30) (40) (50)>.
This means that, apart from 20 and 30 which were purchased together, i.e.
during a common transaction, the items in the sequence were bought separately.
The sequence s0 = < (20) (50) > is a sub-sequence of s because (20) [?]
(20 30) and (50)[?]
(50).
However < (20) (30) > is not a sub-sequence of s since items were not bought during the same transaction 2 In order to efficiently aid decision making, the aim is to discard non-typical behaviors according to the user's viewpoint.
Performing such a task requires providing any data sub-sequence s in the DB with a support value (supp(s)) giving its number of actual occurrences in the DB1 .
This means comparing any sub-sequence with the whole DB.
In order to decide whether a sequence is frequent or not, a minimum support value (minSupp) is specified by the user, and the sequence s is said to be frequent if the condition supp(s) >= minSupp holds.
This definition of sequence is not appropriate for many applications, since time constraints are not handled.
In order to improve the definition, generalized sequential patterns are introduced [6].
When verifying whether a sequence is included in another one, transaction cutting enforces a strong constraint since only pairs of itemsets are compared.
The notion of the sized sliding window enables that constraint to be relaxed.
More precisely, the user can decide that it does not matter if items were purchased separately as long as their occurrences enfold within a given time window.
Thus, when browsing the DB in order to compare a sequence s, assumed to be a pattern, with all data sequences d in DB, itemsets in d could be grouped together with respect to the sliding window.
Thus transaction cutting in d could be resized when verifying if d matches with s. Moreover when exhibiting from the data sequence d, subsequences possibly matching with the assumed pattern, non-adjacent itemsets in d could be picked up successively.
Minimum and maximum time gaps are introduced to constrain such a construction.
In fact, to be successively picked up, two itemsets must occur neither too close nor too far apart in time.
More precisely, the difference between their time-stamps must fit in the range [min-gap, max-gap].
Window size and time constraints as well as the minimum support condition are parameterized by the user.
Mining se1  A sequence in a data-sequence is taken into account once and once only for computing the support of a frequent sequence even if several occurrences are discovered.
quences with time constraints allows a more flexible handling of the transactions.
We now define frequent sequences when handling time constraints: Definition 2 Given a user-specified minimum time gap (minGap), maximum time gap (maxGap) and a time window size (windowSize), a data-sequence d =< d1 ...dm > is said to support a sequence s =< s1 ...sn > if there exist integers l1 <= u1 < l2 <= u2 < ... < ln <= un such i that: (i) si is contained in [?
]uk=l dk , 1 <= i <= n; (ii) i transaction-time (dui ) - transaction-time (dli ) <= ws, 1 <= i <= n; (iii) transaction-time (dli ) - transaction-time (dui-1 ) > minGap, 2 <= i <= n; (iv) transaction-time (dui ) - transaction-time (dli-1 ) <= maxGap, 2 <= i <= n. The support of s, supp(s), is the fraction of all sub-sequences in DB supporting s. When supp(s) >= minSupp holds, being given a minimum support value minSupp, the sequence s is called frequent.
Example 2 As an illustration for the time constraints, let us consider the following data-sequence describing the purchased items for a customer: Date 01/01/2000 02/02/2000 03/02/2000 04/02/2000 05/02/2000  Items 10 20, 30 40 50, 60 70  In other words, the data-sequence d could be considered as follows: d =< (10)1 (20 30)2 (40)3 (50 60)4 (70)5 > where each itemset is stamped by its access day.
For instance, (50 60)4 means that the items 50 and 60 were purchased together at time 4.
Let us now consider a candidate sequence c=< (10 20 30 40) (50 60 70) > and time constraints specified such as windowSize=3, minGap=0 and maxGap=5.
The candidate sequence c is considered as included in the datasequence d for the following two reasons: 1. the windowSize parameter makes it possible to gather together on the one hand the itemsets (10) (20 30) and (40), and on the other hand the itemsets (50 60) and (70) in order to obtain the itemsets (10 20 30 40) and (50 60 70), 2. the minGap constraint between the itemsets (40) and (50 60) holds.
Considering the integers li and ui in the Definition 2, we have l1 = 1, u1 = 3, l2 = 4, u2 = 5 and the data sequence d is handled as illustrated in Figure 1.  maxGap  <  l1 1  [ (10) 2 (20 30) 3 (40) u1 ] l2 [ 4 (50 60) 5 (70) u2 ] > minGap  Figure 1.
Illustration of the time constraints  maxGap  maxGap  < (10)1 (20 30)2  (40)3  (50 60)4 (70)5 >  windowSize minGap  windowSize minGap  Figure 2.
Illustration of the time constraints In a similar way, the candidate sequence c=< (10 20 30) (40) (50 60 70) > with windowSize=1, minGap=0 and maxGap=2, i.e.
l1 = 1, u1 = 2, l2 = 3, u2 = 3, l3 = 4 and u3 = 5 (C.f.
Figure 2) is included in the data-sequence d. The two following sequences c1 =< (10 20 30 40) (70) > and c2 = < (10 20 30) (60 70) >, with windowSize=1, minGap=3 and maxGap=4 are not included in the datasequence d. Concerning the former, the windowSize is not large enough to gather together the itemsets (10) (20 30) and (40).
For the latter, the only possibility for yielding both (10 20 30) and (60 70) is to take into account ws for achieving the following grouped itemsets [(10) (20 30)] and [(50 60) (70)].
maxGap is respected since [(10) (20 30)] and [(50 60) (70)] are spaced 4 days apart(u2 = 5, l1 = 1).
Nevertheless, in such a case minGap constraint is no longer respected between the two itemsets because they are only 2 days apart (l2 = 4 and u1 = 2) whereas minGap was set to 3 days 2 Given a database of data sequences, user-specified minGap and maxGap constraints and a user-specified sliding windowSize, the generalized sequential problem is to find all the sequences whose support is greater than the userspecified minSupp.
3.
Related Work In the following section, we review the most important work carried out within a sequential pattern framework.
Since they consider the generalized sequence problem and as they are the basic of our approach, particular emphasis is placed on the GSP [6] and PSP [3] algorithms.
The concept of sequential pattern is introduced to capture typical behaviors over time, i.e.
behaviors repeated sufficiently often by individuals to be relevant for the decision maker.
The GSP algorithm, proposed in [6], is intended for mining Generalized Sequential Patterns.
It extends previous proposal by handling time constraints and taxonomies (is-a hierarchies).
For building up candidate and frequent sequences, the GSP algorithm performs several iterative steps such as the k th step handles sets of k-sequences which could be candidate (the set is noted Ck ) or frequent (in Lk ).
The latter set, called the seed set, is used by the following step which, in turn, results in a new seed set encompassing longer sequences, and so on.
The first step aims at computing the support of each item in the database.
When completed, frequent items (i.e.
satisfying the minimum support) are discovered.
They are considered as frequent 1-sequences (sequences having a single itemset, itself being a singleton).
This initial seed set is the starting point of the second step.
The set of candidate 2-sequences is built according to the following assumption: candidate 2-sequences could be any pair of frequent items, embedded in the same transaction or not.
From this point, any step k is given a seed set of frequent (k-1)-sequences and it operates by performing the two following sub-steps: * The first sub-step (join phase) addresses candidate generation.
The main idea is to retrieve, among sequences in Lk-1 , pairs of sequences (s, s0 ) such that discarding the first element of the former and the last element of the latter results in two fully matching sequences.
When such a condition holds for a pair (s, s0 ), a new candidate sequence is built by appending the last item of s0 to s. * The second sub-step is called the prune phase.
Its objective is yielding the set of frequent k-sequences Lk .
Lk is achieved by discarding from Ck , sequences not satisfying the minimum support.
To yield such a result, it is necessary to count the number of actual occurrences matching with any possible candidate sequence.
Candidate sequences are organized within a hash-tree data-structure which can be accessed efficiently.
These sequences are stored in the leaves of the tree while intermediary nodes contain hashtables.
Each data-sequence d is hashed to find the candidates contained in d. When browsing a data sequence, time constraints must be managed.
This is performed by navigating downward or upward through the tree, resulting in a set of possible candidates.
For each candidate, GSP checks whether it is contained in the data-sequence.
Because of the sliding window, and minimum and maximum time gaps, it is necessary to switch during examination between forward and  backward phases.
Forward phases are performed for progressively dealing with items.
Accordingly, in earlier work [3], we proposed a new approach called PSP, Prefix-Tree for Sequential Patterns, which fully resumes the fundamental principles of GSP.
Its originality is to use a different hierarchical structure than in GSP for organizing candidate sequences, in order to improve retrieval efficiency.
In the hash-tree structure managed by GSP, the transaction cutting is not captured.
The main drawback of this approach is that when a leaf of the tree is obtained, an additional phase is necessary in order to check time constraints for all sequences embedded in the leaf.
The tree structure, managed by PSP, is a prefix-tree.
At the k th step, the tree has a depth of k. It captures all the candidate k-sequences in the following way.
Any branch, from the root to a leaf stands for a candidate sequence, and considering a single branch, each node at depth l (k >= l) captures the lth item of the sequence.
Furthermore, along with an item, a terminal node provides the support of the sequence from the root to the considered leaf (included).
Transaction cutting is captured by using labeled edges.
Example 3 Let us assume that we are given the following set of frequent 2-sequences: L2 = {< (10) (30) >, < (10) (40) >, < (30) (20) >, < (30 40) >, < (40 10) >}.
It is organized according to our tree structure as depicted in Figure 3.
Each terminal node contains an item and a counting value.
If we consider the node having the item 40, its associated value 2 means that two occurrences of the sequence {< (10) (40) >} have been detected so far 2  root a   @aaa    @ a 20 10 30 40 A   A  301 402 202 402 103 Figure 3.
The PSP Tree data structure  the base given in Figure 4.
Client C1 C1 C1 C1 C1  Date 01/04/2000 03/04/2000 04/04/2000 07/04/2000 17/04/2000  Item 1 2 3 4 5  Figure 4.
A database example  Let us assume that windowSize = 1 and minGap=1.
Upon reading the customer transaction, PSP has to determine all combinations of the data sequence in accordance with the time constraints in order to increment the support of a candidate sequence.
Then the set of sequences verifying time constraints is the following: without time constraints <(1)> ... <(5)> ... < ( 1 ) ( 2 )> ... < ( 3 ) ( 4 )> ... <(1)(2)(3)(4)(5)>  with windowSize & minGap <(2)(5)>* ... <(1)(3)(5)>* ... <(1)(23)(5)>* ... < ( 1 ) ( 2 ) ( 4 ) ( 5 )> * < ( 1 ) ( 3 ) ( 4 ) (5)> * < ( 1 ) ( 2 3 ) ( 4 ) ( 5 )> *  We notice that the sequences marked by a * are included in the sequence marked by a *.
That is to say that if a candidate sequence is supported by < ( 1 ) ( 2 ) ( 4 ) ( 5 ) > or < ( 1 ) ( 3 ) ( 4 ) ( 5 )> then such a sequence is also supported by < ( 1 ) ( 2 3 ) ( 4 ) ( 5 ) >.
The test of the two first sequences is of course useless because they are included in a larger sequence.
Client C1 C1 C1 C1 C1 C1  Date 01/04/2000 07/04/2000 13/04/2000 17/04/2000 18/04/2000 24/04/2000  Item 1 2 3 4 5 6  Figure 5.
A database example  4.
Motivations The PSP approach outperforms GSP by using a more efficient data structure.
If such a structure seems appropriate to the problem of mining generalized sequential patterns, it seems, on the other hand, that the algorithm can be improved by paying particular attention to time constraint handling.
To illustrate, let us consider the following customer data sequence < ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) > of  Let us now have a closer look at the problem of the windowSize constraint.
In fact, the number of included sequences is much greater when considering such a constraint.
For instance, let us consider the database given in Figure 5.
When windowSize=5 and minGap=1, PSP has to test the following sequences into the candidate tree structure (we only report sequences when windowSize is applied):  <(1)(2)(3)(5)(6)>* <(1)(2)(3)(4)(6)>* <(1)(2)(3)(45)(6)>* <(1)(2)(34)(6)>* <(1)(2)(345)(6)>  We notice that the sequences marked by a * (resp.
*) are included in the sequence marked by a * (resp.
).
That is to say that we have only to consider the two following sequences < ( 1 ) ( 2 ) ( 3 ) ( 4 5 ) ( 6 ) > and < ( 1 ) ( 2 ) ( 3 4 5 ) ( 6 ) > when verifying a data sequence in the candidate tree structure.
In fact, we need to solve the following problem: how to reduce the time required for comparing a data sequence with the candidate sequences.
Our proposition, described in the following section, is to precalculate, by means of the GTC (Graph for Time Constraints) algorithm, a relevant set of sequences to be tested for a data sequence.
By precalculating this set, we can reduce the time spent analysizing a data sequence when verifying candidate sequences stored in the tree, in the following two ways: (i) The navigation through the candidate sequence tree does not depend on the time constraints defined by the user.
This implies navigation without backtracking and better analysis of possible combinations of windowSize which are for PSP, as well as for GSP, computed on the fly.
(ii) This navigation is only performed on the longest sequences, that is to say on sequences not included in other sequences.
5.
GTC: Graph for Time Constraints Our approach takes up all the fundamental principles of GSP.
It contains a number of iterations.
The iterations start at the size-one sequences and, at each iteration, all the frequent sequences of the same size are found.
Moreover, at each iteration, the candidate sets are generated from the frequent sequences found at the previous iteration.
The main new feature of our approach which distinguish it from GSP and PSP is that handling of time constraint is done prior to and separate from the counting step of a data sequence.
Upon reading a customer transaction d in the counting phase of pass k, GTC has to determine all the maximal combinations of d respecting time constraints.
For instance, in the previous example, only < ( 1 ) ( 2 ) ( 3 ) ( 4 5 ) ( 6 ) > and < ( 1 ) ( 2 ) ( 3 4 5 ) ( 6 ) > are exhibited by GTC.
Then the Main algorithm has to determine all the k-candidates supported by the maximal sequences issued from GTC iterations on d and increment the support counters associated with these candidates without considering time constraints any more.
In the following sections, we decompose the problem of discovering non-included sequences respecting time constraints into the following subproblems.
First we consider the problem of the minGap constraint without taking into account maxGap or windowSize and we propose an algorithm called GTCminGap for handling efficiently such a constraint.
Second, we extend the previous algorithm in order to handle the minGap and windowSize constraints.
This algorithm is called GTCws .
Finally, we propose an extension of GTCws , called GTC, for discovering the set of all non included sequences when all the time constraints are applied.
5.1.
GTCminGap Algorithm: solution for minGap In this section, we explain how the GTCminGap algorithm provides an optimal solution to the problem of handling the minGap constraint.
1 r  2 - r/  R3 r  / / / / / / / /  4 - r  5 - r   Figure 6.
A data sequence representation  To illustrate, let us consider Figure 6, which represents the data sequence of the base given in Figure 4.
We note, / / / / / / / / , the minGap constraint between two itemsets a and b.
Let us consider that minGap is set to 1.
As items 2 and 3 are too closed according to minGap, they cannot occur together in a candidate sequence.
So, from this graph, only two sequences (< (1) (2) (4) (5) > and < (1) (3) (4) (5) >) are useful in order to verify candidate sequences.
We can observe that these two sequences match the two paths of the graph beginning from vertex 1 (standing for source vertex) and ending in vertex 5 (standing for sink vertex).
From each sequence d, a sequence graph can be built.
A sequence graph for d is a directed acyclic graph Gd (V, E) where a vertex v, v [?]
V , is an itemset embedded in d and an edge e, e [?]
E, from two vertices u and v, denotes that u occurred before v with at least a gap greater than the minGap constraint.
A sequence path is a path from two vertices u and v such as u is a source and v is a sink.
Let us note SPd the set of all sequence paths.
In addition, Gd has to satisfy the following two conditions: 1.
No sequence path from Gd is included in any other sequence path from Gd .
2.
If a candidate sequence is supported by d, then such a candidate is included in a sequence path from Gd .
an itemset occurring before v. The algorithm operates by performing, for each itemset, the following two sub-steps:  Given a data sequence d and a minGap constraint, the sequential path problem is to find all the longest paths, i.e.
those not included in other ones, by verifying the following conditions:  1.
Propagation phase: the main idea is to retrieve the first itemset u by verifying (u.date() - v.date() > minGap)2 , i.e.
the first itemset for which the minGap constraint holds, in order to build the edge (u, v).
Then for each itemset z such as (z.date()-y.date() > minGap), the algorithm updates z.isP rec[x] indicating that v will reach z traversing the itemset u.
2.
"gap-jumping" phase: its objective is to yield the set of edges not provided by the previous phase.
Such edges (v, t) are defined as follows (t.date() - x.date() > minGap) and t.isP rec[x] 6= 1.
1.
[?
]s1, s2 [?]
SPd /s1 6[?]
s2.
2.
[?
]c [?]
As /d supports c, [?
]p [?]
SPd /p supports c where As stands for the set of candidate sequences.
3.
[?
]p [?]
SPd , [?
]c [?]
As /p supports c, then d supports c. The set of all sequences SPd is thus used to verify the actual support of candidate sequences.
Example 4 To illustrate, let us consider the graph in Figure 8 representing the application of GTCminGap to the database depicted in Figure 7.
Let us assume that the minGap value was set to 2.
According to the time constraint, the set of all sequence paths, SPd , is the following: SPd ={< (1) (2) (6) >, < (1) (3 4) (6) >, < (1) (5) >}.
From this set, the next step of the Main algorithm is to verify these sequences into the candidate tree structure without handling time constraints anymore 2  Client C1 C1 C1 C1 C1  Date 01/04/2000 05/04/2000 06/04/2000 07/04/2000 09/04/2000  2 - r/  R34 r/ /  / / / / / / / /  Item 1 2 34 5 6  R5 r/  / / / / / / /  The following theorem guarantees that, when applying GTCminGap , we are provided with a set of data sequences where the minGap constraint holds and where each yielded data sequence cannot be a sub-sequence of another one.
Theorem 1 The GTCminGap algorithm provides all the longest-paths verifying minGap.
Figure 7.
An example database  1 r  Once the GTCminGap has been applied to a data sequence d, the set of all sequences, SPd , for counting the support for candidate sequences is provided by navigating through the graph of all sequence paths.
R6 r  / / / / / / / /  / / / / / / / / / / / / / / / / / / / / / / / / /   Figure 8.
The example sequence graph with minGap =2  We now describe how the sequence graph is built by the GTCminGap algorithm.
Auxiliary data structure can be used to accomplish this task.
With each itemset v, the itemsets occurring before v are stored in a sorted array, v.isP rec of size |E|.
The array is a vector of boolean where 1 stands for  First, we prove that for each p, p0 [?]
SPd , p 6[?]
p0 .
Next we show that for each candidate sequence c supported by d, a sequence path in G supporting c is found.
Let us assume two sequence paths, s1, s2 [?]
SPd such as s1 [?]
s2.
That is to say that the subgraph depicted in Figure 9 is included in G. In other words, there is a path (a, .
.
.
, c) of length >= 2 and an edge (a, c).
If such a path (a, c) exists, we have c.isP rec[a] = 1.
Indeed we can have a path of length >= 1 from a to b either by an edge (a, b) or by a path (a, .
.
.
, b).
In the former case, c.isP rec[a] is updated by the statement c.isP rec[a] - 1, otherwise there is a vertex a0 in (a, .
.
.
, b) such as (a, a0 ) is included in the path.
In such a case c.isP rec[a] - 1 has already occurred when building the edge (a, a0 ).
Then, after building the path (a, .
.
.
, b, .
.
.
, c) we have c.isP rec[a] = 1 and the edge (a, c) is not built.
Clearly the sub-graph depicted in Figure 9 cannot be obtained after GTCminGap .
Finally we demonstrate that if a candidate sequence c is supported by d, there is a sequence path in SPd supporting c. In other words, we want to demonstrate that GTCminGap provides all the longest paths satisfying the minGap constraint.
The data sequence d is progressively browsed starting with its first item.
Then if an itemset x is embedded in a path satisfying the minGap constraint it is included in SPd .
We 2  where x.date() stands for the transaction time of the itemset x.  have previously noticed that all vertices are included into a path and for each p, p0 [?]
SPd , p 6[?]
p0 .
Furthermore if two paths (x, .
.
.
, y)(y 0 , .
.
.
, z) can be merged, the edge (y, y 0 ) is built when browsing the itemset y.  a r  b - r  Rc - r  Definition 3 An itemset i is included in another itemset j if and only if the following two conditions are satisfied:  Figure 9.
Minimal inclusion schema  * i.begin() >= j.begin(), * i.end() <= j.end().
5.2.
GTCws Algorithm: solution for minGap and windowSize In this section, we explain how the algorithm GTCws provides an optimal solution to the problem of handling minGap and windowSize.
As we have already noticed in Section 4, the problem of handling windowSize is much more complicated than handling minGap since the number of included sequences is much greater when considering such a constraint.
(1) r  (2) - r   (3) - r  (4 5) - r  then x and y can be "merged" into the same transaction.
The structure described above is thus extended to handle such an operation.
Each itemset, in the new structure, is provided by both the begin transaction date and the end transaction date.
These dates are obtained by using the v.begin() and v.end() functions.
(6) - r  - r (3 4 5)  Once the graph satisfying minGap is obtained, the algorithm detects inclusions in the following way: for each node x, the set of all its successors x.next must be exhibited.
For each node y in x.next, if y [?]
z, z [?]
x.next and y.next [?]
z.next then the node y can be pruned out from the graph.
The following theorem guarantees that, when applying GTCws , we are provided with a set of data sequences where the minGap and windowSize constraints hold and that each yielded data sequence cannot be a sub-sequence of another one.
Theorem 2 The GTCws algorithm provides all the longest paths verifying minGap and windowSize.
6   Figure 10.
A sequence graph obtained when considering windowSize  To take into account the windowSize constraint we extend the GTCminGap algorithm by generating coherent combinations of windowSize at the beginning of algorithm and, once the graph respecting minGap is obtained, inclusions are detected.
The result of this handling is illustrated by Figure 10, which represents the sequence graph of the database given in Figure 5 when windowSize=5 and minGap=1.
Due to lack of space, we no not provide the algorithm but we give an overview of the method.
To yield the set of all windowSize combinations, each vertex x of the graph is progressively browsed and the algorithm determines which vertex can possibly be merged with x.
In other words, when navigating through the graph, if a vertex y is such that y.date() - x.date() < windowSize,   b - r r  a  b b' Rcr - r  Figure 11.
An included path example  Theorem 1 shows that we do not have included data sequences when considering minGap.
Let us now examine the windowSize constraint in detail.
Let us consider two sequence paths s1 and s2 in Gd such that s1 [?]
s2 .
Figure 11 illustrates such an inclusion.
In the last phase of the GTCws algorithm, we examine for each vertex x of the graph, the set of its successors by using the x.next function.
So, for each vertex y in x.next, if y [?]
z, z [?]
x.next and y.next [?]
z.next, the vertex y is pruned out from the graph.
So, by construction, s1 cannot be in the graph.
5.3.
Solution for all time constraints In order to handle the maxGap constraint in the GTC algorithm, we have to consider the itemset time-stamps into  the graph previously obtained by GTCws .
Let us remember that, according to maxGap, a candidate sequence c is not included in a data sequence s if there exist two consecutive itemsets in c such that the gap between the transaction time of the first itemset (called li-1 in Definition 2) and the transaction time of the second itemset (called ui in Definition 2) in s is greater than maxGap.
According to this definition, when comparing candidates with a data sequence, we must find in a graph itemset, the time-stamp for each item since, due to windowSize, items can be gathered together.
In order to verify maxGap, the transaction time of the sub-itemset corresponding to the included itemset into the graph, must verify the maxGap delay from the preceding itemset as well as for the following itemset.
To illustrate, let us consider the database depicted in Figure 12.
Let us now consider, in Figure 13, the sequence graph obtained from the GTCws algorithm when windowSize was set to 1 and minGap was set to 0.
In order to determine if the candidate data sequence < ( 2 ) ( 4 5 ) ( 6 ) > is included into the graph, we have to examine the gap between item 2 and item 5 as well as between item 4 and item 6.
Nevertheless, the main problem is that, according to windowSize, itemset (3) and itemset (4 5) were gathered together into (3 4 5).We are led to determine the transaction time of each component in the resulting itemset.
Customer C1 C1 C1 C1  Date 01/01/2000 03/01/2000 04/01/2000 06/01/2000  Items 2 3 4 5 6  Figure 12.
A database example  ( 2) r  ( 3 4 5) - r  ( 6) - r  Figure 13.
Sequence graph obtained by GTCws  Before presenting how maxGap is taken into account in GTC, let us assume that we are provided with a sequence graph containing information about itemsets satisfying the maxGap constraint.
By using such an information the candidate verification can thus be improved as illustrated in the following example.
Example 5 Let us consider the sequence graph depicted in Figure 13.
Let us assume that we are provided with information about reachable vertices into the graph according to maxGap and that maxGap is set to 4 days.
Let us now consider how the detection of the inclusion of a candidate sequence within the sequence graph is processed.
Candidate itemset (2) and sequence graph itemset (2) are first compared by the algorithm.
As the maxGap constraint holds and (2) [?]
(2), the first itemset of the candidate sequence is included in the sequence graph and the process continues.
In order to verify the other components of the candidate sequence, we must know what is the next itemset ended by 5 in the sequence graph and verifying the maxGap delay.
In fact, when considering the last item of the following itemset, if we want to know if the maxGap constraint holds between the current itemset (2) and the following itemset in the candidate sequence, we have to consider the delay between the current itemset in the graph and the next itemset ended by 5 in this graph.
We considered that we are provided with such an information in the graph.
This information can thus be taken into account by the algorithm in order to directly reach the following itemset in the sequence graph (3 4 5) and compare it with the next itemset in the candidate sequence (4 5).
Until now, the candidate sequence is included into the sequence graph.
Nevertheless, for completeness, we have to find in the graph the next itemset ended by 6 and verifying that the delay between the transaction times of items 4 and 6 is lower than 4 days.
This condition occurs with the last itemset in the sequence graph.
At the end of the process, we can conclude that c is included in the sequence graph of d or more precisely that c is included in d. Let us now consider the same example but with a maxGap constraint set to 2.
Let us have a closer look at the second iteration.
As we considered that we are provided with information about maxGap into the graph, we know that there is no itemset such that it ends in 5 and it satisfies the maxGap constraint with item 2.
The process ends by concluding that the candidate sequence is not included into the data sequence and without navigating further through the candidate structure 2 Let us now describe how information about itemsets verifying maxGap is taken into account in GTC.
Each item in the graph is provided with an array indicating reachable vertices, according to maxGap.
Each array value is associated with a list of pointed nodes, which guarantees that the pointed node corresponds to an itemset ending by this value and that the delay between these two items is lower or equal to maxGap.
Candidate verification algorithms can thus find candidates included in the graph by using such information embedded in the array.
By means of pointed nodes, the maxGap constraint is considered during evaluation of candidate itemset.
6.
Experiments In this section, we present the performance results of our GTC algorithm.
The structure used for organizing candidate sequences is a prefix tree structure as in PSP.
All experiments were performed on a PC Station with a CPU clock rate at 1.8 GHz, 512M Bytes of main memory, Linux System and a 60G Bytes disk drive.
In order to assess the relative performance of the GTC algorithm and study its scale-up properties, we used different kinds of datasets.
Due to lack of space we only report some results obtained with one access log file.
It contains about 800K entries corresponding to the requests made during March of 2002 and its size is about 600 M Bytes.
There were 1500 distinct URLs referenced in the transactions and 15000 clients.
Figure 14.
Execution times and recursive calls (minGap=1)  Figure 14 shows experiments conducted on the different datasets using different windowSize ranges to get meaningful response times.
minGap was set to 1 and maxGap was set to [?].
Note the minimum support thresholds are adjusted to be as low as possible while retaining reasonable execution times.
Figure 14 clearly indicates that the performance gap between the two algorithms increases with increasing windowSize value.The reason is that during the candidate verification, PSP has to determine all combination of the data sequence according to minGap and windowSize constraints.
In fact, the more the value of windowSize increases, the more PSP carries out recursive calls in order to apply time constraints to the candidate structure.
According to these calls, the PSP algorithm operates a costly backtracking for examining the prefix tree structure.In order to illustrate correlation between the number of recursive calls and the execution times we compared the number of recursive calls needed by PSP and our algorithm and as expected, the number of recursive calls increases with the size of the windowSize parameter.Additional experiments  were performed in order to study performance in scale-up databases.
they showed that GTC scaled up linearly as the number of transactions is increased.
7.
Conclusion We considered the problem of discovering sequential patterns by handling time constraints.
We proposed a new algorithm called GTC based on the fundamental principles of PSP and GSP but in which time constraints are handled in the earlier stage of the algorithm in order to provide significant benefits.
Using synthetic datasets as well as real life databases, the performance of GTC was compared against that of PSP.
Our experiments showed that GTC performs significantly better than the state-ofthe-art approaches since the improvements achieved by GTC over the counting strategy employed by other approaches are two-fold: first, only maximal sequences are generated, and there is no need of an additional phase in order to count candidates included in a data sequence.
In order to take advantage of the behavior of the algorithm in the first scans on the database, we are designing a new algorithm called PG-Hybrid using the PSP approach for the two first passes on the database and GTC for the following scans.
First experiments are encouraging, even for short frequent sequences.
References [1] R. Agrawal and R. Srikant.
Mining Sequential Patterns.
In Proceedings of the 11th International Conference on Data Engineering (ICDE'95), Tapei, Taiwan (1995).
[2] J. Ayres, J. Gehrke, T. Yiu, and J. Flannick.
Sequential Pattern Mining Using Bitmap Representation.
In Proocedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Edmonton, Alberta, Canada (2002).
[3] F. Masseglia, F. Cathala, and P. Poncelet.
The PSP Approach for Mining Sequential Patterns.
In Proceedings of the 2nd European Symposium on Principles of Data Mining and Knowledge Discovery (PKDD'98), LNAI, Vol.
1510, pp.
176-184, Nantes, France (1998).
[4] F. Masseglia, P. Poncelet, and M. Teisseire.
Incremental Mining of Sequential Patterns in Large Databases.
Data and Knowledge Engineering, 46(1):97-121 (2003).
[5] J. Pei, J. Han, B. Mortazavi-Asl, H. Pinto, Q. Chen, U. Dayal, and MC.
Hsu.
PrefixSpan: Mining Sequential Patterns Efficiently by Prefix-Projected Pattern Growth.
In Proceedings of 17th International Conference on Data Engineering (ICDE'01) (2001).
[6] R. Srikant and R. Agrawal.
Mining Sequential Patterns: Generalizations and Performance Improvements.
In Proceedings of the 5th International Conference on Extending Database Technology (EDBT'96), pp.
3-17, Avignon, France (1996).

Recursive Representation of Periodicity and Temporal Reasoning* Luca Anselma Dipartimento di Informatica, Universita di Torino, Corso Svizzera 185, 10149 Torino, Italy Phone: +39 011 6706821 - anselma@di.unito.it Abstract Representing and reasoning with repeated and periodic events is important in many real-world domains, such as protocol and guideline management.
In this set, it is important to give support to complex periodicities, that can involve non-symmetric repetitions, uncertainness, variability, pauses between repetitions, and nested time intervals.
Also, in these domains it can be useful to give support to composite events, as well as classes of events (i.e.
types of actions) and instances of events (i.e.
specific actions).
In this paper we propose a general-purpose domainindependent knowledge server dealing with all these issues.
In particular, we describe a compact and (hopefully) user-friendly formalism for representing repetition/periodicity temporal constraints that supports arbitrarily nested repetitions as well as possibly imprecise and variable delays between repetitions.
Moreover, we define two algorithms for performing consistency checking on knowledge bases of (possibly repeated/periodic) classes and instances of events retaining the efficiency of less expressive approaches.
1.
Introduction The need to represent and reason with time is crucial within many real world domains; in particular, it is useful to be able to manage different kinds of temporal information such as: - qualitative temporal relations, - quantitative temporal relations, - repeated/periodic temporal relations, - classes and instances of events, - composite events.
In the context of temporal constraint reasoning, in literature much attention has been devoted to some of these aspects (e.g.
see the surveys in [2, 23]).
In particular, since the beginning of the eighties, several domain-independent knowledge servers specifically designed to manage temporal information (i.e.
temporal reasoners) have been  proposed.
Temporal reasoners are conceived to deal in an efficient and ad-hoc way with different types of temporal constraints: as regards qualitative temporal constraints it is possible to see, e.g., [1]; as regards quantitative temporal constraints see [5]; as regards "mixed" qualitative and quantitative temporal constraints see [9].
On the other hand, not as much attention has been paid for some other aspects, such as repeated and periodic events (for an approach towards these issues see [10]).
Besides these, there are at least two other aspects which have generally received very little attention, despite their usefulness in a plurality of real-world domains, such as planning, workflow management, and protocol and medical guideline management: classes and instances of events, and composite events.
Classes and instances of events, for example, may result effective when dealing with planning issues.
In fact, one may wish to specify a general plan with generic actions that are constrained by generic temporal constraints and, then, to execute (i.e.
instantiate) the plan with actual actions and actual timings, maybe several times and in different contexts.
In this set, it is possible to look at the generic actions as classes of events, and at the actual actions as instances of events corresponding to the actions in the plan.
Obviously, the instances must follow (i.e.
be consistent with) the temporal constraints on the classes of events.
Furthermore, in domains such as clinical therapies, not only classes and instances of events are needed (because therapies may be regarded as plans), but also both periodic and composite events are, since therapeutic actions often have to be repeated at regular times and may be composed by subactions.
Consider, for example, the following excerpt from a clinical guideline for the treatment of multiple mieloma: (Ex.
1) The therapy for multiple mieloma is made by six cycles of 5 days treatment, each one followed by a delay of 23 days (for a total time of 24 weeks).
Within each cycle of 5 days, 2 inner  * This is a preprint of a paper accepted for publication in the proceedings of 11th International Symposium on Temporal Representation and Reasoning (TIME'04), (c) IEEE Computer Society Press, 2004 (copyright owner as specified in the proceedings)  reasoning to deal with all the aspects mentioned above.
Moreover, we show that, even with the richer representation language, the complexity of the temporal reasoning algorithms does not increase wrt the complexity of the algorithms in [20].
The paper is organized as follows: in section 2, we introduce the problems that our temporal manager has to address; in section 3, we describe the language for representing classes and instances of events, the language for representing constraints on periodicity, and the one employed for internal representation; in section 4, we deal with consistency checking on classes only and with consistency checking on classes+instances by means of constraint inheritance (both with the assumption of complete observability and with the assumption of no observability in the future); moreover, we discuss the complexity of the reasoning mechanisms and present some preliminary experimental results; finally, in section 5, we draw some conclusions, discuss the related works in literature, and present applications and future work.
cycles can be distinguished: the melphalan treatment, to be provided twice a day for each of the 5 days, and the prednisone treatment, to be provided once a day for each of the 5 days.
These two treatments must be performed in parallel.
Ex.
1 shows how both periodic and composite events are necessary for the representation of the therapy for multiple mieloma (which we may regard as a composite event) providing treatments (the melphalan treatment and the prednisone treatment) repeated at regular times.
Moreover, also qualitative temporal relations are needed, in fact the two treatments must be performed in parallel.
It is crucial to give support to all the abovementioned kinds of temporal information with a uniform domain-independent approach and hide the complexity of the "machinery" to the user.
This goal is motivated by our ongoing work in the domain of guidelines automation ([15, 18, 21, 22], see also subsection 5.2).
A first step in this direction has been made in [20], but with limitations that in some real-world cases it is necessary to overcome: in fact, while that approach can handle Ex.
1, it cannot handle more complex periodicities such as the one sketched in Ex.
2, an excerpt from a clinical guideline for the treatment of Childhood Acute Lymphoblastic Leukaemia: (Ex.
2) The therapy lasts 88 weeks and it is repeated twice in four years.
In the therapy, cotrimoxazole must be given twice daily on two consecutive days every week.
This case presents not only multiple nested time intervals (four years, 88 weeks, a week, a day), but also temporal constraints between repetitions - the two days must be consecutive -, and uncertainness - it is not specified when the two consecutive days must occur in the week.
In the present work, we make a step further wrt [20]; specifically, we extend the formalism for the specification of repetitions/periodicity to a considerably more powerful one, and we accordingly define new algorithms for temporal Reservation (RS)  RS1  1-7 days  before  LT2  RP1  constraints  about  The distinction between classes of events and instances of events is only a specific case of the well-known distinction between classes and instances.
Let us show an example: (Ex.
3) The reservation of a laboratory test (RS) must be done within 1 and 7 days before the laboratory test (LT).
The results are reported (RP) within 1 and 48 hours after the end of the test.
Ex.
3 is illustrated in Fig.
1.
In Ex.
3, the event of "performing a laboratory test" (LT) stands for a class of events, i.e.
it stands for a set of individual occurrences (LT1, LT2, ..., LTk) of the actual event "performing a laboratory test", that are associated to specific patients and occur in Report (RP)  before  CLASSES temp.
constraints INSTANCES  RSk  LT1  2.1.
Temporal classes+instances  1-48 hours  Lab_Test (LT)  RS2  2.
An introduction to the problem  LTk RP2  RPk  Instance_of  time-line  Correlation: {<C1,RS1,LT1>,<C2,RS2,LT2>,.., <Ck,RSk,LTk>, <C1,LT1,RP1>, <C2,LT2,RP2>, ..., <Ck,LTk,RPk>}  Fig.
1.
A graphical representation for the execution of Ex.
3.  specific intervals of time.
These are, of course, instance events of LT.
The bottom-up arcs in Fig.
1 represent instance-of arcs.
If we wish to check the temporal consistency of the instances of events wrt the classes of events, we have to instantiate the temporal constraints about the classes (in the figure, 1-7 days before and 1-48 hours before) on the instances: if the plan states that the reservation RS must be done 17 days before the lab test LT, we have to test whether each instance RS1, ..., RSk is 1-7 days before the relative instance LT1, ..., LTk.
In doing this, we have to face the problem of relating the instances, which the correlation relation (discussed e.g.
in [10, 13]) allows us to do (it is no further discussed here because it is out of the scope of this paper).
Moreover, we have to take into account the aspect of prediction.
The classes of events may have an "existential" role, in fact, the classes of events in Ex.
3 not only impose temporal constraints on the instances, but also "existential" constraints.
For example, it would not be possible to have a laboratory test without the relative report.
Whenever we test the consistency of the execution of the guideline, we cannot simply inherit the temporal constraints on the correlated instances, but we also have to check whether the proper instances exist.
This problem is closely related with the semantic assumptions we impose on the observations: are the observations complete or is it possible that any instance of event is not observed?
As regards future events, are they present in the knowledge base, or (as in the majority of the application domains) is it impossible to know in advance when the future instances of events will occur?
Of course, the semantic assumptions closely influence the possible consistency checking mechanisms, in fact, if the report is not observed and we assume that the observations are not complete, the instances may however be consistent (i.e., the predictive role of the classes is not needed), but if we assume that the observations are complete, a missing report surely leads to an inconsistency.
2.2.
Temporal periodicity/repetition  constraints  about  Repeated events are widespread in many application domains.
In many cases (e.g.
in the clinical therapies, such as in Ex.
1 and Ex.
2), repeated events are periodic, since they occur at regular times.
As regards the representation of periodicity constraints, we face two alternatives: the extensional representation or the intensional representation.
In the extensional representation, we explicitly represent all the repetitions of each class of events: this could lead in an unnecessary explosion of events, in particular when the number of repetitions is high, such as in Ex.
2.
Moreover, if we pursue the aim of hiding the complexity of the time-related tasks to the user, an intensional approach with a compact formalism representing the repetitions may result easier to manage and more "user-friendly".
In particular, the formalism has to be powerful (in order to capture the real-world cases), compact, and simple to use (in order to hide the complexity).
As regards its expressive power, it has to be able to: - manage arbitrary nesting of time intervals (e.g., in Ex.
2 we have four nesting levels: four years, 88 weeks, one week, and one day); - give support for some degree of uncertainty (e.g.
in Ex.
2 the guideline does not explicitly constrain the position of the two consecutive days in the week).
In this context, when we impose a repetition pattern, it would be useful to have periodicity constraints.
These may be considered as a special kind of temporal constraints; to be more specific, while duration constraints are monadic constraints because they regard single instances, and delay/precedence constraints are binary constraints because they regard pairs of instances, periodicity constraints are n-ary constraints, because they regard multiple events.
This leads to a further problem that we have to face whenever we want to inherit the constraints from a plan containing repeated classes to instances, that is, we need to associate the instances to the periodicity.
If we have constraints like "repeat twice each day A before B", in order to test the precedence relation, we cannot consider all the (instances of) A and all the (instances of) B, but only the pairs of (the instances of) A and B that belong to the same repetition.
This may be not a trivial task when the knowledge base of instances only contains atomic instances.
In the sample constraint, e.g., it is possible that only the occurrences of the instances of A and B are observable, while the "abstract" instance "occurrence of A+B" is not.
2.3.
Extending Manager  the  Integrated  Temporal  To summarize, the goal of the work described in this paper is to allow users to abstract from the  above-mentioned problems and to solve them once and for all in a general, domain-independent way, in order to hide the "complexity".
To do so, we extend the tractable temporal manager proposed in [20] in the direction of: - enhancing the expressive power of the representation language, especially the formalism regarding the periodicity/repetition constraints; - developing new algorithms for temporal reasoning without increasing the complexity of the algorithms in [20].
3.
The representation formalism As regards the representation of the problem, we have followed a layered approach, making a distinction between a high representation level and a low representation level.
The high level is the level that interfaces with the user: it is desirable that the language is expressive and that it is possible to provide reasoning facilities.
It is important to take into account the trade-off between the expressiveness and the complexity of the reasoning mechanisms.
In our approach we have chosen to retain the tractability.
The low level is meant for the internal representation.
We use a "standard" approach, such as the STP one ([5]), where the reasoning mechanism consists in the propagation of constraints.
It is worth noting that, while reasoning on the low level is quite trivial since all-pairs shortest paths algorithms such as Floyd-Warshall's one are correct and complete, filling the gap from the high level to the low level may not be so simple; in fact, one has to face all the problems mentioned in section 2: correlation, association, semantic assumptions, inheritance of periodic patterns, and prediction.
3.1.
Representing classes+instances The high-level constraint language about instances retains the language described in [20].
It provides primitives in order to represent (possibly imprecise) dates, delays between endpoints of events, durations, and qualitative constraints between endpoints of events.
Such language has been deliberately designed in such a way that only constraints that can be mapped onto conjunctions of bounds on differences (i.e., on an STP framework [5]) can be represented (see [16] for more details).
Besides temporal constraints, this high-level language also allows to specify:  - instance-of relations (between an instance and its class), - correlation relations (between pairs of instances).
Thus, according to our language, the instances are represented with a knowledge base IKB, composed by a quadruple <IKB_Elements, IKB_Instance_Of, IKB_COR, IKB_Constraints>, where the first term represents the set of instances, the second one the instance-of relations, the third one the correlation relations, and the last one the temporal constraints between instances.
As regards qualitative and quantitative temporal constraints between classes, we basically retain the simple high-level constraint language of instances.
Thus, our language provides primitives in order to describe (possibly imprecise) dates, durations, and delays, as well as continuous pointizable qualitative temporal constraints ([25]).
Notice, however, that the semantics of such constraints is different depending on whether they apply to instances or to classes (see [16]).
Furthermore, the high-level language for classes provides primitives to describe composite events ([20]) and periodicity constraints (see section 3.2).
Thus, classes are represented in the knowledge base CKB, that is a triple <CKB_Elements, CKB_Part_of, CKB_Constraints>, where the first term represents the set of classes, the second one represents the part-of relations, and the last one represents temporal constraints (including the constraints on repetition/periodicity).
3.2.
Representing constraints  repetition/periodicity  The constraints on repetitions and periodicities are temporal constraints of the form Repetition(C, RepSpec), where C is the class to be repeated and RepSpec is a parameter that imposes the repetition pattern.
The repetition specification is represented by means of a recursive structure of arbitrary depth, RepSpec = <R1, R2, ..., Rn>, where each level Ri states that the events described in the next level (i.e., Ri+1, or - by convention - the class C, if i=n) must be repeated a certain number of times in a certain time lapse.
To be more specific, the basic element Ri consists of a triple: Ri = <nRepetitionsi, I-Timei, repConstraintsi>,  where the first term represents the number of times that Ri+1 must be repeated, the second one represents the time lapse in which the repetitions must be included, and the last one may impose a pattern that the repetitions must follow.
We can roughly describe the semantics of a triple Ri as the natural language sentence "repeat Ri+1 nRepetitionsi times in exactly I-Timei".
repConstraintsi is a (possibly empty) set of pattern constraints, representing possibly imprecise repetition patterns.
Pattern constraints may be of type: - fromStart(min, max), representing a (possibly imprecise) delay between the start of the I-Time and the beginning of the first repetition; - toEnd(min, max), representing a (possibly imprecise) delay between the end of the last repetition and the end of the I-Time; - inBetweenAll(min, max) representing the (possibly imprecise) delay between the end of each repetition and the start of the subsequent one; - inBetween((min1, max1), ..., (minnRepetitionsi1, maxnRepetitionsi-1), representing the (possibly imprecise) delays between each repetition and the subsequent one.
Please note that any couple (mini, maxi) may be missing, to indicate that we do not give any temporal constraint between the i-th repetition and the (i+1)-th one.
Let us see an example: (Ex.
4) Intrathecal methotrexate must be administered 7 times during 88 weeks, never less than 10 weeks apart or more then 14 weeks apart.
Ex.
4 may be represented with a simple one level specification: Repetition(Intrathecal_methotrexate, < <7,88wk, {inBetweenAll(10wk, 14wk)}>>) It is worth noting that neither repConstraintsi nor nRepetitionsi are mandatory.
If repConstraintsi is an empty set, the repetitions do not necessarily have to follow any particular pattern.
If nRepetitionsi is missing, it is easy to automatically "fill the blank" considering the I-Time of the next level in order to infer the (maximum) number of repetitions that fits in the given I-Time.
Notice that, since we aim at designing tractable algorithms in order to deal with correct and complete consistency checking, we have to impose that I-Times must be specified in an exact way.
It should be pointed out that the formalism we are introducing allows to manage different kinds of uncertainty/variability: - the repetitions are not constrained to completely cover the I-Time, and there may be arbitrary delays between the repetitions; - the (min, max) specifications in repConstraintsi make it possible to indicate variable delays between the repetitions.
repConstraintsi also allows to represent nonsymmetric patterns, such as in Ex.
5: (Ex.
5) Repeat action A every week for 8 weeks on Mondays and Saturdays.
If we regard Sunday as the first day of the week, we represent Ex.
5 as follows: [?
]>, <2,1wk, Repetition(A, <<_, 8wk, {fromStart(1d,1d), toEnd(0d,0d)}> >).
Moreover, the repetitions may be nested at arbitrary depth, representing simple cases with few levels as in Ex.
4 and more complex cases with more levels as in Ex.
2.
Ex.
2 may be represented in the following way: Repetition(Cotrimoxazole, < <2, 4y, [?
]>, <_, 88wk, [?
]>, <2, 1wk, {inBetweenAll(0,0)}>, <2, 1d, [?
]>>), where the pattern constraint inBetweenAll(0,0) in the third triple imposes that the days must be consecutive.
3.3.
Internal representation for temporal constraints As in [20], we model repeated events as composite events and represent the constraints regarding repeated actions into separate STP frameworks, one for each repeated event.
Thus, in our approach, the overall set of constraints between classes of events is represented by a tree of STP frameworks (STP tree henceforth) ([18]).
The root of the tree is the STP which homogeneously represents the constraints between all the classes of events (both composite ones and atomic ones), except repeated events.
Each node in the tree is a STP and has a child for each repeated class.
Each edge in the tree connects a pair of endpoints in a STP (the starting and ending point of a repeated event) to the STP containing the constraints between its subactions and is labelled with the recursive repetition structure RepSpec describing the temporal constraints on the repetitions.
[168d,168d]  N1  Sch  Ech  <<6, 24wk, {inBetweenAll(23d,23d)}>> [0d,0d]  Smc  N2  Spc  [5d,5d]  [5d,5d]  Emc  <<5, 5d, {}>,<2, 1d,{}>>  Epc [0d,0d]  <<5, 5d,{}>, <1,1d,{}>>  [0d,1d]  N3  Sm  [0d,1d]  Em  Sp  Ep  N4  Fig.
2.
Graphical representation of the internal representation for Ex.
1.
In Fig.
2, e.g., a graphical representation regarding Ex.
1 is shown.
4.
Temporal reasoning In order to deal with the more powerful formalism described in this work, it is not possible to use the reasoning mechanisms depicted in [20].
In this section, we first describe an algorithm designed for checking the consistency of a knowledge base of (possibly repeated) classes.
Then, we describe an algorithm that checks in an integrated way the consistency of instances of events wrt the relative classes, and we show that, despite the more expressive formalism, the complexity of the algorithm does not increase wrt [20].
4.1 Consistency checking on (possibly repeated) classes of events The procedure classConsistency in Fig.
3 tests the consistency of the classes by filling the gap between the high-level expressive representation language and the low-level "simple" internal representation as STP ([5]), making explicit the semantic assumptions carried by the intensional high-level language.
At the end of step 2 of the procedure classConsistency, S is a STP which is semantically equivalent to the STP tree T. This task is accomplished by: i) visiting recursively the STP tree (task performed by the procedure unfoldNode); ii) "unfolding" the repetitions (task performed by the procedure unfoldRep).
The procedure unfoldNode, recursively called on each STP node X in the STP tree, inserts in S (step 1) a class CX representing the whole node X, and (steps 2-3) a class CA representing each nonrepeated class.
For each repeated class CR (steps 4-7) it calls the procedure unfoldRep in order to "unfold" the repetitions.
Finally, in steps 8-11 the monadic and  binary temporal constraints are carried to S. The procedure unfoldRep, after inserting in S a class C1 representing the whole I-Time in which the repetitions must take place, exploits the recursive structure of the repetition specifications to recursively call itself (step 5, please note the shift of the triples) as many times as prescribed by the specification R1.
This is done until the last triple in RepSpec is reached (else branch of if statement in step 4), then the procedure unfoldNode is called (step 6) to continue the unfolding on the child node in the STP tree.
Finally, unfoldRep adds to the STP (steps 7-12) the constraints corresponding to the semantic assumptions of the construct: - the repetitions must be included in a time procedure classConsistency(T : CKB) (1) initialize S to an empty STP (2) unfoldNode(root(T), S) (3) S' := FloydWarshall(S) (4) return S' procedure unfoldNode(X : STPNode, S : STP) (1) add to S the placeholder class CX (2) forall CA | CA is not a repeated class in X do (3) add to S the class CA od (4) forall CR | CR is a repeated class in X do (5) let RepSpec = (R1, ..., Rn) be the repetition specification of class CR (6) Csub := unfoldRep(X, CR, RepSpec) (7) add to S the constraints that Csub [?]
CX od (8) for each monadic constraint in X do (9) add the constraint to the corresponding classes in S (10)for each binary constraint in X do (11) add the constraint to the corresponding classes in S return CX procedure unfoldRep(X : STPNode, S : STP, C : Class, RepSpec = < R1, R2,..., Rn>) (1) add to S the placeholder class C1 (2) let R1 = <nRep1, IT1, constrs1> (3) for r := 1 to nRep1 do (4) if R1 is not the last one in RepSpec then (5) Csub,r := unfoldRep(X, C, (R2, ..., Rn)) else (6) Csub,r := unfoldNode(child(C, X)) od (7) add to S the constraint that duration of C1 is IT1 (8) add to S the constraints that Csub,i [?]
C1, i = 1, ..., nRep1 (9) add to S the constraints that Csub,i+1 is after Csub,i, i = 1, ..., nRep1-1 (10)add to S the possible constraint fromStart in constrs1 in R1 between C1 and Csub,1 (11)add to S the possible constraint toEnd in constrs1 in R1 between Csub,nRep1 and C1 (12)add to S the constraints inBetween and inBetweenAll in constrs1 in R1 between Csub,i and Csub,i+1, i = 1, ..., nRep11 return C1 Fig.
3.
Algorithms for temporal reasoning on classes of events.
interval lasting exactly IT1 (step 7); - each repetition must be included in the I-Time (step 8); - the repetitions must not overlap (step 9); - the repetitions must follow the possible pattern in repConstraints (steps 10-12).
It is possible to test the consistency of the resulting STP S (step 3 of procedure classConsistency) by propagating the constraints using the Floyd-Warshall's all-pairs shortest path algorithm.
4.2 Consistency checking on classes+instances We will start by describing an algorithm that assumes full observability of the events even in the future and total ordering of the instances, and then we will relax the first assumption to include the case where there is no observability in the future.
The problem of relaxing the assumption of full observability in the past and the assumption of total ordering is discussed in note 2 and in subsection 5.3.
We intend that, whenever an inconsistency is detected, the algorithms report it and stop.
For the sake of brevity, we assume that all the input instances are correlated.
This is not a restrictive assumption, in fact, since correlation allows to partition instances into independent sets ([10, 13]), the consistency checking of the instances may be iterated for each partition.
In order to test the consistency of classes+instances, the basic idea in the procedure integratedConsistency in Fig.
4 is to: i) test the consistency of the classes and obtain the unfolded STP (step 1); ii) establish a one-to-one correspondence between the classes and the instances (steps 4-6); iii) inherit the constraints from the classes to the instances (steps 8-9); iv) test the consistency of the new "augmented" STP (step 10).
After checking the consistency of the classes (step 1), the instances corresponding to the composite and repeated classes are inserted (step 2) in the IKB1.
Then, the constraints on the instances are propagated in order to infer a possible not explicit total order between the instances.
Steps 4-6, as said above, try to establish a one-toone correspondence between the classes in S and the instances in I.
This task may be efficiently  1  It is worth noticing that they are not already in the IKB, because we assume that only atomic events may be observed.
If this assumption does not hold, then it suffices to simply remove step 2.  procedure integratedConsistency(T : CKB, I : IKB) (1) S := classConsistency(T) (2) add to I the placeholder instances corresponding to the placeholder classes in S (3) I' := FloydWarshall(I) (4) for each class C in S taken in temporal order do (5) let i be the first instance of C in I' not yet taken in consideration (6) if i does not exist then return INCONSISTENT od (7) if there exists an instance in I' not yet considered then return INCONSISTENT (8) inherit the monadic constraints from the classes in S to the corresponding instances in I' (9) inherit the binary constraints from the classes in S to the corresponding instances in I' (10)I" := FloydWarshall(I') Fig.
4.
Algorithm for temporal reasoning classes+instances of events (complete observations).
on  performed thanks to the assumption of total ordering on the instances2.
In the event that an instance that the CKB predicts to be in the IKB is missing (step 6) (e.g.
because a repetition is not complete), the procedure stops and reports an inconsistency.
In step 7, we check whether there are instances that are missing the corresponding classes (e.g., because there are more repetitions than expected).
In step 8, monadic constraints (i.e.
constraints regarding durations of events) are inherited, and in step 9 binary constraints are inherited, according to the semantics of the constraints on classes ([16]).
Steps 8 and 9 may be easily performed thanks to the correspondence between classes in S and instances in I established in the previous steps.
Finally, consistency checking on the STP on instances, augmented by the inherited constraints, is performed by the Floyd-Warshall's all-to-all shortest path algorithm (step 10).
In the procedure integratedConsistencyNoFuture in Fig.
5 we relax the assumption of full observability even in the future to the case where there is no observability in the future.
The steps added or changed wrt the procedure integratedConsistency are the ones with line numbers in bold type.
The procedure integratedConsistencyNoFuture 2  To be more specific, without total ordering it could happen that in the IKB there are instances of a repeated class, and we do not know which specific repetitions they belong to.
In this case, it would be necessary to perform an inefficient search in order to establish which specific instance corresponds to which specific repetition.
Because for each possible correspondence instance-repetition it is necessary to check whether it is consistent with the other temporal constraints, this may lead the problem to intractability.
This is the reason why we retain the assumption of total ordering, at least between the instances of a repeated class.
procedure integratedConsistencyNoFuture(T : CKB, I : IKB, NOW) (1) S := classConsistency(T) (1a)for each instance i in I do (1b) add the constraint the i starts before NOW (1c)hypothesizedInstances := [?]
(2) add to I the placeholder instances corresponding to the placeholder classes in S (3) I' := FloydWarshall(I) (4) for each class C in S taken in temporal order do (5) let i be the first instance of C in I' not yet taken in consideration (6) if i does not exist then (6a) add to I a new instance i' of C (6b) hypothesizedInstances := hypothesizedInstances [?]
i' od (7) if there exists an instance in I' not yet considered then return INCONSISTENT (8) inherit the monadic constraints from the classes in S to the corresponding instances in I' (9) inherit the binary constraints from the classes in S to the corresponding instances in I' (10)I" := FloydWarshall(I') (11)for each i [?]
hypothesizedInstances do if NEC(Start(i) before NOW) then return INCONSISTENT Fig.
5.
Algorithm for temporal reasoning classes+instances (no observations in the future).
on  accepts the additional parameter NOW, corresponding to the time of the present.
In steps 1a-1b we make explicit the fact that it is not possible to observe future events: all observed instances must start before NOW.
However, the main differences between the procedures integratedConsistencyNoFuture and integratedConsistency lie in steps 6-6b and 11: when we do not find an instance that a class predicts to be in the IKB, we no longer report an inconsistency because that instance may start in the future.
Thus, there is an inconsistency only in the case that the temporal constraints in IKB and CKB impose that the instance must be observed before NOW.
Therefore, we collect all the missing instances in the set hypothesizedInstances (steps 6 and 6b) and we provisionally insert them in IKB (step 6a).
Then, we perform the inheritance and the propagation of the constraints on input+hypothesized instances, and, at the end (step 11), we test whether any hypothesized instance necessarily starts before NOW.
In this case we report the inconsistency.
It is worth noting that, in the case that the missing instances belong to a repeated class, it is not necessary to hypothesize all the repetitions, but only the first missing one; in fact, if this instance may start in the future, also the subsequent ones will, and it is not necessary to hypothesize them; on the other hand, if this instance must start before NOW, we may report the inconsistency even  without hypothesizing the others.
4.3 Complexity of the algorithms As regards the consistency checking on the classes, it is useful to observe that the recursive calls (see step 6 of procedure unfoldNode and steps 5 and 6 of procedure unfoldRep in Fig.
3), and the for loops (see step 4 of procedure unfoldNode and step 3 of procedure unfoldRep) basically traverse the STP tree, visiting each node as many times as it is repeated.
In other words, if there exists a class C such that Repetition(C, <R1, R2, ..., Rn>), Ri = <nRepetitionsi, I-Timei, repConstraintsi> - and C is not a component of another repeated class -, then the class C is visited n  [?]
nRepetitions  i  times.
If the class C is a  i =1  composite class, then, thanks to the recursive call in step 6 of unfoldRep, also its component classes n  are  visited  [?]
nRepetitions  i  times.
We  i =1  accommodate this by expressing the complexity of the algorithm wrt the number CU of classes in the extensional representation, where a class C is present as many times as it is repeated.
If we denote with L the number of classes in the intensional representation and with R the maximum number of times that any class is repeated, we can estimate CU as CU = O(R [?]
L).
Because the step 7 of the procedure unfoldNode is constant in time, and is executed - considering all executions - at most O(CU) times, the procedure is dominated by step 11, that is executed - considering all executions - for every couple of classes, i.e.
O(CU2) times.
The procedure unfoldRep is dominated by steps 8 and 12, that add the constraints imposing that the repetitions must not overlap and the constraints corresponding to the repetition patterns.
These steps, considering again all executions, are performed in time O(CU).
Thus, step 2 of procedure classConsistency takes O(CU2), and gives as output a STP which contains CU events and the placeholder classes added by the algorithm.
The added classes are at most CU, so that the STP contains O(CU) points.
Therefore, since Floyd-Warshall's algorithm is cubic on the number of points, step 3 is executed in time O(CU3) and the complexity of classConsistency is O(CU3).
As regards the consistency checking on instances, we denote with S the number of input instances.
Thanks to a possible precompilation that associates with every class its instances  (performable in O(S)), and thanks to the total ordering of the instances, step 5 of procedure integratedConsistency in Fig.
4 is performed in constant time and the entire for loop in steps 4-6 is linear in the number of classes.
As regards the inheritance of the constraints in steps 8-9, dominates the inheritance of binary constraints, which is quadratic over the number of classes.
Therefore, in the procedure integratedConsistency in Fig.
5, steps 1, 3 and 10 dominate, and the overall complexity is O(max{CU3, S3}), or, in terms of the number of input classes L and of the maximum number of repetitions R, O(max{R3 L3, S3}).
In the procedure integratedConsistencyNoFuture, steps 1a-1c are linear in the number of instances, and steps 6-6b are linear on the number of classes.
It is worth noting that for step 11 we may exploit the locality properties of STP constraints proved in [3] and perform step 11 in time linear in the number of instances in hypothesizedInstances.
The cardinality of hypothesizedInstances is at most CU, since we at most hypothesize one instance for each class.
Therefore, even relaxing the hypothesis of complete observability as regards the future, the complexity of integrated reasoning remains O(max{R3 L3, S3}).
It should be pointed out that, despite the more powerful representation language wrt the representation language described in [20], the complexity of the reasoning mechanisms does remain the same.
4.3 Preliminary experimental results The system is currently being developed in Java.
We provide preliminary experimental results regarding the algorithm that checks the consistency of the classes only, namely, the procedures classConsistency, unfoldNode and unfoldRep in Fig.
3.
The system has been implemented in JDK 1.4.
The experiments were run on a PC with a Pentium IV CPU at 2 GHz with 1 GB RAM and Windows 2000 operating system.
The system was provided with five knowledge # of classes (CU) 10 20 50 100 200  Time 344 ms 360 ms 828 ms 3203 ms 22890 ms  Tab.
1.
Number of classes and relative times for checking the consistency of the knowledge base of classes.
bases of classes, with increasing number of classes.
In Tab.
1 the number of classes CU (as described in section 4.3) and the time for the consistency checking of the CKBs are reported.
More extensive experiments are needed in order to evaluate the integrated consistency checking on classes+instances.
5.
Conclusions and discussion In this paper, we describe a formalism for representing temporal constraints on repetition and periodicity in a compact and powerful way.
Its intuitiveness makes it easy to use and its recursive structure proves to be adapt to represent arbitrary nested repetitions and supports some degree of uncertainty.
We have described two tractable algorithms for consistency checking that address all the aspects mentioned in section 1, namely, classes and instances of events, repetition/periodicity constraints, composite events, and qualitative and quantitative temporal relations.
We first have described an algorithm that assumes full observability of the instances of events, and then we have illustrated an algorithm that assumes no observability in the future and full observability in the past.
5.1.
Related works Morris et al.
([10, 11, 12]) dealt only with qualitative constraints between repeated events.
Repeated events are used as "classes" of events, with different quantifiers relating them.
Morris et al.
introduced the notion of consistent scenario in [11] and sketched an algorithm for a scenario consistent with a knowledge base of temporal constraints between repeated events.
Loganantharaj mainly faced the problem of associating possibilistic distributions to qualitative temporal constraints between periodic events ([6]) and to metric constraints concerning the durations of events, which are also expressed using transition rules ([7, 8]).
Such constraints are used in a "predictive" way: temporal reasoning is used for projecting the constraints on the durations in the future using the current domain information.
In [19] Terenziani proposed a high-level language to deal with periodicity and in [13, 23] a highlevel language to deal with period-dependent qualitative temporal constraints between repeated events, which are used as "classes".
In [14] he also defined an initial algorithm for temporal reasoning with such constraints and a set of instances of events exactly located in time.
In [16] he approached the problem of checking the  consistency of classes and instances of events with both qualitative and quantitative constraints; in [17, 18] Terenziani et al.
proposed an approach to deal with periodic, qualitative and quantitative constraints between classes of events in clinical guidelines.
Finally, in [20] Terenziani and the author of this paper defined an approach dealing with periodic, qualitative, and quantitative constraints between both classes and instances of events.
This work represents an extension of the work presented in [20]; in particular, our purpose is to improve the representation language described in [20] preserving its efficiency.
To illustrate this, [20] has a much more limited language for the specification of repetitions and periodicity.
In fact, that work presents 5 parameters to specify a periodicity: the frame time, the action time, the delay time, the I-Time, and the frequency.
The frame time corresponds to the whole time interval in which all the repetitions take place and is subdivided into action times and delay times.
Delay times represent fixed delays between an action time and the next one, whereas action times are in their turn subdivided into ITimes, where finally the events occur, at groups of "frequency".
This structure is narrow: in fact, it does not allow to subdivide the intervals into more than three levels (frame times, action/delay times and Itimes), thus making it impossible to represent a case such as the one depicted in Ex.
2, which requires - as shown in section 3.2 - four levels.
On the other hand, despite its richer expressive power, the formalism described in this work is more "user-friendly".
For example, when dealing with simple cases which do not require multiple levels, the formalism described in [20] implies to arbitrarily impose that action times equal I-Times, whereas the formalism defined in this work allows to simply use fewer recursive levels.
To illustrate this, let us suppose that we want to represent the simple case "repeat A twice for a week".
While with the formalism in [20] it is necessary to state: FrameTime=1wk, ActionTime=1wk, DelayTime=0, I-Time=1wk, freq=2, with the formalism described in this work it is sufficient to state: < <2, 1wk, [?
]> >.
Moreover, in [20] it is mandatory that the subdivisions are a partition of the higher level; in fact, the union of the action times and the delay times must be equal to the frame time, and the union of the I-Times must be equal to the action time.
Furthermore, the "pauses" between the intervals must only be specified at the level of  action times, and they must have a fixed duration, which is equal for all the repetitions.
With the periodicity constraint formalism introduced in this work, we provide a more compact and more expressive language.
Its recursive structure supports an arbitrary number of nested levels, where any level may or may not be a partition of the higher level: this way we provide for uncertainty in the subdivision of the time intervals.
A further support for uncertainty lies in the possibility to specify variable delays between the repetitions.
As regards repetition patterns, not only the repetitions can follow different patterns on each level, but they can also be differently constrained within each level.
5.2.
Applications The need to cope with the various temporal constraints we described in section 1 aroused from our previous work in the field of clinical guideline management.
The described system integrates in a joint project with Azienda Ospedaliera S. Giovanni Battista of Torino for the design and development of GLARE (GuideLine Acquisition, Representation and Execution) ([15, 21, 22, 18]).
Furthermore, it will integrate in a starting joint project with Cancer Research of London.
5.3.
Future work We are currently trying to extend our approach in order to manage repetitions based on conditions (e.g., while B holds, repeat the action A).
This influences both the consistency checking on classes and the consistency checking on instances, because appeals to the predictive role of the classes and therefore deserves specific attention.
Furthermore, we are studying the possibility to exploit the locality properties proved in [3, 4] in order to efficiently answer to temporal queries.
Other possible developments comprise the overcoming of some limiting assumptions, such as those of total ordering of the instances and full observability.
Although both assumptions are reasonable in the domain of clinical guidelines, there may be domains where they cannot hold.
Unfortunately, these two assumptions make it possible to devise tractable temporal reasoning mechanisms, because it is fundamental to associate an instance with the relative repetition.
In fact, if the two assumptions hold, this task may be performed efficiently (as shown in section 4.2), but, if they do not hold, it would be necessary to  generate a "scenario" for each possible pair (instance, repetition), and test its consistency with the temporal constraints in the knowledge base.
Moreover, releasing the tractability for complete reasoning would make it possible to further enrich the expressiveness dealing with different forms of disjunctions of temporal constraints.
In the context of overcoming the limiting assumption discussed above, in order to save some efficiency, we are also investigating the possibility to incorporate the approach described in this work into a backtracking system and to use the temporal constraints in order to restrict the search space.
Acknowledgements The author wishes to thank prof. Paolo Terenziani for the fundamental contribution as well as his precious help and valuable guidance.
References [1] J.F.
Allen.
"Maintaining Knowledge about Temporal Intervals", Comm.
of the ACM, 26(11): 832-843, 1983.
[2] J. Allen, "Time and Time again: the Many Ways to Represent Time", Int'l J.
Intelligent Systems, vol.
6, no.
4, pp.
341-355, July 1991.
[3] V. Brusoni, L. Console, and P. Terenziani, "On the computational complexity of querying bounds on differences constraints", Artificial Intelligence 74(2), pp.
367-379, 1995.
[4] L. Console, P. Terenziani, "Efficient processing of queries and assertions about qualitative and quantitative temporal constraints", Computational Intelligence 15(4), pp.
442-465, November 1999.
[5] R. Dechter, I. Meiri, J. Pearl, "Temporal Constraint Networks", Artificial Intelligence 49, pp.
61-95, 1991.
[6] R. Loganantharaj, and S. Gimbrone.
"Probabilistic Approach for Representing and Reasoning with Repetitive Events", Proc.
second International Workshop on Temporal Representation and Reasoning (TIME'95), Melbourne, FL, pp.
26-30, 1995.
[7] R. Loganantharaj, and S. Kurkovsky.
"A new model for projecting temporal distance using fuzzy temporal constraints", Proc.
IEA/AIE'97, 1997.
[8] R. Loganantharaj, and S. Gimbrone, "Issues on Synchronizing when Propagating Temporal Constraints", Proc.
National Conference on Artificial Intelligence Workshop on Spatial and Temporal Reasoning, 1997.
[9] I. Meiri, "Combining Qualitative and Quantitative Constraints in Temporal Reasoning", In Proceedings National Conference on Artificial Intelligence, pp.
260-267, 1991.
[10] R.A. Morris, W.D.
Shoaff, and L. Khatib, "Path Consistency in a Network of Non-convex Intervals", Proc.
thirteenth Int'l Joint Conf.
on Artificial Intelligence, pp.
655660, Chambery, France, 1993.
[11] R.A. Morris, L. Khatib, and G. Ligozat.
"Generating Scenarios from specifications of Repeating Events", Proc.
second International Workshop on Temporal Representation and Reasoning (TIME'95), Melbourne, FL, pp.
41-48, 1995.
[12] R.A. Morris, W.D.
Shoaff, and L. Khatib, "Domain Independent Temporal Reasoning with Recurring Events", Computational Intelligence 12(3) pp.
450-477, 1996.
[13] P. Terenziani, "Integrating calendar-dates and qualitative temporal constraints in the treatment of periodic events", IEEE Trans.
on Knowledge and Data Engineering 9(5), 1997.
[14] P. Terenziani, "Integrated Temporal Reasoning with Periodic Events", Computational Intelligence 16(2), pp.
210256, May 2000.
[15] P. Terenziani, G. Molino, and M. Torchio, "A modular approach for representing and executing clinical guidelines", Artificial Intelligence in Medicine 23, pp.
249-276, 2001.
[16] P. Terenziani, "Temporal Reasoning with classes and instances of events", Proc.
TIME 2002, Manchester, UK, IEEE Press, pp.
100-107, 2002.
[17] P. Terenziani, S. Montani, C. Carlini, G. Molino, M. Torchio, "Supporting physicians in taking decisions in Clinical Guidelines: the GLARE's 'what if' facility", Journal of the American Association of Medical Informatics (JAMIA), Proc.
Annual Fall Symposium, 2002.
[18] P. Terenziani, C. Carlini, S. Montani, "Towards a Comprehensive Treatment of Temporal Constraints in Clinical Guidelines", Proc.
TIME 2002, Manchester, UK, IEEE Press, pp.
20-27, 2002.
[19] P. Terenziani, "Symbolic User-defined Periodicity in Temporal Relational Databases", IEEE Transactions on Knowledge and Data Engineering, 15(2), pp.
489-509, March/April 2003.
[20] P. Terenziani, and L. Anselma, "Towards an Integrated Approach Dealing with Part-of, Instance-of and Periodicity Constraints", Proc.
TIME 2003, IEEE Society Press, pp.
3746, 2003 [21] P. Terenziani, S. Montani, M. Torchio, G. Molino, and L. Anselma, "Temporal Consistency Checking in Clinical Guidelines Acquisition and Execution: the GLARE's Approach".
Journal of the American Association of Medical Informatics (JAMIA), Proc.
Annual Fall Symposium, pp.
659-663, 2003.
[22] P. Terenziani, S. Montani, A. Bottrighi, M. Torchio, G. Molino, L. Anselma, and G. Correndo, "Applying Artificial Intelligence to clinical guidelines: the GLARE approach", Proc.
of 8th Congress AI*IA, Lecture Notes in Artificial Intelligence, 2829:536-547, Springer-Verlag, 2003 [23] P. Terenziani, "Towards a Comprehensive Treatment of Temporal Constraints about Periodic Events", International Journal of Intelligent Systems, 18(4), 429-468, 2003.
[24] L.Vila, "A Survey on Temporal Reasoning in Artificial Intelligence", AI Communications 7(1), pp.
4-28, 1994.
[25] M. Vilain, H. Kautz, and P. VanBeek, "Constraint Propagation Algorithms for temporal reasoning: a Revised Report", D.S.
Weld, J. deKleer, eds., Readings in Qualitative Reasoning about Physical Systems.
Morgan Kaufmann, pp.
373-381, 1990.
Formalizing Actions in Branching Time: Model-Theoretic Considerations Munindar P. Singh  Microelectronics and Computer Technology Corporation 3500 W. Balcones Center Drive Austin, TX 78759-5398 USA msingh@mcc.com  Abstract  The formalization of actions is essential to AI.
Several approaches have been proposed over the years.
However, most approaches concentrate on the causes and effects of actions, but do not give general characterizations of actions themselves.
A useful formalization of actions would be based on a general, possibly nondiscrete, model of time that allows branching (to capture agents' choices).
A good formalization would also allow actions to be of arbitrary duration and would permit multiple agents to act concurrently.
We develop a branching-time framework that allows great exibility in how time and action are modeled.
We motivate and formalize several coherence constraints on our models, which capture some nice intuitions and validate some useful inferences relating actions with time.
1 Introduction  Actions and time have drawn much attention in arti	cial intelligence (AI).
Whereas much progress has been made in modeling time, corresponding progress has not been made in modeling actions.
Temporal approaches run the gamut from discrete to continuous, point-based to interval-based, and linear to branching.
By contrast, approaches to formalizing actions tend to be restricted to discrete models, typically linear and with additional assumptions such as that exactly one action happens at a time, and all actions have the same duration.
Reasoning about actions focuses on the possible causes and e ects of the actions, but not on their structure.
This work is undoubtedly of value.
However, we submit that its full potential can be realized only if actions themselves are formalized in a general framework.
What are the properties that intuitively acceptable and technically feasible models of actions and time should support?
We  address this question here.
A general model of actions would provide the underpinnings for work on concepts|such as intentions, ability, and know-how|that supervene on actions.
When actions are modeled restrictively, the technical results obtained on the above concepts end up with potentially superuous or even pernicious restrictions.
It is easy to obtain spurious results on the above concepts that rely on some irrelevant aspect of the underlying model of actions Singh, 1992].
Our main interest is in formalizing the above concepts, but we must model actions properly to succeed Singh, 1994].
We introduce this formalization to the temporal representation community here.
Our framework allows (a) time to branch to model agents' choices, (b) multiple agents to act simultaneously, (c) actions to be of varying durations relative to one another, and (d) time to be nondiscrete.
Thus choice and control can be properly captured in this framework.
Our approach is nonrei	ed Bacchus et al., 1989], as is common in the non-AI literature Emerson, 1990].
Time can variously be modeled as linear or branching.
Allen presents an interval-based linear-time theory of actions in 1984].
Turner (p. 88) and Shoham 1988, ch.
2] show that Allen's theory is not conceptually clear, especially with regard to intervals.
Shoham too restricts his models to be linear.
Allen (p. 131) and Shoham (p. 36) argue that branching time is unnecessary since the agents' ignorance can be modeled in other ways.
However, branching into the future is not a matter of ignorance, but of choice.
That is why, ignorance apart, the past can be linear but the future must branch.
(Sometimes, eciency may be gained by treating even the past as branching: we allow this.)
Galton 1990] improves over Allen's approach in some respects but does not address constraints on actions per se.
McDermott's approach, like ours, is point-based and involves branching-time 1982].
But, McDermott requires his models to be dense also, clock values are essential to his semantics.
McDermott notes, correctly, that an action cannot hap-  pen over overlapping intervals: we capture this differently.
But a lot more must be said that his and other approaches do not say.
Related research includes Thomason & Gupta, 1981 van Frassen, 1981 Haddawy, 1990 Dean & Boddy, 1988], but it does not model actions as motivated here.
We present our formal language and model next and discuss our key operators informally.
Then we motivate and formalize a number of coherence constraints on our models that are required for various useful properties.
We use these to prove some important results relating actions and time.
We close with a discussion of some open problems.
2 Technical Framework  The proposed formal model is based on a set of moments with a strict partial order, which denotes temporal precedence.
Each moment is associated with a possible state of the world, which is identi	ed by the atomic conditions or propositions that hold at that moment.
A scenario at a moment is any maximal set of moments containing the given moment, and all moments in its future along some particular branch.
 .. .. ..     q X HX  HX  XX .
.
.
t 1 HX  H   q .
.
.
HH .
.
.
a k c  a k d t2    XHXHXXX b k c  qq .. .. ..   t0 HH XXX   HHH XXXr X HHXXXX q .
.
.
HH t3HX bkd HH q .
.
.
HHH q .
.
.
t4  does action a, then whether t1 or t2 becomes the case depends on whether the second agent does c or d. Intuitively, actions are package deals.
They correspond to the granularity at which an agent can make his choices.
In Figure 1, the 	rst agent can choose between t1 and t2, on the one hand, and between t3 and t4 , on the other hand.
However, he cannot choose between t1 and t2 , or between t3 and t4 .
Both choice and limited control are thus captured.
2.1 The Formal Language  We use a qualitative temporal language, L, based on CTL* Emerson, 1990].
This captures the essential properties of actions and time that are of interest.
Formally, L is the minimalset closed under the following rules.
Here L is the set of \scenario-formulae," which is used as an auxiliary de	nition.
is a set of atomic propositional symbols, A is a set of agent symbols, B is a set of basic action symbols, and X is a set of variables.
L1.
 2  implies that  2 L L2.
p Wq 2 L and a 2 B implies that p ^ q, :p, Pp, ( a : p) 2 L s  LL L4.
p q 2 L , x 2 A, and a 2 B implies that p ^ q, :p, pUq, xa]p, xhaip, xjhaijp 2 L L5.
p 2 L implies that Ap 2 L W L6.
p 2 (L ; L) and a 2 X implies that ( a : p) 2 L 2.2 The Formal Model A model for L is a four-tuple, M = (T < A  ] ).
Here T is a set of possible moments ordered by <.
A assigns agents to di erent moments i.e., A : T 7!
}(A).  ]
is described below.
The relation < is a L3.
s  s  s  s  s  s  strict partial order:  	 Transitivity: (8t t  t 2 T : (t < t ^ t < t )) t < t ) 	 Asymmetry: (8t t 2 T : t < t ) t 6< t) 	 Irreexivity: (8t 2 T : t 6< t) 0  00  Figure 1: The Formal Model Figure 1 shows a schematic picture of the formal model.
Time may branch into the future and, in any interesting application, does.
It may be taken as linear in the past, although nothing hinges upon this.
The agents' ignorance about the past, as about anything else, is captured by beliefs (not discussed here).
Each agent inuences the future by acting, but the outcome also depends on other events.
Figure 1 is labeled with the actions of two agents.
The 	rst agent can constrain the future to some extent by choosing to do action a or action b.
If he does a, then the world progresses along one of the top two branches out of t0  if he does b, then it progresses along one of the bottom two branches.
However, the agent cannot control what exactly transpires.
For example, if he  00  0  0  00  0  0  0  A scenario at a moment t is any single branch of the relation < that includes t and all moments in some future of t that is a linear subrelation of <.
Di erent scenarios correspond to di erent ways in which the world may develop, as a result of the actions of agents and events in the environment.
Formally, a scenario at t is a set S  T that satis	es the following.
Rootedness: t 2 S 	 Linearity: (8t  t 2 S : (t = t ) _ (t < t ) _ 0  (t < t )) 00  00  0  00  0  00  0  	 Relative Density: (8t  t 2 S t 2 T : (t < t < t )) t 2 S ) 0  000  00  000  00  000  0  	 Relative Maximality: (8t 2 S t 2 T : (t < t )) (9t 2 S : (t < t ) ^ (t 6< t ))) 0  00  000  0  00  000  000  0  00  It is possible to extend S (here to t ), then it is extended, either to t (when t = t ), or along some other branch.
By itself, this does not entail that time be eternal.
S is the set of all scenarios at moment t. Since each scenario at a moment is rooted at that moment, the sets of scenarios at di erent moments are disjoint, that is, t 6= t ) S \ S = .
If t is such that t < t , then for every scenario, S 2 S , there is a scenario, S , such that S  S and S 2 S .
Conversely, for every scenario S 2 S , for each moment t 2 S , there is a scenario S 2 S , such that S  S .
S  t t ] denotes a period on scenario S from t to t , inclusive, i.e., the subset of S from t to t .
Thus, if S0 t t ]  S1 , then S0  t t ] = S1  t t ].
However, in general, S0 t t ] 6= S1 t t ].
For notational simplicity, S  t t ] presupposes t t 2 S and t  t .
00  00  000  00  t  0  t0  t  0  0  0  t0  0  t  t  0  t0  0  0  0  0  0  0  0  0  0  0  0  0  0  2.3 Semantics For p 2 L, M j= p expresses \M satis	es p at t." For p 2 L , M j= p expresses \M satis	es p at moment t on scenario S " (we require t 2 S ).
We say p is satisable i  for some M and t, M j= p. The t  s  St  t  satisfaction conditions for the temporal operators are adapted from those in Emerson, 1990].
It is assumed that each action symbol is quanti	ed over at most once in any formula.
Below, pj is the formula resulting from the substitution of all occurrences of a in p by b.
Two useful abbreviations are false  (p ^ :p), for any p 2 , and true  :false.
Formally, we have: M1.
M j=  i  t 2  ] , where  2  M2.
M j= p ^ q i  M j= p and M j= q M3.
M j= :p i  M 6j= p M4.
M j= Ap i  (8S : S 2 S ) M j= p) M5.
M j= Pp i  (9t : t < t and M j= p) W M6.
M j= ( a : p) i  (9b : b 2 B and M j= pj ), where p 2 L W M7.
M j= ( a : p) i  (9b : b 2 B and M j= pj ), where p 2 (L ; L) q and M8.
M j= pUq i  (9t : t  t and M j= (8t : t  t  t ) M j= p)) M9.
M j= xa]p i  (8t 2 S : S  t t ] 2  a] implies that (9t : t < t  t and M j= p)) M10.
M j= xhaip i  (9t 2 S : S  t t ] 2  a] and (9t : t < t  t and M j= p)) M11.
M j= xjhaijp i  (9t 2 S : S  t t ] 2  a] and (9t : t < t  t and (8t : t < t  t implies that M j= p))) M12.
M j= p ^ q i  M j= p and M j= q M13.
M j= :p i  M 6j= p M14.
M j= p i  M j= p, where p 2 L a b  t t  t  t  t  t  t  t  0  t  St  0  t0  t  : c :  00  x  x  0  0  0  St  St  St 00  0  0  St  x  0  0  00  0  St0  St00  0  St  Figure 2: Actions: Nonsynchronized and of Varying Durations Basic actions can be of arbitrary durations.
Multiple agents may act simultaneously.
The set of actions available to an agent can be di erent at di erent moments.
For example, the actions of moving a block may take more or less time than the action of turning a knob.
This case is diagramed in Figure 2, which also shows that actions may begin and end arbitrarily.
The intension,  ] , of an atomic proposition is the set of moments at which it is true.
The intension of an action symbol a is, for each agent symbol x, the set of periods in which an instance of a is performed by x.
Thus t 2  p] means that p is true at moment t and, S  t t ] 2  a] means that agent x is performing action a from moment t to moment t .
When S  t t ] 2  a] , t corresponds to the ending of a, but t does not correspond to the initiation of a.
This is because a may already be in progress before t. Constraints C1 and C2 of section 3 pertain to this aspect.
All basic actions take time.
That is, if S  t t ] 2  a] , then t < t .
a b  s  00  b = move a block a = turn a knob  0  St  a b  b 9 : 9 9 a XXyXXXXX XyXXXXb XXXXX zXXX yXXXXXXXXXX a XXX z XXXXX XdX zX X  t  00  0  00  0  St00  0  00  x  x  0  0  St00  0  00  0  0  000  x  000  00  St000  St  St  St St  St  St  t  2.4 Temporal and Action Operators: Discussion  pUq is true at a moment t on a scenario, i  q holds at a future moment on the given scenario and p holds on all moments between t and the selected occurrence of q. Fp means that p holds sometimes in the future on the given scenario and abbreviates trueUp.
Gp means that p always holds in the future on the given scenario it abbreviates :F:p. Pp denotes p held at  some moment in the past.
The branching-time operator, A, denotes \in all scenarios at the present moment."
Here \the present moment" refers to the moment at which a given formula is evaluated.
A useful abbreviation is E, which denotes \in some scenario at the present moment."
In other words, Ep  :A:p. For example, in Figure 1, EFr and AFq hold at t0 , since r holds on some  moment on some scenario at t0 and q holds on some moment on each scenario.
L also contains operators on actions.
These are based on operators in dynamic logic, but are given a linear rather than a branching semantics.
For an action symbol a, an agent symbol x, and a formula p, xa]p holds on a given scenario S and a moment t on it, i , if x performs a on S starting at t, then p holds at some moment while a is being performed.
The formula xhaip holds on a given scenario S and a moment t on it, i , x performs a on S starting at t and p holds at some moment while a is being performed.
These de	nitions require p to hold at any moment in the (left-open and right-closed) period in which the given action is being performed.
Thus they are weaker than possible de	nitions that require p to hold at the moment at which the given action completes.
It is essential to allow the condition to hold at any moment in the period over which the action is performed.
This is because we are not assuming that time is discrete or that all actions are of equal durations and synchronized to begin and end together.
Intuitively, if we insisted that the relevant condition hold at the end of the action, then an agent could e ectively leap over a condition.
In that case, even if a condition occurs while an action is performed, we may not have xhaip.
For example, if p is \the agent is at the equator," and the agent performs the action of hopping northwards from just south of the equator, he may end up north of the equator without ever (of	cially) being at it.
That would be quite unintuitive.
For this reason, the present de	nitions are preferred although as a consequence, the operators h i and  ] are not formal duals of each other.
But this is made up for by having a more intuitive set of de	nitions, which also enable the right relationship between the action operators and F, G, and U to be captured.
Recall from above that pUq considers all moments between the given moment and the 	rst occurrence of q, not just those at which di erent actions may end.
Further, xjhaijp holds on a scenario S and moment t if x performs action a starting at t and p holds in some initial subperiod of the period over which a is performed.
This operator is necessary to relate actions with time for the following reason.
We allow actions to happen over periods which contain moments between their endpoints.
Such cases can arise even in discrete models if all actions are not unit length.
Consequently, if a is performed at t and q holds at an internal moment of a and p holds throughout, then pUq holds at t. But absent the jh ij operator, we cannot characterize pUq recursively in terms of actions.
One useful characterization is given in section 4: this helps in giving the 	xed point semantics of the temporal operators, which is essential to computing them eciently.
The above action modalities yield scenarioformulae, which can be combined with the branchingtime operators A and E. Axa]p denotes that on all  scenarios S at the present moment, if a is performed on S , then p holds at some moment on S between the present moment and the moment at which a is completed.
Similarly, Exhaip denotes that a is being performed on some scenario at the present moment and that on this scenario p holds at some moment between the present moment and the moment at which a is completed.
In other words, Axa]p corresponds to the necessitation operator and Exhaip to the possibility operator in dynamic logic.
Existential quanti	cation over basic actions is a useful feature.
Of the several basic actions that an agent may do at a given moment, we would often like to talk restrictively of the subset of actions that have some interesting property.
Indeed, we need something like this to formally express the idea of choice: an agent may be able to do several actions, but would, in fact, choose to do one, e.g., one of those that ensure success.
3 Coherence Constraints  For the above models to be coherent and useful, further technical constraints are required.
These are motivated and formalized below.
z z  t0 t1  a }|  }|a  { {  t3  t2  Figure 3: Case Disallowed by Action Uniqueness (1) z  t0  }|a  {  t1  z  a}|  t2  {  t3  Figure 4: Case Disallowed by Action Uniqueness (2) C1.
Uniqueness of Termination of Actions:  Starting at any given moment, each action can be performed in at most one way on any given scenario.
In other words, for any action a, scenario S , and moments t0  t1 t2 t3 in S , we have that S  t0 t2] 2  a] and S  t1 t3] 2  a] implies that, if t0  t1 < t2 , then t2 = t3.
This is needed to exclude ill-formed models in which an action does not have a unique moment of ending (see Figures 3 and 4).
If an agent performs an action and then repeats it, the repetition counts as a separate instance, because it has a distinct starting moment.
This constraint permits di erent actions with possibly distinct endpoints to happen simultaneously.
In discrete models with unit length actions, both endpoints are necessarily unique here only the termination point is assumed to be unique.
a }|  z  t  ...  t  00  z  }|  in Figure 6, would allow a condition to be inevitable and yet unreachable though any 	nite sequence of actions.
It is important that this not be the case for inevitability to relate properly with know-how.
This constraint always holds in discrete models.
{  a  {  t  ... S  0  Figure 5: Actions in Progress  Actions in Progress: It helps in relating moments with actions to require that S  t t ] 2  a] ) (8t : t  t < t ) S  t  t ] 2  a] ).
This allows us to talk of an agent's actions at any moment at which they are happening, not just where they begin.
However, in accordance with constraint C1, actions begun at a moment still have a unique ending moment.
As a result of this constraint, the actions operators behave properly.
For example, if an agent can achieve a condition by performing some action, then he can also achieve it while in the process of performing that action (until it happens).
This constraint holds vacuously in discrete models.
Figure 5 shows how this constraint causes the intension of an action to be 	lled out by suxes of the period over which it is performed.
The period S  t  t ] is not added to  a] , since that would lead to a violation of our assumption that S  t t ] 2  a] implies that t < t .
This would cause ambiguity between an action instance ending at t and another beginning there.
C3.
Passage of Time: Something must be done by each agent along each scenario in the model, even if it is some kind of a dummy action.
This assumption ensures that time does not just pass by itself, and is needed to make the appropriate connections between time and action.
Formally, (8t 2 T x 2 A(t) S 2 S ) ((9t 2 S )) (9t 2 S a : S  t t ] 2  a] ))).
C2.
00  0  0  00  0  0  0  0  0  0  t  x  0  t  t  0  0  t  ... S  00  Figure 6: Limit Sequences Disallowed by Reachability of Moments C4.
S0  0  00  Reachability of Moments: For any scenario and two moments on it, there is a 	nite number of actions of each agent that, if performed on that scenario starting at the 	rst moment, will lead to a moment in the future of the second moment.
Formally, (8S : (8t t 2 S : t < t ) (9t : t  t and (9a1  .
.
.
 a and S  t t ] 2  a1 .
.
.
 a ] )))).
This condition is intended to exclude models in which there are moments that would require in	nitely long action sequences to reach.
Such models, e.g., as 0  0  00  00  0  00  n  n  t  tX 0    a  X-XXXX XXXXX t1 S1  Figure 7: Actions Cannot be Partially Performed on any Scenario  Atomicity of Basic Actions: If an agent is performing an action over a part of a scenario, then he completes that action on that scenario.
This makes sense since the actions in the model are basic actions, performed with one atomic choice by their agent.
If an action in some domain can in fact be chopped into a pre	x and sux such that the sux is optional, then it should be modeled as two separate basic actions, the 	rst of which completes entirely and the second of which may not be begun at all.
Formally, let t t  t1 2 T, such that t < t < t1.
Let S0  S1 2 S , such that S1  t t ] 2 S0 .
Then S1 t t1] 2  a] implies that (9t0 2 S0 : S0 t t0] 2  a] ).
Intuitively, S1 t t1] 2  a] means that x is performing a from t to t1.
Therefore, he must be performing a in any subperiod of that, including S1 t t ], which is the same as S0  t t ].
Thus, a must be completed on S0 .
Higher-level actions do not satisfy this.
For example, Al may be crossing the street (on a scenario) even if he did not cross it successfully on that scenario, e.g., by being run over by a bus.
Our models represent physical systems, albeit nondeterministic ones.
The actions available to the agents and the conditions that hold on di erent scenarios leading from a given state are determined by that state itself.
Constraints on agent's choices, abilities, or intentions can thus be exibly modeled.
A well-known alternative characterization of models of time is by the set of all scenarios at all states.
We relate moments and states as follows.
De	ne a relation  to indicate the state-equivalence of moments and periods.
The state at a moment is precisely characterized by the atomic propositions that hold at that moment.
For moments, t and t , we de	ne t  t i  f 2 jt 2  ] g = f 2 jt 2  ] g. For sets of C5.
0  0  t  0  x  x  x  0  0  0  0  0  moments, L and L , we de	ne L  L in terms of an order-isomorphism, f .
Given two sets L and L with an order <, a map f from L to L is an orderisomorphism i  (a) f is onto, (b) (t 2 L i  f (t) 2 L ), and (c) (8t t0 2 L : t < t0 i  f (t) < f (t0 )).
We can now de	ne L  L as L  L i  (9f : f is an orderisomorphism and (8t 2 L) t  f (t))).
Observation 1  is an equivalence relation 2 Thus, t  t means that the same physical state occurs at moments t and t .
Thus, states are the equivalence classes of  on moments.
L  L means that the moments in L and L represent the same states occurring in the same temporal order.
In other words, L and L represent the same trajectory in state-space.
For a model to represent a physical system and be speci	able by a transition relation among di erent states, the corresponding set of scenarios, S, must satisfy the following closure properties Emerson, 1990].
We generalize these from discrete time.
Sux closure: If S 2 S, then all suxes of S belong to S. 	 Limit closure: If for a set of states T = ft0 .
.
.
t .
.
.g, scenarios containing t0 .
.
.
t , for n  0 are in S, then a scenario S such that T  S is also in S. 	 Fusion closure: If S0 = S0  t  S0 and S1 = S1  t  S1 in S include the same state t, then the scenarios S0  t  S1 and S1  t  S0 formed by concatenating the initial and later parts of S0 and S1 also belong to S ( indicates concatenation).
Lemma 2 By construction, S derived from our models satis	es sux and limit closures.
2 0  0  0  0  0  0  0  0  0  0  0  0  n  n  p  p  f  p  z  f  f  p  a  ...  }|  ...  f  {      t0    0  0  t  x  x  0  t0  4 Results on Time and Actions  It is helpful in intuitively understanding formal de	nitions to attempt to prove some technical results that should follow from them.
For this reason, we state and discuss some consequences of the above model and semantic de	nitions next.
We believe constraint C1 is what McDermott intends by requiring that actions do not overlap.
But, that also eliminates C2, which is essential, e.g., so that Fp can be concluded at all moments which precede p (Observation 6).
Constraints C3 and C4 are required for Observation 6 and related results about G and U.
We also use the fact that x:a]:p means that a is performed and p holds throughout a.
Observation 4 (xhaip)!
Fp 2  Observation 5 (xhaiFp)!
Fp 2 Observation 6 Fp!
p _ (W a : xhaiFp) 2 Observation 8 Gp!
(W a : x:a]:Gp) 2  XXXX  a  t  0  0  Observation 7 Gp!
p 2  XXXXX XXXX6X 6 XXXX6 XXX  t  Weak Determinism: If two moments satisfy exactly the same atomic propositions, then the fragments of the model rooted at those moments must be isomorphic with respect to the temporal precedence relation and the atomic propositions in the formal language.
Thus, we can de	ne weak determinism as the following constraint.
(8x 2 A a 2 B t t  t0 2 T S0 2 S : t  t ) (S0  t t0] 2  a] ) (9S1 2 S  t1 : S1 t  t1 ] 2  a] and S0  t t0]  S1  t  t1]))) Lemma 3 Under weak determinism, S derived from our models satis	es fusion closure.
2 C6.
?z .
.
.
}| ?
.
.
.
{ ?t1 XXXXX XXXXX XXXX XXXXX XX Figure 8: Weak Determinism  However, fusion closure is not satis	ed in general.
We show next how to satisfy it.
Observation 9 (p ^ x:a]:Gp)!
Gp 2 Observation 10 (p ^ q)!
pUq 2 Observation 11 (p ^ x:a]:(pUq))!
pUq 2 Observation 12 (p ^ xjhaij(pUq))!
pUq 2 Observation 13 pUq!
((p ^ q)_W W (p ^ ( a : x:a]:(pUq))) _ (p ^ ( a : xjhaij(pUq)))) 2  Observation 14 In discrete models with unit length actions, xhaip  x:a]:p and xhaip  xjhaijp.
Thus one action operator suces in such models.
2  5 Conclusions and Open Problems  Actions and time are crucial to several subareas of AI.
We sought to generalize the formalization of actions, so that several important properties are not excluded.
These include the actions being of di erent durations, the actions being performed concurrently by di erent agents.
the underlying notion of time being variously continuous or discrete, and the underlying notion of time allowing branching into the future.
We stated various coherence constraints that capture the intuitive properties of actions in di erent cases of interest.
Or model can thus serve as an underpinning for further research on notions such as intentions and know-how.
Previous research on these concepts has been shackled by poor models of time and action, thereby leading to spurious results Singh, 1992].
The logic CTL* was designed over a decade ago for reasoning about programs.
Usually, its models are discrete with unit length actions performed one at a time.
We extended CTL* with new operators and gave our language a more general semantics that allows time to be discrete, dense, or continuous.
One of our concerns was that our de	nitions specialize to each case properly.
This is useful since AI models must often function at multiple levels of abstraction.
We also discovered that several constraints must be stated on models to capture the AI notion of basic actions.
The sole traditional constraint of no overlap McDermott, 1982] says too little and sometimes is too strong.
Even though several decision procedures are known for CTL*, no closed-form axiomatization is still known.
This is an important open problem, as is determining an axiomatization for our language, L. Further research is required to determine the role of past time operators for AI purposes.
Such operators are known to make formulae drastically compact in some cases, but they also raise the complexity of the required decision procedures signi	cantly.
Would it help for AI to augment L with operators such as since ?
For models with exclusively unit length actions, one action operator is enough (instead of three).
Are there other interesting classes of models for which L can be simpli	ed?
We have focused here on representational issues.
We have not explored the tradeo s between expressiveness and computational complexity.
Clearly, eciency can be gained by simplifying the formal language and model.
One class of reasoning techniques that is likely to prove of much value is the one developed in the qualitative reasoning community, which routinely deals with continuous phenomena and looks for ways to express them in terms of signi	cant transitions Kuipers, 1986 Sandewall, 1989].
Model checkers (programs which check whether a given model satis	es a given formula) have drawn much attention lately Burch et al., 1990].
One such  can fairly easily be constructed for L by generalizing the ones known for CTL*.
The recursive characterizations of the temporal operators in terms of actions go a long way in designing this.
Instead of points in discrete models, we have to maintain periods in our model.
Clearly, if a model is 	nitely speci	able in terms of periods, we can compute on it in 	nite time using standard techniques.
However, ecient, specialized data structures for periods would be essential in practice.
References  Allen, 1984] Allen, James F. 1984.
Towards a general theory of action and time.
Articial Intelligence 23(2):123{154.
Bacchus et al., 1989] Bacchus, Fahiem Tenenberg, Josh and Koomen, Johannes A. 1989.
A nonrei	ed temporal logic.
In First Conference on Knowledge Representation and Reasoning.
2{10.
Burch et al., 1990] Burch, J. R. Clarke, E. C. McMillan, K. L. Dill, D. L. and Hwang, L. J. 1990.
Symbolic model checking: 1020 states and beyond.
In LICS.
Dean & Boddy, 1988] Dean, Thomas and Boddy, Mark 1988.
Reasoning about partially ordered events.
Articial Intelligence 36:375{399.
Emerson, 1990] Emerson, E. A. 1990.
Temporal and modal logic.
In Leeuwen, J.van, editor, Handbook of Theoretical Computer Science, volume B. North-Holland Publishing Company, Amsterdam, The Netherlands.
Galton, 1990] Galton, Antony 1990.
A critical examination of Allen's theory of action and time.
Articial Intelligence 42:159{188.
Haddawy, 1990] Haddawy, Peter 1990.
Time, chance, and action.
In Sixth Conference on Uncertainty in AI.
Harper et al., 1981] Harper, William L. Stalnaker, Robert and Pearce, Glenn, editors.
IFS: Conditionals, Belief, Decision, Chance, and Time.
D. Reidel, Dordrecht, Netherlands.
Kuipers, 1986] Kuipers, Benjamin J. 1986.
Qualitative simulation.
Articial Intelligence 29:289{338.
McDermott, 1982] McDermott, Drew 1982.
A temporal logic for reasoning about processes and plans.
Cognitive Science 6(2):101{155.
Sandewall, 1989] Sandewall, Erik 1989.
Combining logic and di erential equations for describing realworld systems.
In Principles of Knowledge Representation and Reasoning.
Shoham, 1988] Shoham, Yoav 1988.
Reasoning About Change: Time and Causation from the Standpoint of AI.
MIT Press, Cambridge, MA.
Singh, 1992] Singh, Munindar P. 1992.
A critical examination of the Cohen-Levesque theory of intentions.
In 10th European Conference on Articial Intelligence.
Singh, 1994] Singh, Munindar P. 1994.
Multiagent Systems: A Theoretical Framework for Intentions, Know-How, and Communications.
Springer Ver-  lag, Heidelberg, Germany.
Thomason & Gupta, 1981] Thomason, Richmond H. and Gupta, Anil 1981.
A theory of conditionals in the context of branching time.
In Harper et al., 1981].
299{322. van Frassen, 1981] van Frassen, Bas C. 1981.
A temporal framework for conditionals and chance.
In Harper et al., 1981].
323{340.
Web Services for Time Granularity Reasoning  Claudio Bettini Data, Knowledge and Web Engineering Laboratory University of Milan, Italy  TIME-ICTL 03 Invited Talk  (1/38)  Overview  * Time granularities * Current approaches to time granularities * Time granularity web services * GSTP: the Granularity Simple Temporal Problem * The GSTP web service  (2/38)  Time granularities  (3/38)  What is a time granularity?
* A granularity can be viewed as the partitioning of a temporal domain in groups of elements, where each group is perceived as an indivisible unit (a granule).
(4/38)  What is a time granularity?
* A granularity can be viewed as the partitioning of a temporal domain in groups of elements, where each group is perceived as an indivisible unit (a granule).
* The set of granules can be used for temporal qualification of statements (e.g., It has been raining for two weeks or Revenues in July 2003 are $500.000 ) and for specifying temporal relationships (e.g., The merchandise should reach its destination in 3 business days from the date of its order.)
(4/38)  What is a time granularity?
* A granularity can be viewed as the partitioning of a temporal domain in groups of elements, where each group is perceived as an indivisible unit (a granule).
* The set of granules can be used for temporal qualification of statements (e.g., It has been raining for two weeks or Revenues in July 2003 are $500.000 ) and for specifying temporal relationships (e.g., The merchandise should reach its destination in 3 business days from the date of its order.)
* It is a powerful abstraction tool  (4/38)  Which time granularities should we model?
Not only the usual ones: hour, day, week, month, year, .
.
.
(5/38)  Which time granularities should we model?
Not only the usual ones: hour, day, week, month, year, .
.
.
* From Ameritrade policies: GTC orders are only open for the traditional trading session each trading day.
(5/38)  Which time granularities should we model?
Not only the usual ones: hour, day, week, month, year, .
.
.
* From Ameritrade policies: GTC orders are only open for the traditional trading session each trading day.
* From UC Berkeley: Students wanting to do a part-time internship during an academic semester must be enrolled at UC Berkeley for the semester of their internship  (5/38)  Which time granularities should we model?
Not only the usual ones: hour, day, week, month, year, .
.
.
* From Ameritrade policies: GTC orders are only open for the traditional trading session each trading day.
* From UC Berkeley: Students wanting to do a part-time internship during an academic semester must be enrolled at UC Berkeley for the semester of their internship * From Morgan Stanley Mutual Fund Prospects: By business day we mean any day in which Banks are open in Luxemburg, New York and Tokyo depending on the specific case.
(5/38)  Time granularities can have complex definitions  Trading, banking and business days are examples of granularities that may depend on local holidays (e.g., California Admission Day).
(6/38)  Time granularities can have complex definitions  Trading, banking and business days are examples of granularities that may depend on local holidays (e.g., California Admission Day).
From NYSE web site: "If an observable holiday occurs on a Sunday it is observed on Monday, If on Saturday, it is observed on Friday.
The exception to this rule is New Years.
If it falls on a Saturday, the Market will be open on Friday, as the NYSE is ALWAYS open on the last trading day of the year."
(6/38)  Time granularity research issues  * Modeling time granularities * Granule conversions * Conversion of information in terms of different granularities * Time granularity constraint reasoning  (7/38)  Current approaches to time granularities  (8/38)  Current approaches to time granularities  * The multilayered logic approach.
Goal: a full-fledged logic to reason about multi-granularity temporally qualified statements.
(9/38)  Current approaches to time granularities  * The multilayered logic approach.
Goal: a full-fledged logic to reason about multi-granularity temporally qualified statements.
* The set-theoretic approach.
Goal: a mathematical model for arbitrary granularities and an associated algebra to manipulate them, plus a set of domain-specific reasoning tools.
(9/38)  The multilayered logic approach * Extensions of topological temporal logic * Temporal universe consisting of a (possibly infinite) set of inter-related differently-grained temporal domains * Logical tools are provided to qualify temporal statements with respect to the temporal universe and to switch temporal statements across temporal domains.
* Main application: real time systems specification and verification  (10/38)  The multilayered logic approach * Extensions of topological temporal logic * Temporal universe consisting of a (possibly infinite) set of inter-related differently-grained temporal domains * Logical tools are provided to qualify temporal statements with respect to the temporal universe and to switch temporal statements across temporal domains.
* Main application: real time systems specification and verification Angelo Montanari.
Metric and layered temporal logic for time granularity.
PhD thesis, University of Amsterdam.
ILLC DIssertation Series 1996-02.
(10/38)  Time granularities in the set-theoretic approach  A granularity is a mapping G from the positive integers to 2T (i.e., all subsets of a lineraly ordered temporal domain) such that for all positive integers i and j with i < j, the following two conditions are satisfied: * G(i) 6= [?]
and G(j) 6= [?]
imply that each element in G(i) precedes all elements in G(j), and * G(i) = [?]
implies G(j) = [?].
(11/38)  Examples of time granularities  day business-day business-week business-month  Example of granules: day(1) = 0001/01/01 day(731405) = 2003/07/09  (12/38)  The set theoretic approach  * Many granularity relationships formally defined (groups-into, finer-than, periodically groups-into, sub-granularity, shifting equivalent, .
.
. )
* Different time granularity systems have been investigated identifying systems with nice properties (e.g., lattices).
(13/38)  The set theoretic approach  * Many granularity relationships formally defined (groups-into, finer-than, periodically groups-into, sub-granularity, shifting equivalent, .
.
. )
* Different time granularity systems have been investigated identifying systems with nice properties (e.g., lattices).
C. Bettini, S. Jajodia, and X. Wang.
Time Granularities in Databases, Temporal Reasoning, and Data Mining.
Springer, 2000  (13/38)  System representations of time granularities  Assume hour is the bottom granularity with hour(1) mapped to the instants corresponding to 2001/1/1:01, then: * monday can be represented in terms of hour by: Period P = 168; Description of one of the periods: {[1, 24]} Bounds: none.
(14/38)  System representations of time granularities  Assume hour is the bottom granularity with hour(1) mapped to the instants corresponding to 2001/1/1:01, then: * monday can be represented in terms of hour by: Period P = 168; Description of one of the periods: {[1, 24]} Bounds: none.
* business-day-until-2003 can be represented by: Period P = 168; Description of one of the periods: {[1,24][25,48][49,72][73,96][97,120]} Bounds: Up=775 (number of business days from 2001 to 2003).
(14/38)  Primitive granule conversion operations  If day(1) is January 1st 2001, then: * d33emonth day = 2 since Feb. 2nd 2001, represented in the system as day(33), is contained in Feb. 2001, the second month, represented in the system as month(2).
(15/38)  Primitive granule conversion operations  If day(1) is January 1st 2001, then: * d33emonth day = 2 since Feb. 2nd 2001, represented in the system as day(33), is contained in Feb. 2001, the second month, represented in the system as month(2).
* b2cmonth day = {[32, 59]} since February 2001 contains the 28 days indexed from 32 to 59 in the granularity system.
(15/38)  Primitive granule conversion operations  If day(1) is January 1st 2001, then: * d33emonth day = 2 since Feb. 2nd 2001, represented in the system as day(33), is contained in Feb. 2001, the second month, represented in the system as month(2).
* b2cmonth day = {[32, 59]} since February 2001 contains the 28 days indexed from 32 to 59 in the granularity system.
* b2cb-month = {[32, 33][36, 40][43, 47][50, 54][57, 59]} since the day second business month includes only the days of February 2001 which are not Saturday nor Sunday.
(15/38)  Investigated applications * Databases: temporal query processing and temporal database interoperability, temporal database design, multiple granularity integrity constraint checking; * Data mining: discovering frequent temporal patterns, discovering temporal relationships; * Artificial Intelligence: multi-granularity constraint processing, scheduling in inter-organizational workflows.
The set-theoretic model and its basic services were a common basis in the proposed solutions (but different techniques were applied).
(16/38)  Time granularity web services  (17/38)  Web services  * A technology to enable distributed Web applications * Based on: HTTP, SOAP, WSDL, UDDI * Language and platform independent  (18/38)  Why time granularity web services?
* Managing distributed repositories of XML time granularity specifications * Offering processing services to web applications.
For example: - Specification of new granularities (using common algebra operators) - Search for equivalent specifications (name clash problem) - Conversion of granules - Constraint processing - ...  (19/38)  Which applications  Essentially all applications requiring multi-granularity time-management.
Focus on some of them: * Inter-organizational workflows * Personal (or group) Information Management (e.g., different views in Outlook-like apps, appointment scheduling) * Medical applications (e.g., monitoring) * ...  (20/38)  Granularity specification web services  Screenshot of a client-server application developed at GMU.
(21/38)  Granule conversion web services Find the month that contains the 27th week in 2003.
Screenshot of a client-server application developed at GMU.
(22/38)  Granule conversion web services (2) Find all the Wednesdays in July 2003.
P. Ning, X. Wang, S. Jajodia.
An Algebraic Representation of Calendars.
Annals of Mathematics and Artificial Intelligence 36(1-2): 5-38, 2002.
(23/38)  GSTP: the Granularity Simple Temporal Problem (The theory underlying a constraint processing web service)  (24/38)  Temporal constraint networks X1 [-1,1]  [2,2]  X0  X3  [1,6]  [0,3] X2  A Simple Temporal Problem (STP).
(25/38)  Temporal constraint networks X1 [-1,1]  [2,2]  X0  X3  [1,6]  [0,3] X2  A Simple Temporal Problem (STP).
Main tasks: consistency, constraint refinement, solution  (25/38)  Temporal constraint networks with granularities  X1 [-1,1]b-day  [2,2]b-week  X0  X3 [0,3]day  [1,6]b-day X2  Variables take values in Z+ .
(x0 , x1 ) satisfies [-1, 1]bday iff (1) dx0 ebday and dx1 ebday are both defined, and (2) -1 <= (dx1 ebday - dx0 ebday ) <= 1  (26/38)  The intuitive approach  Convert the network constraints in terms of a single granularity, and apply known (polynomial time) algorithms for STP.
(27/38)  The intuitive approach  Convert the network constraints in terms of a single granularity, and apply known (polynomial time) algorithms for STP.
Unfortunately, there is no straightforward reduction.
Consistency is NP-hard in terms of the involved granularities [Bettini et al.
TIME96 and AMAI98].
(27/38)  An approximate algorithm: Conversion+PC  * Compute the tightest implied constraints in terms of each granularity, generating a set of networks * Process each network with known algorithms * Rejoin the networks and repeat from step 1 until a fixpoint is reached  (28/38)  The conversion problem [1, 1] bday - [?, ?]
day  [1, 1] bday - [?, ?]
hour  (29/38)  The conversion problem [1, 1] bday - [?, ?]
day  [1, 1] bday - [?, ?]
hour  Why not [1, 1] day?
What about one event on Friday and the other on Monday?
(29/38)  The conversion problem [1, 1] bday - [?, ?]
day  [1, 1] bday - [?, ?]
hour  Why not [1, 1] day?
What about one event on Friday and the other on Monday?
Why not [1, 100] hour?
It is implied but a tighter implied constraint exists: [1, 95] hour.
(29/38)  The conversion problem (2) [1, 1] bday  -  [1, 95] hour  Can [1, 95] hour substitute [1, 1] bday?
(2003/7/9:14,2003/7/12:14) satisfies [1, 95] hour but violates [1, 1] bday  (30/38)  The conversion problem (2) [1, 1] bday  -  [1, 95] hour  Can [1, 95] hour substitute [1, 1] bday?
(2003/7/9:14,2003/7/12:14) satisfies [1, 95] hour but violates [1, 1] bday  Converting constraints is tricky!
New algorithms presented in [Bettini, Ruffini AAAI-WS02 and JUCS].
(30/38)  Why incomplete?
Consider this example with only "standard" granularities: [0,0]year [11,11]month  X1  X0  X3  [1,1]month X2  [30,30]day [0,0]month  (31/38)  Why incomplete?
Consider this example with only "standard" granularities: [0,0]year [11,11]month  X1  X0  X3  [1,1]month X2  [30,30]day [0,0]month  Intuition: We have to take into account the domain of involved variables  (31/38)  A minimal network in terms of month  (32/38)  A minimal network in terms of day  (33/38)  A sound and complete algorithm: AC-G Q := {(Xi , Xj ) | (Xi , Xj ) [?]
A} while Q 6= [?]
do 1. select and delete an arc (Xl , Xk ) from Q 2. if Dom(Xl ) 6=M AX Dom(Xl ) [?]
(Dom(Xk ) ] G(Xk , Xl )) then 2.1.
Q := Q [?]
{(Xi , Xl ) | (Xi , Xl ) [?]
A, i 6= k} 2.2.
Dom(Xl ) := Dom(Xl ) [?]
(Dom(Xk ) ] G(Xk , Xl )) 3. if Dom(Xl ) =M AX [?]
then Q := [?]
; Dom(Xl ) := [?]
end while  [Bettini et al.
CP97 and AIJ02].
Key theorem: a solution exists iff there is one with all values lower than a network-dependent constant M AX.
(34/38)  The GSTP algorithm Repeat 1.
Conversion+PC 2.
AC-G 3.
RefineEdgesFromNodes() Until no change is observed Return Inconsistent or NewNetwork+solution  (35/38)  The GSTP web service * Definition of an XML schema for constraint networks ... <xsd:complexType name = "ArcType"> <xsd:sequence> <xsd:element name="constraint" type="ConstrType" minOccurs="1"/> </xsd:sequence> <xsd:attribute name="sourceNodeID" use="required" type= .../> <xsd:attribute name="targetNodeID" use="required" type= .../> </xsd:complexType> ...  * WSDL service description made available to external applications.
(Services: consistency, refinement, solution, ...)  (36/38)  The GSTP web service architecture  (37/38)  References * C.Bettini, S.Mascetti, V.Pupillo, GSTP: A Temporal Reasoning System Supporting Multi-Granularity Temporal Constraints, In Proc.
IJCAI 2003 (Intelligent System Demonstration), Acapulco, Mexico.
* C. Bettini, X. Wang, S. Jajodia, Solving Multi-Granularity Constraint Networks, Artificial Intelligence, 140(1-2):107-152, 2002.
* C. Bettini, S. Jajodia, X. Wang, Time Granularities in Databases, Temporal Reasoning, and Data Mining.
Springer, 2000.
Thank you for your attention http://webmind.dico.unimi.it/gstp (38/38)
Proceedings of TIME-96  1  Gaining Efficiency and Flexibility in the Simple Temporal Problem Amedeo Cesta  Angelo Oddi  IP-CNR National Research Council of Italy Viale Marx 15, I-00137 Rome, Italy amedeo@pscs2.irmkant.rm.cnr.it  Dipartimento di Informatica e Sistemistica Universita di Roma "La Sapienza" Via Salaria 113, I-00198 Rome, Italy oddi@assi.dis.uniroma1.it  Abstract The paper deals with the problem of managing quantitative temporal networks without disjunctive constraints.
The problem is known as Simple Temporal Problem.
Dynamic management algorithms are considered to be coupled with incremental constraint posting approaches for planning and scheduling.
A basic algorithm for incremental propagation of a new time constraint is presented that is a modification of the Bellman-Ford algorithm for Single Source Shortest Path Problem.
For this algorithm a sufficient condition for inconsistency is given based on cycle detection in the shortest paths graph.
Moreover, the problem of constraint retraction from a consistent situation is considered and properties for repropagating the network locally are exploited.
Some experiments are also presented that show the usefulness of the properties.
1  Introduction  Knowledge-based architectures for planning and scheduling based on constraint propagation, e.g.
[5, 3, 8, 2], perform incremental constraint posting and retraction on a current partial solution.
A complete plan is created by efficiently searching in partial plans space, and, in other cases, it is adapted to new situations by partially removing parts of the solution.
A module for temporal constraint management that supports plan space search and current solution maintenance should be extremely efficient because is called into play at any modification (monotonic or not) of the current plan.
Such an efficiency is usually guaranteed by restricting the expressive power of the temporal representation.
Usually the so called Simple Temporal Problem (STP) [7] is used that allows the representation of binary quantitative constraints without disjunction.
In spite of the restriction of expressivity, also for STP it results useful to consider how the efficiency of manipulation primitives may be improved.
In our research, we have been investigating possible  algorithms for managing temporal information that: (a) allow dynamic changes of the constraint set for both incremental constraint posting and retraction; (b) exploit the localization of effects of any change in a subnetwork of the whole constraint graph; (c) do not compute the minimal network as done in [7] but just check for consistency.
A previous paper [1], in the same line of [6], has concerned the specialization of arc-consistency algorithm to the STP.
The choice of arc-consistency to propagate temporal constraints was motivated by the good trade-off wrt space and time complexity.
In the same paper some properties were given that were shown experimentally to improve the performance of the algorithm in the average case.
The present paper contains a further step in the direction of gaining efficiency in the solution of the STP.
After presenting the essentials of STP (Section 2), it presents dynamic algorithms based on the well known Bellman-Ford algorithm for computing Single Source Shortest Paths (Section 3).
It also introduces (Section 4) the concept of dependency that computes a particular spanning tree on the constraint graphs that allows the definitions of a sufficient condition for inconsistency detection (Section 5) and an algorithm for local constraint retraction (Section 6).
Some experiments (Section 7) show the usefulness of the properties.
2  The Temporal Problem  A Simple Temporal Problem is defined in [7] and involves a set of temporal variables {X1 , .
.
.
, Xn }, having continuous domains [lbi , ubi ] and a set of constraints {aij <= Xj - Xi <= bij }, where aij >= 0, bij >= 0 and aij <= bij .
A special variable X0 is added to represent the origin of the time (the beginning of the considered temporal horizon) and its domain is fixed to [0, 0].
A solution of the STP is a tuple (xi .
.
.
xn ) such that xi [?]
[lbi , ubi ] and every constraint aij <= Xj - Xi <= bij is satisfied.
An STP is inconsis-  Proceedings of TIME-96 tent if no solution exists.
In order to find the set of possible values [lbi , ubi ] for every variable Xi , a direct constraint graph Gd (Vd , Ed ) is associated to the STP, where the set of nodes Vd represents the set of variables {X1 , .
.
.
, Xn } and the set of edges Ed represents the set of constraints {aij <= Xj - Xi <= bij }.
Given a constraint aij <= Xj -Xi <= bij , we can rewrite it as a pair of inequalities: * Xj - Xi <= bij * Xi - Xj <= -aij For every linear inequality Xj - Xi <= wij (with wij equal to bij or -aij ) we have an edge (i, j) in Gd (Vd , Ed ) labeled with the weight wij .
Each path in Gd from the node i to the node j, i = i0 , i1 .
.
.
im = j induces between the variables Xj and Xi the constraint Xj - Xi <= lij , where lij is the sum of weights along the path, that is lij = w01 +w12 +.
.
.+w(m-1)m .
Considering the set of all paths between the nodes i and j, these paths induce a constraint Xj - Xi <= dij , where dij is the length of a shortest path between the nodes i and j.
Finally a cycle on the graph Gd is closed path i = i0 , i1 .
.
.
im = i and a negative cycle is a cycle with associated a negative length (lii < 0).
In [7] some useful properties of an STP are given and reported in the following theorems.
Theorem 1 [7] A Simple Temporal Problem is consistent iff Gd does not have negative cycles.
Defining d0i as the length of a shortest path on the graph Gd from the origin 0 and the node i and di0 as the length of a shortest path from the node i to the origin 0 we can also have the other following theorem.
Theorem 2 [7] Given a consistent Simple Temporal Problem, the set [lbi , ubi ] of feasible values for the variable Xi is the interval [-di0 , d0i ].
Theorem 2 shows that the Simple Temporal Problem is a Shortest Paths Problem and precisely we have to calculate two sets of shortest paths length: (a) the set of shortest paths from the node 0 (that represent the variable X0 ) to the nodes 1 .
.
.
n; (b) and the set of shortest paths from the nodes 1 .
.
.
n to node 0.
3  An Algorithm for the STP  To solve the basic STP we use the Bellman-Ford algorithm for the Single Source Shortest Paths Problem [4] giving an incremental version of the algorithm named Propagation, which accepts as an input the graph Gd and a new constraint Cij (where Cij = aij <= Xj - Xi <= bij ) and produces in output a new set of feasible values [-di0 , d0i ] for every variable Xi or a value fail in the case the new constraint induces a inconsistent situation.
To understand the algorithm, shown in Figure 1, some  2 simple definitions are useful: given a node i of the graph Gd we define EdgesOut(i) as the set of edges which leave from the node i and EdgesIn(i) as the set of edges which arrive to the node i. T and F are the boolean constants T rue and F alse.
The algorithm has two differences wrt the standard implementation on Bellman-Ford with a queue.
First, it calculates at the same time two sets of shortest distances.
Second, the algorithm has an internal test which detects negative cycles on the graph Gd which contain the reference node X0 .
In addition, every node u [?]
Vd has two boolean marks: LB(u) and U B(u).
This marks are useful in order to distinguish the two types of propagation in the graph Gd , that is, respectively U B(u) = T and LB(u) = T when a node is modified by the propagation process for the distance d0i and the distances di0 .
The Propagation calculates the set of distances {d0i } between Steps 6 and 14 and the set of distances {di0 } between Steps 16 and 24.
This last section of the algorithm, in order to calculates the set of distances di0 , (that is, the length of the shortest paths on the graph Gd between the nodes 1 .
.
.
n and the node 0) considers the set of direct edges in Gd as oriented in the opposite direction.
In this way when a shortest path between the nodes 0 and i is found, it is actually a shortest path in the opposite direction.
Finally, the tests at Steps 10 and 20 check for negative cycles in the graph Gd when they contain the node 0.
The algorithm calculates also two shortest path trees.
In fact Steps 11 and 21 respectively update the predecessor function pu, which represents the shortest path tree of the distances {d0i } and the predecessor function pl, which represents the shortest path tree of the distances {di0 }.
The complexity of the algorithm, as well known, is O(EN ).
Where N and E are respectively the number of nodes and the number of edges in Gd .
negative cycles  4  Focusing on Dependency  The temporal meaning of shortest path trees on the Gd graph is simple.
Every bound {d0i } (or {di0 }) is induced by the set of temporal constraints in the shortest paths between the origin 0 and the node i (or between the node i the origin 0).
The following definitions are useful: Definition 1 Let Gd a consistent distance graph.
The tree DTub of the shortest paths from the origin 0 to the nodes 1 .
.
.
n is called Upper Bounds' Dependency Tree.
Definition 2 Let Gd a consistent distance graph.
The tree DTlb of the shortest paths from to the nodes  Proceedings of TIME-96 Propagation (Gd , Cij ) 1. begin 2.
Q - {i, j} 2a.
LB(i) ::= T ; U B(i) ::= T 2b.
LB(j) ::= T ; U B(j) ::= T 3.
While Q 6= [?]
do begin 4. u - P op(Q) 5. if U B(u) then 6.
Foreach (u, v) [?]
EdgesOut(u) do 7. if d0u + wuv < d0v 8. then begin 9. d0v ::= d0u + wuv 10. if d0v + dv0 < 0 then exit(fail) 11. pu(v) ::= u 12.
U B(v) ::= T 13. if v 6[?]
Q then Q - Q [?]
{v} 14. end 15. if LB(u) then 16.
Foreach (u, v) [?]
EdgesIn(u) do 17. if du0 + wvu < dv0 18. then begin 19. dv0 ::= du0 + wvu 20. if d0v + dv0 < 0 then exit(fail) 21. pl(v) ::= u 22.
LB(v) ::= T 23. if v 6[?]
Q then Q - Q [?]
{v} 24. end 25.
LB(u) ::= F 26.
U B(u) ::= F 27. end 28. end Figure 1: Propagation algorithm  1 .
.
.
n to origin 0 is called Lower Bounds' Dependency Tree.
If a given graph Gd is consistent then the trees DTub and DTlb are always defined.
In fact, without negative cycles, the distances {d0i } and {di0 } are always defined.
In general, the trees DTub and DTlb may not be single.
In fact, the graph Gd may contain several paths with the same length.
A relevant situation is verified when the graph Gd contains at least a negative cycle.
In this case, the following Theorem holds.
Theorem 3 Give a distance graph Gd .
If during the update process of the Propagation algorithm the predecessor function pu (pl) represents a graph containing at least a cycle then the graph Gd is inconsistent.
3 Proof.
We give the proof for the distances {d0i }, but an analogous proof can be given for the distances {di0 }.
Suppose by hypothesis that during the update process of the algorithm, a dependency path exists between the nodes i and j named p1 : i = i0 , i1 .
.
.
ir = j, that is, a path such that pu(ik ) = ik-1 , with k = 1 .
.
.
r. If we sum the weights along this path, we have the following relation: d0j - d0i = w01 + w12 + .
.
.
+ w(r-1)r .
(1)  If successively the Propagation algorithm builds a dependency path p2 : j = j0 , j1 .
.
.
js = j, we can write the following relation: dnew - d0j = w01 + w12 + .
.
.
+ w(s-1)s .
0i  (2)  Where dnew is the new value of the distances d0i 0i updated along the path p2 .
If we sum the relations 1 and 2 we obtain the length of the cycle lii : lii = dnew - d0i .
0i  (3)  Observing that the link of two paths p1 and p2 is a cycle and dnew < d0i , then the length lii is negative 0i and this proves the inconsistency of the graph Gd .
2  5  Cycle Detection  In order to use the property expressed by Theorem 3 few changes are introduced in the Propagation algorithm.
Each edge (i, j) in the graph Gd have three new boolean marks: N EW ((i, j)), LBP ((i, j)) and U BP ((i, j)).
The mark N EW is useful in order to distinguish the new edges introduced in Gd , by the new temporal constraint Cij .
In fact, if in the graph there is at least a negative cycle, then it must contain at least one of the new edges introduced.
Instead, the two marks LBP ((i, j)) and U BP ((i, j)) are used to check when a bound changes two times as explained in the next Theorem 4: Theorem 4 Let Gd a consistent distance graph and Cij = aij <= Xj - Xi <= bij the new constraint added.
If during the propagation process the distance d0j (di0 ) changes two times, then the constraint Cij is inconsistent with the other constraints represented in Gd .
Proof.
We give the proof for the distances {d0j }, but an analogous proof can be given for the distances {dj0 }.
If the constraint represented by the edge (i, j) changes the distance d0j a first time, this means every new shortest paths built by the Propagation algorithm will contain the node j.
If the distances is changed a second time, then the algorithm has built a closed dependency path and for the Theorem 3 the graph Gd is inconsistent.
2  Proceedings of TIME-96 Figure 2 shows the modified version of the algorithm to check for cycle detection.
It is interesting to notice the complexity of the algorithm with cycles detection is the same of the Propagation algorithm.
In fact, the only difference with the previous algorithm is the check of the boolean marks N EW ((i, j)) LBP ((i, j)) and U BP ((i, j)).
Propagation-cd (Gd , Cij ) 1.
- 9. as in the Propagation algorithm 10a.
if d0v + dv0 < 0 10b.
then exit(fail) 10c.
else if N EW ((u, v)) 10d.
then if U BP ((u, v)) 10e.
then exit(fail) 10f.
else U BP ((u, v)) ::= T 11.
- 19. as in the Propagation algorithm 20a.
if d0v + dv0 < 0 20b.
then exit(fail) 20c.
else if N EW ((u, v)) 20d.
then if LBP ((u, v)) 20e.
then exit(fail) 20f.
else LBP ((u, v)) ::= T 24.
- 28. as in the Propagation algorithm  Figure 2: Differences introduced by cycle detection the average time  6  Retraction of Temporal Constraints from a Consistent Context  This paragraph deals with the problem of removing temporal constraints from a consistent graph Gd (a graph without negative cycles).
A basic way to do this consists of: physically removing the constraint from the graph Gd ; setting every distance {d0i } and {di0 } to the value +[?
]; finally, running the Propagation algorithm on the whole graph.
As a matter of fact, this method is not very efficient.
In fact, when retracting a constraint from the time map a lot of distances are likely not to be affected by the removal.
The dependency information may be used to focalize the part of the network actually affected by the removal and to run the Propagation algorithm on that part of the graph.
To state same properties some definitions are useful.
Given an upper bounds' dependency tree DTub (VDTub , EDTub ), each sub-tree STub [i](VSTub , ESTub ) of root i [?]
VDTub is called an Upper Bounds' Dependency Sub-tree.
Given a lower bounds' dependency tree DTlb (VDTlb , EDTlb )  4 every sub-tree STlb [i](VSTlb , ESTlb ) of root i [?]
VDTlb is called a Lower Bounds' Dependency Sub-tree.
Given a a distance graph Gd (VGd , EGd ) and a node i [?]
VGd , IN (i) is the set of start nodes of the edges which enter in the node i (in the edge (j, i), j is the start node and i is the end node).
The next Proposition explains the real effects of a removal constraints from a graph Gd and it is a starting point to write a new algorithm to remove temporal constraints from Gd .
Proposition 1 Let Gd be a consistent graph and DTub (VDTub , EDTub ) its upper bounds' dependency tree (DTlb (VDTlb , EDTlb ) its lower bounds' dependency tree).
The retraction of an edge (i, j) [?]
EDTub ( (i, j) [?]
EDTlb ) modifies at most the distances of the nodes k [?]
VSTub [j] ( k [?]
VSTlb [j] ).
No distances are modified when (i, j) 6[?]
EDTub ( (i, j) 6[?]
EDTlb ).
Proof.
We give the proof for the distances {d0i }, but an analogous proof can be given for the distances {di0 }.
The removal of an edge (i, j) [?]
EDTub can't modify a node's distance {d0k } in the case k 6[?]
VSTub [k].
In fact the removal of (i, j) does not change the shortest path between the origin 0 and the node k. If (i, j) 6[?]
EDTub then no distance is changed because no shortest path is changed.
2 The basic idea to write an efficient removal algorithm is run the Propagation algorithm on the only part of the Gd graph affected by the removal of the constraint.
The next Theorem formalize this concept and explains how to initialize the Propagation algorithm.
Theorem 5 Let Gd be a consistent distance graph.
To remove the effects of the constraint represented by the edge (i, j) [?]
EDTub ( (i, j) [?]
EDTlb ) the queue Q of the Propagation algorithm and the set of distances {d0i } ({di0 }) in the graph Gd need of the following initialize operations.
S S 1.
Q - k[?
]VST [j] IN (k) (Q - k[?
]VST [j] IN (k)) ub lb 2. d0u ::= +[?
], u [?]
VSTub [j] (du0 ::= +[?
], u [?]
VSTlb [j]) Proof.
We give the proof for the distances {d0i }, but an analogous proof can be given for the distances {di0 }.
By Proposition 1, for every node k [?]
VSTub [j], the distance {d0k } can change after the removal.
The Propagation algorithm have to rebuild the new shortest paths for every node k [?]
VSTub [j].
In order to update these distances to the new values, it is necessary to initialize them to the maximum possible value +[?].
In fact, it is not known what the new values will be and the Propagation algorithm can only reduce the bounds.
In addition, we have to put in the queue Q all  Proceedings of TIME-96 the nodes of the constraints (i, j) which enter in the set S of updated nodes.
That is, the nodes in the set k[?
]VSTub [i] IN (k).
In fact, these are the only nodes of the graph from which can start the new shortest paths of the nodes k [?]
VSTub [j].
2 The Remove algorithm is shown in Figure 3.
It accepts as an input a graph Gd and a constraint Cij which have to be removed from Gd and return the graph Gd updated.
At the step 13 is used the RePropagation algorithm that is similar to the Propagation algorithm but accepts as an input a list of nodes Q instead of an edge Cij .
The parameter Q is used as an initialization for the internal queue.
Moreover RePropagation does not check for the consistency of a modification because the removal of one or more constraints, relax the STP holding the consistency property.
Remove (Gd , Cij ) 1. begin 2.
Vm - [?]
3.
Q-[?]
4. if (i, j) [?]
EDTub 5. then Vm - Vm [?]
VSTub [j] 6. else if (j, i) [?]
EDTub 7. then Vm - Vm [?]
VSTub [i] 8. if (i, j) [?]
EDTlb 9. then Vm - Vm [?]
VSTlb [i] 7. else if (j, i) [?]
EDTlb 8. then Vm - Vm [?]
VSTlb [j] 9.
Foreach u [?]
Vm do begin 10.
Q - Q [?]
IN (u) 11. end 12.
EGd - EGd - {(i, j), (j, i)} 13.
RePropagation(Gd , Q) 14. end Figure 3: Remove algorithm  7  Performance Evaluation  In order to get some realistic evaluations of the algorithms, we have used a scheduling system described in [3] and the time network generated by the scheduler.
This scheduler solves instances of the Deadline Job Shop Scheduling Problem (DJSSP) by incremental precedence constraint posting between the activities until any conflict in the use of resources is resolved.
In the DJSSP, each activity in a job can request only one resource and a resource is requested only once in a job.
The sequence of resources requested by the activities in a job is random.
Every job has a  5 fixed release date and a due date.
More details on the random problem generator are described in [3].
All the evaluations are given as number of time points explored by the algorithms.
This choice is motivated from the fact that such number is both proportional to the time of computation and machine independent.
We have built two different types of time networks from the resolution of two different DJSSPs: the 8x8x8 (named P 8) and the 10x10x10 (named P 10), where the first number indicate the number of jobs, the second one the number of activities in a job and the third one the number of resources.
The data are obtained running ten instances of each type of problem.
Table 1 shows the number of time points N , the maximum number of distance constraints Emax and maximum connectivity Cmax for each problem.
The connectivity is defined as the ratio between the number of distance constraints E and the number of time points N .
The value N is two times the number of activities plus two (the origin point and horizon point).
The value Emax represents the maximum number of distance constraints which can be contained in a time network associated to the solution of the instance of the DJSSP.
Emax is obtained by the sum of the maximum values of the number of precedence constraints for each resource and the number of constraints before the scheduling algorithm starts to find a solution.
Table 2 and Table 3 present the perfor-  Table 1: Number of time points and maximum connectivity for the experimental time networks Problem P8 P 10  N 130 202  Emax 333 661  Cmax = Emax /N 2.56 3.27  mance of the Propagation algorithm when a modification is either consistent or inconsistent respectively.
This values are shown as a function of the average connectivity Av-conn, that is, every row of the table represents the average value obtained in the interval Av-conn +-0.25.
In order to get several values of the connectivity we have built a solution of an instance of a DJSSP and progressively reduced the number of edges and selected a time constraint Cij in random way.
In order to get the results showed in Table 2, we have modified the distance constraint selected Cij = aij <= Xj - Xi <= bij , in the constraint  Proceedings of TIME-96  6  Cij = aij + (dij - aij )U [0.05, 01] <= Xj - Xi <= bij .
Where U [x, y] represents a random value r with uniform distribution such that x <= r <= y and dij is minimal temporal distance between the nodes i and j on the Gd graph.
In this case, it is possible to make a comparison between the number of nodes scanned by the Propagation algorithm (Loc-prop values) and the number of nodes scanned by an algorithm which works from scratch (Scratch values).
In order to get the results showed in Table 3, we have induced an inconsistent situation by modifying the constraint Cij in the constraint dij (1 + U [0.05, 01]) <= Xj - Xi <= bij In this other case, it is possible make a comparison between the number of nodes visited by the Propagation algorithm which uses the property expresses by Theorem 4 (Cycle-det values) and the number without the previous property (No-cycle-det values).
Table 2: Incremental vs scratch propagation Problem P8  P 10  Av-conn 1.25 1.75 2.25 2.75 1.25 1.75 2.25 2.75 3.25  Loc-prop 38.76 52.33 34.08 39.51 51.42 67.20 64.34 57.00 63.92  Scratch 652.67 1111.47 1641.19 2048.28 1108.38 1928.54 2876.22 3817.79 4388.71  a solution; then we have reduced progressively the number of time constraints by using the Remove algorithm.
In this case, is possible to make a comparison between the average number of nodes scanned by the Remove algorithm (Loc-rem values) and the number of nodes scanned in the same case by a scratch algorithm (Scratch-rem values).
The scratch algorithm eliminates first the constraint from the time map; then puts all the bounds of the time points to the value +[?
]; finally updates all the network.
Table 4: Incremental vs scratch remove Problem P8  P 10  Av-conn 1.25 1.75 2.25 2.75 1.25 1.75 2.25 2.75 3.25  Loc-rem 2.37 35.34 56.02 96.35 2.69 33.12 55.06 70.58 156.97  Scratch-rem 652.67 1111.47 1641.19 2048.28 1108.38 1928.54 2876.22 3817.79 4388.71  Acknowledgments This research is partially supported by: ASI - Italian Space Agency, CNR Special Project on Planning, CNR Committee 04 on Biology and Medicine.
References [1] Cervoni, R., Cesta, A., Oddi, A., Managing Dynamic Temporal Constraint Networks, Proceedings of the Second International Conference on AI Planning Systems (AIPS94), AAAI Press, 1994.
Table 3: Propagation with and without cycle detection Problem P8  P 10  Av-conn 1.25 1.75 2.25 2.75 1.25 1.75 2.25 2.75 3.25  Cycle-det 3.02 2.92 2.47 1.92 3.21 2.78 2.68 2.55 2.63  No-cycle-det 114.42 77.30 43.87 9.75 199.45 133.81 86.85 27.15 14.58  Finally, Table 4 presents the performance of the Remove algorithm.
These results are obtained in the same way as the previous ones.
First we have built  [2] Cesta, A., Oddi, A., DDL.1: A Formal Description of a Constraint Representation Language for Physical Domains, Proceedings of the 3rd European Workshop on Planning (EWSP95), IOS Press, 1996.
[3] Cheng, C. Smith, S.,F., Generating Feasible Schedules under Complex Metric Constraints, Proceedings of the 12th National Conference on Artificial Intelligence (AAAI-94), AAAI Press, 1994.
[4] Cormen, T.H., Leierson, C.E., Rivest, R.L., Introduction to Algorithms, MIT Press, 1990.
[5] Currie, K., Tate, A., O-Plan: the open planning architecture, Artificial Intelligence, 52, 1991, 49-86.
[6] Davis, E., Constraint Propagation with Interval Labels, Artificial Intelligence, 32, 1987, 281-331.
[7] Dechter, R., Meiri, I., Pearl, J., Temporal constraint networks.
Artificial Intelligence, 49, 1991, 61-95.
[8] Ghallab, M, Laruelle, H., Representation on Control in IxTeT, a Temporal Planner, Proceedings of the Second International Conference on AI Planning Systems (AIPS94), AAAI Press, 1994.
Modal Logics of Knowledge and Time Ron van der Meyden UNSW Australia  Abstract The talk will give a "state of the art" overview of modal logics of knowledge and time, covering both axiomatizations and model checking.
In the temporal dimension, we consider both linear and branching time logics.
The semantics of knowledge can be defined in a variety of ways, reflecting differing assumptions about the resources available to the agent in determining what it knows: from its current observation only, to synchrony (observation plus clock) to perfect recall.
We discuss the impact of these assumptions on the axiomatizations and on the complexity of model checking of the combined logics.
We also describe some initial experiments with a model checker based on these results.
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE
Combining Simultaneous Values and Temporal Data Dependencies Avigdor Gal & Dov Dori Information Systems Engineering Department Faculty of Industrial Engineering and Management Technion - Israel Institute of Technology Haifa, 32000, Israel  Abstract  In temporal databases there are situations where multiple values of the same data item have overlapping validity times.
In addition to the common case of multi-valued properties, there are several possible semantics to multiple values with overlapping validity times of the same data item.
We refer to such data items as having simultaneous values.
This paper presents a polynomial algorithm for ecient handling of simultaneous values in a database with temporal data dependencies|integrity rules that dene relationships among values of dierent data items in a temporal database.
The algorithm is demonstrated using a case study from the game theory area.
An implementation of the algorithm is integrated in a prototype of a temporal active database.
keywords: temporal databases, simultaneous values, uncertainty, temporal data dependencies, action reasoning  1 Introduction and Motivation  A temporal database is a database that supports some aspects of time [5].
One of the basic temporal aspects supported by many temporal databases is the valid time, representing the time a data-item is considered to be true in the modeled reality [5].
There are situations where multiple values of the same data-item have overlapping valid times.
The multi-valued property is the most common case, where several values are grouped into a single property [4].
For example, a property that contains the languages that a person speaks, can have a set of values grouped into a single property.
There are situations, however, where multiple values with overlapping valid times of the same data-item exist in the database, but with dierent semantics than the multi-valued case.
We refer to these data-items as having simultaneous values.
While in the multi-valued case all values whose valid time include t are deemed to be valid in the modeled reality, it is possible that only part of the candidate values, i.e.
the values that were assigned to a data-item at time t, represent the data-item's value in the real world.
For example, a data-item that contains a spouse name is limited by law to be single The work was conducted while the author was in the Technion.
He is currently at the Department of Computer Science, University of Toronto, Toronto, Ontario, M5S 3H5 CANADA.
valued.
When there are several alternatives for the spouse name due to uncertain information, then only one value of the set is the data-item's value.
Each value of the set is possibly the data-item's value.
In temporal databases, change of decisions about the value and valid time of a data-item may cause a situation where two values of the same data-item have overlapping valid times.
For example, a value val1 valid during [Jan 1994, Mar 1994) of a data-item  is augmented at time point Aug 1993 by a value val2 valid during [Feb 1994, Apr 1994).
 has more than one value in the interval [Feb 1994, Mar 1994).
In some cases, the value that was inserted later corrects an erroneous value that was inserted earlier.
In other cases, both values are possibly correct, each with respect to a dierent time point.
Simultaneous values enable dierent semantics in mapping the stored values to the modeled reality values.
It is particularly useful in applications where a data-item may have multiple values representing the existence of dierent alternatives.
For example, if knowledge arrives from various sources, then no apriori selection of a single value should be enforced.
Instead, for each database retrieval operation, the user can choose the appropriate value, values or any aggregation of those values.
Handling simultaneous values in a temporal database requires the use of optimized update and retrieval mechanisms.
The maintenance problem of simultaneous values becomes more arduous in temporal active databases [2], where temporal data dependencies are enforced.
A temporal data dependency is a tool that supports rules for manipulating data-items which may have a variety of temporal characteristics.
Temporal data dependencies can be viewed as a type of integrity rules of the temporal active database.
Violating them, activates database operations that react to restore the database integrity.
As an example for the use of temporal data dependencies, we can consider decision support systems [3]|systems that model decisions about actions that should be performed in a target system.
Such systems consist of decision models that are rooted in the operations research or articial intelligence disciplines, and of a database, that stores the necessary data to support the decision models.
Decision support systems can benet signicantly from the temporal active paradigm.
As a concrete motivating case study, we present the following application of a decision support system, based on the Cournot game [7]; [1].
Three instant coffee manufacturers|Bilbo, Frodo and Gandalf, decide each month about the quantity of coee to be produced in the next month.
Each manufacturer bases the decision about its manufactured quantity upon estimation of the quantities manufactured by the other two manufacturers, its own strategy (maximum revenue, a certain market share, etc.
), and general knowledge about the market behavior.
Each manufacturer has its own deadline for making the production quantity decision.
We assume a single market price for the manufactured type of instant coee, which is determined periodically as a function of the total quantity produced in that period.
The relationships between the market price and the total quantity is modeled by the constraint Total ?
Quantity  Market ?
Price = Market ?
Constant (1) Each manufacturer attempts to estimate the best decision to be taken, based on its own competition strategy and the two competitors' decisions and competition strategies.
For example, Bilbo may assume that Frodo and Gandalf have an objective of maximum prot. Consequently, each manufacturer would like to produce as much coee as possible without lowering the price to a level that decreases its total prot. By assuming the competition strategy of both Frodo and Gandalf, Bilbo can estimate their production decisions and determine the optimal production level, based on the following temporal data dependency: Production-Decision q Market-Constant :=  Competitors-Total-Estimation ?
Unit-Cost Competitors-Total-Estimation  This temporal data dependency is a periodical result of maximizing the prot function of a single manufacturer.
The derivation of the temporal data dependency is given in Appendix A.
Other strategies would yield dierent temporal data dependencies.
Competitors-Total-Estimation is the sum of the production estimations of the other two manufacturers.
Since this information is often misleading, each manufacturer should collect as much estimations as possible on each one of the competitors.
The temporal dependency graph can be evaluated each time there is a change in one of the data-items MarketConstant, Competitors-Total-Estimation or the manufacturer's Unit-Cost.
Alternatively, it can be evaluated once each period, just before the manufacturer has to decide about the quantity to be produced for the following period.
Figure 1 presents the data over time of the dataitems Market-Constant, Competitors-Total-Estimation, Unit-Cost, and the resulting Production-Decision of Bilbo.
As the gure shows, the temporal validity of each value is bounded.
For example, Market-Constant has the value 10000 during the interval [Feb 94, June 94).1 The resulting values of Production-Decision are 1  We use a single month granularity, hence this interval is  Market Constant  10000 9500 9000 Feb 94  June 94  Sep Oct Dec Feb Apr 94 94 94 95 95  Aug 95  t  Aug 95  t  Aug 95  t  Aug 95  t  630  Competitors Total Estimations  600 570 540 Feb 94  June 94  Sep Oct Dec Feb Apr 94 94 94 95 95  Unit Cost  7 6 Feb 94  June 94  Sep Oct Dec Feb Apr 94 94 94 95 95  405 400  Production Decision  385 380 375 316 293  Feb 94  June 94  270  Sep Oct Dec Feb Apr 94 94 94 95 95  Figure 1: Exemplary data of Bilbo in the coee manufacturers case study valid during the time intervals as computed using all data-items that determine Production-Decision and shown in the bottom graph of Figure 1.
For example, a Market-Constant of 10000, a CompetitorsTotal-Estimations of 600, and a Unit-Cost of 6, yield a Production-Decision of 400.
Since during the interval [Jan 94, June 94), the value of Market-Constant is 10000, Competitors-Total-Estimations is either 600 or 570, and Unit-Cost is 6, Production-Decision in that interval is either 400 or 405.
As Figure 1 demonstrates, the number of values and their associated temporal intervals resulting from the computation of a single temporal data dependency may be very large.
These values are a subset of the Cartesian product of all the possible values of each data-item.
A naive approach would consider all the possible combinations in the Cartesian set (342=24 combinations in the example of Figure 1), yielding an algorithm with high time complexity.
However, due to the bound temporal validity of values, usually only a small subset of the combinations in the Cartesian set should be considered.
For example, in Figure 1 only interpreted as all the days from February 1, 1994 to May 31, 1994 (June 1, 1994 is not included).
A time interval is dened in [5] as \the time between two insatnces" and can be represented as either close or semi-open intervals.
8 combinations out of the 24 possible ones should be considered.
This work presents an algorithm that eciently computes temporal data dependencies.
Our approach for ecient evaluation of temporal data dependencies is based in part on previous works on computing temporal aggregates, including [8] and [6].
An aggregate function, such as selecting the minimumvalue of a set, is applied to a set of values (e.g.
relations in the relational database model) and yields a scalar value.
In temporal databases, the aggregate function is, in general, time dependent, i.e.
the result of the aggregate function is applied to a set of values, each possibly having a dierent temporal validity.
The calculation of temporal data dependencies is an extension of aggregate computing with temporal grouping, where the resulting values are grouped by time.
To carry out such calculation, it is necessary to know which values have overlapping validity intervals, and to consider each value in its own validity interval.
The approach proposed in [8] rst determines constant intervals as intervals within which there is no change in the data-item value.
It then selects tuples that overlap each of these constant intervals and calculates the result.
The work in [6] is based on a tree data structure for the time axis partition.
Extending these approaches to solve the problem of evaluating temporal data dependencies, we present a polynomialalgorithm for ecient evaluation of temporal data dependencies with simultaneous values.
The computation is not necessarily an aggregate operation that involves a single type of data-item with several values.
Rather, it is a formula that may involve several types of data-items, each of which may consist of simultaneous values.
The algorithm constructs a list sorted according to the time validity of data-items values, and then calculates the result as a function of the values in the list elements.
Section 2 presents the algorithm for calculating temporal data dependencies with simultaneous values, while the properties of the algorithm are discussed in Section 3.
2 Evaluation of temporal data dependencies with simultaneous values  In this section we provide an outline of the algorithm for evaluating temporal data dependencies with data-items that consist of simultaneous values.
The algorithm consists of two phases, namely generating a constant interval list and computing the value for each combination of each constant interval element, as follows.
The constant interval list is sorted by the starting time points of the constant intervals.
The algorithm generates a partition of the valid time interval within which the temporal data dependency is to be determined.
Each element of the constant interval list has a valid time , and it consists of all the values whose validity covers .
Initially, a constant interval element is generated s , te ), where ts and with a valid time interval of [t te are the start and end time points of the interval within which the temporal data dependency is to be  evaluated, respectively.
Each value is processed with respect to an interval that consists of its starting time point, as follows.
Let [ts , te ) be a valid time interval of a value val, and [tis , tie ) a constant time interval associated with a constant interval element cii .
If [ts, te )\[tis, tie )6= ;, then there are six possible relationships between [ts , te ) and [tis, tie ), which are listed below along with the corresponding actions taken by the algorithm.
1. ts = tis and te < tie : replace cii with two constant interval elements, ci with [ts , te ) and ci with [te, tie ).
ci and ci receive the values of cii , and val is added to ci .
2. ts = tis and te = tie: add val to cii .
3. ts = tis and te > tie: add val to cii , and process val again with a valid time of [tie , te ).
4. ts > tis and te < tie : replace cii with three constant interval elements, ci with [tis, ts), ci with [ts , te ) and ci with [te, tie ).
ci , ci and ci receive the values of cii , and val is added to ci .
5. ts > tis and te = tie : replace cii with two constant interval elements, ci with [tis, ts ) and ci with [ts, te ).
ci and ci receive the values of cii , and val is added to ci .
6. ts > tis and te > tie : replace cii with two constant interval elements, ci with [tis, ts ) and ci with [ts, te ).
ci and ci receive the values of cii , and val is added to ci .
In addition, process val again with a valid time of [tie, te ).
As an example of the activation of the rst part of the algorithm, consider the data set of Figure 1, and assume that the interval within which the temporal data dependency is to be evaluated is [Feb 94, Aug 95).
The initial element of the list would be h[Feb 94, Aug 95), Market-Constant=, Competitors-TotalEstimations=, Unit-Cost=i.
The market-Constant values were processed rst, then the CompetitorsTotal-Estimations values, and nally the Unit-Cost values.
The full constant interval list is presented in Figure 2.
To enhance comprehension, the gure presents all the elements that were generated throughout the process, in a form of a tree.
Each node in the tree (except the root node) is an element of the list that was generated as a result of processing a value.
A value at the bottom of a node represents the value whose processing resulted in splitting the node.
The nal Constant Interval List (CIL) is the set of all leaf nodes of the tree, represented in Figure 2 by bold rectangles.
All other nodes were deleted during the process.
The Production-Decision values, shown in Figure 1, are shown within the constant interval rectangles in Figure 2.
0  0  00  00  0  0  00  0  00  00  0  0  00  0  00  00  00  0  00  00  Constant interval: Market Constant: Competitors Total Estimation: Unit Cost: 10000  Constant interval: Market Constant:  Constant interval: 10000  Market Constant:  Competitors Total Estimation:  600, 570  Competitors Total Estimation:  Unit Cost:  6  Unit Cost:  9500  Constant interval:  Constant interval: Market Constant:  Market Constant: 9000  9500  Competitors Total Estimation:  Competitors Total Estimation:  Unit Cost:  Unit Cost:  600  Constant interval:  540  Constant interval:  Constant interval:  Constant interval:  Market Constant:  9500  Market Constant:  Market Constant:  9000  Market Constant:  9000  Competitors Total Estimation:  600, 570, 540  Competitors Total Estimation:  Competitors Total Estimation:  540, 630  Competitors Total Estimation:  630  Unit Cost:  Unit Cost:  Unit Cost:  6  9500  7  Unit Cost:  7  570  Constant interval: Market Constant:  3 Algorithm properties  Constant interval:  9500  This section discusses the algorithm complexity (Section 3.1) and the correctness of the algorithm (Section 3.2).
Market Constant: 9500  Competitors Total Estimation:  570, 540  Competitors Total Estimation:  Unit Cost:  6  Unit Cost:  540  6  Constant interval:  94).
The next constant interval element is scanned, and the same combination is found.
Therefore, the valid time of the combination is set to be [June 94, Oct 94).
The same combination is found again in the subsequent constant interval element, and the valid time of the combination is set to be [June 94, Dec 94).
At this point, the process is terminated since the following constant interval element does not consist of this combination.
The result of the second phase of the algorithm, applied on the data set of Figure 1 is the set of values of Production-Decision, which are also shown in Figure 1.
After deciding on the appropriate interval, the values are used for calculating the derived value for that interval.
For example, in the previous example, the derived value is calculated to be 385, valid during [June 94, Dec 94).
Constant interval:  Market Constant:  9500  Market Constant:  9500  Competitors Total Estimation:  540  Competitors Total Estimation:  540  Unit Cost:  6  Unit Cost:  7  Figure 2: The constant interval list (CIL) generation process A single constant interval element may consist of more than a single value for a data-item.
For example, the constant interval element with the constant interval [Feb 94, June 94) in Figure 2 has two sets of values: h10000, 600, 6i, and h10000, 570, 6i.
Each such set of values is a dierent combination for the calculation of the temporal data dependency, giving rise to a dierent value for the same interval.
The constant interval list may also be over split with respect to a combination, i.e.
it may have consecutive constant interval elements with identical sets of values.
For example, all the constant interval elements with the constant intervals [June 94, Sep 94), [Sep 94, Oct 94), and [Oct 94, Dec 94) consist of the combination h9500, 540, 6i.
This is a result of several overlapping values of the same data-item.
In this case, [June 94, Sep 94) and [Sep 94, Oct 94) are separate constant intervals, since the value 600 of CompetitorsTotal-Estimation is valid only during [June 94, Sep 94).
The second phase of the algorithm uses each of the possible value combinations in the constant interval elements to compute the new values.
The combinations are scanned for each constant interval element, starting from the constant interval element with the minimal valid time.
Subsequent constant interval elements are scanned to nd identical combinations.
If an identical combination is found, the valid time of the constant interval element is added to the valid time of the combination.
This process is repeated until no more identical combinations can be found.
For example, consider the example given in Figure 2.
The combination h9500, 540, 6i is selected from the constant interval element with the valid time of [June 94, Sep  3.1 Complexity  Let n be the number of processed values.
A value with a valid time interval of [ts, te ) can add two constant interval elements at the most, if for a constant interval element cii , ts > tis and te < tie , or if there are two constant interval elements cii and cij such that ts > tis and te < tje .
Consequently, if the number of processed values is n, then the upper bound on the number of constant interval elements is 2n.
For each value, the location of the rst constant interval element to be processed is searched.
This search is bounded by log2(2n).
In the worst case, a valid time of a value covers the valid times of all of the constant interval elements, resulting in 2n comparisons.
Hence, the time complexity of the CIL generation phase is bounded by O(n(log2 (2n) + 2n)) =O(n2 ).
The time complexity of the second phase of the algorithm is bound by O(m3 ), where: m is the number of all the valid combinations of a single value from all the data-items, i.e.
all the combinations for which values have valid time overlapping.
For example, in Figure 1, m=8 and the eight combinations are: h10000, 600, 6i, h10000, 570, 6i, h9500, 600, 6i, h9500, 570, 6i, h9500, 540, 6i, h9500, 540, 7i, h9000, 540, 7i, h9000, 630, 7i.
2n is the maximal number of constant interval elements.
for example, 18 is the maximal number of constant interval elements in Figure 1 since the number of state-elements is 9.
However, The actual number of constant interval elements is 7, as shown in Figure 2.
2mn is the maximal number of possible combinations in the list.
The algorithm generates all of the list combinations (2mn).
At each iteration of the algorithm, a single combination is processed.
The worst case is when at each iteration all the remaining combinations are scanned.
Thus, the worst case complexity is O(m2 n).
Since at each  constant interval element there is at least one combination, m  n. Therefore, the worst case complexity is bounded by O(m3 ).
From the discussion above we can conclude that the worst case complexity of the two phases of the algorithm is bounded by O(m3 ).
3.2 Correctness  Proposition 1 The partition of the time interval:  The entire set of constant interval elements constitute a partition of the evaluated interval.
The proof of Proposition 1 is done using induction on the number of constant interval elements.2 At each iteration we verify that each of the six possible relationships between a valid time of a value and a constant interval element results in a new constant interval list that maintains the partition assertion.
This proposition ensures the correctness of the list construction phase, where each value should allocate a single constant interval element.
Proposition 2 Algorithm correctness: The algorithm generates a correct result.
Given a set of values and a temporal data dependency, a correct result ensures that for each combination of values with overlapping valid times there is a value which is the result of applying the temporal data dependency on this combination, and its valid time consists of the intersection of the overlapping valid times of the values in the combination.
A simple algorithm, in which each combination of the Cartesian product is evaluated, can achieve a correct result but at a high computational cost.
The proof of Proposition 2 shows that if a combination is not processed by the algorithm, then it should not be in the resulting set.
4 Conclusion and future research  We have proposed an algorithm for ecient evaluation of temporal data dependencies in temporal databases with simultaneous values.
The algorithm consists of two phases, the rst generates a constant interval list and the second computes the value for each combination of each constant interval element.
A single constant interval element may consist of more than a single value for a data-item.
The constant interval list may also be over split with respect to a combination, i.e.
it may have consecutive constant interval elements with identical sets of values.
The time complexity of the calculation algorithm is bound by O(m3 ), where m is the number of all the valid combinations of a single value from all the data-items.
This result is less expensive, computation wise, than the 2 Full denitions and proofs of propositions in this paper can be obtained via anonymous ftp to ftp.technion.ac.il under directory/usr/local/servers/ftp/pub/supported/ie.
The le is called proofs.tex.
It is produced using LaTEX.
The proofs can also be obtained through the author's WWW home page, http://www.cs.toronto.edu/avigal.
result of scanning the Cartesian product (O(ni=1 mi ), where mi is the number of values of the i-th dataitem and n is the number of data-items involve in the calculation.
A prototype of a system that implements the algorithm exists on the basis of MAGIC 5.6 for DOS, under DOS 6.2.
Further research is aimed at a more general form of temporal data dependencies, where the valid time is dened indirectly through constraints, or relative to other time points.
References  [1] A. Cournot.
Researches into the Mathematical Principles of the Theory of Wealth.
Macmillan, New York, N.Y., 1897.
[2] O. Etzion, A. Gal, and A. Segev.
Temporal active databases.
In Proceedings of the International Workshop on an Infrastructure for Temporal Database, June 1993.
[3] K.M.
Van Hee, L.J.
Somers, and M. Voorhoeve.
A modeling environment for decision support systems.
Decision Support Systems, 7:241{251, 1991.
[4] R. Hull and R. King.
Semantic database modeling: Survey, application and research issues.
ACM Computing Surveys, 19(3):201{260, Sep 1987.
[5] C.S.
Jensen, J. Cliord, S.K.
Gadia, A. Segev, and R.T. Snodgrass.
A glossary of temporal database concepts.
ACM SIGMOD Record, 21(3):35{43, 1992.
[6] N. Kline and R.T. Snodgrass.
Computing temporal aggregates.
In Proceedings of the International Conference on Data Engineering, pages 223{231, Mar 1995.
[7] J. Tirole.
The Theory of Industrial Organization.
the MIT press, 1989.
[8] P.A.
Tuma.
Implementing historical aggregates in TempIS.
Master thesis.
Wayne State University, Nov. 1992.
Appendix A: The production decision temporal data dependency In this section we present the derivation of the temporal dependency graph given in Section 1.
The following notation is used: A  Prot B  Revenue C  Cost D  Market-Price E  Fixed-Cost F  Unit-Cost G  Market-Constant H  Total-Quantity I  Competitors-Total-Estimation X  Production-Decision We assume that Bilbo's strategy is to produce the amount that would maximize A, as follows.
A = B?C= X  D ?
(E + X  F) = X G H ?
(E + X  F) = X  X G+ I ?
(E + X  F) max (A) =) A0 = 0 =) G  (X(X+ +I) I)?2 X  G ?
F = 0 2 =) G  (X + I) ?
(XX + GI)?2 F  (X + I) = 0 =) F  X2 + 2  F  I  X + F  I2 ?
G  I = 0 p ?
2  F  I  4  F2  I2 ?
4  F  (F  I2 - G  I) =) X = = 2F qG  I ?I F  X is non-negative.
q  =) X = max( GF I ?
I, 0)
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE
In Proc.
of the Third International Workshop on Temporal Representation and Reasoning (TIMEa96) May 19-20, 1996, Key West, Florida.
IEEE Computer Society Press, pp.
144-151.
Guiding and Rening Simulation using Temporal Logic Giorgio Brajnik  Daniel J. Clancy  Dip.
di Matematica e Informatica Universita di Udine 33100 Udine, ITALY  Department of Computer Sciences University of Texas at Austin Austin, TEXAS 78712  Abstract  We illustrate TeQSIM, a qualitative simulator for continuous dynamical systems.
It combines the expressive power of qualitative dierential equations with temporal logic by interleaving simulation with model checking to constrain and rene the resulting predicted behaviors.
Temporal logic expressions are used to specify constraints that restrict the simulation to a region of the state space and to specify trajectories for input variables.
A propositional linear{time temporal logic is adopted, which is extended to a three valued logic that allows a formula to be conditionally entailed when quantitative information specied in the formula can be applied to a behavior to rene it.
We present a formalization of the logic with theoretical results concerning the adopted model checking algorithm (correctness and completeness).
We show also an example of the simulation of a non{autonomous dynamical system and illustrate possible application tasks, ranging from simulation to monitoring and control of continuous dynamical systems, where TeQSIM can be applied.
1 Introduction  Reasoning about change across time is a common problem within articial intelligence and computer science in general.
For systems with discrete state spaces, temporal logic (TL) provides a broad class of formalisms that have been used for such tasks as reasoning about the eect of actions, specifying and verifying correctness of reactive computer programs and synthesizing or analyzing discrete event systems [13, 6, 2, 8].
Qualitative reasoning [10] has been used to reason about continuous change within the eld of dynamical systems in the presence of incomplete knowledge.
The expressiveness of the models used within qualitative simulation, however, is often limited to structural equations constraining the potential values for related variables.
The modeler is unable to express behavioral information about the trajectory of a variable or relationships between the trajectories of interconnected variables.
Such an information allows the modeler to restrict the simulation to a region of the state space and to specify trajectories for input variables.
 The research reported in this paper has been performed while visiting the Qualitative Reasoning Group at the University of Texas at Austin.
The Temporally Constrained QSIM (TeQSIM, pronounced tek'sim) algorithm combines the expressive power of these two paradigms by interleaving temporal logic model checking with the qualitative simulation process.
Temporal logic is used to specify qualitative and quantitative trajectory information that is incorporated into the simulation to constrain and rene the resulting behaviors.
Qualitative simulation constructs a set of possible behaviors consistent with a model of a dynamical system represented by a qualitative dierential equation (QDE).
The QSIM algorithm [10] represents the behavior of a dynamical system by an incrementally generated tree of qualitative states.
Each state describes the system at either a time{point or over a time{interval between two points by a tuple of qualitative values for the variables specied within the QDE.
Each qualitative value is described by a magnitude and a direction of change: the direction of change represents the sign of the variable's time derivative while the magnitude is dened upon a totally ordered set of distinctive landmark values and is either a landmark value or an interval between two landmark values.
The simulator uses the constraints specied within the QDE along with continuity to derive a branching{ time behavioral description.
Each path within the tree represents a potential behavior of the system: branches result from inherent ambiguity within the qualitative description.
Semi{quantitative simulation incorporates quantitative information into the qualitative simulation in the form of numeric ranges for landmark values and bounding envelopes for functions.
TeQSIM interleaves temporal logic model checking with the qualitative simulation process to obtain two major benets.
Behavior ltering tests each partial behavior against the set of temporal logic expressions representing trajectory constraints as the set of behaviors is incrementally generated.
A behavior is eliminated from the simulation when it can be shown that all of its possible completions fail to model the set of temporal logic expressions.
Thus, the space of the behavioral description is restricted to include only behaviors that can satisfy the temporal logic expressions.
Behavior renement integrates numeric information contained within the temporal logic expressions into the qualitative simulation to provide a more precise numerical description.
This process restricts an individual behavior to include only those real valued inter-  Simulation & Model Checking  Preprocessing Extended TL Expressions Event List  QDE & Initial State  Event Replacement  QDE Modifier  TL Expressions  Modified QDE & Initial State  TLaGuide  Filtered and Refined Behavior Tree  QSIM  Figure 1: TeQSIM architecture.
pretations which model the set of temporal logic expressions and therefore improves the precision of the prediction.
The integration of temporal logic model checking and qualitative simulationwas initially investigated by Kuipers and Shults [11].
They use a branching{time temporal logic to prove properties about continuous systems by testing the entire behavioral description against a temporal logic expression.
The appropriate truth value is returned depending upon whether or not the description models the expression.
Our work focuses on constraining the simulation as opposed to testing a simulation after it is completed.
The next section provides an overview of the TeQSIM algorithm.
Section 3 provides an example along with a discussion of some of the applications of this technique while section 4 describes the formal syntax and semantics of our temporal logic.
Soundness and completeness theorems are presented for the TL{ guide algorithm.
Finally, section 5 provides a discussion of some of the extensions that will be investigated in the future.
2 TeQSIM Overview  TeQSIM has been designed to provide the user with a mechanism for specifying trajectory constraints to guide and rene the qualitative simulation process.
By trajectory of a set of variables over a time interval (a; b)  < we mean a mapping from (a; b) to variable values ( < ).
Trajectory constraints on a set of variables restrict the possible trajectories for those variables.
Figure 1 provides an overview of the system architecture.
In general, the algorithm can be divided into two main components: a preprocessing stage that combines the trajectory information provided by the modeler into the qualitative model and generates the appropriate TL expressions; and a simulation and model checking stage that integrates model checking into the simulation process by ltering and rening qualitative behaviors according to a set of temporal logic expressions.
Trajectory information is specied in the form of an event list and a set of extended temporal logic statements.
An event is a time{point distinguished within the simulation.
The event list is a sequence of named, quantitatively bounded events not represented within the QDE that are incorporated into the simulation.
An extended temporal logic statement is simply a tem-  poral logic formula which may include direct references to events occurring within the event list.
These references are replaced by the appropriate formulae.
The simulation provides a complete temporal ordering between these externally dened events and other events dened within the model.
Model checking and behavior renement is performed by TL{guide.
Each time QSIM extends a behavior by the addition of a new state, the behavior is passed to TL{guide.
The behavior is ltered if there is sucient information within the partially formed behavior to determine that all completions of the behavior fail to model the set of TL expressions.
If the behavior can potentially model the set of TL expressions, then it is rened by the incorporation of any relevant quantitative information contained within the TL expressions.
Otherwise the behavior is retained unchanged.
3 Problem solving with TeQSIM  Incorporating temporal logic model checking into the qualitative simulation process allows the modeler to control the simulation by restricting the behavior of input and dependent variables.
Trajectory constraints can be used for a variety of tasks, including simulation of non{autonomous systems, incorporating observations into simulation, analysis of continuous control laws and performing goal oriented simulation.
The TeQSIM algorithm has been tested on a range of examples demonstrating each of the tasks above.
The following example demonstrates the use of TeQSIM to simulate a non{autonomous system incorporating information obtained via observation.
The system being simulated is a very simple one, which generalizes to the real{world problem of water supply control in the domain of lakes, rivers and dams [7].
Consider an open tank, with an uncontrolled inow u, a regulated outow y, valve opening v and amount of liquid A in the tank.
The system is modeled by the dierential equation A_ = u ?
y; y = f (A; v) where f (A; v) is an unknown monotonically increasing function on both arguments, numerically bounded.
This model can be straightforwardly specied using the QSIM QDE language.
Figure 2 shows the trajectory constraints used by TeQSIM for simulating the regulated tank.
Part (a) of the gure shows a totally ordered event list that provides quantitative temporal bounds for external events corresponding to two opening actions on the outow valve.
Four events are dened corresponding to the beginning and the end of each of these actions.
Part (b) contains the trajectory constraints that specify the behavior of the outow valve along with constraints on inow and level (up to the end of the rst opening action; the second action, not shown, is similarly specied): i. the variable Inflow is constant3 and its value is within the range [200, 220] cm /s, ii.
the valve opening is constant at 0.5 until the beginning of the rst opening action, iii.
between events b-open1 and e-open1 (i.e.
the duration of the rst opening action) the valve  (event (event (event (event  b-open1 e-open1 b-open2 e-open2  :time :time :time :time  (30 30)) ; sec (35 36)) (150 150)) (153 155))  (a) External event declaration.
i)  (always (and (qvalue InFlow (nil std)) (value-in InFlow (200 220))) ii) (until (and (value-in Valve (0.5 0.5)) (qvalue Valve (nil std))) (event b-open1)) iii) (between (event b-open1) (event e-open1) (and (qvalue Valve (nil inc)) (qvalue Level (nil inc)))) iv) (occurs-at (event e-open1) (value-in Valve (0.65 0.7)))  (b) Trajectory constraints (rst opening action only).
Figure 2: Input to TeQSIM.
opening is increasing and Level is observed to increase, and iv.
valve reaches a value in [0.65, 0.7] at the end of the rst opening action.
Figure 3 shows the result of the simulation.
TeQSIM yields a single behavior where Level increases and reaches a new equilibrium value, well below the High threshold.
Correctness properties of TeQSIM guarantee that this is the only possible outcome of any real plant that is validly described by the QDE model, the initial state and the trajectory constraints.
This example is very simple but it shows one possible use of trajectory constraints to extend the scope of qualitative simulators (like QSIM, that are limited to simulation of initial value problems only).
A brief description of other tasks to which TeQSIM can be applied along with a discussion of how each task can be demonstrated on the model described above is contained in gure 4.
Additional examples are contained in [5].
4 Guiding and rening simulation  TeQSIM guides and renes the simulation based upon a specication formulated using a variation of propositional linear time logic (PLTL).
PLTL combines state formulae, that express information about an individual state, with temporal operators such as until, always, and eventually to extend these state formulae across time and represent change.
We have extended PLTL by using a three valued logic that allows an expression to be conditionally entailed when quantitative information contained within the expression can be applied to a behavior to rene the description.
A renement condition species numerical bounds extracted from the TL expressions.
Application of these conditions to the behavior eliminates the region of the  state space that extended beyond the quantitative information specied in the TL expression.
In addition, the TL{guide algorithm is designed to handle the incremental nature of a qualitative simulation.
An undetermined result occurs whenever the behavior is insuciently determined to evaluate the truth of a TL expression.
4.1 TL specication language  This section provides a formal description of the syntax and semantics for the TL language.
The syntax and semantics are derived from work done by Emerson [6] and Kuipers and Shults [12].
A discussion of the TL{guide algorithm along with soundness and completeness theorems are presented in the following subsections.
Proofs of these theorems along with additional lemmas and corollaries can be found in [5].
Syntax.
The syntax and semantics for the state for-  mulae are described in gure 5(a).
Path formulae are derived by applying the temporal operators until and strong-next along with boolean operators to state formulae.
Path formulae P are formally dened as P ::= Sj (and P P )j(not P )j(strong-next P )j(until P P ), where S is the set of state formulae.
Informally, (until p q) is true for a path if p holds in all states preceding the rst one where q holds, while (strong-next p) is true for a path if p holds in the second state of the path which must exist.
Figure 5(b) gives a set of useful abbreviations.
We require that formulae are in positive normal form, i.e.
(i) until, releases and strong-next are the only temporal operators in the formula, (ii) for every not in the formula, its scope is an atomic proposition, and (iii) such a scope does not include any proposition constructed using value-<=, value->= or value-in.
The rst two requirements do not restrict the expressiveness of the language since the abbreviations shown above can be used to transform a formula to satisfy these conditions.
The latter requirement is due to the specic representation of numeric information in QSIM, which does not allow open numeric intervals.
Semantics.
Temporal logic formulae are given meaning with respect to the interpretation structures dened below.
These structures are extended from their typical denition (e.g.
[6]) in order to accommodate the renement process.
Path formulae are interpreted against an interpretation structure M = <S; ; ?
; I ; C ; M> where:  S is a set of states;  : S !
S is a partial function, mapping states  to their successors (we are dening a linear{time logic, hence each state has at most one successor);  I : S  S !
ft; f; ?g is an assignment of truth values to propositions and states (?
denotes the \unknown" truth value);  T1  T2  T3  T4  ..a T5 [153 +INF] .
... .... a  HIGH [75 75]  ... L-20 [30.8 37.4] .
..... ..... ..... ..... ..... ..... ..... ..... .. Adeg .... a a a a a a a a a a L* [30 35] 0 [0 0]  0 [0 0] T0  INF  TOP [100 100]  1 [1 1]  ..... ..... ....Adeg Adeg Adeg V-56 [0.900 0.950] ..a. .
.
.
V-28 [0.650 0.700] .. ..... ..... ... Adeg Adeg Adeg ..a ... ..... ..... V* [0.500 0.500] Adeg Adeg Adeg  MINF  T5  T0  VALVE  T1  T2  T3  T4  T5  ..a .
... .... a ...a .
.
.
.... a  .....a ...a ..a.. .
.
.
.
.... a a T0  T1  T4 [153 155] T3 [150 150] T2 [35 36] T1 [30 30] T0 [0 0]  T2  T3  T4  T5  TIME  LEVEL  Figure 3: Output of TeQSIM.
 ?
is a set of renement conditions.
?
is closed with respect to the standard boolean operators f^; :g and contains the distinguished item TRUE;  C : S  S !
?
is a function (condition gener-  ator) that maps state formulae and states into  renement conditions that disambiguate a formula truth value; we require that C ('; s) = TRUE i I ('; s) = t and C ('; s) to be dened when I ('; s) =?.
 M: ?
 S !
S is a function (state modier) that maps a condition and a state into a rened state.
For any state s, M(TRUE; s) = s and M(:TRUE; s) = ?.
We require that if ' is an atomic proposition then renement conditions are necessary and sucient for resolving the ambiguity, i.e.
if C = C ('; s) then I ('; M(C; s)) = t and I ('; M(:C; s)) = f (unless C = TRUE, in which case M(:C; s) = ?).
As customary, a path x is a sequence of states x = <s0 ; s1 ; : : :> such that for any pair of consecutive states (s ; s +1) we have that (s ) = s +1 .
The length of a path is denoted by jxj, which for innite paths is 1.
For all non negative integers i < jxj, x denotes the sub{path <s ; : : :> and x(i) denotes s .
A full{ path extension of a nite path x, denoted with xb, is an innite path formed by concatenating x with an innite sequence of states.
Finally, M is naturally extended to paths in order to rene paths.
In the specic case of QSIM, I may give ?
only for propositions including value-<=, value->= and value-in (as illustrated in g. 5(a)).
A renement condition is an inequality between the partially known numeric value of a variable in a state and an extended real number (or a boolean combination of conditions).
The condition that the QDE variable X in state s has to be less than 5 is written \X < 5".
Notice that ambiguity is not purely a syntactic property, but it depends on state information.
For example, (value-<= X .3) will be (unconditionally) true on a state s where R(X; s) = [0; 0:25], but only conditionally true on s0 where R(X; s0 ) = [0; 1:0].
Because of ambiguity, to dene the semantics of formulae we need to introduce two entailment relations.
The rst one, called models (j=), will be used to characterize non{ambiguous true formulae; the second one, i  i  i  i  i  i  i  s  called conditionally{models ( j=? )
will characterize formulae that are ambiguous.
We will say that an interpretation M and a state s falsify (6j=) a formula ' when neither s j= ' nor s j=? '
hold.
Figure 5(c,d) gives the semantics of the language.
To simplify the analysis of the renement process, the usage of ambiguous formulae must be restricted.
The problem is that an arbitrary ambiguous formula may yield several alternative renement conditions.
A disjunction of renement conditions cannot be applied to states without requiring a change in the successor function  and the introduction of a new behavior which is qualitatively identical to the original behavior in the tree.
Two dierent types of disjunction can result from certain ambiguous formula.
A state disjunction results from a disjunction of ambiguous state formulae.
For example, when interpreted against a particular state (or (value-<= X 0.5) (value->= Y 15)) may yield the condition (X  0:5 _ Y  15).
When applying such a condition to a state, M(C; s) yields two states { s0 in which X is restricted to be  0:5 and s00 where Y  15.
A path disjunction, on the other hand, occurs when an ambiguous formula is included in a path formula in such a manner that a sub{formula can be conditionally true for more than one sub{path.
For example, in the path formula (until p (value-<= X 0.5)) a disjunction occurs across sub{paths regarding when (value-<= X 0.5) should be conditionally true.
The following denitions restrict the syntax to formulae that are well{behaved.
A potentially ambiguous formula is any TL formula that (i) is an atomic proposition constructed using one of the following operators value-<=, value->= or value-in, or (ii) is a path formula which contains a potentially ambiguous sub{formula.
Admissible formulae are those formulae ' that satisfy the following conditions: 1. '
is in positive normal form, and 2. if '  (until p q) then q is not potentially ambiguous, and 3. if '  (releases p q) then p is not potentially ambiguous, and 4. if '  (or p q) then at most one of p and q is potentially ambiguous.
It can be proved that for all admissible formulae ' s  s0  s00  s  Continuous feedback control  A control law is expressed in terms of a set of formulae relating the value of the monitored variable (say Level) with the required value, or trend, of the control variable (say Valve).
The resulting closed{loop behaviors can then be analyzed with respect to the controller's goal.
The following trajectory constraint provides a partial specication of a control law to avoid overowing the tank.
It states that the valve opening must be increasing whenever the magnitude of Level is greater than high and the valve hasn't yet reached its maximum opening max.
(always (between (qvalue Level ((high nil) nil)) (qvalue Level (high dec)) (implies (qvalue Valve ((min max) nil)) (qvalue Valve (nil inc)))))  Continuous feed{forward control  The control law is expressed in terms of a set of formulae relating a predicted value of the monitored variable to the current value or trend of the control variable.
The following trajectory constraint species that if the tank can potentially overow then the valve opening should be increased until it reaches its maximum value or level becomes smaller than high.
(always (implies (eventually (qvalue Level (top nil))) (until (qvalue Valve (nil inc)) (or (qvalue Level (high dec)) (qvalue Valve (max nil))))))  Goal Oriented Simulation  The statements reported below can be used to check whether the tank will overow within a specied time frame.
Since TeQSIM is sound, if no behaviors are produced then the modeled system can not violate these constraints (assuming that the QDE model is valid).
The following trajectory constraint limits the simulation to behaviors in which the tank Level reaches high within 150 seconds.
(and (event horizon :time 150) (before (qvalue Level (high nil)) (event horizon)))  Figure 4: Applying TeQSIM to other tasks using the regulated tank model.
and for any interpretation M and path x, if x j=? '
then any necessary and sucient condition C for rening x into a model for ' (i.e.
M(C; x) j= ' and M(:C; x) 6j= ') is either a single condition or a conjunction of conditions.
Even though the restriction to admissible formulae reduces expressiveness, such a restriction does not hinder the practical applicability of TeQSIM.
As long as important distinctions are qualitatively represented (using landmarks or events), most trajectory constraints can be cast into admissible formulae.
For example, the constraint that \until level goes above 50 the input ow rate has to be below 200" could be expressed with the following non{admissible formula (until (value-<= InFlow 200) (value->= Level 50)) where the two distinctions (200 and 50) do not correspond to qualitative landmarks.
By adding a landmark to the quantity space of Level corresponding to the value 50, the formula can be rewritten in an admissible form (i.e.
(until (value-<= InFlow 200) (qvalue Level (lm-50 nil)))).
QSIM computes in nite time a set of behaviors, each representing a class of trajectories of the system being simulated.
Although a QSIM behavior is a nite structure, it may represent innite trajectories of the simulated system.
In fact, quiescent states are nite descriptions of xed{point trajectories1 .
For-  mally, a QSIM behavior b is a nite sequence of non repeating states <s0 ; : : :; s > such that 8i; 0  i < n : (s ; s +1 ) belongs to the QSIM relations successor or transition.
A behavior b is closed i QSIM detected that s is a quiescent state or that s is a transition state that has no possible successor.
1 QSIM may identify cyclic behaviors as well and represent them throughcycles in a directed graph.
The use of quantitative information, however, only makes sense if time is not cyclic;  furthermore the renement of behaviors requires that they do not share sub{behaviors.
For these reasons cycle detection is disabled within TeQSIM.
n  i  i  n  n  4.2 Model checking  The model checking algorithm is designed to evaluate a behavior with respect to a set of admissible formulae as the behavior is incrementally developed.
This allows behaviors to be ltered and rened as early as possible during the simulation.
Our algorithm is derived from the one described in [11]; however, it has been modied to deal with conditionally true formulae and to cope with behaviors which are not closed.
A detailed discussion of the algorithm is provided in [5].
The algorithm computes the function  : P  Behaviors !
fT; F; Ug Conditions: A denite answer (i.e.
T or F) is provided when the behavior contains sufcient information to determine the truth value of the formula.
For example, a non{closed behavior b, for all interpretations M , will not be suciently determined with respect to the formula ' (eventually p) if p is not true for any sux of b, since p may become true in the future.
A behavior is considered to be suciently determined with respect to a formula whenever there is enough information within the behavior to determine  (a) Syntax and semantics of state formulae.
In the denitions of the most important state formulae given below, v denotes a QSIM variable, R(v; s) the range of potential values for v in state s, and vs the unknown value of v in s. n, ni denote extended real numbers.
(qvalue v (qmag qdir )) where qmag is a landmark or open interval dened by a pair of landmarks in the quantity space associated with v, and qdir is one of finc, std, decg.
NIL can be used anywhere to match anything.
Such a proposition is true exactly when the qualitative value of v in the state s matches the description (qmag qdir ).
(value-<= v n) is true i 8x 2 R(v; s) : x  n; it is false i 8x 2 R(v; s) : n < x; it is unknown otherwise.
In such a case the renement condition is that the least upper bound of the possible real values of v is equal to n (i.e.
vs  n).
(value->= v n) is similar.
(value-in v (n1 n2 )) is true i R(v; s)  [n1 ; n2 ]; it is false i R(v; s) \ [n1 ; n2 ] = ;.
It is unknown otherwise, and the renement condition is that the greatest lower bound is equal to n1 and the least upper bound is equal to n2 (i.e.
n1  vs ^ vs  n2 ).
Non{atomic propositions are dened using standard boolean operators (and, not); standard propositional abbreviations are also allowed (true, false, or, implies, iff).
(c) Entailment relations for state formulae.
(a ranges over atomic propositions, p and q over S ): s j= a i I (a; s) = t s j=?
a i I (a; s) =?
s j= (and p q) i s j= p and s j= q s j=?
(and p q) i s j=?
p and s j= q, or s j= p and s j=?
q, or s j=?
p and s j=?
q s j= (not p) i s 6j= p s j=?
(not p) i s j=?
p  (b) Path formulae abbreviations.
(or p (releases p (before p (eventually (always (never (starts p (follows p (occurs-at (between p  q) q) q) p) p) p) q) q)           p q) q r)     (not (and (not p) (not q ))) (not (until (not p) (not q ))) (not (until (not p) q )) (until true p) (releases false p) (always (not p)) (releases p (implies p (always q ))) (releases p (implies p (strong-next (always q )))) (releases p (implies p q )) (releases p (implies p (strong-next (until r q ))))  The intuitive meaning for some of these forms is: p q): q is true up until and including the rst state in which p is true.
(starts p q ): q holds from the rst occurrence of p. (follows p q ): similar to starts, but q should hold just after the rst occurrence of p. (occurs-at p q ): q is true at the rst occurrence of p. (between p q r ): r holds in the open time interval between the rst occurrence of p and the subsequent rst of q.
(releases  (d) Entailment relations for path formulae.
(p 2 S and '; 2 P ): x j= p i x(0) j= p x j=?
p i x(0) j=?
p x j= (strong-next ') i jxj > 1 and x1 j= ' x j=?
(strong-next ') i jxj > 1 and x1 j=? '
x j= (until ' ) i 9i  0 : xi j= and 8j < i : xj j= ' x j=?
(until ' ) i 9i  0 : (xi j= or xi j=? )
and ? ')
and 8j < i : (xj j= ' or xj j= ?
occurs at least once j= The semantics of (and ' ) and (not ') is similar to the propositional case.
Figure 5: Syntax and semantics of the language.
a single truth value for all completions of the behavior.
If a behavior is not suciently determined for a formula, then U is returned by  and the behavior is not ltered out by TL{guide.
Notice that indeterminacy is a property independent from ambiguity: the former is related to incomplete paths, while the latter deals with ambiguous information present in states of a path.
The following recursive denition characterizes determinacy.
Given an interpretation M , a path x is suciently determined for a positive normal formula ' (written x  ') i one of the following conditions is met: 1. x corresponds to a closed behavior or ' is a proposition or x j= ' or x j=? '
2. '
 (strong-next p) and jxj > 1 and x1  p  3. '
 (until p q) and 9i: i < jxj and x 6j= p and x  p and 8j  i: x 6j= q and x  q 4. '
 (releases pq) and 9i: i < jxj and x 6j= q and x  q and 8j  i: x 6j= p and x  p 5. '
 (and p1 p2) and 9i: x 6j= p and x  p 6. '
 (or p1 p2 ) and 8i: x 6j= p and x  p We will write x = ' to signify that x is not suciently determined for '.
We are now ready to state the main correctness and completeness theorem on model checking.
i  i  j  j  i  j  j  i  i  i  i  i  Theorem 1 ( is sound and complete) For any  admissible formula ' and for any QSIM behavior b the following statements hold:  1.
('; b) = (T; TRUE) () there exists an interpretation M such that b j= '.
2.
('; b) = (T; C ) and C 6= TRUE () there exists an interpretation M such that b j=? '
and if b0 ; b00 exist such that b0 = M(C; b) and b00 = M(:C; b) then b0 j= ' and for all full{path extensions bb00 of b00: bb00 6j= '.
3.
1 ('; b) = F () for all interpretations M and for all full{path extensions bb: bb 6j= ' and b  '.
4.
('; b) = (U; C ) () for all interpretations M , b = ' and if b0 = M(:C; b) exists then for all full{path extensions bb0 : bb0 6j= '.
Proof is based on induction on formula length.
It follows by a case{by{case analysis of the algorithm.
4.3 Guiding and rening the simulation  When given a behavior b and an admissible formula ', TL{guide computes  ('; b) = (v; C ).
The behavior b is refuted i v = F; it is retained unmodied i v 6= F and C = TRUE; and it is rened into M(C; b) i v 2 fT; Ug and C 6= TRUE.
The following theorem justies our use of temporal logic model checking for guiding and rening the simulation.
Theorem 2 (TL{guide is sound and complete)  Given a QSIM behavior b and an admissible formula ' then TL{guide: 1. refutes b i for all interpretations M and for all full{path extensions bb: bb 6j= ' and b  '.
2. retains b without modifying it i (a) there exists an interpretation M such that b j= '; or (b) for all interpretations M , b  = ' and there is0 no necessary condition C such that if b = M(:C; b) exists then for all full{path extensions bb0: bb0 6j= '.
3. replaces b with b0 i (a) there exists an interpretation M such that b j=? '
and exists C such that b0 = M (C; b) j= '00 and C is necessary (i.e.
exists b00 such that b = M(:C; b) and for all full{path extensions bb00: bb00 6j= '); or (b) for all interpretations M , b  = ' and there is a necessary condition C such that b0 = M(:C; b) and for all full{path extensions bb0: bb0 6j= '.
Proof follows almost directly from the previous theorem.
5 Discussion and future work  TeQSIM is designed to provide a general methodology for incorporating trajectory constraints into the qualitative simulation process.
The current trajectory specication language is, however, insucient to express certain constraints relevant to dynamical systems (e.g.
stability requirements for controllers).
Three dierent extensions to the language are currently being investigated.
Limited rst order expressiveness - The temporal logic used is limited to PLTL and is unable to quantify over attributes of states.
Certain trajectory constraints require the ability to refer to values across states within the behavior.
For example, the specication of a decreasing oscillation requires the ability to compare the magnitude of a variable across states.
A limited form of rst order logic may provide a suciently expressive language while still giving satisfactory performance with respect to complexity.
Metric temporal logic - Due to the introduction of landmarks during the simulation process, QSIM behaviors are potentially innite structures.
Getting a denite answer for formulae such as (eventually p) is not always possible when potentially innite behaviors are encountered since it is always possible for p to occur in the future.
Metric temporal logic [1] allows the denition of a horizon for a temporal logic expression.
This would allow statements such as \within 50 seconds the tanks level reaches 70 inches."
This statements is only expressible within our logic using an external predened event.
Such an extension oers the modeler more exibility to express relevant constraints.
Functional envelopes - Semi{quantitative reasoning [3] within TeQSIM uses interval bounds and static functional envelopes for monotonic functions to derive quantitative information about a behavior.
NSIM [9] derives dynamic envelopes describing a variable's behavior with respect to time.
Currently, only interval information can be specied within TeQSIM trajectory constraints.
Extending the language to include information about bounding envelopes with respect to time would increase the precision of solutions computed by TeQSIM.
Finally, the current algorithm for incremental model checking is inecient if compared to the on{ the{y model checker algorithm developed by Bhat and colleagues [4].2 We plan to incorporate it within TeQSIM.
6 Conclusions  Qualitative simulation and temporal logic provide two alternative formalisms for reasoning about change  2 In all the examples we have run so far, the practical time{ complexity of a TeQSIM simulation is denitely dominated by other operations(like quantitativeinferences)rather than model checking.
across time.
TeQSIM integrates these two paradigms by incorporating trajectory information specied via temporal logic into the qualitative simulation process.
Behaviors that do not model the set of temporal logic expressions are ltered during simulation.
Numeric information specied within the TL expressions can be integrated into the simulation to provide a more precise numerical description for the behaviors which model these expressions.
The correctness of the TL{guide algorithm along with the correctness of QSIM guarantee that all possible trajectories of the modeled system compatible with the QDE, the initial state and the trajectory constraints are included in the generated behaviors.
In addition, the completeness of TL{guide ensures that all behaviors generated by TeQSIM are potential models of the trajectory constraints specied by the modeler.
Acknowledgments  We would like to thank Benjamin Shults for letting us use part of his program to implement TeQSIM, and to the Qualitative Reasoning Group for many fruitful discussions.
Thanks also to a careful anonymous referee.
QSIM and TeQSIM are available for research purposes via anonymous ftp at ftp.cs.utexas.edu in the directory /pub/qsim.
These and other results of the Qualitative Reasoning Group are accessible by World-Wide Web via http://www.cs.utexas.edu/users/qr.
This work has taken place in the Qualitative Reasoning Group at the Articial Intelligence Laboratory, The University of Texas at Austin.
Research of the Qualitative Reasoning Group is supported in part by NSF grants IRI-9216584 and IRI-9504138, by NASA grants NCC 2760 and NAG 2-994, and by the Texas Advanced Research Program under grant no.
003658-242.
References  [1] R. Alur and T. Henzinger.
Real{time logics: complexity and expressiveness.
Information and Computation, 104(1):35{77, 1993.
[2] M. Barbeau, F. Kabanza, and R. St-Denis.
Synthesizing plant controllers using real-time goals.
In Proc.
of IJCAI{95, pages 791{798.
IJCAI, Morgan Kaufman, August 1995.
[3] D. Berleant and B.J.
Kuipers.
Using incomplete quantitative knowledge in qualitative reasoning.
In Proc.
of the Sixth National Conference on Articial Intelligence, pages 324{329, 1988.
[4] G. Bhat, R. Cleaveland, and O. Grumberg.
Ecient on{the{y model checking for CTL*.
In Proc.
of Conference on Logic in Computer Science (LICS{95), 1995.
[5] G. Brajnik and D. J. Clancy.
Temporal constraints on trajectories in qualitative simulation.
Technical Report UDMI{RT{01{96, Dip.
di Matematica e Informatica, University of Udine, Udine, Italy, January 1996.
[6] E.A.
Emerson.
Temporal and modal logic.
In J. van Leeuwen, editor, Handbook of Theoretical Computer Science, pages 995{1072.
Elsevier Science Publishers/MIT Press, 1990.
Chap.
16.
[7] A. Farquhar and G. Brajnik.
A semi{quantitative physics compiler.
In Tenth International Conference on Applications of Articial Intelligence in Engineering, Udine, Italy, July 1995.
Presented also at the Eighth International Workshop on Qualitative Reasoning on Physical Systems, 1994, Nara, Japan.
[8] D. Jonescu and J.Y.
Lin.
Optimal supervision of discrete event systems in a temporal logic framework.
IEEE Transactions on Systems, Man and Cybernetics, 25(12):1595{1605, Dec. 1995.
[9] H. Kay and B.J.
Kuipers.
Numerical behavior envelopes for qualitative models.
In Proc.
of the Eleventh National Conference on Articial Intelligence.
AAAI Press/MIT Press, 1993.
[10] B.J.
Kuipers.
Qualitative Reasoning: modeling and simulation with incomplete knowledge.
MIT Press, Cambridge, Massachusetts, 1994.
[11] B.J.
Kuipers and B. Shults.
Reasoning in logic about continuous change.
In J. Doyle, E. Sandewall, and P. Torasso, editors, Principles of Knowledge Representation and Reasoning, San Mateo, CA, 1994.
Fourth International Conference (KR{94), Morgan Kaufmann.
[12] B. Shults and B. J. Kuipers.
Qualitative simulation and temporal logic: proving properties of continuous systems.
Technical Report TR AI96{244, University of Texas at Austin, Dept.
of Computer Sciences, January 1996.
[13] J.G.
Thistle and W.M.
Wonham.
Control problems in a temporal logic framework.
International Journal on Control, 44(4):943{976, 1986.
Efficient Aggregation over Moving Objects* Peter Revesz Yi Chen Computer Science and Engineering Department University of Nebraska-Lincoln Lincoln, NE68588, USA {revesz,ychen}@cse.unl.edu Abstract We study two types of aggregation queries over a set S of moving point objects.
The first asks to count the number of points in S that are dominated by a query point Q at a given time t. The second asks to find the maximum number of points in S that are dominated by a query point at any time.
These queries have several applications in the area of Geographic Information Systems and spatiotemporal databases.
For the first query and any fixed [?
]dimension d, we give two different solutions, one using O( N ) time and O(N ) space and another using O(log N ) time and O(N 2 ) space, where N is the number of moving points.
When each of the points in S is moving piecewise linearly along the the same line and the total number [?]
of pieces is O(N ), then we can do the count query in O( N ) time and O(N ) space.
For the second query, when all objects move along the xaxis we give a solution that uses O(log N ) time and O(N 2 ) space in the worst case.
Our solutions introduce novel search structures that can have other applications.
1.
Introduction Aggregation operators are frequently used in database queries.
The efficiency of database queries with aggregate operators is well understood and studied in the context of traditional relational data.
However, aggregation operators are also important for more complex data that cannot be represented in relational databases.
Example 1.1 Suppose that a large company has a number of manufacturing plants P1 , P2 , P3 , .
.
.. Each plant produces four different products X1 , X2 , X3 and X4 .
The profit at each plant for each product changes over time as shown in Table 1.1.
* This research was supported in part by NSF grant EIA-0091530 and a Gallup Research Professorship.
Table 1.
Profits for various plant and product combinations.
Id 1 2 3 4 5 .. .
X1 t + 2t + 10 t3 - 8t - 10 t2 - 50 t4 - 16 t3 + 81 .. .
2  X2 80 10t 3t 7t 4t .. .
X3 t + 30 t2 - 2t 5t - 10 5t2 3 t - 21 .. .
X4 5t - 10 t3 - 3t + 4 t - 10 t - 30 t + 10 .. .
T t t t t t .. .
The company has the opportunity to buy a new plant Q where profits are rising rapidly.
The board of directors would approve the buy only if five years from now Q will be more profitable for each product than 10 of the current plants.
In this case, the input relations P (Id, X1 , X2 , X3 , X4 , T ) and Q(X1 , X2 , X3 , X4 , T ) form a constraint database [10, 12, 16].
Therefore, we can find out how many plants are less profitable in 2007 by the following SQL query: select count(Id) from P, Q where P.X1 < Q.X1 and P.X2 < Q.X2 and P.X3 < Q.X3 and P.X4 < Q.X4 and P.T = 2007 and Q.T = 2007; Suppose that the company has a long-term plan to eliminate all products except X1.
Therefore, the board of directors gives an approval for the purchase plan subject to the following extra condition: Q should have the potential to some day be more profitable on product X1 than 20 of their current plants.
We can find out the maximum number  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  of plants that will be less profitable than Q by the following SQL query: select count(Id) from P, Q where P.X1 < Q.X1 and P.T = Q.T group by T having count(Id) >= all (select count(Id) from P, Q where P.X1 < Q.X1 and P.T group by T);  taking pictures of the ground, which is represented as the rectangular area in Figure 1.
Given a time instance, find out how many cars will be covered in the picture at that time.
= Q.T  While Example 1.1 can be extended to any higher dimension, many practical aggregation queries use only 1, 2 or 3-dimensional moving objects.
Example 1.2 Consider a set of ships moving on the surface of the ocean.
The locations of these ships are known by an enemy submarine which moves secretly underwater at constant depth.
If the submarine fires, it calls attention to itself.
Hence the submarine wants to wait until the maximum number of ships are within its firing range (which is some rectangle with the submarine in the center) before firing at as many ships as possible.
Let us assume that we have the relations Ship(Id, X, Y, T ) and Range(X, Y, T ), which describe the ships and the firing range of the submarine, respectively.
A ship is in the firing range at a time instance if its (X, Y ) location is equal to a point in the Range at the same time instance.
Hence the above can be expressed by the following SQL query using a maximum aggregation operator.
select max(ship-count)) from (select count(Id) as ship-count from Ship, Range where Ship.X = Range.X and Ship.Y = Range.Y and Ship.T = Range.T group-by T); There are many alternatives to express in SQL the same query.
For example, the above SQL query could be also expressed in by another SQL query that has a structure similar to the second SQL query in Exercise 1.1.
It is a practical problem to recognize that these different SQL structures both express max-count queries, which we will define below.
In this paper, we do not deal with the parsing problem.
Example 1.3 We show in Figure 1 three cars driving along three path, which can be represented by piecewise linear constraints.
We assume each car travels at a constant speed in each line segment.
Assume a plane flying in the air keeps  Figure 1.
Aggregations on piecewise linearly moving points  Examples 1.1 and 1.2 are both cases of a group of frequently occurring aggregation problems where the input data can be visualized as a set S of N number of kdimensional moving points.
In Example 1.1 each point represents one plant and the value of the ith dimension represents the profit of the i-th product at that plant.
In Example 1.2 each point represents one ship in 2-dimensions using latitude and longitude.
In Example 1.3, the speed and direction of the cars change as they enter new line segments, but the movement can still be represented by piecewise linear constraints.
We say that point Pi dominates point Pj if and only if Pi has a larger value than Pj has for each dimension.
Then the queries in Examples 1.1 and 1.2 can be generalized as follows: Count: Given a moving point Q and a time instance t, find the number of points in S dominated by Q at time t. Max-Count: Given a moving point Q, find the maximum number of points in S that Q can dominate at any time.
In this paper we focus only on the above two aggregation queries, because several other more complex aggregation queries can be reduced to them or can be solved similarly.
For example: Range-Count: Given a time instance t and two moving points Q1 and Q2 , find the number of points in S located in the hyper-rectangle defined by Q1 and Q2 .
(This reduces to a sequence of count queries.)
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  Max-Time: Given a moving point Q and time instance t, find out whether Q dominates at time t the maximum possible number of points in S. (This reduces to testing whether the results of a count and a max-count query are the same.)
Sum: Assign a value to each moving point.
Then given a moving point Q and time instance t, find the sum of the values of the points in S dominated by Q at time t. (This requires only minor changes in the index structures that we develop for count queries.)
Aggregation queries can be evaluated in O(N log N ) time and O(N ) space (see Appendix).
However, this performance is not acceptable in applications where the input data is large and the query evaluation speed is critical, like in Example 1.2.
The goal of this paper is to develop novel indexing structures that can greatly speed up count and maxcount aggregate query evaluation.
There are some indexing structures for moving objects [1, 2, 5, 11, 17].
One may use these indices to answer the count and the range-count query by first finding the set of points S  [?]
S dominated by a new point Q or being within a hyper-rectangle defined by Q1 and Q2 , and then counting the number of points in S  .
However, the counting may require linear time in the size of S  .
Our goal is to find the count in logarithmic time.
Further, these indices cannot be used to answer the max-count and max-time queries.
As shown by Zhang et al.
[20], if we have a static set of points, then the range-count problem can be solved by generalizing some earlier work on dominance by Bentley [3].
Zhang and Tsotras [19] also considered the max-count aggregation problem for static sets of points in S. However, these methods are not easily generalizable to moving points, which is our focus in this paper.
Lazaridis and Mehrotra [13] , Choi and Chung [6] and Tao et al.
[18] study the approximation of aggregate queries for spatial and spatiotemporal data.
In contrast to them, our algorithm produce precise answers without a significant loss in performance.
This paper is organized as follows.
In Section 2, we review some basic concepts, including partition trees for indexing moving objects proposed by Agarwal et al.
[1].
In Section 3, we consider two different methods for answering count aggregation queries.
The first method extends partition trees, to partition aggregation trees.
The second method uses a novel data structure called dominance-time graphs.
Dominance-Time graphs are faster than partition aggregation trees and they can also be used when the position of moving points are represented by polynomial functions of time.
In Section 4 we consider max-count aggregation queries.
Finally, in Section 5 we discuss some open problems.
Our main results are summarized in Table 2,  Table 2.
Computational complexity of aggregation on moving objects.
Query Count Count Max  I/O [?]
N log N log N  S  D  Function  Method  N N2 N2  d d 1  linear polynomial linear  PA tree DT graph Dome subdiv  where D means dimensions and S means space requirements.
2.
Basic Concepts We review two basic concepts.
Duality [8] allows mapping k-dimensional moving points into 2k-dimensional static points.
Partition Trees proposed by Agarwal et al.
[1] are search trees for moving points.
2.1 Duality Suppose the positions of the moving points in each dimension can be represented by linear functions of time of the form f (t) = a*t+b, which is a line in the plane.
We may represent this line as a point (a, b) in its dual plane.
Similarly, a point (c, d) can be represented as a line g(t) = c*t+d in its dual plane.
Suppose line l and point P have dual point L and dual line p respectively.
Then, l is below P if and only if L is below p .
Lemma 2.1 Let P = aP *t+bP and Q = aQ *t+bQ be two moving points in one dimensional space, and P  (aP , bP ) and Q (aQ , bQ ) be their corresponding points in the dual plane.
Suppose P overtakes Q or vice versa at time instance t, then bP - bQ t=- aP - aQ Let Slope(P  Q ) denote the slope of the line P  Q .
Then we have t = -Slope(P  Q ), that is, t is equal to the negative value of the slope of the line P  Q .
Hence, given a time instance t, the problem of finding how many points are dominated by Q reduces to the problem of finding how many points are below l, where l is a line crossing Q in the dual plane with the slope -t. Definition 2.1 Let S be a set of N points and l be a line in the plane.
We define the function CountBelow(l) as follows.
If l is a vertical line with r1 points on the left and r2 points on the right, then CountBelow(l) = max(r1 , r2 ).
Otherwise, if r number of points are below l, then CountBelow(l) = r.  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  [?]
we can answer the count query in O( N ) time.
Then we describe a more novel data structure, called an dominancetime graph, that needs only logarithmic time.
l1  3.1 Partition Aggregation Trees l2  (A)  (B)  Figure 2.
Rank of a line.
Note that Definition 2.1 is logical, because if l is a vertical line, then we can always tilt it slightly left or right to get another line that has the same value of CountBelow as we defined.
Example 2.1 Figure 2 shows a set of points and two lines l1 and l2 .
There are four points below l1 , hence CountBelow(l1 ) = 4.
There are five points to the left and one point to the right of l2 , which is a vertical line.
Hence CountBelow(l2 ) = 5.
2.2 Partition Trees Given a set S of N points in two dimensional space, we represent a simplicial partition of S as P = {(S1 , [?
]1 ), (S2 , [?
]2 ), ..., (Sm , [?
]m )}, where Si 's are mutually disjoint subsets of S whose union is S, and [?
]i is a triangle that contains all points of Si .
For a given parameter r, 1 <= r < N , we say this simplicial partition is balanced if each subset Si contains between N/r and 2N/r points.
Figure 3(A) shows an example of balanced simplicial partition for 35 points with r = 6.
The crossing number of a simplicial partition is the maximum number of triangles crossed by a single line.
The following is known about crossing numbers: Theorem 2.1 (Matousek[14]) Let S be a set of N points in the plane, and let 1 < r <= N/2 be a given parameter.
For some constant a (independent of r), there exists a balanced simplicial partition P of size r, such that any line crosses at most cr1/2 triangles of P for a constant c. If r <= N a for some suitable a < 1, P can be constructed in O(N log r) time.
Using Theorem 2.1, it is possible to recursively partition a set of points in the plane.
This gives a partition tree.
3.
Count Aggregation Queries In this section, we first make a simple extension of partition trees, described in Section 3.1.
With the modification,  Definition 3.1 Let S be a set of N points in k dimensional space and T be a multi-level partition tree for S. Let vi be an internal node in T , which stores a triangle [?
]i .
We attach a new value Ai to node vi , such that Ai is the number of points in Si .
We call the new tree structure Partition Aggregation Tree (PA Tree).
Theorem 3.1 PA Tree is a linear [?]
size data structure that answers the count query in O( N ) I/Os.
Example 3.1 Figure 3(B) shows a partition tree with four top level triangles A, B, C and D. The query line q crosses two top level triangles A and B.
There are three second level triangles A4, B2 and B3 that are crossed by q.
Figure 3(C) shows the structure of the PA-tree.
For simplicity, we only show for each node the triangle name and the count of the points contained in that triangle.
To find CountBelow(q), we start from the root of the PA-tree, load all top level triangles into memory and compare them to the query line q.
Since both triangles C and D are below the line, we add the precomputed value to the result CountBelow(q) = 12 + 17 = 29.
For the triangles A and B, we traverse their children recursively.
In this case, triangle B4 is below q, then we have CountBelow(q) = CountBelow(q) + CountIn(B4) = 29 + 4 = 33, where CountIn(B4) is the number of points in the subset associated with B4.
When we reach the leaf nodes of the PA-tree, we compare each point in the node with q, and add the number of points below q.
There is one point in triangle B3 that is below q.
Finally, the answer to the aggregation problem is 34.
In Figure 3(C), we indicate using double sided rectangles those nodes that are accessed by this algorithm.
In Example 1.3, the movement of a car can be represented by piecewise linear functions.
When the direction or speed changes, we may consider the car to be replaced by a new car with different direction or speed.
We have the following theorem for the piecewise linearly moving points in one dimensional space: Theorem 3.2 Let S be a set of piecewise linearly moving points with N number of pieces in one dimensional space.[?
]The dominance-sum problem of S can be answered in O( N ) I/Os with O(N ) space.
The above talks about one dimensional space.
That may occur when each car is going on a straight highway, but each car may slow down in certain intervals due to road construction or heavy traffic, and they change direction only if they  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  A  B  A2  B1  A3  B2 q  A1 B3 B4  A4  C C1 C2  D1 D2  D  C3 (A) D3 D4 (B)  S  A1 5  A  16  A2 4  A3 4  B  A4 3  61  16  C  B1 B2 B3 B4 4 4 4 4  12  C1 C2 C3 4 4 4  D 17  D1 D2 D3 D4 4 4 4 5  (C)  Figure 3.
A partition aggregation tree.
make U-turns.
It is an open problem to find a similarly efficient solution for two or higher dimensional space.
true for time instance t that is within any of the open intervals.
Note that any real number and -[?]
and +[?]
are allowed as interval endpoints.
3.2 Dominance-Time Graph Partition aggregation trees are limited because they only work when the points are moving linearly.
In this section we introduce dominance-time graphs, a novel index data structure that can handle polynomial functions of time.
Definition 3.2 For two k-dimensional moving points P = (f1 , ..., fk ) and Q = (g1 , ..., gk ), we say P dominates Q at time t, denoted as dom(P,Q,t), if and only if fi (t) > gi (t) for 1 <= i <= k. If P does not dominate Q at time t, then we write ndom(P,Q,t).
Definition 3.3 Let S be a set of N moving points in k dimensional space.
The dominance-time graph G(V, E) for S is a directed labeled graph, such that for each point in S, there exists a corresponding vertex in V , and there is an edge in G from P to Q labeled by the set of disjoint intervals {(a1 , b1 ), ..., (am , bm )}, if and only if dom(P, Q, t) is  Example 3.2 Suppose that we are given the following set of two dimensional moving points: P1 = (t + 10, t - 5) P2 = (2t, 2t - 10) P3 = (3t + 5, 3t - 15) P4 = (4t - 5, 0) The dominance-time graph of these moving points is shown in Figure 3.
Note that for any time instance t [?]
(5, 10) the condition dom(P3 , P4 , t) is true.
Hence the edge from P3 to P4 is labeled {(5, 10)}.
The labels on the other edges can be found similarly.
Definition 3.4 Let P and Q be two moving points and t0 and t be two time instances such that t0 < t. We say that  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  P2  , 5)  )  8  (-  , -5)  8  (5,+  , 2.5)  8  (-  (-  8  P1  )  8  (10, +  (2.5, 5) )  8  (5, +  P3  P4 (5, 10) Figure 4.
A dominance-time graph.
between t0 and t an increment event happens to P with respect to Q if ndom(P, Q, t0 ) and dom(P, Q, t).
Similarly, we say that between t0 and t a decrement event happens to P with respect to Q if dom(P, Q, t0 ) and ndom(P, Q, t).
Definition 3.5 Let Rank(P,t) be the number of points that are dominated by P at time t. Lemma 3.1 An increment event happens to P with respect to Q if and only if there is an outgoing edge from P that has a label in which no interval contains t0 and some interval contains t. Similarly, a decrement event happens to P with respect to Q if and only if there is an outgoing edge from P that has a label in which some interval contains t0 and no interval contains t. Lemma 3.2 Let t0 and t be two time instances such that t0 < t. Let P be any vertex in a dominance-time graph.
Let m (and n) be the number of increment (and decrement) events that happen to P with respect to different other vertices between t0 and t. Then the following is true: Rank(P, t) = Rank(P, t0 ) + m - n Example 3.3 Table 3 shows the rank of each point of Example 3.2 at time instances t = -8 and t = 12.
Note that dom(P2 , P3 , -8) and ndom(P2 , P3 , 12) are both true.
Hence, an increment event happened to P2 between time t = -8 and t = 12.
Similarly, ndom(P2 , P1 , -8) and dom(P2 , P1 , 12) are also both true.Hence a decrement event happens to P2 between the same times.
Thus, according to Lemma 3.2, we have Rank(P2 , 12) = Rank(P2 , -8) + 1 - 1 = 1  Table 3.
Location and rank of points at times t = -8 and t = 12.
Point P1 P2 P3 P4  Location t = -8 (2, -13) (-16, -26) (-19, -39) (-37, 0)  Rank t = -8 2 1 0 0  Location t = 12 (22, 7) (24, 14) (41, 21) (43, 0)  Rank t = 12 0 1 2 0  3.3 Time and Space Analysis In this section we describe the basic structure of dominance-time trees and show how to use them to answer count aggregation queries in O(log mN ) I/Os, where N is the number of moving points and m is the maximum degree of the polynomial functions used to represent the position of the points.
A dominance-time tree for point P is a B-tree to index the consecutive time intervals: (-[?
], t1 ), (t1 , t2 ), .
.
.
, (ti , ti+1 ), .
.
.
, (tn , +[?])
such that during each interval (ti , ti+1 ), the rank of P remains unchanged.
The rank of P during these intervals and the ti endpoints of these intervals can be precomputed and stored in the B-tree.
Therefore, the B-tree can find the rank of P for any time instance in (-[?
], +[?]).
Let S be a set of N moving points.
For any point P in S, we may compute (precisely for polynomials up to degree 5  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  Theorem 3.3 Let S be a set of N moving points in kdimensional space.
Let m be a fixed constant and assume that the position of each moving point in each dimension is represented by a polynomial function of degree at most m. Given a point P in S and a time instance t, the DominanceTime Tree for each P [?]
S requires O(N ) space.
Hence the count aggregation problem can be done in O(logB N ) I/Os using a total of O(N 2 ) space.
e1 e2 e3 e4 5  9  18 22  30 35  t  Figure 5.
A time line.
and approximately for higher degree polynomials) a set of n time instances ti (1 <= i <= n) such that during each interval (ti-1 , ti ) the rank of P remains unchanged.
Example 3.4 Suppose in a dominance-time graph, there are four outgoing edges, e1 , e2 , e3 and e4 for a point P .
They are labeled as the following respectively: e1 : (5, 18), (22, 35) e2 : (9, 30) e3 : (0, 9), (22, +[?])
e4 : (0, 22) Figure 5 shows the intervals contained in the labels with thick line segments.
In this case, the B-tree contains the time instances 0, 5, 9, 18, 22, 30, 35 and the following time intervals: (-[?
], 0),(0, 5),(5, 9),(9, 18),(18, 22), (22, 30),(30, 35), (35, +[?])
Definition 3.6 Suppose G is a dominance-time graph for a set of moving points and P is a vertex in G. A DominanceTime Tree TP is a data structure based on a B-tree, which indexes all end points of time intervals contained in the labels of outgoing edges from P .
The leaf node of the dominance-time tree contains a list of consecutive time instances, t1 , t2 , ..., tb , and b + 1 data fields v1 , v2 , ..., vb+1 where b is chosen according to the size of the disk pages.
For each field vi for 1 <= i <= b we store the precomputed rank of P during the interval (ti-1 , ti ).
Given a time instance t, the rank of P can be found by searching the dominance-time tree until we find the leaf node with the interval that contains t. Now we can prove the following.
The preprocessing of the dominance-time tree structure involves computation of polynomial functions.
However, for a moving point which is represented by a polynomial function, it is not difficult to use piecewise linear functions to approximately represent its trajectory.
Using this approximation method, the number of time intervals when the rank of a particular point remain unchanged will remain unchanged.
4.
Max-Count Aggregation Queries Our max-count aggregation algorithm uses a novel data structure built on the concept of domes, which we introduce here as a new type of spatial partition of the dual plane of a set of one-dimensional moving points.
We start this section with a few definitions.
Definition 4.1 Let S be any set of points in the plane.
For any new point Q, we define MaxBelow(Q) to be the maximum number of points below any line that passes through Q.
Definition 4.2 Let S be any set of points in the plane.
Let L be the set of lines that cross at least two points in S or cross at least one point in S and are vertical.
For 0 <= i <= N , we define Li = {l [?]
L|CountBelow(l) + CountOn(l) >= i}, where CountOn(l) is the number of points in S crossed by line l. Definition 4.3 For any line l, let Below(l) be the halfplane below l, or if it is a vertical line, then the half-plane on that side of the line that contains more points.
Let Below(Lk ) be the intersection of the half-planes associated with the lines in Lk .
Let k-dome, denoted as dk , be the boundary of the region Below(Lk ).
The intuition is that any point above dk has a line through it with at least k points below.
Definition 4.4 Layer(k)= {Q|Q [?]
Below(Lk+1 ) and Q [?]
Below(Lk )}.
Example 4.1 We show in Figure 6 a set of seven points.
In this case, L7 is composed of the dotted lines (i.e., the lines crossing P2 P3 , P3 P4 ,P4 P5 and the two vertical lines  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  crossing P2 and P5 ), while L6 is composed of the union of the dotted and dashed lines (i.e, the lines crossing P2 P7 , P3 P5 , P4 P6 , P4 P7 and the two vertical lines crossing P3 and P6 ).
The two thick polygonal lines in the figure are d7 and d6 , respectively, and Layer(6) is the area between them.
Now we prove some properties of the above concepts.
Lemma 4.1 For any i and j such that i <= j, the following hold.
(1) Li [?]
Lj .
(2) Below(Li ) [?]
Below(Lj ).
(3) no point of dome di is above any point of dome dj .
Lemma 4.2 Layer(k) consists of those points that are strictly outside dk and on or inside dk+1 .
Lemma 4.3 Each point belongs to only one layer.
We can now show the following characterization of layers.
Theorem 4.1 Q [?]
Layer(m) - M axBelow(Q) = m. Theorem 4.1 implies that the layers partition the plane in such a way that there is a one-to-one correspondence between any element of the partition and the M axBelow value of the points in that element.
We can use this theorem to build a data structure for efficiently identifying which element of the partition a new point is located in, using the following well-known result from computational geometry.
Theorem 4.2 [15] Point location in an N-vertex planar subdivision can be effected in O(log N ) time using O(N ) storage, given O(N log N ) preprocessing time.
Lemma 4.4 Any dome dk has O(N ) edges.
Lemma 4.5 Let S be any set of N points in the plane and Q a query point.
Then we can find in O(log N ) time using an O(N 2 ) space data structure M axBelow(Q) = m. Lemma 4.6 Let S be a set of N points and Q a query point moving along the x axis.
Let S  and Q be the duals of S and Q, respectively.
Then the following hold.
(1) For any time instance t the moving point Q dominates CountBelow(l) number of points in S, where line l crosses Q and has slope -t. (2) The maximum number of points that Q dominates is M axBelow(Q ).
Finally, we have the following theorem.
Theorem 4.3 The Max-Count aggregation query can be answered using an O(N 2 ) size data structure in O(log N ) query time and O(N 2 log N ) preprocessing time.
The above considers only objects that exist at all times.
Suppose that objects only exist between times t1 and t2 .
That means that only lines passing Q and having slopes be(t ,t ) tween -t2 and -t1 are interesting solutions.
Let Li 1 2 be the modification of Li that allows only lines that have slopes between -t2 and -t1 and cross two or more points or cross only one point and have slopes exactly -t2 or -t1 .
With this modification, we can correspondingly modify the definition of layers.
Then Theorems 4.1 and 4.3 still hold.
5.
Further Work There are several interesting open problems.
We list below a few of these.
1.
Are there count or max-count aggregation algorithms that are more efficient in time or space than our algorithms, or can a tight lower bound be proven for these aggregation problems?
2.
Can the count aggregation algorithm for piecewise linear moving points in one dimension be [?]
extended to higher dimensions while keeping the O( N ) time and O(N 2 ) space in the worst case?
3.
Can the max-count aggregation algorithm in one dimension be extended to higher dimensions while keeping the O(log N ) time and O(N 2 log N ) space in the worst case?
4.
How can we make the data structures dynamic, that is, allow efficient deletions and additions of new moving points?
We have partial solution to this problem when only insertions are considered.
5.
What is the average case of the count and max-count algorithms?
6.
Can the algorithms be improved by considering approximations?
As described in Section 1, approximations for the count aggregation query were considered in the work of [13, 6, 18].
However, there are no approximation algorithms for the max-count aggregation problem.
7.
Moving objects can be represented not only by moving points but also by parametric rectangles [4], by geometric transformation objects [7, 9], or by some other constraint representation [12, 16].
These constraint representations are more general because they also represent the changing (growing, shrinking) shape of the objects over time.
It is possible to consider count and max-count aggregation queries on these more general moving objects.
Is it possible to solve these queries within the same time complexity?
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  P4 P3 P5  P7 P2 P6  P1 layer(6)  d6  d7  Figure 6.
Layer(6) for seven points.
We are also interested in implementations of these algorithms and testing them on real data, for example, aviation data sets, and truck delivery data sets.
References  International Converence on Management of Data, pages 57-64, 2000.
[6] Y.-J.
Choi and C.-W. Chung.
Selectivity estimation for spatio-temporal queries to moving object s. In SIGMOD, 2002.
[1] P. K. Agarwal, L. Arge, and J. Erickson.
Indexing moving points.
In Symposium on Principles of Database Systems, pages 175-186, 2000.
[7] J. Chomicki and P. Revesz.
A geometric framework for specifying spatiotemporal objects.
In Proc.
International Workshop on Time Representation and Reasoning, pages 41-6, 1999.
[2] J. Basch, L. J. Guibas, and J. Hershberger.
Data structures for mobile data.
In SODA: ACM-SIAM Symposium on Discrete Algorithms (A Conference on Theoretical and Experimental Analysis of Discrete Algorithms), 1997.
[8] M. de Berg, M. van Kreveld, M. Overmars, and O. Schwarzkopf.
Computational Geometry: Algorithms and Applications.
Springer Verlag, Berlin, 1997.
[3] J. L. Bentley.
Multidimensional divide-and-conquer.
Communications of the ACM, 23(4), 1980.
[9] S. Haesevoets and B. Kuijpers.
Closure properties of classes of spatio-temporal objects under Boolean set operations.
In Proc.
International Workshop on Time Representation and Reasoning, pages 79-86, 2000.
[4] M. Cai, D. Keshwani, and P. Revesz.
Parametric rectangles: A model for querying and animating spatiotemporal databases.
In Proc.
7th International Conference on Extending Database Technology, volume 1777, pages 430-44.
Springer-Verlag, 2000.
[5] M. Cai and P. Revesz.
Parametric r-tree: An index structure for moving objects.
In Proc.
10th COMAD  [10] P. C. Kanellakis, G. M. Kuper, and P. Z. Revesz.
Constraint query languages.
Journal of Computer and System Sciences, 51(1):26-52, 1995.
[11] G. Kollios, D. Gunopulos, and V. J. Tsotras.
On indexing mobile objects.
In ACM Symp.
on Principles of Database Systems, pages 261-272, 1999.
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  [12] G. Kuper, L. Libkin, and J. Paredaens.
Constraint Databases.
Springer Verlag, 2000.
[13] L. Lazaridis and S. Mehrotra.
Progressive approximate aggregate queries with a multi-resolution t ree structure.
In SIGMOD, 2001.
[14] J. Matousek.
Efficient partition trees.
Discrete Comput.
Geom., 8:315-334, 1992.
[15] F. P. Preparata and M. I. Shamos.
Computational Geometry: An Introduction.
Springer Verlag, New York, 1985.
[16] P. Revesz.
Introduction to Constraint Databases.
Springer Verlag, 2002.
[17] S. Saltenis, C. S. Jensen, S. T. Leutenegger, and M. A. Lopez.
Indexing the positions of continuously moving objects.
In SIGMOD Conference, pages 331-342, 2000.
[18] Y. Tao, J.
Sun, and D. Papadias.
Selectivity estimation for predictive spatio-temporal queries.
In ICDE, 2003.
[19] D. Zhang and V. J. Tsotras.
Improving min/max aggregation over spatial objects.
In ACM-GIS, pages 88-93, 2001.
[20] D. Zhang, V. J. Tsotras, and D. Gunopulos.
Efficient aggregation over objects with extent.
In Symposium on Principles of Database Systems, pages 121-132, 2002.
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE
ficha MOKHTARI Institut dInformatique USTHB BP 32 El Alia Alger Algdrie mokhtari @ist .ibp.dz  Daniel KAYSER LIPN URA 1507 du CNRS Institut GalilCe Universitk Paris-Nord 93430 Villetaneuse France Daniel.Kayser@ural507.univ-paris 13.fr  Abstract  inferences can be achieved without ever making that choice (see e.g.
[4]), we adopt in this paper a pointbased point of view, for reasons which will appear in the next section.
However, it is likely that an interpretation in terms of intervals of what we call below time points would not change drastically the core of our approach.
The next section motivates our choices, in order to satisfy the kind of reasoning we wish to capture.
We then go on to provide a formalism intended to link an incomplete description with any normal course of the world: we give preliminary notations and definitions, then describe a theory of action, and compare it with related works.
We conclude with a discussion about current and future work.
This paper discusses the temporal aspect of a causal theory based on an "interventionist" conception of causality, i.e.
a preference to select causes among a set of actions which an agent has the ability to perform or not to perform (free will).
Casting causal reasoning in this framework leads to explore the problem of reasoning about actions, generally considered as a nonmonotonic temporal reasoning.
Most of the works on nonmonotonic temporal reasoning have used simple temporal ontologies, such as situation calculus, or temporal logic with discrete time.
The theory presented in this paper also has a simple temporal ontology based on "time points" organized on branching "time lines ", with the possibility of modelling continuous evolutions of the world for various fitures (prediction) or pasts (diagnostic).
2.
Actions, effects, and time points Actions and effects will be the only temporal propositions considered in our framework.
An effect can be, among other things, an event or a fact.
But as argued in [13], a fine distinction is unnecessary.
In order to introduce temporal aspects, some choices must be made.
The first one concerns the basic temporal element: point or interval ?
Intervals can be related in various ways: "I1 is completely before 12"; "I1 abuts I2", "I1 overlaps I2", etc.[l].
All these relations are possible between a cause and an effect, but they are subsumed by the general principle, according to which effects never precede their causes; therefore we find it simpler to use only t i m e points.
We do not mean to reduce causation merely to a temporal relationship: what we present below shows the contrary.
Let us mention, in addition, that most approaches choose discrete sets, isomorphic to integers, to represent time.
In order to better reflect our intuitions on continuity, we prefer to take reals, but we do not consider this choice as critical,  1.
Introduction A definition of the concept of cause, if at all possible, would involve deep philosophical questions ; we do not need to tackle them, however, in order to use a practical notion of cause.
Intuitively, this notion is necessary in our everyday reasoning, both to anticipate what should happen if we decide to perform an action, and to diagnose what might have happened to yield a given state of affairs.
We propose to prune the collection of propositions which might be considered as causes by preferring to take as causes the result of the free will of an agent, i.e.
hidher ability to opt for performing or not performing a given action.
Casting causal reasoning in this framework leads to explore the problem of reasoning about actions, generally considered to be a nonmonotonic (i.e.
defeasible) temporal reasoning.
Temporal reasoning is said to require a choice among two ontologies : point-based or intervalbased.
Although we consider that many non-trivial  14 0-8186-7528/96 $5.00 0 1996 IEEE  the result of causal relations.
The possibility of having a branching past implies that the "interesting propositions" do not include "historical" statements, since otherwise different pasts could never lead to the same time point.
Having time points defined both by the subset of true propositions and by the date allows to distinguish several occurrcnces of the same state of affairs.
If we need to represent cyclic phenomena, where this distinction is useless, we may add an equivalence relation on time points: t l E t2 if they differ only by their date, and then reason on the quotient set.
The alternative, i.e.
define time points only by the subset of true propositions, does not allow to restore the notion of date when needed.
To extract the date of a time point, we define a function date : T I+ 93 which maps every time point on a real number representing its date.
To simplify, we write date(t) as dt.
especially as our examples need only to consider a finite number of time points.
Our second choice amounts to select either a linear or a branching model of time; time is intuitively linear.
However, as we deal with choice making, the very representation of a choice immediately suggests the use of branching time.
Among the time points, some particular ones, called choice points, are intended to represent states of affairs where an agent can take the decision of performing or not an action: obviously, not all time points are choice points, since the conditions allowing for the action are not always satisfied.
The decision of the agent is represented as a time line splitting into two futures, or more accurately, as two distinct time lines having the same time points up to the choice point.
This is consonant with most systems, which have a branching future [lo].
But we consider as well a branching past [14], because we often need to examine two different courses of events leading to the same situation.
We also allow time lines to meet again in the future (case of an action without long-range effects, for example).
Fig.
1 below will provide an illustration.
We now present more formally the general framework corresponding to our choices.
Definition 2: We call time line 1 (somehow similarly to McDermott's "chronicle" [IO]) a set of time points in bijection with the set of dates, meant to represent a possible evolution of the universe.
A time line hence conveys the complete evolution of the truth value of the "interesting propositions".
The time points of a time line are supposed to comply with the general principle: "there is no effect without a cause".
Their propositions are then the result of cause-effect relations governed by causal rules.
The set of causal rules is gathered in a rule base called BR.
The time points of a time line are totally ordered by a precedence relation written "I",where tKt2 means that time point t2 does not precede t l , hence whenever tlSt2 we have dtlSdt2, but the converse does not hold (see Fig.1).
3.
Temporal ontology Definition 1: A time point t is a "snapshot" state of the universe defined by a subset of true propositions at a certain date and by this date.
T is the set of time points.
The subset mentioned in the definition is not arbitrary: we sometimes refer to it as the set of "interesting propositions", i.e.
in our framework,  15  dO  d2  dl  Figure 1: the structure of branching time in the past and in the future.
The thick line represents a time line, I, including among others time points to, t i , t2.
The other curved lines represent other time lines.
tO c tl c t2 holds, as do tO < t"2 and t i < t'2, but there is no relation between t i and t"2, although dtl< d t y is true.
- L is the set of time lines, 4.
The language  -  3 is the set of real numbers, if t has exactly as  - t E 1 is true in the model  its true propositions the set I(l,dt), - v(p,l,dt) is true in the model iff p E Z(l,dt), i.e.
proposition p is true at the time point determined by time line 1 and date dt.
It follows that nocc(p,l,dt,A) is true in the model iff (b't') ((t'E1 A dtld&+A) p 6i Z(1,dtt)).
The proposed langage A!
is defined at two levels: * the first level is meant to represent static information.
It is a plain propositional language in which: P is a set of propositions we are interested in, A, subset of P, is a set of actions, and E, subset of P, is a set of effects, with A n E = 0 and A v E = P. the second level expresses dynamic information.
It contains predicates with time variables.
If p is a formula of the first level, I a time line, t E 1 a time point, a formula of the second level has the form: - v(p, I , d t ) with the intended meaning that formula p is true in 1 at the date of time point t, and: - nocc(p,l,d,A) with the intended meaning that p is never true in line 1 from the date of time point t on, during the delay A.
In other words, nocc(p,l,dt,A) is a short-hand for : (Vt') ((t'E I A O 2 d t d f < A) 3 -~V(p,Z,df))  =I  The dependence of the effect on the cause may vary according to the context.
We shall therefore introduce a subset of preferred time lines and augment the language with an operator denoted "a'' meaning normally implies (in a more comprehensive presentation, see [ 111, we also have an operator "-+" meaning implies in all cases).
The intuitive idea behind these two new notions is as follows: when an agent chooses to perform an action, he or she does not anticipate every possible outcome of his or her choice: several circumstances, unknown to, or neglected by, the agent at the moment of the choice, may alter the predictable effect of the action.
Informally, the "preferred" time lines are the futures that the agent normally "had in mind" when he or she opted to perform the action.
The effects which are present in all "preferred time lines" following an action are said to be "normally implied" by the action.
The idea of "normal implication" is inspired by the work of  We are going to extend gradually this language, but first let us define its semantics.
The associated model theory is a generalization of Kripke [8] possible world semantics.
In this model, an interpretation is defined as function I mapping the Cartesian product L X % into a subset of propositions, i.e.
Z : L X 93 I+ 2 p , where:  16  Delgrande [3].
We defer a more precise explanation until we introduce some more notions.
To make sense of the notion of "normality" requires to reason with uncertain information: in the absence of specific information, we are entitled to believe that things behave normally.
This brings us to a problem similar to the well-known "frame problem", which is inherent to any theory of change.
We must therefore take into account: the preconditions of an action a , i.e.
represent what is reasonable to assume whenever performing a is considered; the hindrances of an action a, i.e.
represent the effects of other actions that can inhibit the effects of a; the persistence of states, corresponding to the fact that some propositions continue to hold true for some duration, unless an external event entails their falsity.
All these aspects generally require the use of a nonmonotonic reasoning.
That is the reason why we devote next section to this issue.
time line, as we introduced it in 54.
Its definition requires the preliminary notion of coincidence, viz.
Definition 3: Two time lines  11 and 12 coincide up to time point t, property written coincide(ll,12,t) ifffor every time point t'preceding t, t' E 11 = t' E 12.
In other words, a model satisfies coincide (Zi, 12,t) iff (b't')(t'lt 2 Z(ll, dti)=Z(12,dtl))  Definition 4: The set of preferred time lines for line 1 ut time point t, noted Lp(1,dt) is a subset of L obtained by a function Lp :L X 3'I+ 2L such that: (Vl,l',t) (I' E Lp(l,dt) 3 coincide(l,l',t)) We are now in position to define formally normal implication:  Definition 5: Action a normally implies effect e within the delay A, noted a 3 e [A], ifs (Vl,t ) ( C l A C2) where C1 and C2 stand for the following conditions: C1 {tv(a,Wt) A (YP)(pcnorm(a)3 v ( p , W t ) ) l 3 { Vl') (l'sLp(1,dt)I> [(dt') (t'sl' A dtldt.ldt+A A v(e,l',dt~))v (3e',t") (e'EUR inhibit(e,a) A t"s1' A v(e',l ',dy)A dt.GQGit+A)l)} C2 { v ( - r a , l , d t ) I> (31') ( I ' E L p ( l , d t ) A noccte,l',dt,A))J  5.
Nonmonotonicity In [ l l ] , we show how to include implicit premises in a normal inference.
We suppose the existence of a function, called norm, to define the normal conditions under which an action is executed.
Technically, norm : A I+ 2p is such that for any action a , n o r m ( a ) contains those propositions (preconditions) which are true unless otherwise specified when an agent considers to perform a.
Extending the domain of norm to our "first-level" language, i.e.
defining compositionally norm (a op a') where op is a boolean operator is not a trivial task, if we want to remain compatible with our intuitions.
We will not treat this problem here.
As we saw the importance of defining "hindrances", we suppose similarly the existence of a function inhibit that, for any couple <e,a> where e is a normal effect of a, determines the events which are liable to prevent e from following a .
Technically, inhibit : E X A + 2E is such that inhibit(e,a) is the subset E' of E where e' E E' iff whenever e' occurs during the delay after a where e should turn true, e may actually remain false.
Notice that action a' causing e' may happen before, with, or after a.
Similarly, extending the domain of inhibit to couples of formulas instead of couples of atoms is a very thorny issue.
We turn now to what we mean by "normal case".
This notion is often attached to a preference ordering, but we notice that the definition of a socalled "correct order" is rather difficult; therefore, we find it more convenient to use the notion of preferred  This rather intricate definition calls for some explanations: C1 tells that whenever a occurs under normal conditions, in all preferred futures, either there exists a subsequent occurrence of event e within the delay A, or there is an occurrence of event e' known to inhibit the effect of a ; e' must then occur after t within the prescribed delay (notice that, even in this case, e may become true); C2 checks that if a is not executed, there exists at least one preferred future in which e will not occur within the specified delay A.
This condition reflects the implicit counterfactuality always present in causation: we are not ready to say that a normally implies e if we think that, even without performing a, e will nevertheless occur in every likely future.
We now turn to the last problem related with the "frame problem", namely persistence.
Example 1: Suppose that the following facts and rule are given.
They represent the well-known "Yale Shooting Problem" (Y.S.P.)
according to our no tation: - v(Fred is alive,l,dto) - v(gun is loaded,l,dtl)  17  one in which the action is performed, the other in which it is not.
The free will of the agent is exactly histher ability to choose which of these two lines will correspond to reality.
- v(shoot at Fred,l,dt2) - shoot at Fred lalive Fred [A]  *  with dtoldt11dt2 and A: a few seconds.
Two problems have to be considered in relation with the persistence of a given event e: 1. temporal nonmonotonicity, i.e.
the possibility that an external event prevents e from remaining true, and 2. the estimation of the duration of the persistence of a fact, i.e.
how long, after e has begun to be true, is it likely that it is still true ?
Suppose that we heve the answer to point 2., and let a be the "normal duration" of fact e. We can define persistence as follows:  Definition 7: t is called a choice point relative to action a, among lines 11 and 12 {noted pchoix(a,11,12,t)) iff ( V t ' ) ((t'<t 3 coincide (11,12,t')) A v(a,li,dt) A v(-a,L2,dt)) The set of causal rules BR and the definitions provided so far allow us to define the set of voluntary causes of an effect e observed on time line I' at time point t':  Definition 6 (persistence): We note persist(e,d)  Definition 8: the voluntary causes are defined as a partial function: causev .
E x L x T + 2A defined only if v(e,l',dtf)holds.
Then, causev (e,l',t') is the subset A ' of A such that U E A ' iff a satisfies conditions C K 4 (the scope of t and 1 extends from C2 to C4, and of A includes C1 and C2): C l ( 3 A ) (a 3 e[A] E BR) C2 (3t,1,1") (pchoix(a,1, l",t) A dt_<ti_<t+RelevantDelay), where: if ( 33) (persist(e, 3) E TP) then Relevant-Delay = A + delse Relevant-Delay = A, c3 1'E Lp(1,dt) C4 v( T e ,1,d,) .
the fact that, without any external influence, event e is believed to remain true for a duration d. We have: (t'E1 A persist(e,d) 2 {Vto,l) ((v(e,l,dto)A (3') nocc(e,l,t',dt,-dtI))) 3 {Vl',t) ( ( t e l l A C1 A C2 A C3) 3 v(e,l',dt))) where C l , C2, and C3 abbreviate the following conditions: C1 I'E Lp(1,dtO) C2 dtoldtldto+ d 7 e [ A ] ) ~ B RI> nocc(a',l',dto-A, C3 { Va',A) ((a' dt-dto+A)) to is the time point where fact e becomes true in time line 1; C1 expresses that persistence is predicted at least in the preferred futures; C2, that persistence lasts at least for (without prohibiting it beyond this duration), provided that no action a', known in B R to make e false, occurs during the relevant lapse (C3).
(We cannot use the function inhibit here, since we want the persistence of an event to be defined without reference to its cause, while inhibit defines a set of events related to both a cause and its effect).
In the same way as implications are gathered in a rule base BR, the known persistences are collected in a "table of persistences" TP.
We now have all the prerequisites necessary to investigate which set of actions can reasonably be held as causally responsible for a given event e.  C1 selects the set of causal rules of B R containing the effect e in their right part (we examine in [ 111 the possibility of exploring what we call closure(BR) instead of BR, in order to take into account the actions which are known to cause an effect e ' , of which e is a tautological consequence); C2 means that the agent had the choice (at a time which is relevant for the observation of e ) between doing and not doing action a, and that he or she chose to do a ... C3 ... in a time line 1 for which time line I' (where event e has actually occurred) is among the preferred futures at the time where the choice has been made; C4 specifies the relevance of the action to the observed event: e must not already be true at the moment of executing a in 1.  a  6.
Explanation The reader should remember that the notion of action is essential in our theory, since we decided to privilege, when asked to find the causes of a state of - which we take to be actions executed by agents in virtue of their free will - over "natural laws".
We that a choice point is a time point from which stem (at least) two different time lines,  Suppose that we have a description of the evolution of a world by of a set D L of statements using the predicates and nocc.
The above definition can be used to solve the explanation problem, if we also have at our disposal general information such as BR and TP, and provided that some assumptions concerning the completeness of  18  DL are accepted.
[ll] gives further results, and  simultaneously providing the same result) and the fact that we require the actions to be instantaneous.
extends Definition 8 to the case where an operator + for "implies in all cases" is added.
8.
Conclusion  7, Related works  In this paper, we have developped:  - a simple temporal ontology,  Recent publications [ 12,151 contain thorough discussions of other approaches, namely chronological minimization [6,7,13] or causal minimisation [9].
They show why such approaches fail to handle adequately prediction, explanation, or ramification problems.
The aim of this section is not to restart these discussions.
However, we would like to show briefly how we tackle the central problem illustrated by the already mentionned Y.S.P.
To DL and BR given in section 5, we add TP containing the facts that alive and loaded persist indefinitely; we add also the fact that inhibit( - d i v e , shoot) contains facts like deviation-of-bullet and so on.
The conflict of persistence between alive and loaded does not arise in our approach.
As a matter of fact, the assumption of completeness on DL enables us to derive that no inhibiting fact prevents - d i v e to occur, once shoot has been done; therefore, we predict lalive.
If we also have a rule like shoot 4oaded in BR, definition 6 cannot be used to predict the persistence of loaded.
Knowing what belongs to inhibit(Tloaded, shoot) - or assuming this set to be empty, in the absence of any information on the subject -, we predict lloaded as well.
We now consider backward reasoning, adding to D L a statement such as v(Fred is alive,l,dt~)and dt2+A<dt3 : if we have to explain this anomalous state, our approach will consider two possible tracks to follow: the persistence of loaded has stopped (the gun has somehow become unloaded between dtl and dt2) or some inhibiting effect (e.g.
deviation-ofbullet) has occurred between the action shoot and its normal effect lalive, that is between dt2 and dt3.
However, we cannot prefer one track over the other, nor can we guess exactly when, in the intervals defined, the anomalous fact took place.
In contrast to this intuition, the chronological systems tend to prefer the sequence of world states where the gun becomes unloaded just before shooting, because this sequence postpones the change as long as it is consistent to do so.
More recent approaches [2,12,151 do not present these anomalies, but they should be augmented with the possibility of inhibiting effects after the action; otherwise, they are not able to propose the second kind of explanations.
Finally, the solution advocated for in this paper also runs into some difficulties.
An important one concerns concurrent actions (two or more actions  - the role of an agent in the evolution of the  world.
This approach seems to provide an intuitively correct analysis of the main problems encountered in the A.I.
literature: the explanation problem, the prediction problem, and the ramification problem (see [ 111 for examples).
Our approach should easily extend to the case where the first-level language is first-order.
We do not anticipate too many difficulties to take into account the duration of actions: instead of a threeplace predicate v(a,t,dt), we might abbreviate the formula: ( V t ) ( t l < t < t 2 3 v ( a , l , d t ) ) into v(a,1,dtl ,dt2), a four-place predicate.
This should also help us to handle the case of concurrent actions.
As another direction of further research, we are taking advantage of a cospus of car-crash reports, which is currently studied in our Laboratory [ 5 ] .The goal is to determine what should be put in BR and TP in order to find intuitively correct answers to questions concerning the causes of the accident.
As it is often the case in Artificial Intelligence, real-size problems reveal issues which are not even visible in toy problems, such as those which illustrate the present paper.
REFERENCES [l] James ALLEN: Towards a general theory of action and time.
Artificial Intelligence vo1.23 pp.
123-154,  1984 [2] A.B.BAKER: A simple solution to the Yale Shooting Problem.
Intern.
Con$ on Knowledge Representation and Reasoning pp.
11-20, 1989 [3] James P.DELGRANDE: A first-order conditional logic for prototypical properties.
Artificial Intelligence vo1.33 pp.105-130, 1987 Frangoise GAYRAL, Philippe GRANDEMANGE: Evtnements : ponctualitt et durativitt.
81hA F C E T RFIA Congress pp.905-910, Lyon (F), Nov. 1991 [5] Frangoise GAYRAL, Philippe GRANDEMANGE, Daniel KAYSER, FranGois LEVY: InterprCtation des constats d'accidents : reprksenter le rCel et le potentiel Approches se'maiztiques t.a.1.
vo1.35 n"1 pp.65-81, 1994 [6] B.A.HAUGH: Simple causal minimization for  [PI  temporal persistence and projection.
AAA1 pp.218223.
1987  19  [7] Henry A.KAUTZ: The logic of persistence.
Y h National Conference on Artificial Intelligence pp.401-  [12] Erik SANDEWALL: The range of applicability of nonmonotonic logics for the inertia problem.
13th IJCAI pp.738-743, Chambery, 1993 [I31 Yoav SHOHAM: Reasoning about change: time and causation from the standpoint of Artificial Intelligence.
M. I. T.Press 1988 [14] Yoav SHOHAM: Time for Action : On the Relation Between Time, Knowledge and Action.
l l t hIJCAI pp.954-959 & 1173, Detroit, 1989 [ 151 Lynn A.STEIN, Leora MORGENSTERN: Motivated action theory: a formal theory of causal reasoning.
Artificial Intelligence voI.7 1 pp.
1-42, 1994  405, 1986 [8] Saul A.KRIPKE: Semantical considerations on modal logic.
Acta philosophica fennica vo1.16 pp.8394, 1963 [9] Vladimir LIFSHITZ: Computing Circumscription.
9th IJCAI pp.121-127, Los Angeles, 1985 [ l o ] Drew V.McDERMOTT: A Temporal Logic for Reasoning about Processes and Plans.
Cognitive Science vo1.6 pp.101-155, 1982 [ 111 Aicha MOKHTARI: Action-based causal reasoning.
Applied Intelligence to appear  20
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE
