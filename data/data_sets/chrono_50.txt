Extending Temporal Reasoning with Hierarchical Constraints Fei Song Dept.
of Computing and Information Science University of Guelph Guelph, Ontario, Canada N1G 2W1 Abstract Existing reasoning algorithms for Allen's interval algebra may produce weak results when applied to temporal networks that involve decompositions of intervals.
We present a strengthened procedure for reasoning about such hierarchical constraints, which works interactively with an existing algorithm for temporal reasoning, to produce the desired stronger results.
We further apply our algorithm to the process of plan recognition and show that such an application can both reduce the number of candidate plans and make the constraints in the remaining plans more specific.
1.
Introduction Allen's (1983a) interval algebra has shown to be useful for such applications as knowledge-based systems, natural language processing, and planning (as described in Vilain, et al., 1989).
For example, a simplified plan for making a pasta dish can be represented as the temporal network in figure 1, where a node corresponds to the time interval over which a state holds or an event occurs, and a link label represents the temporal constraint between two intervals1.
InKitchen  Includes  Meets Make Pasta Dish  Includes Includes  Includes Includes  Make BeforeMeets Noodles Make Sauce  Ready ToEat  Boil BeforeMeets Put Noodles Together BeforeMeets  Figure 1: Temporal network of a cooking plan 1 Here, "Includes" and "BeforeMeets" are high-level  constraints, defined as the sets {si, di, fi, eq} and {b, m}, where b, m, eq are the basic relations "before", "meets", "equal", and si, di, fi are the inverses of "starts", "during", and "finishes" in Allen's interval algebra.
Given a temporal network, an important reasoning task is to compute the so-called minimal labeling, that is, to find the set of minimal constraints if the network is consistent (Vilain and Kautz, 1986).
A constraint (or a link label) is minimal if each of its basic relations is part of a consistent singleton labeling, for which each link is labeled by a basic relation and all the links in the network are satisfied.
Vilain and Kautz (1986) show that the time complexity of such a reasoning task is NP-complete for the interval algebra, where the problem size is the number of intervals.
However, this does not prevent people from proposing polynomial algorithms that are approximate for the interval algebra (Allen 1983; Van Beek 1989).
Allen's algorithm has O(n3 ) time complexity and is shown to be exact only for a subset of the interval algebra (Van Beek 1989).
Van Beek then proposes an O(n4 ) algorithm which is exact for a larger subset (the subset of the interval algebra that can be translated into the point algebra).
To get more exact results for the full interval algebra, one may have to use some exponential-time algorithms (e.g., Valdes-Perez 1987).
One problem with these existing algorithms is that they may produce weak results when applied to temporal networks that involve hierarchical constraints (i.e.
the decompositions of intervals into low-level subintervals).
In Song and Cohen (1991), we proposed a strengthened algorithm for temporal reasoning about hierarchical constraints.
The algorithm guarantees that the result is no weaker than that obtained from the existing temporal reasoning algorithms.
However, whether it can derive the minimal labeling for the hierarchical constraints depends on the order in which lower-level constraints are combined.
In this paper, we present a new order-independent reasoning procedure for hierarchical constraints, along with formal proofs for its associated properties.
We further apply our  algorithm to the problem of plan recognition, and show that the observed temporal constraints can both reduce the number of candidate plans and make the constraints in the remaining candidate plans more specific.
2.
Weak results from the existing algorithms A hierarchical constraint corresponds to the decomposition of an interval to a set of subintervals.
In terms of Allen's interval algebra, this means that the interval temporally includes all the subintervals.
Suppose that initially there is no specific constraint between a1 and a2 in figure 2(a).
Then, all we can decide is that A Includes a1 and A Includes a2, where Includes stands for the constraint {si, di, fi, eq}.
(a) Includes  {si,di}  a1  A  {b}  A1  Includes  a1  a2  a1  a3  Common  (b)  A2  A1 {si,di}  {di,fi}  {b}  a1  {b}  a2 {fi}  (c)  {si}  {fi}  {b}  a3  A2  A1 {si}  {di,fi}  {si,di}  {fi}  {b}  a2  a3 Figure 3: (a) Two decompositions, (b) Weak results from Allen's , and (c) Strong results desired  (c) {si}  includes  includes  a2  a1  a2  {di,fi}  A2  includes  includes  A  a1 (b)  (a)  A  {b}  {fi}  a2  Figure 2: (a) One decomposition, (b) Weak results from Allen's algorithm, and (c) Strong results desired  Now, if we add a new constraint {b} between a1 and a2, then we can use Allen's algorithm to propagate the constraint and produce the results shown in figure 2(b).
However, since a1 and a2 are the only subintervals of A and we know that a1 is located before a2, we should be able to decide that a1 is the starting part of A and a2 is the finishing part.
In other words, we should get the desired results shown in figure 2(c).
Such weak results can be carried further for networks that consist of more than one decomposition.
Suppose that initially we have the network shown in figure 3(a).
Later, if we add the constraints a1 {b} a2 and a2 {b} a3, we get the results shown in figure 3(b) using Allen's algorithm, where "Common" stands for the constraint: {o, oi, s, si, d, di, f, fi, eq}.
However, using a similar argument as made for the previous example, we should be able to get the stronger results shown in figure 3(c).
There are also networks that are considered to be consistent by Allen's algorithm but in fact are not when decompositions are involved.
For example, the network in figure 4(a) is regarded as consistent by Allen's algorithm, since we get the same network after applying the algorithm.
However, this is actually not true because if a1 and a2 are the only subintervals of A and a1 is located before a2, a2 should be the finishing part of A, not an interior part, as shown in figure 4(b).
(a) {si}  a1  A  {b}  (b) {di}  {si}  a2  a1  A  {fi}  {b}  a2  Figure 4: (a) Weak results from Allen's, and (b) Strong results desired  Such weak results are not simply caused by the inexactness of Allen's algorithm.
In fact, Allen's algorithm is exact for all these examples since the constraints used fall into a subset of the interval algebra for which Allen's algorithm is guaranteed to find the set of minimal labels (Van Beek 1989).
The reason for these weak results is that Allen's algorithm treats all the intervals as independent of each  other.
This is certainly not true for decompositions, since the abstract intervals are temporally dependent on their subintervals.
To make these dependencies explicit in the reasoning process, we need to assume that the decomposition of an abstract interval into its subintervals is complete, that is, no more subintervals can be added to the decomposition.
As a result, we can compute how an abstract interval is temporally bounded by its subintervals based on the constraints between all the subintervals.
For instance, if there is a linear ordering between all the subintervals, then we can clearly decide that the abstract interval is temporally bounded by the subintervals that occur the earliest and the latest.
We say that a decomposition is closed if the constraints between the abstract interval and its subintervals are minimal with respect to the constraints between all the subintervals.
More formally, we describe an abstract interval as the convex hull or the minimal cover of its subintervals, denoted by the equation: A = x1 + x2 + [?][?][?]
+ xn where A denotes the abstract interval and x1 , x2 ,..., xn denote the subintervals.
For the example in figure 3, the two decompositions can be represented as: A1 = a1 + a3 and A2 = a 2 + a3 .
Closing a decomposition means closing every decomposition edge between the abstract interval and its subintervals, which further implies computing the minimal labels on the decomposition edges.
We formally define a closure operation by the following first-order formula: A(Ci1 ,Ci2 ,...,Cim )c xi = xi C1 x1 [?]
xi C2 x2 [?]
[?][?][?]
[?]
xi Cm xm [?]
(A = x1 + x2 + [?][?][?
]xm ) where Ci1 ,Ci2 ,...,Cim are the constraints between the subinterval xi and all the subintervals from x1 to xm .
This formula suggests that a decomposition edge can be closed by using the existing constraints between a subinterval and all the subintervals and the fact that the abstract interval is the minimal cover of all its subintervals.
3.
Development of a strengthened temporal reasoning algorithm  To describe our strengthened algorithm for temporal reasoning with hierarchical constraints, we start with the simple case of closing one and two subintervals, and then, we generalize the result to close more than two subintervals.
After that, we provide a recursive procedure to close more than one decomposition, and finally, we present the strengthened algorithm that closes all the hierarchical structures in a temporal network.
3.1.
Closing one and two subintervals As described earlier, closing a decomposition means computing the minimal labels on all the decomposition edges.
If an abstract interval has only one subinterval, then the minimal label on the decomposition edge is obviously {eq}, that is: A(Cii )c xi = xi Cii xi [?]
(A = xi ) = xi {eq}xi [?]
(A = xi ) = A{eq}xi In the case of two subintervals, we can derive from the definition: A(Cii ,Cij )c xi = xi Cii xi [?]
xi Cij x j [?]
(A = xi + x j ) = xi {eq}xi [?]
xi Cij x j [?]
(A = xi + x j ) Since xi {eq}xi always holds for an interval (the so-called node consistency), we define the basic closure operation on a constraint as: A Cijc xi = xi Cij x j [?]
(A = xi + x j ) .
Lemma 1: Given Cij as a set of basic relations R1 , R2 ,..., Rm , the basic closure Cijc can be computed as {R1c , R2c ,..., Rmc}, where Rc is one of the four basic relations: si, di, fi, and eq, as defined in table 1.
R  Table 1: basic closure on basic relations b bi m mi o oi s si d di f fi eq  Rc si fi si fi si fi si eq di eq fi eq eq The validity of table 1 can be easily verified.
For example, if xi {b}x j , then the closed edge between A and xi is {si}, since if A consists of only xi and x j and xi is located before x j , then xi must be the starting part of A.
This basic closure operation also applies to  the case of one subinterval, i.e., (Cii )c = Ciic , since ACiic xi = A{eq}c xi = A{eq}xi .
3.2.
Closing more than two subintervals Having defined the basic closure operation, we can now extend it to close a decomposition of more than two subintervals.
More are the constraints specifically, if Ci1 ,Ci2 ,...,Cim between the subinterval xi and all the subintervals from x1 to xm , including the subinterval xi , then we can close xi and another subinterval x j using the basic closure operation: Cijc .
To get the final closed decomposition edge to xi , however, we need to somehow combine all of the Cijc 's.
It turns out that these Cijc 's can be combined with the normal composition operation in Allen's interval algebra.
Lemma 2 Given the basic relations Ri1 , Ri2 ,..., Rin ,we have A(Ri1 , Ri2 ,[?][?][?
], Rin )c xi = A(R x R x [?][?][?]
x R )xi .
c i1  c i2  c in  Proof.
We prove this lemma by induction on the number of subintervals.
For n = 1, we showed in the last subsection that A(Rii )c xi = ARiic xi .
For n = 2, we have: A(Rii , Rij )c xi = ARijc xi A(Riic x Rijc )xi = A({eq}c x Rijc )xi = ARijc xi So, the lemma holds for both n = 1 and n = 2.
Assume the lemma holds for n = k, that is, A' (Ri1 , Ri2 ,..., Rik )c xi = A' Ri1c x Ri2c x [?][?][?]
x Rikc xi , where A' = x1 + x2 + [?][?][?]
+ xk , we need to prove that the lemma also holds for n = k+1.
We know from table 1 that Rijc can only be one of the four basic relations: si, di, fi, and eq.
By checking table 2, a sub-multiplication table drawn from Allen's (1983a), we see that these four basic relations are closed under multiplication.
Table 2: A Sub-Multiplication Table  x si di fi eq  si si di di si  di di di di di  fi di di fi fi  eq si di fi eq  It follows that Ri1c x Ri2c x [?][?][?]
x Rikc can only be one of the four basic relations: si, di, fi, and eq.
Let us denote Ri1c x Ri2c x [?][?][?]
x Rikc as R , and Rikc +1 as S. Now, from the definition of the closure operation, we have: A(Ri1 ,..., Rik , Rik +1 )c xi = xi Ri1 x1 [?]
[?][?][?]
[?]
xi Rik xk [?]
xi Rik +1 xk +1 [?]
(A = x1 + [?][?][?]
+ xk + xk +1 ) = xi Ri1 x1 [?]
[?][?][?]
[?]
xi Rik xk [?]
(A' = x1 + [?][?][?]
+ xk ) [?]
xi Rik +1 xk +1 [?]
(A"= xi + xk +1 ) [?]
(A = A' + A") = A' (Ri1 ,..., Rik )c xi [?]
A" Rikc +1 xi [?]
(A = A' + A") = A' (Ri1c x [?][?][?]
x Rikc )xi [?]
A" Rikc +1 xi [?]
(A = A' + A") = A' Rxi [?]
A"Sxi [?]
(A = A' + A") To further evaluate the above expression, we need to consider the following special cases: (1) If A' {si}xi [?]
A"{si}xi , then we have A{si}xi , since if xi is the starting part of both A' and A" and A = A' + A", then xi should also be the starting part of A.
(2) If A' {fi}xi [?]
A"{fi}xi , then we have A{fi}xi .
The reason is similar to case (1) above.
(3) If A' {si}xi [?]
A"{fi}xi , then we have A{di}xi .
The reason for this is that if xi is the starting part of A', then there must be another interval that finishes after xi .
Similarly, if xi is the finishing part of A", then there must be another interval that starts before xi .
Thus, there are intervals that starts before xi and finishes after xi , and xi must be an interior part of the covering interval A.
(4) If A' {di}xi [?]
A"Sxi , then we have  A{di}xi , since if xi is an interior part of A', it is also an interior part of A.
(5) If A' {eq}xi [?]
A"Sxi , then we have ASxi .
This is obviously true since A' equals xi .
Since conjunctions are commutative, it is easy to see that these results are exactly the same as table 2 above.
In other words, we have proved that: A(Ri1 ,..., Rik , Rik +1 )c xi = A(R x S)xi = A(Ri1c x [?][?][?]
x Rikc x Rikc +1 )xi .
that is, lemma 2 also holds for n = k+1.
Lemma 2 implies that if the constraints between one subinterval and all the subintervals are one of the basic relations, the decomposition edge to the subinterval can be closed by multiplying the basic closures of these constraints.
Theorem 1: Given Ci1 ,Ci2 ,...,Cim as the constraints between xi and all the subintervals from x1 to xm , the closed edge between A and xi can be computed as follows: c c c c A(C  i1 ,Ci2 ,...,Cim ) xi = ACi1 o Ci2 o [?][?][?]
o Cim xi .
Proof.
The theorem can be proved by expanding constraints into sets of basic relations, converting the result into disjunctions of conjunctions, and applying lemma 2 to all the conjunctions: A(Ci1 ,Ci2 ,...,Cim )c xi = xi Ci1 x1 [?]
xi Ci2 x2 [?]
[?][?][?]
[?]
xi Cim xm [?]
(A = x1 + x2 + [?][?][?]
+ xm ) = xi {R11 , R12 ,..., R1n1 }x1 [?]
xi {R21 , R22 ,..., R2n2 }x2 [?]
...... xi {Rm1 , R2 m2 ,..., Rmnm }xm [?]
(A = x1 + x2 + [?][?][?]
+ xm ) = (xi R11 x1 [?]
xi R21 x2 [?]
[?][?][?]
[?]
xi Rm1 xm [?]
A = x1 + x2 + [?][?][?]
+ xm ) [?]
(xi R11 x1 [?]
xi R21 x2 [?]
[?][?][?]
[?]
xi Rm2 xm [?]
A = x1 + x2 + [?][?][?]
+ xm ) [?]
...... (xi R1n1 x1 [?]
xi R2n2 x2 [?]
[?][?][?]
[?]
xi Rmnm xm [?]
A = x1 + x2 + [?][?][?]
+ xm ) = A(R11 , R21 ,..., Rm1 )c xi [?]
A(R11 , R21 ,..., Rm2 )c xi [?]
...... A(R1n1 , R2n2 ,..., Rmnm )c xi c = AR11c x R21c x [?][?][?]
x Rm1 xi [?]
c c c AR11 x R21 x [?][?][?]
x Rm2 xi [?]
...... c c AR1nc 1 x R2n x [?][?][?]
x Rmn xi 2 m c c c = ACi1 o Ci2 o [?][?][?]
o Cim xi .
Lemma 3 Given constraints as subsets of {si, di, fi,eq}, the composition is commutative and associative, that is,  C1 o C2 = C2 o C1 , and (C1 o C2 ) o C3 = C1 o (C2 o C3 ).
Proof.
Given two subsets of {si, di, fi,eq}, the composition is both commutative and associative since for each pair of the basic relations, the results of multiplications are symmetric, as shown previously in table 2.
Theorem 2: In closing a decomposition constraint using theorem 1, we get the same result no matter what order we do the compositions.
Proof.
This follows directly from lemma 3, since the composition is both commutative and associative for subsets of {si, di, fi,eq}.
Based on theorems 1 and 2, we now present a new procedure for closing a decomposition of any number of subintervals.
procedure CLOSE(k, S) begin for each i [?]
S do begin t - {eq} for each j [?]
S do t - t o Cijc t - t [?]
Cki if t [?]
Cki then begin Cki - t Cik - INVERSE(t) Q - Q [?]
RELATED_PATHS(k, i) end end end Figure 5: Procedure for closing a decomposition  The above procedure closes all the decomposition edges in turn, and if a closed edge is more specific than the existing edge, the existing edge will be updated and all the related paths will be queued for further propagation.
Theorem 3.
The time complexity of the CLOSE procedure is O(m2 ) where m is the number of subintervals in a decomposition.
3.3.
Closing all the decompositions in a hierarchical structure A hierarchical structure often consists of more than one decomposition.
Our strategy is to close a hierarchy in a post-order fashion, since higher-level intervals can be defined in terms of lower-level subintervals.
In other words, we start the closing process from the bottom-level decompositions and work our way up until all the decompositions are closed in the hierarchy.
procedure CLOSE_ALL (k) begin get a list S of subintervals for k if S is not empty then begin for each i [?]
S do CLOSE_ALL (i) CLOSE (k, S) end end Figure 6: Closing all the decompositions in a plan  Theorem 4.
The time complexity of the CLOSE_ALL procedure is bounded below by O(n) and above by O(n2 ), where n is the number of intervals in a plan.
3.4.
The Strengthened Algorithm The CLOSE_ALL procedure closes all the decompositions in a hierarchical structure.
To get stronger results for a temporal network, we first use an existing reasoning algorithm to compute the set of constraints to be as specific as possible.
Then, for each hierarchical structure in the temporal network, we recursively close all the decompositions using the CLOSE_ALL procedure.
After that, some of the decomposition edges may be updated,  and we call the temporal reasoning algorithm again to propagate the effects of these new constraints.
Thus, we generally need to call interactively an existing reasoning algorithm and our CLOSE_ALL procedure.
Such a process will eventually terminate since every time we update a constraint, some of its basic relations will be eliminated and there are at most 13 basic relations in any constraint.
We now give the strengthened algorithm for temporal reasoning with hierarchical constraints: algorithm STRENGTHENED begin Q-{initial paths in a temporal network} H-{roots of all hierarchical structures} while Q is not empty do begin MODIFIED_TR foreach k [?]
H do CLOSE_ALL (k) end end Figure 7: The strengthened algorithm for temporal reasoning about plans  The set H contains the roots of all hierarchical structures, and CLOSE_ALL closes all the decompositions in a hierarchy.
The set Q contains those paths whose effects need to be propagated, and MODIFIED_TR is the same as an existing algorithm for temporal reasoning except that the initialization of Q is removed from the procedure.
Theorem 5.
The time complexity of our strengthened algorithm is at most O(T log2n), where n is the number of intervals in a temporal network and T is the time complexity of an existing reasoning algorithm (n3 for the path-consistency procedure, n4 for Van Beek's procedure, and exponential for some more exact procedures).
Proof: First, each iteration of our algorithm takes O(T+n 2 ) time or O(T), since all the existing algorithms take time at least O(n3 ).
Second, the worst case corresponds to a balanced binary tree, with maximum levels of decompositions and for which the effects of closed decompositions need to be propagated upwards.
Thus, we have the factor of log2n for the number of iterations.
4.
An application to plan recognition Plan recognition is the process of inferring an agent's plan based on the observation of the agent's actions.
A recognized plan is useful in that it helps to decide an agent's goal and predict the agent's next action.
For example, if we observe that John has made the sauce and he is now boiling the noodles, then based on the plan shown in figure 1, we can decide that John's goal is to make a pasta dish and his next action is to put noodles and sauce together.
Plan recognition has found applications in such areas as story understanding, psychological modeling, natural language pragmatics, and intelligent interfaces.
(a)  Make Noodles Includes  Includes  Includes  Measure BeforeMeets Mix BeforeMeets Flour Dough Make Sauce  (b) Includes  Thaw Beef (c)  Roll Dough  Includes  BeforeMeets  Mix {b,m} Dough  Includes  Heat BeforeMeets Beef  Add Paste  Thaw {b,m} Roll Beef Dough {b,m} Heat {b,m} Boil {b,m} Beef Noodles  Add Paste  Figure 8: An extended plan for making a pasta dish  Most existing models for plan recognition assume a library of all possible plans that might occur in a particular domain.
Then, through some kind of search and matching mechanism, one can find all the plans that contain the observed actions, called candidate plans.
Since the observation of an agent's actions is often incomplete and some actions may appear in many different plans of the plan library, it is often difficult to determine the unique plan that an agent is pursuing.
Kautz (1987) suggests  that one way of reducing the number of candidate plans is to use various kinds of constraints, including the temporal relations explicitly reported in the observations, to further eliminate those inconsistent plans2 .
However, Kautz only adopted a subset of Allen's interval algebra and did not use fully the temporal constraints that correspond to the decomposition edges in a candidate plan.
Our approach to plan recognition is to represent a plan as a temporal network and perform temporal reasoning to eliminate those candidate plans that are inconsistent with the temporal constraints explicitly given in the observations.
Such a reasoning process can have two useful effects: the given constraints can be used to reduce the number of candidate plans (an example of this effect can be found in Song and Cohen (1991)) and the given constraints can be made more specific by combining them with the prestored constraints in a candidate plan.
To illustrate the second effect, we extend the plan for making a pasta dish in section 1 by adding the decompositions for MakeNoodles and MakeSauce, shown in figure 8(a) and (b).
Suppose that the observation of an agent's actions is given in figure 8(c).
We can then use Allen's algorithm or our strengthened algorithm to make some of the constraints in the plan more specific.
Using Allen's algorithm, we can make some of the constraints more specific, as shown in figure 9(a).
Using our strengthened algorithm, we can make these constraints even more specific, as shown in figure 9(b).
MakePastaDish {si, di} MakeNoodles MakePastaDish {si, di} MakeSauce MakePastaDish {di, fi} PutTogether MakeNoodles {si, di} MeasureFlour MakeNoodles {di, fi} RollDough MakeSauce {si, di} ThawBeef MakeSauce {di, fi} AddTomatoPaste MakeNoodles {o, s, d} MakeSauce MakeNoodles {b, m} BoilNoodles Figure 9(a): Results from Allen's Algorithms 2 Other solutions include the use of preference heuristics  (Allen, 1983b; Litman, 1985; Carberry, 1986) and probabilities (Goldman and Charniak, 1988).
MakePastaDish {si} MakeNoodles MakePastaDish {di} MakeSauce MakePastaDish {fi} PutTogether MakeNoodles {si} MeasureFlour MakeNoodles {fi} RollDough MakeSauce {si} ThawBeef MakeSauce {fi} AddTomatoPaste MakeNoodles {o} MakeSauce MakeNoodles {b} BoilNoodles Figure 9(b): Results from our strengthened algorithm  5.
Conclusion We presented a strengthened algorithm for temporal reasoning about plans, which improves on straightforward applications of the existing reasoning algorithms for Allen's interval algebra.
We view plans as both temporal networks and hierarchical structures.
Such a dual view allows us to design a closing procedure which makes as specific as possible the temporal constraints between abstract actions and their subactions.
The procedure is then used interactively with an existing reasoning algorithm to help obtain the strengthened results.
We applied our algorithm to the problem of plan recognition and showed that such an application can both reduce the number of candidate plans make the constraints in the remaining plans more specific.
One possible area for future work is to improve the efficiency of our algorithm, which calls interactively an existing reasoning algorithm and our closing procedure.
Although the strengthened algorithm only adds a factor of log 2 n to the time complexity of an existing reasoning algorithm, it is worth investigating whether such interactions can be localized and reduced.
Some results on localizing the propagation of temporal constraints in Allen's interval algebra have been reported (Koomen 1989).
This would form a useful starting point for our future research.
Acknowledgments This work was supported in part by the Natural Sciences and Engineering Research Council of Canada.
ALLEN, J. F. 1983a.
Maintaining knowledge about temporal intervals.
Communications of the ACM, 26: 832-843.
_____1983b.
Recognizing intentions from natural language utterances.
In Computational models of discourse.
Edited by M. Brady and R. Berwick.
The MIT press, Cambridge, Mass.
pp.
107-166.
ALLEN, J. F., and KOOMEN, J.
A.
1983.
Planning using a temporal world model.
Proceedings of the Eighth International Joint Conference on Artificial Intelligence, pp.
741-747.
CARBERRY, S. 1986.
Pragmatic modeling in information system interfaces.
Ph.D. Dissertation, University of Deleware.
GOLDMAN, R., and CHARNIAK, E. 1988.
A probabilistic ATMS for plan recognition.
Proceedings of the AAAI Workshop on Plan Recognition.
KAUTZ, H. A.
1987.
A formal theory of plan recognition.
Ph.D. Dissertation, University of Rochester, Rochester, N.Y. KOOMEN, J.
A.
1989.
Localizing temporal constraint propagation.
Proceedings of the First International Conference on Principles of Knowledge Representation and Reasoning, Toronto, Ontario, Canada, pp.
198-202.
LITMAN, D. 1985.
Plan recognition and discourse analysis: an integrated approach for understanding dialogues.
Ph.D. Dissertation, University of Rochester.
SONG, F. 1991.
A processing model for temporal analysis and its application to plan recognition.
Ph.D. Dissertation, University of Waterloo, Waterloo, Ontario, Canada.
SONG, F., and COHEN, R. 1991.
Temporal reasoning during plan recognition.
Proceedings of the Ninth National Conference on Artificial Intelligence, Anaheim, CA, pp.
247-252.
VALDES-PEREZ, R. E. 1987.
The satisfiability of temporal constraint networks.
Proceedings of the Sixth National Conference on Artificial Intelligence, pp.
256-260.
VAN BEEK, P. 1989.
Approximation algorithms for temporal reasoning.
Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, pp.
1291-1296.
VILAIN, M., and KAUTZ, H. 1986.
Constraint propagation algorithms for temporal reasoning.
Proceedings of the Fifth National Conference on Artificial Intelligence, pp.
377-382.
VILAIN, M., KAUTZ, H., and VAN BEEK, P. 1989.
Constraint propagation algorithms for temporal reasoning: a revised report.
In Readings in qualitative reasoning about physical systems.
Edited by D.S.
Weld and J. de Kleer.
Morgan Kaufman, San Mateo, CA, pp.
373-381.
Localized Temporal Reasoning: A State-Based Approach Shieu-Hong Lin and Thomas Dean Department of Computer Science Brown University, Providence, RI 02912  Abstract  We are concerned with temporal reasoning problems where there is uncertainty about the order in which events occur.
The task of temporal reasoning is to derive an event sequence consistent with a given set of ordering constraints immediately following which one or more conditions have specified statuses.
Previous research shows that the associated decision problems are hard even for very restricted cases.
In this paper, we present a framework of localized temporal reasoning which use subgoals and abstraction to exploit structure in temporal ordering and causal interaction.
We investigate (1) locality regarding ordering constraints that group events hierarchically into sets called regions, and (2) locality regarding causal interactions among regions, which is characterized by subsets of the set of conditions.
The complexity for an instance of temporal reasoning is quantified by the sizes of the characteristic subsets of conditions and the numbers of child regions of individual regions in the region hierarchy.
This demonstrates the potential for substantial improvements in performance.
1 Introduction  We are interested in reasoning about a dynamical system whose state is modeled by a set of conditions P .
Each condition has a status, true or false, at a given point in time and the evolution of the system corresponds to a sequence of state changes.
The evolution of the system depends on the interactions among a set of events E .
An event e in E changes the system state according to a set of causal rules associated with e. A rule associated with an event e is a STRIPS-like operator describing the causal e ects of e when the preconditions of the rule are satisfied.
In this way, each event specifies a state-transition function on the state space determined by P 10].
The ordering of the events is uncertain.
The possible event sequences are determined by a given set of ordering constraints on  the events.
Generally, many event orderings are possible, each of which may result in a di erent evolution of the system.
Given the initial state and a set of goal conditions G , the task of temporal reachability 10] is to (1) detect the existence of a possible event sequence immediately following which the conditions in G have specified statuses, and (2) generate one such event sequence if one exists.
In other words, the task of temporal reachability is to predict the possibility of reaching the goal states where the conditions in G have the specified statuses, and report one possible event sequence which ends in the goal states.
For example, consider a partial plan where the ordering of the components is constrained by some partial order.
Using temporal reachability, we can coordinate the component events to reach the goal states in a way consistent with the inherent ordering constraints.
On the other hand, we can also apply temporal reachability to validate the partial plan by (1) specifying the undesirable states as goal states where the set of goal conditions have the specified statuses, (2) detecting the possibility of reaching undesirable states, and (3) reporting such a possible event sequence as evidence.
This research extends the earlier work of Dean and Boddy 5] on reasoning about partially ordered events.
In special cases, temporal reasoning with uncertainty about the ordering of events is harder than the corresponding planning problem 11].
This happens because in the corresponding planning problem, the planner is free to select an arbitrary set of event instances from a fixed set of event types with no restrictions on the ordering of event instances.
In most cases, if the events are not under the planner's control, however, the problems are computationally equivalent.
Previous research shows that the associated decision problems for temporal reachability 10] and planning are hard even for very restricted cases 1] 2] 3] 5] 6].
We have been trying to understand why temporal reachability and planning are so dicult and what, if any, structure might be extracted from such problems to expedite decision making and inference 10].
In this paper, we present a framework of localized temporal reasoning.
Using this framework, temporal  reachability is viewed as search in the global search spaces associated with individual problem instances.
The sizes of the global search spaces are determined by (1) the total number of the conditions in P , and (2) the total number of the events in E .
Our investigations have focused on the notions of locality in time and the structure of the search spaces.
Locality in time in a particular problem instance is modeled by a hierarchy of regions.
Each region is composed of an encapsulated event subset.
The events outside a region R must occur either before or after the events in R. We allows the child regions of a region to be partially ordered.
Given a hierarchy of regions, we can induce a set of coupling conditions and a set of abstract conditions for each individual region, which reect locality in the associated search space, and enable us to construct local search spaces.
The coupling conditions of a region R are the media for interregional interactions between the events in di erent child regions of R. The abstract conditions of a region R are the media for the interactions between the events in region R and the events not in R. Instead of search in the global search space, we can search the local search spaces and propagate the e ects of local search.
The sizes of the sets of coupling conditions and the numbers of child regions of individual regions determine the complexity for an instance of temporal reachability.
This research utilizes the notions of localized planning 9], subgoals 8], and abstraction 4] 7] 8] 12] 13] to expedite temporal reasoning.
The use of eventbased localized reasoning in planning has been advocated by Lansky in GEM 9].
In GEM, locality regarding domain behavior is also modeled by \regions".
\Regions" in GEM are composed of sets of interrelated events whose behaviors are delimited by event-based temporal logic constraints.
The GEMPLAN planner utilizes localized planning to exploit locality by subdividing the global search space into regional search spaces.
In this paper, we identify the corresponding notion of regions in the temporal reachability problem, which models locality in time.
In addition, we study the dependencies between conditions and regions, and identify the coupling conditions and the abstract conditions associated with individual regions.
These structures model locality in search spaces.
The state-based approach allows us to use knowledge regarding subgoals, abstract events, and local search spaces to conduct and analyze localized reasoning in a clear and elegant way.
The usefulness of subgoals, macro-operators, and abstraction in reducing the search e ort in planning has been investigated by Korf 8].
We identify subgoals and abstract events as useful knowledge for temporal reachability.
We develop techniques to derive this critical knowledge from individual temporal reachability instances.
Our techniques are di erent from previous work on abstraction 4] 7] 13] in that we transform encapsulated event subsets into abstract events, instead of  transforming individual operators into abstract operators.
2 The Temporal Reachability Problem  An instance of the temporal reachability problem is defined by hP  I  G  E  Oi.
 P is a set of conditions modeling the world state.
Each condition in P has a status, true or false, at a given point in time.
 I is a set of condition/status pairs, that specifies the initial statuses of each condition in P .
 G is a set of condition/status pairs, that specifies the goal statuses of all or a subset of the conditions in P .
 E is a set of events, each of whose causal effects are represented by a set of causal rules.
A causal rule r is a STRIPS-like operator, which describes the status changes of a subset of the conditions in P (i.e.
the consequent e ects) when the conditions in another subset of P have the statues specified by r (i.e.
the antecedent requirements).
In this way, each event determines a state-transition function on the state space modeled by P .
 O is a set of arbitrary constraints on E .
The task of temporal reachability is to (1) detect the existence of event sequences of size jEj consistent with O such that the goal specified by G is achieved immediately following the event sequences, and (2) generate one such event sequence if one exists.
Figure 1 describes an example problem instance.
 The world state is modeled by a set of conditions P = fa b c d e f g hg.
 The initial state is described by I .
All the conditions are initially true except that h is false.
 E = fe1  e2  e3 e4 e5  e6g.
Each event in E is associated with a causal rule while in general an event can be associated with multiple causal rules.
 Figure 2 depicts the ordering constraints on E .
(For notational convenience, we use W as an alias name for E .)
The events in the three event subsets X , Y , and Z must occur as three atomic groups, where X = fe1  e2g, Y = fe3  e4 g,Z = fe5 e6 g. The events in Y must occur before the events in X .
Event e1 must occur before event e2 .
Our task is to (1) determine the existence of event sequences consistent with O such that the goal G = f(a false) (b false) (d false) (e false) (h false)g is achieved immediately following the event sequences, and (2) generate one such event sequence if one exists.
3 Locality in Temporal Ordering Initial state:  {(a,true),(b,true),(c,true),(d,true), (e,true),(f,true),(g,true),(h,false)};  Goal : {(a,false),(b,false)(d,false),(e,false),(h,false)}.
W X  e1: If a=true and b=true, then c=true, a=false, b=false; otherwise, the statuses of all conditions remain unchanged.
e2: If a=true and c=true, then b=true, a=false, c=false; otherwise, the statuses of all conditions remain unchanged.
Y  e3 : If d=true and e=true, then f=true, d=false, e=false; otherwise, the statuses of all conditions remain unchanged.
e4 : If d=true and f=true, then e=true, d=false, f=false; otherwise, the statuses of all conditions remain unchanged.
Z  e5 : If a=false and d=true, then a=true, d=false, g=false; otherwise, the statuses of all conditions remain unchanged.
e6 : If a=true and d=false, then d=true, a=false, h=false; otherwise, the statuses of all conditions remain unchanged.
Figure 1: An instance of the temporal reachability problem  W  X e1  e2  Z e5  e3  Y  e4  e6  Figure 2: Ordering constraints on events  Temporal reachability turns out to be NP-Complete even if events are totally unordered (i.e.
O = ) and the associated state space is polynomial in the size of the event set (i.e.
jPj = O(log jEj)) 10].
This complexity result motivates our e ort to exploit inherent locality regarding event ordering and the dependencies among conditions and events.
When totally unordered, events can occur in arbitrary order.
However, there is locality in event ordering if the occurrences of events or subsets of events closely relate to one another.
In this paper, we consider locality in event ordering regarding hierarchical task networks.
We define a hierarchical task network as a partial plan such that  the plan is organized as a hierarchy of tasks, where an individual task may be composed of subtasks that are also tasks in the hierarchy,  the subtasks of an individual task may be constrained by some partial order, and  the leaf tasks of the task hierarchy are events, each of which changes the world states according to a set of associated causal rules.
Each causal rule is a STRIPS-like operator.
For example, we may be working on several programming tasks.
Each task is composed of a set of subtasks, including prototype design, prototype testing, problem reformulation, etc., such that each subtask involves a set of events that we wish to occur as a group.
We allow ourselves to switch between programming tasks, but, once we begin a subtask in a given programming task, we commit to completing all of the events associated with that subtask before switching to any other subtasks.
In the following, we define the concepts of regions , child regions , region hierarchy , and hierarchical ordering constraints.
Given a hierarchical task network, locality in event ordering can be abstractly modeled by a hierarchy of regions, and a set of hierarchical ordering constraints on the regions.
Denition 1.
Given a set of events E , an event sub-  set X of E is a region in E if the events in X occur as an atomic group with the events outside the region occurring either before or after the events inside the region.
E and each individual event in E are regions by themselves.
In a hierarchical task network, a region models the set of descendant events (i.e.
leaf tasks) of a task, which always occur as an atomic group.
In other words, each task is associated with a region.
Denition 2.
Given a set of events E , the regions in E form a region hierarchy E , either they are disjoint  of the other.
if, for any two regions in or one is a proper subset  W  Y  X  e1  e2  e3  {} {a,d}  W  Z  e4  e5  e6  X  Figure 3: A region hierarchy The events in a hierarchical task network form a region hierarchy, since they have the property described in Definition 2.  e1  {a,b,c} {a,b,c}  {a,b,c,d,e,f,g,h}  Local conditions  {a,d}  Subgoal conditions  {a} {a,b,c} {b,c}  {a,d} {a,d} {g,h}  {b,c}  {}  e2  Z  {a,b,c} {a,b,c}  {}  {}  {}  {}  Denition 3.
In a region hierarchy, region X is a  child region of another region Y if Y is the smallest region containing X .
is a 3-tuple hR  fii, where R is a region,  = fR1 R2 : : :g is the set of child regions of R, and fi is a partial order on .
The events in Ri must occur before the events in Rj if Ri fi Rj .
{a,d} {a,d}  e5  e6  {g} {g}  Y  {a,d} {a,d} {h} {h}  {d} {d,e,f} {e,f}  For a task k, the regions associated with k's subtasks are the child regions of the region associated with k.  Denition 4.
A hierarchical ordering constraint  Abstract conditions Coupling conditions  {e,f}  e3  {d,e,f} {d,e,f} {} {}  e4  {d,e,f} {d,e,f} {} {}  A hierarchical ordering constraint models a partial order on the subtasks of a task.
Figure 4: Characteristic condition subsets  In our example problem instance, S XS= fe1 e2g, Y = fe3  e4g, Z = fe5  e6g, W = X Y Z and every individual event are regions.
Figure 3 depicts the corresponding region hierarchy.
The possible event sequences regarding our example problem instance are determined by the following hierarchical ordering constraints: hX fe1  e2g fe1 fi e2 gi, hY fe3  e4g i, hZ fe5 e6g i, and hW fX Y Z g fY fi X gi.
dependent of R. If p is independent of R, we do not need to concern ourselves with p in reasoning about the changes caused by R. In Figure 1, the conditions a, b, c are dependent on events e1 , e2 and region X while the conditions d, e, f , g, h are independent of events e1 , e2 and region X .
4 Locality in Causal Interactions  In this section, we derive useful knowledge regarding the causal interactions among regions.
This knowledge allows us to describe subgoals and abstractions for individual regions, which yields a localized reasoning algorithm in next section.
Our investigation focuses on the causal dependencies among regions and subsets of conditions.
Events that a ect all or even most conditions are rare.
An event tends to a ect or be a ected by a small subset of the set of conditions P .
We say that a condition p is dependent on an event e if (1) event e may change the truth value of condition p immediately following e or (2) the truth value of condition p prior to event e may a ect the truth values of some other conditions immediately following e otherwise, we say that condition p is independent of event e. Similarly, a condition p is dependent on region R if p is dependent on at least one of the events encapsulated in R otherwise p is in-  4.1 Characteristic Condition Subsets  In the following, we generalize this notion of dependency to characterize subsets of conditions for each individual region R in a given problem instance, namely (1) the set of local conditions of R, (2) the set of subgoal conditions of R, (3) the set of abstract conditions of R, and (4) the set of coupling conditions of R. These subsets of conditions characterize the causal interactions among regions.
Figure 4 shows such knowledge derived from the example problem instance in Figure 1.
Local conditions: A condition p is a local condition of region R if p is dependent on R but not dependent on any events outside R. The statuses of the local conditions of R can only a ect and be a ected by the events in R. The initial statuses of the local conditions of R will not be changed until the events in R occur and change their statuses.
Subgoal conditions: The subgoal conditions of R describe the regional subgoal in R. A condition p is a subgoal condition of region R if (1) p is a local con-  dition of R, and (2) p is dependent on more than one child region of R if R has more than one child region.
As soon as all the events in R occur, the statuses of the subgoal conditions of R will not be changed any more.
At that time, the statuses of the subgoal conditions must have the specified conditions given by the goal of the problem instance.
This is the subgoal to be achieved by the events in R. For example, in Figure 1 the status of the subgoal condition d in region Y must be false immediately after region Y is finished.
Abstract conditions: We use the knowledge of the abstract conditions of a region R in deriving the abstraction of R. A condition p is an abstract condition  of region R if p is (1) dependent on at least one event in R, and also (2) dependent on at least one event not in R. Both the events in R and the events outside R can a ect and be a ected by the statuses of the abstract conditions of R. Therefore the set of abstract conditions of R is the media for inter-regional interactions between the events in R and the events outside R. For example, in Figure 1 the events e1 , e2 in region X interact with the other events through the abstract condition a of X .
Coupling conditions: In next section, we use the  knowledge of the coupling conditions of a region R in constructing the local search space for R.  A condition p is a coupling condition of region R if (1) p is an abstract condition of region R, or (2) p is an abstract condition of one or more child regions of R. The coupling conditions of R are the media for (1) the inter-regional interactions between the events in R and the events outside R, and (2) intra-regional interactions among di erent child regions of R. For example, in Figure 1 the three child regions X , Y , and Z of region W interact with one another through the coupling conditions a, d of W .
4.2 Subgoals and Abstractions  Based on the knowledge of the sets of subgoal conditions and abstract conditions of individual regions, we define the concepts of regional subgoals and the abstract events of individual regions, which yields the localized reasoning algorithm in Section 5.
4.2.1 Regional subgoals  The statuses of the subgoal conditions of R must have the specified statuses described by the goal as soon as the events in R have all occurred, and the statuses of the subgoal conditions of R will not be changed after that time.
Denition 5.
The regional subgoal GR is a subset of  the goal G .
GR is composed of the condition/status pairs in G whose condition component is a subgoal condition of region R.  Figure 5 depicts the regional subgoals regarding the example problem instance in Figure 1.
By achiev-  W Regional subgoal  {(a,false),(d,false)} X {(b,false)} e1 {}  Z  Y  e2 {}  {}  {(e,false)} e3 {}  e4 {}  e5 {}  e6 {(h,false)}  Figure 5: Subgoals in individual regions ing the subgoal in every individual region, the goal is achieved.
This describes a problem decomposition for temporal reachability.
Our task in an individual region R is to (1) achieve the subgoals in the children regions of R recursively, (2) determine the ordering of the child regions of R to achieve the regional subgoal of R. For example, the subgoal for Y is f(e false)g, which indicates that the status of condition e must be false immediately after the events in Y have all occurred.
he3  e4 i is the only ordering of e3 , e4 to achieve this subgoal.
4.2.2 The abstractions of individual regions  The e ects caused by the events in a region R are determined by (1) the initial statuses of the local conditions of R, (2) the statuses of the abstract conditions of R immediately before the events in R occur, and (3) the ordering of the events in R, which can be recursively described by the orderings on the child regions of R and R's descendant regions.
In general, there are many possible orderings on the child regions of R to achieve the subgoal in an individual region R. In order to achieve the subgoals in region R, (1) the set of abstract conditions of R must have appropriate statuses immediately before any events in R occur, and (2) the child regions in R must be properly ordered.
In other words, (1) whether or not an ordering on R's child regions may achieve the subgoal in R depends on the statuses of the abstract conditions of R immediately before the events in R occur, and (2) in turn, that ordering also a ects the statuses of the abstract conditions of R after achieving the subgoal in R. An abstract event eR of a region R provides a causal abstraction of R regarding how to achieve the subgoal in R using the events in R. Denition 6.
The abstract event eR of a region R is represented by a set of causal rules.
Each causal rule of eR encodes one possibility regarding (1) the statuses of the abstract conditions of R immediately before any events in R occur, which represent the antecedent requirements to achieve the subgoal, and (2)  W eW : If a=true and d=true, then a=false, d=false.
X e X: If a=true, then a=false.
Z  Y e Y : If d=true, then d=false.
eZ : If a=false and d=true, then a=true, d=false; If a=true and d=false, then d=true, a=false.
Figure 6: Abstract events for individual regions the statuses of the abstract conditions immediately after the regional subgoal is achieved by the events in R, which represent the consequent eects.
The abstract event eR is what we need to reason about the inter-regional interactions between the events in R and the events outside R. The derivation of abstract events for individual regions is investigated in Section 5.
Figure 6 depicts the abstract events of the individual regions in our example problem instance.
For example, the abstract event eY of region Y is associated with a single rule.
This is because (1) the subgoal for Y is f(e false)g, (2) when the condition d is true immediately before e3 and e4 occur, the event ordering he3  e4i can achieve the regional subgoal, and (3) when the condition d is false immediately before e3 and e4 occur, the subgoal cannot be achieved.
5 Localized Temporal Reasoning Using Subgoals and Abstractions  In this section, we first present temporal reachability as search in global search spaces.
We then investigate the use of local reasoning to expedite temporal reasoning by exploiting the structure of a region hierarchy.
Finally, we demonstrate the potential of substantial improvements in performance, which are quantified by the numbers of child regions and the sizes of the sets of coupling conditions of individual regions.
5.1 Temporal Reachability as Search  Given an instance of the temporal reachability problem hP  I  G  E  Oi, we describe the global search space as a directed graph G = (V A) where A node in V indicates (1) the state of the conditions in P , i.e., the status of p for each p in P , and and (2) for each event e in E , whether e has occurred.
An edge (u v) in A models the occurrence of an event e where (1) e can transform the statuses of the conditions in P from the state indicated by u to the state indicated  by v, (2) node u indicates that e has not yet occurred while node v indicates that e has occurred, and (3) e can occur immediately after those events that are marked as occurred at node u without violating the ordering constraints in O.
The task of temporal reachability can be viewed as search for a path from a root node u0 to any goal node t where (1) u0 is the node in which all events are marked as not yet occurred, and the conditions in P have the specified initial statuses given by I , and (2) a node t is a goal node if all events are indicated as occurred and the conditions have the statuses specified by the goal G .
Since an edge in G = (V A) models the occurrence of an event, such a path corresponds to a possible event sequence immediately following which the conditions have the statuses specified in G .
5.2 Local Reasoning in Local Search Spaces  In the following, we consider the kind of problem instances (hP  I  G  E  Oi) where the events in E form a region hierarchy according to a set of hierarchical ordering constraint O.
Instead of reasoning about (1) the whole set of events and (2) the whole set of conditions to achieve the goal, we can conduct local reasoning in each individual region about (1) the abstract events of their child regions and (2) the set of coupling conditions to achieve the regional subgoal.
The goal is attained by incrementally achieving the regional subgoals.
This yields the following localized temporal reasoning algorithms.
5.2.1 Constructing Local Search Spaces  For a region R, the local search space of R embeds the information regarding (1) how the child regions of R interact with one another through R's coupling conditions, and (2) the ordering constraints on the child regions of R. Given (1) the abstract events of region R's child regions and (2) the set of coupling conditions of R, we can construct R's local search space as a directed graph GR = (VR  AR ) in the following way.
 Construct a set of nodes VR such that each node in VR encodes one possibility regarding (1) the statuses of the set of coupling conditions, (2) for each child region R in R, whether the events in R as a whole have occurred.
 Construct an edge (u v) in AR if there exists a child region R of R such that (1) eR , the abstract event of R , can transform the statuses of the coupling conditions indicated by node u to the statuses indicated by node v while achieving the subgoal in R , (2) node u indicates that R has not yet occurred while node v indicates that R has occurred, and 0  0  0  0  0  0  0  0  (3) R can occur immediately after those child regions that are marked as occurred at node u without violating the ordering constraints on the child regions of R. (Note that edge (u v) models the occurrence of the events in the child region R as a whole.)
0  0  5.2.2 Deriving Abstract Events  Given the local search space GR = (VR  AR ) of a region R, we define the following two types of nodes in GR .
Type-I nodes: A node v in VR is a Type-I node if all child regions of R are marked as not yet occurred at v. Type-I nodes: A node v in VR is a Type-II node if (1) at v, all child regions of R are marked as occurred, and (2) at v, the subgoal conditions of R have the statuses specified by the regional subgoal of R. Type-I nodes represent the possible statuses of the coupling conditions of R immediately before any child region of R occurs.
Type-II nodes represent the possible statuses of the coupling conditions of R immediately after all child regions of R occur and the regional subgoal of R is attained.
Property 1.
The local search space GR of a region  R is a directed acyclic graph.
A path in GR from a Type-I node to a Type-II node represents an ordering of the child regions of R to achieve the regional subgoal in R.  Procedure Region-Abstraction Input: (1) the abstract events of the child regions of R, (2) the regional subgoal of R. Output: if the regional subgoal of R can be achieved,  report the abstract event eR  otherwise, stop and report failure in achieving the subgoal.
1.
Construct the local search space GR = (VR AR ).
Derive the reachability information between Type-I nodes and Type-II nodes by searching GR .
2.
If no Type-I nodes can reach Type-II nodes, stop and report failure.
3.
If a Type-I node u can reach a Type-II node v, we encode a rule r associated with the abstract event eR such that (i) the antecedent requirement of r is that the abstract conditions of region R must have the statuses indicated at node u, and (ii) the consequent e ect of rule R is that the abstract conditions of region R must have the statuses indicated at node v.  5.2.3 Achieving Regional Subgoals  For a region R, the following procedure generates a sequence of the events in R to achieve all the subgoals of R and R's descendant regions.
Procedure Generate-Sequence Input: (1) the statuses of R's coupling conditions  before the events in R occur, (2) the abstract events and the subgoals of R and R's descendant regions.
Output: a sequence of the events in R to achieve the subgoals of R and R's descendant regions.
1.
Search the local search space GR for a path from u to v where (i) u is a Type-I node in which R's coupling conditions have the specified initial statuses and, (ii) v is a Type-II node.
2.
According to the derived path (by Property 1), derive (i) an ordering of the child regions of R, (ii) for each child region R of R, the statuses of the coupling conditions of R immediately before the events in R occur.
3.
For each child region R of R, recursively calls procedure Generate-Sequence to generate a sequence of the events in R to achieves the regional subgoals of R and the child regions of R .
According to the derived ordering of R's child regions, concatenate these sequences to generate a sequence of the events in R. 0 0  0  0  0  0  0  5.2.4 A Localized Temporal Reasoning Algorithm  In the following, we present a localized temporal reasoning algorithm for temporal reachability.
Procedure Localized-Reasoning input: a problem instance hP  I  G  E  Oi where the events in E form a region hierarchy.
output: if there exist possible event sequences to  achieve the goal, report one such sequence otherwise, report failure in finding such a sequence.
1.
Derive the knowledge regarding the sets of abstract conditions, coupling conditions, and subgoals of individual regions.
2.
Starting from the bottom level of the region hierarchy, we conduct local reasoning described in Step 3 for the regions at the same level respectively, and then proceed in the same way level by level until we finish the local reasoning in the root region of the region hierarchy.
3.
For each individual region R, call procedure Region-Abstraction to derive the abstract event eR to achieve the regional subgoal of R. If the regional subgoal can not be achieved, stop and report failure otherwise, propagate the knowledge of eR to R's parent region.
4.
In the root region, according to the initial statuses of the coupling conditions call procedure GenerateSequence to generate a solution event sequence.
5.2.5 Quantifying the Computation Eciency Theorem 1.
The time complexityPof the2(Blocalized +C ) temporal reasoning algorithm is O( R 2 R R ) where BR and CR are the number of the child regions and the number of the coupling conditions of an individual region R.  Proof (Sketch).
The local search space GR =  (VR  AR ) is a directed acyclic graph of size O(2BR +CR ).
To search the graph, record and process the reachability information, it takes O(22(BR +CR ) ) time and space.
2 The localized reasoning algorithm is a polynomialtime algorithm if BR and CR are of O(log jEj) size for each individual region R. Since temporal reachability is NP-Complete 10], this demonstrates the potential for dramatic improvements in performance by exploiting inherent locality, which happens when the numbers of child regions (BR 's) and the sizes of the sets of coupling conditions (CR 's) in individual regions are small with respect to the total number of events jEj and the total number of conditions jPj.
The performance degrades to be exponential in jEj when some BR or CR is of O(jEj) size.
This is under our expectation, since (1) a set of totally unordered events corresponds to a single region containing the individual events as child regions, and (2) temporal reachability regarding totally unordered events is NPcomplete 10].
5.2.6 An example.
We illustrate the use of the localized reasoning algorithm in solving our example problem instance in Figure 1.
First, we derive causal knowledge and the subgoals for individual regions depicted in Figure 4 and Figure 5 respectively.
At the bottom level, we conduct local reasoning regarding the six events respectively, which are regions by themselves.
The subgoal in e6 is that h must be false after e6 occurs, while we have null subgoals for the other five events.
The subgoal in e6 is always achieved, since e6 can only make h false and h is false initially.
At the second level of the region hierarchy, we conduct local reasoning regarding the regions X , Y and Z respectively.
The subgoal in region X (Y ) is that b (e) must be true (false) after all the events in region X (Y ) occur, while we have null subgoal in region Z .
We call procedure Region-Abstraction to determine the abstract event eX (eY and eZ respectively).
At the top level, the subgoal in regionW is to make the conditions a and d true after all events occur.
Here the child regions X , Y and Z are treated as the abstract events eX , eY , eZ respectively.
We call procedure Region-Abstraction to determine eW .
Finally, we call procedure Generate-Sequence to derive a solution sequence, given that the coupling conditions a and d of region W are initial true.
Procedure Generate-Sequence recursively generates the ordering heZ  eY  eX i, he5  e6i, he1  e2 i, and he3  e4i for the regions W , Z , Y , X respectively.
We replace eZ ,eY and eX in heZ  eY  eX i with he5  e6 i, he1  e2i, he3  e4i respectively, and derive he5  e6 e1 e2  e3 e4 i as a sequence to achieve the goal.
6 Conclusion  We investigate the locality in (1) the ordering constraints that group events hierarchically into sets called regions, and (2) the dependencies among conditions and regions.
This enables us to describe subgoals and abstractions for individual regions.
We develop a localized temporal reasoning algorithm to exploit locality and demonstrate the potential for dramatic improvements in performance.
References  1] Backstrom, C. and Klein, I., Parallel Non-Binary Planning in Polynomial Time, Proceedings IJCAI 12, Sydney, Australia, IJCAII, 1991, 268{ 273.
2] Bylander, Tom, Complexity Results for Planning, Proceedings IJCAI 12, Sydney, Australia, IJCAII, 1991, 274{279.
3] Chapman, David, Planning for Conjunctive Goals, Articial Intelligence, 32 (1987) 333{377.
4] Christensen, J., A Hierarchical Planner that Generates its own Abstraction Hierarchies, Proceedings AAAI-90, AAAI, 1990, 1004{1009.
5] Dean, Thomas and Boddy, Mark, Reasoning About Partially Ordered Events, Articial Intelligence, 36(3) (1988) 375{399.
6] Gupta, Naresh and Nau, Dana S., Complexity Results for Blocks-World Planning, Proceedings AAAI-91, Anaheim, California, AAAI, 1991, 629{633.
7] Knoblock, Craig A., Search Reduction in Hierarchical Problem Solving, Proceedings AAAI-91, Anaheim, California, AAAI, 1991, 686{691.
8] Korf, Richard, Planning as Search: A Quantitative Approach, Articial Intelligence, 33(1) (1987) 65{88.
9] Lansky, Amy L., Localized Event-Based Reasoning for Multiagent Domains, Computational Intelligence, 4(4) (1988).
10] Lin, Shieu-Hong and Dean, Thomas, Exploiting Locality in Temporal Reasoning, Proceedings of the Second European Workshop on Planning, Vadstena, Sweden, 1993.
11] Nebel, Bernhard and Backstrom, Christer, On the Computational Complexity of Temporal Projection, Planning, and Plan Validation, Articial Intelligence, (1993), To appear.
12] Sacerdoti, Earl, Planning in a Hierarchy of Abstraction Spaces, Articial Intelligence, 7 (1974) 231{272.
13] Yang, Qiang and Tenenberg, Josh D., Abtweak: Abstracting a Nonlinear, Least Commitment Planner, Proceedings AAAI-90, AAAI, 1990, 204{209.
Believing Change and Changing Belief Peter Haddawy  haddawy@cs.uwm.edu Department of Electrical Engineering and Computer Science University of Wisconsin-Milwaukee Milwaukee, WI 53201  Abstract  We present a rst-order logic of time, chance, and probability that is capable of expressing the relation between subjective probability and objective chance at dierent times.
Using this capability, we show how the logic can distinguish between causal and evidential correlation by distinguishing between conditions, events, and actions that 1) in	uence the agent's belief in chance and 2) the agent believes to in	uence chance.
Furthermore, the semantics of the logic captures commonsense inferences concerning objective chance and causality.
We show that an agent's subjective probability is the expected value of its beliefs concerning objective chance.
We also prove that an agent using this representation believes with certainty that the past cannot be causally in	uenced.
1 Introduction  The ability to distinguish evidential from causal correlation is crucial for carrying out a number of dierent types of problem solving.
To perform diagnosis we must be able to identify the factors that caused an observed failure in order to determine how to repair the faulty device.
If we cannot distinguish causal from evidential correlation, we may end up treating the symptoms rather than the causes of the fault.
When reasoning about plans, an agent may have goals that involve achieving a specied state of the world, or achieving a specied state of knowledge, or a combination of both.
In order to eectively reason about such goals, we need to distinguish actions that in	uence the state of the world from those that only in	uence our state of knowledge of the world.
In this paper we extend Haddawy's 3] logic of time, chance, and action L by adding a subjective probability operator.
We show how the resulting rst-order logic of time, chance, and probability, L , can distinguish between causal and evidential correlation by distinguishing between conditions and events that 1) in	uence the agent's belief in chance and 2) the agent believes to in	uence chance.
Furthermore, the semantics of the logic captures some commonsense inferences tca  tcp  This work was partially supported by NSF grant #IRI9207262.
concerning causality and the relation between objective chance and subjective probability.
We prove that an agent's subjective probability is the expected value of its beliefs concerning objective chance.
We also prove that an agent whose beliefs are represented in this logic believes with certainty that the past cannot be causally in	uenced.
On the other hand, an agent can execute actions that in	uence its subjective beliefs about the past.
2 Ontology  We brie	y present the ontology of the logic, which includes the representation of time, facts, events, objective chance, and subjective probability.
For simplicity of exposition, we will omit the representation of actions and will treat them as events.
For a more detailed development of chance, facts, events, and actions see 3].
Time is modeled as a collection of world-histories, each of which is one possible chronology or history of events throughout time.
A totally ordered set of time points provides a common reference to times in the various world-histories.
We represent an agent's beliefs with subjective probabilities.
Since beliefs may change with time, subjective probability is taken relative to a point in time.
We represent it by dening a probability distribution over the set of world-histories at each point in time.
So an agent can have beliefs concerning temporally qualied facts and events.
We represent causal correlation with objective chance.
Objectively, actions and events can only aect the state of the world at times after their occurrence.
That is to say, at each point in time, the past is xed| no occurrences in the world will cause it to change but at each point in time the future might unfold in any number of ways.
So relative to any point in time, only one objectively possible past exists, but numerous possible futures exist.
Thus we represent objective chance by dening a future-branching tree structure on the world-histories and by dening probabilities over this tree.
Like subjective probability, chance is taken relative to a point in time.
By dening chance in this way, conditions in the present and past relative to a given time are either certainly true of certainly false.
So actions and other events can only aect the chances of future facts and events.
This property distinguishes objective chance from subjective probability.
Subjectively the past can be uncertain but objectively it is completely determined.
The present characterization of objective chance is not to be confused with the frequentist interpretation of probability 10, 11] which is often called objective probability.
Frequentist theories dene probability in terms of the limiting relative frequency in an innite number of trials or events.
The current work does not rely on relative frequencies for its semantics.
Rather it models objective chance by formalizing the properties that characterize objective chance.
Thus while frequentist theories have diculty assigning meaningful probabilities to unique events like a sh jumping out of the water at a given location and time, our model has no problem in assigning nontrivial probabilities to such events.
Our model of objective chance and subjective probability is motivated by the subjectivist theories of objective chance 6, 8, 9], which dene chance in terms of properties that one would expect a rational agent to believe objective chance to possess.
This distinction between the frequentist theory of probability and our conception of objective chance puts the present work in sharp contrast with Bacchus's 1] logic of statistical probabilities which models exactly relative frequency type probabilities.
One telling dierence between the two logics is that Bacchus's logic Lp assigns only probability 0 or 1 to unique events (more precisely, to all closed formulas).
The present logic can assign any chance value to unique events in the future, while events in the past are assigned only chance values 0 or 1, as required by our denition of objective chance.
It is reasonable to expect the subjective beliefs of a rational agent concerning objective chance to obey certain constraints.
Skyrms 7, Appendix 2] has argued for a constraint he calls Millers' principle.
This asserts that an agent's subjective belief in a proposition, given that he believes the objective chance to be a certain value, should be equal to that value.
Skyrms argues that this is a plausible rule for assimilating information about chance.
We will call this relation the subjective/objective Miller's principle.
The world is described in terms of facts and events.
Facts tend to hold and events tend to occur over intervals of time.
So facts and events are associated with the time intervals over which they hold or occur in the various world-histories.
Facts are distinguished from events on the basis of their temporal properties.
A fact may hold over several intervals in any given world-history and if a fact holds over an interval then it holds over all subintervals of that interval.
Events are somewhat more complex than facts.
First, one must distinguish between event types and event tokens.
An event type is a general class of events and an event token is a specic instance of an event type.
Event tokens are unique individuals { the interval over which an event token occurs is the unique interval associated with the event token and an event token can occur at most once in any world-history.
The present work deals with event types, which for brevity are simply referred to as events.
3 The Logic of Time, Chance, and Probability 3.1 Syntax  The language of L contains two predicates to refer to facts and event types occurring in time: HOLDS (FA t1 t2) is true if fact FA holds over the time interval t1 to t2, and OCCURS (EV t1  t2) is true if event EV occurs during the interval t1 to t2 .
Henceforth we will use the symbol t, possibly subscripted, to denote time points , fi, and  to denote formulas and  and  to denote probability values.
In addition to the usual rst-order logical operators, the language contains two modal operators to express subjective probability and objective chance.
The operators are subscripted with a time since according to the ontology subjective probability and objective chance are taken relative to a point in time.
We write P () to denote the subjective probability of  at time t and we write pr () to denote the objective chance of  at time t. Probability is treated as a sentential operator in the object language.
So the probability operators can be arbitrarily nested and combined with one another, allowing us to write complex sentences like: \I believe there was a one in a million chance of my winning the lottery, yet I won."
P 3 (pr 2 (OCCURS (win t1 t2)) = 10;6^ OCCURS (win t1 t2)) = 1 where t1 < t2 < t3.
We also allow conditional probability sentences such as P (jfi) = , which is interpreted as shorthand for P ( ^ fi) =   P (fi).
The language of L is fully rst-order, allowing quantication over time points, probability values, and domain individuals.
A formal specication of the syntax is provided in the full paper 2].
tcp  t  t  t  t  t  t  t  tcp  3.2 Semantics  We describe only the more interesting aspects of the models of L .
The models are completely specied in the full paper.
A model is a tuple hW D, FN, NFN, PFN, FRL, ERL, NRL, FA, EVENTS, EV, R, X , PR , PR , F i, where: fi W is the set of possible world-histories, called worlds.
fi D is the non-empty domain of individuals.
fi FA is the set of facts, a subset of 2(<<) .
A fact is 0 a set of htemporal interval, worldi pairs: fhht1 t1 i w1i ::: hht  t0 i w ig.
If fa is a fact and hht1  t2i wi 2 fa then fa holds throughout interval ht1  t2i in world-history w. fi EVENTS is the set of event tokens, a subset of (<<)  W .
An event token is a single htemporal interval, worldi pair.
fi EV is the set of event types, a subset of 2EVENTS .
An event type is a set of event tokens: fhht1 t01 i w1i ::: hht  t0 i w ig.
If ev is an event and hht1  t2i wi 2 ev then ev occurs during interval ht1 t2i in world-history w. tcp  o  s  W  n  n  n  n  n  n  1.
HOLDS (rf (trm 1  ::: trm ) ttrm1  ttrm 2)]] = true i hh ttrm 1 ]   ttrm 2] i wi 2 F (rf )(trm 1]  :::  trm ] ): 2.
OCCURS (re(trm 1  ::: trm ) ttrm 1  ttrm 2)]] = true i hh ttrm 1 ]   ttrm 2] i wi 2 e for some e 2 F (re)(trm 1 ]  ::: trm ] ): 0 3.  prttrm ()]] =  o ttrm] (fw 2 R ttrm] :  ] = trueg).
4.
Pttrm ()]] =  s ttrm] (fw0 :  ] = trueg).
Mwg  n Mwg  Mwg  Mwg  Mwg  n  Mwg  n Mwg  Mwg  Mwg  Mwg  n  Mwg  Mwg  w  w  Mwg  w  0  Mw g  Mwg  0  Mw g  Mwg  Figure 1: Semantic denitions fi R is an accessibility relation dened on <W W .
R(t w1 w2) means that world-histories w1 and w2 are indistinguishable up to and including time t. If R(t w1 w2) we say a world-history w2 is Raccessible from w1 at time t. The set of all worldhistories R-accessible from w at time t will be designated R .
For each time t, the R partition the world-histories into sets of equivalence classes indistinguishable up to t. fi X is a -algebra over W 1 , containing all the sets corresponding to w's in the language, as well as all R-equivalence classes of world-histories.
fi PR is the objective probability assignment function that assigns to each time t 2 < and worldhistory w 2 W a countably additive probability distribution  o dened over X .
fi PR is the subjective probability assignment function that assigns to each time t 2 < and worldhistory w 2 W a countably additive probability distribution  s dened over X .
Given the models described above, the semantic definitions for the well-formed formulas can now be dened.
Denotations are assigned to expressions relative to a model, a world-history within the model, and an assignment of individuals in the domain to variables.
The denotation of an expression  relative to a model M and a world-history w, and a variable assignment g is designated by  ] .
Figure 1 shows the less familiar semantic denitions.
The remainder are provided in the full paper.
w t  w t  o  w t  s  w t  Mwg  3.2.1 Semantic Constraints  In order to obtain the properties discussed in the ontology, we impose eight constraints on the models.
The future-branching temporal tree is dened in terms of the R relation over world-histories.
To capture the property that the tree does not branch into the past, we say that if two world-histories are indistinguishable up to time t2 then they are indistinguishable up to any earlier time: (C1) If t1t2 and R(t2 w1 w2) then R(t1 w1 w2).
A -algebra over W is a class of subsets that contains W and is closed under complement and countable union.
1  Since R just represents the indistinguishability of histories up to a time t, for a xed time R is an equivalence relation, i.e., re	exive, symmetric, and transitive: (C2) R(t w w) If R(t w1 w2) then R(t w2 w1) If R(t w1 w2) and R(t w2 w3) then R(t w1 w3) As mentioned earlier, facts and events dier in their temporal properties.
This distinction is captured by the following two semantic constraints.
If a fact holds over an interval, it holds over all subintervals, except possibly at the endpoints: (C3) If t1 t2 t3t4 t1 6= t3  t2 6= t4  fa 2 FA and hht1 t4i wi 2 fa then hht2  t3i wi 2 fa : An event token occurs only once in each world-history: (C4) If evt 2 EVENTS , hht1 t2i wi 2 evt, and hht3  t4i wi 2 evt then t1 = t3 and t2 = t4 .
If two worlds are indistinguishable up to a time then they must share a common past up to that time.
And if they share a common past up to a given time, they must agree on all facts and events up to that time.
To enforce this relationship, we impose the constraint that if two world-histories are R-accessible at time t, they must agree on all facts(events) that hold(occur) over intervals ending before or at the same time as t: (C5) If t0 t1 t2 and R(t2  w1 w2) then hht0  t1i w1i 2 A i hht0  t1i w2i 2 A, where A is a fact or event.
The ontology discussed two desired characteristics of objective chance.
The rst is that the chance at a time t be completely determined by the history up to that time.
The second desired characteristic is that the chance of the present and past should be either zero or one, depending on whether or not it actually happened.
These two properties follow as meta-theorems from the following two constraints: (C6) For all 0 X 2 X  tt0 and w w0 such that R(t w w )   (R ) > 0 !
(X ) =   (X jR ).
w t  w t  0  w t0  0  w t  w t0  0  (C7)   (R ) > 0.
Meta-theorem 1 The probability of the present and w t  w t  past is either zero or one.
(R ) = 1 w t  w t  Theorem 7 Objective Miller's Principle (OMP)  1.
(R ) > 0 (C7) 2.
(R ) =   (R jR ) Modus Ponens: (C6),1 3.
(R ) = 1 def of c-prob w t w t w t  w t w t w t  w t  w t  w t  tcp  t  Dening the probabilities in this way makes good intuitive sense if we look at the meaning of R. R designates the set of world-histories that are objectively possible with respect to w at time t. It is natural that the set of world-histories that are objectively likely with respect to w at time t should be a subset of the ones that are possible.
w t  Meta-theorem 2 If two worlds are indistinguishable  up to time t then they have identical probability distributions at that time.
If R(t w w0) then   (X ) =   (X ) w t  1.
2.
3.
4.
5.
0  w t  (R ) > 0 (C2), (C7) (R ) =   (X jR ) Modus Ponens: (C6),1 (X jR ) =   (X jR ) (C2) (R ) = 1 Meta-theorem 1 (X ) =   (X ) def of c-prob  w t w t w t w t 0 w t  0  w t 0 w t  w t  w t  w t  0  w t  w t  0  w t  w t  In the ontology, we argued that subjective probability and objective chance should be related to one another by Millers' principle.
This relation is enforced by the following constraint, which says that the probability of a set of worlds X , given some R equivalence class, should just be the objective chance in that equivalence class.
(C8)  s (X jR ) =  o (X ) w t  w t  w t  4 Theorems  We rst provide several simple theorems that will be used in later proofs.
Then we prove two forms of Miller's principle and provide two associated expected value properties.
Proofs not provided here appear in the full paper.
Theorem 3 From  $ fi infer pr () = pr (fi): t  t  Theorem 4 Stronger sentences have lower probability.
From  !
fi infer P ()  P (fi).
Theorem 5 Certainty cannot be conditioned away from.
P ( ^ fi) = P (fi) !
P ( ^ fi ^  ) = P (fi ^  ) Theorem 6 The present and past are objectively certain.
Let be a fact or event: HOLDS ( t  t0 ) or OCCURS ( t  t0 ) then 8t t  t0 (t0  t) !
pr ( ) = 0 _ pr ( ) = 1] t  t  t  t          t  t      t  All instances of the following sentence schema are valid in L .
8 t0 t1 (t0  t1 ) !
pr 0 ( j pr 1 () = ) =  Proof:    t  The semantic constraints on objective chance give us a version of Miller's principle that relates objective chance at dierent times.
It says that the chance of a sentence  at a time, given that the chance of  at the same or a later time is , should be .
t  We rst prove an expected value property and then use it to prove Miller's principle.
Let t t0 be two time points t  t0 and consider the R-equivalence classes of worlds at time t0 .
Let the variable r range over these equivalence classes.
The r form a partition of W , so the probability of a set X can be written as the integral over this partition: Z  o (X ) =  o (X jr) o (dr)  Since the history up to time t0 determines the probability at time tZ0, this can be written as  o (X ) =  o (X ) o (dr)  where  o denotes the probability at time t0 in equivalence class r. Since the probability at a given time is assumed to be constant over all worlds in an Requivalence class, the probability at a given time is the expected value Z of the probability at any future time:  o (X ) =  o (X ) o (dw0 ): Next we show that Miller's principle is valid in the probability models.
By the expected value property,  o (ZX \ fw0 :  o (X ) = g) =  o (X \ fw0 :  o (X ) = g) o (dw00): Now, by semantic constraints (C6) and (C7) it follows that 8 w 2fw0 :  o (X ) = g  o (fw0 :  o (X ) = g) = 1 8 w 62fw0 :  o (X ) = g  o (fw0 :  o (X ) = g) = 0: So we can restrict the integral to the set fw0 :Z o (X ) = g: =  o (X \ fw0 :  o (X ) = g) o (dw00): w t  w t  r  W  r  W  w t  w t  r t0  w t  r t0  w t0  w t  0  w t  W  w t  w t0  w t0  0  00  w t0  0  w t  W  w t0  w t0  w t0  0  w t0  0  w t0  w t0  w t0  0  0  0  w  00  w t0  t0 0 w 0 fiow0 X t  f  ( )=g  :  0  w t  And by the above property again  o (XZ\ fw0 :  o (X ) = g) = , so =   o (dw00): w t0  00  w t0  f  0  ( )=g  0 w 0 fiow0 X t  :  w t  =    o (fw0 :  o (X ) = g): By the semantic denitions it follows that P ( ^ P () = ) =   P (P () = ): And by a slight generalization of the proof it follows that 8(t  t0) P ( ^ P ()  )    P (P ()  ): 2 From the Objective Miller's Principle it follows directly that current chance is the expected value of current chance applied to current or future chance.
w t  t  w t0  t0  0  t  t  t0  t0  t  t0  Theorem 8 Objective Expected Value Property  All instances of the following sentence schema are valid in L .
8  t1 t2 (t1  t2 ) !
pr 1 (pr 2 ()  )   !
pr 1 ()     ] tcp  t  t  t  As discussed in the ontology, the current subjective probability of a sentence, given that the current or future chance is some value should be that value.
The following theorem shows that this property follows from the semantics of the logic.
Theorem 9 Subjective/Objective Miller's Principle (SOMP)  All instances of the following sentence schema are valid in L .
8 t0 t1(t0  t1 ) !
P 0 (jpr 1 () = ) =  Proof: We rst prove an expected value property and tcp  t  t  then use it to prove Miller's principle.
Let t t0 be two 0 time points t  t and consider the R-equivalence classes of worlds at time t0 .
Let the variable r range over these  equivalence classes.
The r form a partition of W , so the probability of a set X can be written as the integral over this partition:   s (X ) =  Z  w t  r   s (X jr) s (dr) w t    W  w t  Z  w t  r   o (X ) s (dr) r t    W  w t  where  o denotes the objective chance at time t in equivalence class r. Since the chance at a given time is assumed to be constant over all worlds in an Requivalence class, the subjective probability at any time is the expected value of the subjective probability applied to the objective chance at that time: r t   s (x) =  Z  w t   o (X ) s (dw0) w t  W  0  w t  Next we show that the Subjective/Objective Miller's principle is valid in the probability models.
By the above subjective/objective expected value property,  s (ZX \ fw0 :  o (X ) = g) =  o (X \ fw0 :  o (X ) = g) s (dw00) By Objective Miller's Principle,  s (XZ\ fw0 :  o (X ) = g) =   o (fw0 :  o (X ) = g) s (dw00) w t  w t0  w t  0  00  w t0  0  w t  W  w t  w t0  w t  0  00  w t0  W  0  w t  Finally, by the subjective/objective expected value property,  s (X \ fw0 :  o (X ) = g) =  s (fw0 :  o (X ) = g) So by the semantic denitions it follows that 8t t0 (t  t0 ) !
P (jpr () = ) =  w t  w t0  w t  0  w t0  t  0  t0  t  t0  Theorem 10 Subjective/Objective Value Property  Expected  All instances of the following sentence schema are valid in L .
8  t1 t2 (t1  t2) !
P 1 (pr 2 ()  )   !
P 1 ()     ] tcp  t  t  t  5 Distinguishing Evidential and Causal Correlation  We wish to distinguish between two situations in which an agent may believe that two conditions are correlated.
An agent may believe that two conditions are correlated because one is simply evidence for another and an agent may believe that they are correlated because one causes the other.
Let stand for the formula HOLDS ( t  t0 ) or OCCURS ( t  t0 ) and let !
stand for the formula HOLDS (fi t  t0 ) or OCCURS (fi t  t0 ).
We represent evidential correlation as correlation in the subjective probability distribution, which is the standard approach in Bayesian decision theory.
Denition 11 We say that !
is evidence for or   By semantic constraint (C8), this can be written as   s (X ) =  And by a slight generalization of the proof it follows that 8t t0 (t  t0 ) !
P (jpr ()  )   2 From the subjective/objective Miller's principle it follows directly that subjective probability is the expected value of current subjective probability applied to present or future chance.
          against i Pnow ( j !)
= 6 Pnow ( )      (1) It follows from this denition that !
is not evidence for or against i Pnow ( j !)
= Pnow ( ) We represent causal correlation by reference to the objective chance distribution.
We represent an agent's belief that !
causally in	uences by saying that there is some value for the objective chance of such that the agent's belief in given the objective chance of just before !
holds or occurs is not the same as the agent's belief given also knowledge of !.
In other words, knowledge of !
overrides knowledge of the objective chance of .
Denition 12 We say that !
is a cause of i 9 Pnow ( j pr ( ) =  ^ !)
6= : (2) Note that this does not necessarily imply that Pnow ( j!)
6= Pnow ( ).
Thus we may have causal correlation without evidential correlation and, conversely, we may have evidential correlation without causal correlation.
It follows from this denition that !
is not a cause of i 8 Pnow ( j pr ( ) =  ^ !)
= : t  t  5.1 Example  We now present an example demonstrating the use of the denitions and theorems.
We wish to describe the following situation.
You have a coin that may be biased 3:1 towards heads or 3:1 towards tails.
You believe there is an equal probability of each.
You can observe the coin.
If the coin looks shiny, this increases your belief that the coin is biased towards heads.
You also have a magnet that you can use to in	uence the outcome of the coin toss.
Turning on the magnet biases the coin more toward heads.
We can describe the situation with the following set of sentences in which \heads" is the event of the coin landing heads, \shiny" is the event of the coin being observed to be shiny, and \magnet" is the fact that the magnet is on.
(now < t0 < t1 < t2 < t3 < t4 ) Turning on the magnet in	uences the chance of heads.
Pnow (OCCURS (Heads t2 t3)j (3) pr 1 (OCCURS (Heads t2 t3)) = 3=4 ^ HOLDS (Magnet t1  t4)) = 7=8 Pnow (OCCURS (Heads t2 t3)j (4) pr 1 (OCCURS (Heads t2 t3)) = 1=4 ^ HOLDS (Magnet t1  t4)) = 1=2 The probability that the coin is biased toward heads and the probability that the coin is biased toward tails are equal.2 Pnow (pr 1 (OCCURS (Heads t2 t3)) = 3=4) = (5) Pnow (pr 1 (OCCURS (Heads t2 t3)) = 1=4) = 1=2 Observing the coin doesn't in	uence the chance of heads.
(6) 8 t (t > now) !
Pnow (OCCURS (Heads t2 t3) j pr (OCCURS (Heads t2 t3)) =  ^ OCCURS (Shiny t0  t2)) =  Observing the coin gives us knowledge of its bias.
Pnow (pr 0 (OCCURS (Heads t2 t3)) = 3=4 j (7) OCCURS (Shiny t0  t2)) = 5=8 Pnow (pr 0 (OCCURS (Heads t2 t3)) = 1=4 j (8) OCCURS (Shiny t0  t2)) = 3=8 Turning on the magnet does not give us knowledge of the coin's bias.
8 Pnow (pr 1 (OCCURS (Heads t2 t3)) =  j (9) HOLDS (Magnet t1  t4)) = Pnow (pr 1 (OCCURS (Heads t2 t3)) = ) The coin is either biased toward heads or toward tails.
8t pr (OCCURS (Heads t2 t3 )) = 3=4 _ (10) pr (OCCURS (Heads t2 t3)) = 1=4 (11) t  t  t  t  t  t  t  t  t  t  t  It would be more appropriate to say that our belief that the current chance is 3/4 or 1/4 is 1/2 and that in the absence of events that will inuence the chance, chance will remain unchanged till time t1 .
Such an inference would require some kind of theory of persistence, which is beyond the scope of this paper.
2  Using this information, we can make several useful inferences.
First we can derive the unconditional probability that the coin will land heads.
From (5) by SOMP we have Pnow (OCCURS (Heads t2 t3)) = (12) (1=2)(3=4) + (1=2)(1=4) = 1=2 Next, we can use the above information to derive the probability that the coin will come up heads given that it is observed to be shiny.
Instantiating (6) with  = 3=4 and t = t0 and multiplying the result by (7) we get Pnow (OCCURS (Heads t2 t3)^ (13) pr 0 (OCCURS (Heads t2 t3)) = 3=4 j OCCURS (Shiny t0  t2)) = (5=8)(3=4) = 15=32 And instantiating (6) with  = 1=4 and t = t0 and multiplying the result by (8) we get Pnow (OCCURS (Heads t2 t3)^ (14) pr 0 (OCCURS (Heads t2 t3)) = 1=4 j OCCURS (Shiny t0  t2)) = (3=8)(1=4) = 3=32 From (10), (13), and (14) by the law of total probability we get Pnow (OCCURS (Heads t2 t3) j (15) OCCURS (Shiny t0  t2)) = 9=16 We can also derive the probability of heads given that we activate the magnet.
From (3), (5), and (9) we get Pnow (OCCURS (Heads t2 t3)^ (16) pr 2 (OCCURS (Heads t2 t3)) = 3=4 j HOLDS (Magnet t1 t4)) = (1=2)(7=8) = 7=16 From (4), (5), and (9) we get Pnow (OCCURS (Heads t2 t3)^ (17) pr 2 (OCCURS (Heads t2 t3)) = 1=4 j HOLDS (Magnet t1 t4)) = (1=2)(1=2) = 1=4 From (10), (16), and (17) by the law of total probability we get Pnow (OCCURS (Heads t2 t3) j (18) HOLDS (Magnet t1 t4)) = 11=16 t  t  t  t  5.2 The temporal ow of causality  Using our denition of causal in	uence and SOMP we can now show that an agent whose beliefs are represented with L believes that the past cannot be in	uenced.
tcp  Theorem 13 Let be a fact or event: 0 0  HOLDS ( t  t ) or OCCURS ( t  t ) and let !
be a fact or event: HOLDS (fi t  t0 ) or OCCURS (fi t  t0 ).
Then all instances of the following sentence schema are valid in L .
8 t t  t0  t  t0 (t0  t ) ^ (t  t ) !
P ( jpr ( ) =  ^ !)
=              tcp    t      t              Proof: We prove a slightly more general result of which  the above sentence is an instance.
By the Subjective/Objective Miller's Principle, 8 t t0 t  t0 (t0  t0 ) ^ (t  t0) !
(19) P ( ^ pr ( ) = ) =   P (pr ( ) = ) Since valid formulas have probability one, it follows by Theorem 6 that, 8 t t0 t  t0  t  t0 (t0  t0 ) ^ (t  t0) !
(20) P ( ^ pr ( ) =  ^ pr ( ) = 0 _ pr ( ) = 1]) =   P (pr ( ) =  ^ pr ( ) = 0 _ pr ( ) = 1]) Since pr ( ) = 0 and pr ( ) = 1 are mutually exclusive, we have 8 t t0 t  t0 (t0  t0 ) ^ (t  t0) !
(21) P ( ^ pr ( ) =  ^ pr ( ) = 0) + P ( ^ pr ( ) =  ^ pr ( ) = 1) =   P (pr ( ) =  ^ pr ( ) = 0) +   P (pr ( ) =  ^ pr ( ) = 1) Now we have three cases to consider: i)  = 0, ii)  = 1, iii) 0 <  < 1.
      t0  t            t0  t  t0  t0  t  t0  t  t0  t0  t0  t0  t0      t  t0  t  t0    t0 t0  t  t0  t0  t  t0  t0  Case i)  Expression (21) reduces to (22) 8t t0 t  t0  t  t0 (t0  t0 ) ^ (t  t0) !
P ( ^ pr ( ) = 0) = 0  P (pr ( ) = 0) So by Theorem 4 and universal generalization, 8t t0 t  t0  t  t0 (t0  t0 ) ^ (t  t0) !
(23) P ( ^ pr ( ) = 0 ^ !)
= 0  P (pr ( ) = 0 ^ !)
          t0  t      t0  t        t0  t  t0  t  Case ii)  Expression (21) reduces to (24) 8t t0 t  t0 (t0  t0 ) ^ (t  t0) !
P ( ^ pr ( ) = 1) = P (pr ( ) = 1) So by Theorem 5 and universal generalization, 8t t0 t  t0  t  t0 (t0  t0 ) ^ (t  t0) !
(25) P ( ^ pr ( ) = 1 ^ !)
= P (pr ( ) = 1 ^ !)
      t0  t    t          t0  t  t0  t  t0  Case iii)  For 0 <  < 1, P (pr ( ) = ) = 0.
So by Theorem 4 and universal generalization, 8 t t0 t  t0  t  t0 (26) 0 0 0 (t  t ) ^ (t  t ) ^ (0 <  < 1) !
P ( ^ pr ( ) =  ^ !)
= P (pr ( ) =  ^ !)
Therefore we have proven that the following sentence is valid (27) 8 t t0 t  t0  t  t0 (t0  t0 ) ^ (t  t0) !
P ( jpr ( ) =  ^ !)
=  from which it follows that the past cannot be in	uenced.
2 t0  t            t0  t    t    t0    t      t0  6 Related Work  Three outstanding subjective theories of objective chance from the philosophical literature are those of van Fraassen 9], Lewis 6], and Skyrms 7].
van Fraassen's model of objective chance is more constrained than Lewis's model which is more constrained than Skyrms's model.
Thus, in van Fraassen's model, chance has more inherent properties than in either Lewis's or Skyrms's models.
van Fraassen's theory is the only one of the three that is cast in a temporal framework.
All three are semantic theories and do not provide logical languages.
The model of objective chance used in L is based on van Fraassen's 9] model of objective chance.
He presents a semantic theory that models subjective probability and objective chance, using a future-branching model of time points.
van Fraassen places two constraints on objective chance: 1.
The chance of a past is either 0 or 1, depending on whether or not it actually occurred.
2.
Chance at a time is completely determined by history of the world up to that time.
From these assumptions, he shows the following relation between subjective probability and objective chance P (X jY ) = E C (X )] where P is the subjective probability at time t, C is the objective chance at time t, E is the expected value given Y , and provided the truth of Y depends only on the history up to t. This relation entails both Miller's principle and Lewis's principal principle, discussed below.
Note that van Fraassen does not show that a similar relation holds between objective chances at dierent times.
In van Fraassen's models, objective chance can change with time but truth values cannot.
Lewis's 6] theory of objective chance is based on his assertion that ... we have some very rm and denite opinions concerning reasonable credence (subjective probability) about chance (objective chance).
These opinions seem to me to afford the best grip we have on the concept of chance.
He describes a number of intuitive relationships between subjective probability and objective chance and shows that these are captured by his principal principle: Pr(Ajpr (A) =  ^ E ) =  where Pr is subjective probability, pr is objective chance, and E is any proposition compatible with pr (A) =  and admissible at time t. The interesting thing here is the proposition E .
The constraint that E be compatible with pr (A) =  means that Pr(E ^ pr (A) = ) > 0.
Admissibility is less readily dened.
Lewis does not give a denition of admissibility but he does characterize admissible propositions as \the sort of information whose impact on credence about outcomes comes entirely by way of credence about the chances of those outcomes."
So objective chance is invariant with respect to conditioning on tcp  t  Y  t  t  t  Y  t  t  t  t  admissible propositions.
This concept of invariance under conditioning is the central notion of Brian Skyrms's theory of objective chance.
Skyrms 7] works with the notion of resiliency.
A probability value is resilient if it is relatively invariant under conditionalization over a set of sentences.
The resiliency of Pr(q) being  is dened as 1 minus the amplitude of the wiggle about : The resiliency of Pr(q) being  is 1;maxj; Pr (q)j over p1  ::: p , where the Pr 's are gotten by conditionalizing on some Boolean combinationof the p 's which is logically consistent with q. Skyrms then denes propensity (objective chance) as a highly resilient subjective probability.
Independent of his resiliency notion, Skyrms requires that propensities and subjective probabilities be related by Miller's principle: Pr(Ajpr(A) = ) =  where Pr is a subjective probability and pr is a propensity.
He shows that Millers' principle entails that subjective probabilities are equal to the expectation of the subjective probabilities applied to the objective probabilities.
But Skyrms 7, p158] points out that, counter to intuition, independence in every possible objective distribution does not imply independence in the subjective distribution.
This observation provided the motivation for our use of the two probabilities to distinguish causal from evidential correlation.
Halpern 4, 5] presents a probability logic that can represent both statistical and subjective probabilities.
Statistical probabilities represent proportions over the domain of individuals, while propositional probabilities represent degrees of belief.
The two probability operators in the language can be nested and combined freely with other logical operators.
So the language is capable of representing sentences like \The probability is .95 that more than 75% of all birds can 	y."
The models for the language contain a domain of individuals, a set of possible worlds, a single discrete probability function over the individuals, and a single discrete probability function over the possible worlds.
The rst probability function is used to assign meaning to the statistical probability operator, while the second is used to assign meaning to the propositional probability operator.
Although he does not place constraints within the logic on the relation between the two probabilities, he does discuss a form of Miller's principle that relates subjective and objective probabilities.
His version of the principle states that \for any real number r0 the conditional probability of (a), given that the probability of a randomly chosen x satises  is r0, is itself r0."
He points out that this could be used as a rule for inferring degrees of belief from statistical information.
Bacchus 1] presents a logic essentially identical to that of Halpern.
He goes further than Halpern in exploring the inference of degrees of belief from statistical probabilities.
According to his principle of direct inference, an agent's belief in a formula is the expected j  n  i  j  value with respect to the agent's beliefs of the statistical probability of that formula, given the agent's set of accepted objective assertions.
References  1] F. Bacchus.
Representing and Reasoning With Probabilistic Knowledge.
MIT Press, Cambridge, Mass, 1990.
2] P. Haddawy.
Believing change and changing belief.
Technical Report TR-94-02-01, Dept.
of Elect.
Eng.
& Computer Science, University of Wisconsin-Milwaukee, February 1994.
Available via anonymous FTP from pub/tech_reports at ftp.cs.uwm.edu.
3] P. Haddawy.
Representing Plans Under Uncertainty: A Logic of Time, Chance, and Action, volume 770 of Lecture Notes in Arti	cial Intelligence.
Springer-Verlag, Berlin, 1994.
4] J.Y.
Halpern.
An analysis of rst-order logics of probability.
In Proceedings of the International Joint Conference on Arti	cial Intelligence, pages 1375{1381, 1989.
5] J.Y.
Halpern.
An analysis of rst-order logics of probability.
Arti	cial Intelligence, 46:311{350, 1991.
6] D. Lewis.
A subjectivist's guide to objective chance.
In W. Harper, R. Stalnaker, and G. Pearce, editors, Ifs, pages 267{298.
D. Reidel, Dordrecht, 1980.
7] B. Skyrms.
Causal Necessity.
Yale Univ.
Press, New Haven, 1980.
8] B. Skyrms.
Higher order degrees of belief.
In D.H. Mellor, editor, Prospects for Pragmatism, chapter 6, pages 109{137.
Cambridge Univ.
Press, Cambridge, 1980.
9] B.C.
van Fraassen.
A temporal framework for conditionals and chance.
In W. Harper, R. Stalnaker, and G. Pearce, editors, Ifs, pages 323{340.
D. Reidel, Dordrecht, 1980.
10] J. Venn.
The Logic of Chance.
MacMillan, London, 1866.
(new paperback edition, Chelsea, 1962).
11] R. von Mises.
Probability, Statistics and Truth.
Allen and Unwin, London, 1957.
Temporal Bayesian Networks Ahmed Y. Tawk and Eric Neufeld  Department of Computational Science, University of Saskatchewan Saskatoon, Saskatchewan, Canada S7N 0W0  Abstract  Temporal formalisms are useful in several applications such as planning, scheduling and diagnosis.
Probabilistic temporal reasoning emerged to deal with the uncertainties usually encountered in such applications.
Bayesian networks provide a simple compact graphical representation of a probability distribution by exploiting conditional independencies.
This paper presents a simple technique for representing time in Bayesian networks by expressing probabilities as functions of time.
Probability transfer functions allow the formalismto deal with causal relations and dependencies between time points.
Techniques to represent related time instants are distinct from those used to represent independent time instants but the probabilistic formalism is useful in both cases.
The study of the cumulative eect of repeated events involves various models such as the competing risks model and the additive model.
Dynamic Bayesian networks inference mechanisms are adequate for temporal probabilistic reasoning described in this work.
Examples from medical diagnosis, circuit diagnosis and common sense reasoning help illustrate the use of these techniques.
that of the dog dog-out.
The arc between dog-out and hear-bark means that the dog's barking is heard when it is out.
The topology of the graph represents the fact that the joint distribution of the variables can be written as the product of the conditional probability of each node given its immediate predecessors.
From now on, fo do lo and hb stand for family-out, dogout, light-on and hear-bark respectively.
The joint probability distribution for the network in Figure 1 is P(fo lo do hb) = P(fo)P(lojfo)P(dojfo)P(hbjdo): The topology of the network together with the probability calculus allow the calculation of the probability of any random variable e.g.
family-out given some evidence e.g.
light-on and/or hear-bark.
The probability ) or P(fojlo) = of fo given lo is P (fojlo) = P P(folo (lo) P (folo) P (lofo)+P (lo:fo) where P (fo lo) = P(lojfo)P(fo) and P(lo :fo) = P(loj:fo)P(:fo): Many AI and decision problems can be solved using Bayesian networks.
family- out  dog-out  light-on  hear-bark  1 Introduction  Bayesian networks are a probabilistic approach to reasoning about uncertainty.
A Bayesian network 16] is a directed acyclic graph where nodes denote random variables and arcs represent causal dependencies between them.
Throughout this paper we shall use a network, from 3], shown in Figure 1.
An English equivalent for this network might be `When the fam-  ily goes out they turn on the light of the outdoor lamp and put the dog in the backyard.
The dog's barking is heard when it is out in the backyard.'
This network has four random variables: family-out, light-on, dog-out and hear-bark.
From the network structure, family-out aects the status of the light light-on and  Figure 1: Bayes Network Time is an important dimension for AI and reasoning.
A rst wave of investigators studied logics of time, see for example, Allen 1], McDermott 13] and McCarthy 12].
The study of probabilistic temporal formalisms is relatively new.
Berzuini 2] uses Bayesian nets for temporal reasoning about causality.
Berzuini's formalism considers each time period of interest as an additional random variable.
This  may considerably increase the number of variables in the network and the complexity of inference.
Dean and Kanazawa 5] show how to represent persistence and causation.
They use survivor functions to represent changing beliefs with time.
This temporality allows predictive inferences but does not seem able to make inferences about independent time points.
Dagum, Galper and Horvitz 4] use a dynamic belief network for forecasting.
The dynamic network model shows how random variables at time t are affected on one hand by contemporaneous variables at time t as well as by the random variables at time t ; 1.
Meixner 14] axiomatizes a probabilistic theory of possibility based on temporal propensity.
Haddawy 7] introduces a probabilistic branching future model that corresponds to modal temporal logic.
2. if the family is not out during the day they open some windows.
Figure 2 illustrates the new network.
day-time  family- out  window-open dog-out  light-on  hear-bark  2 Temporal Networks  A probabilistic temporal representation, like most other temporal representations, must address the issue of discrete versus continuous time.
Discrete time is useful for various applications and seems simpler to deal with than continuous time.
However interval based dense time seems more natural and elegant for reasoning.
In temporal logics both representations are used , Allen 1] uses a dense time while McDermott 13] uses a point based time allowing intervals to be dened by its two end points.
Probabilistic temporal representations have also used both time models, Berzuini 2] uses a continuous time and Dean and Kanazawa consider time to be discrete.
Here, the position taken regarding this issue is to allow both, leaving the choice to the knowledge engineer.
This is possible because discrete time corresponds to discrete probability and continuous time corresponds to continuous probability.
Probability theory can handle both cases.
In the continuous case, probability density functions are dened as functions of time.
In the discrete case probabilities themselves are functions of time.
For continuous time, the evaluation of the probability of the truth of certain uents between two instants marking a period is the integral of the probability density function between those two instants.
The discussion will deal mostly with discrete time but the equations for the continuous case will be mentioned and a circuit diagnosis example in Section 3.1 illustrates how to deal with probability density functions.
The second basic issue is how to associate probabilities and time.
Two possibilities are considered: the rst is to follow Berzuini's networks of dates model and consider times as random variables and the second is to parameterize probabilities with time.
To motivate the discussion around this issue, let us introduce temporality to the family-out example in the previous section by adding the following statements: 1.
If the family is out during the day they do not turn the light on,  Figure 2: The Modied Network The variable window-open (wo) represents the probability of the window being open.
The temporal variable day-time (dt) is true when it is day and false otherwise.
The joint probability distribution expressed by the network is P(fo dt wo lo do hb) = P(fo)P(dt)P(wojfo dt)P(lojfo dt)P(dojfo)P (hbjdo): Representing day-time by a random variable complicates the network by increasing the number of nodes.
The probabilities of wo and lo depend on the joint probability of dt and fo, this further complicates the representation.
It may even get more cumbersome if we try to represent the following statements: 3.
Usually the family is out between 9:00 am till 5:00 pm, 4. sometimes they come home for lunch between 12:00 noon and 1:00 pm, 5. when they come for lunch they do not bring the dog in, 6. they go to visit friends between 7:00 pm and 11:00 pm.
Moreover, temporal variables like dt do not seem to meet the denition of a random variable.
Consider the rather intuitive denition of a random variable in 8] `A random variable may be dened roughly as a variable that takes on di	erent values because of chance.'
Does day-time take on dierent values because of  chance?
The dt variable takes on the values `true or false' depending on the deterministic motion of earth in space.
Whether or not it is daytime can be determined from the sunrise and sunset times from the local newspaper.
Treating time as a random variable complicates our reasoning unnecessarily.
Here  family- out  P(fo) 0.8  window-open  time  dog-out  8 9 10 11 12 1  2 3  4  5 6  7  8 9 10 11 12 1 2 3 4  5 6  8 9 10 11 12 1  2 3  4  5 6  7  8 9 10 11 12 1 2 3 4  5 6  8 9 10 11 12 1  2 3  4  5 6  7  8 9 10 11 12 1 2 3 4  5 6  8 9 10 11 12 1  2 3  4  5 6  7  8 9 10 11 12 1 2 3 4  5 6  P(do|fo) 0.9  light-on  time  hear-bark P(lo|fo) 0.6  time  Time is expressed in Hours on the horizontal scale  P(wo|fo)  0.2  time  Figure 3: Probabilities as functions of time the probabilities are functions of time, resulting in a simpler network.
Figure 3 illustrates this approach.
The probabilities' variation with time is shown over a single day.
The period `a day' is a complete cycle after which probabilities follow the same pattern repeatedly.
Such probability patterns capture the cyclic property of time useful in applications such as diagnosis of dynamic circuits with feedback.
When the problem does not exhibit this cyclic property, the reasoning requires the expression of probability over a window of interest.
In the next section, a circuit diagnosis example illustrates how to deal with acyclic time.
Returning to Figure 3, P (fo) captures conditions (3),(4) and (6) above.
The change in P(dojfo) between noon and 1:00 pm reects statement (5).
The rst two statements are represented by the probabilities P(lojfo) and P (wojfo) where the daytime is from 6:00 am to 6:00 pm.
The sharp changes in the probabilities in Figure 3 reect the precision expressed in the statements.
A smoother curve would be used to represent `around 9 am' as opposed to `9:00 am'.
It is reasonable to assume P (dojhb) is time independent.
This may seem unusual here but note this representation does allow this exibility.
In Section 3.2 hb is treated as an event.
It is worth noting that although this formalism can answer questions of the form `what is happening at time t?'
it cannot answer the question `when does x happen ?'.
However in most applications, a direct probabilistic answer for the rst question at dierent time points, approximates the probability distribu-  tion for the answer to the second, hence giving an indirect answer.
3 Temporal reasoning  From a temporal reasoning viewpoint, there are at least two types of probabilistic relationships between two dierent time points or periods they may be completely independent of each other (unrelated), or dependent (related).
If belief in uent f at time ti is not aected by the knowledge K at another time tj then f at ti is temporally independent of K at tj .
This independence means the probability of f at ti and that of K at tj are related by P(f@ti jK@tj ) = P(f@ti ).
In our example, if an observer goes past the house every few hours, instantaneously looks at the lights and checks if the dog is barking, then this observer can use the independence assumptions to reach a conclusion about family-out.
Every observation is independent of the others provided that they are distant.
On the other hand, if the observer stays to watch the light and listen for the dog for few hours, then the reasoning in this case should be able to relate what happens at one instant with previous and future instants.
In this case belief in uent f at time ti is aected by the knowledge K at time tj and P(f@ti jK@tj ) 6= P(f@ti ).
Persistence and causation are interesting special cases of reasoning about related time points.
3.1 Independent Time Instants or Periods  As mentioned before, the assumption of independence between dierent time points holds for the instanta-  neous observer.
This means that observations made at time point ti do not aect conclusions at tj if i 6= j.
If this observer sees the light and hears the dog barking then this observer can use the probabilities in Figure 3, to evaluate the probability of family-out.
Later, new observations and conclusions are made that are completely independent of the previous ones because many events could have happened between the two time points.
For the instantaneous observer, temporal reasoning is therefore a set of atemporal Bayes nets except for the probabilities, they must correspond to the time point under consideration.
Reasoning is not much harder than the atemporal case.
To further illustrate the applicability of this assumption, consider the following circuit diagnosis problem.
Always off  Always on  switch  light bulb  Defective bulb  battery  Defective wiring or switch  Defective battery  (a) The Circuit  (b) The Network  PDF(wiring or switch|off)  PDF(wiring or switch | on)  hours  10  10,000  10  PDF(battery | off)  25  10,000  hours  PDF(bulb| off)  hours  1,000  hours  is the life time of the torch, expected to be some tens of thousands of hours.
Replacement of a component can be represented by simply shifting the corresponding density function to start at the replacement time.
Section 4 justies such a shift within the temporal formalism.
At time t, the probability malfunction Mj is caused by the failure of component ci is given by t P (cijMj ) = P DF(ci jMj )dt 0 where P DF is the probability density function.
Z  3.2 Dependent (Related) Time Instants or Periods  Now, suppose the observer, in the family-out example is monitoring the status of the light and the barking of the dog.
If at time ti the dog's barking is heard, one should conclude that the dog is out in the backyard at this instant and for some time afterwards.
But after listening a while and not hearing the dog, one should be less certain about whether the dog is out or not.
This decay in certainty with time is also a function of time that relates probabilities at all instants with an event where an event is an occurrence.
In the present example any change in observations would be an event.
Events tend to occur over time intervals of zero or longer duration.
The distinction between event types and event tokens made by Hanks 9] is useful here.
Event types are classes of events and event tokens are particular instances.
While hearingbark is an event type,hearing-bark at 10:00 is an event token.
Interaction between event tokens is discussed in Section 4.2.
(c) Probability Density Functions  Figure 4: Circuit Diagnosis Example A simple torch (ashlight) circuit consists of a bulb, a switch and a battery connected as in Figure 4-a.
The probability distribution for the life time of the bulb and the battery are normal with means of 1000 and 25 hours respectively, as shown in Figure 4-c.
The wiring and the switch rarely fail but their probability of failure is high initially due to burn-in faults.
Then it drops as these defects usually a	ect the torch during the rst few hours of operation.
The failure probability nally rises again with aging.
To evaluate the probability of wiring-problem given  the torch is not working, it is necessary to know the number of operating hours after which the torch stopped working.
The network used here, Figure 4b, is similar to the symptom-disease network in 15].
Failures defective switch/wiring, defective bulb and defective battery cause or explain the malfunction of the torch.
Two malfunctions can be observed: always o	 and always on.
The window of interest here  P(do|hb) 0.8  family- out  t  0  time  dog-out  hear-barking (event)  light-on  hear-bark t  0  time  Figure 5: Probabilities dependence on events For each event type, a probability transfer function represents the eect of an event token of this type on other variables.
A probability transfer function denes a relation between P(xje) and the time t for all t where x is a random variable, e an event type.
The network in Figure 5 represents the decaying probability for dog-out given hear-barking using an exponential decay probability transfer function.
Changes aecting the same object may have dierent implications and hence dierent probability transfer functions.
If the light is turned on somebody at home might have turned it on.
Observing the light going o can have two possible explanations, the rst that it was turned-o or it just burnt out.
Observing the light going on is slightly stronger evidence that someone is at home than observing it going o.
Figure 6 shows the network and the probabilities associated with such a turn-light-on scenario.
The lightturned-on event is represented by a node that reduces the probability of family-out.
Turning the light on instantaneously reduces belief in fo.
On the other hand this event token should aect light-on by making it true.
The transfer function is a step function in this case.
As time progresses, the observation that the light was turned-on some time ago does not contribute to the conclusion of family-out or otherwise.
The arc marking this causal relationship is then either removed from the network or the conditional probability, as dened by the transfer function, saturates at a value chosen to mark indierence.
family- out  dog-out  light-on  0.99  P(lo|lton)  hear-bark  0.5  0  0  t  0  t  4 Convolutions, Probabilities and Models  It is necessary to unify the ideas temporal prole of random variables, conditional probabilities and the transfer function for events.
How should a transfer function combine with a temporal prole?
Should hearing the dog barking continuously for ve minutes, increase our certainty about the dog being out?
What would be the certainty of dog-out if the dog just barked from time to time during the observation period?
Some mathematical tools and models are necessary to answer these questions.
4.1 Convolution in Probability Theory  light-turned-on  P(fo|lton)  the transplant or the transfusion.
The incubation period is dierent for virus A and virus B but in both cases has a normal distribution.
It is easy to calculate the probability P(f ^ ojvirA) on a given day provided the time of transplant is known.
The distinction between monitoring and occasional sampling is a critical one.
If occasional sampling is done frequently enough, it is equivalent to monitoring.
The limit at which sampling can replace continuous monitoring, according to information theory, is equal to twice the maximum frequency in the signal.
This may seem articial in our present example but is useful in applications like diagnosis.
0  Figure 6: Light goes on Probability transfer functions let us represent many forms of temporal dependencies.
Consider the following example from 2] to see how this formalism expresses causal relations: Either transplant or a subsequent transfusion may have caused an accidental inoculation of virus A or virus B.
The inoculated virus, after a period of incubation, overgrows causing fever.
Fever, however may also develop due to other causes.
This can be represented as shown in Figure 7 and 8.
The incubation period is represented as a delay between the infection and the fever and overgrowth.
The probability of transfusion is high during and just after a transplant.
This probability decreases exponentially with time as shown in Figure 8.
The inoculation of virus A and B may happen any time during  The convolution integral of two distributions f1 and f2 is another distribution f written f = f1  f2 : For continuous time, f is evaluated with the formula 1 f(t) = f1 (t ; fi)f2 (fi)dfi: ;1 or 1 f1 (m)f2 (n ; m) f(n) =  Z  X  m=0  for discrete time.
If f1 and f2 represent the distribution of two random variables X and Y respectively, f is the distribution of a random variable Z = X + Y: Now let f1 be the distribution of a random variable t1 dened as the time when we heard the dog bark, and let f2 be the distribution of the random variable t2 dened as the duration during which we continue to think that the dog is out after hearing the barking.
Thus f allows us to evaluate the probability of dogout following hear-bark.
As shown in 4.2, convolution can also handle the cumulative eect of several events for additive storage models and a special case of the competing risks model.
Whenever the principle of superposition applies, that is when the eect of a set of inputs is the sum of the eect of each considered independently, the convolution can be used to evaluate the eect of a sequence of events.
4.2 Modeling Interaction of Events and Eects  Fever and overgrowth  Fever only  Virus B  Virus A  other reasons  transfusion  transplant  Figure 7: Medical Diagnosis example  P(transfusion|transplant)  tp   Storage process with additive inputs  P(virusB|transfusion)  tf  P(virusA|transplant)  P(Fever&overg.|virusA)  tp  ti  P(virusB|transplant)  P(Fever&overg.|virusB)  tp  ti  P(virusA|transfusion)  ti+tincA  ti+tincB  P(fever|other)  tf tp : transplant time  ti= tp or tf  tf : transfusion time  tincA: Virus A incubation time  ti : infection time  tincB : VirusB incubation time  Figure 8: The Probabilities  Random variables representing beliefs, probability of events and eects of the events on belief interact in many interesting ways.
In a Bayesian network representation, these interactions are reected by dependencies in the joint probabilities.
Consider for example the relation between leaving-home, arriving-at-work and crowded-streets.
The probability of arriving-at-work at 9:00 as a result of leavinghome at 8:30 given that the streets are crowded is dierent from the probability if the the streets are not crowded.
Possible causes for crowded-streets are rain, accident or construction.
In general the transfer function can depend on time, rain causes crowdedstreets only if it rains during the rush hour for more than 15 minutes, for example.
There may be also causal dependencies between events: e.g.
rain makes accident more probable.
Dealing with this type of situation requires that the temporal prole of the joint probability distribution is known.
In some situations the interaction between events follows simpler models and the net eect of a number of events can be evaluated from the eect of a single event and information about the time at which they occurred.
Storage processes, competing risks and domination are such models.
A storage process can be thought of in terms of a warehouse or a dam, characterized by its inow, its capacity and its release rule.
See for example Glynn 6].
Let events be additive inputs, let release rules be functions in the inputs and let the storage level be the degree of belief such that the change in storage level reects the change of belief with time.
This model can represent our dog-out and hear-bark causal relation.
Every token of hear-bark tends to ll the belief in dog-out to a certain level.
The release rule guarantees an exponential decay of this belief.
These systems follow the conservation of mass principle.
This principle, when applied to our example implies that we cannot believe dog-out unless we hear barking at least once.
It may be useful however to allow do to have a non zero probability before any hb event token.
One way to do this is to use the P(hb) as an input causing P (do) > 0.
Convolution can be used to evaluate the belief (storage) resulting from the accumulation of events (inputs) of a storage process.
Glynn 6] shows a storage process can be approximated by a nite state space model.
Transfer functions can be derived from the state space model or designed to reect the same behavior.
Figure 9 illustrates how the probability of dogout changes given dierent hear-barking event patterns, where probabilities are calculated us-  hb  P(do|hb)  0.8  hb  P(do|hb)  0.8  hb  hb  P(do|hb)  P(do|hb)  1.0  0.9  Figure 9: Convolution Results ing convolution.
In this gure, the belief in dogout rises sharply whenever hear-bark takes place, and the degree of belief reached each time is slightly higher.
If the barking is heard continuously over a period, the belief in dog-out keeps rising during this period and then decays after barking ceases to be heard.
The fourth event pattern in the gure deals with the case when the dog is heard continuously.
In this case belief rises and then almost saturates.
The use of two release rules allows us to compute the probability that the city on the horizon is Regina after driving from Saskatoon for time t. The rst release rule lets driving accumulate until after you have arrived in Regina with high probability.
Then the second rule is applied letting the probability of being in Regina decay with driving to express the idea that you should have passed Regina.
Both rules may be non-linear such that the probability get a shape similar to the expected distribution.
 Competing Risks Model  As the name suggests, the competing risks model represents two or more potential dangers competing to cause the failure of an organism.
An interesting generalization of this model is when dierent potential causes compete to produce the same eect and the success of one of them prevents the others from successeding.
Kalbeisch and Prentice 11], and Hutchinson 10] consider some applications of this model.
In the context of events, competitions occur frequently: e.g.
repeated exposures to some viruses compete until one causes infection after which the body develops an immunity.
For continuous time, the probability density of failure f(t) due to two risk is f(t) = f0 (t) +f1 (t): Here f0 (t) = l0 (t)  L1 (t) and f1 (t) = l1 (t)  L0 (t): L1 (t) and L0 (t) are the probabilities of survival at time t while l0 (t) and l1 (t) are the probability density of failure (hazard function).
The case for discrete time is simpler and the proba-  bility is f(n) = l0 (n)+l1 (n) ; l0 (n)  l1 (n): l0 (n) and l1 (n) are the probability of failure at time n for the rst and second risk respectively.
In both cases if the eects of the two infections do not overlap in time we obtain a simpler form f(t) = l0 (t) + l1 (t): If the two risks have the same hazard function l(t) but at dierent times, which is the case if a person is exposed twice to the same virus then l0 (t) = l(t ; t0 ) l1(t) = l(t ; t1) where t0 and t1 are the times of exposure to the virus.
In this case convolution can be used to evaluate the probabilities.
Otherwise the use of the original equations would be required.
 Dominating Events Model  In this model a particular event tends to dominate the others.
Rules to determine the dominating event along with the transfer function of this event are needed in this context.
The rules can be simple, like the most recent event and this rules applies to our circuit diagnosis example.
Given a sequence of changing-bulb events, the probability that the circuit failed due to burned-out-bulb depends on the last changingbulb and the lifetime of the bulbs.
The condition for this model to apply is that the dominating event makes the dominated events irrelevant to the reasoning.
5 Conclusion  Representing probabilities as functions of time seems to be a simple and useful technique for implementing probabilistic temporal reasoning.
Probability transfer functions can represent dierent types of events.
Known Bayes net inference methods can evaluate probabilities of outcomes.
By characterizing the dependencies between dierent instants or periods temporal reasoning can be done with a small computational overhead.
Decoupling temporal reasoning across dierent time points from the reasoning at a given time point simplies reasoning.
Simple interaction models such as the storage model, competing  risks model or the domination model can be used to represent some useful temporal phenomena without complicating inference.
6 Acknowledgements  The rst author thanks the Institute for Robotics and Intelligent Systems (IRIS) and the University of Saskatchewan for support.
Research of the second author is supported by IRIS and the Natural Science and Engineering Research Council of Canada (NSERC).
References  1] J. Allen.
Towards a general theory of action and time.
Articial Intelligence, 23(2):123{154, July 1984.
2] C. Berzuini.
Representing time in causal probabilistic networks.
In Uncertainty in Articial Intelligence 5, pages 15{28, (North-Holland), 1990.
Elsevier Science Publishers B.V. 3] E. Charniak.
Bayesian networks without tears.
AI Magazine, 12(4):51{63, Winter 1991.
4] P. Dagum, A. Galper, and E. Horvitz.
Dynamic network models for forecasting.
In Proceedings of the 1992 Workshop on Uncertainty in Articial Intelligence, pages 41{48.
Association for  5] 6] 7] 8] 9] 10] 11] 12]  Uncertainty in Articial Intelligence, 1992.
T. Dean and K. Kanazawa.
A model for reasoning about persistence and causation.
Computational Intelligence, 5(3):142{150, August 1990.
J. Glynn.
A discrete-time storage process with a general release rule.
Journal of Applied Probability, 26:566{583, 1989.
P. Haddawy.
A temporal probability logic for representing actions.
In Proc.
of the 1991 Conference on Knowledge Representation, pages 313{324, 1991.
M. Hamburg.
Statistical Analysis for Decision Making, 4th ed.
HBJ Publishers, Orlando, FL, 1987.
S. Hanks.
Representing and computing temporally scoped beliefs.
In Proc.
of AAAI- 88, St. Paul, MN, pages 501{505, 1988.
T. Hutchinson.
A note on applications of the competing risks model.
Accident Analysis and Prevention, 15(3):225{226, 1983.
J. Kalbeisch and R. Prentice.
The Statistical Analysis of Failure Time Data.
John Wiley and Sons, New York, 1980.
J. McCarthy and P. Hayes.
Some philosophical problems from the standpoint of articial intelligence.
In B. Meltzer and D. Michie, editors, Machine Intelligence 4.
Edinburgh University Press, Edinburgh, 1969.
13] D. McDermott.
A temporal logic for reasoning about processes and plans.
Cognitive Science, 6(2):105{155, April 1982.
14] U. Meixner.
Propensity and possibility.
Erkenntnis, 38(3):323, May 1993.
15] J. Pearl.
Distributed revision of composite beliefs.
Articial Intelligence, 33:173{215, 1988.
16] J. Pearl.
Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.
Morgan Kaumann, San Mateo, CA, 1988.

Ecient Handling of Context-Dependency in the Cached Event Calculus Luca Chittaro and Angelo Montanari  Abstract  Dipartimento di Matematica e Informatica, Universita di Udine, Via Zanon, 6, 33100 Udine, Italy e-mail: fchittaro j montanarig@uduniv.cineca.it  This paper deals with the problem of providing temporal deductive databases with an ecient implementation in a logic programming framework.
We restrict our attention to historical databases based on Kowalski and Sergot's Event Calculus extended with context-dependency.
The paper aims at being benecial to both the theoretically-minded and the implementation-oriented research communities.
It provides a mathematical analysis of the computational complexity of query and update processing in the Event Calculus, and proposes a cached version of the calculus that moves computational complexity from query to update processing, and features an absolute improvement of performance when contextdependency is added.
1 Introduction  The paper deals with the problem of providing temporal deductive databases (TDDs) with an ecient implementation in a logic programming framework.
TDDs provide the possibility of managing temporal information not only to retrieve information as it was stored in the database, but also to automatically derive further data  1, 2, 14, 20].
In the case of incomplete temporal data, information that is neither explicitly asserted nor monotonically implied by the available knowledge can be inferred by drawing conclusions according to suitable assumptions such as closed world or default persistence.
Conclusions derived in this way are obviously defeasible and TDDs must withdraw them if the addition of further information makes them inconsistent.
Dierent TDDs have been proposed in the literature  9, 11, 12, 16].
They dier from each other either in the underlying model of change (state-based in  15, 16], event-based in  11, 12]), or in the programming language paradigm they adopt (functional programming in  9, 10], logic programming in  11, 12]), or in both.
We restrict our attention to historical databases based on Kowalski and Sergot's Event Calculus model of change enriched with context-dependency  5, 11].
From a description of events that occur in the real world, the Event Calculus (EC) allows one to derive various properties and the time periods for which they  hold.
The addition of context-dependency makes it possible to constrain the initiation and termination of properties to the validity of some given conditions at the time at which events occur.
EC database and rules are formulated in a logic programming framework as a logic program.
EC presents several advantages over relational temporal databases as well as over most TDDs  12].
First of all, while relational temporal databases only record the starting and ending of properties, thus losing the semantical structure of causing events, EC supports an explicit representation of events.
In this way, updates are performed by entering events and deriving the starting and ending of properties as a logical consequence from event descriptions by means of general rules which express the semantics of events.
Moreover, the domain modeler is fully in charge of ensuring the integrity of data, and thus expensive integrity checking procedures are not needed.
Secondly, insertion of events in the database is not required to follow the chronological order of their occurrences.
Thirdly, default persistence can be dened both in the future and the past.
The cost of update processing in EC is the cost of entering a new event, and thus it is constant  11], while query processing is expensive.
The addition of context-dependency, which is crucial to deal with real-world problems  3, 5, 8], heavily deteriorates performance of query processing.
The paper extends EC with a lemma storage mechanism (Cached Event Calculus) that records maximal validity intervals (MVIs) of properties for later use in query processing and updates them as soon as a new event is entered in the database.
CEC moves computational complexity from query to update processing, and features an absolute improvement of performance when contextdependency is added, because CEC update processing costs less than EC query processing.
Furthermore, CEC preserves the basic requirement of EC of making no assumptions about the temporal order of input events.
As an example, CEC accepts an event happened before some of the already acquired events and revises all and only the aected MVIs.
Obviously, acquiring a complete sequence of events according to their chronological order avoids the revision of past data, thus strongly increasing the performance of CEC.
This is the case of several application domains,  such as patient monitoring  8].
The paper is organized as follows.
Section 2 introduces EC extended with context-dependency.
Section 3 presents in detail the basic features of CEC.
It illustrates how entering an event in the database may cause the assertion of new MVIs for the properties it aects or the shortening of cached ones, and how assertions and retractions are possibly propagated to other properties.
A sample execution of CEC concludes the section.
Section 4 analyzes and compares the complexity of query and update processing in EC and CEC.
Conclusions provide an assessment of the work.
2 The logical calculus of events  In this section, we present the main features of the logical calculus of events.
It extends EC with the notions of type and context-dependency.
EC proposes a general approach to represent and reason about events and their eects in a logic programming framework.
It takes the notions of event, property, timepoint and time-interval as primitives and denes a model of change in which events happen at timepoints and initiate and/or terminate time-intervals over which some property holds.
Time-points are unique points in time at which events take place instantaneously.
Time-intervals are represented as pairs of time-points.
EC also embodies a notion of default persistence according to which properties are assumed to persist until an event occurs which terminates them.
A specic domain evolution is called an history and is modeled by a set of event occurrences (event instances).
The calculus allows us to infer a set of time intervals over which the properties initiated and/or terminated by event occurrences maximally hold (property instances).
Instances of events and properties are obtained by attaching a time-point and a time-interval to event and property types and are denoted by the pairs (event,time-point) and (property, time-interval), respectively1 .
Formally, we represent event occurrences by means of the happens at predicate: happens_at(event,timePoint).
Furthermore, we represent (part of the) domain knowledge by means of initiates at and terminates at predicates that express the eects of events on properties: initiates_at(event1,prop1,T):happens_at(event1,T).
terminates_at(event2,prop2,T):happens_at(event2,T)  Initiates at (terminates at) predicates state that each instance of event1 (event2) initiates (terminates) a period of time during which prop1 (prop2) holds, The pair (event, time-point) uniquely identies an event occurrence provided that two events of the same type can not simultaneously happen.
In situations where this assumption is not acceptable, explicit identiers for event occurrences have to be added.
1  respectively2 .
A particular initiates at clause can be used to deal with initial conditions.
Initial conditions describe a possibly partial initial state of the world and are specied by means of a number of events of type initially(prop).
Their validity from the beginning of time can then be derived by means of the clause: initiates_at(initially(Prop),Prop,0):happens_at(initially(Prop),0).
Such a clause is parametric with respect to the property argument and takes 0 as the initial instant of the time axis.
This allows us to distinguish explicitly stated initial conditions (starting at 0) from initial conditions derived by persistence in the past, and to assign a greater degree of condence to the former.
2.1 The basic axioms of EC  The basic Event Calculus model of time and change is dened by means of a set of axioms.
The rst axiom we introduce is the mholds for.
It allows us to state that the property P holds maximally (i.e.
there is no larger time-interval for which it also holds) over  Start End] if an event E1 which initiates P occurs at the time Start, and an event E2 which terminates P occurs at time End, provided there is no known interruption in between: mholds_for(P,Start,End]):initiates_at(E1,P,Start), terminates_at(E2,P,End), End gt Start, \+ broken_during(P,Start,End]).
mholds_for(P,Start,infPlus]):initiates_at(E1,P,Start), \+ broken_during(P,Start,infPlus]).
mholds_for(P,infMin,End]):terminates_at(E2,P,End), \+ broken_during(P,infMin,End]).
where the predicate gt extends the ordinary ordering relationship > to include the cases involving innite arguments, sintactically denoted by infMin and infPlus.
Analogously, we dened the predicates ge, lt and le which extend  < and , respectively.
The negation involving the broken during predicate is interpreted using negation-as-failure.
This means that properties are assumed to hold uninterrupted over an interval of time on the basis of failure to determine an interrupting event.
Should we later record an initiating or terminating event within this interval, we can no longer conclude that the property holds over the interval.
This gives us the non-monotonic character of the calculus which deals with default persistence.
The predicate broken during is dened as follows: 2 Dierently from the original denition of the Event Calculus 11], devoid of any notion of event type and then only supporting an extensional denition of initiates and terminates predicates at the instance level, domain relations are intensionally dened in terms of event and property types.
broken_during(P, Start,End]):(terminates_at(E,P,T)initiates_at(E,P,T)), Start lt T, End gt T.  This axiom provides a so-called strong interpretation of initiates at and terminates at predicates: a given property P ceases to hold at some point T during the time-interval  Start End] if there is an event E which initiates or terminates P occurring at a time T belonging to  Start End].
Notice that if we record three events e1, e2 and e3 such that e1 precedes e2, e2 precedes e3, both e1 and e2 initiates a property p and e3 terminates p, we can only conclude that p holds between the occurrence times of e2 and e3.
This behaviour can be explained as follows: the strong interpretation assumes that an event terminating p actually occurred between e1 and e2, but it is not known when it occurred, and thus it is not possible to derive any validity interval for p between e1 and e2.
According to this interpretation, pending events like e1 characterize situations of incomplete information about event occurrences.
An alternative interpretation of initiates at and terminates at predicates, called weak interpretation, is also possible.
According to such an interpretation an event initiates a property unless it has been already initiated and not yet terminated.
To support this weak interpretation the denition of broken during must be revised so that only terminating events can break the validity of property P  8].
Finally, we add the holds at axiom relating a property to a time-point rather than to a time-interval: holds_at(P,T):mholds_for(P,Start,End]), T gt Start, T le End.
The holds-at predicate conventionally assumes that a property is not valid at the starting point of the MVI, while it is valid at the ending point.
These axioms constitute the kernel of basic (typed) EC.
They provide a simple and eective tool to reason about events and their eects, but have a limited expressive power.
In particular, they provide no primitives for modeling relevant features such as contextdependency, discrete and continuous processes, time granularity.
Several extensions  5, 7, 12, 13, 19, 18, 20] have been proposed to overcome these limitations.
In this paper, we deal with the addition of contextdependency.
In basic EC, both initiates at and terminates at are context-independent predicates: the occurrence of an event of the given type initiates, or terminates, the validity of the relevant property whatever is the context in which it occurs.
On the contrary, making initiates at and terminates at predicates contextdependent allows us to state that the occurrence of an event of a given type at a certain time point initiates or terminates the validity of the associated property provided that some given conditions hold at such an instant  3].
Formally, initiates at and terminates at predicates are generalized as follows3 : initiates_at(event,prop1,...,propN],prop,T):happens_at(event,T), holds_at(prop1,T),...,holds_at(propN,T).
terminates_at(event,prop1,...,propN],prop,T):happens_at(event,T), holds_at(prop1,T),...,holds_at(propN,T).
where N is greater than 0 when initiates at and terminates at are context-dependent and equal to 0 when they are context-independent.
In this last case, we simply dene initiates at and terminates at as: initiates_at(E,],P,T):initiates_at(E,P,T).
terminates_at(E,],P,T):terminates_at(E,P,T).
As shown elsewhere, the inclusion of holds at atoms in the body of terminates at and initiates at makes EC no more stratied  17, 4].
As a consequence, termination of the computation of MVIs is not guaranteed anymore.
2.2 Comparing EC with and without context-dependency  Consider a simple lighting system that is operated by its user using just one switch, whose functioning can be described as follows.
The user can set the switch in one of two positions: on or o.
If there is electrical power available, the eect of setting the switch in the on or o position is to switch the light on or o.
If there is no electrical power available, the eect of setting the switch in the on position is delayed until electrical power is provided.
A failure in providing power can anticipate the eect of setting the switch in the o position.
We identify four types of events and three types of properties.
Event types are: turnOn (the user sets the switch in the On position), turnO (the user sets the switch in the O position), pwrFail (a failure in the electrical power distribution network), pwrRstr (the failure is xed, and power is restored).
Property types are: switchOn (the position of the switch is on), pwrAvail (electrical power is available), lightsOn (the lights are lit).
Using EC extended with context-dependency, the knowledge about eects of events on properties can be formalized as follows: initiates_at(turnOn,],switchOn,T):happens_at(turnOn,T).
terminates_at(turnOff,],switchOn,T):happens_at(turnOff,T).
initiates_at(pwrRstr,],pwrAvail,T):happens_at(pwrRstr,T).
3 The 2nd argument in the 4-argument version of initiates at and terminates at allows to statically inspect domain axioms in order to detect dependencies among properties as shown in Section 3.2.  update  terminates(pwrFail,],pwrAvail,T):happens_at(pwrFail,T).
initiates_at(turnOn,pwrAvail],lightsOn,T):happens_at(turnOn,T), holds_at(pwrAvail,T).
terminates_at(turnOff,pwrAvail],lightsOn,T):happens_at(turnOff,T), holds_at(pwrAvail,T).
update Init  breaking I  extending I  update Termin  breaking T  extending T  initiates_at(pwrRstr,switchOn],lightsOn,T):happens_at(pwrRstr,T), holds_at(switchOn,T).
terminates_at(pwrFail,switchOn],lightsOn,T):happens_at(pwrFail,T), holds_at(switchOn,T).
On the contrary, in the case of basic EC the modeler would be forced to use context-independent initiates at and terminates at predicates also to represent context-dependent knowledge.
This can be done only by introducing additional types of events and naming them properly to highlight that they assume a given context.
Considering for example the eect of the turnOn event on the property lightsOn, we can reformulate it with the EC axiom: initiates_at(turnOnwithPwrAvail,lightsOn,T):happens_at(turnOnwithPwrAvail,T).
This not only introduces a more complex event, but forces us to introduce another type of event (turnOnwithNoPwrAvail) in order to rewrite the description of the eects of turnOn on the property switchOn.
The drawbacks of this solution aect both the modeler and the nal user of the database.
On one hand, the modeler is indeed forced to (i) devise and introduce a number of new events that increases greatly with the complexity of the considered domain and the possible combinations of preconditions, (ii) highlight the dierences among these events using long, awkward names.
On the other hand, the nal user is burdened with the responsability of precisely evaluating contexts in the real world in order to choose the proper event that has to be entered in the database (for example, in the simple lighting system example, the user must know if power was supplied when he/she pushed the button).
Moreover, it becomes impossible in this way to enter an event when there's only partial knowledge about the context it assumes.
These limitations become unbearable in complex domains and unnecessarily narrow the deductive power of the database.
If EC is indeed extended with context-dependency, it becomes able to automatically identify contexts and therefore the user has only to enter basic types of events which he/she easily recognizes, leaving to the database the task of evaluating and updating the context and the eects.
Furthermore, the database becomes able to reason with incomplete knowledge about contexts.
propagate Retract  propagate Assert  Figure 1: Architecture of CEC.
3 The Cached Event Calculus (CEC)  In this section we describe the main features of the Cached Event Calculus (CEC).
It extends EC with a caching mechanism that receives instances of events as input and updates accordingly the cached set of MVIs (mholds for assertions).
Temporal reasoning in EC is performed at query time: the system logs any input event without processing it, and accesses the log when a query is posed.
Thanks to the negation as failure rule, conclusions no longer supported are not derived anymore.
The fact that the results of computations are never cached for later use makes query processing in EC very inecient.
A great deal of unnecessary computation is indeed performed whenever the same mholds for query about a property p is processed several times between two consecutive updates aecting MVIs of p, because EC repeats each time the whole computation from the beginning instead of saving previous conclusions.
Even worse, when context-dependency is added multiple computations of identical mholds for (sub)queries can occur during the processing of a single mholds for query, due to contexts evaluation.
To eliminate these useless computations, EC should be provided with a mechanism for caching the currently believed set of MVIs, so that accesses to cached data replace blind recomputations in answering mholds for queries.
However, conceiving a suitable caching mechanism is not a trivial task, because it should be able to incrementally add new results to the old ones and to update or delete only the old results which need it.
Moreover, dierently from general caching mechanisms, we do not only need to add and/or remove assertions, but also to clip and/or extend existing MVIs according to domain and temporal knowledge.
The general architecture of CEC is depicted in Figure 1, where all its conceptual modules and their hierarchical relations are highlighted.
We briey summarize here the purpose of the modules their func-  tioning will be described in detail in the next sections.
Each new event is entered into the database by update updateInit and updateTermin are then called in order to manage properties which are initiated and terminated by the event, respectively.
In both cases, the event can lead either to clip an existing MVI (this case is handled by breakingI and breakingT) or to possibly create a new MVI (this case is handled by extendingI and extendingT).
When a MVI (or a part of it) is retracted (asserted), propagateRetract (propagateAssert) takes care of propagating that change to properties which depend on the changed one.
Propagation of assertion and retractions can recursively activate the process of breaking or extending MVIs.
In the next two sections, we rst describe the process of breaking and extending MVIs and then the process of propagating retractions and assertions.
3.1 Breaking and adding maximal validity intervals  CEC allows to enter events in the database by means of the update predicate.
This predicate rst explicitly records the instance of the event and then the properties initiated and terminated by the event are considered separately, by means of the updateInit and updateTermin predicates: update(E,T):assert(happens_at(E,T)), bagof(P,(updateInit(E,T,P) updateTermin(E,T,P)),_).
update(_,_).
If E initiates a property Prop at time T , CEC tests if there exists a MVI  T 1 T 2] for Prop such that T 1  T < T 2: updateInit(E,T,Prop):initiates_at(E,_,Prop,T), (insideLeftClosedInt(Prop,T,T1,T2]) -> breakingI(Prop,T,T1,T2]) extendingI(Prop,T)).
insideLeftClosedInt(Prop,T,T1,T2]):mholds_for(Prop,T1,T2]), T1 le T, T lt T2.
If such an interval exists, we face two possibilities, which are handled by the breakingI predicate: (i) if T 1 = T , there is already an event occuring at T , that initiates Prop and then no changes are needed to interval  T 1 T 2] at the moment (ii) otherwise, interval  T 1 T 2] is shortened in such a way that the new starting point becomes T , Prop does not hold anymore in the clipped part  T 1 T ], and the retraction of  T 1 T ] has to be propagated.
breakingI(_,T1,T1,_]):-!.
breakingI(Prop,T,T1,T2]):retract(mholds_for(Prop,T1,T2])), assert(mholds_for(Prop,T,T2])), propagateRetract(T1,T],Prop).
If instead there are no MVIs for Prop satisfying the test T 1  T < T 2, then T is the starting point of a new MVI for Prop.
The new interval of validity  starting at T is added by the extendingI predicate, and the new assertion has to be propagated.
extendingI(Prop,T):new_termination(Prop,T,NewEnd]), assert(mholds_for(Prop,T,NewEnd])), propagateAssert(T,NewEnd],Prop).
The new termination predicate nds the ending point of the new interval, distinguishing the following two cases: (i) there is a `pending' terminating event for property Prop occurring at time NewEnd after T and thus the new MVI becomes  T NewEnd], (ii) there are no initiating or terminating events for property Prop occurring after T and thus the new MVI becomes  T infPlus]: new_termination(Prop,T,NewEnd]):terminates_at(_,_,Prop,NewEnd), T lt NewEnd, \+broken_during(Prop,T,NewEnd]), !.
new_termination(Prop,T,infPlus]):\+broken_during(Prop,T,infPlus]).
The case in which the entered event E terminates instead of initiating a property is handled by a set of predicates (updateTermin, insideRightClosedInt, breakingT, extendingT, new initiation), that is symmetrical to the previously discussed one  6].
3.2 Propagation of retractions and assertions  Each time a MVI  T 1 T 2] for a property is retracted (asserted), the update has to be propagated to properties whose validity may rely on such an interval.
The retraction (assertion) of  T 1 T 2] indeed modies the context of events occurring at time points belonging to it and, then, it can possibly invalidate (activate) their eects.
More precisely, only those contextdependent initiates at or terminates at clauses having the retracted (asserted) property as a condition have to be reconsidered over the interval (we implement this selection by inspecting the domain axioms using the standard clause predicate).
In the case of propagation of retractions, we distinguish two relevant cases: (i) possibly invalidated initiations the retracted property RetractedProp is a condition for the initiation of property P at the occurrence of event E and the occurrence time T 3 of E belongs to  T 1 T 2].
In this case, the right part of the retracted interval  T 1 T 2] overlaps the possibly aected MVI  T 3 T 4] for P : propagateRetract(T1,T2],RetractedProp):clause(initiates_at(E,PropList,P,_),_), memberchk(RetractedProp,PropList), happens_at(E,T3), rightOverlap(T1,T2],P,T3,T4]), retractForRightOverlap(P,T2,T3,T4]), fail.
rightOverlap(T1,T2],P,T3,T4]):T1 lt T3, T3 le T2,  mholds_for(P,T3,T4]).
(ii) possibly invalidated terminations RetractedProp is a condition for the termination of property P at the occurrence of event E and the occurrence time T 4 of E belongs to  T 1 T 2].
In this case, the left part of the retracted interval  T 1 T 2] overlaps the possibly aected MVI  T 3 T 4] for P : propagateRetract(T1,T2],RetractedProp):clause(terminates_at(E,PropList,P,_),_), memberchk(RetractedProp,PropList), happens_at(E,T4), leftOverlap(T1,T2],P,T3,T4]), retractForLeftOverlap(P,T1,T3,T4]), fail.
leftOverlap(T1,T2],P,T3,T4]):T1 lt T4, T4 le T2, mholds_for(P,T3,T4]).
The clauses for propagateRetract end with a fail predicate in order to cause the examination of all aected properties.
When no more initiations and terminations are left to examine, a third clause guarantees success: propagateRetract(_,_).
When an initiation is invalidated, there are four possible situations: 1. independency P still initiates at T 3, because there exists a successful initiates at clause, which does not include RetractedProp as condition: retractForRightOverlap(P,_,T3,_]):initiates_at(_,_,P,T3),!.
The MVI for P is thus unchanged.
2. revised initiation with nite termination property P terminates at a time instant T 4 (it does not hold forever) and either there exists an initiating event preceding T 4 or no initiating or terminating events occur before T 4 and P is assumed to hold from infMin by default.
The new initiation predicate is used to identify the proper case, determining the new starting point NewStart of the interval.
Finally, if T 3 < NewStart then the retraction of validity over  T 3 NewStart] is propagated, otherwise the extension of validity over  NewStart T 3] is propagated.
retractForRightOverlap(P,_,T3,T4]):T4 \== infPlus, new_initiation(P,NewStart,T4]), !, retract(mholds_for(P,T3,T4])), assert(mholds_for(P,NewStart,T4])), ((T3 lt NewStart) -> propagateRetract(T3,NewStart],P) propagateAssert(NewStart,T3],P)).
3. revised initiation with innite termination property P holds until infPlus and there exists an event occurring at NewStart that initiates P with P holding uninterrupted after  NewStart, that is, there are no events initiating or terminating P occurring after NewStart.
In such a case NewStart becomes the new starting point and this modication is propagated: in case T 3 < NewStart, the retraction of validity over  T 3 NewStart] is propagated otherwise, the extension of validity over  NewStart T 3] is propagated.
retractForRightOverlap(P,_,T3,infPlus]):initiates_at(_,_,P,NewStart), \+broken_during(P,NewStart,infPlus]),!, retract(mholds_for(P,T3,infPlus])), assert(mholds_for(P,NewStart,infPlus])), ((T3 lt NewStart) -> propagateRetract(T3,NewStart],P) propagateAssert(NewStart,T3],P)).
4. vanishing if none of the above described situations applies, the MVI for P is fully retracted and this retraction is propagated: retractForRightOverlap(P,_,T3,T4]):retract(mholds_for(P,T3,T4])), propagateRetract(T3,T4],P).
In the case of invalidated terminations, there are four possible situations, which are handled by retractForLeftOverlap that is symmetrical to the previously described predicate  6].
In the case of propagation of assertions, we distinguish two relevant cases: (i) possibly new initiations the asserted property AssertedProp is a condition for the initiation of property P at the occurrence of event E , the occurrence time T of E belongs to  T 1 T 2] and there is not already a MVI for P with T as its starting point.
In this case, the already described updateInit predicate is used to check if P is now initiated at T and possibly revising the database accordingly.
propagateAssert(T1,T2],AssertedProp):clause(initiates_at(E,PropList,P,_),_), memberchk(AssertedProp,PropList), happens_at(E,T), T1 lt T, T le T2, \+mholds_for(P,T,_]), updateInit(E,T,P), fail.
(ii) possibly new terminations the asserted property AssertedProp is a condition for the termination of property P at the occurrence of event E , the occurrence time T of E belongs to  T 1 T 2] and there is not already a MVI for P with T as its ending point.
In this case, the already described updateTermin predicate is used to check if P is now terminated at T and possibly revising the database accordingly.
propagateAssert(T1,T2],AssertedProp):clause(terminates_at(E,PropList,P,_),_), memberchk(AssertedProp,PropList), happens_at(E,T),  T1 lt T, T le T2, \+mholds_for(P,_,T]), updateTermin(E,T,P), fail.
As in the case of propagateRetract, the fail predicate has been used to force backtracking in order to examine all aected properties.
Therefore, a third clause guarantees success as before: propagateAssert(_,_).
3.3 Running the lighting system example in CEC  We will now show how CEC builds and maintains the set of cached MVIs, by examining one execution of the lighting system example.
Suppose to perform the following updates in the shown order, that is not chronological: update(initially(pwrAvail),0), update(turnOff,8), update(turnOn,4), update(turnOn,10), update(pwrFail,6), update(pwrRstr,12).
Figure 2 illustrates pictorially the eects of each of the six updates.
The eect of the rst update is to initiate a pwrAvail property that holds between 0 and infPlus (extendingI with initial conditions).
Propagation of the assertion of this property has no eect because there are no inuenced events in the database.
The eects of the second update are to terminate: (i) a lightsOn interval that holds between infMin and 8 and (ii) a switchOn interval that also holds between infMin and 8 (extendingT in both cases).
Propagation of assertion for both properties has no eect because there are no other events in the interval of time considered by propagation.
The eects of the third update are: (i) the MVI of property lightsOn is broken at 4, and validity between infMin and 4 is thus retracted and (ii) the MVI of property switchOn is also broken at 4, and validity between infMin and 4 is thus retracted (brokenI in both cases).
Propagation of retraction for both properties has no eect because there are no other events in the interval of time considered by propagation.
The eects of the fourth update are to initiate: (i) a lightsOn interval that holds between 10 and infPlus and (ii) a switchOn interval that also holds between 10 and infPlus (extendingI in both cases).
Propagation of assertion for both properties has no eect because there are no other events in the interval of time considered by propagation.
The eects of the fth update are: (i) the MVI of property pwrAvail is broken at 6 and validity between 6 and infPlus is thus retracted (brokenT), (ii) retraction is propagated and leads to reconsider the eects of event turnOn at 10, leading to the retraction of property LightsOn holding between 10 and infPlus (vanishing), (iii) the eects of event turnO at 8 are then reconsidered and property LightsOn is retracted between 6 and 8 (revised termination with nite initiation).
Propagation of the retractions of lightsOn  have no eect because there are no inuenced events.
The eects of the sixth update are to initiate: (i) a pwrAvail interval that holds between 12 and infPlus and (ii) a lightsOn interval that also holds between 12 and infPlus (extendingI in both cases).
Propagation of assertion for both properties has no eect because there are no other events in the interval of time considered by propagation.
As already pointed out, changing the order of execution of the six updates has no inuence on the nal contents of the database, but could aect eciency.
In particular, if the complete sequence of events was entered chronologically, there would be no need for a substantial revision of the database as that caused by the fth update.
4 Complexity Analysis  In this section we analyze the complexity of executing EC and CEC with an ordinary Prolog intepreter.
We focus on the execution of mholds for queries returning the full set of MVIs for a given property prop.
Such queries take the following form: ?- bagof(MVI,mholds_for(prop,MVI),MVIs).
We also assume that the database contains a set of n initiating events and n terminating events for any property.
First of all, we determine the complexity of query processing in EC devoid of contextdependency, and show how EC performance heavily decreases when context-dependency is added.
Then, we prove how the addition of a caching mechanism strongly reduces the cost of query processing.
We show that the cost of query processing in CEC, with and without context-dependency, is linear, and that the complexity of CEC update processing is less than the complexity of EC query processing except in the case of context-independency where it is equal.
In all proofs we assume the strong interpretation of initiates at and terminates at predicates.
It is possible to show that in the case of weak interpretation the worst-case analysis leads to the same results.
We only sketch out the structure of proofs the details can be found in  6].
4.1 The complexity of query processing in EC  We initially consider the case of a database with only context-independent denitions of initiates at and terminates at.
It is possible to prove the following theorem.
Theorem 4.1  The complexity of EC query processing, measured in terms of accesses to happens at facts, is O(n3 ), where n is the number of initiating (terminating) events in the database for the considered property.
We performed the proof in two steps.
We rst determined a cubic upper bound (2n3 +n2 +n) for the total number of accesses to happens at facts in answering  update(initially(pwrAvail),0).
update(turnOn,10).turnOn initially(pwrAvail)  initially(pwrAvail)  pwrAvail  0  ++++++++++++++++++++++++ pwrAvail  8  4  0  lightsOn turnOn  10 ++++++++  turnOff  ++++++++ switchOn  update(turnOff,8).
update(pwrFail,6).
turnOff initially(pwrAvail) 0  pwrFail initially(pwrAvail)  pwrAvail  8  ++++++++++ lightsOn  ---------------------  pwrAvail  8  4  0  ---  lightsOn turnOn  ++++++++++ switchOn  turnOff  switchOn  update(turnOn,4).
pwrRstr  turnOn pwrAvail 4 -----lightsOn ------  turnOn  update(pwrRstr,12).
initially(pwrAvail)  initially(pwrAvail) 0  10 -----------  ++++  pwrAvail 8 turnOff  4  0  6  = valid initiation/termination of properties = invalidated initiation/termination = new initiation/termination  10  lightsOn turnOn switchOn  switchOn  8  turnOff  12 ++++  turnOn  = maximal validity interval (MVI) - - - - - - = retraction of validity + + + + + = assertion of validity  Figure 2: A sample execution of CEC.
mholds for queries with only the property argument instantiated.
Then we showed that the cubic limit is actually reached.
In order to determine the complexity of query processing in EC with context-dependency some preliminary notions are necessary.
We must take into account that a condition of a context-dependent initiates at or terminates at may itself be contextdependent.
Therefore, we must consider in general an arbitrary nesting level of context-dependent properties.
Let Lbk be the maximum level of nesting from a single property.
To formally dene Lbk , we introduce the notion of property dependency graph associated with a set of initiates at and terminates at clauses.
A property dependency graph is a directed acyclic graph where4 : (i) each vertex denotes a property p The requirement that the graph is acyclic is needed to make it possible the comparison between EC and CEC complexities.
EC indeed loops whenever there is a cycle among context-dependent properties.
On the contrary, there exist sucient conditions for the termination of CEC also in case of property cycles (the main condition is that at each time instant at most one of the properties involved in the property cycle may hold).
Examples of non-critical property cycles in a medical application domain are given in 8].
The general problem of looping in EC and CEC is currently under investigation.
4  (ii) there exists an edge (pj  pi ) if and only if there exists an initiates at clause for pi having pj as one of its conditions or a terminates at clause for pi having pj as one of its conditions.
Lbk is dened as the length of the longest path in the graph.
It is possible to prove the following theorem by induction on the nesting level of context-dependent properties Lbk .
Theorem 4.2  The complexity Comp(query(Lbk )) of EC query processing, measured in terms of accesses to happens at facts, is O(n(Lbk +1) 3 ), where n is the number of initiating (terminating) events in the database for the considered property.
If we indeed consider Lbk = 0, we fall in the already discussed case of the basic calculus, whose complexity has been shown to be O(n3 ).
What happens when Lbk is 1 or more is that the evaluation of each condition in the initiates at or terminates at predicates for p will result in the evaluation of a mholds for predicate for this condition with the temporal argument unbound and a Lbk ; 1 nesting level.
We showed that the relationship between the complexity upper bounds for query(Lbk ) and query(Lbk ; 1) is expressed by the recurrent expression: Comp(query(Lbk )) = n2 2n (1+Cbk Comp(query(Lbk 1))) ;  were Cbk is the maximum number of conditions found in the context-dependent initiates at or terminates at clauses.
We then proved by induction that the order of complexity of query(Lbk ) is at most O(n(Lbk +1) 3 ).
As in the basic calculus, it is straightforward to show that this limit is actually reached5 .
4.2 The complexity of update processing in CEC  In the case of CEC, the complexity has to be measured in terms of accesses to both happens at and mholds for facts.
Dierently from EC, where each evaluation of a mholds for query results into a number of accesses to happens at facts, in CEC mholds for predicates are explicitly recorded in the database as facts.
Under the given assumption that there is a set of n initiating events and n terminating events for every property in the database, there are at most n disjoint MVIs for each property (n mholds for facts recorded in the database).
Thus all EC accesses to happens at facts for a mholds for query for a given property collapse into at most n CEC accesses to mholds for facts about such a property.
Therefore, the cost of a mholds for query in CEC is linear in the number of cached MVIs for the considered property, whatever is the value of Lbk .
Dierently from EC, the complexity of update processing in CEC is not constant at all.
To precisely determine such a complexity some preliminary notions are needed.
First of all, let P be the maximum number of properties initiated or terminated by a single event.
Furthermore, we must take into account that the assertion of a new mholds for fact (the retraction of an existing mholds for fact) may cause (suppress) the initiation or the termination of a property depending on it and then the assertion of an additional mholds for fact (the retraction of an existing mholds for fact) concerning such a property.
Therefore, we must consider in general an arbitrary level of propagation of assertions (retractions).
Let Lfw be the maximum level of propagation from a single property.
Lfw can be formally dened as the length of the longest path in the property dependency graph, and then it it is equal to Lbk 6 .
It is possible to prove the following lemma by induction on the propagation level Lfw .
Lemma 4.3  The complexity of propagating assertions, measured in terms of accesses to happens at and mholds for facts, is O(nLfw +3 ), where n is the number of initiating (terminating) events in the database for the It is worth noting that this is a worst case analysis.
In fact, Cbk and Lbk can be redened for each single property to evaluate the complexity of specic queries or classes of queries.
6 Note that if Lbk and Lf w are redened to account for nesting and propagation levels of specic properties, they are not necessarily equal anymore.
5  considered property, and it is equal to the complexity of propagating retractions.
The proof is accomplished in three steps.
We rst determine the recurrent expression of the costs of propagateRetract and propagateAssert with respect to a level of propagation Lfw in terms of their costs with respect to the level Lfw ; 1 then we prove that the cost of propagateRetract is always less than the cost of propagateAssert nally we provide the general cost expressions of propagateRetract and propagateAssert.
On the basis of Lemma 4.3, it is straightforward to prove the following theorem about the complexity of update processing in CEC.
Theorem 4.4  The complexity Comp(update(Lfw )) of CEC update processing, measured in terms of accesses to happens at and mholds for facts, is O(nLfw +3 ), where n is the number of initiating (terminating) events in the database for the considered property.
It is worth noting that in the context-independent case (Lfw = 0), the complexity of update processing is O(n3 ).
In such a case there are neither propagation of assertions nor propagation of retractions, and the worst-case complexity of update processing is just the complexity of adding at most P new mholds for facts.
The orders of complexity of query and update processing in EC and CEC are summarized in Table 1.
5 Conclusions  This paper has proposed a caching mechanism for an ecient implementation of Kowalski and Sergot's Event Calculus (EC).
In the context-independent case (Lbk = Lfw = 0), the cached version of EC we developed (CEC) makes the complexity of query processing linear, shifting temporal reasoning from query to update processing.
The complexity of update processing in CEC is indeed equal to the complexity of query processing in EC.
In the more significant context-dependent case (Lbk = Lfw 1), CEC allows us to obtain an absolute improvement in performance because the order of complexity of update processing in CEC (Lfw + 3) is strictly lower than the order of complexity of query processing in EC ((Lbk + 1)  3).
CEC has been fully implemented on a Sun Sparc2 in Quintus Prolog and extensively tested in order to ensure that it yields the same outputs as EC (but much more eciently).
Acknowledgements  We would like to thank one of the anonymous referees for the useful and perceptive comments.
This work has been partially supported by the Italian National Research Council (Special Project on Management of Temporal Information in Data and Knowledge Bases), and the P.A.O.L.A.
Consortium, whose members are ASEM Resolution, INSIEL, and Universita di Udine.
EC update  EC query  CEC update CEC query  Lbk = Lfw = 0  const  O (n 3 )  O(n3 )  O(n)  Lbk = Lfw = 1  const  O (n 6 )  O(n4 )  O(n)  Lbk = Lfw = 2  const  O (n 9 )  O(n5 )  O(n)  :::  :::  :::  :::  :::  Lbk = Lfw = k  const  O(n(k+1) 3 )  O(nk+3 )  O(n)  Table 1: Comparing complexities of EC and CEC.
References  1] J. Allen, Planning as Temporal Reasoning in Proc.
of KR-91, Cambridge, MA, pp.3{14.
2] M. Baudinet, J. Chomicki, P. Wolper, Temporal Deductive Databases.
Chapter 13 in 21], pp.
294{320.
3] M. Boddy, Temporal Reasoning for Planning and Scheduling, SIGART Bulletin, Vol.4, No.3, 1993, pp.1720.
4] I. Cervesato, A. Montanari, A. Provetti, On the Nonmonotonic Behavior of Event Calculus for Deriving Maximal Time-Intervals to appear in The International Journal of Interval Computations, 1994.
5] L. Chittaro, A. Montanari, Experimenting a Temporal Logic for Executable Specications in an Engineering Domain, in G. Rzevski, J. Pastor, R.A. Adey (eds), Applications of Articial Intelligence in Engineering VIII, Computational Mechanics Publications & Elsevier Applied Science, Boston and London, 1993, pp.
185{202.
6] L. Chittaro, A. Montanari, Facing E	ciency and Looping Problems of the Event Calculus through Caching.
Part I: E	ciency Research Report RR18/93, Dipartimento di Matematica e Informatica, Universita di Udine, November 1993.
7] L. Chittaro, A. Montanari, A. Provetti, Skeptical and Credulous Event Calculi for Supporting Modal Queries to appear in Proc.
of ECAI'94, Amsterdam, The Netherlands, Wiley & Sons Publishers, August 1994.
8] L. Chittaro, A. Montanari, M. Dojat, C. Gasparini, The Event Calculus at work: a Case Study in the Medical Domain (submitted), 1994.
9] T. Dean, D. McDermott, Temporal Data Base Management Articial Intelligence, Vol.
32, 1987, pp.
1{55.
10] T. Dean, Using Temporal Hierarchies to E	ciently Mantain Large Temporal Databases Journal of the ACM, Vol.
36, No.
4, October 1989, pp.
687{718.
11] R. Kowalski, M. Sergot, A Logic-based Calculus of Events New Generation Computing, 4, 1986, pp.
67{ 95.
12] R. Kowalski, Database Updates in the Event Calculus Journal of Logic Programming, Vol.
12, June 1992, pp.
121{146.
13] A. Montanari, E. Maim, E. Ciapessoni, E. Ratto, Dealing with Time Granularity in the Event Calculus Proceedings FGCS-92, Fifth Generation Computer Systems, Tokyo, Japan, IOS Press, 1992, pp.
702{712.
14] A. Montanari, B. Pernici, Temporal Reasoning.
Chapter 21 in 21], pp.
534{562.
15] J. Pinto, R. Reiter, Temporal Reasoning in Logic Programming: A Case for the Situation Calculus, Proc.
ICLP'93, Budapest, Hungary, 1993, pp.
203-221.
16] R. Reiter, Proving Properties of States in the Situation Calculus Articial Intelligence, Vol.
64, no.2, 1993, pp.
337-351.
17] M. Shanahan, Prediction is Deduction, but Explanation is Abduction Proc.
of IJCAI'89, Detroit, The MIT Press, 1989, pp.
1055{1060.
18] M. Shanahan, Representing Continuous Change in the Event Calculus Proc.
of ECAI'90, Stockholm, Sweden, 1990, pp.
598{603.
19] M. Sergot, (Some Topics in) Logic Programming in AI Advanced School on Foundations of Logic Programming, Alghero, Italy, 1990.
20] S. Sripada, Temporal Reasoning in Deductive Databases PhD thesis in Computing, Imperial College, London, 1990.
21] A. Tansell, J. Cliord, S. Gadia, S. Jajodia, A. Segev, and R. Snodgrass (eds.
), Temporal Databases: Theory, Design and Implementation, The Benjamin/Cummings Series on Database Systems and Applications, Benjamin/Cummings Publishers, 1993, pp.
534{562.
Belief Revision in a Discrete Temporal Probability-Logic  Scott D. Goodwin  Department of Computer Science University of Regina, Canada  Howard J. Hamilton  Department of Computer Science University of Regina, Canada  Abdul Sattar  School of Computer & Information Technology Grifith University, Australia  Abstract  We describe a discrete time probabilitylogic for use as the representation language of a temporal knowledge base.
In addition to the usual expressive power of a discrete temporal logic, our language allows for the specication of non-universal generalizations in the form of statistical assertions.
This is similar to the probability-logic of Bacchus, but diers in the inference mechanisms.
In particular, we discuss two interesting and related forms of inductive inference: interpolation and extrapolation.
Interpolation involves inferences about a time interval or point contained within an interval for which we have relevant statistical information.
Extrapolation extends statistical knowledge beyond the interval to which it pertains.
These inferences can be studied within a static temporal knowledge base, but the further complexity of dynamically accounting for new observations makes matters even more interesting.
This problem can be viewed as one of belief revision in that new observations may conict with current beliefs which require updating.
As a rst step toward a fulledged temporal belief revision system, we consider the tools of inductive logic.
We suggest that Carnap's method of conrmation may serve as a simple mechanism for belief revision.
1 Introduction  Standard discrete temporal logics allow the representation of what is true at a point, in a situation, or over an interval.
To introduce uncertainty, many researchers in AI have turned to nonmonotonic \logics," but semantic and computational diculties have led some to consider probability as a representational device.
Here we describe a discrete time probabilitylogic for use as the representation language of a tem-  Eric Neufeld  Department of Computational Science University of Saskatchewan, Canada  Andre Trudel  Jodrey School of Computer Science Acadia University, Canada  poral knowledge base.
In addition to the usual expressive power of a discrete temporal logic, our language allows for the specication of non-universal generalizations in the form of statistical assertions.
This is similar to the probability-logic of Bacchus 1], but diers in the inference mechanisms.
In particular, we discuss two interesting and related forms of inductive inference: interpolation and extrapolation.
Interpolation involves inferences about a time interval or point contained within an interval for which we have relevant statistical information.
Extrapolation extends statistical knowledge beyond the interval to which it pertains.
These inferences can be studied within a static temporal knowledge base, but the further complexity of dynamically accounting for new observations makes matters even more interesting.
This problem can be viewed as one of belief revision in that new observations may conict with current beliefs which require updating.
As a rst step toward a full-edged temporal belief revision system, we consider the tools of inductive logic.
We suggest that Carnap's method of conrmation 3] may serve as a simple mechanism for belief revision.
We begin by introducing our temporal logic and then turn to the problem of inferencing.
The rst form of inference we consider is what Carnap calls direct inference: the inference from a population to a sample.
In the case of temporal information, this amounts to inference from an interval statistic to a subinterval or point.
Before moving on to more complex kinds of inference, we introduce the learning (or belief revision) component, Carnap's method of conrmation, which incorporates new observations into the direct inference process.
Next we consider the general case of direct inference: interpolation.
Then we turn our attention to the problem of extrapolation of statistical information (what Carnap calls predictive inference).
Finally, we consider the problem of belief revision in connection with these temporal inferences.
2 Discrete temporal probability-logic  In this section, we introduce a discrete probabilitylogic which serves as a representation language for temporal applications.
The probability-logic, which we call PL(T ), is similar to that of Bacchus 1].
The most important dierence is in the inference machinery and the addition of time into the ontology.
PL(T ) allows the expression of standard rst order logic expressions plus two kinds of probability statements.
Before examining the probability-logic, we rst explore the two kinds of probability.
2.1 Statistical and inductive probabilities  Carnap 3] has suggested the need for two distinct concepts of probability (the relevance of this view to AI was recently suggested 1, 8]).
The statistical concept of probability, having the sense of relative frequency, is needed for empirical knowledge (e.g., most birds y).
As well, the inductive concept of probability, measuring the degree of conrmation of a hypothesis on the basis of the evidence, is needed for beliefs (e.g., to what degree is the belief that Tweety ies supported by the evidence that Tweety is a bird and most birds y).
While statistical probability is empirically based, inductive probability is epistemologically based that is, inductive probabilities constitute a logical relationship between belief (or hypothesis) and evidence.
To give such beliefs empirical foundations, a connection must be made between the statistical and inductive probabilities.
This connection is made on the basis of an appeal to some form of the principle of indierence which says that if our knowledge does not favour the occurrence of one event over another, then the evidence provides equal conrmation for the events.
The inference of inductive probabilities from statistical probabilities via a principle of indierence is called direct inference.
As Carnap 4] has noted, the form of indierence used must be carefully restricted to avoid the introduction of contradictions at the same time, it must remain strong enough to sanction the appropriate conclusions.
The principle of indierence comes into play when choosing the prior probabilities of hypotheses.
Each consistent assignment of priors constitutes a dierent inductive method.
Carnap 4] described two inductive methods which we outline next.
2.2 Two inductive methods  Carnap's two methods are most easily explained with reference to the example shown in Figure 1.
In this example, we have four individuals (balls in an urn) and one property (colour).
Since each ball is either blue (B) or white (W), we regard colour as a binary property (blue or not-blue).
An individual distribution is specied by ascribing one colour to each individual e.g., in individual distribution #2, the rst three balls are blue and the last ball is not.
A statistical distribution is specied by stating the number of individuals for which the property is true, without identifying the individuals e.g., in statistical distribution #2, three of the balls are blue and one is not.
There are 16 possible individual distributions and 5 statistical distributions.
As can be seen in Figure 1, several individual distributions may correspond to a single statistical distribution.
If equal prior probabilities are assigned to each of the individual distributions, the result is Carnap's Method I, and if equal prior probabilities are assigned to each of the statistical distributions, the result is Method II.
Method I consists of applying the principle of indierence to individual distributions and, in the examples, gives each individual distribution a prior probability of 1/16.
Method II consists of rst applying the principle of indierence to the statistical distributions, and then, for each statistical distribution, applying the principle to its individual distributions.
In the example, each of the ve statistical distribution is assigned 1/5, and each 1/5 is divided equally among the individual distributions of the appropriate statistical distribution.
Method II assigns 1/20 to each of individual distributions #2 to #5 because they are the four possibilities (arrangements) for statistical distribution #2 (3 blue balls and 1 white ball).
Method II is consistent with the principle of learning from experience, but Method I is not.
The principle of learning from experience is: \other things being equal, a future event is to be regarded as the more probable, the greater the relative frequence of similar events observed so far under similar circumstance" 4, p. 286].
Suppose we draw three blue balls in sequence, and then consider the probability of the fourth ball being blue.
There are two individual distributions consistent with the evidence: #1 (in which the fourth ball is blue) and #2 (in which the fourth ball is not blue).
Using Method I, the probability is 1/2 because each of individual distributions #1 and #2 is assigned a probability of 1/16, and 1/2 is the relative weight of 1/16 to (1/16 + 1/16).
Using Method II, the probability of the fourth ball being blue is 4/5 because individual distribution #1 is assigned a probability of 1/5 and individual distribution #2 is assigned a probability of 1/20, and 4/5 is the relative weight of 1/5 to (1/5 + 1/20).
Because Method II incorporates the principle of learning from experience, it is better suited to our intended application of temporal reasoning in dynamic situations where new observations are being made.
In Section 3.1, we apply Method II to direct inference from temporal statistical knowledge, but rst we turn to the description of our temporal probability logic.
2.3 PL(T ): A 	rst order temporal probability-logic PL(T ) is a four sorted, rst order, modal logic.1 The 1  Some material in this section is derived from 2].
STATISTICAL INDIVIDUAL METHOD I METHOD II DISTRIBUTIONS DISTRIBUTIONS Initial Initial Probability of Number Number Probability Statistical Individual of of of Individual Distributions Distributions Blue White Distributions 1.
4 0 1.
    1/16 1/5 1/5 = 12/60 2.
3  1  2.
3.
4.
5.
                    1/16 1/16 1/16 1/16  1/5  1/20 = 3/60 1/20 = 3/60 1/20 = 3/60 1/20 = 3/60                              1/16 1/16 1/16 1/16 1/16 1/16  1/5  1/30 = 2/60 1/30 = 2/60 1/30 = 2/60 1/30 = 2/60 1/30 = 2/60 1/30 = 2/60            3.
2  2  6.
7.
8.
9.
10.
11.
4.
1  3  12.
13.
14.
15.
          1/16 1/16 1/16 1/16  1/5  1/20 = 3/60 1/20 = 3/60 1/20 = 3/60 1/20 = 3/60  5.
0  4  16.
    1/16  1/5  1/5 = 12/60  Figure 1: Carnap's Two Methods (from 3, p. 285]) sorts are: object types, object instances, numbers, and times.
Suitable function and predicate symbols are associated with each sort.
Time invariant assertions can be made about domain objects via their object types, while object instances are used to describe domain objects at particular times.
The numeric sort is used to make numeric assertions, specically, assertions about the numeric values of certain probabilities.
The temporal sort allows assertions to include references to the time.
Both the numeric and temporal sort include the constants 1, -1, and 0.
The functions + and fi and the predicates = and < are \overloaded" to provide for all necessary combinations of argument and result sorts.
Additional inequality predicates, numeric and temporal constants can easily be added by denition, and we use them freely.
The formulas of the language are generated by applying standard rst order formula formation rules.
Additional rules are used to generate object instance terms from object type terms and temporal terms: 1.
If o is an object type term, t is a temporal term, ~o is a vector <o1 ,: : : ,on> of n object type terms, and ~t is a vector <t1 ,: : : ,tn> of n temporal terms, then (a) o@t is an object instance term (b) ~o@~t is a vector of n object instance terms,  specically, <o1 @t1,: : : ,on @tn> (c) ~o@t is a vector of n object instance terms, specically, <o1 @t,: : : ,on@t> (d) o@~t is a vector of n object instances terms, specically, <o@t1 ,: : : ,o@tn>.
Two additional rules are used to generate new numeric terms (specically probability terms) from existing formulas: 2.
If  is a formula and ~x is a vector of n distinct object type, object instance and/or temporal variables, then ]~x is a statistical probability term.
3.
If  is a formula, then prob() is an inductive probability term.
These new (numeric) terms denote numbers (that correspond semantically to the values of the probability measure) which can in turn be used as arguments of numeric predicates in the generation of additional new formulas.
We dene conditional probability terms (of both types): j ]~x =df  ^  ]~x = ]~x, and prob(j ) =df prob( ^  )=prob ( ).2 Semantically the language is interpreted using For ease of exposition, we ignore the technicalities of dealing with division by zero.
See 1] for details.
2  models of the form3 M = hO S #  O   S i where: 1.
O is a domain of objects types (i.e., the domain of discourse).
S is a set of states or possible worlds.
# is a state dependent interpretation of the symbols.
Numbers are interpreted as reals IR and the set of times T is taken as integers ZZ .
The associated predicate and function symbols are interpreted as relations and functions over the appropriate domain while +, fi, 1, -1, 0, < and = are given their normal interpretation in every state.4 2.
O is a discrete probability measure P over O. ThatPis, for every A  O,  O (A) = o2A  O (o) and o2O  O (o) = 1.
3.
S is a discrete probability measure P over S .
ThatPis, for every S 0  S ,  S (S 0 ) = s2S  S (s) and s2S  S (s) = 1.
4.
T is a discrete probability measure P over T .
ThatPis, for every T  T ,  T (T ) = t2T  T (t) and t2T  T (t) = 1.
5.
O@T is a discrete probability measure over the set of object instances O@T .
That is, for every O @T P  O@TP ,  O@T (O@T ) = o@t2O@T  O@T (o@t) and o@t2O@T  O@T (o@t) = 1.
O@T is a product measure formed from  O and  T .
The formulas of the language are interpreted with respect to this semantic structure in a manner standard for modal languages.
In particular, the interpretation of a formula depends on a structure M , a current state s 2 S , and a variable assignment function .
The probability terms are given the following interpretation:   1.
(]~x)(Msfi) =  nfi f~as:t:(M s ~x=~a]) j= g , where ~x=~a] is the variable assignment function identical to  except that (xi ) = ai , and  nfi is the n-fold product measure formed from  fii where i = O or O@T or T depending on whether xi is an object type variable, an object instance variable, or a temporal variable.
 ; 2.
(prob())(Msfi) =  S fs0 s:t:(M s0  ) j= g .
So we see that ]~x denotes the measure of the set of satisfying instantiations of ~x in  and prob() denotes the measure of the set of states that satisfy .
Unicorns have a single horn: 8o: unicorn(o) !
singleHorn(o).
Unicorns have never, do not, and will never exist: 8o t: t < now & unicorn(o) !
:exists(o@t), 8o: unicorn(o) !
:exists(o@now), 8o t: t > now & unicorn(o) !
:exists(o@t).
or, more simply: 8o t: unicorn(o) !
:exists(o@t): Most birds y: fly(o)jbird(o)]o > 0:5.
Most birds y now: fly(o@now)jbird(o@now)]o > 0:5.
At any time, most birds y: 8t: fly(o@t)jbird(o@t)]o > 0:5.
For most object instances, if the object type is a bird at the time then it ies at the time: fly(o@t)jbird(o@t)]o@t > 0:5.
Most of the time, most birds y: fly(o@t)jbird(o@t)]o > 0:5]t > 0:5.
Informally, the above expression says that if we pick a time at random, chances are that more than 50% of the birds y at that time.
In addition to statistical assertions, we can also represent inductive probability assertions (which correspond to an agent's beliefs).
For example,  Halpern has called such structures type III probability structures.
4 We ignore the technicalities of dealing with overloading and argument/result type conversions.
The degree of belief in the proposition \Tweety is ying now" is 0.9: prob(fly(tweety @now)) = 0:9:  0  3  In addition to the statistical and inductive probabilities, we need an extension that allows us to represent epistemic expectation.
Specically, if p is a statistical probability term, then E(p) is a new numeric term whose denotation is dened as follows: X (E(p))(Msfi) =  S (s0 ) fi p(Ms fi) : 0  s 2S 0  That is, the expected value of a term is the weighted (by  S ) average of its denotation over the set of states.
2.4 Representation in PL(T ) PL(T ) allows for the representation of a rich variety of statistical and probabilistic temporal information.
Because time is associated with object instances rather than with properties of objects, we can describe objects that come into existence or cease to exist.
We can also talk about properties of object types that have no instances, such as unicorns.
The following examples gives some idea of the expressive power of the language.
The degree of belief in the proposition \Most birds y" is 0.75:   prob fly(o)jbird(o)]o > 0:5 = 0:75: Two remarks are in order here.
First, although PL(T ) supports the representation of beliefs about temporal assertions, there is no support for temporal beliefs, i.e., only the current set of beliefs is representable.
This shortcoming while be addressed in future work.
Second, some form of direct inference is needed to connect the inductive probabilities to the statistical ones as was discussed in section 2.1.
We are now in a position to provide this connection.
3 Inferences in PL(T )  The choice of the distributions  O ,  S , and  T affect inferences in PL(T ).
Choosing a \uniform" distribution for  O ,  S , and  T corresponds to Carnap's Method I.
In the case of  T , we can not have a true uniform distribution since T is innite, so we take  T (T ) = jT j/jTnj, where Tn = f0 ;1 1 ;2 2 :: : ;1n;1 fibn=2cg, and then we consider the situation in the limit as n !
1.
For any nite set of times T , the measure is 0 so we must amend the interpretation of conditional statistical probabilites.
With j ]~x =df limTn !T  ^  ]~x= ]~x, what matters is the relative sizes of the sets of times involved in the numerator and denominator.
We can also choose distributions which result in inferences corresponding to Carnap's Method II.
To do this, the distributions  O and  T are taken as above, but to dene the distribution  S , we need to introduce the concept of structures which are equivalence classes of states.
Two states, s1 and s2 are considered isomorphic if replacing the individuals of s1 with one of their permutations results in s2 .
Let S be the set of structures corresponding to the set of states S .
We can now dene the distribution  S in such a way that every subset S 0 of S which is a member of S has the same measure and every member of S 0 has the same measure.
This results in inferences corresponding to Carnap's Method II.
We examine both Method I and II inferences in section 3.1.
Then in sections 3.2 and 3.3, we discuss interpolation and extrapolation.
Finally, we consider temporal belief revision issues in section 3.4.
1  3.1 Direct inference  We can connect inductive and statistical probabilities in a similar manner as Bacchus did in 1].
We start by assuming that an agent expresses assertions about his environment in a xed statistical language Lstat.
Assertions in Lstat, which are all the assertions of PL(T ) excluding those involving inductive probability, are called objective assertions.
The agent's degree of belief in the objective assertions are represented in another language Lcomb which extends Lstat with the inductive probability operator prob and an  royal elephant(clyde) & elephant(clyde) 8x:royal elephant(x) !
elephant(x) gray(x)jelephant(x)]x > 0:5 :gray(x)jroyal elephant(x)]x > 0:5:  Figure 2: Redundant Information epistemic expectation operator E. Formulas of Lcomb that are also in Lstat are called objective formulas.
The knowledge base KB is the complete nite collection of objective formulas   which are fully believed by the agent i.e., prob KB = 1.
De	nition 1 (Randomization 1]) stat  Let  be a formula of L .
If hc1 : : : cni are the n distinct object constants5 that appear in  ^ KB and hv1  : : : vni are n distinct object variables that do not occur in  ^ KB, then let KBv (v ) denote the new formula which results from textually substituting ci by vi in KB (), for all i.
(KBv is referred to as the randomization of KB or KB randomized.)
De	nition 2 (Direct Inference Principle 1] ) If the agent fully believes that KBv ]~v > 0 and if  is a formula of Lstat then the agent's degree of belief in  should be determined by the equality prob() = E(v jKBv ]~v ): Method I inferences: If the distributions are chosen for Method I as described in section 3, inferences in PL(T ) have similar properties to those described in 6], e.g., desirable inheritance properties.
For example, in Figure 2, PL(T ) infers that prob(:gray(clyde)) > 0:5.
That is, we have inheritance with specicity in spite of the redundant information elephant(clyde).
This method supports a number of desirable inferences such as those involving simple inheritance, multiple inheritance with specicity, ambiguity, cascaded ambiguity, cycles, redundant information, and negative paths (see 6]).
Such a system might be sucient for most needs.
It even includes the ability to revise beliefs about individuals, i.e., inheritance of properties is aected by receiving more specic information about an individual.
Furthermore, the inclusion of additional statistical assertions may aect properties inherited to individuals.
What is lacking, however, is an ability to revise beliefs in statistical formulas given individual observations.
This can be addressed by Method II.
Method II inferences: If the distributions are chosen for Method II as described in section 3, inferences in PL(T ) have in addition to the desirFor our purposes, these refer to object types, object instances, and/or times.
5  able inheritance properties described in 6], the ability to dynamically account for observations in beliefs about statistical assertions.
For example, in Figure 3, if O contains only ve object types, s1, s2, s3, s4, and s5, then, as reported in 7], initially prob(fly(s5)jsparrow(s5)) = 0:6 (see Figure 3).
The table in Figure 3 was computed by the method of exhaustive enumeration as described in 7].
In the table, probI means prob with the distributions set for Method I probII means prob with the distributions set for Method II.
Upon learning that s1 is a ying sparrow, prob(fly(s5)jsparrow(s5)) = 0:5714 under Method II as compared to 0.5 under Method I.
Comparing this to prob(fly(s5)jbird(s5)) which is 0.5, we see that in spite the the observed ying sparrow, Method I sticks to straight inheritance of the ying birds statistic to sparrows, whereas Method II adjusts to the observation and infers sparrows are even more likely to y than birds.
So far, the examples in this section have not involved time.
In sections 3.2 and 3.3, we examine the temporal inferences we call interpolation and extrapolation.
3.2 Interpolation  Suppose we have the following situation: Over the year, it rains 40% of the time.
During winter (December 21{March 20), it rains 75% of the time.
Over the summer (June 21{September 20), it rains 20% of the time.
What percentage of rainfall occurs during December?
What is the chance of rain on December 24th?
We can represent this in PL(T ) by letting the integers 1 through 365 represent the days (i.e., each day is a time point) and provide axioms such as shown in Figure 5.
Inferences about the rainfall in December or on December 24th based on the given statistical information are in a class of inferences we call interpolation.
These inferences involve using interval statistics to induce subinterval statistics or point probabilities.
For instance, the actual percentage of rainfall in December is: P3 = P rain(t)jr3(t)]t rain(t) = R3jR3 j (t)jr3b(t)]tjR3b j = 	rain(t)jr3a(t)]tjjRR33aajj++	jRrain 3b j where the value of the numerator is unknown.
(Note in the summation, we are treating rain as if it were a 0-1 function with value 1 at t if there is rain at time t and value 0 at t otherwise.)
To compute the amount of rainfall in December we divide the month (region R3 from Figure 5) into subregions R3a = dec1 dec20] and R3b = dec21 dec31]: The specic information about R3a is obtained from R5 where R5 = mar21 jun20] + sep21 dec20] = R1 ; R2 ; R4.
rain(t)jr1(t)]t = 0:4, rain(t)jr2(t)]t = 0:75, rain(t)jr4(t)]t = 0:2, % plus axioms dening the % regions r1, r2, r3, r4 R3 : ??
R2 : 75%  Dec1 Dec21 Dec31 Jan1  Mar20  R1 : 40% R4 : 20% Jun21  Sep20  Figure 5: Rainfall Interpolation The statistic for R5 can be computed from the statistics for R1, R2, and R4, and from the relative sizes of these intervals.
We compute the actual percentage of rain P5 over R5 to be approximately 33% (see Figure 4).
By assuming every subset of R5 has the same expected percentage of rain (i.e., using Method I), we conclude the expected percentage of rain over R3a is P5 : The most specic reference class (for which we have or can compute the actual percentage of rainfall) that contains R3b is R2.
By assuming every subset of R2 has the same expected percentage of rain (Method I), we conclude the expected percentage of rain over R3b is 75%.
The expected percentage of rain over R3 equals a weighted average based on R3a and R3b:   E(P3) = E rain(t)jr3(t)]t :75jR3bj = P5 jR3a jj+0 R3 j :7511  0:3320+0 31  48%.
The answer to the original question is that it rains roughly half the time during December.
3.3 Extrapolation  Persistence (the frame problem) has been viewed in two ways: 1) action-invariance of a property: whether a property that is true before an action or event will remain true afterwards, cf.
temporal projection 9] or, 2) time-invariance of a property: if a property is true at some point in time, how long is it likely to remain true 5].
Under these views, a property such as raining at a given point in time is highly action-invariant (few actions aect rain) and slightly time-invariant (it rains for a while and then stops).
Here we consider a previously unexplored aspect of the frame problem: action and time invariance of statistical knowledge.
Given statistical information about various time  A Statistical KB fly(x)jbird(x)]x = 0:6, 8x:sparrow(x) !
bird(x).
Method I and II Inferences Known ying sparrows => none probI (fly(s5)jbird(s5)): 0.6 probII (fly(s5)jbird(s5)): 0.6 probI (fly(s5)jsparrow(s5)): 0.6 probII (fly(s5)jsparrow(s5)): 0.6  s1 0.5 0.5 0.5 0.5714  s1, s2 0.3333 0.3333 0.3333 0.4286  Figure 3: Belief Revision  P5 = rain(t)jr5(t)]t = 	rain(t)jr1(t)]tjR1 j;	rain(t)jjrR2(5tj)]tjR2 j;	rain(t)jr4(t)]tjR4 j ;0:75	79+11];0:292  33%.
= 0:4jR1j;0:75jRj5 jR2j;0:2jR4j = 0:4365365 ;	79+11];92 Figure 4: Calculation of P5.
intervals, we wish to make reasonable inferences about past or future intervals.
For example, Figure 6 depicts a situation where we know that it rained 75% of the time in the winter, and 20% of the time during the summer.
We have no statistical information about the coming year (R6: December 1 to November 30) so the interpolation technique in the previous section is not applicable.
The temporal projection technique of Hanks and McDermott 9] is also inappropriate.
We cannot determine from the statistical information whether it was raining on September 20.
Even if we knew it was raining at that time, it does not make sense to allow raining to persist indenitely.
We have no information about actions or events that may aect raining.
Finally, Dean and Kanazawa's 5] probabilistic temporal projection cannot be used as it requires the construction of a survivor function for raining based on many observations of raining changing from true to false.
In our example, we have no observations of raining at particular points.
We only have interval statistics.
Instead of considering persistence at the level of individual time points, we can view it at the interval level and describe the persistence of statistical information.
If we take the observed statistics to be samples of raining over time (i.e., over the whole time line), we can base our inferences for other intervals on these samples.
For instance, we can infer a statistic for R6 in Figure 6 using R2 and R4 as follows:   E rain(t)jr6(t)]t rain(t)jr4(t)]tjR4 j = 	rain(t)jr2(t)]tjjRR22 jj+	 +jR4 j :292 = 0:7590+0 182  47%.
This result corresponds to that obtained by both Method I and II.
Space considerations force us to omit a detailed discussion of the precise mechanics of interpolation and extrapolation inferencing in PL(T ), but we have provided enough detail to highlight relevant issues.
As well, our discussion of interpolation and extrapo-  lation, so far, has not touched on belief revision issues.
We turn to consideration of this next.
3.4 Temporal belief revision  In the preceeding two subsections, we have described two forms of inferencing in PL(T ).
For a xed temporal knowledge base which includes only interval level statistics (such as in the examples of Figures 5 and 6), the results for Method I and Method II are the same.
The situation becomes more interesting when the knowledge base is updated with new statistics and point information.
There are three important cases to consider: 1) new interval statistics 2) new point information aecting the relevancy of interval statistics and 3) new point information aecting the predicted value of interval statistics.
New interval statistics: Suppose in the example of Figure 6, as time passed, we came to observe the rainfall in December of the coming year (R7, a subinterval of R6) and found it to be 60%.
Prior to learning this, we had predicted the rainfall for the coming year to be about 47%.
The newly acquired interval statistic for December should cause us to revise our prediction for the coming year.
Under both Method I and II, this is indeed the case.
Referring to December of the coming year as region R7, the result under either method would be approximately 50% (see Figure 7).
New point information (relevance): Suppose in the example of Figure 5, we wanted to predict the chances of rain on the day of a party to be held in December (R3).
Since we do not know the exact day, the prediction about rain on the day of the party given the day will be in December is based on the inferred statistic for R3, i.e., prob(rain(party day)) is about 48% (cf.
the example of Figure 4).
As time passes, we come to learn the party will be held on December 24.
This (point level) information should cause us to base our prediction of rain on the statistic for R3b (which is derived from R2) which is more relevant than the inferred statistic for R3 given that the  R2 : 75%  R7 : 60% R4 : 20%  Dec21  Mar20 Jun21  Sep20  Dec1  R6 : ?
?%  Jan1  Figure 6: Rainfall Extrapolation  Nov30      E rain(t)jr6(t)]t = E 	rain(t)jr7(t)]tjR7 j+	rainjR(t6)jjr6(t) & :r7(t)]t jR6 ;R7 j     = E 	rain(t)jjrR7(6tj)]tjR7 j  + E 	rain(t)jr6(t) &jR:6rj7(t)]t jR6 ;R7 j  	 rain ( t ) j r 2( t )] j R t 2 j+	rain(t)jr 4(t)]t jR4 j+	rain(t)jr 7(t)]t jR7 j jR6 ;R7 j   = 	rain(t)jjrR7(6tj)]tjR7 j + jR2 j+jR4 j+jR7 j jR6 j = 0:631 + (0:7590+0:292+0:631)334  50%.
365  213365  Figure 7: Calculation of next year's expected rainfall.
day of the party is in R3b.
Again, this is indeed the case in both Method I and II, and the revised belief becomes: prob(rain(party day)) is 75%.
New point information (value): So far, there is has been no reason to choose between Method I and II.
A dierence arises, however, as we incorporate point level observations that aect the predicted value of interval statistics.
To see this, again consider the example from the previous paragraph about rain on the day of the party.
Suppose we observe the rain on certain days in December (but not the day of the party).
Let us suppose that, although we have made these observations, we have not come to learn the party is not on one of those days.
(This could happen, say, if a friend was telling us about the party and we had independently observed the weather.)
Now suppose each of the days we observed was a rainy day.
This should cause us to revise our belief in rain on the party day, i.e., we should increase our belief in rain on the party day.
Method I does not do this.
It stubbornly holds to the belief prob(rain(party day)) is about 48% based on an unchanged R3 (inferred) statistic.
Method II, however, increases the predicted value of the R3 statistic and hence increases the value of prob(rain(party day)).
4 Conclusion  We have described the discrete temporal probabilitylogic we call PL(T ) which is expressive enough to represent and reason with a rich variety of problems.
Underlying the probability-logic is a choice of distributions over objects, states, and times.
Dierent choices correspond two dierent inductive methods.
We have focused on two methods described by Carnap.
For most purposes, either method seems adequate, but we found there are cases in the context of belief revision where Method II is superior.
This is  particularly true when new point level observations are made which aect the value of predicted interval statistics.
References  1] F. Bacchus.
Representing and Reasoning with Probabilistic Knowledge.
MIT Press, Cambridge, Massachusetts, 1990.
2] F. Bacchus and S.D.
Goodwin.
Using statistical information in planning.
unpublished extended abstract], May 1991.
3] R. Carnap.
Logical Foundations of Probability Theory.
University of Chicago Press, Chicago, Illinois, 1950.
4] R. Carnap.
Statistical and inductive probability.
In Readings in the Philosophy of Science.
Prentice-Hall, 1989.
5] T. Dean and K. Kanazawa.
Probabilistic temporal reasoning.
In Proceedings of the Seventh National Conference on Articial Intelligence, pages 524{528, St. Paul, Minnesota, August 1988.
6] S.D.
Goodwin.
Second order direct inference: A reference class selection policy.
International Journal of Expert Systems: Research and Applications, 5(3):1{26, 1992.
7] S.D.
Goodwin and H.J.
Hamilton.
An inheritance mechanism for default reasoning that learns.
In International Symposium on Articial Intelligence, pages 234{239.
Monterrey, Mexico, 1993.
8] J. Halpern.
An analysis of rst-order logics of probability.
In Proceedings of the Eleventh International Joint Conference on Articial Intelligence, pages 1375{1381, August 1989.
9] S. Hanks and D.V.
McDermott.
Nonmonotonic logic and temporal projection.
Articial Intelligence, 33(3):379{412, November 1987.


Managing Large Temporal Delays in a Model Based Control System Fano Ramparany  ITMI BP 87, 38244 Meylan, France  Abstract:  In this paper we explain how we have integrated the functionalities of a constraint management system and a temporal data base system to enable a model-based control of systems that exhibit large delays between the events characterizing its behaviour.
Examples of problems where our approach is appropriate, include the monitoring and controlling of complex and geographically distributed systems.
Such applications require a robust modeling of the behaviour of the systems, in terms of causal relationships among its state variables, and the handling of temporal delays that may span between an event and its causal inuences all over the system.
Our approach as been applied to build a KBS for assisting heating central operators to optimize the eciency and profitability of the heating process.
1 Introduction  The main diculty in controlling urban heating systems stems from the geographical distribution of the system to be controlled as it introduces large delays between events and their eects.
Another source of diculty is related to the complexity of the heating system, in terms of the number of its components, the behaviour of these components and the interactions among them.
An example of the topology of an urban heating system is displayed in gure 1.
End user heating stations are depicted with the symbol 1.
Such a network involves more than a hundred stations, all interconnected with heating ow pipes.
When an operator located at Bissy station, modies the temperature of the primary network (heating ow), its eect impacts on the temperature of the secondary network (heated ow), from one to three hours later depending on the speed of the primary ow.
The problem is complicated by the number of parameters which need to be measured, and perhaps  S.V.T.
Heating Central  Bissy  Place de Geneve Sembat  Tercinel Lycee vaugelas  voutes Hopital  Covet Exchanger  Croix des Brigands  Place St.Pierre de Mache Prefecture  Petit Biollay  Figure 1: Part of the map of Chambery city's urban heating system  eventually modied to change the behaviour of the system.
Furthermore the space of control parameters itself contains redundancy.
For example, fullling an increase of the users consumption needs can be realized either by opening the primary network oodgates, or by raising its temperature.
Using a deep model of a system to control and monitor its process has largely been proven a successful approach 3].
Model based control and more generally model based reasoning qualify reasoning processes that draw their inferences upon functional, structural or behavioural models of the system to be controlled.
Several techniques to build and exploit such so called \deep models" have been developed and used so far.
Quantitative models are usually dened as a set of numerical or symbolical constraints that relate relevant variables of the system to each other.
When the number of equations is too high, or if some of them are not available, a qualitative approach is often used.
This constructs an approximate model of the system, and reasons more abstractly about it to derive consistent states which encompass physical qualitative behaviors.
However, none of current \deep" modeling techniques is able to manage large delays between variables.
Instead of extending one of them to t this specic requirement, we propose a more exible approach which integrates a constraint management system with a temporal database system.
The constraint management system enables the causal modeling of the system.
The temporal database system is used to store the temporal histories of some properties of the system that are required when modeling the behaviour of the system.
In this paper, we describe the problems of integrating causal and temporal reasoning.
We then introduce the main concepts of our approach for tackling them.
We show how we have applied this approach for developing a system assisting urban heating power station operators to tune the control parameters of the system, in order to optimize the eciency and protability of the heating process, while satisfying the user consumption requirements.
We nally discuss our approach and compare it to related approaches.
2 Problem Statement  When controlling complex systems with large temporal delays, two types of requirements have to be faced.
Reasoning about time, and reasoning about system functions and behaviour.
More specically, the following capabilities are generally required: Temporal projection: makes it possible to know which properties of the system are known and  what those properties are at some future point in time.
Anticipatory action: enables the planing of an action suciently in advance, so that it eects will occur at the right time.
For example, if we want somebody to receive a parcel within two days, we should better post it now.
Temporal queries: are queries about which property holds within a specic temporal window.
For example, a football match organizer might be interested in checking whether it has rained yesterday evening as to how the terrain grass will look like.
Causal modeling: allows describing the set of parameters that are relevant to describe the state of the system, and to explicitly represent the network of causal relationship among these parameters.
Simulation: completes the description of the system state or infers its subsequent states, based on a partial description of the initial state.
This kind of inference is mainly supported by a causal model of the system to be controlled.
At the requirements analysis level, we can clearly separate representation and reasoning about time, from representation and reasoning about causal dependencies.
Temporal projection, anticipation and queries are typical inferences expected from a temporal reasoning system.
Causal modeling and simulation are typical inferences expected from a causal modeling tool.
From a methodological point of view we propose to preserve the separation between temporal and causal reasoning at the design and implementation stage of the system.
It has been argued that such methodological option allows a modular design of the target system, thus clarifying its conceptual architecture and easing its development as well as its maintenance, specially when dierent development teams are to be involved.
This decomposition approach to design is also a very pragmatic way to \reuse" existing modules within new applications.
In the following sections we will show that reusing and combining existing and standard techniques for handling temporal reasoning and causal reasoning, can reveal a very powerful and ecient development strategy.
3 Integration framework  We describe here the integration framework that has been adopted to enable temporal and causal reasoning to cooperate.
We very briey summarize the basic notions of temporal databases and constraints satisfaction problems as far as they are relevant for this paper.
The reader who is conversant in these areas may skip the respective subsections, or just browse through to pick up the terminology.
3.1 Temporal databases (TDBMS)  As a conventional database system a TDBMS store facts and objects, the specicities of TDBMS is that it provides facilities to describe their temporal properties 10].
For example one can specify the time at which some fact becomes true, and when it becomes false.
So that one can query at which time(s) some fact was true in the past, or will be true in the future.
One can can even ask about which facts are true within a specic temporal window.
The temporal database that we have used is based on Sergot and Kowalski Event Calculus (EC) framework 6].
The EC is a treatment of time in rst-order classical logic, based on the notion of events in order to temporally generate properties that hold for a given time interval.
EC combines the expressive power of situation calculus and the computational power of logic programming.
The EC axioms allow generation of answers to queries about the holding properties.
Although many extensions to the EC framework are possible, such as the handling of dierent temporal granularities and continuous change, we have basically restricted its use to the basic framework.
3.2 Constraint management systems (CMS)  The idea of using constraints as a problem solving and programming paradigm is very useful for model based reasoning, such as diagnosis, simulation and control.
This is because much of the modelling deals with relationships which are naturally described as constraints.
For example, the underlying mathematical models for a range of physical phenomena such as the conservation of heat, uid ow, the relationship between current and voltage, etc.
can be described by constraints.
The constraint management system built here is based on the constraint logic programming language CLP(R) 5, 4].
The essential idea of the constraint programming paradigm is that constraints represent relationships between the objects.
The underlying constraint solver then guarantees the consistency of the constraints as a whole in the system and also solves for the values whenever possible.
When we do not have specic unique values for the variables then \answer constraints" also serve as output to describe the solution space.
Thus when posing constraints to the CMS we do not care if there are values for the variables, we know that those values will be computed either now or later implicitly when more information is obtained or when more constraints are added.
Constraint systems are characterised by the kinds of constraints expressible and the computational power of the constraint solver.
The underlying CLP(R) solves all linear arithmetic constraints directly and completely solves these class of constraints.
More complex non-linear constraints are  not solved directly and are deferred to be solved by local propagation when they become suciently linear.
3.3 Interfacing the TDBMS with the CMS  We will now see how these two paradigms can be associated into a system that can manage a constraint network of temporal properties.
Specifying the interface between the TDBMS and the CMS amounts to dening the conceptual mapping between their respective representation (in terms of what a conceptual primitive in one representation corresponds to the conceptual primitives of the second representation), the functional interface which dene the services that one of the system will request from the other system in runtime, and the control architecture which species the conditions required for such interaction.
3.4 Conceptual Mapping  CMS events, i.e.
variables and their associated values, are simply stored as items in the TDBMS which are dated according to the absolute date of their computation.
Not all variables histories are worth storing in the TDBMS, only those that will eectively be used for causal simulation are.
3.5 Functional Interface  The TDBMS oers its full functionalities to the CMS.
Thus allowing the CMS to post dated events, to perform temporal queries including retrieving of properties that hold at a given time point, or interval.
Conversely, the CMS may be requested by the TDBMS to post new constraints, or variable instantiations (which are considered by the CMS as a special constraint specication), in which case the CMS will automatically propagate the eects of these new constraints on all the variable of the system to derive an updated consistent state, or detect some constraint violations.
This mechanism allows the simulation of eects of future events.
3.6 Control architecture  The control architecture is specied using metarules that identify specic situations that are recognized by one of the reasoning schemes, in which it has to request services from the other reasoning scheme.
The description of the service to be requested is also given by the metarule.
An example of such metarule is:  \ If the temperature of component A has not exceeded a critical threshold Tco during the past 2 hours, then component A exhibits the nominal behaviour, and its output temperature will be twice that of its input temperature."
In this metarule, the situation which is \component A's temperature is below Tco during the last 2 hours" is recognized by the temporal data base, whereas the action to be taken \output temperature  CMS  temporal queries  T  Lrp  TBDMS  temporal assertions  U  Z=3  Bissy Station  Tdrp1  Tarp1  Covet Exchanger Tdrp2  Y < 4  X Z  Vp1  constraints  Vp2 Drp  Y time  Trrp1a  Figure 2: Integration architecture of component A is twice that of its input value" is  interpreted by the causal reasoner.
Metarules can be domain dependent, or task dependent.
In the latter case this metarule can be reused in other domain for similar problem.
The gure 2 summarizes the features of the integration framework.
In the following section we show how it instantiates in the context of a real application.
4 Application  We have implemented a system for providing operators with recommendations on which control actions to take in order to optimize the heating process of Chambery city.
A urban heating system can roughly be described as interconnected networks of heating components such as heat exchangers and urban heating power stations, and mechanical components such as Pump and Flow pipes.
Heat exchangers allow the heating of a secondary ow, using the heat brought by an incoming primary ow.
Urban heating power stations consume fuel and gas to produce heat which is transferred to a circulating ow.
Heating components are linked together through ow pipes carrying liquid that transmit heat from one component to the others.
Hydraulic pumps maintain a controlled ow of the heating liquid that circulate in the pipes.
The basic association of a central and exchanger is shown in gure 3.
During the analysis phase of the system we have interviewed two types of experts: System operators who daily monitor the heating power station and tune its control parameter to satisfy the users requirements, and adapt the system to the weather condition uctuations.
Operators possess implicit models of how some part of the heating network behave.
For example, they know that increasing the speed of the primary ow pump will increase the temperature of the secondary ow through the heat exchanger.
Trrp2  Trrp1  Pc  Figure 3: Example of two components of the urban heating system  System designers who know the physical charac-  teristics of the heating system, such as the length of the various networks, the characteristics of the exchangers.
A rst analysis with the operators helped identify which parameters are relevant for characterizing the state of the heating system.
This preliminary analysis was completed by interviewing the system designers, to rene the relationships between those parameters and make explicit their underlying theoretical justications.
Several additional parameters were introduced by the system designer that were ignored or unconsciously skipped by the operators.
For example, the dependency between the pump speed V p and the temperature of the secondary ow exiting from the heat exchanger Tdrp2 that has been identied by the operators, can be explained with a causal chain comprising four links.
The rst one relates V p to the ow Drp1 circulating in the primary ow pipe.
The second one relates Drp1 to the power transmitted to the exchanger PC.
The third one relates P C to the \loss" of temperature between the primary and secondary ow of the exchanger DTCe, which is itself obviously related to Tdrp2.
Each of these causal link can be formalized as a numerical constraint, that involves the two variable linked and eventually other variables.
For example, the rst causal link described above represents the following equation: Drp1 = Kp  V p where K1 is a constant characterizing the pump.
This two stages analysis phase has produced a behavioural model of the system.
Part of this model is displayed in gure 4, where arrows depict causal links.
Each of the arcs of the network depict a numerical constraint that has been registered in the CMS constraints base.
Prediction of the users power consumption is computed by a specic external module which is not described here.
They are stored in the TDBMS along with the corresponding time of the prediction.
M  Dp  Pay  Evay Prx U  Vp  Drp2  Drp1  Cost  PC  Trrp1  Trrp2 DTrp2  DTrp1 Tarp1  Tdrp2  DTCe  Tdrp1  Tdrpx  Figure 4: Part of the behavioural model of the heating system Pay  Evay  Control variables  U  Vp Drp1  C  Legend  M  Dp  Prpx  input variable  computed variable  Drp2  causal link  PC  Trrp1  Trrp2 DTrp1  DTrp2  Tarp1  Tdrp1  constraint propagation flow  Tdrp2  DTCe Tdrpx  Figure 5: Example of a causal constraint propagation for control recommendation generation The constraints on the prediction of users future power consumption, are registered by the CMS which also propagates the eects of these constraints.
Due to the geographical distribution of the various user stations, the eects of a constraint occur at various instants in time depending on the location of the stations and the speed of the ows that feed them.
This temporal information is stored in the TDBMS.
The CMS ensures the constraints are applied and kept consistent.
As constraints are setup to describe the causal model of the heating system network, the input parameters to the system cause variables which are directly related to be solved, and this eect propagates throughout the causal model which in turn enables solving other variables.
The constraint propagation ends when the control parameters (those used by the operators to tune the heating system) are assigned their values.
Figure 5 displays a trace of such constraint propagation that instantiates the pump speed V p and the temperature of primary ow Tdrp1 exiting from the \Bissy" principal heating power station.
V p and T drp1 are the control parameters of the \Bissy" heating power station.
5 Discussion  The problem of handling of large temporal delays for process control has been identied for some time by  research in control theory 2].
Specic techniques have been introduced for this matter.
Model based approaches of process control problems, have been introduced during the last decade in order to overcome the shortcomings of classical control theory techniques with respect to the needs for high level explanations and incompleteness in the knowledge of the process to control.
We rst looked into qualitative physics and systems like FTQ 1] and QSIM 7], as such models have been used in the application domain of central heating systems 8].
However, FTQ requires variables histories to be continuous, as well as a rather complete knowledge of the system to model in terms of qualitative transfer functions.
On the other hand, QSIM allows more approximations to be done while modeling the physical system and its behaviour, but QSIM doesn't provide for managing large delays.
The power of qualitative modeling technique for modeling transient states was not necessary in our case as the time granularity we worked at, was far coarser than the time constant of thermodynamical phenomena.
Thus steady state equations that don't involve any temporal derivatives are sucient in our case.
Due to the limitation of current causal modeling techniques when reasoning on a phenomenon with large temporal delays, we have proposed a solution that loosely integrates the functionalities of a temporal database management system and that of a constraint management system.
This approach enabled us to combine existing robust temporal and causal reasoning techniques.
For this purpose we have introduced a general framework for integrating heterogeneous representation and reasoning formalisms, that we have applied to the more restricted problem of integrating temporal and causal reasoning.
From a more general perspective, characteristics shared by complex dynamic applications that hamper their deployment include the requirement to handle more than one of the following: evolution of the information with time, incompleteness, unreliability and uncertainty of information which has to be managed.
Complex applications also often require a cooperation between specialized knowledge-based facilities for simulation, fault diagnosis, planning/scheduling, emergency decision making, etc... We believe that such approach of loosely integrating heterogeneous formalisms helps in developing such applications.
Along this direction, we are currently engaged in the ongoing Esprit-III project UNITE 9] which aims at tackling these integration issues.
More specically, we are developing appropriate techniques for integrating heterogeneous knowledge models including incompleteness, uncertainty and time-dependency (internal integration) and for the cooperation between knowledge based facilities (external integration).
These techniques will be implemented in an integrated support environment  composed of a generic platform which supports the development and integration of complex applications.
This support environment and the applicability of our approach are being assessed in the framework of two other pilot applications in the domain of space control centers and neonatal intensive care unit in hospitals.
One main lesson that we have learned through the work described in this paper is that the following three aspects should be carefully specied when integrating heterogeneous formalisms.
Conceptual mapping: \what a conceptual primitive in one representation corresponds to in terms of the conceptual primitives of the second representation?"
Functional interface: \which services will one system request from the other system at runtime?"
Control architecture: \ what the conditions required for interaction are, and which corresponding actions should then be triggered?"
The second lesson is that such loosely integration approach reveals a ecient development strategy, as it allows existing techniques and tools to be eectively reused.
Acknowledgements  Thanks to Renaud Zigmann, Roland Yap and the anonymous referees for their valuable comments on early drafts of this paper.
The research reported here was carried out in the course of a project partially funded by the \Agence pour la Ma^trise de l'Energie".
The partners in this project are Cofreth, LIAC-Lyonnaise des Eaux, SCDC and ITMI.
References  1] P. CALOUD.
Raisonnement Qualitatif.
PhD thesis, INPG, 1989.
2] S. DADEBO and R. LUUS.
Optimal control of time-delay systems by dynamic programming.
Optimal Control Applications & Methods, 1992.
3] F. HAYES-ROTH, D.A.
WATERMAN, and D.B.
LENAT.
Building Expert Systems.
Addison Wesley, 1983.
4] N.C. HEINTZE, J. JAFFAR, S. MICHAYLOV, P.J.
STUCKEY, and R.H.C.
YAP.
The CLP(R) Programmer's Manual, 1992.
5] J. JAFFAR, S. MICHAYLOV, P.J.
STUCKEY, and R.H.C.
YAP.
The CLP(R) language and system.
ACM Transactions on Programming Languages and Systems, 1987.
6] R. KOWALSKI and M. SERGOT.
A logic-based calculus of events.
New Generation Computing, 4, 1986.
7] B. KUIPERS.
Qualitative simulation.
Articial Intelligence, 1986.
8] F. LACKINGER and W. NEJDL.
Diamon: A model-based troubleshooter based on qualitative reasoning.
IEEE Expert, 1993.
9] F. Ramparany and al.
Knowledge integration and interchange issues in imperfect and time dependent information systems.
In Proceedings of  the IJCAI'93 Knowledge Sharing and Information Interchange workshop, 1993.
10] S. M. SRIPADA.
A logical framework for temporal deductive database.
In Proceedings of the 14ht VLDB Conference, 1988.
A temporal structure that distinguishes between the past, present, and future Andre Trudel  Jodrey School of Computer Science Acadia University Wolfville, Nova Scotia, Canada, B0P 1X0  Abstract  We present a two dimensional temporal structure that has an ever changing present.
Relative to each present, there is a past and future.
The main representational advantage our two dimensional structure has over traditional linear temporal structures is the ability to record when knowledge is added or updated.
We dene a rst order logic that has this structure as its temporal ontology.
1 Introduction  Most temporal rst order logics in Articial Intelligence have a linear (i.e., non-branching) temporal ontology.
Examples of logics with a linear structure are those of Allen 1], Kowalski 4], and Shoham 6].
Even the logic of McDermott 5] uses linear time: Note that, contrary to what is often stated, McDermott's system does not use branching time: time itself is represented by the linear ordering of the real numbers branching only occurs with respect to the totality of possible states ordered by date.
(2], p. 1178) Linear time has its drawbacks.
There is no distinguised element in the ontology to represent the present.
Consequently, there is no concept of a past or future.
Another drawback is that a linear time based logic represents the current state of aairs.
There is no record of when knowledge is obtained or updated.
Humans do not view time as being linear.
Instead, we neatly compartmentalize time into the past, present, and future.
As the present changes, so does the past and future.
For example, we are continually learning things about our past and revising our future plans.
We present a two dimensional temporal structure that captures some of our intuitions about the past, present and future.
It has an ever changing present, and a past and future relative to each present.
We then formally dene a rst order logic that has this structure as its temporal ontology.
2 Proposed logic  Each predicate has two temporal arguments.
For example, red(1,1,house) and alive(5,10).
The two temporal arguments do not specify an interval.
For example, alive(5,10) is not used to represent the fact that alive is true over the interval (5,10).
Instead, the two temporal arguments are cartesian coordinates.
The relation alive(5,10) species that alive is true at the point (5,10) on the cartesian plane.
The temporal ontology consists of a cartesian plane.
The line y = x is used to represent the actual state of the world.
Relative to any point (p p) on the line y = x the line segment fy = x x > pg represents the actual future, fy = x x < pg represents the actual past, fy = p x > pg represents the expected future, and fy = p x < pg represents the perceived past (see gure 1).
What an agent observes or experiences at time p is recorded at the point (p p): Any plans or expectations the agent may have about the future at time p is recorded on the line fy = p x > pg: Similarly, any knowledge the agent learns or is given about the past at time p is recorded on the line fy = p x < pg: On the diagonal line y = x we record what actually happens in the world.
For example, in gure 2 the house is red at time 10 (i.e., red(10,10)).
At time 10, we plan to paint the house white at time 20 (i.e., white(20,10)).
But for some unforeseen reason, the house gets painted earlier at time 15 (i.e., white(15,15)).
We also know that at time 2, the house is white (i.e., white(2,2)).
At time 10, we learn that the house was blue at time 5 (i.e., blue(5,10)).
Note that blue(5,10) records two items of information.
The rst is that the house is blue at time 5, and the second is that this fact was recorded (learned) at time 10.
Formulas along a vertical line need not be consistent.
Figure 3 shows a situation where at time 10 we plan to go to the movies at time 15 (i.e., movies(15,10)).
But at time 15, something comes up that prevents us from going to the movies (i.e., not movies(15,15)).
Also, at time 5 we thought the house had been painted red at time 2 (i.e., red(2,5)).
We later learn at time 10 that the house was not red at time 2 (i.e., not red(2,10)).
The x and y axes of the cartesian plane must be linear and of the same type.
No further restrictions  6 y perceived past        past     x      future y=x   expected   (p,p)  future  x  -  Figure 1: The dierent pasts and futures relative to (p,p)  6  white(15,15)x   y blue(5,10)  x    x     x       x   red(10,10)   white(20,10)   white(2,2) x  -  Figure 2: Colors of a house over time  6  not movies(15,15)  y not red(2,10)  x  red(2,5)  x                    x      x  movies(15,10)  x Figure 3: Inconsistent information  -  19  6  15                       .h    .
.
nancing(19,(15,19)) .
.
.h .
.
.
.h  university((19,23),15) 19  23  -  Figure 4: Intervals are placed on the axes.
They can be discrete, dense, points, intervals, points-intervals, etc.
If intervals are allowed, they appear as one of the temporal parameters.
For example, in gure 4 a 15 year old student plans to attend university between the ages of 19 and 23 (i.e., university( (19,23), 15)).
Also, between the ages of 15 and 19 the student believes that nancing will be in place when entering university at age 19 (i.e., nancing(19, (15,19) )).
Although the examples in this paper only use the north-east corner of the cartesian plane, the whole plane can be used to represent information.
We conclude with an outline of the syntax and semantics for the proposed logic.
2.1 Syntax The logic has two disjoint sorts called temporal and non-temporal.
All terms are sorted.
Predicates have 2 temporal arguments followed by m  0 non-temporal arguments.
Terms and well formed formulas are dened in the standard fashion.
2.2 Semantics An interpretation is a tuple hT U i where T is a non-empty temporal universe, U is a non-empty nontemporal universe, and  is an interpretation function which maps each temporal constant to an element of T, each non-temporal constant to an element of U, each n-ary temporal function to an n-ary function from T n to T , each n-ary nontemporal function to an n-ary function from U n to U, and each (2 m)ary predicate to an (2 m)-ary predicate on T 2  U m .
Quantied variables range over the appropriate universe.
Well formed formulas are interpreted in the usual fashion.
3 Examples  3.1 Leave lights on  Information recorded on the line y = x may later be discovered to be false.
For example in gure 5, the driver of the car believes that he shut o the headlights when he left the car at time 5.
Upon returning to the car at time 20, he discovers the battery is dead.
He then checks the light switch and it is in the \on" position.
Therefore, the lights were not shut o at time 5.
3.2 Course  The proposed logic can be used to model an agent's changing expectations or beliefs over time.
For example, assume a course starts at time 5 and ends at the end of the term at time 25.
At the start of the course, the student believes he will pass (see gure 6).
At time 10, the student does very poorly on the rst assignment and thinks he will not pass the course.
The student does very well on the midterm at time 15 and now believes that he has a chance of passing.
But, the student does poorly on the second assignment at time 20 and once again believes he will fail.
The story has a happy ending.
The student aces the nal exam and passes the course.
3.3 Planning  Assume that at time 5, an agent constructs a plan to enter a room.
The plan consists of going to the door over the interval (5,10) (i.e., gtd( (5,10), 5)), opening the door over the interval (10,15) (i.e., od( (10,15), 5)), and then entering the room over the interval (15,20) (i.e., er( (15,20), 5)).
Note that we represent the plan along with the time that it was constructed.
The plan is shown in gure 7.
Over the interval (5,10), the agent excutes the rst action of the plan which is to go to the door.
Once at the  6 y    not lights-o(5,20)  x          x lights-o(5,5)          x  battery(20,20,dead)  x  -  Figure 5: Lights  6 y   nal(25,25) assgn2(20,20) x  midterm(15,15) x assgn1(10,10)  x begin(5,5) x             x pass(25,25) x x x x  not pass(25,20) pass(25,15) not pass(25,10) pass(25,5) x  -  Figure 6: Expectations of passing the course changes over time      6 10 5  .
gtd((5,10),(5,10)) .
.
.
   x door-locked(10,10) .  .x .
.
.
.
x .
.
.
.
x. .
.
.
.x  gtd((5,10),5) od((10,15),5) er((15,20),5)   -  5  10  15  Figure 7: Remembering a plan and re-planning  20  door, the agent observes that the door is locked which is unexpected.
The agent cannot execute the next action which is to open the door.
At this point, the agent must construct another plan which would be stored on the line fy = 10 x > 10g: The old plan constructed at time 5 remains untouched.
It can be used as a guide while re-planning at time 10.
It can also be used to answer queries.
For example, if we ask the agent why he is at the door at time 10 without a key, the agent can examine the old plan and reply that he expected the door to be unlocked at time 10.
3.4 Multi-agents  The proposed temporal structure is two dimensional.
Additional dimensions can be added to the structure to represent and reason about multi-agent problems.
The addition of a third temporal parameter (i.e., (x y z)) allows us to represent individual knowledge of an agent and common knowledge.
Each agent is assigned a plane.
Information about the n'th agent is stored on the plane (x y n): Information that is common to all agents is stored on the plane (x y 0): For example, assume there are three agents, and all three know that block A is on block B at time 5: on(5,5,0, A,B).
Agent 1 also knows that A is on B at time 6: on(6,6,1, A,B).
At time 10, agent 2 plans to move block C on top of A over the interval (15,20): move((15,20),10,2, C,A).
Agent 3 knows that block A is red at time 7: red(7,7,3, A).
We could also have the situation where all three agents know a fact, but don't realize it is common knowledge (i.e., not contained on the 0'th plane).
For example, each agent has local knowledge that block B is blue at time 10: blue(10,10,1, B), blue(10,10,2, B), blue(10,10,3, B).
Each agent does not know that the other 2 agents also have the information that B is blue at time 10.
Instead of assigning a plane to each agent, we can add a fourth temporal parameter to the structure and assign a cube to each agent.
In agent i's cube (i.e., (x y z i)), information agent i has about agent n is stored on the n'th plane (i.e., (x y n i)), and i's personal information is stored on plane i (i.e., (x y i i)).
For example, agent 1 knows that block B is blue at time 10, and also believes that agent 2 has this information: blue(10,10,1,1, B), blue(10,10,2,1, B).
Information common to all agents is stored on the plane (x y 0 0): A fth dimension can be used to represent groups of agents.
Each group consists of one or more agents.
Information about group n is stored using (x y z a n): The rst four parameters are used to store information about a particular agent in group n: For example, information about the third agent in group 2 is stored in (x y z 3 2):  Other dimensions can be added as needed.
4 Persistence  If the house is blue at time 10, is it also blue at time 15?
Given no knowledge of the house changing color, it seems reasonable to assume that the color of the house persists from time 10 to 15, and we conclude the house is blue at time 15.
This is called the persistence problem.
Traditional linear temporal structures only need to deal with persistence along a single axis.
Here, we must consider two dimensional persistence.
In gure 8, the house is blue at time 10 (i.e., blue(10,10)).
As discussed above, persistence should be allowed into the future (i.e., along the line fy = 10 x > 10g).
Using a similar argument, persistence into the past should also be allowed (i.e., along the line fy = 10 x < 10g).
For example, if the house is blue at time 10, it was probably also blue at time 9.
We also need persistence in the upward direction (i.e., along the line fy > 10 x = 10g).
For example, at the point (11,11), we should remember that the house was blue at time 10 (i.e., blue(10,11)).
Upward persistence models the agent's memory.
We do not allow persistence in the downward direction.
The relation blue(10,10) also records the fact that the color of the house was learnt at time 10.
Therefore at time 9, we have no informationabout the color of the house (i.e., the truth value of blue(10,9) is unknown).
To summarize, we have horizontal bi-directional persistence and vertical upward persistence.
Persistence is not allowed in the vertical downward direction.
In either of the three directions where persistence is allowed, standard algorithms can be used.
Problems arise when vertical and horizontal persistence are inconsistent.
For example in gure 9, at time 20 we know the house was not blue at time 5, and at time 15 we know the house was blue at time 10.
At time 20, was the house blue at time 10 (i.e., is blue(10,20) true)?
Using horizontal persistence and not blue(5,20) we can conclude not blue(10,20).
We can also conclude the opposite using vertical persistence and blue(10,15).
Which answer do we prefer?
The preference between vertical and horizontal persistence depends on the particular situation.
In this case, either answer is reasonable.
In the future, we will investigate algorithms for resolving persistence conicts.
5 Conclusions  We presented a general rst order logic that has a unique two dimensional temporal structure.
The structure consists of a cartesian plane.
The present moves along the line y = x: At any point on the line y = x we can record plans or expectations about the future, and information about the past or present.
The proposed temporal structure has the appearance of being a branching one.
But, it is not.
Time  yes  6  6  y yes fi  x  - yes  no x  -  Figure 8: The persistence of blue(10,10)  6 y  not blue(5,20)  x  - x(10,20) 6 x  blue(10,15) x Figure 9: Vertical and horizontal persistence are inconsistent  -  moves along the single line y = x: The branches emanating from each point on the line y = x are used to store information about the past or future obtained at that point in time.
The main representational advantage our two dimensional structure has over traditional linear temporal structures is the ability to record when knowledge is added or updated.
For example, simple English sentences like \Last night I planned to go to the movies tonight, but now I don't feel like going" cannot be represented using a linear structure.
A linear structure can either represent the fact that the person is going to the movies or not.
It cannot represent the fact that going to the movies tonight was true yesterday and false today.
The sentence is easily represented in the proposed logic: movies(tonight yesterday) ^ not movies(tonight tonight): Instead of using the proposed logic, it is possible to extend the syntax and semantics of traditional linear time logics so that they use a two dimensional structure.
For example, RGCH 3] uses the real numbers.
We can easily add another temporal argument to the logic.
Acknowledgements  Thanks to Denis Gagne for discussing the material contained in this paper.
Research supported by Natural Sciences and Engineering Research Council of Canada grant OGP0046773.
References  1] J.F.
Allen.
Towards a general theory of action and time.
Articial Intelligence, 23(2):123{154, 1984.
2] A. Galton.
Reied temporal theories and how to unreify them.
In 12th International Joint Conference on Articial Intelligence, pages 1177{1182, Sydney, Australia, 1991.
3] S.D.
Goodwin, E. Neufeld, and A. Trudel.
Temporal reasoning with real valued functions.
In  Pacic Rim International Conference on Articial Intelligence (PRICAI'92), pages 1266{1271,  Seoul, Korea, Sept 1992.
4] R.A. Kowalski and M. Sergot.
A logic-based calculus of events.
New Generation Computing, 4:67{95, 1986.
5] D.V.
McDermott.
A temporal logic for reasoning about processes and plans.
Cognitive Science, 6:101{155, 1982.
6] Y. Shoham.
Temporal logics in AI: Semantical and ontological considerations.
Articial Intelligence, 33:89{104, 1987.

Propagating Possibilistic Temporal Constraints Rasiah Loganantharaj  1 Introduction  Automated Reasoning laboratory The Center for Advanced Computer Studies University of SouthWestern Louisiana Lafayette, LA - 70504  The notion of time plays an important role in any intelligent activities.
Time is represented either implicitly or explicitly.
We are interested in explicit representation of time.
The popular approaches for such representation are based on points or intervals or a hybrid of both.
Propositional temporal assertions are represented as relations among the points, or among the intervals.
The indenite information among either the points or the intervals are represented as disjunctions.
In real world, information is often incomplete, imprecise, uncertain and approximate.
Temporal knowledge is not an exception to this reality.
For example, consider the following information: John often drinks coee during his breakfast.
Sometimes, he drinks his coee before the breakfast and drinks orange juice during his breakfast.
There were few occasions he drank water during his breakfast and drank coee after the breakfast.
Mike talked to John over the phone while John was having coee.
Suppose, we are interested in nding out how Mik's telephone conversation was related to John's breakfast.
From the given information, we have denite relation between the telephone call and John's coee, but there is no information about the relationship between the coee and the breakfast on that particular day.
In the absence of such information, we can use John's habitual pattern to infer plausible relations.
Suppose, Ic and Ib respectively represent the interval over which John was having coee, and John was having breakfast.
Let It represents the interval over which Mike was having telephone conversation with John.
In interval logic, the information is represented as It is during Ic , and Ic is before or during or after Ib .
In such representation, the disjunctive relations do not provide any clue about which relation is highly probable than the others.
Instead, the representation may let us believe that all the relations of a disjunction have equal probability, for example, having coee before, during or after breakfast has equal probability.
This is not what the original information tells us.
Based on John's habit, having coee during breakfast is much more probable than having coee either before or after the breakfast.
This issues have not been studied in temporal reasoning.
In  this paper we will provide a representation to specify uncertain information and to propagate them over temporal constraint network.
This paper is organized as the following.
We introduce interval-based logic in Section 2.
In Section 3, we describe the representation of uncertain temporal knowledge and its propagation.
This paper is concluded by a summary and discussion in section 4.
2 Background on Interval Based System Allen 1] has proposed an interval logic that uses time intervals as primitives.
In this logic, the following seven relations and their inverses are dened to express the temporal relations between two intervals: before(after), meets(metby), overlaps(overlapped-by), starts(started-by), during(contains), ends(ended-by), and equals.
Here, the inverse relations are indicated within parentheses.
Since the inverse of equal is same as itself, there are, in fact, only thirteen relations.
Temporal inferencing is performed by manipulating the network corresponding to the intervals.
Each interval maps onto a node of a network called temporal constraint network (TCN).
A temporal relation, say R, from an interval, say Ii , to another interval, say Ij , is indicated by the label Rij on the directed arc from Ii to Ij .
Obviously, the label Rji of the directed arc from Ij to Ii is the inverse relation of Rij .
If we have denite information about the relation Ii to Ij then Rij will be a primitive interval relation, otherwise it will be disjunctions of two or more primitive interval relations.
Suppose the relation Ii to Ij (Rij ) and the relation Ij to Ik (Rjk ) are given.
The relation between Ii and Ik , constrained by Rij and Rjk , is given by composing Rij and Rjk .
In general, Rij and Rjk can be disjunctions of primitive relations, they are represented as: 1  r2  : : : rn g Rij = frij1  rij2  : : : rijm g and Rjk = frjk jk jk where rij is one of the primitive relations dened in the system.
The interval Ii is related to the interval Ik by the temporal relation given by the following expression:  Rij  Rjk =  (  p) (rijl  rjk  )  l=1:::m p=1:::n p l where (rij  rjk) is a composition (transitive relap , and is obtained from the entry tion) of rijl and rjk j of the transitivity table 1] at the riji row and the rjk  column.
Alternatively this could be written as Rij  Rjk = fT(rij  rjk)jrij 2 Rij ^ rjk 2 Rjkg where T(rij  rjk) is the value of Allen's look up composition table of row rij and column rjk .
Temporal constraints are propagated to the rest of the network to obtain the minimal temporal network in which each label between a pair of intervals is minimal with respect to the given constraints.
Vilain et al.
7] have shown that the problem of obtaining minimal labels for an interval-based temporal constraint network is NP-complete.
Approximation algorithms, however, are available for temporal constraint propagation.
Allen proposed an approximate algorithm that has an asymptotic time complexity of O(N 3 ) where N is the number of intervals.
His algorithm is an approximate one in the sense that it is not guaranteed to obtain the minimum relations, but it always nd the superset of the minimal label.
Since any set is a super set of a null set it is not very comforting because it is possible that global inconsistency may be hiding under 3-consistency.
3 Representation of Uncertainty  The problem of uncertainty is not new to AI problem solving.
In many expert system applications, uncertainty have been studied under approximate reasoning.
Mycin system 6, 2] has used certainty factor whose value varies from -1 to 1 through 0 to represent the condence on an evidence or on a rule.
The value 1 indicates the assertion is true while the value -1 indicates the evidence is false.
0 indicates no opinion on the evidence.
The other values correspond to some mapping of the belief on the evidence onto the scale of -1 to 1.
Prospector model 4, 5] uses probabilistic theory with Bayes' theorem and other approximation techniques to propagate evidences over causal network.
Fuzzy logic 8] has also been used in expert systems to capture knowledge with fuzzy quantiers such as `very much', `somewhat' etc.
Other techniques have also been used to handle uncertainty in expert systems.
Let us look back at the same example presented in the introduction of this paper.
John often drinks coee during his breakfast.
Sometimes he drinks his coee before the breakfast and drinks orange juice during his breakfast.
There were few occasions he drank water during his breakfast and drank coee after the breakfast.
The statement has fuzzy quanties indicating that the frequency of John having coee during his breakfast is much higher than he is having coee either before or after his breakfast.
We should capture the fuzzy quantiers into probabilistic measures in our temporal constraint representation such that the summation of the probabilities of the relations between a pair of intervals is equal to 1.
Let us represent this idea more formally.
The relation Rij , the relation Ii to Ij , is represented as frij1 (w1) rij2 (w2) ::: rijm(wm )g where rijl (wl ) is a primitive relation with its probability or the relative weight of wl .
Since each primitive relation between a pair of intervals is associated with a weight to represent the probability or the relative strength, the summation of the weights must be equal to 1 which we P call probabilistic condition for the weights.
That is, mi=1 wi = 1.
The boundary value of the weight 1 indicates that the relation is true while the value 0 indicates that the relation is false.
Further, the inverse relation of Rij will be the inverse of all the primitive relations of Rij with the same weights.
In the presence of new evidences, the probability values of the relations are modied to take account of the new evidences.
That is, when the relations between a pair of intervals are modied or rened because of other constraining relations, the probability or the weights of the relations are adjusted to satisfy the probabilistic condition.
This process is called normalization.
Let us explain this with an example: Suppose (1) R12 = fb(0:6) O(0:3) d(0:1)g, (2) R13  R32 = fb(0:4) o(0:5) m(0:1)g. The relation R12 is rened to fb(w1) o(w2)g. The weights w1 and w2 are computed using the intersection operation and then the weights are normalized such that w1 + w2 = 1.
When propagating constraints with probabilistic weights, we may need to dene such as union, intersection, composition and normalization operations which are used in propagation.
0  0  Union operation:  Rij p Rij = fr(w)jr(wij ) 2 Ri ^ r(wij ) 2 Rj ^ w = max(wij  wij )g 0  0  0  Intersection Operation:  Rij \p Rij = fr(w)jr(wij ) 2 Ri ^ r(wij ) 2 Rj ^ w = min(wij  wij )g 0  0  0  Composition Operation: Rij  Rjk =  l (wl ) 2 Rij ^ rm (wm ) 2 Rjk fp r(w)jrij jk l  rm ) ^ w = min(wl  wm )g ^ r = T(rij jk  Normalization operation: Suppose the label Rij takes the following form after renement 1 (w ) r2 (w ) ::: rm(w )g. Let w = Pm w .
frij 1 ij 2 ij m i=1 i After normalization operation the label becomes frij1 (w1) rij2 (w2) :::Prijm(wm )g where wi = wi=w which ensures that mi=1 wi = 1 In this approach we use possibilistic ways of combin0  0  0  0  0  ing the constraints as has been used in many expert systems under uncertainty.
3.1 Temporal Propagation  A temporal constraint network (TCN) is constructed from the given temporal assertion such that each node of the constraint network represents each interval of the temporal assertions.
The labels on each arc of a TCN corresponds to the relations between the corresponding intervals.
Further the summation of the weights of each component in each arc should be equal to 1.
If no constraint is specied between a pair of intervals it will take a universal constraint1 as label in the TCN and it will not be used for the purpose of propagation.
When propagating a constraint, other label of the network may get updated to a subset of its label.
The process of updating of a label as a result of propagating a constraint is called label renement.
The label renement takes the following forms: (1) The weights of the labels of an arc get changed, or (2) the primitive relations of a label get reduced.
In either case normalization operation is performed to ensure the summation of the weights adds to 1.
Suppose we are propagating the label Rij of the arc < I J > to the arc < I K > of the triangle IJK.
Let the labels of the arcs < J K > and < I K > are respectively Rjk and Rik.
The new label of the arc < I K > is computed as Rik \ fRij  Rjkg.
When the new relation is not null the weights on the label are normalized.
3.2 Temporal Constraints Propagation Algorithm for uncertain constraints  This is an extension of Allen's propagation algorithm.
The algorithm uses a rst in rst out (FIFO) queue to maintain the constraints that need to be propagated.
Initially all the pairs of constrained intervals are placed into the queue.
The propagation of constraints is initiated by removing an arc, say < Ii  Ij >, from the queue and checking whether the relation between Ii to Ij can constrain the relations on all the arcs incident to either Ii or Ij except the arc < Ii Ij >.
When a new relation is constrained, that is, the old label is modied, the arc (the pair of intervals related by this relation) is placed in the queue.
The main propagation algorithm is described in Figure 1.
In this algorithm, we use the notation Rij to denote the label of the arc i to j.
When we omit the weights on the labels and use the union, the intersection operations of set, and the composition operation of temporal logic, the algorithm becomes identical to the one of Allen's 1].
One may expect the asymptotic complexity to be O(N 3 ) where N is the number of nodes of the TCN.
Intuitively one may make a conclusion that this algorithm 1 a universal constraint is the weakest constraint and thus it is a disjunctions of all the primitive relations of the time model  will also converge in O(N 3) time.
This may be misleading because we have not yet considered the instability eect of the algorithm due to it normalization operation.
An arc is placed back in the queue when either the number of primitive relations of the label is reduced or the weight of the primitive relation of the label is modied even when the label remains unchanged.
An arc may be placed in the queue at most 12 times as a result of the disjunctive relations being rened one at a time till it becomes a singleton relation.
On the other hand, the number of times an arc is placed in the queue due to the change of weight depends on the following parameters: (1) the resolution of the weights (the number of decimal places that counts) and (2) the error bound we are prepared to tolerate.
A complete study on these issues can be found in one of our report 3].
Let us consider an example.
The probability of `having coee' before breakfast is 0.15, during breakfast is 0.8 and after breakfast is 0.05.
The probability of `having a coee' overlapping `reading morning newspaper' is 0.8 and `having coee' meets `reading the newspaper' is 0.2.
Suppose we want to nd the relation between having breakfast and reading newspaper.
Let us propagate the constraints and nd out how the labels gets rened.
Suppose Ib  Ic  Ir respectively represent the intervals of having breakfast, coffee and reading newspaper.
Using the notations dened in this paper we can dene the labels of the initial TCN as following.
Initial TCN Rcb = fb(0:15) d(0:8) a(0:05)g Rcr = fo(0:8) m(0:2)g Rbr = to be computed TCN after propagating Rbc to < Ib  Ir > Rcb = fb(0:15) d(0:8) a(0:05)g Rcr = fo(0:8) m(0:2)g Rbr = fo(:8) oi(:15) d(:15)di(:8)f(:15) fi(:8) b(:05) a(:15)g After normalizing Rbr Rbr = fo(:26) oi(:049) d(:049) di(:26)f(:049) fi(:26) b(:024) a(:049)g This is also the 3-consistent TCN labels.
4 Summary and Discussion  In many real world applications we are faced with the information that is incomplete, indenite, imprecise and uncertain.
When explicit time was used for temporal reasoning, indenite information is accommodated as disjunctions.
For example, drinking coffee is either before, during or after the breakfast.
The disjunctive information implicitly assume equal probability of occurrence even though exactly one can be true between a pair of intervals (points).
In such representation we fail to distinguish or dierentiate the highly probable one from the remotely possible one.
According to our example, having coee during breakfast is highly probable than having coee before  procedure propagate1() 1 While queue is not empty Do 2  f  3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  g  get next < i j > from the queue /* propagation begins here */ For ( ( k 2 set of intervals ) ^k 6= i ^ k 6= j )Do f temp 	 Rik \p (Rij  Rjk) If temp is null Then signal contradiction and Exit Normalize temp If Rik 6= temp Then f place < i k > on queue Rik 	 temp g temp 	 Rkj \p (Rki  Rij ) If temp is null Then signal contradiction and Exit Normalize temp If Rkj 6= temp Then f place < k j > on queue Rkj 	 temp g  g  Figure 1: propagation algorithm or after a breakfast.
In this paper we have proposed a formalism to represent temporal constraints with the associated probabilistic weights and use them to propagate to the rest of the network to obtain 3-consistency.
We have extended Allen's algorithm to handle probabilistic relations among intervals.
The operations we have dened for the labels with weights are applicable to both the points and the intervals.
Therefore, our formalism will be applicable to both the points and the intervals.
Acknowledgement  This research was partially supported by a grant from Louisiana Education Quality Support Fund (LEQSF).
References  1] J. F. Allen.
Maintaining knowledge about temporal intervals.
Communications of ACM, 26(11):832{843, 1983.
2] B. G. Buchanan and E. H. Shortlie Eds.
RuleBased Expert Systems.
Addision-Wesley, 1984.
3] R. Loganantharaj.
Complexity issues of possibilistic temporal reasoning.
preperation, 1994.
4] J. Gaschnig R. O. Duda and P. E. Hart.
Model design in the prospector consultant system for mineral exploration.
In D. Michie, editor, Expert Systems in the Microelectronics Age.
Edinburgh University Press, 1979.
5] P. E. Hart R. O. Duda and N. J. Nilsson.
Subjective bayesian methods for rule-based inference  systems.
In Proceedings of the AFIPS National Computer Conference, volume 45, 1976.
6] E. H. Shortlie.
Computer Based Medical Consultations:MYCIN.
Elsvier, 1976.
7] M. Vilain and H. Kautz.
Constraint propagation algorithm for temporal reasoning.
In Proceedings of AAAI-86, pages 377{382, 1986.
8] L. A. Zadeh.
Making computers think like people.
IEEE Spectrum, pages 26{32, August 1984.
On The Representation of Temporal Object Roles in Object Oriented Databases Niki Pissinou  Kia Makki  The Center For Advanced Computer Studies Department of Computer Science The University of Southwestern Louisiana University of Nevada Lafayette, Louisiana 70504 Las Vegas, Nevada 89154  Abstract  This paper outlines a generic, core temporal object model that provides support for the modeling of temporal object roles.
This model draws from notions introduced in some of our previous works on temporal object modeling by providing a specic database model that provides a conceptual modeling context for temporal object roles 3, 4, 5, 6, 9, 7, 2, 8, 10, 11].
The model achieves an integration of the abstract concepts that characterize temporal objects.
The main purpose of the model is to provide a basic framework for temporal object information models and information systems.
The model is based on a small number of simple \temporal" constructs and primitive \temporal" operations.
The simple set of modeling constructs of this model are temporal objects and temporal mappings.
A set of primitive operations is dened on temporal objects that allows uniform viewing, denition, insertion and manipulation of objects in temporal object databases.
A simple set of temporal constraints is also provided.
1 Introduction  One of the problems faced by existing data models is their weak expressiveness of the representation of temporal information and in particular the representation of the temporal roles of objects.
In consequence, typical database systems that embody these models, support the storage and retrieval of \facts" about the world and certain aggregation and operations on them, but do not perform operations on either the perception to map situations in the world into the database contents, temporal object roles or temporal operations.
In view of this, a strong interest of ours is in developing a database model that represents \realistic worlds."
To achieve this, in our previous works we  have studied the semantics of time in the context of object database systems, identied changes to existing notions of temporal data that are necessary because of the transition from the relational to the object model and specied an approach to extending an archetyped, extensible object model, to incorporate a number of generic modeling concepts, with concrete denitions, underlying temporal objects 3, 4, 5, 6, 9, 7, 2, 8, 10, 11].
The work is in line with the infrastructure presented in 11].
A major aim of this research, is to achieve an integration of the abstract concepts that were assumed to characterize temporal data, presented in our earlier works.
In order to achieve this goal, in this paper we map these abstract concepts to a simple object data model, thus articulating them as concrete concepts 9, 10, 7].
This provides a specic context for our approach to temporal object database modeling, and allows us to dene a conceptual framework for a simple and generic object model.
The model addresses temporal issues at the nest level of data granularity, viz.
the object level.
Our work presents the foundations towards the synthesis of a powerful integrated object data model that supports the temporal/dynamic aspects of data modeling in addition to the structural and behavioral ones.
Our model is intended to be both a valuable temporal model in its own right, as well as a mechanism for describing and analyzing temporal extensions to other database models as illustrated in 3, 5, 4].
This can be mainly achieved by using our model's small number of simple constructs and primitive operations, that can be used as the basis for the specication and stepwise development of temporal information models and systems of increasing complexity and levels of abstraction.
The remainder of this paper is organized as follows: In the next section we outline some preliminary concepts and denitions.
In section 3, we describe the modeling elements that encompass our temporal object model.
Section 4 briey describes the temporal object denition and manipulation language while section 5 denes a simple set of temporal constraints.
In the nal section we provide some nal remarks.
2 Preliminary Definitions To describe the structure of an object as it evolves over time and across its multiple representations (i.e., versions and history) we introduce the notions of universal object identity and possible object world.
Together these two notions allow us to describe the meaning, characteristics, properties, behavior and role of each individual object at dierent times and at a particular point in time.
Denition.
The Universal Object Identity of an object, denoted as UOI, refers to an object's interpretation or sense and remains time invariant.
Denition.
The Possible Object World of an object, denoted as POW, refers to an object's denotation and is usually time variant.
An object cannot have a possible object world unless its universal object identity is dened.
An object can have dierent possible object worlds at dierent times, and also several possible object worlds at a particular time, but its universal object identity remains unique over a specied lifespan.
For example, the object \ght" could be a physical struggle in 1989 followed by an emotional struggle in 1990.
Also the object \ght" can concurrently be a physical and an emotional struggle.
In general, a universal object identity encapsulates the time invariant semantic meaning of an object, while a possible object world encapsulates the static and dynamic structure, behavior and role of each individual object at dierent temporal intervals.
Further, the UOI of an object can be derived from all its possible POWs at all different times with respect to our model and the world we are modeling by taking the union of possible POWs over time.
Thus, we can formally dene the UOI of an object from all its possible POWs at all different times with respect to POWs, our database model and the world we are mod-  eling, as follows: UOI (M)=def ft !
POW (M t)jt 2 T g where fi UOI is the universal object identity of an object  fi POW is the possible object world of an object  fi M is our database model fi t 2 T is the time a possible object world is active (current) The above denition assumes that users interact with only a single world during a time period.
When users interact with dierent worlds then the UOI of an object is dened as follows: UOI (M) =def f< w t >!
POW (M w t) j w 2 W and t 2 T g where fi UOI , POW and M are as dened previously fi w 2 W is the \mini-world" we are modeling (may be assumed to be a version) Given these two new notions of an object, we are now able to give the denitions of a universal object identity and possible object world class and time slice.
These are as follows: Denition.
A Universal Object Identity Class, related to a universal object identity UOI, is dened as all objects that can potentially, independent of time, have the same universal object identity.
Denition.
A Possible Object World Class, related to a possible object world POW, is dened as a collection of (POW,ti) s.t.
t = 1 2 ::::n tuples, of all objects that can potentially have the same possible object world.
Since an object can have several POWs at a given time, an object can belong to several possible object world classes.
From our formal denition of a UOI of an object we can conclude that an object can be a member of only a single Universal Object Identity Class but belong to several Possible Object World Classes during a particular lifespan.
Consequently, an object is not active if it does not belong to some particular Universal Object Identity Class at any given time t. If an object is not active it can not belong to any Possible Object World Classes.
In other words, an object can not be a member of a Possible  Object World Class if it were not a member of a Universal Object Identity Class.
Denition.
A Time Slice of a possible object world class is all objects with the same possible object world at time ti.
It can be viewed as a set of instances of a possible object world class at time ti .
Thus the union of all time slices of a possible object world class is the possible object world class.
Denition.
A Universal Object Existence is a function F : (UOI t) !
f0 1g.
A value of zero at time t implies that the universal object identity of an object does not exist at that time or is undened.
Denition.
A Possible Object World Existence is a function F : (POW,t) !
f0,1g.
A value of zero at time t implies that the POW of an object does not exist at that time or is undened.
One major requirement for the kind of temporal object model described here, is to have a general and convenient means for extracting the lifespan of an object and the history of that object.
For this reason, we introduce the notions of time-priority sequence and object history.
In the following denitions, we specically adopt modied notions of a time sequence introduced in 12].
In these denitions we also incorporate the notion of priorities for POWs of an object.
The idea of assigning priorities to possible object worlds is particularly useful in resolving possible conicts among co-existing possible object worlds for a particular object and in the metadata.
For example, given that an object has two possible object worlds teaching assistant and research assistant then by assigning priorities to these two POWs we can determine the object's primary role at any given time ti .
Denition.
The Time Sequence of a POW of an object, denoted as TSEQ(POW) is dened as (POWt1 , ..... , POWt ) = (POW, (t1  t2 :::::  tn )).
Thus the lifespan of a POW of an object can be easily determined since t1 denotes the valid from time and tn denotes the valid to time.
Denition.
The Time-Priority Sequence of a POW of an object, denoted as TPSEQ(POW) is dened as n  (POW(t1p ) , ..... , POW(t p ) ) = (POW, (t1 pi ), (t2  pj ), ..... , (tn  pk )).
Denition.
An Object History refers to the collection of time sequences TSEQ(POW) of all the POWs of an object.
Thus the lifespan of an object can be derived from the lifespans of all the POWs of that object.
There is a partial function Flpow that determines the lifespan of a POW of an object.
The domain of Flpow is POW = fPOW1 P OW2 :::: POWng, the set of all possible object worlds of an object, and its range consists of nite subsets of T = ft1 ::: tng.
i  n  k  3 Modeling Constructs  Given the temporal semantics introduced in our previous works on temporal object modeling, we can now illustrate our proposed approach to the modeling constructs appropriate for the design and implementation of a generic and core temporal object database model.
In this section we show how objects with several possible object worlds are dened, and develop the necessary mappings that allow a transition from one possible object world to another to occur over time.
Much in the spirit of several other data models, the temporal object model is based entirely on the notions of objects and inter-object relationships.
A temporal relationship among object cannot exist unless all the components are existing database objects, or relationship is proactive such as \James is going to be promoted to full professor as of next month" (given that in the second case proactive changes are not allowed.)
In addition since temporal databases need to retrieve and manipulate historical data, an object is not removed from a database unless there was an error.
Instead the object and all the relationships in the database in which it participates are \virtually deleted", which is a form of archiving the object.
Formally, a temporal object database system can be thought of as consisting of temporal objects and temporal relations.
Each temporal object in the database consists of a single universal object identity and a set of possible object worlds, as dened shortly, and corresponds to a temporal relation on the set of all database temporal objects.
Let (t  t  t ) be a relation.
For each temporal relationship, i  j  k  (t  t  t ), t is a temporal relation.
In particular, t = f ( t ,  t ) j the temporal relationship ( t  t   t )existsg.
If proactive relationships are disallowed then the constraint, ti < tj and tk < tj is explicitly imposed on the temporal relationship.
In the following section we introduce our denition of an object.
This denition draws heavily from the notions introduced in 3, 4, 5, 6, 9, 7, 2, 8, 10, 11].
i  j  k  j  j  0  0  i  0  i  j  k 0  k  3.1 Temporal Objects  The restrictions that some object models, impose on an object are too strong in the face of the desire to model the role of temporal objects.
For example, in may such models an object is restricted to having only a globally time invariant object definition generated by the system.
In an eort to model the \real-world" more realistically, we relax these restrictions by dening two new concepts: the universal object identity of an object that refers to an object's interpretation or sense and remains time invariant, and the possible object world of an object that refers to an object's denotation or reference.
An object only has a single universal object identity but can have several possible object worlds at a given time or at dierent times during its lifespan.
Each temporal object is a tuple consisting of a universal object identity and a sequence of possible POWs i.e.
object = (UOI, (POW ( 1 ) , ..... , POW!
( ) )) where UOI is the universal object identity of the object, which is time invariant and POW ( 1 ) , ..... , P OW!
( ) is a sequence consisting of the possible object worlds of the object and their corresponding priorities at some specic time.
In our model an object id (oid) has no intrinsic meaning.
Instead it is an entry point (reference or handle) for accessing information about a temporal object such as an object's UOI and POWs.
It \derives" its meaning from its associated universal object identity and from its possible POWs.
With each universal object identity we associate a universal object identity id, and with each possible object world POW , we associate a possible object world id, a time priority sequence TPSEQ, a set of temporal constraints, and a set of operations.
t pi  tn pk  t pi  tn pk  Our model also allows the creation and manipulation of Temporal Composite Objects (TCO.)
A temporal composite object consists of a group of temporal objects.
It exists at time T if each of its components exist at time ti i.e.
TCOT  (O1 1 + O2 2 + .
.
.
+ On ) where T > ti s.t i = 1 2 ::::,  denotes the temporal compose operator, O denotes the temporal object or an atomic temporal object, and + denotes the temporal aggregation operator.
A temporal composite object has a possible object world if the possible object world or reference of the whole object can be described as a function of the possible object worlds or the references of its parts.
Thus, for a composite object to exist at tj , the universal object identity and possible object world existence of each of its components should be 1 at ti such that tj > ti.
To more accurately represent temporal data and the evolution of temporal objects, it is desirable to model dierent time dimensions such as valid, transaction and user-dened times.
For simplicity in representing this information and in developing an initial experimental prototype we consider only valid time, but the other two kinds of time can easily be incorporated into the model.
In our model we treat time as dened in 11].
t  t  tn  4 Temporal Object Definition and Manipulation  The data denition and manipulation language is described as a set of primitive operations that are embedded within a host programminglanguage.
The host language supports the data types of temporal objects and set of objects, the usual set operations, a looping construct to iterate on elements of sets, a counter for elements of sets, and a data type for time.
Programs written in the host language can reference temporal objects.
The purpose here is not to propose a specic approach to host language embedding of data manipulation operations, but rather to dene a set of primitive building blocks for highlevel database systems.
In this section we briey present our temporal operations 1. and address some constraint issues.
1 The operations presented here are by no means exhaustive we are only presenting a fiavor of the temporal operations supported by our approach.
In our model all operations can be specied as either messages or processes.
A process provides inferencing capabilities such as creating messages from other given messages.
It consists of a set of preconditions and a set of actions.
In addition to these processes there are other implied ones that are initiated automatically as a result of some specied relationship.
We have used these concepts to develop a set of specialized temporal operations on objects and mappings.
In the following we describe a sample subset of such operations.
4.1 Create Operations  The set of Create Operations supports the creation, activation, deactivation, and context release of objects.
In particular, there are three primitive operations associated with the POW of an object.
These are create-pow, deactivate, and activate.
The possible object world of an object may be created, activated or deactivated using the following messages: 1. createpow(oid,(ti pk),powid).
This operation creates a new POW for an object.
ti is the object's time of creation and pk is the initial priority assigned to that object.
Here powid is a possible object world identier.
This new POW may not be created at tj unless its associated UOI exists at some time ti such that ti < tj .
(An UOI for an object may be created only once during its lifetime.)
Initially, no object is bound to a POW.
2. deactivate(oid,(ti pk ),tj ,powid).
The deactivate operation suspends the POW of an object at time ti until tj  If tj is set to 1 then the object is suspended indenitely.
In our temporal object model an object is never removed from the database.
It is either suspended indenitely or it is archived through the archive operation, which is similar to a \virtual" delete.
3. activate(oid,(ti pk ),powid).
The activate operation resumes the POW of an object at time ti where ti is greater than the time of its suspension.
Before we activate (and even create) a new POW for an object we need to verify that it is not currently active.
This is done through the exist-pow operation which is described later.
4. release(oid,(ti pk ),powid).
The release operation releases a specied POW for a specied time from being bound to a specied object.
If the POW is not bound to the object the operation has no eect.
5. error delete(oid,(ti pk ),powid).
operaThe error delete tion deletes the POW of an object.
Since in our model we only allow virtual deletes, this operation is meant to be used where there is a transaction error which is of no use and has no historical value e.g., a user inputs the wrong time.
Thus, the sole purpose of this function is to delete erroneous information.
6. create fun(oid,(ti pk ),powid,ag).
The create fun operation creates one of the four mapping functions discussed in 3].
These mapping functions are: POW-to-POW, POWto-UOI, UOI-to-POW and UOI-toUOI.
The type of transition is denoted through the ag.
Also, the function create map creates the actual relation.
4.2 Attach and Exist Operations  The set of Attach and Exist Operations is used for providing context to universal object identities of objects and temporal contexts to possible object worlds of objects.
Before we can create the POW of an object, we need to verify that it has an associated universal object identity.
If it does not have a UOI then the appropriate universal object identity is attached to it.
This sequence of events is captured through the following operations.
1. exist-uoi(oid,ti,uoid).
The existuoi operation veries that an object's universal object identity is active at ti .
If so, it returns true otherwise it returns false.
2. attach-uoi(oid,meaning).
If an object's universal object identity does not exist at ti i.e.
if the specied UOI is already bound to the specied object, it is created through the attach-uoi operation.
3. exist-pow(oid,ti,pow ).
The exist-pow operation veries that an object's POW is active at ti .
If so, it returns true otherwise it returns false.
4. attach-  pow(oid,(pow  ti),powmeaning).
The attach-pow operation attaches temporal context to a possible object world.
If the given POW of an object does not exist at ti , it is created through the create-pow operation and bound through the attach-pow operation.
If the specied pow is already bound to the specied object, the operation has no eect.
4.3 Temporal Migration Operations  To facilitate the temporal evolution of objects and to accommodate the four types of mappings described in section 3.1, we have designed the following set of corresponding operations.
These operations facilitate an object's migration from one UOI to a different UOI (or similarly from one possible object world (or a set of POWs) to another possible object world (or a set of POWSs)) at any given time.
1. transfer-pow(oid,uoid, POW( ti),POW( tj)).
The transfer-pow operation allows the migration of an object from one possible object world at time ti , to a dierent possible object world at a dierent time tj , where ti < tj .
If the object's possible object worlds at ti already exists then an appropriate warning is broadcasted.
2. multitransferpow(oid,uoid,POW( ti), f(POW( tj),...(POW(	 tj )))g).
The multitransfer-pow operation allows the migration of an object from one possible object world to a new set of possible object worlds.
The transitional path of this migration is also recorded.
As with the previous operation, if the new set of possible object worlds already exists then an appropriate warning is broadcasted.
3. evolve(oid,uoidi,uoidj ).
The evolve operation creates a new universal object identity for a given object.
As such, this operation facilitates the transformation of an object to an entirely new object over time.
4. grow(oid,uoidi,POW( tj )).
The grow operation allows an object that has only a universal object identity to obtain its rst temporal  context.
As such, this operation facilitates the rst temporal representation of objects.
4.4 Temporal Evolution Operations  In addition to the above operations we have developed a set of operations that allows us to manipulate the time priority sequences of an object.
1.
TPSEQevolve(oid,TPSEQ1,TPSEQ2 ), The TPSEQ-evolve operation creates a new time priority sequence for an object.
This operation allows us to extract the temporal behavior of objects over a period of time.
A dierent operation is used for changing the priorities of a given possible object world at any given time ti ,  4.5 Historical Operations  We have also developed a specialized set of operations which allows us to manipulate and extract the lifespan of an object and its history, compare inter-temporal relationships among objects, as well as to accommodate the relations introduced in 3].
In general, this set of operations supports the historical denition and manipulation of objects.
5 Temporal Object Constraints  The interaction of objects with other objects has to be subjected to some strict temporal constraints.
This tends to be a rather complicated task since there is a signicant dierence between conventional \non-temporal" data models and temporal object models.
While in the rst case only true messages are allowed, and any checks on such messages are made at the time the message is input, in temporal data models any message could potentially be true.
Therefore, our model's constraints should categorize information into true, false and meaningful the later making the specication of such constraints complicated.
In our previous works 3, 11] we have dened various temporal operators such as starts, meet, nish.
Based on these operators, we have developed a set of primitive explicit constraints which can be applied on the mappings or the objects themselves, based on 1], some of which are  stated below.
These constraints are enforced when we process such queries as \who were UK's prime ministers while Mr. Bush was in oce?".
In dening these constraints, and for simplicity we assume the existence of only two POWs and explicitly indicate the valid to and valid from times of a TSEQ(POW).
However, the generalization of these constraints is straightforward.
These constraints are dened as follows:  Constraints for Lifestarts:  The create time of POW equals the create time of POWfi  i.e., given (POW  (ti tj )) starts (POWfi  (tk  tl )) then ti equals tk .
Constraints for Lifenishes:  The virtual-delete time of POW equals the virtual-delete time of P OWfi  i.e., given (POW  (ti tj )) finishes (POWfi  (tk  tl )) then tj equals tl .
Constraints for Lifequals:  The create time of POW equals the create time of POWfi and The virtualdelete time of POW equals the virtuali.e, given delete time of P OWfi .
(POW  (ti  tj )) equals (POWfi  (tk  tl )) then ti equals tk and tj equals tl .
Constraints for lifeoverlaps  The create time of POW is less than the create time of P OWfi and the virtualdelete time of POW is less than the virtual-delete time of POWfi and the create time of POWfi is less than the virtual-delete time of P OW  i.e., given (POW  (ti  tj )) lifeoverlaps (POWfi  (tk  tl )) then ti < tk ^tj < tl ^tk < tj .
Constraints for lifeduring  The create time of POW is greater than the create time of P OWfi and the virtualdelete time of POW is less than the virtual-delete time of POWfi .
i.e., given, (POW  (ti  tj )) during (POWfi  (tk  tl )) then ti > tk ^ tj < tl  Constraints for lifecontains  Given, (POW  (ti tj )) contains (POWfi  (tk  tl )) then (POWfi  (tk  tl )) during (POW  (ti tj )).
Constraints for lifemeets  The create time of POW equals the create time of POWfi  i.e., given (POW  (ti  tj )) meets (POWfi  (tk  tl )) then tj equals tk .
Constraints for lifebefore  The virtual-delete time of POW is less than the create time of POWfi  i.e, given  (POW  (ti tj )) lifebefore (POWfi  (tk  tl )) then tj less than tk .
Constraints for lifeafter  (POW  (ti tj )) lifeafter (P OWfi  (tk  tl )) then (POWfi  (tk  tl )) lifebefore (POW  (ti  tj ))  Constraints for Temporal relationships For each relationship, (t  t  t ), and assuming no future relationships can be established ti before tj and tk before tj .
Finally, we can have a set of general state constraints viz., semantic constraints such as: fi an object cannot have a POW unless its UOI is dened.
fi a temporal composite object cannot exist at tj if the UOI and POW of each of its components is not 1 at ti such that ti < tj .
In addition to these constraints in our previous works 3, 11] we have suggested a set of temporal principles that our model should adhere to.
We call this set \metatemporal semantic constraints."
Although the constraints studied in this section are very restricted, it is believed that our temporal object database system provides a good basis for studying other, more general constraints.
i  j  6 Concluding Remarks  k  In this paper we have outlined an approach to the design and development of a model that integrates time with object databases.
Our work applies temporal notions to the problem of object and meta-data evolution.
It is a step towards the synthesis of a powerful integrated object data model that supports the temporal aspects of data modeling in addition to the structural and dynamic ones.
In our results we developed a set of temporal constructs, operations and constraints for supporting and manipulating temporal objects and their roles.
Our framework is a very simple temporal object database model for modeling temporal objects and temporal relationships.
All data in the database are treated uniformly as temporal objects relationships among these objects allow the temporal evolution of objects to be modeled.
Operations on the data allow temporal behavioral properties to be modeled.
Mechanisms are provided to allow the modeling of temporal relationships of objects, the temporal evolution of objects and object  migration.
In addition, the model provides a high degree of temporal and semantic expressiveness.
Our temporal object database system is not a high-level model appropriate for unsophisticated database users for example, it is easy to create meaningless relationships and operations.
The model lacks mechanisms for data protection and integrity control, and high-level constraints.
However, the main purpose of our system is not to dene a high-level temporal object database model, but on the contrary, to dene a small set of fundamental concepts to be used as a vehicle in the design and implementation of temporal object models and for providing more expressive models.
References 1] J. F. Allen.
An interval-based representation of temporal knowledge.
In  Proceedings of the International Joint Conference on Articial Intelligence,  pages 191{204, Vacouver, B.C., 1981.
2] K. Makki and N. Pissinou.
A new storage organization for temporal databases.
International Journal of Systems and Sciences (to appear), 1994.
3] N. Pissinou.
Time in Object Databases.
PhD thesis, Department of Computer Science, University of Southern California, Los Angeles, California, December 1991.
4] N. Pissinou and K. Makki.
T3dis: An approach to temporal object databases.
In Proceedings of the International Conference on Information and Knowledge Management, pages 185{192, 1992.
5] N. Pissinou and K. Makki.
A Framework for Temporal Object Database Models.
In T.W.
et.
al.
Finin, editor, Information and Knowledge Management:Expanding the Denition of Database, volume 752.
Springer-  Verlag, 1993.
6] N. Pissinou and K. Makki.
Separating semantics from representation in a temporal object databases.
In ACM Proceedings of the International Conference on Information and Knowledge Management, pages  295{304, Washington, DC, November 1993.
7] N. Pissinou and K. Makki.
A unied model and methodoly for temporal object databases.
International  Journal on Intelligent and Cooperative Information Systems, 2(2):201{  223, 1993.
8] N. Pissinou and K. Makki.
Separating semantics from representation in a temporal object databases.
Inter-  national Journal of Computer Information Systems, Spring, 1994.
9] N. Pissinou, K. Makki, and Y. Yesha.
On temporal modeling in the context of object databases.
ACM SIGMOD RECORD, 22(3), September 1993.
10] N. Pissinou, K. Makki, and Y. Yesha.
Research perspective on time in object databases.
In R. Snodgrass, editor, Proceedings of the Interna-  tional Workshop on Infrastructure for Temporal Databases Databases,  pages 295{304, Arlington, Texas, June 1993.
11] N. Pissinou, R. Snodgrass, R. Elmasri, I. Mumick, M.T.
Ozsu, B. Pernici, A. Segev, and B. Theodoulidis.
Towards an infrastructure for temporal databases.
Technical Report TR93, Departmet of Computer Science, University of Arizona, 1993.
12] A. Segev and A. Shoshani.
Logical modeling of temporal data.
In Proceedings of the ACM SIGMOD International Conference on Management of Data, pages 454{466, 1987.

An application-independent support system for integrated assumption-based temporal reasoning Clemens Beckstein  Univ.
Erlangen-Nfiurnberg, IMMD-8 Am Weichselgarten 9 D-91058 Erlangen, Germany beckstein@informatik.uni-erlangen.de  Abstract  Reason maintenance systems and temporal reasoning systems are among the most prominent application-independent support systems used for complex AI applications and there are many applications that need both support for logical and temporal reasoning.
Apparently it is not enough to just provide two isolated support modules.
The logical and temporal subsystems have to be coupled in the right way.
We present a hybrid support system integrating assumption-based logical and temporal reasoning, give a formal characterization, and show what an ecient incremental implementation of the system would look like.
1 Assumption-based temporal reasoning  Suppose we want to mass produce some machines the details of which are not important here.
As part of this, we have to plan how to manufacture the parts of the machines.
These plans can then be used to search for a schedule that guarantees a cost eective production of the parts on a given set of machines.
When planning or executing plans, assumptions have to be made | about future world states, the availability of resources or the duration of actions.
These assumptions have to be managed explicitly in order to be able to make or retract them as needed.
Explicit representation and processing of assumptions can be accomplished with assumption-based reason maintenance systems as the ATMS dK86].
On top of the ATMS there usually are systems like the PNMS (Plan Network Maintenance System) BLS92] representing and maintaining non-linear plans with assumptions.
It is often impossible to separate and sequentialize planning and scheduling in applications due to the strong interactions between them.
The isolated generation of an \optimal" plan can render it impossible  Tim Geisler  Univ.
Mfiunchen, Institut ffiur Informatik Wagmfiullerstr.
23 D-80538 Mfiunchen, Germany geisler@informatik.uni-muenchen.de  to nd the best schedule.
When viewed separately, a global optimum can often be produced from two suboptimal partial solutions.
Therefore planning and scheduling have to be considered together.
Temporal reasoning is necessary for scheduling as well as for planning, but the requirements are dierent.
For planning, a lot of qualitative temporal information is needed because the executability of plans has to be ensured and propositions about the ordering of events in time have to be made.
In order to produce a schedule, propositions about absolute time points and durations have to be made, which require metric temporal information.
For scheduling, metric constraints are not absolute they can be dierent under dierent assumptions.
This problem can be solved by making the assumptions underlying these constraints explicit , i.e.
by going from temporal constraints to assumption-based temporal constraints.
By reasoning with temporal rules, normal boolean propositions become derivable, and by reasoning with logical rules new temporal constraints can be computed.
Therefore, an integration of the assumption-based processing of temporal constraints and the assumption-based processing of logical constraints is necessary.
The product of this integration can then be added to a problem solver as a module which acts both as a conventional reason maintenance system and a temporal reasoning system.
In the rest of this paper, we present how such a module can be constructed from the ATMS by merging it with a simple assumption-based temporal reasoning system.
2 A simple assumption-based temporal reasoning system  The STP: The starting point of our development 1  was the Simple Temporal Problem (STP ) DMP91], a very simple formalism for temporal reasoning with 1 In the sequel we will use the abbreviation (A)STP both for the problem and the system that can solve (assumption-based) simple temporal problems.
metric constraints.
The STP is a constraint satisfaction problem over a set T of variables with the range D, given by a set of constraints C the elements of which all have the form tj ; ti  a where ti  tj 2 T and a 2 D. The elements of the domain can be interpreted as time points or time distances and the constraints express upper bounds for the (directed) distance of two time points.
Implicitly for each variable ti a constraint t0 ; ti  0 is introduced, where t0 is a distinguished variable.
A solution of an STP is an assignment of the variables from T satisfying all constraints C .
With this kind of constraints a lot of interesting temporal relations can be expressed (like the duration of jobs, partial ordering of jobs or deadlines and other information of importance for scheduling).
An STP can be represented as a labeled directed graph.
Each variable is represented by a node and each constraint tj ; ti  a is represented by an edge from ti to tj with the label a.
The test for satisability as well as the solutions can be computed from the shortest paths between the node representing the variable t0 and all other nodes in the graph.
Hence, the Floyd-Warshall algorithm (cf.
AHU74, CLR90]), which has a time complexity of O(n3) where n is the number of variables, can be used to solve an STP.
The algorithm computes, as a side eect, maximal directed distances between each pair of time points represented by a variable.
These can be viewed as solutions to the constraints in the sense described above.
Extension to the ASTP: In a lot of applications temporal constraints cannot be expressed absolutely as in the STP.
We therefore generalized the STP to an ASTP1 (Assumption-based Simple Temporal Problem).
In the ASTP, constraints are expressed relative to a set of assumptions.
Let A denote the universe of all assumptions.
An ASTP is a constraint satisfaction problem over a set T of variables with the range D, given by a set of constraints C with elements of the form U ` tj ; ti  a (ti tj 2 T and a 2 D and U  A is a set of assumptions).
In the ASTP we also have for each variable the implicit constraint  ` t0 ; ti  0.
The ASTP is apparently downward compatible to the STP | an STP-constraint of the form tj ; ti  a is represented in the ASTP as  ` tj ; ti  a.
The solutions P (A) !
(T !
D) of an ASTP for C are all the functions which map sets of assumptions2 U  A to assignments for the variables in T and satisfy all constraints (U 0 ` tj ; ti  a) 2 C where 0 U  U.
2 P (A) denotes the power set of the set A.
As the Floyd-Warshall algorithm can be generalized to path problems in weighted directed graphs where the edge weights are elements of a closed semiring (S 	   0 1) AHU74], a suitable semiring has to be identied to get an algorithm for the solution of an ASTP.
Path problems in closed semirings can be formalized as follows: The label (p) of a path p = hv1  : : : vk i in such a graph is dened as (p) := (v1  v2 )   : : :   (vk;1 vk ) where the label (u v) of an edge (u v) is their weight.
If (u v) is not an edge of the graph, then (u v) = 0.
The generalized Floyd-Warshall algorithm computes the summary luv of all labels of all paths for all pairs (u v) of nodes in the graph: M (p) luv := p=hu:::vi  It can be sketched as follows: for i j := 1 to n if i = j then lij := 1  (i j ) else lij := (i j ) for k := 1 to n for i j := 1 to n lij := lij  (lik  (lkk )?
 lkj ).
(*) The iteration operator ?
is dened as a?
:= L1i=0unary i a , with a0 := 1 and ai := a   ai;1.
This algorithm has a time complexity of O(n3 (Tfi + T ) + n2T?
), where Tfi , T and T?
are the time complexities for the corresponding operations.
Note that for the semiring identied in this paper, these operations do not have a constant time complexity because minimized labels can get arbitrarily large.
Since we want to design a hybrid system consisting of the ATMS and the ASTP, we assume in the following that a unary predicate nogood over P (A) exists with :nogood() ^ 0 (nogood(U) !
(8U : U  U 0 !
nogood(U 0 ))): Let D be the set of all possible temporal distances.
This set makes up a commutative monoid (D + 0) where D := R  f;1 +1g and + is the generalization of real addition on D according to: a + (+1) = (+1) + a = +1, (;1) + (+1) = (+1) + (;1) = +1 and a+(;1) = (;1)+a = ;1 where a b 2 R. It is now possible to express the set of all temporal distances relative to a set of assumptions as DA := D  P (A) and to order it partially with v: d1 U1] v d2 U2 ] () d1  d2 ^ U1  U2 The set L^ of the so called prelabels is L^ := fL j L  DA ^ 9d ] 2 Lg:  The idempotent function m : L^ !
L^ minimizes prelabels: m(L) := minv (L) nfd U] j d U] 2 L ^ nogood(U)g: minv is a function mapping a partial ordered set w.r.t.
v to the set of its minimal elements.
The domain of the semiring, the set L of the minimized labels, can then be dened as ^ L := fm(L) j L 2 Lg and the semiring (L 	   0 1) itself can be specied as follows (with L1  L2 2 L): L1 	 L2 := m(L1  L2 ) L1   L2 := m(f d1 + d2 U1  U2 ] j d1 U1] 2 L1 ^ d2 U2 ] 2 L2 g) 0 := f+1 ]g 1 := f0 ]g It is easy to show that the requirements for a closed semiring as stated in AHU74] are fullled: (L   1) and (L 	 0) are commutative monoides, 0 is absorbing w.r.t.
the concatenation operator  , the summary operator 	 is idempotent and   is distributive over 	.
Summaries over countable innite sequences exist and 	 is associative, commutative and idempotent for innite summaries.
Furthermore   is distributive over countable innite summaries.
When applying the iteration operator to a label Lii, it can happen that ;1 U] 2 L?ii , that is U ` ti ; ti  ;1.
This constraint is not satisable.
Therefore the ASTP has no solutions for assumption sets U 0 where U 0  U.
In ATMS terminology the assumption set U is a nogood.
For our formalization in this case the predicate nogood has to be extended by U and all supersets of U: ;1 U] 2 L?ii =) nogood := nogood  fU 0 j A  U 0  U g An ASTP with the constraint set C can be transformed into a directed graph (called ASTP-graph) weighted with temporal labels by creating a node for each variable ti and creating an edge (ti  tj ) from ti to tj with a temporal label Lij for each pair of nodes: For i 6= j: M Lij := f1 ]g  fd U ] j (U ` tj ; ti  d) 2 Cg: For i = j: M Lij := f0 ]g  fd U ] j (U ` ti ; ti  d) 2 Cg: For each environment U  A maximal temporal distances d(U ti  tj ) for the time points represented by the variables ti and tj can be extracted from the labels Lij of the edges (ti  tj ) of the graph computed by the Floyd-Warshall algorithm.
The corresponding function d : P (A)  T  T !
D is dened as: d(U ti tj ) := minfd0 j U 0 d0] 2 Lij ^ U 0  U g:  We can use this function to transform temporal labels of edges in the ASTP-graph into logical labels of temporal constraints in the ATMS: U is an environment of the logical label of the implicit temporal constraint tj ; ti  d(U ti tj ) unless it is subsumed by another environment in the ATMS label.
It only has to be communicated to the ATMS, if the temporal constraint tj ; ti  d(U ti tj ) is relevant .
Typically, this is the case if the constraint is mentioned in a justication or queried by the problem solver.
Moreover, two extreme solutions with t0 = 0 can be extracted from the ASTP: Smin (U) := f(ti  ;d(U ti t0)) j ti 2 T g Smax (U) := f(ti  d(U t0 ti )) j ti 2 T g Smin (Smax ) is a solution mapping environments to assignments describing minimal (maximal) distances to t0 .
Minimized temporal labels L 2 L have the following properties: 8T 2 L T 0 2 L : T v T 0 !
T = T 0 (minimality) 8d U] 2 L : :nogood(U) (consistency) 8U  A : 9d U 0] 2 L : U 0  U (closedness) The following examples show the eects of the operators (assumption sets like fA B C g are noted as ABC for brevity).
m(f5 AB] 3 A] 1 C] 1 ]g) = f3 A] 1 C] 1 ]g f3 A] 2 B] 1 ]g   f1 BC] 5 ]g = f8 A] 7 B] 3 BC] 1 ]g f3 A] 2 B] 1 ]g 	 f1 BC] 5 ]g = f3 A] 2 B] 1 BC] 5 ]g We will give an example demonstrating how the Floyd-Warshall algorithm computes the minimized labels once we have shown in the following section how to couple an ATMS and an ASTP.
3 Integration of ATMS and ASTP  The ATMS considers assumption-based temporal constraints of the form U ` tj ; ti  a as uninterpreted propositions (nodes) of the form dtj ; ti  ae the logical labels of which include the environment U (explicitly or implicitly, because it was removed by subsumption).
Temporal nodes are handled specially they are interpreted by the ASTP.
The interface of the hybrid system to the problem solver corresponds to the interface of the ATMS to the problem solver (cf.
dK86]).
In addition, the problem solver can query the AST processor for solutions of the AST problem.
Especially, consistent variable assignments can be computed for a given context.
To cope with the special semantics of temporal nodes, we generalize the notion of derivability for the ATMS: A node n holds in an environment U with a set R of justications R`U !n if there is a series of node sets S1  : : : Sm with  S1 = tc(U),  Si+1 = tc(Si  fy j 9(x1  : : : xk ) y) 2 R with xj 2 Si (1  j  k)g),  n 2 Sm .
The function tc denotes the temporal closure  tc(N) := t^ci(N) i  of a node set N, where t^c(N) := N  fdtj ; ti  de j dtk ; ti  aik e 2 N ^ dtj ; tk  akj e 2 N ^ d  aik + akj g: Given the formal specication of the integration of the ATMS and the ASTP, how can the two systems be combined in practice?
What would an implementation of it look like?
It is evident that changes in the logical labels of temporal nodes that are computed by the ATMS have to be collected and communicated to the ASTP.
This now causes changes in the temporal labels of edges in the ASTP-graph.
Hence, the ASTPgraph must be recompleted with the Floyd-Warshall algorithm.
This causes another change in the temporal label of an ATMS-node, if the newly discovered constraint is relevant, and the game starts again.
An ecient implementation of the hybrid system (in the sequel called ATTMS | the assumption-based temporal truth maintenance system) requires changes to the ATMS label propagation algorithm and the generalized Floyd-Warshall algorithm.
The ATMS works incrementally: one justication at a time is added to the dependency network.
After each addition a label update is invoked, even when the problem solver is not currently interested in the label of a node.
The generalized Floyd-Warshall algorithm as realized in AHU74] does not work incrementally.
Using it as is for the ASTP therefore means computing all temporal labels after every change which is not acceptable.
As will be seen in a moment, both modules of the hybrid system have to be adapted.
This adaption essentially aects the following areas: Processing of nogoods: The ATMS and the ASTP must maintain a common set of nogoods.
This set of nogoods is taken into consideration in the algorithm used for solving an ASTP (cf.
the denition of the minimizing function m and its use in the denition for the operators 	 and  ).
The ASTP can discover new nogoods.
As shown previously, this happens when the iteration operator computes a temporal label with the temporal distance ;1.
In this situation all the newly discovered nogoods are automatically communicated to the ATMS.
Computing the temporal labels in the ASTPgraph: Computing the temporal labels from  scratch can be very inecient.
In general, it is more ecient to use an incremental algorithm for the update of the ASTP labels.
There are two ways to develop an incremental label update algorithm.
First, the generalized Floyd-Warshall algorithm can be made more \incremental".
In RR91] it is shown that it is not possible to construct a real incremental algorithm (having a time bound depending only on the size of the incremental change of the input and the minimal change of the output) for the general all pairs shortest path problem and thus for the path problem on closed semirings.
But it is very easy to develop an algorithm with a time complexity of O(n2(Tfi +T +T? ))
for an edge insertion operation using the approach described in IK83].
During label propagation in the ATMS, usually more than one label of a temporal constraint is updated.
All these updates are collected and added to the ASTP at the same time.
The outer loop of the Floyd-Warshall algorithm only has to process the endpoints of added edges.
Another big eciency improvement can be obtained by incrementalizing the semiring operations, analogous to the incremental ATMS label update algorithm.
A formal description of this incremental version is left out for brevity and can be found in Gei94].
Processing the information resulting from temporal propagation eficiently: Even an \in-  cremental" Floyd-Warshall algorithm as described above works in a kind of batch processing mode.
With each batch a number of temporal labels is updated, which can be transformed into logical labels of temporal constraints as previously shown.
The ATMS therefore must update more than one label at a time.
Technically these updates can be seen as new justications for the ATMS.
The ATMS is not interested in all of the temporal constraints maintained by the ASTP, but only in temporal constraints used in justications or queries of the problem solver.
Only changes of labels belonging to these constraints must be communicated to the ATMS.
Nevertheless, in general, a set of justications has to be communicated to the ATMS.
We call the problem of eciently adding a set of justications to the dependency network as the bulk update problem .
A solution for the bulk update problem for the ATMS is shown in BG94, Gei94].
The two central ideas for this algorithm are avoiding redundant label products  and an optimal serialization of the label updates in the dependency network.
Typically, there are a lot of common antecedents in large justication sets as occurring in the justication sets communicated by the ASTP.
Computing label products of these antecedents as needed during the ATMS label propagation algorithm leads to a waste of computation when done several times for the same antecedents.
With a fast heuristic common subexpression elimination algorithm these redundant computations can mostly be recognized and avoided.
In order to do that, label propagation has to be synchronized so that the set of pending updates on which the common subexpression elimination is performed is as large as possible.
A side eect of the synchronization is the minimization of the number of propagation waves started at each node.
This can be achieved by using the partial order underlying the dependency network.
A non-trivial subproblem is the incremental actualization of this topological information based on the structure of the strongly connected components of the dependency network.
An ecient -incremental algorithm maintaining this information as new justications are added was developed.
4 An example  Suppose the following justications are communicated to the ATTMS (assumptions are denoted with capital letters)3 : (A ^ B ) r) (B ^ C ) ?
) (A ) d5  T2 ; T1  6e) (B ) d3  T3 ; T2  4e) (C ) d1  T3 ; T2  3e) (D ^ dT1 ; T3  ;6e ) s) Now the label propagation in the ATMS is started.
After it has is nished, the logical labels of the nodes in the ATMS are updated to4 hr fAB gi hdT1 ; T3  ;6e fgi hs fgi hdT2 ; T3  ;3e fB gi hdT2 ; T3  ;1e fB C gi: As this causes changes in the logical labels of some temporal nodes, the corresponding temporal labels in the ASTP-graph change as well.
The ASTP-graph then looks as follows:  #"f fi gg T !
fi fi0 ]    D zzzz DDDDDD z DDDD ffi1fi]gzzzzzz DffiD;3B] fi;1C ] fi1fi]g z z f fi4B ] fi3C ] fi1fi]g DDDD z z DDDD zzzzz ffi1fi]g DDD z zzz ffi6A] fi1fi]g <  3  b   # T fi f fig !
fi0 ]  O  o  #"f fi gg T !
fi  2  /  ffi;5A] fi1fi]g  O  D zzzz DDDDDD z DDDDffi;3B] fi;1C ] fi1fi]g ffi10AB] fi9AC ] fi1fi]zg zzzzz DD z z zzz ffi4B] fi3C ] fi1fi]g DDDDDD z z z ffi;8AB] fi;6AC ] fi1fi]g DDDD D zzzzz ffi6A] fi1fi]g 3  b   # T f fi g fi !
 "f fi g fi ! "
|  1  fi0 ]  /  ffi;5A] fi1fi]g  o  O  T2  fi0 ]  O  This causes several logical labels to change.
Here are the labels of a few selected nodes in the ATMS: hr fAB gi hdT1 ; T3  ;6e fAB AC gi hs fABD ACDgi hdT3 ; T1  9e fAC gi: Suppose that a new justication (A ) dT3 ; T1  7e) is added to the ATTMS now.
As 7 A] v 10 AB] and 7 A] v 9 AC], the temporal label of the ASTP-edge T1 !
T3 can be simplied to f7 A] 1 ]g: After the subsequent completion of the temporal labels with the Floyd-Warshall algorithm, the ASTPgraph looks at follows:  #"f fi gg T !
fi fi0 ]    D zzzz DDDDDD z DDDD ffi7A] fi1fi]gzzzzzz DffiD;3B] fi;1C ] fi1fi]g z z f fi4B ] fi3C ] fi1fi]g DDDD z z zzzzffi;8AB] fi;6AC ] fi1fi]g DDDDDD D zzzzz ffi6A] fi4AB] fi1fi]g <  3  b   #fi T f!
;  " fi  !f fi g "  |  1  /  o  O  fig  fi 1AB ] fi0 ]  ffi;5A] fi1fi]g  T2  fi0 ]  O  Since the label of the ASTP-edge T1 !
T1 is containing a relative temporal distance ;1 AB] with a negative distance value, the environment AB is discovered as being inconsistent.
This produces a new nogood AB for the ATTMS and all environments that are a superset of this nogood have to be removed from the logical labels in the ATMS.
In addition, all relative distances with environments which are a superset of the new nogood have to be removed from the temporal labels in the ASTP.
Eventually, the ASTPgraph looks as follows:  #"f fi gg T !
fi fi0 ]    D zzzz DDDDDD z DDDDffi;3B] fi;1C ] fi1fi]g ffi7A] fi1fi]gzzzzzz DD z z zzz ffi4B] fi3C ] fi1fi]g DDDDDD z DDDD z z ffi;6AC ] fi1fi]g D zzzzz ffi6A] fi1fi]g  fi0 ]  3 (A ) d5  T2 ; T1  6e) is an abbreviation for the three justications (A ) dT2 ; T1  6e) (A ) dT1 ; T2  ;5e) (dT2 ; T1  6e ^ dT1 ; T2  ;5e ) d5  T2 ; T1  6e).
4 A node n with label l(n) is written as hn l(n)i  fi0 ]    <  "  |  1   T " fi  !f fi g  Now the Floyd-Warshall algorithm must recomplete the temporal labels in the ASTP-graph:  <  3  b   # T fi f fig !
fi0 ]  O  o   T " fi  !f fi g "  |  1  2  /  ffi;5A] fi1fi]g  O  fi0 ]  The logical labels of the interesting nodes in the ATMS are then given by: hr fgi hdT1 ; T3  ;6e fAC gi hs fACDgi:  5 Related work  Only a few approaches can be found in the literature which attempt to merge assumption-based reasoning with temporal reasoning.
All these systems share the application domain \model-based diagnosis".
Propositions are associated with time points or time intervals in order to model states at certain time points or during time intervals.
A rst attempt to integrate temporal reasoning into an ATMS was made in DF89].
In this approach, propositions, assumption sets and intervals are related via a global table.
Symbolically represented time intervals are propagated similar to assumptions along time-independent logical justications resulting in expressions over intervals.
The temporal dimension is just a second indexing scheme besides the assumption sets.
HEART JR90] combines propositions and time intervals with start and end points instantiated with numbers.
The result of this combination (called an episode ) is treated as a node.
Assumptions as well as antecedents and the conclusion of a justication are episodes.
\Environments" are sets of episodes.
As the assumptions are structured in a purely propositional and an interval component, the notion of label minimality is generalized.
In order to keep \labels" minimal, episodes with the same propositions and overlapping intervals are merged.
Redundant episodes (based on interval inclusion) are removed.
In an eATMS justication TL93] every proposition P is associated with a time point variable t or the set of all time points .
Justications have the form P1:t1 ^ : : : ^ Pk :tk ^ R(t1  : : : tk) ) Q:f(t1  : : : tk ) where the ti are time point variables or , R is a predicate over tuples of time points enabling the specication of temporal relations and f is a function mapping tuples of time points to time points.
Assumptions are time-independent.
The label of a time-independent proposition is a set of ordinary ATMS labels indexed with time points instantiated with numbers and are only minimized w.r.t.
the  entry.
The ATMS propagation algorithm is extended to handle justications of the form described above.
One disadvantage of this approach is that the termination of the propagation algorithm is not guaranteed because there are temporal variables in justications allowing innite deductions.
Another disadvantage is the restriction to time points.
In the TARMS HNP91], the most complex of these hybrid systems, the assumptions are timeindependent as well.
The temporal information is  represented in the labels of time-independent propositions, which are sets of ordinary ATMS labels indexed with xed time intervals.
These labels are minimized by merging adjacent intervals when they are associated to the same ATMS labels.
The justications are splitted in two parts: an ATMS-like justication and a temporal method.
The temporal method determines the intervals used in the consequents' label depending on the labels of the antecedents.
Using dierent temporal methods, a variety of temporal relationships can be modelled.
None of the systems mentioned here is able to process temporal constraints as usual temporal reasoning systems do because they are not able to solve temporal constraint satisfaction problems.
Also, inconsistencies in the temporal dimension are not handled as nogoods in the ATMS.
Furthermore these systems are not suitable for scheduling applications because no metric temporal information can be processed.
Time map management systems as the well known TMM DM87] do not support the simultaneous management of multiple contexts.
They are therefore not well-suited as support systems that have to manage assumptions explicitly which is important in application domains like planning and scheduling.
A main dierence between our approach and the approaches cited is the treatment of temporal constraints as nodes, which can be justied in the same manner as other nodes and therefore can have dierent status in dierent contexts.
6 Summary and future work  We have presented a hybrid system integrating assumption-based logical and temporal reasoning.
This system is a common generalization of the ATMS and an assumption-based temporal reasoning system.
The resulting hybrid system (the ATTMS) is especially suited to support integrated planning and scheduling.
The system is implemented already in Common Lisp, but some more work has to be done in order to evaluate and improve the performance of the overall system.
We are currently investigating the formal complexity of subalgorithms and of the whole system.
We are also thinking about what an interface to a problem solver on a higher level of abstraction might look like.
Other temporal reasoning approaches including Allen's interval algebra All83] will have to be examined whether they can be extended in an assumption-based manner and integrated into the ATMS.
The approach of Allen in a sense also uses the Floyd-Warshall algorithm, which is not complete for Allens interval algebra.
It would be interesting to investigate the restriction to the maximal subclass ORD-Horn of the interval algebra found by Nebel NB93] for which the Floyd-Warshall algorithm is complete.
Last but not least we are evaluating the practical suitability of our approach based on several larger planning and scheduling problems.
References  AHU74] Aho, A. V. Hopcroft, J. E. Ullman, J. D.: The Design and Analysis of Computer Algorithms.
Addison-Wesley, Reading, MA,  1974.
All83] Allen, J. F.: Maintaining Knowledge about Temporal Intervals.
Communications of the ACM, 11(26):832{843, 1983.
BG94] Beckstein, C. Geisler, T.: An Improved  Dependency Network Management Algorithm for the ATMS.
Technical report,  IMMD8, Universit!at Erlangen-N!urnberg, 1994.
In preparation.
BLS92] Beetz, M. Lindner, M. Schneeberger, J.: Temporal Projection for Hierarchical, Partial-order Planning.
Technical Report AIDA-92-15, FG Intellektik, TH Darmstadt, 1992.
CLR90] Cormen, T. H. Leiserson, C. E. Rivest, R. D.: Introduction to Algorithms.
MIT Press/McGraw-Hill, Cambridge, MA, 1990.
DF89] Dressler, O. Freitag, H.: Propagation of Temporally Indexed Values in Multiple Contexts.
In: Proc.
GWAI-89, Eringerfeld,  dK86]  pages 88{99, 1989. de Kleer, J.: 1.
An Assumption-based  TMS, 2.
Extending the ATMS, 3.
Problem solving with the ATMS.
Arti cial Intelligence, 28:127{224, 1986.
DM87] Dean, T. L. McDermott, D. V.: Temporal Data Base Management.
Arti cial Intelligence, 32:1{55, 1987.
DMP91] Dechter, R. Meiri, I. Pearl, J.: Temporal Constraint Networks.
Arti cial Intelligence, 49:61{95, 1991.
Gei94] Geisler, T.: Ein anwendungsunabhangiges Unterstutzungssystem zum integrierten annahmenbasierten temporalen Schlieen.
Diplomarbeit, Universit!at Erlangen-N!urnberg, IMMD8, 1994.
HNP91] Holtzblatt, L. J. Neiberg, M. J. Piazza, R. L.: Temporal Reasoning in an Assumption Based Reason Maintenance System.
Technical Report M91-22, MITRE, Bedford, MA, 1991.
IK83] Ibaraki, T. Katoh, N.: On-Line Computation of Transitive Closures of Graphs.
Information Processing Letters, 16:95{97, 1983.
JR90] NB93]  Joubel, C. Raiman, O.: How Time Changes Assumptions.
In: Proceedings of  the 9th European Conference on Arti cial Intelligence (ECAI '90), Stockholm, 1990.
Nebel, B. B!urckert, H.-J.
: Reasoning about Temporal Relations: A Maximal Tractable Subclass of Allen's Interval Algebra.
Technical Report RR-93-11, DFKI,  Saarbr!ucken, 1993.
RR91] Ramalingan, G. Reps, T.: On the Com-  putational Complexity of Incremental Algorithms.
Technical Report TR 1033, Univer-  TL93]  sity of Wisconsin, Madison, 1991.
Tatar, M. M. Letia, I.
A.: Embedding Tem-  poral Reasoning into the ATMS Framework.
In Puppe, F. G!unther, A., editors: Expertensysteme '93 (XPS-93), Hamburg,  pages 276{282, 1993.
Hypothetical Reasoning From Situation Calculus to Event Calculus Alessandro Provetti  CIRFID - Universita di Bologna Via Galliera 3/a, Bologna I-40121 ITALY provetti @cird unibo it :  Abstract  Pinto and Reiter have argued that the Situation Calculus, improved with time handling axioms, subsumes the features of linear time temporal formalisms such as Event Calculus and Interval Logic.
In this note we nd answers to some of their remarks by showing a modied version of Event Calculus that seems to match Situation Calculus handling of hypothetical reasoning and projection.
Further consideration on semantics and expressive power of Event Calculus put forward by Pinto and Reiter are discussed in the light of recent proposal for an unifying semantics for languages for time and actions.
1 Introduction  In their very recent production, Reiter and Pinto7, 8] have introduced an upgraded version of Situation Calculus (SC) which makes it possible: to represent dates and time-stamp actions and situations which actually occurred in the world to represent actual situations as a branch of the tree of possible developments of things that Situation Calculus handles.
This new features are obtained by adding new predicate denitions and introducing a new sort of constants for representing dates, a convenient ordering, and functions such as Start(action) or End(action), linking actions to their dates.
Pinto and Reiter argue that the improved version matches the so-called linear time formalisms, viz.
Allen's Interval Logic and the Calculus of Events(EC) of Kowalski and Sergot3], on their own ground: representing actions and change over time.
Nonetheless, the resulting Situation Calculus maintains intact its native characteristics(set out in 5]) of dealing with alternative, hypothetical Work done during author's stay at Computer Science Department of University of Texas at El Paso, which is gratefully acknowledged.
:  plans/sequences of actions and projecting their effects.
Another point raised by Pinto and Reiter is on semantics: they present a logic programming implementation of a subset of the formalism which enjoys a clear completion-based semantics, in contrast with EC relying on Negation as Failure.
In this paper it will be counter-argued that Situation Calculus specic -and indeed desirable- features are easily implementable in a linear-time formalism like Event Calculus.
In chapter 2 a simple version of EC is presented which departs from the original version criticized by Pinto et al.
but can be taken as representative of current versions of EC.
In chapter 3 new predicates are introduced for allowing reasoning about a ctional sequence of actions and projecting the value of uents.
This simulation can be either performed in the future, for exploring the result of alternative plans or starting from a date in the past, which allows for counterfactual reasoning.
In chapter 4 the declarative semantics aspect is discussed if an EC axiomatization is seen as a logic program, then the most common declarative semantics agree, yielding what is believed a clear semantics.
Indeed, a new semantics is proposed by translating EC axiomatizations to the language A of Gelfond and Lifschitz 1], which enjoys a semantics conceived for actions and change.
A translation from a domain description EC-style to one in A is proposed which maps also the closed-world assumption into the target axiomatization.
This technical result is kept for a full version of the paper, while it would be necessary to dene a similar translation from Pinto and Reiter's formalisms to A itself it will then result very interesting to compare the two axiomatizations and their models within the same language.
This approach is specular to that of Kartha in 2] on translating A to chosen nonmonotonic formalisms In the end, the author argues for a substantial equivalence of the two (improved) formalisms.
In the rest of the paper acquaintance with Situation Calculus and the semantics of Logic Programming is assumed.
2 The Event Calculus of the 90s  The Event Calculus has been proposed by Kowalski and Sergot 3] as a system for reasoning about time and actions in the framework of Logic Programming.
Event Calculus is based on an ontology of events, assumed as primitive.
These events are represented by means of constants that uniquely identify them.
The second ontology is that of uents 1, which represents descriptions of the reality being modeled.
A uent holds over time from the moment when an event initiates it, i.e.
the event makes it true in the world.
Events may also terminate, i.e.
make false in the world, uents.
The Event Calculus is based on forward default persistence: a uent holds over time until a terminating event is recorded.
Since the rst proposal, a number of improved formalization have steamed, in order to adapt the calculus to dierent tasks.
Hence, the reduced version of Shanahan in11] in presented, since it can be taken as a common-core denition embedded in the latest applications2.
Events are represented by sets of instantiations like the following: Happens(E1) Date(E1  T1) Act(E1 Unstack(B )) Notice that there are both event-tokens, labeled with the constants E1  E2 : : : and events-types named by Unstack, Stack etc.
The eect of an actiontype(its meaning) is understood by looking at the Initiates=Terminates axioms where it appears.
The denitions of Initiates and Terminates are for expressing domain knowledge.
A convenient example is the Block World, as both Shanahan and Pinto et al.
use it: Initiates(e On(x y))  Act(e Move(x y))  Initiates(e Clear(z ))  Act(e Move(x y)) Date(e t) HoldsAt(On(x z ) t) z 6= y T erminates(e Clear(y))  Act(e Move(x y)) T erminates(e On(x y))  Act(e Move(x z )) z 6= y Elsewhere called properties or relationships.
This version is even more simplied, as it assumes events are recorded in the database in the same order as they happened in reality.
For discussing a fuller formalization, the reader is invited to consult late works of Sergot10] and Sripada13].
1 2  Starting from a database of events and a domain description by Initiates=Terminates the axioms of EC makes it possible to derive atoms:  Holds(F T ) which are understood as "uent F is true at time T".
Axiom ECI means that a uent holds at a certain time if an event happened earlier initiated the uent itself and there is no evidence in the database of the uent stopping to hold in the meantime.
In other words, in the interval between the initiation of the uent and the time the query is about, no terminating events must happen.
This is made sure by axiom ECII .
The forward default persistence rule is implemented by using Negation as Failure on Clipped in ECI.
(ECI ) HoldsAt(f t)  Happens(e) Initiates(e f ) Date(e ts ) ts < t not Clipped(ts  p t) (ECII ) Clipped(ts  f t)  Happens(e ) Terminates(e  f ) Date(e  t ) ts < t  t fit The predicates < and fi establish an ordering on events.
We stipulate that temporal constants T1  T2 T3 : : : are mapped on naturals, and that the ordering relations are also mapped on the same relations on naturals, thus inheriting their properties.
In chapter 3, an improved version of the axioms will be presented in order to deal with hypothetic events.
The hypothetic events have no timestamping, so that the problem of integrating the linear order of actual events and the order on those hypothetical is not addressed directly.
2.1 The Assumption Underlying Event Calculus  EC is a formalism based on negation-as-failure.
This device implements the implicit assumptions on the knowledge of the domain that are used by EC.
Techniques are available, viz.
explicit negation, for making these closure assumptions explicit.
Let us list these assumptions, taking advantage of the discussions in 9, 11]: It is assumed that no events occur other than those which are known to occur.
It is assumed that all the events are timestamped.
These two assumptions seems too strong for real applications such as database updates in fact, they are lifted in enriched versions of EC.
It is assumed that no types of events can aect a given uent other than those which are known to do so This assumption can be made explicit by resorting to classical negation with these axioms: :Initiates(e f )  not Initiates(e f )  :Terminates(e f )   not Terminates(e f ) This approach is semantically founded on the Answer Sets semantics of Gelfond and Lifschitz and, for matter or generality, won't be used in the rest of the paper.
It is assumed that uents persist until an event happen that inuence them.
Conversely, It is assumed that every uent has an explanation in terms of events.
That is, at least one initiating event is necessary for making a uent true.
This is particularly interesting for generating explanations of uents by abducing events11].
If observations on the value of uents can be introduced in the formalization, i.e.
HoldsAt updates are allowed, a transformation of the axioms is necessary for giving consistent answers, at cost of a loss of elegance Sripada13] presents a version of the calculus for accommodating such updates.
3 Hypothetical Reasoning in EC  In this section we dene new predicates (on top of those already existing) for performing projection of hypothetical sequences of actions.
The purpose is that eectively illustrated by Pinto and Reiter7]: By preserving the branching state property of the Situation Calculus, we can express and answer a variety of hypothetical queries, although counterfactuals cannot be expressed.
For example "At time Tp in the past, when you put A on B, could A have been put on C instead?"
can be simply expressed as: during(Tp  s) ^ actual(s)  possible(put(A C ) s): "If I had performed put(A C ), would F have been true?"
V holds(F do(put(A C ) Sp)) possible(put(A C ) Sp ): None of these features is possible in linear temporal logics.
We need the branching structure of the situation calculus, coupled with a linear time line in that branching structure.
In the following, the new axioms and a modied and enriched version of the old ones will be illustrated, so that to deal with the sample queries proposed.
3.1 The new predicates  The ideas motivating the new predicates denition are the following: to rewrite situation calculus axioms within EC, in order to carry out projection to provide a link between the point in time t where the simulation begins and the value of uents in the simulation.
That is, uents that are true at t are still true during the simulation as long as an event does not terminate them.
To this extent, the eect of the simulation depends from the time it starts to make it possible both to project in the future and to reason hypothetically about a sequence of actions to this extent, the eect of a simulation does not depend from the time it starts.
HypHolds  The new predicate HypHolds is the counterpart of Situation Calculus Holds and it is understood as follows: a) HypHolds(F E type T ) is true if -has E type been performed at time T - F would be true thereafter.
(EC 1) HypHolds(f Res(e type t))  MayHappen(e type t) Initiates(e type f t) (EC 2) HypHolds(f Res(e type t))  MayHappen(e type t) not Terminates(e type f t) HoldsAt(f t) Now the predicate is dened for an arbitrary sequence of actions performed starting from T : b) HypHolds(F Res(An  Res(: : : Res(A1  T ) : : :))) is true if -has the sequence of actions A1 : : :An been performed starting from T - then F would be true thereafter.
In practice T replaces S0 , thus linking the chain of actions to the starting point of the simulation.
(EC 3) HypHolds(f Res(e type s))  HypMayHappen(e type s) HypInitiates(e type f s) (EC 4) HypHolds(f Res(e type s))  HypMayHappen(e type s) HypHolds(f s) not HypT erminates(e type f s)  Starting the simulation with t = 0, where each uent is false (by NAF) is a way to study in insulation the net eect of a plan.
MayHappen  In order to ensure that an action(i.e.
a type of event) can be performed at a certain time or in a certain state of aairs, the predicate MayHappen and HypMayHappen are introduced: MayHappen(E type t)  HoldsAt(C1  t) ::: HoldsAt(Cn  t) For instance: MayHappen(Move(a b) t)  HoldsAt(Clear(b) t) For each MayHappen instantiation, a relative instantiation of HypMayHappen is made For instance: HypMayHappen(Move(a b) s)  HypHolds(Clear(b) s)  HoldsAt and Clipped  The modications to these predicates are not substantial, some folding operation has been carried out and the arity of Initiates and Terminates has been increased to accommodate the parameter time.
As far as it goes, this version is expected to give the same results as Shanahan's in terms of success of HoldsAt queries.
Initiates and Terminates  Also for these predicates duplication is necessary in order to handle both dates and situations.
The new denition of Initiates and HypInitiates are like in this example: Initiates(e On(x y) t)  Act(e Move(x y)) Date(e t)  Initiates(e Clear(z ) t)  Act(e Move(x y)) HoldsAt(On(x z ) t) Date(e t) z 6= y HypInitiates(Move(x y) On(x y) s) HypInitiates(Move(x y) Clear(z ) s)  HypHolds(On(x z ) s) z 6= y A similar transformation must be applied to the denition of Terminates.
3.1.1 The new predicates at work  The rst question addressed by Pinto and Reiter: "At time Tp in the past, when you put A on B, could A have been put on C instead?"
translates into the following: ?
; MayHappen(Put(A C ) Tp ) Conversely, the second example: "At time Tp in the past, when you put A on B, could A have been put on C instead?"
translates into: ?
; HypHolds(On(A C ) Res(P ut(A C ) Tp))  4 Comparing the Semantics  Pinto and Reiter7] have compared the standard "rst-order + circumscription" semantics with that of EC: One advantage of this is the clean semantics provided by our axiomatization, in contrasts to the event calculus reliance on the Negation as failure feature of logic programming, whose semantics is not well understood.
The argument is rather appropriate, EC has been natively dened within Logic Programming 3, 10, 4] and the use of negation as failure for implementing default persistence is somehow intrinsic to EC.
It is nonetheless the case to notice that the set of axioms described in this paper (PEC ) form together a stratied logic program in the sense of Apt et al.6], under the following stratication 3: <p = fHoldsAt HypHolds MayHappen HypMayHappen Initiates HypInitiatesg < fClipped T erminates HypTerminatesg < f< fig < fHappens Act Dateg On stratied programs the semantics common in literature hold a unique minimal model.
This is the case for Przymusinki's perfects models semantics6] by taking the partition as an ordering over predicates the same goes for Apt et al.
6] iterated Fixpoint technique and for Gelfond and Lifschitz's Stable Models semantics.
The resulting, minimal and unique model of these semantics should carry an unambiguous meaning for EC4 .
Taking <p as a circumscribing policy, the perfect model results in a model of prioritized circumscription CIRC (PEC  <p ) for the theory PEC  it may be rewarding to compare the respective circumscriptive 3 This stratication is in fact redundant, but ts better intuition on layers of predicates.
To the extent of dening the declarative semantics predicates < and can be dened as a set of ground instances on time constants.
4 Notice in passing that Conjecture 1 of Apt et al.
in 6] ascribes to stratied programs the completeness of SLDNF resolution.
models of two intuitively equivalent theories in EC and SC.
This has not yet been carried out to author's knowledge.
4.1 Alternative Semantics  Beside the stratication-based semantics discussed above, there have been eorts to provide alternative semantics for event calculi a rst attempt is probably that of Shanahan12], who discussed a characterization in terms of circumscription.
In this section it is proposed an alternative approach by translation of Event Calculus formalizations to the language A of Gelfond and Lifschitz1], which enjoys a declarative semantics purported to actions and uents..
The translation  transforms a set of event descriptions in terms of Happens, Date etc.
into a correspondent set of A axioms.
The result sought after is soundness and completeness of the translation of an EC domain description D and of a query ?
; HoldsAt(F T ) into an domain description  (D) and a v-proposition F after CD (T ) such that:  D `EC HoldsAt(F T ) ()  (D) j=A F after CD (T ) where the chronicle CD (T ) is the list of actions happened before T in D and ordered by means of their dates.
The proof of this proposition will be included in the full version of paper.
The advantages of the translation are twofold: EC is given a new semantics and, in principle, at least a signicant class of A axiomatizations might be eectively computed in Prolog by dening a reverse translation to EC programs.
As soon as a similar translation from extended SC to A will become available, it will be possible to compare the two languages within the same semantical framework.
5 Conclusion  Similarities and dierences between Event Calculus and Situation Calculus have been subject of much attention in the latest literature4, 7, 8].
On the one hand, Pinto and Reiter have successfully implemented the treatment of time into SC thus matching the results obtainable with EC.
This work, on the other hand, has shown an improved version of EC which performs hypothetical reasoning on the eect of actions, one of the features that motivated Situation Calculus at its birth5].
Far this undertake from being nished, the author argues for a substantial equivalence of the two formalisms on the ground of expressive power, clear semantics and computational properties.
As for exibility, extended versions of Event Calculus existing in the literature for dealing with compound events, temporal granularities and continuous processes are quite encouraging, as well as applications to abductive planning, deductive databases and process modeling in areas such as engineering and Law.
As for elegance, tastes probably matter.
The present author feels easier at Event Calculus because of a more intuitive ontology of events and dates rater than actions, situations and dates5 , because of a plain computational value of the axiomatization and because the closed-world based semantics need not careful metatheoretical specications(circumscription) to yield the expected results.
This is not to say that all the aws of EC Pinto and Reiter point to can be easily xed.
As an instance, the aim to provide names for intervals of time bounded by events partially known has resulted in the rst formalization of EC allowing unintended models, as shown in 7].
The quest for improving EC is helped by such criticisms, as long as they recognize the long way EC has gone since 1986.
Acknowledgments  My thanks to Michael Gelfond, Chitta Baral, Stefania Costantini, Gaetano Lanzarone, Paulo Azevedo and Angelo Montanari.
References  1] Michael Gelfond and Vladimir Lifschitz.
Representing Actions and Change by Logic Programs.
In The Journal of Logic Programming., Vol.
17(2,3,4),november 1993. pages 301-355.
2] G. Neelakantan Kartha.
Soundness and Completeness Theorems for Three Formalizations of Action.
Proc.
of IJCAI'93 Conference, 1993. pages 724{729.
3] Robert Kowalski and Marek Sergot.
A Logicbased Calculus of Events.
New Generation Computing, volume 4 pages 67{95.
Ohmsha Ltd and Springer Verlag, 1986 4] Robert Kowalski.
Database Updates in the Event Calculus.
Journal of Logic Programming, volume 12, June 1992, pages 121{146.
5] John McCarthy and Patrick Hayes.
Some philosophical problems from the standpoint of articial intelligence.
In B. Meltzer and D. Michie, editors, Machine Intelligence, volume 4, pages 463{ 502.
Edinburgh University Press, Edinburgh, 1969.
6] Jack Minker, editor.
Foundations of Deductive databases and Logic Programming.
Morgan Kaufmann Publ., 1988.
7] Javier Pinto and Raymond Reiter.
Adding a Time Line to the Situation Calculus.
Working Papers of Common Sense '93, The second AAAI symposium on logical formalizations of common sense reasoning.
Austin(Tx), January 1993.
See 9] for a discussion on the ontologies of such formalisms 5  8] Javier Pinto and Raymond Reiter.
Temporal Reasoning in Logic Programming: A Case for the Situation Calculus.
Proceedings of ICLP'93 Conference.
Budapest, June 1993.
9] Alessandro Provetti.
Action and Change in Logic Programming: Event Calculus, Situation Calculus and A .
Manuscript.
Spring 1993.
10] Marek J. Sergot.
(Some topics in) Logic Programming in AI.
Lecture notes of the GULP advanced school on Logic Programming.
Alghero, Italy, 1990.
11] Murray P. Shanahan.
Prediction is Deduction but Explanation is Abduction.
Proc.
of IJCAI'89 Conference.
Detroit, 1989. pages 1055{1050.
12] Murray P. Shanahan.
A Circumscriptive Calculus of Events.
Imperial College Dept.
of Computing Technical Report.London, 1992.
13] Sury Sripada.
Temporal Reasoning in Deductive Databases.
PhD Thesis in Computing.
Imperial College, London, 1991.
A presentation of this work can be found in the Proc.
of IJCAI'93, pages.
860{865.
The persistence of statistical information  Scott D. Goodwin  Eric Neufeld  Dept.
of Computer Science University of Regina Regina, Saskatchewan, Canada, S4S 0A2 (306) 585-5210 goodwin@URegina.ca  Dept.
of Computational Science University of Saskatchewan Saskatoon, Saskatchewan, Canada, S7N 0W0 (306) 966-4887 eric@USask.ca  Andre Trudel  Abstract  Jodrey School of Computer Science Acadia University Wolfville, Nova Scotia, Canada, B0P 1X0 (902) 542-2201 trudel@AcadiaU.ca  The frame problem was originally dened in the context of the situation calculus.
The problem also manifests itself in more sophisticated temporal logics that can represent interval information.
In interval based logics, we call it the persistence problem.
Almost all proposed solutions are only applicable in the context of the situation calculus.
A few of the solutions can deal with a restricted class of interval based information (i.e., information that is true throughout an interval).
We consider a version of the persistence problem where we have statistical information about an interval.
For example, given the average rainfall over the summer, what is the expected average rainfall during the fall?
We describe a logic that can represent statistical interval based information, and then give a solution to the persistence problem for this kind of information.
1 Introduction  In previous work 	6], we dened a category of temporal interval-based information called denite integral information.
For example: I spent four hours in the museum.
I walked a total of one hour, and paused briey in front of most of the paintings.
and: Over the summer, it rains 20% of the time.
Both examples require representing uncertainty at the subinterval level.
In the rst example, walking is  known to be true for some unknown number of subintervals of a four hour interval, and the total length of all the subintervals is one hour, but the starting points and duration of each subinterval is unknown.
In the second example we know how long it rains, but cannot say if it is raining at any particular point.
We are also interested in the kinds of inferences that can be made from denite integral information.
For example, from Over the summer, it rains 20% of the time.
we may wish to make an inference about rainfall for the month of July or at noon on June 20th.
It is impossible to deduce point valued answers to these questions but the information imposes certain constraints on the answer.
For example, given a three month summer, the rate of rainfall in July can vary between zero and 60%.
As well, we can make certain reasonable inductive inferences.
For example, a reasonable inference is that the rate of rainfall in July is 20%, and the probability of rain at noon on June 20 is 0.2.
We call the problem of making reasonable inferences about subintervals and interior points for which we have denite integral information the problem of interpolating denite integral information.
Other reasonable questions to ask are what will be the rainfall during the fall and at 5pm on November 1st?
These questions involve making predictions about an interval or point that lies outside the interval for which we have denite integral information.
We call this instance of the persistence problem the problem of extrapolating denite integral information.
We review previous work that dened denite integral information, presented a solution to the interpolation problem, and a limited solution to the extrapolation problem.
We combine and generalize these two solutions to provide a solution to the extrapolation of denite integral information.
2 Denite integral information  The logic we use to represent denite integral information is called RGCH 	4].
In RGCH, we represent point based qualitative temporal information with 0{1 valued functions.
For example, \the book is on the table at time point 3", is represented as on(book table 3) = 1 and, \John is not running at time point t9 ", is represented as running(J ohn t9) = 0: In both cases, the last function parameter denotes a time point.
Our representation of point-based qualitative information facilitates the representation of qualitative interval-based information.
We derive the duration of truth of temporal information over an interval by integrating the corresponding 0{1 valued function (a similar approach is used in 	12]).
For example, if the book is on the table between times 3 and 7, then: 8t : 3 < t < 7 !
on(book table t) = 1: We integrate:  Z  7 3  (  on book table t  ) dt = 4  to get the duration of time (in this case 4 time units) that the book is on the table over the interval (3,7).
Another example of qualitative interval-based information is \John ran a while between times t5 and t8 " which is true if and only if the integral of running over the interval (t5  t8) is non-zero:  Z  t8  (  ) dt  running J ohn t  t5  >  0:  Recall from Section 1 the following example of definite integral information: Over the summer, it rains 20% of the time.
This type of information is easily represented in RGCH:  Note that the cases where  = 0 and  = t2 ; t1 coincide with Shoham's 	11] point-point-liquid proposition types, Allen's 	1] properties, and McDermott's 	9] facts.
For example, the block is red over the interval (0,10) is written as:  Z 10 0  (  )  colour block red t dt  = 10  and the block is not green:  Z 10 0  (  )  colour block green t dt  = 0:  3 Interpolation  In this section, we review a solution to the interpolation problem that appears in 	5, 6].
For interpolation, we wish to make plausible inferences that go beyond those deductively entailed by the denite integral information.
These inferences hinge on an assumption of epistemological randomness (or, roughly, a principle of indierence) that is, taking all we know into account, each possible interpretation is assumed to be interchangeable (i.e., equally expected).
From this we can infer an expected value for the function  at a particular point.
We can compute this value from the given denite integral information by rst determining an interval of points, all of which have the same expected value for  , and using denite integral information about the interval to determine the expected value at the particular point.
This interval of points having the same expected value for  is a maximally specic reference interval.
Denition 3.1 (Max.
Specific Ref.
Interval)  The (possibly non-convex) interval R is a maximally specic reference interval for  (t0 ) if 1. t0 2 R, and Z sep20 2.
 has the same expected value at every point in R. raining (t)dt = 0:2  (sep20 ; jun21): jun21 We do not need to know the expected value of  at every point in an interval to determine if it is maximally In 	6], we formally dene denite integral information specic|we only need to know the expected values as: are the same.
If the information we have concerning two points is identical (with respect to  ), then the Denition 2.1 (Definite Integral Information) expected value of  at the points is the same.
So all we need do is verify that we have the same informaInterval-based information is called detion for every point in the interval.
nite integral information if it can be represented in terms of a denite integral:  Z  t2  ( ) =   t dt  t1  where 0    (t2 ; t1 ) and  is a 0{1 function possibly containing other parameters.
Denition 3.2 (Interchangeable Points)  Suppose the only denite integral information our knowledge base contains concerning  is for the intervals: R1 R2 : : :  Rn (we refer to these as explicit reference intervals for  ).
We say two points t1 and t2  are interchangeable with respect to  , by which we mean that  has the same expected value at t1 and t2, if for every explicit reference interval Ri for  , we have t1 2 Ri i t2 2 Ri .
This says if the points fall within the same explicit reference intervals, then the expected values are (dened to be) the same.
Proposition 3.3 (Finding Max.
Spec.
Ref.
Int.)
If I is the intersection of the explicit reference intervals for  that contain the point t0 , and U is the union of the explicit reference intervals for  that do not contain the point t0 , then a maximally specic reference interval for  (t0 ) is R = I ; U .
Once we have identied a maximally specic reference interval for  (t0 ) we can relate the expected value of  (t0 ) to the denite integral information concerning the reference interval.
To do this, we use the following property of mathematical expectation: Proposition 3.4 (Expectation of Sums)  For any Z interval I  Z E  (t)dt = I  Q1  ( ( ))dt:  I  Interval I need not be convex.
For example, if I = (Rx y)Ry (uRvv) and (x y) and (u v) are disjoint, then I = x + u : We use Proposition 3.4 and the fact that the points of the reference interval have the same expected value for  to derive the following direct inference rule: Proposition 3.5 (Direct Inference Rule 1)  If R is a maximally specic reference interval for  (t0 ) then R   (t)dt  R : E ( (t0 )) = E jRj  The expected value of  at t0 is equal to the average value of  over the interval R. Note that this follows trivially from the fact that  has the same value at every point in R since R is a maximally specic reference interval.
The following property relates subintervals to intervals: Proposition 3.6 (Direct Inference Rule 2)  If S is a (possibly non-convex) subinterval of a maximally specic reference interval R for  Rthen   (t)dt   R  (t)dt  S R E = E : jS j jRj  : 60%  0  5  : 90%  10  15  Figure 1: Overlapping reference intervals.
We can exploit this idea to permit many interesting inferences.
For example, suppose we know (see Figure 1):  Z 10 0  () =6   t dt  ^  Z 15 5  ( ) = 9:   t dt  Notice the explicit reference intervals Q1 = 	0 10] and Q2 = 	5 15] overlap.
For  (7), we have Q3 = Q1 \ Q2 = 	5 10] is a maximally specic reference interval.
We derive from the denite integral information that  Z 10 5  E  t  Q2  ()   t dt  2 	4 5]  and then by Direct Inference Rule 1, we have that  fi R 10  !
() 5 2 	 54  1]: E (  (7)) = E 5 In the limit, it can be used to obtain the probability that temporal information is true at a point.
In 	5], we show that given additional reasonable constraints, for example, that the temporal information was continuous, we can establish bounds for interior point and subinterval inferences.
 t dt  4 Limited extrapolation  Almost all previous work on persistence hinges on McCarthy's common sense law of inertia (CSLI): things tend not to change.
The obvious consequence of adopting this view is that it becomes reasonable to infer that the duration of non-change is arbitrarily long.
For instance, a typical inference in systems that appeal to CSLI is that if a person is alive now, the person will remain alive (arbitrarily long) until something happens that results in the person's death.
CSLI seems like a reasonable assumption in microworlds where the duration of events is relatively short and the number of measurable forces acting upon any object is small.
In the real world, it seems that a more natural assumption is that nothing persists forever, but instead persists according to a probability distribution.
Inferences, such as a wallet dropped on a busy street tends to remain where it fell for a shorter duration than a wallet lost on a hunting trip, can be drawn in this framework.
Unlike  the CSLI approach, this inference is possible without knowing what happened to change the wallet's location.
Another issue is the direction of persistence: does information persist into the past as well as the future?
For example, if we notice a building on the way home from work, then is it not just as reasonable to assume that the building was there the previous day as it is to assume it will be there the following day.
Most approaches (e.g., 	8, 10, 7, 3] to name only a few) restrict persistence to the forward direction only.
\Arbitrary-duration" persistence is too crude an approximation.
Instead, temporal information persists for some nite period of time into the future and/or past.
How long does information actually persist?
In most cases, we cannot give a denitive answer to this question.
For example, if John is currently alive, we cannot determine with certainty the time of his death (assuming we don't murder John).
But, from actuarial tables, we can estimate his life expectancy.
This is true of most temporal reasoning: although we don't know exactly how long something should persist, we can make reasonable statistical estimates.
In this section, we review a bi-directional limitedduration solution to the extrapolation problem that appears in 	5].
Note that this is a solution to the case where the temporal information is true throughout the interval (i.e., only applicable to a subset of denite integral information).
We approximate the truth values of a piece of information over time with regions of persistence.
For example, let running be true at time 100.
Assume that a person usually runs for 30 minutes and may sometimes run for up to 50 minutes.
We expect running to be true for some 30 minute interval that contains time 100.
For simplicity, we assume 100 is located in the center of the interval.
We then expect running to persist over the interval (100-15,100+15) and we expect running not to persist outside (10025,100+25).
Over the intervals (100 ; 25 100 ; 15) and (100 + 15 100 + 25) we are unwilling to predict whether running persists.
The regions of persistence for running are shown in Figure 2.
The regions of persistence (rop) are represented by the relation: rop(running ;25 ;15 15 25): The general form is: rop( ;t1  ;t2  t3  t4 ) where  is a point-based item of information.
If  is true at time X and nothing is known to aect  , then  is expected to persist throughout the interval (X ; t2 X + t3 ),  may or may not persist over (X ; t1 X ; t2) and (X + t3  X + t4 ), and  is expected to be false before X ; t1 and after X + t4 .
So, we predict  is true over (X ; t2  X + t3 ), we predict  is false before (X ; t1 ) and after (X + t4 ) and otherwise we make no prediction.
The general regions of persistence are shown in Figure 3.
Note the regions are not necessarily symmetric around X .
It may be that t2 6= t3 and/or t1 6= t4 .
In this instance, we can give the rop relation a simple statistical semantics.
Assume the duration of  is normally distributed with typical duration (mean)  and typical variation (variance) 	2 about that mean.
Suppose we are satised to predict  remains true if the probability of  remaining true is greater than 50% and we wish to predict  is false if the probability is less than 5% (approximately) and otherwise we make no prediction.
In this case, the relation rop( ;t1  ;t2 t3  t4) holds if and only if t2 + t3 = , and t1 + t4 =  + 2	 .
This statistical semantics subtly changes the meaning of persistence rather than stating that we can be reasonably sure  persists over (X ; t1 X + t4 ) it states that we can be reasonably sure it does not persist beyond the interval.
This is consistent with the usual interpretation of a continuous probability distribution function.
For example, if running truly has a normal distribution, the duration of a run is less than the mean 50% of the time.
Thus at time X we expect the run to end within t3 minutes with probability 0.5.
The semantics of the rop relation may vary with the problem domain.
For example, if we must be 95% sure that running is true to predict that it is true, we let t2 + t3 =  ; 2	 and t1 + t4 is unchanged.
As with the case of interpolation, this formalism can be extended to reason in other interesting settings.
For example, suppose running is known to be true over the interval (80,120).
In 	5, 6] we show how to estimate the time at which running will end.
Alternately, suppose instead the same parameters for running, but John has stopped running we can compute an estimate for when John began his run.
Thus the rop formalism, unlike others, is bidirectional.
  5 General extrapolation  Given statistical information (i.e., denite integral information) about a time interval, we wish to make reasonable inferences about past or future intervals.
For example, assume it rains 20% of the time during the summer.
What is the rainfall during the months that immediately precede and follow the summer?
For how many months following the summer should we make predictions about?
The rop solution from the previous section is not applicable in this case because the rainfall is not continuous throughout the summer.
The temporal projection technique of Hanks and McDermott 	7] is also inappropriate.
We cannot determine from the statistical information if it was raining during the last day of summer.
Even if we knew it was raining at that time, it does not make sense to allow raining to persist indenitely.
We have no information about  False  Uncertain  True  Uncertain  False  100-25  100-15  100  100+15  100+25  Figure 2: The regions of persistence for running False  Uncertain  True  Uncertain  False  X-t1  X-t2  X  X+t3  X+t4  Figure 3: The general regions of persistence actions or events that may aect raining.
Finally, Dean and Kanazawa's probabilistic temporal projection 	2] cannot be used as it requires the construction of a survivor function for raining based on many observations of raining changing from true to false.
In our example, we have no observations of raining at particular points.
No solution to the extrapolation problem appears in the temporal litterature.
Our proposed solution to the extrapolation problem is based on the interpolation formula:  R  (t)dt   R  (t)dt  S E = E R jRj jS j that states we may infer that the proportion of points in R for which  (t) is true is equal to the proportion of points in S for which  (t) is true, provided that S  R (and R is a maximally specic reference interval).
To use this for extrapolation, we change the restrictions on regions R and S so that S is a region adjacent to R rather than a subinterval.
We then use rop information to estimate the \amount" of persistence while the modied interpolation formula determines the \spread" of persistence.
Denition 5.1 (Extrapolation Assumption)  Let R be a convex region for which we have denite integral information about  , and S be a convex region adjacent to R for which we have no information about  .
The extrapolation assumption is that:  R  (t)dt   R  (t)dt  R S = E (1) E jS j jRj  R  where S  (t)dt is estimated using rop information.
This assumption is an implementation of bidirectional limited-duration persistence.
The justication for it is that it captures the intuition that the extrapolated region S is like the adjacent region R. For example, suppose running is known to be true over the interval R = (80 120): How long should running persist beyond time 120 (i.e., S = (120 X ))?
Using formula (1), we have:  fiR X  E  120  !
running(t)dt =E X ; 120  Substituting for  R 120 80  fiR X  E  120  ( )=E  80  running(t)dt 40  !
:  running(t)dt = 40 gives:  !
running(t)dt = 1: X ; 120  Solving for E (X ) gives: E X  fi R 120  fiZ  X  120  !
running(t)dt + 120  (2)  Using rop information from the previous section, we can estimate the value of the integral on the right hand side.
Assume that running is normally distributed with a mean of 30.
Suppose we wish to predict running will persist if we are 50% sure it will continue, and we wish to predict it won't if we are 97:5% sure it won't and we make no prediction otherwise.
We consider the expected remaining time  for those runs longer than 40 minutes.
By conventional methods, we nd that about 50% of runs longer than 40 minutes last about another 4 minutes and 97:R5% are completed within about 16 minutes.
Thus, X E( 120 running(t)dt) = 4 and formula (2) reduces to: E (X ) = 124: R This holds only in the case where R  (t) = jRj i.e., continuous running.
We now show that the interpolation rule can be combined with the rop rule to allow the extrapolation of general denite integral information.
Consider the R case where running is not continuous, for example 60120 running(t) = 40 (R = (60 120) S = (120 X )).
Using formula (1), we have:  fiR X  E  120  !
running(t)dt =E X ; 120  fi R 120 60  running(t)dt 60  !
:  (3) Using the rop information (taking the amount of running, rather than running interval length, to be normally R Xdistributed with  a mean of 30) we have that E running ( t)dt = 4 and formula (3) is reduced 120 to:  40   4  = E 60 : E X ; 120 So R  ( ) = 126.
Thus we expect that running(t)dt = 4: We can now also compute the expected value of running at a point in the interval (120,126) by doing interpolation.
126 120  E X  6 Future work and Conclusion  The extrapolation assumption is only one of many possible assumptions for extrapolating our knowledge.
One weakness of the assumption is that it does not take trends into account.
Points near R may be expected to behave more like points inside R than points further away.
In future work, we will study variations of the extrapolation assumption which incorporate such eects.
Another weakeness is the assumption's restriction to convex intervals.
We are currently investigating ways to extend the assumption to handle non-convex intervals as well.
The problem of extrapolation appears in the AI literature as the frame problem and in the statistical literature (as well as the uncertainty in AI literature) as the problem of forecasting.
Previous work on regions of persistence and extrapolation from definite integral information considered the special case where the temporal information persisted throughout the entire interval for which integral information was available.
This work consolidates previous work on interpolation and regions of persistence into a unied framework that allows bidirectional reasoning about persistence about events.
Acknowledgements  The rst author acknowledges the support of the Institute for Robotics and Intelligent Systems (IRIS) and NSERC grant OGP0122131.
Research of the second author was supported by IRIS and NSERC grant OGP0099045.
Research of the third author was supported by NSERC grant OGP0046773.
References  	1] James F. Allen.
Towards a general theory of action and time.
Articial Intelligence, 23:123{ 154, 1984.
2] T. Dean and K. Kanazawa.
Probabilistic causal reasoning.
In Seventh Biennial Conference of the Canadian Society for Computational Studies of Intelligence (CSCSI'88), pages 125{132, Edmonton, Canada, May 1988.
3] T. Dean and G. Siegle.
An approach to reasoning about continuous change for applications in planning.
In Eighth National Conference on Articial Intelligence, pages 132{137, Boston, USA, 1990.
4] S.D.
Goodwin, E. Neufeld, and A. Trudel.
Temporal reasoning with real valued functions.
In Proceedings of the Second Pacic Rim International Conference on Articial Intelligence, pages 1266{1271, Seoul, Korea, Sept 1992.
5] S.D.
Goodwin, E. Neufeld, and A. Trudel.
Probabilistic temporal representation and reasoning.
International Journal of Expert Systems, to appear.
6] S.D.
Goodwin, Eric Neufeld, and Andr"e Trudel.
Denite integral information.
In Proceedings of the Ninth Biennial Conference of the CSCSI, pages 128{133, 1992.
7] Hanks, S. and McDermott.
D. (1987) Nonmonotonic Logic and Temporal Projection.
Articial Intelligence 33(3), 379{412.
8] H. Kautz.
The logic of persistence.
In Proceedings AAAI-86, pages 401{405, 1986.
9] D.V.
McDermott.
A temporal logic for reasoning about processes and plans.
Cognitive Science, 6:101{155, 1982.
10] Y. Shoham.
Chronological ignorance: Time, nonmonotonicity, necessity and causal theories.
In Fifth National Conference on Articial Intelligence, pages 389{393, Philadelphia, USA, 1986.
11] Y. Shoham.
Temporal logics in AI: Semantical and ontological considerations.
Articial Intelligence, 33:89{104, 1987.
12] A. Trudel.
Temporal integration.
In Eighth Biennial Conference of the Canadian Society for Computational Studies of Intelligence, pages 40{ 45, Ottawa, Canada, 1990.
Using Constrained Resolution for Abductive Temporal Reasoning Nicolas Chleq  INRIA Sophia-Antipolis BP 93 { 06902 Sophia Antipolis Cedex { France chleq@sophia.inria.fr  Abstract  We describe in this article an abductive procedure based on a constrained resolution principle.
The choice of constrained resolution is motivated by the whish to gain full advantage of using reified temporal logics.
For this purpose, it is interesting to deal eciently with temporal ordering and equality relation between instants.
The constrained resolution principle described here is a solution to this point.
It is an instance of the more general constrained resolution principle of H.J.
Burckert.
It also relies on the work done in the area of temporal constraint propagation.
For the purpose of temporal reasoning it is also necessary to cope with temporal persistency of known and deduced facts.
This point is solved by handling persistency in an abductive fashion.
1 Introduction  This article describes a resolution-based abductive procedure.
This procedure is based on works done in the area of abductive logic programming and uses a constrained resolution principle.
Such a resolution principle is necessary in order to be able to deal with any reified temporal logic based on instants.
For the purpose of this paper, we use it on a simple temporal logic based on a simple and nave ontology.
It is also suciently expressive for practical use, but this gain on expressiveness is due to an increased complexity of the language itself (more axioms), hence a more complex reasoning task.
The use of abduction in temporal reasoning has been motivated by 12].
This reasoning method is complementary to prediction as it allows to deal with persistency and produce explanation, while retaining a very intuitive form for causal rules eect if causes.
Abduction has also been applied to planning with temporal formalisms such as the Event Calculus 5, 10].
Most abductive procedure are based on resolution, and practical use of abduction relies on the feasibility of resolution based reasoning for temporal reasoning.
Though it is not necessary to argue again on the usefulness of a resolution principle, practical use of this method is not straightforward.
In particular, some features of the formul on which this method is applied can suer from great eciency problems.
Equality relation and self-resolving clauses are some examples of these problematic features.
Most of these problems are to be solved by adapted strategies and specialized inference rules.
A lot of work has been done on the combination of the resolution principle with some particular \algorithmic" theories: for example the Theory Resolution of Stickel 14], and the Constrained Resolution of Burckert 2].
In the next section, we begin by an informal presentation of a temporal logic.
This logic is used throughout this paper, and its features are representative of most reified temporal logics.
It also illustrates the need of a specialized resolution principle, which I describe as an instance of Burckert's one.
The rest of the paper is devoted to abductive reasoning, and focuses on the abductive procedure we developed as an extension of the abductive logic programming procedure described in 7].
2 Reified Temporal Logics  Reified temporal logics are sorted predicate calculi: one of the sorts is used for time points or time intervals.
A formal description of these logics is given in 13].
They are usually based on two primitive entities: instants as in McDermott's logic 9], or intervals as in Allen's one 1].
The basic construct h  ti of these logics associates a formula with a temporal entity t, this last one being either an instant or an interval depending on the kind of logic.
The intuitive meaning of this expression is that the formula is true at the instant denoted by the term t, or true throughout the interval denoted by t. These languages can express the truth of such-and-such proposition over time, they are hence good candidates for the expression of temporal knowledge.
2.1 A temporal logic  The logic we use is a two-sorted predicate calculus, where time is the sort of all expressions denoting time instants, and proposition is the sort of all terms associated with temporal entities.
It is based on in-  f]t1 t2] pg !
t1 < t2 f]t1 t2] pg !
begin(t1  p) f]t1 t2] pg !
end(t2  p) f]t1 t2] pg !
persist(t1  t2 p) persist(t1  t4 p) ^ (t1 fi t2 < t3 fi t4) !
persist(t2  t3 p) persist(t1  t3 p) ^ (t1 < t2 fi t3) !
true(t2  p) persist(t1  t2 p) ^ begin(t3  p) !
(t3 fi t1 ) _ (t2 < t3 ) persist(t1  t2 p) ^ end(t3 p) !
(t3 < t1) _ (t2 fi t3) begin(t1  p) !
(8t2 > t1 persist(t1  t2 p) _ (9t3 (t1 < t3 < t2 ) ^ end(t3  p))) begin(t1  p) ^ persist(t1  t2 p) ^ end(t2 p) !
f]t1 t2] pg  ( A1 ) (A2 ) (A3 ) (A4 ) (A5 ) (A6 ) (A7 ) (A8 ) (Ap ) (A9 )  Figure 1: Some axioms of the temporal logic.
stants as the primitive temporal entity, and intervals are written with two instants as their lower and upper bounds.
The simplest expression associates a proposition with an instant or an interval: ft P g means that P holds at time t, and f]t t ] P g means that P holds throughout the interval ]t t ].
Instants are taken from a set that we want to be dense, so that we are able to speak of an instant between any two other instants: the set of rational numbers suits our need.
We divide the set of propositions into the ephemeral ones, used to describe \instantaneous" phenomena, and durable ones.
Propositions of the first class are always associated with instants by expressions of the form ft pg, while propositions of the second type are associated with intervals by expressions of the form f]t t ] pg.
The set of durable propositions can be refined similarly to the classification established by Shoham 13].
For our purpose, we are only interested in the liquid propositions in this taxonomy, or homogeneous in ETL 11]: we call these propositions stable and this means that their truth throughout an interval implies their truth at all instants within the interval.
The truth of p at one instant t, as a consequence of the truth of p over an interval comprising t, is expressed by true(t p).
The set of stable propositions is a subset of the set of durable ones.
An instant can be represented by five means: a number, a variable, a symbolic constant, an arithmetic expression such as t + n where t is an instant and n a number, and a functional term f(t1  : : : tn).
We use two relation symbols for the ordering of instants: < and fi.
Between these ordering relations and expressions of the form fI P g, we propose axiom A1 in figure 1.
Another useful feature for exibility of a temporal logic as a knowledge expression language, is the ability to give partial information about truth periods.
For this purpose, we introduce the following expressions: begin(t, p) means that one of the truth period of proposition p begins at the instant t. When the 0  0  0  beginning of the interval is known by this way, the term e(t p) refers to the end (the upper bound) of this interval.
The axiom A2 establishes the relationship with the expression fI pg where I is an interval.
end(t, p) means that one of the truth period of p ends at the instant t. In the same way as above, the term b(t p) refers to the lower bound of this interval.
This expression is formally defined by axiom A3 .
persist(t1, t2 , p) means that the interval ]t1 t2] is included in one of the truth period of p. This expression is defined by the two axioms A4 and A5 .
The maximality of truth period expressed by formulae like f]t t ] pg entails that the lower bound of these intervals are really the time when the proposition becomes true, and that the upper bounds are the instants when the proposition ceases to be true.
This entails that overlapping truth periods of the same proposition lead to a contradiction between the truth inside one of the intervals, and the non-truth outside the other one.
We suggest axioms A7 and A8 to express that overlapping of distinct truth periods is not allowed for a durable proposition.
Axiom A9 completes the definition of the logic and enables to deduce a complete truth period from partial information.
0  2.2 Example  We propose to illustrate the use of this logic by the well known \Yale Shooting Problem" which is written in figure 2.
We can deduce from this example that the gun will become unloaded at time 4 because of the firing.
We express it by end(4 loaded) and the proof of it involves reasoning about the temporal persistency of loaded.
The truth period of this proposition begins at time T1 because of the loading action on the gun.
Then, thanks to axiom Ap , it will last as long as needed, provided it is not interrupted.
At this moment, the upper bound of the persistence can not be fixed, but we can assume it to be at least equal to  fT1  loadingg true(2 alive) f4 pull;triggerg T1 < 2 R1 : 8t ft loadingg !
begin(t loaded) R2 : 8t ft unloadingg ^ true(t loaded) !
end(t loaded) R3 : 8t ft pull;triggerg ^ true(t loaded) !
end(t loaded) R4 : 8t ft pull;triggerg ^ true(t loaded) ^ true(t alive) !
end(t alive) Figure 2: The Yale Shooting Problem.
loading and pull-trigger are ephemeral propositions, and loaded and alive are stable.
4.
It can not be greater than 4 because of axiom A7 .
Thus, the only solution is that this upper bound is equal to 4.
3 The constrained resolution principle  In this section, we focus on the ability to do resolution-based reasoning with some temporal logic similar to the one described in the previous section.
The main problem of these languages comes from the use of equality and ordering relation symbols.
The usual axiomatization of the ordering relation fi, which describes the transitivity, reexivity and antisymmetry involves some self-resolving clauses.
This feature entails that for some queries the resolution process may spend a lot of time with repeated use of these clauses, without any way to know whether or not these inferences are relevant for the original query.
Our solution is an instance of the more general resolution principle proposed by H.J.
Burckert 2].
This constrained resolution principle is introduced in the framework of a particular logic, called logic with restricted quantifiers.
In this logic, quantifiers are associated with formul interpreted as restriction on the variables of the overall formula.
Clausal formul with restriction are noted C k R , which means 8X R !
C, where X is the vector of variables in C. T denotes the theory of restriction formul: it is given in such a way that (un)satisfiability and validity of these formul can be decided by an algorithmic mean.
Burckert simply assumes given a class of models for the restriction theory T .
The RQ-resolution principle is given by: fP (x1 	 	 	  xn)g   C k R f:P (y1 	 	 	  yn)g   D k S R ^ S ^ ; is T -satisfiable C  D k R^S ^; (1) where ; is the conjunction of equations x1 = y1 : : : xn = yn .
One of the completeness result from 2] says that given an unsatisfiable set of clauses, it is possible to derive by RQ-resolution an empty clause 2 k R such that T j= 9(R).
For the purpose of temporal reasoning, we consider that the restriction theory T is the theory where fi is interpreted as an ordering relation between time  instants and = means that two instants are at the same position on the time line.
To enable the use of constrained resolution, one first needs to have a constrained clausal form of the input formul.
For example, axiom A1 in Figure 1 produces the following constrained clause:  f]t1 t2] pg k :(t1 < t2 ) while axiom A6 is transformed in: true(t2  p)  f]t1 t3] pg k (t1 < t2 fi t3 ) The constrained forms of the axioms have a restriction which is a conjunction of temporal constraints.
However, some of these constraints are negative literals.
To simplify the use of constrained resolution, we choose to assume that fi is a total ordering relation.
This allows to use rewriting rules such as :(t < t ) !
t fi t to eliminate negative constraints.
We also split clauses with disjunctive restriction: C k R1 _ R2 !
f C k R1  C k R2 g 0  0  3.1 Deciding satisability  Provided that the restrictions of clauses are conjunction of positive litterals, it is possible to use temporal constraint propagation techniques to decide satisfiability.
Thus, the satisfiability of a conjunction of temporal expressions is equivalent to the global consistency of the constraints network built from these expressions.
For our problem, we are interested in both the symbolic and numeric relationships between instants.
We choose to rely on the formalism of Simple Temporal Problem (STP) studied by Dechter 3].
In this formalism, a constraint between two instants is represented by an edge between two nodes representing the instants, the label of the edge being a numeric interval.
Such a constraint is written x : a b] : y where x and y are two instants, a and b are two numbers belonginq to R   f;1 +1g.
This constraint means that a fi y ; x fi b.
A set of these constraints gives a network of binary constraints, such that an O(n3) path consistency algorithm is a complete decision procedure for the global consistency of the constraint set.
All expressions comparing instants denoted with numbers, variables, constants and arithmetic terms can be expressed within this constraint formalism.
Some simplification rules for unification 8], especially the ones for decomposition of functional terms,  are used inside the constraint solver when an equality is encountered.
The purpose is to: (1) handle non-arithmetic functional terms involved in equations by simplifying these equations (2) identify equations that involve variables so that they are used to instantiate the resolvent clause.
This keeps the set of constraints as small as possible.
Thus, given the set of constraints R ^ S ^ ; of rule (1), the satisfiability test produces a pair h C i where  is a substitution and C is a set of constraints such that (R ^ S ^ ;)  C. Then, the resolution principle is formulated as a variant of Burckert's one.
This gives: fP (x1 	 	 	  xn)g   C k R f:P (y1 	 	 	  yn)g   D k S R ^ S ^ ; is satisfiable (C   D) k ; (2) where ; is the set fx1 = y1  : : : xn = yn g, and h ; i is the pair resulting from the satisfiability test of R ^ S ^ ;.
0  0  4 Abductive temporal reasoning  This section describes an extension of the abductive logic programming procedure described by Kakas et Mancarella in 7].
This extension handles constrained resolution and, contrary to the original one, can handle non ground abducible litterals.
The original abductive procedure is an extension of SLD-resolution, and is inspired from the first one described by Eshghi and Kowalski in 4] to handle negation as failure in a abductive fashion.
The definition assumes a logic program P (a set of clauses of the form C  L1 : : :Ln , where C is the head and L1 : : :Ln the body), a set H of predicate symbols called abducible predicate, and a set IC of integrity constraints (clauses with empty head).
The purpose of the original procedure is to find, for a query Q, a set " of hypotheses (ground instances of abducible predicates) such that there exists a stable model M 6] of P   " such that M j= Q and M j= IC.
4.1 Denition of the procedure  The particular features of the procedure are the following:  we use the ability for a refutation using constrained resolution to produce \conditional answers", as it is done in Constraint Logic Programming.
For this purpose, we consider that, at the end of an abductive refutation, the ground temporal constraints in the restriction of the derived empty clause represent, if they are not satisfied, some additional ordering hypotheses which can be assumed if they are consistent  in the same way, it is possible to force a failure in a derivation by assuming some additional constraints.
When an empty clause is derived,  the constraint part of this empty clause can be made unsatisfiable.
The simplest possibility is to add a new temporal constraint to the current set of hypothesis such that the constraints set of the empty clause becomes inconsistent The procedure builds interleaved sequences of states.
The first sequence form is called an abductive refutation where each state has the form hGi  "ti "i $i  Iii.
At the beginning, G0 is the original query.
Gi is a goal clause, "i is a set of constraints, and "i is the current set of hypotheses.
The set Ii is initialized with the integrity constraints in IC and is used to collect new integrity constraints from the failure in the consistency check part of the procedure.
Denition 1 Let G be a goal clause of the form B .
An abductive refutation of G is a finite sequence of tuple:      G1  "t1 "1 $1 I1 : : : Gn "tn "n $n In where Gi is a goal clause, "ti is a set of ground constraints, "i is a set of ground literals, $i is a substitution, Ii is a set of integrity constraints, G1 = G $1 =  I1 = IC Gn = 2 k R such that either T  "tn j= R or "tn ^ R is T satisfiable and for each i = 1 : : : n, Gi has the form L L k R where L is the selected literal, and the  next state Gi+1  "ti+1 "i+1 $i+1  Ii+1 is obtained 0  according to one of the following rules: (A1 ) L is positive, C is the resolvent of Gi and of a variant of some clause in P on the selected literal L with the pair h ;i, then:  Gi+1 = C "ti+1 = "ti $i+1 =   "i+1 = "i Ii+1 = Ii  (A2 ) L is either positive and abducible or negative, L unifies with an element of "i with the pair h ;i, (R ^ ;) is T -satisfiable, then:  Gi+1 = L k (R ^ ;) "ti+1 = "ti "i+1 = "i $i+1 =  Ii+1 = Ii 0  (A3 ) L is either positive and abducible or negative, neither L nor its negation unifies with an element of "i, and there exists a consistency derivation from hF0 "ti "i   fLg Ii i to hfg "t "  I i then: 0  0  0  Gi+1 = L k R "ti+1 = "t "i+1 = " $i+1 =  Ii+1 = I where  is a substitution which maps each variable of L to a new skolem constant, and F0 is the set of all resolvents of the clause L  with clauses of Ii .
0  0  0  0  A consistency derivation implements the test of consistency of an hypothesis.
It is very similar in essence to a negation as failure call in logic programming.
The aim is to check whether an assumption is consistent with the program P and the current set of hypotheses " and of temporal constraints "t. A consistency derivation is a sequence of states of the form hFi  Dit Di  Ii i, where Fi is a set of goal clauses, Dit is the set of temporal constraints, and Di the set of current hypotheses.
The set Ii collects the failed goals during the test, so that they will be used with further hypotheses.
Denition 2 A consistency derivation is a finite sequence of tuple      F1 D1t  D1  I1 : : : Fm  Dmt  Dm  Im such that for each i 2 1 m], Fi is a set of goal clauses and has the form f L L k R g  Fi , Fm is the empty set, Dit is a set of ground temporal constraints, Di is a set of ground literals, and Ii is a set of integrity constraints, and L is selected  in the body of L L k R .
Fi+1  Dit+1 Di+1  Ii+1 is obtained according to one 0  0  0  of the following rules: (C1) there exists in Fi an empty clause C = 2 k R , then: 0  Fi+1 = Fi ; fC g Di+1 = Di Dit+1 = Dit   fD g Ii+1 = Ii where D is a ground constraint such that R ^ D is inconsistent, and Dit+1 is T 0  0  0  0  satisfiable.
(C2) L is positive, C is the set of all resolvents of clauses in P with the clause L L k R on the literal L, then: 0  Fi+1 = C   Fi Dit+1 =Dit Di+1 = Di I  i Ii+1 = I   f L L k R g ifif CC 6= =  i 0  0  (C3) L is either positive and abducible or negative, C is the set of all resolvents of L L k R with elements of Di on the literal L, then: 0  Fi+1 = C   Fi Dit+1 = Dit Di+1 = Di Ii+1 = Ii   f L L k R g 0  0  (C4) L is either positive and abducible or negative, L is ground, the opposite of L is in Di , then:  Fi+1 = Fi Di+1 = D  Dit+1 = Dt Ii+1 = Ii  0  0  0  (C5) L is either positive and abducible or negative, L is ground, and theret exists an abductive refutation from h:L Di  Di  Iii to the state h 2 k R  Dt D  $  I i then: 0  0  0  0  Fi+1 = Fi Ii+1 = I  0  0  0  Di+1 = D  0  and either Dit+1 = Dt if T  Dt j= R , or Dit+1 = Dt ^ R if T  Dt 6j= R and Dt ^ R is T -satisfiable.
0  0  0  0  0  0  0  0  0  For our purpose, the logic program P is made of the constrained clause form of the axioms of Figure 1 together with rules describing domain relationships, such as the clauses of Figure 2 for the YSP example.
Axioms A1 , A7, and A8 are integrity constraints in the set IC.
The predicate symbol persist is abducible: this means that whenever a new persistency hypothesis is needed, the consistency check will try to refute goals of the form begin(t p) and end(t p) where t falls within the period of the persistency assumption.
Of course, the aim is that these refutations fail so that the assumption does not violate integrity constraints.
4.2 Example  Recall the YSP example of Figure 2.
The query Q =end(t p), means that we are interested in finding when the gun will cease to be loaded.
Rule R3 yields the goal true(4 loaded).
Axiom A6 produces the goal persist(t1  t2 loaded) k t1 < 4 fi t2 where the literal persist(t1  t2 loaded) is abducible.
We begin a consistency derivation with a set of temporal ordering assumptions "t = fS1 < 4 fi S2 g, and a set of hypotheses " = fpersist(S1  S2  loaded)g, where S1 and S2 are new temporal constants.
The set of goals that we want to fail is: 8 :persist(S1  S2 loaded) k >  9 > > < = 2 k S 2 fi S1  F1 = > end(t loaded) k S1 fi t < S2  > : begin(t loaded) k S1 < t fi S2   The first clause in F1 disappears because the opposite of the literal :persist(S1  S2  loaded) is in ", and the second clause also disappears when we add the ordering constraints S1 < S2 to the set "t. At the end of the consistency derivation the set of clauses is empty, and the temporal ordering constraints in "t force S2 to be equal to 4 and S1 to T1 .
The primary abductive refutation ends and yields the substitution ft 7!
4g as an answer to the query Q.
One limitation of the procedure lies in the ability to handle repeated events: although the logic is able to describe those situations, the refutation procedure must be protected for infinite queries by a bound on the depth of the refutation.
It should be noted that those queries do not lead to subsumption between the current goal and one of its ancestors: it appears to be a \translation" on the time line.
At this moment, we do not have any mean to identify this relationship, nor can we characterize \translated" goals with respect to their utility in the refutation process.
5 Conclusions  In this paper, we describe an abductive procedure using a constrained resolution principle.
Such a resolution principle is very useful in the area of  temporal reasoning.
Constrained resolution allows an important gain on eciency by reducing nondeterminism, which is otherwise too much a trouble in pure resolution-based reasoning methods.
The abductive procedure is based on work done by kakas and Mancarella on abductive logic programming.
This procedure can be used to handle persistency as an assumption, and for planning problems where the set of computed hypotheses and temporal constraints describes a plan to achieve the requested goal.
References  1] James F. Allen.
Towards a general theory of action and time.
Artificial Intelligence, 23(2):123{ 154, 1984.
2] Hans-Jurgen Burckert.
A Resolution Principle for a Logic with Restricted Quantifiers, volume 568 of Lecture Notes in Artificial Intelligence.
Springer-Verlag, Berlin Heidelberg, 1991.
3] Rina Dechter, Itay Meiri, and Judea Pearl.
Temporal constraints networks.
Artificial Intelligence, 49:61{95, 1991.
4] K. Eshghi and R. A. Kowalski.
Abduction compared with negation by failure.
In G. Levi and M. Martelli, editors, Logic Programming: Proc.
of the Sixth International Conference, pages 234{254.
MIT Press, Cambridge, MA, 1989.
5] Kave Eshghi.
Abductive planning with event calculus.
In Proc.
of the 5th Int.
Conf.
on Logic Programming, pages 562{579, 1988.
6] Michael Gelfond and Vladimir Lifschitz.
The stable model semantics for logic programming.
In Proc.
of ICLP'88, pages 1070{1080, 1988.
7] A. C. Kakas and P. Mancarella.
On the relation between truth maintenance and abduction.
In Proc.
of PRICAI'90, pages 438{443, 1990.
8] A. Martelli and U. Montanari.
An ecient unification algorithm.
ACM Trans.
Programming Languages and Systems, 4(2):258{282, 1982.
9] Drew V. McDermott.
A temporal logic for reasoning about processes and plans.
Cognitive Science, 6:101{155, 1982.
10] Lode Missiaen.
Localized Abductive Planning with the Event Calculus.
PhD thesis, K.U.
Leuven, September 1991.
11] Erik Sandewall.
Non-monotonic entailment for reasoning about time and action Part I : Sequential actions.
Research Report LiTH-IDA-R-8827, Linkoping University, September 1988.
12] Murray Shanahan.
Prediction is deduction but explanation is abduction.
In Proc.
of the 11 th Int.
Joint Conference on Artificial Intelligence (IJCAI), pages 1055{1060, 1989.
13] Yoav Shoham.
Temporal logics in AI: Semantical and ontological considerations.
Artificial Intelligence, 33:89{104, 1987.
14] Mark E. Stickel.
Automated deduction by theory resolution.
Journal of Automated Reasoning, 1:333{355, 1985.
Event Tracking for an Intelligent Automated Agent Milind Tambe and Paul S. Rosenbloom Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 Email: {tambe, rosenbloom}@isi.edu Abstract: In a dynamic, multi-agent environment, an automated intelligent agent is often faced with the possibility that other agents may instigate events that actually hinder or help the achievement of its own goals.
To act intelligently in such an environment, an automated agent needs an event tracking capability to continually monitor the occurrence of such events and the temporal relationships among them.
This capability enables an agent to infer the occurrence of important unobserved events as well as obtain a better understanding of interaction among events.
This paper focuses on event tracking in one complex and dynamic multi-agent environment: the air-combat simulation environment.
It analyzes the challenges that an automated pilot agent must face when tracking events in this environment.
This analysis reveals some novel constraints on event tracking that arise from complex multi-agent interactions.
The paper proposes one solution to address these constraints, and demonstrates it using a simple re-implementation of an existing automated pilot agent.
1.
Introduction An automated intelligent agent pursuing its goals in a dynamic, multi-agent environment often encounters a large number of events that significantly impact the actions it takes to achieve its goals.
Some of these events may be instigated by the agent itself.
Others may be instigated by other agents as they pursue their own goals, which may conflict or coincide with the goals of this agent.
As time marches on, these events continue to unfold.
To act intelligently in a world that is rapidly moving by, the automated agent needs to monitor the occurrence of events in its world and monitor the temporal relationships among them (e.g., the particular sequence in which they occur).
This information is essential for a variety of reasons.
For instance, this information can be used to infer the occurrence of important unobserved events.
Consider the following example from the simulated air-combat domain [5].
This domain involves simulated air combat, where  the intelligent agents act as automated pilots for the simulated aircraft.
These automated pilots will take part in exercises with human fighter pilots, where they will aid in tactics development and training.
For effective performance in this domain, these automated pilots must, among other things, continually monitor events in their environment.
For instance, one crucial event is an opponent's firing a missile at an automated pilot's aircraft, threatening its very survival.
Yet, the automated pilot cannot directly see the missile until it is too late to evade it.
Fortunately, the automated pilot can monitor the opponent's sequence of maneuvers, and infer the possibility of a missile firing based on them, as shown in Figure 1.
The automated pilot is in the dark-shaded aircraft and its opponent in the light-shaded one.
x  (a)  (b)  (c)  Figure 1: Inferring a missile firing event.
Suppose, initially, the two aircraft are headed as shown in Figure 1-a.
After reaching her missile firing range, the opponent turns her aircraft to attack heading (a point slightly in front of the automated pilot's aircraft, as shown by a small x in Figure 1-b).
In this situation, the opponent fires a missile.
While the automated agent cannot observe this missile, based on the opponent's turn, it can infer that the opponent may be attempting to achieve attack heading as part of her missile firing behavior.
Unfortunately, at this point, it cannot be certain about the opponent's missile firing, at least not to an extent where trained fighter pilots would infer a missile firing.
However, if the opponent subsequently executes an Fpole maneuver then that considerably increases the likelihood of a missile firing.
This maneuver involves a 25-50 degree turn away from the attack heading, as shown in Figure 1-c (it is executed after firing a missile to provide radar guidance to the missile, while slowing the closure between the two aircraft).
While at this point the opponent's missile firing is still not an absolute certainty, its likelihood is high enough, so that trained fighter pilots react as though a missile has actually been fired.
The automated pilot must react in a similar manner.
Thus, if the opponent engages in this sequence of turns and changes in heading after reaching her missile firing range, then the automated agent can infer a missile firing.
The above example illustrates that an automated pilot needs to continually monitor events in its world, such as the opponent's turns and her (inferred) missile firing behavior, and record information about the temporal relationships among them, so as to react to them appropriately.
We refer to this capability as event tracking.
An event here may be considered as any coherent activity over an interval of time.
This event may be a lowlevel action, such as an agent's Fpole turn, or it may be a high-level behavior, such as its missile-firing behavior, possibly inferred from a sequence of such turns.
The event may be internal to an agent, such as maintaining a goal or executing a plan, or external to it, such as executing an action.
The event may be instigated by any of the agents in the environment, including the agent tracking the events, or by none of them (e.g., a lightning bolt).
The event may be observed by an agent, or simply inferred.
Tracking any one of these events refers to recording that event in memory, recording the temporal relationship of that event with other events, and monitoring the progress of that event.
To understand event tracking in more detail, it is useful to view it from another perspective.
In particular, event tracking can be viewed as answering queries of the following form: given an event E which consists of a set of subevents {E1, E2,...EN} and a set R = {(Ei r1 Ej),  (Ei r2 Ek)...} of temporal relationships among the sub-events, does E occur in the world?
That is, does the set {E1, E2,...EN} occur in the world so as to satisfy the relationships in R?
While this query type may appear limited in scope, there are at least two degrees of freedom that make it powerful, enabling it support a variety of capabilities.
The first degree of freedom is that there are no restrictions on the event type E: it can be any one of the variety of event types (higher/lower-level, observed/unobserved, etc.)
mentioned above.
This allows an automated agent to infer events related to the behaviors of other agents.
Thus, the event E may be opponent's missile firing behavior, and the relevant query may check if an opponent has engaged in a particular sequence of turns and changes in heading, after reaching her missile-firing range.
Alternatively, a query may involve events instigated by different agents.
For example, a query may check if the automated pilot fired its missile before the opponent fired a missile at it.
Responses to such queries allow an agent to better understand event interactions so as to react to them appropriately.
The second degree of freedom involves the time at which a query can be presented.
In the most unrestricted form, a query about an event E may be presented at any time before, during or after E's occurrence.
In this paper, we will restrict this degree of freedom: queries about E may be presented only during the occurrence of that event.
We will refer to these restricted queries as local queries, and the unrestricted queries as non-local queries.
Non-local queries may be used in service of planning and post-hoc explanation, and require that the system either maintain a long-term memory of past events or be capable of longterm prediction of future events.
Thus, the event tracking problem can be seen as covering a broad spectrum of problems.
One aspect of this problem [?]
related to inferring events based on other events [?]
is closely related to plan-recognition [6, 9, 3, 4] and model tracing [2].
Another aspect of this problem [?]
related to maintenance of temporal relationships among  events [?]
corresponds to temporal reasoning [1].
Other aspects of the problem, corresponding to non-local queries, are related to planning and explanation.
One possible approach to the event tracking problem then is to address all its different aspects individually.
An alternative approach is to treat event tracking as a single problem with a single, unified solution.
We believe it is reasonable to work towards such a unified solution, since it may be able to exploit the interdependencies involved among the different aspects of this problem.
Therefore, this is the approach that we take in this paper (although at present the scope of the problem is restricted to local queries).
The rest of this paper is organized as follows: Section 2 analyzes the requirements for event tracking in the air-combat simulation domain, revealing some challenging constraints.
Section 3 proposes a solution for event tracking that addresses these constraints.
The key idea is to exploit the similarity among the characteristics of the different agents.
This solution is demonstrated using a simple reimplementation of an automated pilot agent for air-combat simulation.
This automated pilot is based on a system called TacAir-Soar [5], which is developed within Soar, an integrated problem-solving and learning architecture [8].
For the purposes of this paper, we need to focus only on Soar's problem space model of problem solving.
Very briefly, a problem space consist of states and operators.
An agent solves problems in a problem space by taking steps through it to reach a goal.
A step in a problem space usually involves applying an operator in the problem space to a state.
This operator application changes the state.
If the changes are what are expected from the operator application, then that operator application is terminated, and a new operator is applied.
If the changes do not meet the expectations, then a subgoal is created.
A new problem space is installed in the subgoal to attempt to achieve the expected effects of the operator.
2.
Constraints on Event Tracking The primary constraint on event tracking in air-combat simulation arises from the fact that this is a dynamic environment, where agents continually interact.
This continuous interaction implies that the agents cannot rigidly commit to performing a fixed sequence of actions.
Instead, they need high behavioral flexibility and reactivity in order to achieve their goals.
For example, suppose an automated pilot is tracking an opponent's turns and changes in heading while the opponent is executing its maneuver to fire a missile (see Figure 1).
Suppose just before the opponent fires its missile (Figure 1-b), the automated pilot suddenly turns its own aircraft.
In response, the opponent may need to turn its aircraft again before firing its missile, and its Fpole (Figure 1-c) may be executed in a different manner as well.
(For detailed examples of agent interactions in this domain, see [10].)
This dynamic interaction among the agents leads to the primary constraint on event tracking in this domain: an agent must be able to track highly flexible and reactive behaviors of its opponent(s).
This is a challenging constraint [?]
previous investigations in the related areas of plan/situation recognition [6, 9, 4, 3] and model tracing [2] have not addressed this constraint.
In particular, plan recognition models have not been applied in such dynamic, interactive situations, and hence do not address strong interactions among agents and the resulting flexibility and reactivity in agent behaviors.
A second related constraint on event tracking here is that it must occur in real-time and must not hinder an agent from acting in real-time.
For instance, in Figure 1, if the automated pilot does not track the missile firing event in real-time or does not react to it in real-time, the results could be fatal.
The third and final constraint on event tracking is that agents must be able to expect the occurrence of unseen, but on-going events.
This constraint arises from the weakness of the sensors in this domain [?]
an agent must sometimes track opponent's actions even though they are not visible on radar.
In particular, an opponent may drop off (become invisible from) the automated pilot's aircraft  during the combat.
In such situations, expectations about the opponent's location can help in quickly re-establishing radar contact with her.
These constraints rule out a variety of solutions for event tracking.
In fact, as a first try, we attempted to address the event tracking problem by explicitly recording in memory all of the events and all of the temporal relationships among them.
In the implementation of this solution in TacAirSoar, if a new event EN+1 was seen to occur after (or at the same time as) N events E1,...,EN, then this was stored in TacAirSoar's memory using N explicit records of the form: after(Ei, EN+1) (or same-time(Ei, EN+1)).
Unobserved events such as missile firings were inferred by using pre-compiled queries, where each query listed a sequence of events {E1,..EN} and the temporal relationships among them.
However, this solution could not satisfy the constraints outlined above.
More specifically, the solution only recorded events that had already occurred, and did not generate expectations.
It also caused a slowdown [?]
the automated pilots were unable to function in real-time.
More importantly, this solution ran into a problem in inferring unobserved events.
Basically, a small number of pre-compiled queries were insufficient to capture the range and complexity of events resulting from the highly flexible and reactive agent behaviors.
The next section proposes an approach for event tracking that addresses all of the constraints outlined above.
3.
A Solution for Event Tracking The key idea in the proposed solution for event tracking is based on the following observation.
All of the agents in this environment possess similar types of knowledge, they have similar goals, and similar levels of complexity in their behaviors.
In particular, consider an automated pilot agent in this environment that requires the ability to track the complex chain of events corresponding to the flexible and reactive actions and behaviors of other agents.
This automated pilot itself instigates an equally complex chain of events corresponding to its  own flexible and reactive behaviors.
Thus, the key idea is that all the knowledge and implementation level mechanisms that the automated pilot itself uses in instigating events may be used in service of tracking events instigated by other agents.
To understand this idea in detail, it is first useful to understand how an agent generates its own flexible and reactive behaviors.
For this we turn to a concrete implementation of an automated pilot agent (call it Ao) in TacAirSoar.
Figure 2 illustrates the problem spaces and operators Ao employs while it is trying to get into position to fire a missile.
In the figure, problem spaces are indicated with bold letters, and operators being applied in italics.
In some problem spaces, alternative operators are also shown (these are not italicized).
In the topmost problem space, named TOP-PS, Ao is attempting to execute its mission by applying the execute-mission operator.
This is the only operator it has in this problem space.
The expected effect of this operator is the completion of Ao's mission, which may be for example to protect its aircraft carrier.
Since this is not yet achieved, a subgoal is generated.
This subgoal involves the EXECUTEMISSION problem-space.
There are various operators available in this problem space to execute Ao's mission, including intercept (to intercept an attacking opponent), fly-racetrack (to fly in a racetrack pattern searching for opponents), etc.
Ao selects the intercept operator [?]
given the presence of the opponent, this is the best option available.
Since the intercept is not yet complete, a subgoal is generated.
This subgoal involves the INTERCEPT problem space, where Ao applies the employ-missile operator.
However, the missile firing range and position is not yet reached.
Therefore, Ao applies the get-missile-lar operator in a subgoal.
(LAR stands for launch-acceptability-region, the position for Ao to fire a missile at its opponent).
This subgoaling continues until the application of the start-turn operator in the DESIRED-MANEUVER problem space, which causes Ao to turn.
Later, the stop-turn operator is applied to stop the aircraft's turn  when it reaches a particular heading (called collision course).
This heading will be maintained until missile firing position is reached.
At that time, the expected effect of Ao's get-missile-lar operator will be achieved, and that will be terminated.
EXECUTE-MISSION  INTERCEPT  TOP-PS  EXECUTE-MISSION  FLY-RACETRACK .......  EMPLOY-MISSILE  INTERCEPT  CHASE-OPPONENT .......  GET-MISSILE-LAR  EMPLOY-MISSILE  FINAL-MISLE-MANVER .......  ACHIEVE-PROXIMITY GET-MISSILE-LAR CUT-TO-LS .......  START-TURN  DESIRED-MANEUVER  STOP-TURN ....... .......
Figure 2: Ao's problem space/operator hierarchy.
Thus, by subgoaling from one operator into another a whole operator/problem-space hierarchy is generated.
This organization supports reactive and flexible behaviors given appropriate operator selection and termination mechanisms [7].
For instance, there is a global state shared by all of the problem spaces.
If this state changes so that the expected effects of any of the operators in the operator hierarchy is achieved, then that operator can be terminated.
All of the subgoals generated due to that operator are then automatically deleted.
Ao can also terminate an operator even if its expected effects are not achieved (e.g., if another operator is found to be more appropriate for a given situation).
Since all of the above operators are used in generation of Ao's own actions, they will be henceforth denoted using the subscript own.
For instance, employ-missileown will denote  the operator Ao uses in employing a missile.
Operatorown will be used to denote a generic operator that Ao uses to generate its own actions.
The global state in these problem spaces will be denoted by stateown.
Problemspaces that consist of stateown and operatorown will be referred to as self-centered problem spaces.
The motivation for using this method for denoting states, operators and problem spaces will become clearer below.
3.1.
Tracking Other Agent's Behaviors Given the similarities between Ao and its opponent, the key idea in our approach to event tracking is to use Ao's operator hierarchy to track opponent's behaviors.
To illustrate this idea, we begin with the simplifying assumption that Ao and its opponent are exactly identical in terms of problem spaces and operators at their disposal to engage in the air-combat simulation task.
Thus, Ao can essentially use a copy of its own problem-spaces and operators to track the opponent's actions and behaviors.
We will refer to these copies as opponent-centered problem spaces.
Operators in these problem spaces represent Ao's model of its opponent's operators.
These operators are denoted using the subscript opponent.
Thus, the execute-missionopponent operator is used in modeling an opponent's execution of her mission.
Similarly, operatoropponent is used to denote a generic operator used by the opponent.
The global state in these problemspaces represents Ao's model of the state of its opponent, and is denoted by stateopponent.
We assume for now that Ao can easily generate an accurate stateopponent.
(Note that we will make a few such assumptions in explaining event tracking in this section, and address them later in Section 3.2.
Note also that while this section does not directly describe the operation of an actual implementation, it is based on an actual implementation that will be described in Section 3.3.
Basically, the description presented here will be used to motivate some representational modification leading up to the implementation described in Section 3.3.)
To engage in air-combat simulation, it is possible for Ao to execute the two sets of  problem spaces [?]
self-centered and opponentcentered [?]
in parallel.
This will allow Ao to generate its own actions with the help of selfcentered problem spaces, and track opponent's behaviors with the help of opponent-centered problem spaces.
With the opponent-centered problem spaces, Ao can essentially pretend to be the opponent.
Ao can then track the opponent's behaviors and actions by pretending to engage in the same behaviors and actions as the opponent.
In particular, Ao applies operatoropponent to stateopponent, thus modeling the opponent's actual application of her operator to her actual state.
Since Ao is modeling the opponent's action, operatoropponent does not change stateopponent.
Instead, if the opponent takes some action in the real-world, then that change is modeled as a change in stateopponent.
If this change matches the expected effects of operatoropponent, then that effectively corroborates Ao's modeling of operatoropponent.
Operatoropponent is then terminated.
To understand this in concrete terms, consider the example in Figure 1.
Specifically, consider the situation just at the beginning of Figure 1-a.
Given our assumptions above, Ao can use an opponentcentered copy of its set of self-centered problem spaces from Figure 2 to track its opponent's behaviors.
Here, the execute-missionopponent operator models the opponent's execution of her mission.
Since the opponent's mission is not completed, a subgoal is generated, where the operator interceptopponent is applied.
This operator subgoals into employ-missileopponent and so on, until start-turnopponent is applied to stateopponent.
If the opponent actually starts turning, then the operator start-turnopponent is corroborated and terminated.
The next operator in this problem space is stop-turnopponent.
When the opponent actually stops turning after reaching collision course, then stop-turnopponent is corroborated.
This is the situation in Figure 1-a.
Right at the beginning of Figure 1-b, the opponent actually  reaches the missile LAR, and get-missile-laropponent is corroborated and terminated.
Ao now applies final-missile-maneuveropponent in the EMPLOY-MISSILE problem space.
This subgoals into achieve-attack-headingopponent.
This subgoals into start-turnopponent.
When the opponent actually turns to attack heading as shown in Figure 1-b, achieve-attack-headingopponent is corroborated and terminated.
A new operator from the FINAL-MISSILE-MANEUVERS problem space [?]
push-fire-buttonopponent [?]
is now applied.
This operator predicts a missile firing, but it is known that that cannot be observed.
Hence, push-fire-buttonopponent is terminated even though there is no direct observation to support that termination.
(This corroboration without observation requires the addition of some knowledge that is specific to opponent-centered problem spaces, i.e., not copied from self-centered problem spaces.)
This also corroborates and terminates final-missile-maneuversopponent.
However, the resulting missile firing is marked as not being highly likely.
Following that, an Fpoleopponent operator in the EMPLOY-MISSILE problem space predicts an Fpole turn.
When the opponent executes her Fpole turn in Figure 1-c, the Fpoleopponent operator is corroborated and terminated.
With this Fpole turn, the missile firing is now considered as being highly likely.
Ao may now attempt to evade the missile.
Viewing the above in terms of event tracking, essentially, each operatoropponent is an event E. The suboperators of operatoropponent correspond to the set {E1, E2..EN}, and their sequence of execution corresponds to the temporal relationships R among the events.
Thus, the execution of operatoropponent corresponds to the dynamic generation and execution of the query regarding the event E. This method allows Ao to track events related to behaviors of other agents without pre-compiling all possible queries related to those behaviors.
However, this method does face an interesting challenge when responding to queries involving events  instigated by different agents.
For instance, consider a query that checks if the automated pilot fired its missile before the opponent fired a missile at it.
Responding to such a query requires the execution of an operator in both self-centered and opponent-centered problem spaces, which this method does not directly support.
The next section outlines a solution that facilitates responding to such queries.
3.2.
Addressing Constraints The previous section described a mechanism for Ao to generate its own behaviors while tracking opponent's behaviors: Ao executes two sets of problem spaces [?]
self-centered and opponent-centered [?]
in parallel.
This section analyzes the weaknesses in this mechanism particularly given the constraints on event tracking identified in Section 2.
The first constraint on event tracking was for an agent to track highly flexible and reactive behaviors of its opponent.
The use of opponent-centered problem spaces with operatoropponent and stateopponent helps in partly addressing this constraint (this was the motivation behind this approach to begin with).
In particular, operatoropponent can be activated and terminated in the same flexible manner as operatorown.
There is complete uniformity in the treatment of the two types of operators.
However, one assumption in our description of event tracking with opponent-centered problem spaces is that Ao can generate an accurate stateopponent.
While this seems like a highly problematical assumption at first glance, there are several ways in which this problem is simplified.
First, there are some strong assumptions that Ao can make about its opponent's state based on the "intelligence" information.
This information may include things such as the approximate range of the opponent's radar, the range and types of missiles the opponent's aircraft can carry, and so on.
Based on these strong assumptions, Ao can make further weak assumptions about the opponent's state.
For instance, based on the opponent's radar range, Ao can assume that its aircraft would be visible to the opponent's radar at a particular range.
This is a weak  assumption because information about the opponent's radar range is approximate, and more importantly, the opponent's radar may not be pointed in Ao's direction.
So should Ao assume that it is visible on opponent's radar?
If Ao assumes that it becomes visible on opponent's radar as soon as the range is reached, Ao may not maneuver to gain positional advantage.
On the contrary, as Ao moves closer and closer to the opponent, the chances of the opponent seeing it continually increase, and Ao can commit a serious mistake if it continues to assume that the opponent cannot see it.
Thus, the general problem here is understanding when to make (nor not make) such weak assumptions, without committing serious mistakes.
The solution we are experimenting with is to inject the weak assumption into stateopponent at the point where the opponent indicates (by turning her aircraft) that there is likely some change in her state.
The motivation here is to avoid making the weak assumption too soon, by triggering it with at least some indication of a change of state from the opponent.
The injected assumption is verified by corroborating the resulting operatoropponent with the opponent's actual actions.
For instance, if the weak assumption of Ao being visible to the opponent is injected into stateopponent, the resulting operatoropponent indicates that the opponent is likely to turn to collision course.
If the opponent does indeed turn to collision course, the weak assumption is considered validated.
Besides the weak assumptions, the second major issue in stateopponent is the overhead of computing and maintaining derived information in stateopponent.
For instance, assuming for now that Ao is indeed visible to the opponent's radar, stateopponent needs to be elaborated with the information that the opponent is likely to obtain from her radar.
This includes Ao's heading, altitude, the range between the two aircraft, target aspect from the opponent's perspective (the angle between the Ao's flight path and the opponent's position) the angle off from the opponent's perspective (the angle between the opponent's flight path  and Ao's position) and so on.
Calculating angles such as target aspect, angle off etc from the opponent's perspective can be very expensive.
Additionally, given the dynamic nature of the environment, there is a need to continuously update all of this information to keep it consistent.
For instance, as Ao turns, stateopponent has to be modified to change all of the information on it regarding Ao's heading, target aspect and so on.
The problem Ao faces here is that opponentcentered and self-centered problem spaces are compartmentalized operator hierarchies.
This leads to problems in modeling the strong agent interactions present in this domain.
As one entity changes in one compartment, there is a substantial overhead of keeping all the information consistent with it in (all of the) other compartments.
The solution we are investigating here is to merge the different operator hierarchies into a single compartment, which we will refer to as a world-centered problem space (WCPS for short).
WCPS eliminates the boundaries between different self-centered and opponent-centered problem spaces.
Instead, the different operator hierarchies are maintained within the context of a single WCPS, with a single world state.
This single world state can now allow information sharing between stateown and stateopponent, thus helping to keep it consistent.
For instance, the Ao's range to its opponent is identical to the opponent's range to Ao.
Thus, the range information, which Ao has available on stateown from its radar, can be directly shared with stateopponent.
As this range changes in stateown, it is automatically updated in stateopponent.
The angle off (target aspect) from the opponent's perspective is also shared, since that turns out to be the target aspect (angle off) from Ao's perspective.
WCPS encourages such sharing of information, and thus dramatically reduces the burden of modeling stateopponent.
WCPS also facilitates responding to queries such as the one discussed at the end of Section 3.1, which checks if the automated pilot fired its missile before the opponent.
In WCPS this query can be executed by directly executing a  multi-agent operator, i.e., operatorself-and-opponent, which can operate on both stateown and stateopponent.
The second constraint on event tracking relates to Ao's ability to track events in realtime.
One key impact of this constraint is on the generation of an accurate operatoropponent hierarchy.
In particular, this constrains the amout of time Ao can spend in generating an accurate operator hierarchy: an exhaustive search is definitely ruled out.
This issues is on the top of our list of items for future work.
The third constraint on event tracking was the generation of expectations for an unseen, but on-going event.
In WCPS, the application of an operatoropponent in essence is the expectation for the opponent to execute a certain plan or action.
Thus, this constraint can be addressed in a straightforward manner.
Finally, the key assumption in the previous section was that the automated pilot agent Ao and its opponent are identical.
The main implication of this assumption is that Ao can create a copy of its own operator and problem space hierarchy to model the opponent.
If Ao does have some additional knowledge about how some of the opponent's operators differ from its own, then Ao could use those operators in modeling the opponent, instead of using copies of its own operators.
If Ao does not have this additional knowledge, then Ao will need to model its opponent with incomplete information, or to learn that information from observation of the opponent's actions.
3.3.
A Prototype WCPS-based Agent An important test of the WCPS model is its actual application in a dynamic, multi-agent environment.
The task of developing an automated pilot for the air-combat simulation domain is tailor-made for this test.
The development of automated pilots in this domain is currently based on a system called TacAir-Soar [5], which is being developed using Soar.
TacAir-Soar is a non-trivial system that includes about 800 rules.
We have implemented a variant of TacAir-Soar that is fully based on WCPS.
To create this variant, we started with the operators and problem  spaces that are used by a TacAir-Soar-based automated pilot in generating its flexible actions and behaviors.
We then generated (by hand) a copy of these operators and problem spaces to model the automated pilot's opponent within a single WCPS.
The result is an implementation that is able to track events while generating expectations.
It is also promising in terms of being more robust in tracking events that the current TacAir-Soar implementation.
The implementation tracks opponent's action and behavior as described in Section 3.1.
Simultaneously, by using WCPS it reduces the overheads as outlined in Section 3.2.
The implementation currently only works in simple single opponent situations.
Work on extending the implementation to more complex situations is currently in progress.
4.
Summary This paper makes two contributions.
First, it presents a detailed analysis of event tracking in the "real-world", dynamic, multi-agent environment of air-combat simulation.
This analysis raises some novel issues for event tracking.
The second contribution is the idea of world-centered problem spaces (WCPS).
WCPS is independent of problem spaces as such [?]
the key ideas are that an agent treats the generation of its own behavior and tracking of others uniformly and that it shares as much information as possible to avoid computational overheads.
WCPS was used in (re)implementing automated pilots for aircombat simulation.
The paper also outlined several unresolved issues in WCPS.
Among them, resolving ambiguity in opponent's actions, learning from observation of opponent's actions, and so on.
We hope that addressing these issues will help in allowing WCPS to perform event tracking in a more robust fashion.
References 1.
Allen, J.
"Maintaining knowledge about temporal intervals".
Communications of the ACM 26, 11 (November 1983).
2.
Anderson, J. R., Boyle, C. F., Corbett, A. T., and Lewis, M. W. "Cognitive modeling and intelligent tutoring".
Artificial Intelligence 42 (1990), 7-49.
3.
Azarewicz, J., Fala, G., Fink, R., and Heithecker, C. Plan recognition for airborne tactical decision making.
National Conference on Artificial Intelligence, 1986, pp.
805-811.
4.
Dousson, C., Gaborit, P., and Ghallab, M. Situation Recognition: Representation and Algorithms.
International Joint Conference on Artificial Intelligence, 1993, pp.
166-172.
5.
Jones, R. M., Tambe, M., Laird, J. E., and Rosenbloom, P. Intelligent automated agents for flight training simulators.
Proceedings of the Third Conference on Computer Generated Forces and Behavioral Representation, March, 1993.
6.
Kautz, A., and Allen J. F. Generalized plan recognition.
National Conference on Artificial Intelligence, 1986, pp.
32-37.
7.
Laird, J.E.
and Rosenbloom, P.S.
Integrating execution, planning, and learning in Soar for external environments.
Proceedings of the National Conference on Artificial Intelligence, July, 1990.
8.
Rosenbloom, P. S., Laird, J. E., Newell, A., and McCarl, R. "A preliminary analysis of the Soar architecture as a basis for general intelligence".
Artificial Intelligence 47, 1-3 (1991), 289-325.
9.
Song, F. and Cohen, R. Temporal reasoning during plan recognition.
National Conference on Artificial Intelligence, 1991, pp.
247-252.
10.
Tambe M., and Rosenbloom, P. S. Event tracking in complex multi-agent environments.
Proceedings of the Fourth Conference on Computer Generated Forces and Behavioral Representation, May, 1994.
Table of Contents 1.
Introduction 2.
Constraints on Event Tracking 3.
A Solution for Event Tracking 3.1.
Tracking Other Agent's Behaviors 3.2.
Addressing Constraints 3.3.
A Prototype WCPS-based Agent  4.
Summary References  i  List of Figures Figure 1: Inferring a missile firing event.
Figure 2: Ao's problem space/operator hierarchy.
ii
A Proper Ontology for Reasoning About Knowledge and Planning Leora Morgenstern  IBM T.J. Watson Research Center P.O.
Box 704, Yorktown Heights, N.Y. 10598 leora@watson.ibm.com will be successful.
Abstract:  Research on the knowledge preconditions problems for actions and plans has sought to answer the following questions: (1) When does an agent know enough to perform an action?
(2) When can an agent execute a multi-agent plan?
It has been assumed that the choice of temporal ontology is not crucial.
This paper shows that this assumption is wrong and that it is very di cult to develop within existing ontologies theories that can answer both questions (1) and (2).
A theory of linear time does not support a solution to the knowledge preconditions problem for action sequences.
A theory of branching time solves this problem, but does not support a solution to the knowledge preconditions problem for multi-agent plan sequences.
Linear time supports prediction, but does not support hypothetical reasoning branching time supports hypothetical reasoning, but does not support prediction.
Since both prediction and hypothetical reasoning are essential components of the solution to the knowledge preconditions problems, no comprehensive solution has yet been proposed.
To solve this problem, we introduce a new temporal ontology, based on the concept of an occurrence that is real relative to a particular action.
We show that this ontology supports both hypothetical reasoning and prediction.
Using this ontology, we dene the predicates needed for the proper axiomatization for both knowledge preconditions problems.
1 Introduction  Intelligent agents not only possess knowledge, but they reason about the knowledge that they possess.
This sort of introspection is particularly crucial for planning.
Agents are not capable of performing every action, so an agent who constructs a plan must reason about his ability to perform the actions in his plan.
Since the ability to perform many actions rests directly upon an agent's knowledge, he must reason about whether he has that knowledge, or how he can get that knowledge.
For example, an agent who plans to perform the sequence of actions: (open up safe, remove money) must know that he knows the combination of the safe in order to predict that his plan  There has been a fair amount of research in the eld of knowledge and planning in the last 15 years.
Most of this work (Moore, 1980], Konolige, 1982]) has focussed on the knowledge preconditions problem for actions: what does an agent need to know in order to perform an action?
This question is only part of the story, however: if an agent does not know enough to perform an action, he will presumably not just drop his goal.
Instead, he will either plan to get the information, possibly by asking another agent, or by delegating the task to another more knowledgeable agent.
In either case, he will have to construct a more complex multi-agent plan.
This gives rise to the knowledge preconditions problem for plans: what does an agent have to know in order to successfully execute a plan?
For example, if I don't know the combination of the safe, I may ask Bob to tell me the combination.
To predict that this plan will work, I must know that Bob knows the combination, that he will tell it to me, and so on.
Presumably, the knowledge preconditions for this sort of plan are weaker than for my plan to open the safe { but they are di cult to make explicit.
In Morgenstern, 1988], we studied the knowledge preconditions problem for plans in detail, and furnished axioms giving su cient knowledge preconditions for various sorts of plans, including sequences, conditionals, and loops.
However, as noted there, these axioms are overly strong they entail that an agent has su cient knowledge to execute a plan even when intuition tells us otherwise.
For example, what seems to be a straightforward or \natural" way of axiomatizing the knowledge preconditions for plan sequences entails that an agent could always do a sequence of actions as long as he could perform the rst action, but (and this is the crucial point) he never actually did.
This was true even if the second action was impossible to perform.
The theory is still valid for forward reasoning planning however, it is clearly undesirable to have a theory that legitimates degenerate plans.
This paper addresses and solves this problem.
We had previously suggested that the problem was most probably due to the use of linear time, and claimed that using a more sophisticated temporal ontology such as branching time would solve the prob-  lem.
As we will show in this paper, branching time is also not su cient.
We need to construct a new and richer underlying temporal ontology.
This paper is structured as follows: We briey describe the logical language used, and give a natural language characterization of the solution to the knowledge preconditions problems.
Next we show that formalizing these axioms in a linear theory of time will not work.
The following section shows that the seemingly obvious solution { recasting these axioms using a branching theory of time { does not work either.
Finally, we introduce a new temporal ontology, called relativized branching time, which takes elements of both branching and linear times and is based on the notion of the \most real" world, relative to a particular action.
We show that this temporal ontology can be used to construct a correct theory of knowledge preconditions for actions and plans.
2 The Logical Language  We will be working in a logical language L, an instance of the rst order predicate calculus.
(What follows is terse and incomplete, due to space considerations.
L is modeled on the logic used in Morgenstern, 1988] ) L is distinguished by the following features: 1] L contains a 3-place predicate Know.
Know(a,p,s) means that agent a knows the sentence represented by the term p in the situation (\time-point") s. When we say that the term p represents a sentence, we are indicating that a quotation construct is present in L. Thus: 2] L allows quotation.
We can use a term or a w in L and talk about that term or w in L. We do this by associating with each term or w of L the quoted form of that term or w.
In general, we will denote the term representing a w or term as that w or term surrounded by quotation marks.
Some notes on quotation: unrestricted use of quotation can lead to paradox Montague, 1963] some sort of resolution is necessary.
Here we choose: 3] L is interpreted by a three-valued logic, which is transparent to the user and ignored in the remainder of this paper.
4] Quantication into quoted contexts is a somewhat messy enterprise, involving some sort of quasi-quotes.
We use the notation of Davis, 1990] : The delimiters ^^ and ## are used when the variables that are quantied into quoted contexts range over strings @ is used for variables that range over objects other than strings.
The partial function h maps a string onto the term it represents it is abbreviated as the .
(the period).
Those unfamiliar with quasi-quotation should just ignore these symbols.
As we have indicated, and will be arguing at greater length, the choice of a temporal ontology will be crucial for our endeavor.
Nevertheless, there are some elements that will be present in any choice.
They  are: 5] The basic building block is the situation, or time point.
(How these points are organized is the crux of the dierences between approaches).
Intervals of time are indicated by a pair of time-points, the starting time and the ending time.
An action or event is a collection of intervals { intuitively those intervals in which the action takes place.
An event is an action restricted to a particular agent (the performing agent).
The function Do maps an agent and an action onto an event.
Actions and events can be structured using standard programming language constructs.
A plan is any structure of events, e.g., Sequence(Do(Susan(ask(Bob,combination)), Do(Bob(tell(Susan, combination)))) A restricted subset of actions are primitive: - they cannot be further decomposed.
Other actions are complex and are built  up out of primitive actions using our programming language structures.
In all formulas of the theory and metatheory, all variables are assumed to be universally quantied unless otherwise indicated.
3 What We Want to Say  In English, the solution to the knowledge preconditions problem for actions can be stated as follows: it is assumed that all agents know how to perform the basic action types of primitive actions.
In order to know enough to perform a primitive action, then, one must only know what the parameter of the action is.
That is, one must know of a constant equivalent to the parameter.
Thus, for example, suppose that dial is a primitive action.
Then one knows how to dial the combination of a safe if one knows of a sequence of digits equivalent to the combination of the safe.
The knowledge preconditions for complex actions are given recursively in terms of the knowledge preconditions for primitive actions.
If an action is complex, an agent must explicitly know its decomposition into primitive actions, and know how to perform the decomposition.
Moreover, if one cannot perform an action, one generally constructs some multiple agent plan whose end result is the achievement of the original goal.
The solution to the knowledge preconditions problem for plans can therefore be stated as follows: An agent knows how to execute a plan if he knows how to perform all of the actions of the plan for which he is the performing agent, and can predict that the other agents in the plan will perform their actions when their time allows.
For example, Susan can execute the plan sequence sequence(do(Susan, ask(Bob, combination)), do(Bob, tell(Susan, combination))) if Susan can ask Bob for the combination, and she knows that as a result of her asking him for the combination, he will tell it to her.
Note that in order for Susan to predict that Bob will tell her the combination, she must know that Bob in fact knows it, and that he is willing to  share the information.
The above natural language description is a succinct summary of the observations of Moore, 1980] (for primitive actions) and Morgenstern, 1988] (for complex actions and multi-agent plans).
The di culty now is in formalizing this { correctly { within a formal logic.
It is necessary to formalize prediction { knowing that an event will happen in the future { and the notion of vicarious control { controlling a plan even if you are not involved in it.
The problem addressed in this paper arises in the characterization of the knowledge preconditions for complex plans in terms of primitive plans.
We focus here on sequences of plans.
We would like to say that an agent knows how to perform a sequence of actions if he knows how to perform the rst action, and as a result of performing the rst action, he will be able to perform the second action.
Similarly, an agent knows how to execute a sequence of plans if he can execute the rst, and as a result of the rst plan's occurrence, he can execute the second.
We turn to the formalization of these principles in the next section.
4 Diculties With Linear Time  One of the simplest ways to view time is as a straight line - i.e., the standard time line of school history books.
There is a total ordering on time points or situations.
We call this representation of time \linear time."
An interval of time is a segment of the time line as mentioned in Section 2, intervals are denoted by their start and end points.
An action is a collection of intervals Occurs(act1,s1,s2) is true i (s1,s2) is an element of act1.
The knowledge preconditions for primitive actions are omitted here.
They can be found in Morgenstern, 1988].
The axiom for one simple case can be found in this paper's appendix.
We focus here on complex actions.
Recall that we would like to say that an agent knows how to perform a sequence of act1, act2 if he knows how to perform act1 and knows that as a result of performing act1, he will know how to perform act2.
A reasonable try at the knowledge preconditions axiom for action sequences might thus be: Axiom 1: (Knows-how-to-perform(a,act1,s1) & (Occurs(do(a,.act1),s1,s2) ) Knows-how-to-perform(a,act2,s))) ) Knows-how-toperform(a,`sequence(^act^ ^act2^)',s1)  Despite this axiom's plausibility, it does not say what we want.
It allows agents to know how to perform some very odd action sequences.
In particular, it entails that an agent knows how to perform a sequence of two actions if (s)he knows how to perform the rst act but does not perform this act { even if (s)he doesn't know how to perform the second act!
For example, consider the agent Nancy Kerrigan, the  Figure 1: McDermott's branching time.
Real chronicle in bold  action sequence (ice skate, build atom bomb) , and the situation S1 representing January 7, 1994.
It is clear that on January 7, Nancy Kerrigan knew how to ice skate.
We know, however, that due to injuries, she did not skate on that day.
Then the statement  Knows-how-to-perform(Kerrigan,`sequence(ice skate, build atom bomb)', S1 is true, since the second con-  junct of the left-hand side of the axiom is vacuously true.
The problem, when we examine this anomaly more closely, seems to be that material implication is being used to capture the notion of \as a result of performing action 1."
The truth is that material implication is quite dierent from, and much stronger than, the notion of result.
This is the reason it is so much more di cult to modify Axiom 1 than one might suppose.
It is not merely that we have somehow missed something in the formalization.
The problems inherent in material implication have appeared in many suggested modications of this axiom as well, since material implication plays a central role in these axioms as well.
This problem strikes a familiar chord.
In fact, there are many types of reasoning, such as counterfactual reasoning, and concepts in temporal reasoning, such as prevention and causality, that would seem to be straightforward to implement, but which fail due to the very strong nature of material implication.
One approach to solving such problems has been to examine these concepts within the framework of a richer ontology.
Often, the ontology chosen has been branching time McDermott, 1982].
We examine the knowledge precondition problems in the context of branching time in the next section.
5 Diculties With Branching Time  In branching time, time points are ordered by a partial order as opposed to a total order.
There is a unique least point, and one cannot have s1 s2 and s3 s2 unless either s1 s3 or s3 s1 (that is, every child has at most one parent).
Thus, while one could visualize linear time as a straight line, the best way to visualize branching time is as a sideways tree (See Figure 1).
Conceptually, the branch points correspond to action choice points each branch represents a dierent action performed.
Following Mc<  <  <  <  Dermott 1982], any linearly ordered set of points (or path), beginning with the least point, and without gaps, is called a chronicle.
There ia s one chronicle that is designated as the \real chronicle" this corresponds to the way the world is.
A time point is called real if it lies on the real chronicle.
An interval is called real if it contains only real time points.
We introduce the predicate Real-occurs:  Definition:  Real-occurs(act,s1,s2) Occurs(act1,s1,s2) & Real( (s1, s2) ).
,  Since we used linear time in the last section, the  Occurs predicate used there corresponds to the Realoccurs predicate of this section.
Axiom 1 is now cor-  rect.
The left-hand conjunct is not vacuously true in the Nancy Kerrigan example, above the axiom now says: if Nancy Kerrigan knew how to ice skate on January 7, and in any possible world resulting from her skating on January 7, she knew how to build an atom bomb, then she knows how to perform the sequence of actions.
In fact, it is safe that assume that in no possible world resulting from Nancy Kerrigan's skating did she know how to build an atom bomb thus she does not know how to perform the sequence of actions.
This is just what we would anticipate.
Indeed, the fact that the axiom now works is to be expected Moore 1980] used branching time (his temporal ontology was a variation of the situation calculus) and was able to correctly formalize knowledge preconditions for action sequences.
The problem now is that branching time cannot be used for formalizing knowledge preconditions for plans.
The reason, briey, is that in order for an agent to reason that he can execute a multiagent plan, the agent must be able to predict that other agents will perform certain actions.
Predicting means knowing that an event will actually occur - i.e., that the occurrence will be part of the real chronicle.
But suppose, now, that an agent, Susan, is reasoning about her ability to execute sequence(pln1, pln2).
E.g., assume that Susan is reasoning about her ability to execute the plan sequence(Do(Susan,ask(Bob,combination)),Do(Bob,tell (Susan,combination))).
We assume that pln1 is a  single action where Susan is the performing agent pln2 is a single action where Bob is the performing agent.
Then Susan must know that she can perform pln1 1 and that as a result of performing pln1, Bob will perform pln2.
That is, she must know that in any possible world resulting from the event Do(Susan,ask(Bob,combination)), Bob will perform Do(Bob,tell(Susan,combination)).
But this is impossible by nature of the denitions: Bob can only really perform pln2 in the one real chronicle, not in every branch in which Susan performs pln1.
Moreover if In order to reason about plan execution, one must reason not only about knowledge preconditions, but also physical and social feasibility.
When all three are satisfied, an agent can-perform an action.
See Appendix.
1  Figure 2: Branching time doesn't support hypothetical  reasoning: Bob doesn't \really" tell Susan the number when Susan asks for it (non-bold segments)  Susan doesn't perform pln1, then Bob's performance of pln2 will only occur in non-real chronicles!
This situation is shown in Figure 2.
Thus, we are now in a situation that is precisely the opposite of the situation that occurred in linear time.
The theory based on linear time is too liberal it entails that agents know how to perform sequences of two actions even if they do not know how to perform the second action.
The theory based on branching time, on the other hand, is too restrictive.
It is virtually impossible to prove, under reasonable assumptions, that an agent can execute a standard sequence of plans, such as asking a friend for a piece of information, and receiving that information.
More formally, consider the following axioms:  Axiom 2:  Can-execute-plan(a,`sequence( ^pln1^,^pln2^)',s1) , Know(a,`Vicarious-control(@a, #pln1#,@s1)',s1) & Know(a,`Occurs(^pln1^,@s1,s2))Vicariouscontrol(@a,# pln2 #,s2)',s1)  Axiom 3:  actors(pln) = f a g & Can-perform(a,`action(@a, ^pln ^ )',s) ) Vicarious-control(a,pln,s)  Axiom 4:  actors(pln) 6= f a g & 9 s2 Real-occurs(.pln,s,s2) ) Vicarious-control(a,pln,s) .
Vicarious-control, in the axioms above, can be  thought of meaning \one of the following: I can do it or it will happen."
That is, one vicariously controls a plan if one can count on it happening.
I can count on my xing myself a scrambled egg in the morning because I know how to perform the action thus, by Axiom 3, I vicariously control it.
I can count on the sun rising this morning because I can predict that it will happen thus, by Axiom 4, I vicariously control it.
Axiom 2 states that I can count on a sequence of  plans if I can count on the rst plan, and as a result of the rst plan's occurrence, I can count on the second.
Now consider the plan sequence  sequence(do(Susan,ask(Bob,comb)), do(Bob, tell(Susan,comb))).
It can easily be seen that under most normal sets of assumptions, Can-executeplan(the above plan) cannot be proven using Axioms  1 through 4.
This is just one anomalous case.
Similar problems occur with conditional plans, and in cases where agents are not directly involved in any aspect of their plan { i.e., when the entire plan consists of actions that have been delegated.
The problem arises whenever one must predict that an action will take place if a piece of a plan has occurred.
6 A Solution That Works: Branching Time With Relativized Real States  Thus far, we have demonstrated that linear time is di cult to use to formalize knowledge preconditions because it does not allow for generalized hypothetical reasoning that branching time is likewise di cult because it emphasizes hypotheticals too strongly and does not allow for generalized prediction.
What we want is a theory that supports both hypothetical reasoning and prediction.
2 That is, we would like to develop a theory in which we can say: given that act1 has occurred, act2 will surely occur.
This \sureness" or \realness" is relative to the action that has occurred.
We call this relativized branching time.
To capture this concept, we modify the ontology of branching time as follows.
We introduce a collec(to be read as \more real tion of partial orders than" ) on branch segments of our tree.
There is a partial order at each branching point  is <r  <ri  i  <r  2 Other, less satisfactory approaches are possible.
We could use linear time, but introduce an explicit predicate Causes and thus eliminate the problems of material implication.
Our axiom on knowledge preconditions for action sequences would then read: (Know-how-to-perform(a,seq(act1,act2),s) & Causes(act1,Know-how-to-perform(a,act2))) ) Know-how-to-perform(a,seq(act1,act2),s) .
But there are several problems with this strategy: We need to give a semantics to Causes.
If we cannot, the theory is somewhat bogus if we reduce Causes to material implication, the problems return through the back door.
Moreover, sometimes the fact that one knows how to perform an action act2 after performing an action act1 does not mean that performing act1 caused the agent to know how to perform act2.
One can imagine a situation in which I know that I will be told the combination of the safe at some point late in the day.
In the meantime, I spend my day chopping wood.
Now, it is perfectly plausible that I will know how to open the safe after I chop wood { but I would not want to say that the wood chopping caused me to know how to open the safe.
Another approach would be to develop an ontology using only \axiomatically possible worlds."
The disadvantages here would be that it would be non-intuitive and hard to modify.
the collection of for all .
Where no confusion will result, we will simply write for .
has the following properties: For each branch point i with n branch segments 1    , 9!
3 1.
1  ;1 +1    <ri  i  <r  b  bn  <ri  <r  bj  bj <r b  bj <r bj  bj <r bj  bj <r bn  (existence and uniqueness of least element under <r ) 2.
8k l = 6 j :bk <r bl :bl <r bk (Other than the least element, branches are incomparable.)
This bj is the \most real branch " at point i.
Intuitively, it is the branch most likely to occur at time i. i  is the unique minimal element in the partial order induced by .
3 Note also that condition (2) may be dropped if we wish to model a world in which there are dierent levels of preferred occurrences relative to some action.
For example, condition (2) would most likely be dropped in a theory that allowed for defeasible reasoning.
If one originally inferred that some action would happen because it was on the most preferred branch, and then had to retract that conclusion, it would be helpful to know which of the remaining branches was most likely to occur, and make new predictions based on this information.
We can use the notion of a most real branch to dene the concept of a most real path at a point s. Specically, dene a path in a tree as a sequence of branches 1 ,    where for each , 2 (1 ; 1), the endpoint of is the starting point of +1 .
Definition: ( 1,    ) is the most real path i for all 2 (1,j) is the most real branch segment relative to 's starting point.
Thus, for example, in Figure 3, the path ( 0 , 2 , 6 , 11, 13, 14) is the most real path at the point 0 because all the branch segments are the most real at their starting points.
On the other hand, the path ( 0 , 2 , 6 , 7, 10) is not most real at 0 because ( 6 , 7) is not the most real branch segment at 6 .
Let 0 be the root of a branching tree structure.
Note that the most real path at 0 corresponds precisely to McDermott's real chronicle.
Our move to a richer temporal ontology has thus lost us nothing in expressivity.
We now extend the relation to range over subtrees in the obvious way.
We thus have the following: Denition of for subtrees: Assume 1 2, where 1 has the endpoints ( 1 ) and 2 has the endpoints ( 2 ).
Let 1 be the subtree rooted at 1 and 2 be the subtree rooted at 2 .
Then 1 2. bj  <ri  b  bj  bi  i  j  bi  b  i  bi  bj  bi  bi  s  s  s  s  s  s  s  s  s  x  s  s  s  s  s  <rs  b  s  s  s  <r  <r  b  b  s s  t  s s  b  s  t  t  s  <rs t  We have imposed the condition of uniqueness for ease and simplicity of presentation but this condition is not strictly necessary.
It is likely that in complex domains with varying degrees of granularity of representation, there can be several most preferred branches.
For example, if Susan asks Bob for the combination, the branch in which he answers her orally and the branch in which he answers her in writing could both be most preferred branches.
We deal with this in the longer version of this paper.
3  Goal(a,act,s) & Can-perform(a,act,s) ) 9 s2 Real-wrt(s,s2) & Occurs(do(a,act),s,s2)  We can now formalize the concept of relativized prediction as follows:  Axiom 4'  actors(pln) 6= f a g& 9 s2 Real-wrt(s,(s,s2)) & Occurs(pln,s,s2) ) Vicarious-control(a,pln,s).
Using this axiomatization of the solution to the knowledge preconditions problem, we can build a theory of commonsense reasoning in which benchmark planning problems can be solved.
As an example, we consider the example of section 3, in which Susan plans to learn the combination of a safe by asking a cooperative agent Bob.
Consider a situation 1.
Assume that Bob in 1 knows the combination of some safe and that Susan knows this fact in 1.
Consider, further, a common set of social protocols governing agents' behavior, as discussed in Morgenstern, 1988] or Shoham, 1993].
Examples of such protocols are: that cooperative agents will accept one another's goals if possible, and that cooperative agents are constrained to tell the truth to one another.
Assume that these protocols hold for Susan and Bob in 1, that Susan and Bob are aware of these facts, and that both obey the S4 axioms of knowledge.
Then we have the following theorem: S  Figure 3: relativized branching time: at each branch-  ing point, there exists a unique preferred branch (in bold).
Note that since (so,s2) is more real than (s0,s19), the tree rooted at s2 is more real than the tree rooted at s19.
See Figure 3 for examples of these denitions.
Using this ontology, we can now introduce the concept of a state that is real relevant to some point in time.
Specically, we introduce the predicate Realwrt(s1,s2), which is given by the following metatheoretic denition: Denition: j= Real-wrt(s1,s2) i s2 is a point on where is the most real branch point originating from s1.
We extend Real-wrt to range over intervals in the obvious way.
Specically: bj  bj  Definition: Real-wrt(s1,(si,sj)) , 8 s 2 (si,sj) Real-  wrt(s1,s)  Those causal rules which have action occurrences in their consequent must now be written in terms of this predicate.
In general, where before we would have: Holds(uent,s1) ) 9 s2 Occurs(act,s1,s2)  we would now have:  Holds(uent,s1) ) 9 s2 Real-wrt(s1,s2) & Occurs(act,s1,s2)  and where before we would have: Occurs(act1,s1,s2) ) 9 s3 Occurs(act2,s2,s3)  we would now have:  Occurs(act1,s1,s2) ) 9 s3 Real-wrt(s2,s3) & Occurs(act2,s2,s3).
In the above transformation rules, the term  Holds(uent,s1) is really just syntactic sugar in fact,  in our notation, the situation is just another argument to the predicate.
Here is an example of a transformation: Where before we had Goal(a,act,s) & Can-perform(a,act,s) ) 9 s2 Occurs(do(a,act),s,s2)  we would now have:  S  S  S  Theorem: Can-execute-plan(Susan, sequence( do(Susan,ask(Bob,comb)), tell(Susan,comb)))  do(Bob,  We sketch the main points of the proof.
Axiom numbers refer to the axioms listed in the appendix.
We rst prove the following lemmas: Lemma1: If A and B are cooperative agents, then A can tell P to B i A knows P. Proof: By Axiom 5, an agent A can perform the ac-  tion of telling P to B i the knowledge preconditions, the physical preconditions, and the social protocols are all satised.
We assume for simplicity that the physical preconditions are satised (Axiom 6).
Moreover, all agents always know how to perform the simple act of uttering a string.
(Axioms 7 and 8).
It remains to satisfy the social protocol.
By Axiom 9, the social protocols are satised i agent A tells the truth { i.e., if he knows P. Thus, if A knows P, the social protocols are satised, and since the knowledge and physical preconditions are satised, he can tell P to B. Conversely, if he can tell P to B, the social protocols must be satised, and thus he must know P. 2 Lemma2: Assume A and B are cooperative agents.
If A asks B to do Act1, and B can do Act1, then B will subsequently perform Act1  Formally,  Cooperative(a,b,s1) & Occurs(do(a,ask(b,act1)),s1,s2) ) 9s3 Real-wrt(s2,s3) & Occurs(do(b,.act1),s2,s3)  Proof: Axiom 10 tells us that cooperative agents adopt one another's goals.
That is, if A asks B, during some interval (s1,s2) to do some act, it is then B's goal in s2 to do this action.
Moreover, we have from Axiom 11 that if an agent has a goal of performing a certain action, and he can perform that action, he will subsequently perform the action.
2 Note that Axiom 11 explicitly uses the concept of relativized realness.
Neither a stronger nor a weaker concept will su ce.
If Axiom 11 had read: Goal(a,act,s) & Can-perform(a,act,s) ) 9 s2 Real(s2) & Occurs(do(a,.act),s,s2) then it would be false.
On the other hand, if Axiom 11 had read: Goal(a,act,s) & Can-perform(a,act,s) ) 9 s2 Occurs(do(a,.act),s,s2)  it would not be strong enough to prove Lemma 2.
Indeed the proof of Lemma 2 depends on the ontology developed here.
It seems unlikely that it could be proven in a standard McDermott-type branching logic.
The proof of the theorem then goes as follows: By protocol (Axiom 14), agents can ask other cooperative agents for information.
Moreover, the physical preconditions and knowledge preconditions are satised (Axioms 12 and 13).
Thus, Susan can perform the rst part of her plan.
Thus, Susan can vicariously control the rst part of her plan (Axiom 3).
We must now show that if she performs this part, Bob will perform the second part.
First we must show that Bob can perform the action of telling Susan the combination.
By assumption, Bob knows the combination in S1.
Moreover, agents do not forget (Axiom 15).
Thus, Bob knows the combination in any situation subsequent to S1.
Therefore, by Lemma 1, he can perform the action of telling Susan the combination in S1.
Now, using Lemma 2, we can show that if Susan asks Bob the combination, he will subsequently tell it to her.
This means that Susan vicariously controls the second part of the plan (Axiom 4') by Axiom 2, Susan can execute the plan consisting of the sequence (Do(Susan,ask(Bob,combination)),Do(Bob, tell(Susan,combination))).
2 Again, this proof will not hold in a branching temporal logic.
Note, however, that the theory is not too powerful.
In particular, it will not entail degenerate plans like Nancy Kerrigan's plan, above.
Thus, the theory based on relativized branching time avoids both the problems of linear time and of standard branching time.
7 Conclusion and Further Directions  In the late seventies and early eighties, many researchers (Allen, 1984], McDermott, 1982]) argued for the importance of a correct ontology of time.
The pendulum shifted somewhat subsequently, with McDermott 1984] arguing that some ontological distinc-  tions were not all that crucial.
In particular, he argued that the dierence between linear and branching time was not that great, and would probably not make much of a dierence in temporal reasoning.
We have shown that, contrary to McDermott's hopes, this distinction is crucial for theories of knowledge and planning, and that in fact, neither ontology is adequate for such theories.
Linear time does not allow hypothetical reasoning, and thus cannot properly handle knowledge preconditions for action and plan sequences.
Branching time can handle hypothetical reasoning, but it cannot handle prediction properly, especially in hypothetical reasoning contexts.
(Recently, Pinto and Reiter 93] have also noted the problems of using standard branching time.)
Thus, those who ignore the issue of ontology do so at their own peril: all researchers who have used standard ontologies for reasoning about knowledge and planning have developed theories that are inadequate in some respect.
We have developed a dierent ontology for time, relativized branching time, which allows for relativized realness.
This allows prediction in hypothetical contexts, and thus allows the proper axiomatization of knowledge preconditions.
The resultant theory can handle standard benchmark problems correctly, while avoiding the anomalies of previous theories.
Relativized branching time appears promising for other research areas as well.
Because it supports certain types of hypothetical reasoning, it may be a suitable ontology for counterfactual reasoning.
In particular, relativized branching time may help give structure to the rather vague concept of \most similar possible worlds" which has been used (see, e.g.
Lewis, 1963]), to explain the semantics of counterfactuals such as \If I had struck a match (at S1), it would have burst into ames."
In our ontology, such a sentence can be analyzed as follows: it is true if given a (typically non-real) branch segment (S1,S2) during which the match is struck, it is true on the most real branch segment of S2 that the match burst into ames.
Most similar can be understood as the most real subtree of the endpoint of a non-real branch.
Such an analysis is very preliminary but suggests promising directions for future research.
8 Acknowledgements:  The author thanks the anonymous reviewers for comments on an earlier draft of this paper.
9 Bibliography  Allen, 1984] Allen, James: Toward a General Theory of Action and Time, Articial Intelligence, vol.
23, no.
2, 1984, pp.
123-154 Davis, 1984] Davis, Ernest: Representations of Commonsense Knowledge, Morgan Kaufmann, Los Altos, 1990  Konolige, 1982] Konolige, Kurt: A First Order Formalizationof Knowledge and Action for a Multi-agent Planning System, J.E.
Hays and D. Michie, eds.
Machine Intelligence 10, 1982 Lewis, 1963] Lewis, David: Counterfactuals, Oxford, 1963 McDermott, 1984] McDermott, Drew: The Proper Ontology for Time, unpublished, 1984 McDermott, 1982] McDermott, Drew: \A Temporal Logic for Reasoning About Processes and Plans," Cognitive Science, 1982 Montague, 1963] Montague, Richard: Syntactical Treatments of Modality with Corollaries on Reexion Principles and Finite Axiomatizability, in Acta Philosophica Fennica, fasc.
16, pp.
153-167, 1963 Moore, 1980] Reasoning About Knowledge and Action, SRI TR 191, 1980 Morgenstern, 1988] Morgenstern, Leora: Foundations of a Logic of Knowledge, Action, and Communication, NYU Ph.D. Thesis, Courant Institute of  Mathematical Sciences, 1988 Pinto and Reiter, 1993] Pinto, Javier and Raymond Reiter: Adding a Time Line to the Situation Calculus Shoham, 1993] Shoham, Yoav: Agent-Oriented Programming, Articial Intelligence, 1993  10 Appendix:  Below, a list of the axioms and denitions used in the proofs of the lemmas and main theorem of Section 6.
All variables are assumed to be universally quantied unless otherwise noted.
Axioms 1 through 4' are taken from sections 4 through 6 of this paper.
Axiom 1: (Knows-how-to-perform(a,act1,s1) & (Occurs(do(a,.act1),s1,s2) ) Knows-how-to-perform(a,act2,s))) ) Knows-how-toperform(a,`sequence(^act^ ^act2^)',s1) Axiom 2: Can-execute-plan(a,`sequence( ^pln1^,^pln2^)',s1) , Know(a,`Vicarious-control(@a, #pln1#,@s1)',s1) & Know(a,`Occurs(^pln1^,@s1,s2))Vicariouscontrol(@a,# pln2 #,s2)',s1) Axiom 3: actors(pln) = f a g & Can-perform(a,`action(@a,^pln ^ )',s) ) Vicarious-control(a,pln,s) Axiom 4': actors(pln) = 6 f a g & 9 s2 Real-wrt(s,s2) & occurs(pln,s,s2) ) Vicarious-control(a,pln,s) .
Axiom 5: Can-perform(a,act,s) , Know-how-to-perform(a,act,s) & Physsat(a,act,s) & Socialsat(a,act,s)  An agent can perform an action if the knowledge preconditions, the physical preconditions, and the social  protocols are all satised.
Axiom 6: Physsat(a,`tell(@b,#p#)',s)  For the sake of this paper, it is assumed that there are no physical preconditions for communicative actions.
In reality, there are a variety of preconditions, including being at the same place as the hearer (or being connected in some way).
Axiom 7: Primitive-act(`tell')  The simple locutionary action of just uttering a string is considered to be primitive, with correspondingly simpler knowledge preconditions.
Axiom 8: Primitive-act(f) ) Know-how-to-perform(a,^f^(^x1,  ,^xn)',s) where all of x1    xn are constants.
An agent knows how to perform any primitive action if all the arguments are constant.
Axiom 9: Cooperative(a,b,s) ) Socialsat(a,`tell(@b,#p#)',s) , Know(a,p,s)  Cooperative agents are constrained to tell the truth.
Axiom 10: Cooperative(a,b,s1) Occurs(do(a,ask(b,info)),s1,s2) ) Goal(b,tell(a,info),s2)  ^  If one agents asks a cooperative agent for information, the second agent will subsequently have the goal of giving over the information.
The above axiom has quite a bit of syntactic sugar in it.
The term \info" is shorthand for what is really going on: Agent a is asking agent b to tell him a string of the form: `Equal(term,p)', where p is a constant.
Agent b adopts the goal of telling him a string of that form.
In Morgenstern 1988], this axiom is presented without any syntactic sugar.
Axiom 11: Goal(a,act,s) & Can-perform(a,act,s) ) 9 s2 Real-wrt(s,s2) & Occurs(do(a,.act),s,s2))  If an agent has the goal of performing an act, and can perform the act, he will perform the act.
Note the crucial use of the Real-wrt predicate.
Axiom 12: Primitive-act(`ask')  Asking is a primitive action.
Axiom 13: Physsat(a,ask(b,info),s)  The physical preconditions of asking for information are always satised.
Axiom 14: Cooperative(a,b,s) ) Socialsat(a,ask(b,info),s)  If agents are cooperating, it is always all right to ask for information.
Axiom 15: Know(a,#p#,s) ) 8 s2  s Know(a,#p#,s2)  This is the axiom of perfect memory.
Agents never forget.
A Constraint Database System for Temporal Knowledge Roman Gross and Robert Marti Institut fur Informationssysteme ETH Zentrum, CH-8092 Zurich Switzerland (gross,marti)@inf.ethz.ch  Abstract  This paper describes how the technology of deductive constraint database systems and constraint query languages can be used to represent and reason with temporal knowledge.
First, we summarize our approach to manipulating constraints over reals within deductive database systems.
This approach is based on the compile-time rewriting of clauses which are not admissible.
Then, we show how the timestamping of facts and rules in temporal databases can be mapped to constraints over reals.
Subsequently, we present a more ecient and elegant approach which is based on a special temporal constraint solver.
Keywords: temporal databases, temporal reasoning, deductive databases, constraint query languages  1 Introduction  Current database management systems ultimately return a set of ground substitutions for the variables occurring in a query, that is, equations of the form X = c where X is a variable and c is a constant.
In other words, every answer corresponds to a tuple of constant values, typically of type integer, real or string.
However, many real-world problems can not be solved by associating constant values to all of the variables e.g.
because insucient information is available in order to compute a precise answer.
Instead, some variables may take part in constraints in the form of complex equations or inequalities, or they may even be completely free.
Problems which naturally give rise to such \partial" answers which are subject to certain constraints include con guration tasks, circuit design and temporal reasoning.
In a similar vein, conventional database systems typically fail to answer queries if at least one of the subqueries has in nitely many solutions.
Hence, the answer set cannot be enumerated as it would be in ordinary systems.
Typical problems of this kind are  periodically recurring events such as the weekly departure of a particular ight.
FlightNr DepDay DepTime `SR100' 1994=12=10 13:05 `SR100' 1994=12=17 13:05 .. .. .. .
.
.
However, the solutions may be subject to certain conditions which can be represented as a  nite collection of constraints.
The tuples in the example above have in common that the DepDay is equal to 1994/12/10 modulo 7 days.
F lightNr DepDay DepT ime `SR100' 1994/12/10 mod 7 days 13:05 Jaar et al.
have successfully merged constraint solving techniques to deal with the above mentioned problems with logic programming 9].
In the last few years, many systems based on the approach of constraint logic programming (CLP) have been implemented, e.g.
CLP(R).
This approach has been extended to temporal annotated CLP 6] to handle temporal constraints as investigated in 13].
A drawback of these systems is that they are main memory based and can only handle a limited amount of data (eciently).
Moreover, these approaches cannot handle concurrent access by several users adequately Therefore, 12] have proposed the concept of a constraint query language (CQL) in order to represent and handle constraints in deductive databases.
CQLs are similar to CLP-languages in that they support the management of non-ground facts and non-allowed rules as well as non-ground answers.
However, to the best of our knowledge, no complete and reasonably ecient implementation of 1a CQL exists with the exception of our own DeCoR system (8], see below).
During the same period, a lot of research has been done in the areas of temporal databases (see e.g.
17, 1]) and temporal reasoning (e.g.
13]).
Temporal databases not only contain information about the 1 DeCoR stands for DEductive database system with COnstraints over Reals.
present state of the real world, but also the history leading up to this state and/or possible future developments.
Most of these eorts are based on the relational model and SQL.
In this paper, we argue that the constraint handling technology of DeCoR can elegantly be applied to managing temporal information, providing the functionality associated with typical temporal database systems.
Moreover, temporal information which is incomplete and/or potentially in nite can be represented by the system.
The paper is structured as follows: Section 2 gives an overview of DeCoR and how it deals with constraints.
Section 3 sketches how temporal knowledge can be represented in the existing DeCoR system, using constraints over reals.
Section 4 presents a more elegant approach which relies on building a special purpose constraint solver which can deal with constraints over both time points (instants) and time intervals.
2 Overview of DeCoR  The DeCoR system is a prototype of a deductive constraint database system implemented as a strongly coupled front-end (written in Prolog) running on top of an SQL-based commercial database product (Oracle).
As a result of this architecture, the DeCoR system features full database functionality (i.e., concurrency control, logging and recovery, authorization and physical data independence).
A DeCoR database consists of a set of clauses (facts and rules) which are mapped to SQL base tables and views in a straightforward way.
Rule bodies and queries are translated into SQL statements which can then be executed by the underlying relational database system (DBMS).
The queries are evaluated in a bottom-up fashion in an attempt to minimize the calls to the DBMS.
In addition to the components of typical deductive database systems, the DeCoR system also contains components to store and handle constraints.
2.1 Deductive Database with Constraints  In the following we assume familiarity with deductive database systems, as e.g.
described in 5, 19, 15].
These systems usually handle equations and inequalities (= 6= <  > fi) on arithmetic terms (with + ; ) only if the variables occurring in these expressions are ground and therefore the arithmetic expressions can be evaluated.
DeCoR generalizes the treatment of arithmetic relations by allowing variables to be non-ground.
Hence, these built-in predicates can be viewed as constraints on potential values for those variables.
Definition 2.1 A constraint is a relation (t1 t2 ) where t1  t2 are arithmetic terms and the symbol  represents any of the symbols (= 6= <  > fi).
Definition 2.2 A generalized clause is an implication of the form  p0 (X0 )  p1(X1 ) : : : pk(Xk ) cl (Xl ) : : : cm (Xm ): where pi are user dened predicates and cj are constraints.
The Xi are vectors of variables.
A generalized clause does not have to be range-restricted.
This de nition follows that given by Kanellakis et al.
12] for a generalized fact.
Definition 2.3 A system of generalized clauses form  a database with constraints, called a constraint database.
Formulas in the DeCoR system may contain conjunctions (), existential quanti ers (9) as well as negation (:).
Disjunctions have to be represented by multiple clauses.
Without loss of generality, we assume in the following that clauses and queries are in standard form, i.e.
all arguments in atoms are variables and each variable occurs at most once in a userde ned predicate, c.f.
8, 10].
Example 2.1 The standard form of the user-dened clause \p(X Y )  q(X 5) r(X Y Y ):" is  p(X Y )  q(X1  Z) r(X2  Y1 Y2) Z =5 X =X1  X1 =X2  Y =Y1  Y1 =Y2 :  2.2 Types of Constraints  In deductive database systems which demand allowedness, all variables become instantiated (ground) during bottom-up evaluation 5].
This is no longer guaranteed in constraint database systems because some variables might only be constrained by inequalities or even completely free.
Therefore, two types of variables are distinguished according to their groundness which can be determined in a bottom-up fashion.
Definition 2.4 A variable X in the body of a clause is ground hgi if   X is bound to a constant c.   X can be bound to a term f(Y1  : : : Yn) by solving the constraints of the clause.
(For this to be possible, the Yi have to be ground.)
X appears in a user-dened predicate at an argument position which is ground.
(The groundness of the arguments is determined by the groundness patterns of the body predicates, see below.)
Otherwise the variable is non-ground hni.
The arguments in the head of a clause inherit the groundness information from the body.
The groundness of the arguments in turn determines the groundness patterns of the predicates.
Example 2.2 The groundness pattern hggngi for a predicate determines that the rst, second and fourth arguments are ground whereas the third argument is non-ground.
Based on the groundness information of the variables, constraints can be separated into evaluable and non-evaluable ones.
Definition 2.5 A constraint C is evaluable if   C is an equation which can be transformed into X = f(Y1  : : : Yn ) where each Yi is ground.
C is an inequality not containing any nonground variable.
Otherwise, the constraint is non-evaluable.
It can easily be seen that evaluable constraints correspond exactly to those allowed in \ordinary" deductive database systems.
As shown e.g.
in 5, 4] these constraints can be translated into selection conditions in relational algebra in a straightforward way.
This is not the case with non-evaluable constraints which have to be manipulated separately.
2.3 Constraint Lifting Algorithm  The constraint database system DeCoR delays the evaluation of non-evaluable constraints until they become evaluable (if ever).
For that purpose, the clauses are rewritten at compile-time in such a way that at run-time, only evaluable parts have to be dealt with 8].
This is achieved by propagating nonevaluable constraints into dependent clauses.
This process is denoted as constraint lifting (CL).
The constraint lifting algorithm (CLA) consists of the following 5 steps which are applied iteratively on each clause C in a bottom-up fashion on the reduced dependency tree.
1.
Rewrite clause C into standard form.
In doing so, all equality constraints become explicit.
2.
For each literal in the body of C, lift the nonevaluable constraints occurring in its respective de nitions into C. 3.
Simplify (solve) the resulting conjunction of constraints.
4.
Split the simpli ed constraints into evaluable and non-evaluable ones.
5.
Fold the evaluable parts (user-de ned predicates and evaluable constraints) into a unique auxiliary predicate which contains as arguments the variables of the non-evaluable part (nonevaluable constraints).
Step 3 of the CLA depends on a domain-speci c constraint solver which has to meet some special requirements 7].
For example, it must be able to deal with variables for which it is only known whether or not they will become ground at run-time.
The DeCoR system contains a solver for constraints over reals based on Kramer's rule and Fourier elimination.
The following example shows the abilities of the CLA in the DeCoR system.
Example 2.3 All facts to the predicate tax rate are supposed to be ground.
They contain the lower and  upper bound of income ranges and the tax rate to be applied.
Min Max Rate 0 5 000 0 5 000 25 000 0:03 25 000 55 000 0:07  tax(Inc Tax)  tax rate(Min Max Rate) Min  Inc Inc < Max Tax = Rate  Inc Min < 10 000:  w w w   (1)  Steps 1 to 3 of the CLA do not aect the rewriting of this clause because tax rate is a base predicate.
Steps 4 and 5 result in  tax(Inc Tax)  tax aux(Inc T ax Min Max Rate) Min  Inc Inc < Max (2) Tax = Rate  Inc: (3) tax aux(   Min Max Rate)  tax rate(Min Max Rate) Min < 10 000:  (2) contains the non-evaluable parts of the original clause and (3) the evaluable ones.
The query ?
{ tax(I T ) is also rewritten by the CLA.
In step 2, the body of (2) is lifted into the query and replaces the predicate tax rate.
The resulting query ?
{ tax aux(I T Min Max Rate)  Min  I I < Max T = Rate  I:  does not have to be processed further and can be answered by  I  T 0  Constraints 0  I I < 5 000 5 000  I I < 25 000 T = 0:03  I  As a consequence of this approach, the run-time query evaluation mechanism of the DeCoR system only has to deal with the evaluable parts of the clauses and hence can be realized using well known techniques developed for deductive databases.
In particular, the usual optimization techniques developed for standard bottom-up evaluation (e.g.
Magic Template 14], Constraint Pushing 16]) can be combined with the CLA.
3 Managing Temporal Knowledge in DeCoR  Managing temporal knowledge can be considered as an application of the general-purpose constraint database system DeCoR.
The DeCoR system provides the functionality to implement most of the  typical features of temporal databases.
Similar to the constraint-based framework of 13], the temporal database should support time points (following 11], we will use the term instant) and time intervals as well as relations between objects of these types.
As mentioned earlier and in contrast to our approach the system described in 13] is main-memory based and therefore it is limited to handle large amount of data.
In this paper we restrict ourselves to relations with only one temporal argument which can be considered as the valid-time of a tuple.
Nevertheless, our ideas can easily be generalized to relations with multiple temporal arguments such as bitemporal relations.
In the following, familiarity with temporal databases as e.g.
described in 17, 1] is assumed.
Moreover, we attempt to adhere to the terminology introduced in 11].
In order to improve readability, a special notation for temporal terms is used: Instant variables (event variables) will be denoted by E1 E2 : : : whereas interval variables will be denoted by T1 T2 : : :.
The start (end) point of a time interval T will be denoted by T s (T e ).
A concrete instant is represented as e.g.
/1994/5/2213:40:13.6.
An instant such as /1994/6/1500:00:00 will be abbreviated to /1994/6/15.
Finally, time intervals are considered as closed at the lower bound and open at the upper one.
The format for such an interval is /1994/6/15 { /1994/6/23).
3.1 Temporal Datatypes and Relations  While the DeCoR system supports the datatypes string, integer and real, only constraints over reals can currently be solved.
To apply constraint solving ability to temporal terms, an instant E can be mapped to a value of type real.
This is done by calculating the number of seconds between E and a reference point, e.g.
/1970/1/1.
Example 3.1 The predicate rate shows the instant and the exchange rate between two currencies rate(=1993=4=23  13:45:12:4 `US$' `SFR' 1:14) m  rate(7:251183e + 08 `US$' `SFR' 1:14) A consequence of the above mapping is that the best precision is obtained for instants near the reference point.
This is usually not a problem because most databases contain instants within a few decades only or do not need a temporal granularity smaller than microseconds.
In historical databases, each fact is usually timestamped with a valid time interval which represents the knowledge that the fact is true at every instant within this interval.
The distinction between a valid time interval and all the instants within the interval leads to two dierent representations of intervals in the DeCoR system.
The implicit representation emphasizes the validity of the fact at every instant in the interval.
The explicit representation emphasizes the  time interval and gives direct access to the bounds of the interval.
Definition 3.1 The implicitsrepresentation of a fact p(X) valid in the interval T {T e ) is p(X E )  T s  E  E < T e: The explicit representation of the same fact is p(X T s  T e ): The advantage of the implicit representation is that the temporal conjunction (p(X E ) q(X E )), which requires the intersection of the valid times associated with p and q, can be directly performed by the constraint solver of the constraint database system.
On the other hand, it is not possible to extract the bounds of the interval.
Hence, it is not possible to calculate the duration of the interval.
This drawback disappears if the intervals are represented explicitly.
However, this representation requires that temporal relations have to be de ned explicitly too.
The advantages and disadvantages of these two representations are similar to those of instants respectively time intervals.
(We refer to 1] for the discussion of time intervals versus instants and further references.)
In contrast to \ordinary" deductive database systems, the temporal relations can easily be represented in a constraint database system such as DeCoR.
As a consequence, the explicit representation of time intervals is preferred.
The following three examples serve to convey the idea how temporal relations can be realized as clauses in a constraint database.
Example 3.2 The interval T3s {T3e ) is the s(none  empty) temporal intersection of the intervals T1 {T1 ) and T2s {T2e )  =  inter(s T1se T1es T2se T2es T3se T3e ):  s= s e e inter(T1  T1  T2  T2  T1  T1 )  T1 fi T2  T1  T2 : inter(T1s  T1e T2s  T2e  T1s  T2e )  T1s fi T2s  T1e > T2e : inter(T1s  T1e T2s  T2e  T2s  T1e )  T1s < T2s  T1e  T2ee : inter(T1s  T1e T2s  T2e  T2s  T2e )  T1s < T2s  T1e > T2 : Example 3.3 The relation T1s {T1e ) before T2s{T2e ) can be written as  before(T1s  T1e  T2s T2e )  T1e < T2s : Example 3.4 The duration D of an interval calculated in seconds and can be written as  T  is  duration(T s  T e  D)  D = T e ; T s : The temporal relations can be used as ordinary user-de ned predicates.
This can be seen at the following example adapted from 1].
Example 3.5 Supposing a system contains thes timee  stamped relation works in(EmpNo DepNo T  T ), the query: \In what period (T 3s T 3e)) did the person with the employee number 1354 and 245 work in  the same department (Dep) and how many days (D) were this?"
would be written as: ?
{ 9 T1s T1e T2s T 2e S  (works in(1354 Dep T1s T 1e) works in(245 Dep T2s T 2e) inter(T1s T 1e T2s T2e T3s T 3e) duration(T3s T3e S) S = 86400  D): The last constraint S = 86400  D is required because the predicate duration calculates the dierence of T3s and T3e in seconds (S) and not in days (D) as demanded in the query.
If the predicate works in contains ground facts only, all variables in the query will be ground and hence all constraints lifted from inter and duration turn out to be evaluable.
However, this query will return a partially instantiated answer if there is a person for whom only the starting point of his or her employment in a department is known.
Such an answer cannot be returned by conventional temporal database systems because they do not have a constraint component.
This weakness of conventional system can not be removed even by introducing a value \forever".
3.2 Temporal Reduction (Coalescing)  As pointed out e.g.
in 2, 3, 1], it is necessary for temporal complete systems to allow coalescing time intervals for value-equivalent facts (facts which are identical with the exception of their temporal arguments).
They refer to this operation as temporal reduction.
Top-down constraint handling systems such as described in 6, 13] do not coalesce value-equivalent facts because they are tuple oriented.
Hence, in contrast to our approach, these systems are not temporally complete 3].
As shown in 3] such systems are less expressive than temporally complete ones.
Example 3.6 If a historical database contains the timestamped relation works in Name Dep  Start  End  `Mary' `R&D' 1982=01=01 1988=01=01 `Mary' `Eng' 1988=01=01 1995=01=01 it is necessary to coalesce the timestamps to generate the intended answer to the query \Who worked more than 10 years in the company".
Of course, we have to perform temporal reduction in our representation of a temporal database as well.
Because the reduction operator is second order and because it is not integrated in the underlying DeCoR system, the reduction has to be programmed explicitly for every user-de ned predicate.
In the following, the reduction scheme for a generic predicate p(X T s  T e ) is presented.
The reduction requires two additional relations.
p clos(X T s  T e ) contains the transitive closure obtained by pairwise coalescing the intervals pertaining to value-equivalent  tuples.
The relation p red(X T s  T e ) contains the temporally reduced facts.
p clos(X T s T e )  p(X T s  T e ): p clos(X T1s T2e )  p(X T1s  T1e ) p clos(X T2s T2e ) T1s < T2s  T2s  T1e  T1e < T2e : p red(X T s T e )  p clos(X T s T e ) :9T1s T1e (p clos(X T1s  T1e ) T1s  T s  T e  T1e  :(T1s = T s  T e = T1e )):  3.3 Assessment  The typical functionality associated with temporal database systems can be implemented as an application of the DeCoR system.
Moreover, the possibility to manipulate partially instantiated facts and answers as supported by the DeCoR system can be exploited nicely.
Unfortunately, this approach has some serious drawbacks.
First, it is not very ecient because the CLA has to lift many constraints.
In particular, each temporal conjunction results in the lifting of four constraint parts.
This overhead is not acceptable for such a frequent operation.
Second, as seen in the examples above, it is clumsy to explicitly write two arguments for every time interval.
Both drawbacks are primarily due to the facts that time intervals are represented by two points and that the database system knows nothing about the special semantics of these two arguments as interval bounds.
4 A Temporal Extension of DeCoR  The drawbacks mentioned above disappear if temporal semantics are directly built into the underlying database system.
This is done in the TDeCoR system which is an extension of the constraint database system DeCoR.
The extensions consist of temporal datatypes, constraints relating temporal and nontemporal variables and the incorporation of the reduction algorithm.
The two main goals are improving (1) the eciency of the evaluation of temporal queries and (2) the readability of temporal clauses and queries.
4.1 Constraints over Temporal Domains  The DeCoR system is extended by the two datatypes instant and interval.
Constants of one of these types are denoted as described at the beginning of Section 3.
In contrast to the ChronoLog system 2, 1], there is no special syntax for temporal arguments.
However, as shown below, the uni cation of temporal arguments has a special semantics.
We mention in passing that this design decision supports the de nition of multiple temporal arguments, e.g.
to represent valid and transaction time.
T1 precedes T2 T1 follows T2 T1 contains T2 T1 equals T2 T1 overlaps T2  Table 1: interval  interval constraints T1 before T2 T1 starts T2 T1 during T2 T1 ends T2 T1 after T2  E before T E starts T E during T E ends T E after T  Table 2: interval  interval or instant  interval constraints Similar to the constraints (= 6= > < fi ) between terms of type real currently supported in the DeCoR system, we introduce the temporal constraints shown in Tables 1 and 2 (as suggested in 13]).
As seen in Table 2, some constraints are overloaded in the sense that they represent relations between two intervals as well as between an instant and an interval.
For example, the constraint starts can not only be used to relate intervals with the same start point, but also to extract or set the start point of an interval.
In addition to the above constraints, we introduce relations to extract dierent properties of an instant (see Table 3).
year(E  I) month(E  I) day(E  I) Ith day of the month weekday(E  I) Ith day of the week hour(E  I) minute(E  I) second(E  I) Table 3: instant  integer constraints Note that constraints are relations rather than functions and can therefore be used in a bidirectional way.
Example 4.1 In the query \in which month is Peter  born" the instant argument E is the given argument for predicate month ?
{ 9E birthday('Peter' E ) month(E  Month): whereas in the rule \Paul has to present the progress of his work every Monday" the instant argument E is not known: duty('Paul' 'present work' E )  weekday(E  1):  As seen in the example above, it is possible to express periodical knowledge in the TDeCoR system.
(Indeed, the constraints listed in Table 3 are somewhat similar to the modulo relations of 18].)
The constraint duration (interval  integer  unit) calculates the duration of an interval.
The third argument which has to be given determines the granularity in which the duration is measured.
Example 4.2 The constraint duration can be used to dene the span between the two instants E1 , E2 to be 6 month  starts T  duration(T  6 month) E2 ends T The constraint intersect(T1,T2,T3) determines the explicit intersection T3 of two time intervals T1 and T2.
In addition to the above datatypes and constraints, special syntax expresses which parts of a formula have to be temporally coalesced.
The TDeCoR system does not make an implicit reduction because in some cases, the user does not want to perform a reduction.
Moreover the system is more ecient if the coalescing is performed only on demand.
Following 1], the formula F which has to be coalesced must be enclosed by braces fg.
(If multiple temporal arguments are present the user has to specify which ones have to be reduced by supplying a second argument within the braces) E1  Example 4.3 The query \Who worked in our company for more than 25 years" needs coalescing ?
{ 9 T  Y (f(9Dep works in(Emp Dep T )) T g  duration(T  Y year) Y fi 25): The reduction algorithm realized in the TDeCoR system is similar to the approach described in 2].
In addition, it is extended in a straightforward way to temporally reduce facts with multiple temporal arguments in each user speci ed direction.
This is for example necessary to reduce bitemporal relations.
4.2 Handling of Temporal Constraints  The constraint lifting algorithm presented in Sect.
2.3 is not restricted to constraints over variables of type real.
Indeed, similar to the CLP scheme 9], the CLA depends on a concrete domain.
Ultimately, the constraint solver invoked in step 3 of the CLA only determines which constraints can be simpli ed and solved.
The integration of the constraint solver of DeCoR with a temporal constraint solver will be investigated in the next section.
In order to apply the CLA on clauses with temporal constraints, it is necessary to de ne (1) the standard form of a temporal formula and (2) when a temporal constraint is evaluable.
Non-temporal formulas are rewritten into standard form by replacing the second and every further occurrence of a variable X by a new unique variable Xi and a constraint X = Xi (see Example 2.1).
However, if a  variable of type time interval occurs multiple times in a formula, this transformation is not so useful since it imposes the constraint that the two intervals be exactly the same.
In temporal databases, it is usually much more interesting to merely impose the constraint that time intervals associated with dierent facts overlap (i.e., have a non-empty intersection).
As a result, multiple occurrences of variables denoting time intervals are related via the intersect constraint.
(Note that this strategy corresponds to the notions of temporal join respectively temporal conjunction.)
Example 4.4 The temporal formula  emp(ENo Name T ) work in(ENo  T ) salary(ENo Sal T )  will be rewritten in standard form as  emp(ENo Name T ) work in(ENo1   T1) salary(ENo2  Sal T2) ENo = ENo1  ENo = ENo2 intersect(T  T1 T3) intersect(T3  T2 ) As a consequence of this approach, the readability is improved.
As a case in point, consider the query of Example 3.5 which will be written works in(1354 Dep T ) works in(245 Dep T ) duration(T  D day):  In order to de ne the evaluability of a temporal constraint, the de nition of the groundness of a variable has to be extended.
The terms ground or nonground given in Def.
2.4 do not allow to adequately categorize variables bound to partially instantiated intervals.
Definition 4.1 A partially instantiated interval is an interval with one bound ground and the other nonground.
Definition 4.2 A variable X bound to the partially instantiated interval T is start-ground (s) if the start point of T is ground.
If the end point of T is ground then the variable X is end-ground (e).
A ground interval variable is both start-ground and end-ground.
Example 4.5 A variable bound to the partially instantiated interval /1994/8/2312:24:13.8 { E ] is start-ground whereas a variable bound to the interval E { /1996/1/1] is end-ground.
Table 4 shows which minimal groundness tags are required for every argument of a temporal constraint in order to guarantee its evaluability.
(Note that the following partial order holds for groundness tags: n < s, n < e, s < g, e < g. Also, instant variables can only be ground or non-ground.)
Example 4.6 The temporal constraint T1 before T2  can be evaluated if the end point of T1 and the start  before after precedes follows during contains equals starts ends overlaps year month : : : duration intersect  hesi hsei heni hnsi hsni hnei hggi hgni hsei hesi hngi hsni hnsi heni hnei hegi hgni hgni hsgi hegi hggni hsgei hegsi hnggi hgsei hesgi hgesi hsegi hgngi  Table 4: Groundness patterns for evaluable temporal constraints point T2 are known.
The following situation would evaluate to true.
T1  )    T2  4.3 Constraint Solving  For space reasons, the speci cation of a temporal constraint solver can not be given in this paper.
However, a general speci cation for a solver in a constraint database system is given in 7].
Nevertheless, a temporal constraint solver can not simply be added to the existing non-temporal one.
Instead, the two solvers have to collaborate by passing information about freshly bound variables between them.
This is illustrated in example 4.7.
Example 4.7 In the query \at which instant would  John have worked twice as long in the sales department as in the engineering department?"
?
{ 9 T1 T2 M1 M2 (works in(`John' `Eng' T1) works in(`John' `Sal' T2)  duration(T1 M1 month) M2 = 2  M1  duration(T2 M2 month) E ends T2 ):  the temporal variable E (the result we are looking for) depends on the non-temporal variable M2 .
M2 depends on M1 which in turn depends on the interval variable T1.
The rst call of the temporal solver determines that the constraint \duration(T1  M1 month)" is evaluable so that M1 becomes ground.
The temporal constraint \duration(T2  M2 month)" remains nonevaluable because the rst argument is suggested to be only start-ground and the second argument is non-ground.
In the next step, the non-temporal solver determines \M2 = 2  M1 " as evaluable and  therefore M2 as ground.
This enables the temporal constraint solver to identify the constraints \duration(T2 M2 month)" and \E ends T " as evaluable.
This example shows that it is necessary to iterate steps 3 and 4 of the constraint lifting algorithm.
During this iteration ostensivebly non-ground variables are determined to be ground because new constraints are identi ed to be evaluable.
This may in turn lead to the solving of further constraints, and so on.
The iteration terminates when no new nonground variables can be determined as ground.
If m non-temporal or event variables and n interval variables are present this is the case after at most max(m 2  n) steps.
This iteration takes place during the constraint lifting algorithm which on its own is executed at compiletime.
At query-evaluation time no further manipulation of the temporal constraints is necessary.
5 Conclusions  In this paper, we have  rst shown how a deductive database system which handles generalized facts and non-allowed rules can be realized.
Subsequently, we have demonstrated how its ability to manipulate constraints over reals can be used to represent the timestamping information typical of temporal databases.
Finally, we have presented a more sophisticated approach to represent temporal knowledge which is based on the development and integration of a special purpose temporal constraint solver.
References  1] Michael B!ohlen.
Managing Temporal Knowledge in Deductive Databases.
PhD thesis, ETH Zurich No.
10802, 1994.
2] Michael B!ohlen and Robert Marti.
Handling temporal knowledge in a deductive database system.
In Datenbanksysteme in Buro, Technik und Wissenschaft, 1993.
3] Michael B!ohlen and Robert Marti.
On the completeness of temporal database query languages.
In Proc.
1st Int.
Conf.
on Temporal Logic, July 1994.
4] Jan Burse.
ProQuel: Using Prolog to implement a deductive database system.
Technical Report TR 177, Departement Informatik ETH Z!urich Switzerland, 1992.
5] Stefano Ceri, Georg Gottlob, and Letizia Tanca.
Logic Programming and Databases.
Surveys in Computer Science.
Springer Verlag, 1990.
6] Thom Fruehwirth.
Temporal logic and annotated constraint logic programming.
In IJCAI Workshop on Executable Temporal Logic, 1993.
7] Roman Gross and Robert Marti.
Compile-time constraint solving in a constraint database system.
In Workshop Constraints and Databases, Int.
Logic Programming Symposium, Ithaca, November 1994.
8] Roman Gross and Robert Marti.
Handling constraints and generating intensional answers in a deductive database system.
Journal of Computers and Articial Intelligence, 13(2-3):233{256, 1994.
9] Joxan Jaar and Jean-Louis Lassez.
Constraint logic programming.
In Proc.
of the 14th ACM Symposium on Principles of Programming Languages, pages 111{119, January 1987.
10] Joxan Jaar and Michael J. Maher.
Constraint logic programming: A survey.
Journal of Logic Programming, 19/20:503{582, 1994.
11] C. S. Jensen, J. Cliord, R. Elmasri, S. K. Gadia, P. Hayes, and S. Jajodia (eds).
A glossary of temporal database concepts.
SIGMOD Record, 23(1), March 1994.
12] Paris C. Kanellakis, Gabriel M. Kuper, and Peter Z. Revesz.
Constraint query languages.
In Proc.
9th ACM Symp.
on Principles of Database Systems (PODS), pages 299{313, Nashville,  1990.
13] Itay Meiri.
Combining qualitative and quantitative constraints in temporal reasoning.
In AAAI, pages 260{267, 1991.
14] Raghu Ramakrishnan.
Magic Templates: A spellbinding approach to logic programs.
In Proc.
Int.
Conf.
on Logic Programming, pages 140{159, 1988.
15] Raghu Ramakrishnan, Divesh Srivastava, S. Sudarshan, and P. Seshadri.
Implementation of the CORAL deductive database system.
In Proc.
ACM Conf.
on Management of Data (SIGMOD), 1993.
16] Peter J. Stuckey and S. Sudarshan.
Compiling query constraints.
In Proc.
ACM Symp.
on Principles of Database Systems (PODS), 1994.
17] Adbullah Uz Tansel, James Cliord, Shashi K. Gadia, Sushil Hajodia, Arie Segev, and Richard Snodgras.
Temporal Databases: Theory, Design and Implementation.
Benjamin/Cummings Publishing Company, Inc., 1993.
18] David Toman, Jan Chomicki, and David S. Rogers.
Datalog with integer periodicity constraints.
In Proc.
Int.
Logic Programming Symposium, November 1994.
19] J. Vaghani, K. Ramamohanarao, David Kemp, Z. Somogyi, and Peter Stuckey.
Design overview of the Aditi deductive database system.
In Proc.
7th Int.
Conf.
on Data Engineering, pages 240{ 247, 1991.
titative temporal information 7].
Moreover, we are also applying LaTeR to model-based diagnosis of dynamic systems.
In both cases, LaTeR high-level language provide a useful interface for obtaining a loosely coupled integration, and LaTeR's e	cient treatment of queries (and updates) provides crucial advantages.
A discussion on such applications can be found in 5].
A prototype of LaTeR has been implemented in C on Sun workstations, under the UNIX operating system.
References  1] J. Allen.
Maintaining knowledge about temporal intervals.
Communications of the ACM, 26:832{ 843, 1983.
2] J. Allen.
Time and time again: the many ways to represent time.
Int.
J.
Intelligent Systems, 6(4):341{355, 1991.
3] R. Arthur and J. Stillman.
Temporal reasoning for planning and scheduling.
Technical report, AI Lab, General Elettric Research Center, 1992.
4] V. Brusoni, L. Console, B. Pernici, and P. Terenziani.
LaTeR: a general purpose manager of temporal information.
In Methodologies for Intelligent Systems 8, pages 255{264.
Lecture Notes in Computer Science 869, Springer Verlag, 1994.
5] V. Brusoni, L. Console, B. Pernici, and P. Terenziani.
Dealing with time in knowledge based systems: a loosely coupled approach.
In Proc.
FLAIRS '95, Melbourne, FL, 1995.
6] V. Brusoni, L. Console, and P. Terenziani.
On the computational complexity of querying bounds on dierences constraints.
Articial Intelligence (to appear), 1995.
7] L. Console, B. Pernici, and P. Terenziani.
Towards the development of a general temporal manager for temporal databases: a layered and modular approach.
In Proc.
of the Int.
Work.
on an Infrastructure for Temporal Databases, Arlington, Texas, 1993.
8] E. Davis.
Constraint propagation with interval labels.
Articial Intelligence, 32:281{331, 1987.
9] T. Dean and D. McDermott.
Temporal data base management.
Articial Intelligence, 32:1{ 56, 1987.
10] R. Dechter, I. Meiri, and J. Pearl.
Temporal constraint networks.
Articial Intelligence, 49:61{ 95, 1991.
11] A. Gerevini and L. Schubert.
E	cient temporal reasoning through timegraphs.
In Proc.
13th IJCAI, pages 648{654, Chambery, 1993.
12] H. Kautz and P. Ladkin.
Integrating metric and qualitative temporal reasoning.
In Proc.
AAAI 91, pages 241{246, 1991.
13] J. Koomen.
The TIMELOGIC temporal reasoning system.
Technical Report 231, Computer Science Department, University of Rochester, Rochester, NY, March 1989.
14] L. McKenzie and R. Snodgrass.
Evaluation of relational algebras incorporating the time dimension in databases.
ACM Computing Surveys, 23(4):501{543, 1991.
15] I. Meiri.
Combining qualitative and quantitative constraints in temporal reasoning.
In Proc.
AAAI 91, pages 260{267, 1991.
16] R. Snodgrass, editor.
Proc.
of the Int.
Work.
on an infrastructure for Temporal Databases.
1993.
17] A. Tansell, R. Snodgrass, J. Cliord, S. Gadia, and A. Segev.
Temporal Databases: Theory, design and implementation.
Benjamin Cummings, 1993.
18] P. VanBeek.
Approximation algorithms for temporal reasoning.
In Proc.
11th IJCAI, pages 1291{1297, 1989.
19] P. VanBeek.
Temporal query processing with indenite information.
Articial Intelligence in Medicine, 3:325{339, 1991.
20] M. Vilain.
A system for reasoning about time.
In Proc.
AAAI 82, pages 197{201, 1982.
21] M. Vilain and H. Kautz.
Constraint propagation algorithms for temporal reasoning.
In Proc.
AAAI 86, pages 377{382, 1986.
22] M. Vilain, H. Kautz, and P. VanBeek.
Constraint propagation algorithms for temporal reasoning: a revised report.
In D.S.
Weld and J. de Kleer, editors, Readings in Qualitative Reasoning about physical systems, pages 373{381.
Morgan Kaufmann, 1989.
23] Ed Yampratoom and J. Allen.
Performance of temporal reasoning systems.
SIGART Bulletin, pages 26{29, 1993.  long less than 100 and more than 75% for sequences long from 100 to around 200.
A more detailed evaluation of the results can be found in 6].
5 Comparisons with Related Work  Dierent criteria can be considered in order to compare the temporal managers developed in the articial intelligence literature.
A rst important criteria concerns completeness.
In LaTeR, as in many articial intelligence approaches, we choose to retain completeness.
since it seems important to us in order to provide users and applications with uncontestable and reliable results.
This rises a trade-o between expressive power and computational complexity of complete temporal reasoning.
As e.g.
in Timegraph 11] and in Tachyon 3] we chose to limit the expressive power in order to retain tractability.
In particular, the expressive power of LaTeR is comparable to that of Tachyon, which deals with temporal constraints that can be mapped onto conjunctions of bounds on differences, too.
In 23], Allen distinguishes between two dierent class of temporal managers: (i) managers that use a constraint satisfaction technique at assertion time, building an all-to-all graph with the constraints between each pair of temporal entities in the knowledge base (ii) managers that build partial graph structures, which need further processing at query time.
For instance, Allen classied TimeLogic 13], MATS 12] and Tachyon 3] as systems of the rst type, and Timegraph 11] and TMM 9] as systems of the second type.
In 23], Allen, considering only atomic queries (i.e., queries for extracting the constraints between two entities in the graph, or yes/no queries without conjunction) pointed out that the approaches computing the all-to-all graph are more e	cient than those computing only partial graphs when dealing with queries.
In fact, in these approaches, queries can be answered in constant time, by reading the values from the graph, while in the approaches in (ii) some further reasoning may be needed.
On the other hand, the approaches in (i) are less e	cient when dealing with assertions (updates), since the whole all-to-all graph has to be computed after each assertion.
LaTeR is a system computing the all-to-all graph (which is the minimal network in the case of LaTeR) that reconciles the advantages of both types of approaches, thanks to its e	cient treatment of complex queries and of assertions as hypothetical queries.
This  result has been obtained via the treatment of complex types of queries.
As shown in van Beek's work 19], as soon as one considers non-atomic queries (even only conjunctions of yes/no queries), two problems arise: on the one hand, the distinction between queries about necessity and queries about consistency is needed on the other hand, constraint propagation may be required.
Van Beek's work has two major limitations with respect to the work in this paper: (i) it deals with qualitative information only and (ii) it performs constraint propagation on the whole network (global propagation) both for queries about necessity and queries about consistency (notice, however, that Van Beek allows the use of all logical connectives in the query language, although answering queries becomes exponential).
On the other hand, we showed that propagation is needed only for queries about consistency (and hypothetical queries, which are not considered in 19]) and, even in such a case, local propagation is su	cient.
Thus, LaTeR retains the e	cient query processing typical of approaches computing the all-to-all graph also in case complex queries.
Furthermore, since in LaTeR assertions followed by queries can be simulated by hypothetical queries (which are answered by local temporal reasoning), LaTeR does not have to recompute the whole all-to-all graph at each assertion, so that also assertions are managed e	ciently.
Besides providing the computational advantages above, LaTeR treatment of dierent (and complex) types of queries seems to us a main feature of the system in itself, since queries (and assertions) constitute the main way of interacting with temporal managers.
Thus, we believe that the expressive query language (and manipulation language) constitutes an advantage of LaTeR with respect to the other systems in the literature.
For instance, high-level interface languages are widely used in the temporal databases community.
However, most of the approaches to temporal databases only deal with time stamps associated with information and do not consider temporal relations between entities (see, e.g., 14]), so that temporal constraint propagation is not needed.
6 Conclusions  In the paper we showed how queries on an heterogeneous temporal knowledge base can be answered e	ciently, independently of the dimension of the knowledge base Currently, LaTeR is being loosely coupled with Oracle, in order to extend relational databases to deal also with (possibly imprecise) qualitative and quan-  the consistency of the knowledge base and thus consistency must be checked after each update and before answering the queries following the update itself.
Since answering queries in an inconsistent knowledge base is meaningless, the consistency check must be performed anyway.
Moreover, the minimal network of the updated knowledge base can be produced by the same algorithms that check consistency.
This means that the presence of updates does not aect the e	ciency of our approach: consistency has to be checked anyway but this produces the minimal network and queries can be answered e	ciently given the minimal network (see the previous section).
Our approach, on the other hand, suggests an e	cient way for dealing with a class of updates, specifically updates that add new constraints (which are the most common in many applications, see the discussion in 5]).
In fact, in such a case one can answer the queries following an update as hypothetical ones.
More specically, a query Q following an update U can be answered as the hypothetical query: Q if U which only involves local propagation.
If a query Q follows a sequence of updates U1  : : : Uh , this can be simulated as the query Q if U1  : : :Uh .
The advantage of such an approach is that during a session of interleaved queries and updates all the operations can be performed with local propagation and the actual update of the knowledge base (which can be very costly) can be delayed with respect to the query process (e.g., performed once and o-line at the end of the session).
Dealing with updates as hypothetical queries can provide signicant computational advantages.
However, when the sequence of updates and queries becomes very long and the updates involve signicant parts of the knowledge base, such advantages may be lost.
A detailed evaluation of the such computational advantages and trade-os can be found in 6] where we compare: the case where the minimal network is recomputed after each update (and then queries are answered with local propagation as discussed in the previous section) the case where queries are dealt with as hypothetical ones.
The evaluation is performed by taking into account three dierent parameters: The length of the sequences (\k" in (7)) The average dimension of updates/queries (we assume that updates and queries have the same  average dimension), i.e., the average ratio between the dimension of queries/updates and the dimension of the knowledge base How extensive the updates are, that is: how many entities involved in the i ; th update were not involved in the previous ones.
At one extreme, all the updates may involve the same set of variables (i.e., the same part of the knowledge base is repeatedly changed) at the other extreme, each update may involve a part of the knowledge base that was not involved by any previous update and thus the updates in the sequence tend to involve larger and larger parts of the knowledge base as the length of the sequence increases (in general, both extreme cases are unlikely taking this as a parameter allows us to consider all possibilities).
Two dierent evaluations are then performed: First of all we evaluated the break-even point between the two approaches that is: the maximum length of the sequence for which dealing with updates as hypothetical queries provides advantages, given the average dimension and extension of the updates or, conversely, which is the maximum dimension for the updates for which there are advantages, given the length of the sequence.
For example, it turned out that for a sequence of 40 updates and queries in which one half of the variables involved in each update was not involved by previous ones (so that the updates tend to extend to signicant parts of the knowledge base), dealing with updates as hypothetical queries provides advantages when the average dimension of each update/query is less that 7% of the knowledge base.
Conversely, when the average dimension of each query/update is 1% of the knowledge base, the approach is advantageous when the length of the sequence is less than 320.
From our experience in the practical application of LaTeR (see 5]), these dimensions are realistic in the sense that it is common that the dimension of updates/queries is around 1% of the dimension of the knowledge base and in any case never more than 5%.
We evaluated how big the computational advantage is.
For example, when the average dimension of update/queries is 1% of the knowledge base (and one half of the variables involved in each update were not involved in previous ones), the advantage is around 90% if the sequence is  Each one of the constraints in (6), taken in isolation, is consistent with (5), but the conjunction in (6) is inconsistent with (5).
Thus constraint propagation is needed in order to check whether a set of constraints is consistent with a given knowledge base.
However, we proved that global propagation of the constraints in the query to the whole knowledge base is not needed for computing the answer.
In fact, since the minimal network is available and since the goal is not to update the whole knowledge base but just to answer the query, local propagation is su	cient (local propagation concerns only the variables in the query).
More formally, we proved the following theorem (the proof can be found in 6]):  Theorem 1 Let S be a set of variables, K:B: a set  of bounds of dierences on such variables and NS the consistent minimal network computed by the (complete) propagation algorithm.
Let us consider a query MAY (Q) on K:B: (where Q is a conjunction of atomic tests) referring to a set G  S of variables (i.e., all the constraints in the query involve only variables in G).
Let NS be the minimal network obtained by propagating the constraints in Q to NS (i.e., to all the variables - global propagation in S ) and NG the minimal network obtained by propagating the constraints in Q to NG , where NG is the restriction of NS to the variables in G (local propagation) then NS is consistent if and only if NG is consistent.
0  0  0  0  The theorem guarantees that in order to answer MAY queries of the form: MAY (C1 AND C2 : : : AND Cn) it is su	cient to propagate the constraints Ci in the query to the part of the minimal network whose nodes are the variables in the query (i.e., occurring in C1 C2 : : : Cn ).
Therefore conjunctive MAY queries can be answered in a time that is cubic in the number of variables in the query and that is independent of the dimension of the knowledge base.
3.2.3 Hypothetical Queries.
Hypothetical queries are queries of the form Q if C where Q is a query of one of the types discussed in the previous subsections and C is a conjunction of temporal constraints, expressed in LaTeR's high-level language.
For example, given the knowledge base in gure 1, the following queries could be asked:  HowLong John work If Mary work Lasting 4h 50min?
Answer : 5h MUST ( Tom work During Mary work) If Mary work Lasting 4h 50min?
Answer : Y es In principle, an hypothetical query should be answered in 3 steps: (i) adding the constraints C to the temporal knowledge base (ii) computing the minimal network N for the new knowledge base (iii) answering the query Q given N .
However, we proved the following theorem (see 6] for more details): Theorem 2 Given S, NS , G, NG , Q, NS and NG as in Theorem 1, then for each pair of variables hX Y i in G, the maximal admissibility range for X ; Y provided by NG (minimal network computed with local 0  0  0  0  0  propagation) is the same as the maximal admissibility range for X ; Y provided by NS (minimal network computed with global propagation).
0  In other words, as regards the variables in G, local propagation to the part of the minimal network concerning the variables in G produces the same results as global propagation to the whole minimal network.
This means that, for any query Q If C, it is su	cient to proceed as follows: perform local propagation of the constraints in C to the part of the minimal network involving the variables in C fi Q Answer Q as discussed in the previous subsections.
The theorem guarantees that this procedure provides the same result that would be obtained by propagating the constraints in C to the whole knowledge base before answering the query Q.
Thus, also hypothetical queries are answered in LaTeR in a time which is independent of the dimension of the knowledge base (more specically, in a time that is cubic in the number of variables in C fi Q).
4 Dealing with updates  In the practical applications of temporal reasoning queries are interleaved with updates.
In other words, a typical session with a temporal knowledge server could have the form of a sequence: U1  Q1 U2 Q2 : : : Uk  Qk (7) of alternated updates (Ui ) and queries (Qi ).
An update corresponds to the the addition or removal of some temporal assertion.
Each update may aect  MAY ( Tom work During Mary work AND start(John work) After 1620) which involves checking that the conjunction of the two assertions is consistent with the knowledge base.
Given the example in gure 1 the answer to such a query is negative.
Queries about consistency/necessity are mapped into conjunctions of atomic tests each one of which is a check on the distance between two time points (and thus the dierence between two variables in the minimal network).
The mapping is the same used for translating assertions into bounds on dierences sketched in section 2.
For example, the conjunction of atomic tests corresponding to the queries (1) and (2) above are respectively: MUST( 0 < STW ; SMW AND 0 < EMW ; ET W) MAY ( 0 < ST W ; SMW AND 0 < EMW ; ET W ) (STW and ET W are as above SMW and EMW are the starting and ending points of \Mary work").
In other words, high level queries about consistency (necessity) are answered by checking that a conjunction of bounds on dierences (atomic tests) is consistent (follows necessarily) from the constraints in the knowledge base.
Given the minimal network, atomic tests can be performed as local checks on such a network.
However, dierent checks are performed in case of MUST and MAY queries as a result the computational complexity of the cases is dierent, as it will be discussed in the two following subsections.
3.2.1 MUST queries  Let us consider a query about necessity of the form MUST(C1 AND C2 : : : AND C ), where each C is an atomic test of the form c  X ; Y  d .
We distinguish two cases:  (n = 1), i.e., the query involves only one atomic test and thus has the form: MUST(c  X ; Y  d) Let afi b] be the maximal admissibility range for the dierence X ; Y (read from the minimal network).
The query is satised i all the values for X ; Y which satisfy the constraints are in cfi d], that is: MUST (c  X ; Y  d) , cfi d]  afi b] (3) Intuitively, since the maximal admissibility range afi b] includes all the values for X ; Y satisfying the constraints, then any interval cfi d] such that cfi d]  afi b] includes all the values for n  i  i  i  i  i  X ; Y satisfying all the constraints.
A query involving one constraint can thus be answered in constant time with a simple lookup in the minimal network and a containment check.
 (n > 1), i.e., the query involves a conjunction of atomic tests and has the form MUST (C1 AND C2 : : : AND C ).
In this case each one of the C can be checked independently of the others since the following property holds: MUST(C1 AND C2 : : : AND C ) , MUST(C1 ) AND : : :AND MUST(C ) n  i  n  n  Thus a query about necessity can be answered in time linear in the number of constraints (and thus in the number of variables) in the query.
3.2.2 MAY queries  Let us consider a query about possibility, i.e., of the form MAY (C1 AND C2 : : : AND C ), where each C is an atomic test of the form c  X ;Y  d .
This case is more complex than the one of MUST queries since the MAY operator does not distribute over a conjunction.
The base case, however, is similar, in the sense that when n = 1, the answer to a query of the form: MAY (c  X ; Y  d) can be provided with a local check on the minimal network.
Let afi b] be the maximaladmissibility range for the dierence X ; Y (read from the minimal network).
The query is satised i there is (at least) a value p 2 cfi d] for X ; Y which satises all the constraints, that is: MAY (c  X ; Y  d) , cfi d] \ afi b] 6= 	 (4) Intuitively, since the maximal admissibility range afi b] includes only values for X ; Y satisfying the constraints in the knowledge base, then any interval cfi d] intersecting afi b] contains at least one value for X ; Y satisfying all the constraints.
The case where the consistency of a conjunction of constraints has to be checked is more complex since atomic tests are not independent of each other.
For instance, consider the knowledge base formed by the following constraints: f0  X ; Z  30fi 5  Z ; W  25fi 10  Y ; X  20fi 15  Y ; Z  30g (5) and the query: MAY (15  Y ; Z  20 AND 15  X ; Z  20) (6) n  i  i  i  i  i  where STW and ET W (SJW and EJW) are the variables associated with the starting and ending points of \Tom work" (\John work") respectively.
Given a knowledge base of temporal information (expressed as bounds on dierences), its consistency must be checked before answering queries (or performing updates), since query processing is not interesting in an inconsistent knowledge base.
LaTeR checks the consistency of a set of bounds on dierences constraints using the complete algorithm discussed in 10], whose complexity is O(N 3), where N is the number of variables.
This algorithm produces the minimal network of the set of constraints, i.e., a compact representation of all the solutions.
More specically, for each pair hXfi Y i of variables, the minimal network provides the maximal admissibility range afi b] for the dierence X ; Y .
In other words afi b] is the set of all and only the values for X ; Y consistent with the knowledge base.
LaTeR keeps track of such a network since, as we shall discuss in the following section, this provides interesting computational advantages during query processing.
3 Ecient Query Answering in LaTeR  At least three dierent types of high-level queries are important for querying a temporal knowledge base: queries for extracting some piece of information from the knowledge base (e.g., the duration of an event or the relation between two events), queries for checking whether a set of temporal constraints is consistent with or follows necessarily from the knowledge base and hypothetical queries.
LaTeR provides a high-level language for expressing all these types of queries.
Queries in the high-level language are then translated into the corresponding low-level queries on bounds on dierences constraints, which are answered eciently, in a time that is independent of the dimension of the knowledge base.
Let us consider the types of queries listed above one at a time.
3.1 Queries for extracting temporal information.
Dierent high-level primitives are provided: When, HowLong, Delay and Relation, which give as answer respectively (1) the temporal location of temporal entities (points or intervals), (2) the duration of time intervals, (3) the delay between two time points and (4) the temporal relations between two temporal entities.
These queries can be answered by a simple lookup in the minimal network.
For example, given the knowledge base in gure 1, the following query could be asked:  HowLong John work?
Answer : 4hfi 50min ; 5h This query can be answered by simply reading in the minimal network the maximal admissibility range of the dierence between the variables corresponding to the end and start of \John work".
As a further example, the following query could be asked: Relation Mary workfi John work Answer : start(Mary work) After start(John work) end(Mary work) non strict Before end(John work) Also in such a case the answer can be read directly from the minimal network (and is then translated in the output format above).
Notice that the answer corresponds to the following relation in Allen's interval algebra: Mary work (During OR Finishes) John work  3.2 Queries about consistency/necessity.
A second important type of query is that of Yes/No queries for asking whether a set (conjunction) of constraints is true in the given knowledge base.
Since in LaTeR temporal information may be imprecise, it is necessary to distinguish whether some conclusion must necessarily hold (i.e., it is entailed by the knowledge base) or whether it may hold (i.e., it is consistent with the knowledge base).
This distinction is similar, e.g., to the one in 19].
Therefore, modal operators must be introduced in the query language in order to distinguish between queries asking whether a set of constraints is possible (consistent) given the knowledge base or whether it follows from the knowledge base.
In LaTeR queries about necessity/consistency are expressed by prexing the MUST or MAY operator to the primitives of the high level manipulation language.
For instance, given the knowledge base in gure 1, one could ask: MUST (T om work During Mary work) (1) MAY (T om work During Mary work) (2) (1) corresponds to asking whether the relation Tom work During Mary work is entailed by the knowledge base (2) asks whether it is consistent with the knowledge base.
Given the knowledge base in gure 1, the answer to (1) is negative while the answer to (2) is positive.
Conjunction is also provided, so that one can ask for the necessity/consistency of a conjunction of temporal constraints.
For example, one could ask the following query:  the approaches that maintain the minimal network and those that perform reasoning at query time 23] discussing how our approach strongly supports the former alternative (since we deal eciently with complex queries and with a class of updates).
2 Representing time in LaTeR  LaTeR is a general purpose manager of temporal  information conceived as a \knowledge server" that can be loosely-coupled with dierent Articial Intelligence and database applications 4, 5].
We believe that a knowledge server must have a predictable behavior.
This has at least two main consequences: (i) from the inferential point of view, complete temporal reasoning must be performed (ii) from the computational point of view, reasoning must be performed in polynomial time.
Moreover, a friendly interface language for interacting with the system must be available in particular, a powerful query language must be provided and query processing must be performed very eciently.
LaTeR is a two-level architecture: the higher level provides the manipulation and query interface language (to which we shall return in the following) the lower level is based on the use of a constraint framework.
LaTeR assumes that time is linear, totally ordered, continuous and metric.
Time points are the basic entities an interval I is dened as a convex set of time points with a starting and an ending point, denoted respectively as start(I) and end(I) (with start(I) < end(I)).
The distance between time points is the basic primitive in our approach and is dened as follows: Given two time points P1 and P2, the assertion distance(P1,P2,afi b]) is true i the distance between P1 and P2 is between a and b, where afi b 2 Rfi a  b:1 The notion of distance is isomorphic to the notion of dierence between reals.
Thus, standard and well-known constraint propagation techniques (see 8] or frameworks such as tcsp and stp 10]) can be used to implement such a notion: the variables correspond to the time points and each assertion distance(P1,P2,afi b]) can be represented as a bound on the dierence between the variables X1 and X2 corresponding to P1 and P2, i.e., as a linear inequality of the form: a  X2 ; X1  b In order to achieve the goal of tractable complete reasoning we limited the expressive power to deal 1 We consider also the case where one of the extremes and b or both of them are not included, i.e.
the range is partially or completely open.
a  only with conjunctions of bounds of dierences, in which complete constraint propagation is performed in O(N 3 ) (where N is the number of variables).
The expressive power of LaTeR's lower level is thus the one of stp 10].
LaTeR provides a high-level interface language for manipulating and querying a temporal knowledge base.
Each assertion in such a language is translated (in constant time) into a set of lower-level constraints (bounds on dierences).
Given the restrictions above on the lower level, we have some restrictions on the expressive power of the interface language.
In particular, the following types of information can be expressed: precise or imprecise location of time points and intervals, precise or imprecise duration of time intervals, precise or imprecise delay between time points, qualitative relations between points, intervals or points and intervals, limiting to the continuous pointisable relations 22] (as discussed in 19] this is not too restrictive in practice since many commonly used relations are indeed continuous pointisable).
Figure 1 provides examples of assertions in LaTeR's high level language (see 4] for a denition of the language).
John work Since 1400 ; 1430 Until 1800 ; 1900 start(Mary work) 10 ; 40 min After start(John work) Mary work Lasting AtLeast 4 hfi 40 min end(Mary work) non strict Before end(John work) Tom work Since 1415 Until 1830 Tom work During John work Figure 1: A simple knowledge base.
For example, the rst assertion localizes (in an imprecise way) the interval of time corresponding to \John work" the second denes a delay between the starting point of \Mary work" and the starting point of \John work" the third denes the duration of \Mary work".
The non strict operator can be used in conjunction with the precedence (and containment) relations to express that the relation itself is not strict (in the example the meaning is that the end of \Mary work" is before or equal the end of \John work").
As an example of the translation of high-level assertions into bounds on dierences, the last assertion in gure 1 is translated into bounds on dierences as follows: (0 < STW ; SJW ) ^ (0 < EJW ; ET W)  Ecient query answering in LaTeR  V. Brusoni and L. Console and P. Terenziani Dip.
Informatica, Universitfia di Torino, Corso Svizzera 185, 10149 Torino, Italy E-mail: fbrusoni,lconsole,terenzg@di.unito.it  Abstract  In the paper we address the problem of answering queries eciently in heterogeneous temporal knowledge bases (in which qualitative and quantitative pieces of information are amalgamated).
In particular, we rst outline a powerful high-level language for querying a temporal knowledge base.
We then show that, in our language, if the minimal network computed during consistency checking is maintained, then queries can be answered eciently in time that depends only on the dimension of the query and is independent of the dimension of the knowledge base.
Finally, we discuss how our approach can deal eciently also with updates and, specically, with sequences of interleaved updates of the knowledge base and queries.
1 Introduction  A lot of attention has been paid in the Articial Intelligence community to the problem of dealing with time 2, 22].
In particular, most articial intelligence approaches focus on reasoning issues 1, 15, 18, 20, 21].
On the other hand, the problems of (i) designing a high level language for manipulating and querying temporal knowledge bases and (ii) answering (complex) queries eciently have been often disregarded.
Some of these problems have been faced in the database community 14, 16, 17] where, however, reasoning and complexity issues received only a limited attention.
The aim of this paper is to reconcile these two complementary tendencies in a general-purpose manager of temporal information: LaTeR (Layered Temporal Reasoner).
In LaTeR heterogeneous temporal information (that is, qualitative and quantitative information) is amalgamated in a principled way startThis work was partially supported by CNR under grant no.
94.01878.CT07.
ing from the notion of distance between time points.
LaTeR, moreover, provides a high-level language for manipulating temporal information the expressive power of the language has been limited in such a way that complete constraint propagation can be performed in polynomial time (section 2 sketches those aspects of LaTeR that are relevant in this paper, see 4, 5] for more details).
The paper denes a powerful query language including modal operators for asking whether a set of assertions follows necessarily from a knowledge base or it is only possibly true and supporting yes/no queries, queries for extracting temporal information and hypothetical queries.
We believe that having a powerful language for querying temporal knowledge bases is fundamental for the practical applicability of managers of temporal information.
The main goal of the paper is to propose an approach for answering queries eciently in a temporal knowledge base (section 3).
Notice that the problem is interesting only in case the knowledge base is consistent since answering queries such as those mentioned above in an inconsistent knowledge base is banal.
We show that in our language, if we maintain the propagated knowledge base (\minimal network") obtained as a result of checking consistency of the knowledge base, then the complexity of answering queries is independent of the dimension of the knowledge base and depends only on the dimension of the query, where the dimension of a knowledge base (query) corresponds to the number of temporal entities involved in the knowledge base (query).
A critical aspect when the minimal network is maintained is that of updating such a network each time the knowledge base is updated (since, in principle, the whole network has to be recomputed after every update).
In the paper we discuss how updates to the temporal knowledge base and interleaved sequences of updates and queries can be dealt with efciently in our approach (section 4).
In section 5 we compare our approach to related ones.
In particular, we consider the trade-o between
ral networks that may be used in any system relying upon such networks and giving account of some particular structure in which a <<strict decomposition>> may appear.
[LePape,90] C. Le Pape - A Combination of Centralized and Distributed Methods for Multi-Agent Planning and Scheduling.
Proc.
IEEE Robotics and Automation, 1990.
7 .
References  [LePape,94] C. Le Pape - Implementation of Resource Constraints in ILOG SCHEDULE: A Library for the Development of Constraint-Based Scheduling Systems.
to appear in <<Intelligent Systems Engineering>>, 1994.
[Boddy,93] M. Boddy - Temporal Reasoning for Planning and Scheduling.
SIGART Bulletin, 4(3), 1993.
[Collinot,87] A. Collinot & C. Le Pape - Controlling Constraint Propagation.
Proc.
10th IJCAI, Milan (It) 1987.
[Collinot,88] A. Collinot, C. LePape, G. Pinoteau - SONIA: A Knowledge-Based Approach to Industrial Job-Shop Scheduling.
International Journal for Artificial Intelligence in Engineering, 3(2), 1988.
[Dean,86] T. Dean - Intractability and TimeDependent Planning.
"Reasoning About Actions and Plans", 1986.
[Dechter,89] R. Dechter, I.Meiri, J.Pearl - Temporal Constraint Networks.
Technical Report, Oct 1989.
[Erschler,91] J. Erschler, P. Lopez & C. Thuriot - Raisonnement Temporel sous Contrainte de Ressource et Problemes d'Ordonnancement.
Revue d'Intelligence Artificielle, 5(3), 1991 [in french].
[French,82] S. French - Sequencing & Scheduling: an Introduction to the Mathematics of Job-Shop.
Whiley, 1982.
[Ghallab,89] M. Ghallab & A. Mounir-Alaoui Managing Efficiently Temporal Relations Through Indexed Spanning Tree.
Proc.
11th IJCAI, 1989.
[Ghallab,94] M. Ghallab & T. Vidal - Focusing on a Sub-Graph for Managing Efficiently Numerical Temporal Constraints.
LAAS Report 94303, Jan. 1994.
[Vidal,94] T. Vidal, M.Ghallab & R.Alami - Dynamical Allocation of Predefined Tasks to a Large Team of Robots.
LAAS Report n 94303, Aug 1994.  action brings some real duration value that is propagated to the following expected actions, thanks to an arc-consistency algorithm that runs efficiently in O(m.r) in worst case, where m is the number of time-points in the graph managed by the execution process, and r the number of robots.
3.
For big databases, the initial off-line process of expanding the graph, with initial propagations became really costly.
But the global in-line allocation/execution process behaves with nearly the same experimental complexity, thus paying off for the initial preprocessing.
One can notice that in fact allocation and execution processes work on the same graph, but not exactly on the same sub-set of time-points: the sliding interleaving technique leads to a certain time-lag between both.
Thus, we assume a robust interleaving process, with the quality of the solution produced depending upon the level of imprecision characterising the instance of the application.
***  If we have a lot of imprecision, we cannot require a tight discrimination criteria between two robots(i), because the execution would let this imprecision unresolved, and the process would stop.
So, we need to adapt our requirement of quality to the level of imprecision.
We should tune it, with statistical or learning methods for example, such that each robot is given at each time an horizon of one up to three tasks, thus maintaining a sufficient time-lag between allocation and execution.
The application presented throughout this paper describes techniques for allocating predefined identical tasks to a large team of identical robots.
This is done at a high-level of abstraction, with centralised techniques, in order to give to each robot a global description of the tasks it has to perform.
Path planning and trajectory control capabilities are managed in a distributed way in the ESPRIT project MARTHA.
6 .
Experimental Results and Conclusion  The need for near-optimal solutions, considering the imprecision of temporal information, led us to adopt an allocation/execution interleaving process, which gave birth to the necessity of finding highly efficient temporal management techniques.
This was made possible thanks to  Our method was implemented in CommonLisp in a SunSparc environment.
The formal details about the tests can be found in [Vidal,94].
We can summarize our results as follows: 1.
With less imprecise data, we get better near-optimal solutions.
2.
The temporal propagation processes appear to be negligible compared to the global allocation process, which complexity itself appears to be strictly bounded and runs in the order of the second(ii).
i. formally a short overlapping of the two intervals corresponding to the possible values of the availability times of the two robots  * the separation between heuristic choices and temporal management.
* the use of the same graph, with distinct propagation algorithms, for managing allocation as well as execution steps, * and essentially, the proof of some important property of the path-consistency mechanism made it possible to restrict it to local propagations, keeping the global completeness of the propagation process with respect to the application needs.
This is a general property of tempoii.
which has to be compared to durations of actions in the order of 10 to 15 minutes at the less.
Thus we have to run the two decision steps that are usually addressed in search techniques especially when backtrack-free search is required, and that will be made through heuristic functions that are briefly introduced here (see [Vidal,94] for more details): 1.
<<variable ordering>>: which task to schedule next, i.e.
in our case which container to take next.
We will look for the "most critical" container (in a temporal sense).
This search runs in linear time in the number of reachable containers, which are generally few, as far as the containers are <<highly stacked>> in the conveyors.
2.
<<value ordering>>: which resource and temporal placement to choose for this task, i.e.
in our case which robot will take care of the container.
We will look for the robot that is likely to arrive first at the unloading area.
This search runs in linear time in the number of robots.
*** Once the choices have been made, temporal precedence constraints will be added * between the last availability time-point of the robot and the beginning of its new task, * and ordering constraints for the PICKUP and PUTDOWN operations.
This constraints adding process goes with temporal propagations as it is depicted in section 3, leading to updating the graph, and then to new values on the availability date of the robot and the temporal windows on each unloading/loading operation.
In [Vidal,94], we detail a least-commitment approach, mixed with some heuristic choice when needed, to obtain a backtrack-free near-optimal ordering of the unloading/loading operations.
To summarize, the global allocation process runs through the following steps: 1.
Heuristic choice of the most critical container: CONT.
2.
Heuristic choice of the best robot to convey it: ROB.
3.
Strict ordering of the last PUTDOWN made by ROB, with associated temporal propagations.
4.
Temporal propagation in the sub-graph of the currently allocated task.
5.
Strict ordering of the PICKUP of the currently allocated task, with associated temporal propagations.
One can easily denote the separation between heuristic decisions and temporal management, which allows flexibility of the global system: it is always possible to improve the heuristic functions, without challenging the overall system.
5 .
The Execution Process and the Global Control Loop As we have sketched out in the introduction, one cannot allocate the robots to all the tasks, because of lack of precision in the numerical constraints given in input.
The more tasks we give to a robot, the more imprecision we get in its new availability time, because of imprecision growing within successive propagation processes.
We reach a point when it is no more possible to choose between two robots in a sufficiently deterministic way: we can only make an unreliable choice that could lead to a future need to backtrack.
We then decide to stop the allocation loop and wait until the execution process provides more precise values: each executed  turn propagated in that neighbour cluster.
The process goes on that way until no constraint is modified.
Hence we will not compute the complete graph, but we can easily prove that every constraints included in a cluster will be minimal at the end of the process.
This intuitive behaviour is summarized through the two following properties, that can be easily proved: 1.
The task-graphs defined above represent a strict decomposition of the overall graph of the application.
2.
The clusterised propagation is complete: it always detects a global inconsistency if there is one, and moreover provides minimal constraints (thus new precedence constraints as well) within each cluster.
The only constraints requested by the allocation process (see above) appear, from the definition of the task-graph, to be confined to those taskgraphs.
Thus we can say that our clusterised propagation process behaves in the same way as a global propagation would do, regarding the global process we have to address.
Concerning the efficiency, the decomposition leads to the definition of sub-graphs each containing 14 time-points.
We thus get a complexity of O(k.143) at each constraint addition, with k being the number of sub-graphs that will have to be propagated in a recursive way.
As we only allocate within a short-term horizon, the total number of clusters in the graph corresponding to the tasks being allocated is strictly bounded, thus k is strictly bounded, which leads to nearly constant time propagation process.
To end with this section, let us look at the global temporal management process during the allocation process.
At the beginning, the global graph is expanded, with all the task-graphs in parallel.
Initial propagation in each sub-graph (not yet connected one to another) is also made.
Each time a robot is allocated to a mission, the precedence constraints corresponding to the interactions with other clusters are added (as in our graph example), launching clusterised propagation steps.
Thus, the global graph evolves from a highly parallel one to a more sequential one, where, as usually in scheduling problems, tasks for a robot are incrementally ordered, and constraints between those robot sequences of tasks appear because of the ordering of the unloading/loading operations.
4 .
The Allocation Decision-Making Loop Just remember that in our interleaving approach, we have to dynamically allocate one robot to predefined tasks of handling containers, until some threshold in time is reached, that will require to wait for results of the execution of the allocated task (see part 5).
So we will define a loop whose incremental steps are allocation of one robot to one container, with the aim of not backtracking on these decisions.
At each of these steps, we are given: * (a) the current partially ordered overall graph, * (b) the current position and <<availability time>> of each robot (i.e.
the resources availability), * (c) the current stacks of containers remaining on each conveyor, and the loading area for each container (i.e.
the set of tasks requiring sequencing and resource allocation).
We want to get some near-optimal (or <<good quality>>) solution according to the criteria which appears to be preeminent in our application: minimising the earliest end time of the overall plan (as in [LePape,90]).
In other words, we wish to process all the unloading/loading tasks with a minimal loss of time, in order to get the best chances of having unloaded/loaded all the containers in a conveyor before its leaving.
Thus, the global graph appears to be composed of task-graphs like in the first figure of last page, some of them connected one to another by precedence constraints.
Let us define more precisely those task-graphs: each task is associated to a cluster of 14 timepoints containing: * the origin of time 0 (reference frame of the dates), * the end time-point of the last task assigned to the same robot, if there is one.
* the initial and final points of each action in the task (8 time-points), * the current temporal windows for PICKUP and PUTDOWN actions (4 time-points).
Those intersecting clusters are represented in the second figure in last page, by means of circles, not including, for clarity concerns, the timepoint 0.
For the same reason, the precedence constraints between 0 and some conveyor availability beginning time-points are not represented.
In fact, the only temporal constraints that are useful for the global mission allocation process, as we will see in next part, are: * the date of availability of a robot, i.e.
the date of the end of the last task allocated.
* the temporal windows for the PICKUP and PUTDOWN operations.
This means that the only minimal constraints that are useful are constraints between timepoints belonging to the same cluster.
In other words, we will never need to know the temporal distance between the beginning of PICKUP(Rob1,Cont2,Boat) and the end of GOTO(Rob4,Park1,Train), for example.
Thus we only need to get minimal constraints within those clusters to guide the allocation decisions.
Let us now briefly present the properties that are at the heart of our process:  << Definition 1 >> * A strict minimal precedence (i,j) is a precedence constraint that cannot be entailed by another (by transitivity).
* A <<strict decomposition>> D of a graph G is a set of clusters (Cu)u=1,..c , corresponding to overlapping sub-sets of time-points {iu1, ..., ium}, such that:  [?
]i [?
]G, [?]
Cu such that i [?
]Cu (complete partition upon time-points),  [?
]i [?
]G, [?
]j [?
]G, such that (i,j) is a strict minimal precedence, then necessarily [?]
at least one u such that i [?
]Cu, and i [?
]Cu (complete partition upon constraints).
* The <<neighbour clusters>> of a cluster Cu are Cv1, ..., Cvm such that for each Cvj, [?]
a timepoint ivj such that ivj [?
]Cvj and ivj [?
]Cu at the same time.
The <<clusterised>> propagation process will then be as follows: * If D is a strict decomposition of G, if (i,j) is a temporal constraint being modified, then the propagation process will be the following recursive process: execute a propagation step within Cu and within each of its neighbour clusters Cv1, ..., Cvm, and then repeat the same process for each modified cluster, until there is no cluster modified.
Let us illustrate those definitions through the graph in the second figure of last page.
The arrows represent strict precedences of the graph.
We can see that all of them are included in at least one cluster.
Then, a modification of such a constraint will be surely propagated to all the other constraints of the cluster, that will be updated.
Some modified constraints are included in other clusters as well: those are the precedence constraints between tasks, or the ordering constraints between PICKUP/PUTDOWN actions (in dotted lines in the figure).
They are in  all the minimal constraints (i.e.
where only values globally consistent with the other constraints are kept), and at the same time gives account of new precedence constraints.
As our application involves large number of robots and containers, n is quickly in the order of a few thousands time-points (about ten times as many as the number of containers).
Hence a cubic complexity will end in unbearable time consuming algorithms.
We can take some advantage of the particular structure of the temporal network in our application.
The global decision-making process consists of: * allocating each robot to successive tasks, which leads to a strict ordering of tasks for each robot, * and adding constraints on PUTDOWN and PICKUP operations (following the constraints on crane actions), thus inducing precedence constraints between tasks.
3 .
The Temporal Graph Decomposition Scheme  u1 IN_AREA (boat) u2  [0',10']  i1  0  GOTO [10',15']  [300',360']  i3 PICKUP i4  i2  i1 time-point action simple precedence constraint sequencing actions conveyor availability temporal window [0',10'] numerical temporal constraint  x  x  x  PICKUP  x  IN_AREA (boat)  x  0  GOTO  x  GOTO  x  x  x  x  x  time-point action simple precedence within a task-graph conveyor availability temporal window simple precedence between distinct task-graphs task-graph  l1  IN_AREA (train) [90',120']  PUTDOWN  x  x  x  x x  x  x x  [6',10']  x  x x  i7PUTDOWN i8  i6  IN_AREA (train)  x  x  GOTO [20',30']  [60',75']  x x  i5  [5',8']  x  x  x  x  x  x  x  x  x  x  x  x  x  x  x x  x  x x  l2  Our proposal is to decompose the global temporal graph into sub-parts (or clusters) in which propagation can be confined.
Our decomposition scheme relies upon the application-dependent structure of the graph, whereas for example [Dean,86] proposes a temporal decomposition based upon complex problem characterisation techniques.
We prove that our method meets some important property that keeps its algorithmic completeness (i.e.
it always detects an inconsistency when there is one); it also gives back the minimal constraints [Dechter,89], when considering only the constraints that are required by the decision-making process.
We then achieve a highly reliable temporal constraints manager whose propagation runs in nearly constant time, keeping at the same time the total expressiveness of the [Dechter,89] temporal constraint networks.
One important thing to notice is that we take advantage of a separate temporal constraint management system, which can be compared to the [Erschler,91] system and also to [Boddy,93] system relying on the Dean's TMM.
Another important feature is that our method is incremental, which means that ordering constraints are added one by one, updates and consistency checking being made at each step.
2 .
Application Context and Basic Representation Issues The global mission in our application domain is the following: a large team of robots have to load/unload ships in a harbour (ESPRIT project MARTHA).
Containers (a few hundreds per mission) arrive in boats or trains (we will use the generic term of <<conveyor>>).
They are to be brought to some other place: stockage areas, or conveyor areas.
Those tasks are to be carried out by a crew of robots (about 50), all identical, through predefined routes linking the different areas in the harbour.
A mission allocation system has to decide which robot is going to take  care of which container.
The generic task performed by a robot is a sequence of the four actions: * Moving from the robot current position to the unloading area (GOTO action).
* Picking up the assigned container by use of a crane located in this area (PICKUP).
* Moving to the loading area (GOTO).
* Putting down the container (PUTDOWN).
Each of these actions have an associated duration, and the PICKUP and PUTDOWN actions have to be made during the availability temporal window of the corresponding conveyor.
The containers are arranged inside a conveyor by stacks, which defines an initial partial order on unloading operations.
But there is just one crane per area, which means that PICKUP and PUTDOWN operations have to be strictly ordered in the final plan for each area.
Moreover, cranes are not explicitly represented but implicitly managed throughout the temporal ordering of PICKUP/PUTDOWN operations.
*** The representation of time within our temporal system IxTeT relies upon a graph-based structure with time-points (the nodes in the graph) that are constrained by precedence constraints (directed edges in the graph, see [Ghallab,89]), and also by imprecise numerical constraints (durations and dates) given as possible durations labelling the precedence edges (see [Dechter,89] and [Ghallab,94]).
This leads to represent the above general task with the conveyors availability temporal windows as it is shown in the first figure of next page.
As far as numerical constraints are concerned, a path-consistency propagation algorithm, running in O(n3), n being the total number of timepoints in the overall graph, is required.
It allows to check for global consistency, making explicit  Efficient Temporal Management through an ApplicationDependent Graph Decomposition Thierry Vidal Malik Ghallab thierry@laas.fr malik@laas.fr LAAS-CNRS - 7, avenue du Colonel-Roche, 31077 Toulouse, France  Abstract In the MARTHA project, a large number of robots in a harbour are given the global task of transporting standardized containers from one area to another (ships, trains, stocking areas).
The global decision-making process consisting of allocating robots to those predefined tasks can be viewed as a scheduling and resource allocation problem, which is addressed here in a centralised way.
Imprecision of temporal constraints (expected arrival and leaving times of ships and trains, durations of actions) make it meaningless to search for a strict optimal schedule.
Our approach interleaves task allocation and execution, scheduling in a sliding short-term horizon, as the execution process runs, and providing near-optimal solutions.
For large applications as our, the complexity of temporal management is a crucial issue.
We present here a technique of graph decomposition, leading to nearly-constant time temporal propagation, without any loss of information.
We finally relies on a complete, highly expressive and efficient temporal reasoner used by our global decision-making loop.
1 .
Introduction and Related Work When addressing scheduling and resource allocation problems in the context of real applications involving dynamical uncertain worlds, one has to manage temporal constraints as well as resource constraints, trying to get a robust schedule of good quality as fast as possible.
In our application context, we have to allocate predefined tasks to a number of identical robots.
It is in fact what [LePape,94] calls a joint problem, mixing constraint-based scheduling and resource allocation.
We are mainly concerned with the imprecision of temporal constraints on expected events and goals.
To decide which robot to allocate to a task, we use heuristic functions approximating a given optimality criteria along a greedy search approach, thus leading to near-optimal solutions.
Here, the global objectives that have to be met are temporal ones (earliest end time of the overall plan), hence the choices are guided by the temporal constraints.
As those are imprecise, short term predictions and objectives are usually quite reliable whereas long term ones are not.
This lead us to adopt a dynamic approach (as it is defined in [French,82]), interleaving task allocation and execution, like in [Collinot,88], or in [Dean,86].
The efficiency of the allocation process is a crucial issue.
Like in [Erschler,91] or in [Collinot,87], the temporal constraints propagation process is at the heart of the problem, making explicit new constraints that will help guide the scheduling choices, which will in turn add new constraints that will need to be propagated.
But [Erschler,91] as most authors use an O(n3) complexity algorithm that is too expensive in our application, whereas [Collinot,87] uses focusing techniques that, although enhancing the propagation efficiency, do not achieve its completeness any longer, and then leads to <<deviations>> in the decision process.
Time and Uncertainty in Reasoning about Order Robert A. Morris and Dan Tamir Florida Institute of Technology Melbourne, FL 32901 email:morris@cs.
t.edu  Abstract  The ability to intelligently order events is important for planning and scheduling in the presence of uncertainty about the expected duration of those events.
This paper presents a time-based theory of an agent in a dynamic environment, and a framework for reasoning for the purpose of generating eective orderings of events.
1 Setting the Stage In this study, the interest is in the role of time in the ability of intelligent agents to plan or schedule events, especially actions, events of which they are the agent.
Researchers in AI have, for a number of years, oered analyses and computational models of the temporal reasoning underlying these abilities.
These models have explained the intuitive complexity of reasoning about time in terms of proving the consistency of a set of temporal constraints, an inherently intractable problem.
This study adds further complexity by folding into the framework a dynamically changing environment, wherein temporal knowledge becomes outdated, as well as being partial and incomplete.
How, we ask, can an agent utilize the information found in such an environment in order to eectively solve planning and scheduling problems?
The impetus for this investigation is a system which provides a solution to the dicult problem of scheduling telescope observations  Kenneth M. Ford  University of West Florida Pensacola FL 32514 email: kford@ai.uwf.edu 1].
This solution required attention be given to the fact that the duration of a repeating event may be dierent on dierent occasions.
This made any generated schedule \fragile", which means that there was a tendency for it to \break" during execution.
The novelty of the approach of the researchers was the integration of statistical information about past occurrences of events in order to predict how well a schedule will stand up against a contrary night sky.
This allowed for sensitive locations in the schedule to be identied, and made it feasible to maintain a library of contingency schedules.
We feel this approach to solving planning and scheduling problems in a changing world can be extended and generalized to other problems with similar, dynamic environments.
One objective of this study is to perform these transformations.
The objectives of this paper consist of Constructing an abstract representation of the intelligent behavior which is manifested in the telescope scheduling example, as well as others Proposing a formal representation of the knowledge required to realize this behavior and Presenting a computational model of learning the requisite knowledge based on statistical evidence inferred from the experience of temporal duration and order.
2 Abstract Representation of Behavior The interest here is in systems embedded in a dynamic environment with feedback in the form of rewards.
It is desirable for the system to learn from these rewards in order to maximize its rewards over the long run.
Traditionally, such a system is modeled in terms of state transition networks, consisting of states, actions and state-transition functions.
Here, an alternative model is presented with an underlying temporal ontology.
Specically, there are events represented by their durations, and a single atomic temporal ordering relation, immediately precedes (<).
Given a set E = fA B C g of events, if an agent prefers a certain ordering of their occurrences, say, A < B < C (\A immediately before B immediately before C ") to another, the reason may have to do with constraints which lead to a preference for that order.
There are many varieties of constraints possibly underlying this preference.
Here the interest is in criteria for orderings that are based on temporal constraints.
One example of such a constraint involves minimizing the overall extent of the performance of all the tasks.
By overall extent is meant the interval of time it takes all the events in E to complete.
On this criterion, an ordering of E which is expected to minimize the overall extent of E will be the most preferred ordering.
Another criterion for ordering will be in terms of minimizing the overall duration uncertainty of the set of tasks.
Intuitively, duration uncertainty is manifested in terms of a relative lack of condence concerning how long an event, or a set of events, will take.
If it is possible to predict that one ordering of the tasks will exhibit less duration uncertainty than another, then choosing the ordering with less uncertainty will be preferred.
This is analogous to taking a \sure bet", even if the payo is less than another choice which  is less likely.
The inability to predict how long an event will last on a given occasion (duration uncertainty) is a pervasive feature of common sense experience.
Things that happen in a given day, e.g., breakfast, driving to work, faculty meetings, going to the dentist, exhibit varying amounts of duration uncertainty.
Duration uncertainty is undesirable to a rational agent because it leads to failure in the completion of plans and schedules, and the need for time-consuming repair and revision.
To satisfy one or the other of these constraints, an agent can choose to order the occurrences of the events in such a way that events in close temporal proximity share one or more stages.
Informally, a stage of an action or event E is an action or event which occurs as part of the occurrence of E .
For example, \preparing the cleaning utensils" can be viewed as a stage in most or all cleaning actions.
Often, an event can be \sliced" in dierent ways to uncover its stages.
Suppose two cleaning room actions, clean kitchen (K ) and clean bath (B ) are performed together, say K < B .
There will be a tendency for the preparation stage of B to not be required (or be simplied) hence the overall duration of performing both should be reduced.
Furthermore, since the duration uncertainty of the whole will be a function of the duration uncertainty of the dierent stages, there's a chance that duration uncertainty can also be reduced as a result of this pairing.
This situation is illustrated in Figure 1.
In this gure, stage S1 of K is shared with B .
The temporal eect of sharing stages is that the events can be viewed as overlapping in time.
Notice that when speaking of such relations, there is no assumption of convexity (no interruption) with respect to the intervals making up the durations of the events.
It follows that an agent should be able to more accurately predict how long the bathroom cleaning will take when preceded by the  K B  S1 S1  Figure 1: Eect of Pairing Similar Events in Close Temporal Proximity kitchen cleaning action than it could predict its duration in isolation, or when preceded by a event sharing no stages with it.
The point of the examples, then, is that events that share stages will tend to be mutually inuencing with respect to duration, especially when paired in close temporal proximity.
This sort of information would be useful for an agent who is either lacking the requisite knowledge about the events for which it needs to nd an intelligent ordering, or in which the environment is constantly changing, making its knowledge outdated.
Consider, for example, a robot assigned the task of delivering mail in a dynamically changing environment.
Oces may move, for example, or construction to dierent parts of the complex may require dynamically revising the routes, and hence possibly the order, in which mail is delivered.
Similarly, it may be equipped with only a crude or outdated map of its environment.
We proceed to formalize a model of an agent in a dynamically changing environment.
The model is based on the familiar idea of using a network to store temporal information.
Here, the nodes, or variables represent events in terms of their durations, and the arcs store values which represent the eect of orderings of events on the durations of events that follow them in close temporal proximity.
Denition 1 (Duration Network )A Duration Network N is a set of variables V = V1 : : : Vn , and a set of labeled edges E =  '$ &% '$ &% -2  '$ &% '$ &% V =6  2 ( ( ( ( ( V1 = 4 ((( 0 ;; C C QQ CC QQ ;; CC 0 Q -1 C ; C ;; QQQ 0 CC CC ; QQ C Q  V3 = 5  -1  V4 = 2  Figure 2: Instantiated Duration Network  instantiation of  fhVi  Vj i : 8Vi Vj 2 V g. An N is a function I : V fi E !
Z , such that, all Vi  Vj 2 V , Eij 2 E :  for  I (Vi) > 0 I (Eij )  0 and Let Eij = hVi  Vj i.
Then jI (Eij )j < I (Vi) and jI (Eij )j < I (Vj ).
A duration network is a complete network in which the variables stand for events, and their values are durations of these events.
The labels on the arcs represent the eect of sharing stages on durations.
A negative value for I (hVi  Vj i) represents the advantage of performing Vi and Vj together by virtue of their sharing a stage the negative value is the \reward" for doing them in close temporal proximity.
Figure 2 depicts an instantiated duration graph.
To illustrate the meaning of the graph, consider the nodes V1 and V2 .
The order V1 < V2 < V3 would yield a \reward" of 2 time units.
This means that the overall duration of performing this sequence would be 4 ; 2 + 6 + 0 + 5 = 13 time units.
Compared with performing V1 < V3 < V2, which has overall duration 4 ; 1 + 5 + 0 + 6 = 14 time units, the rst ordering would have the smaller overall extent.
It is useful to distinguish what we will call legs of a tour of a duration network N .
Intuitively, if a tour is a complete path through the network, a leg of the tour is any subpath of that path.
More formally, we use the notion of sub-sequence of a sequence (using the notation t v t) to characterize tour legs.
If t = hVt1  Vt2  : : : Vtn i is a tour through N , then, for example, hVt3  Vt4  Vt5 i is a leg.
To relate a leg to its tour, we use t=hVti  : : :Vti +mi to mean \the part of t consisting of the indicated leg".
Denition 2 (Process/Tour of a Duration Network)A k-process of a duration network N = (V E ) is a sequence P = hI1 I2 : : :  Iki of instantiations of N .
A tour t of a duration network N with variables V = fV1 : : : Vn g is a permutation of V .
We write t = hVt1  : : :  Vtn i to enumerate the elements of t. Denition 3 (Cost of a Tour/Tour Series)Given an Instantiation I and tour t = hVt1  : : :  Vtn i, the cost of a tour in I (c(t I )) is c(t I ) = I (Vt1 )+I (hVt1  Vt2 i)+: : :+I (hVtn;1  Vtn i)+I (Vtn ) Given a k-process P = hI1 : : :Ik i and a sequence of corresponding tours T = ht1 : : :  tk i, called a tour series, the cost of the series T in process P (C (T P )) is X C (T P ) = c(ti Ii) 0  1ik  More generally, we can introduce the notion of \cost of a leg L = hVti  : : :Vti+m i of a tour t in I " as follows: ( 6v t c(t=L I ) = c(L I0) :: Lotherwise Finally, we can speak of the cost of a leg in a tour series T and a process P :  C (T=hVti  : : :Vti+m i P )  as the sum of the costs of this leg in all the tours in the series containing this leg.
The interest now is to dene a set of oneperson games involving tours of the duration graph.
The specic goal of interest is to nd a tour series Tmest of length k which is an agent's estimate of the minimal tour series Tmin .
The latter is the tour series which, given a duration network N and k-process P , incurs the minimum cost over all possible tour series.
Other goals are of course possible.
One is to minimize the standard deviation from the mean of tour durations in the series.
Another goal is to complete as many of the events (i.e., visit as many of the variables) as possible, given rigorous time constraints (i.e.
cost).
Other versions of the game dier on assumptions concerning either the agent's initial knowledge of N , its abilities to update the knowledge based on experience in the form of tours it has made, or on the properties of P .
The interest is in nding denitions of P which characterize properties and relations of the abstract world which are homomorphic to those properties and relations which occur in real world planning and scheduling domains.
First, let us say that an instantiation I is totally repeating in P = hI1 I2 : : : Imi if 9i ji 6= j I = Ii = Ij 2 P .
We can rene this to \repeats n times" in an obvious way.
Two total repetitions of I , say Im and Ip are n units apart if km ; pk = n. If n = 1, then the repetitions will be said to be consecutive.
I will be said to be p-periodic in P if any pair of occurrences of I in P repeat r units apart, where r is a factor of p. Similarly, I is almost periodic in P if there exists a p such that any pair of occurrences of I in P occur a distance apart which is \close" to being a factor of p. We assume this notion of being almost periodic is intuitive enough to remain qualitative, although obviously it can be made more precise.
Finally, we can dene a notion of a partially repeating instantiation  in P , and derivative notions, in terms of instantiations that share some of their values.
Secondly, a process will be said to be invariant if the values of the dierent instantiations do not dier a great deal.
We distinguish two kinds of invariance, duration and path invariance.
First, consider total duration invariance.
We can draw an even ner distinction between weak and strong total duration invariance.
We can express strong invariance in terms of mean, or average duration, and standard deviation.
Thus, let the mean duration of an event represented by Vi in a process P be the average duration of Vi over all instantiations in P .
Let VPi be a variable denoting the standard deviation from the mean.
We say that a process P is -invariant if for each Vi , the value of VPi is less than .
Finally, we say that a process P is totally invariant if there exists a  which is close to 0 such that P is -invariant.
Path invariance means that there is never a large dierence in the cost among dierent paths through N throughout a process P .
More precisely, let c(t1 I ) and c(t2 I ) be the costs of any two of the n!
tours through a duration network N with n variables, given I .
Then path invariance implies that the the dierence between these values is not greater than some small value .
Strong invariance is a global property of a process: intuitively, it says that the duration of any variable or edge of a duration network never strays excessively from the mean.
This does not allow a \real good" path ever to become \real bad", although it may become less good.
Weak invariance is a strictly local phenomenon: it constrains every pair Ii Ii+1 of consecutive instantiations in P to be \close in their assignments" to all elements of N .
(This notion can be made precise in an obvious manner.)
Thus, weak invariance allows for a good path to become bad over the long run.
We can generalize any of these notion of invariance to (partial) invariance, in which a  subset of I exhibits invariance.
Again, for our purposes, it is enough to leave this intuitive notion qualitative.
We view the world as exhibiting varying degrees of invariance and periodicity.
An intelligent agent can learn and apply knowledge about invariance and periodicity in order to make plans which are intelligent.
This is the case although the knowledge the agent has is incomplete, and partial, and the world is in constant ux.
In the next section, we consider this capability in the context of constructing the tour series Tmest.
3 Computational Theory As noted, the ability of an agent to eectively solve the class of problems abstractly characterized as a traversal of a duration network depends on The goal of the game The properties of P  and Assumptions about the agent's knowledge of N and P .
For example, if the agent is given the requisite knowledge to determine P , then it does not matter whether P exhibits any invariance or periodicity: the agent will be able to \precompute" an optimal Tmest based on an exhaustive search of each instantiation.
The case to be examined here is the one in which the knowledge the agent has of P is, at best, partial.
In this section, we describe a version of the game in which 1.
The agent has, initially, an \abstract map" of N  2.
The agent has no quantitative knowledge about P  3.
P exhibits total strong duration and path invariance.
4.
The goal of the game is for the agent to construct Tmest .
We next present a computational theory which explains and realizes this behavior.
To solve for a goal, given the initial constraints, the agent needs to have a means to learn and apply knowledge it discovers about P to select a tour tj , given t1 : : : tj 1, as part of a series.
To make use of the rewards aorded by certain paths, we introduce the notion of relative mean duration: Denition 4 (Relative Duration) Let N = (V E ) be a duration network, T = ht1 : : :tk i be a tour series and P = hI1 : : : Ik i be a process.
The relative duration of an event Vi with respect to an event Vj in an instantiation In and corresponding tour tn (rd(hVi  Vj i tn In)) is c(tn=hVi  Vj i In) + c(tn=hVj  Vii In ).
Furthermore, the relative mean duration of an event Vi with respect to an event Vj over a set of k occurrences of Vi and Vj (rmdViVj (I T ))is ;  C (T=hVi Vj i I ) + C (T=hVj  Vii I ) k Let rmdViVj (I T ) denote the standard deviation of rmdViVj (I T ).
If I and T are given, the notation for these values is simplied to rmdViVj and rmdVi Vj .
Intuitively, relative duration is the cost of the leg Vi < Vj or Vj < Vi in a tour, given an instantiation of the variables and the edge connecting them.
Since the relation of \sharing a stage" is symmetrical, these costs are assumed to be identical e.g., any reward for pairing cleaning actions K and B in immediate temporal proximity will be collected, whether the order be K < B or B < K .
Relative mean duration, then, consists of the average relative duration of pairs of events over a set of tours in a series.
Assuming P exhibits total duration and path invariance, an agent can incrementally  '$ &% '$ &%  '$ &% '$ &%  8 (((( V2 ((( ( V1 11 ;; CC QQ CC QQ ;; CC 8 Q ; 8 CC CC ; QQQ 6  CC ;; V3  6  QQ C Q  V4  Figure 3: A Possible -Graph Associated with Figure 2  learn the requisite knowledge for constructing Tmest on the basis of computing and storing relative mean durations.
This information will be stored in what will be called a -network: Denition 5 Given a duration network N and a process P , a -network for N = (V E ) is a weighted undirected network with the following characteristics.
Each vertex is labeled by one of the elements in a set V .
Each edge (Vi  Vj ) is labeled.
The value of the label represents rmdViVj .
Figure 3 is an example of a -network corresponding to the duration network in the previous gure.
The labels on each edge represent values for rmdViVj .
These values would be accurate, for example, at the end of a kprocess P consisting of k repetitions of the instantiation depicted in the previous gure.
With the information in the -network, an agent can determine the next best tour in Tmest .
Let us assume that the process P exhibits strong duration and path invariance, but incorporates no assumptions about periodicity.
The method TS for constructing Tmin is summarized in Figure 4.
For the sake of simplicity, there is an assumption of a \learn-  UpdateMean(var :  ; graph t : tour I :  instantiation k : index)  Algorithm TS Input:  A process P = hI1 : : :  Iki A Duration Network N = (V E ), V = fV1  V2  : : :  Vn g, initialized by an instantiation Iinit A -network = (V  E ), where V = V and E = E .
For each edge Eij in , let v(Eij ) represent the value of the label on that edge.
Initially, this value is 0.
Output: The updated -network , which now contains statistical information about P based on its having executed a tour series Tmest = htmest1  : : :tmestk i and C (Tmest I ).
For each edge Eij in do v(Eij )   rd(hVi  Vj i Iinit) k   1 Tmest   hi  loop  tk   HamiltonPath( ) UpdateMean(  tk  Ik k) Tmest   Tmest + htk i /* hui + hvi = hu vi */ k  k+1 until k = p Return and C (Tmest P ) Figure 4: Algorithm for Constructing a Tour Series which Minimizes Overall Extent  begin For each edge Eij = hVi  Vj i in do rmdViVj   c(tk =hVi  Vj i I ) + c(tk =hVj  Vii I ) if rmdViVj > 0 then v(Eij )   (k 1)v(Eijk)]+rmdViVj end ;  Figure 5: Updating Algorithm for -graph ing phase" in which the agent is supplied values for one instantiation Iinit of N .
This can be viewed as, e.g., a robot being supplied a \map" of the world it needs to navigate repeatedly.
The main loop iteratively generates tk , the next tour in the series, from by performing a Hamilton Tour of this network, and updates based on information it has acquired about Ik as the result of its tour tk .
The Hamilton tour gives the best estimate of the tour with the lowest overall extent.
The \nal score" of the game is the overall cost of the tour series Tmest.
The UpdateMean Algorithm records the cost of tk as the result of Ik , by updating the -network accordingly.
The procedure is summarized in Figure 5.
This procedure simply updates the mean relative duration rmdViVj for each edge in , based on the result of the cost of traversing this edge in Ik by tour tk (if this tour contains this leg if not, then this cost is 0 and no updates are made).
This algorithm, we claim, realizes behavior which, under the constraints posed by this version of the game, is useful in the generation of intelligent orderings of a set of events.
As a variation on the game, suppose the agent is interested, not in reducing overall extent, but rather in reducing the duration uncertainty associated with a set of events.
This would be the case, e.g., if the agent has no constraints on the time of the completion  of a set of tasks, but wanted to be reasonably sure, at each moment in every tour, on which leg of the tour it is located.
A minor modication of the game in the preceding section will allow the agent to estimate a tour series Tdu, which approximates the tour series which is minimal with respect to duration uncertainty.
Again, let rmdViVj be the standard deviation of the relative mean duration of a set of occurrences of events Vi and Vj in immediate temporal succession.
Imagine modifying the -network so that the labels on each edge Eij stores values of rmdViVj .
We can replace UpdateMean with a procedure, call it UpdateSD, for updating standard deviations, based again on the result of the most recent tour.
Then, applying TS with UpdateSD computes Tdu , which estimates the tour series with the minimal duration uncertainty.
Numerous other enhancements to the representation are possible.
For example, incorporating duration uncertainty as a constraint would lead to a variation of the one-person game in which the agent's goal is to minimize the duration uncertainty associated with a tour.
Another enhancement to the game involves incorporating assumptions regarding periodicity to I would make such information useful to store in a -network.
Relative durations would be further relativized to time periods, which are represented by the index k on the instantiation Ik .
Relative mean durations, and their standard deviations, would be required to reect this relativization.
For this information, it is possible that a quantitative model for probabilistic temporal reasoning such as found in 2], could be applied alternatively, a qualitative model of recurrence, such as 3], might serve the same purpose.
4 Conclusion This paper has provided a framework for developing planning and scheduling systems in a dynamic world.
One primary assumption  motivating this framework is that events tend to exhibit varying degrees of duration uncertainty, and that an intelligent agent needs to confront this uncertainty in planning situations.
One aid in reducing duration uncertainty exploits the fact that events share stages with other events.
References  1] Drummond, M. Bresina, J. Swanson, K., 1994.
Just-In-Case Scheduling.
In Proceedings of the Twelfth National Conference on Articial Intelligence (AAAI94).
AAAI Press/MIT Press, Menlo Park, 1994:1098-1104.
2] Goodwin, Scott D., Hamilton, H. J., Neufeld, E., Sattar, A., Trudel, A.
Belief Revision in a Discrete Temporal Probability-Logic.
Proceedings of Workshop on Temporal Reasoning, FLAIRS94, 3] Morris, R., Shoa, W., and Al-Khatib, L., (1994) Domain Independent Reasoning About Recurring Events.
Forthcoming in The Journal of Computational Intelligence.
F2), F2-F1 and F1\F2.
The Qualitative Relations Manager is simply our implementation of Allen's operations of inversion, intersection and composition of Interval Algebra relations.
Finally, the third module implements our operations of inversion (not described in the paper), check-intersection and composition.
Given an I-Time de nition, one could ask the temporal location of the instances of the I-Time in a given Frame-Time (e.g., one could want in output the list of all Mondays between 1-1-90 and 1-6-90).
This operation is managed by the Unfolding Manager.
Besides these queries, TeMP also deals with queries concerning the KB of periodic events speci cations.
For example, one can ask which are the temporal relations between two periodic events ev1* and ev2*, in a given Frame-Time F. In such a case, TeMP gives as output the list of temporal speci cations of the form F'\F ev1* EACH C R ev2* in the KB of periodic events where F' is a Frame-Time intersecting F. TeMP has been implemented in Quintus Prolog and runs on Sun workstations, under Unix.
8 Conclusions  The temporal framework sketched in this paper constitutes an integration of part of the works developed by the two mainstreams of research about periodic events in AI and TDB, extending current approaches to deal with both user-de ned I-Times and qualitative temporal relations concerning periodic events.
Although we believe that our framework is signi cantly more powerful and expressive than the other approaches dealing with periodic events in the AI and TDB literature, many other aspects have to be taken into account in order to obtain a comprehensive approach to periodic events.
In particular, we are currently investigating the possibility of extending our framework for dealing with quanti ers such as "only", "sometimes" etc.
in Morris et al., 93] and with partial relations between I-Times.
This work was partially supported by the Italian CNR, project "Ambienti e strumenti per la gestione di informazioni temporali".
References  Allen, 83] J.F.
Allen: "Maintaining Knowledge about Temporal Intervals", Comm ACM 26(11), 832843 (1983).
Allen, 91] J. Allen: "Time and Time again: the Many Ways to Represent Time", International Journal of Intelligent Systems 6(4), 341-355 (1991).
Baudinet et al., 93] M. Baudinet, J. Chomicki, P. Wolper: "Temporal Deductive Databases", in A. Tansell, R. Snodgrass, J. Cliord, S. Gadia, A. Segev  (eds): Temporal Databases: Theory, Design, and Implementation, Benjamin-Cummings (1993).
Chandra and Segev, 93] R. Chandra, A. Segev: "Managing Temporal Financial Data in an Extensible Database", Proc.
19th International Conference on Very Large Databases (1993).
Chomicki and Imielinsky, 88] J. Chomicki, T. Imielinsky: "Temporal Deductive Databases and In nite Objects", Proc.
ACM Symposium on Principles of Database Systems, 61-73 (1988).
Kabanza et al., 90] F. Kabanza, J.-M. Stevenne, P. Wolper: "Handling In nite Temporal Data", Proc.
ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, 392-403 (1990).
Ladkin, 86a] P. Ladkin: "Primitive and Units for Time Speci cation", Proc.
AAAI'86, 354- 359 (1986).
Ladkin, 86b] P. Ladkin: "Time Representation: A Taxonomy of Interval Relations", Proc.
AAAI'86, 360-366 (1986).
Leban et al., 86] B. Leban, D.D.
McDonald, D.R.
Forster: "A representation for collections of temporal intervals", Proc.
AAAI'86, 367-371 (1986).
Ligozat 91] G. Ligozat: "On Generalized Interval Calculi", Proc.
AAAI'91, 234-240 (1991).
Morris et al., 93] R.A. Morris, W.D.
Shoa, L. Khatib: "Path Consistency in a Network of Nonconvex Intervals", Proc.
IJCAI'93, 655-660 (1993).
Niezette and Stevenne, 92] M. Niezette, J.-M. Stevenne: "An Ecient Symbolic Representation of Periodic Time", Proc.
First International Conference on Information and Knowledge Management (1992).
Poesio, 88] M. Poesio: "Toward a Hybrid Representation of Time", Proc.
ECAI'88, 247-252 (1988).
Soo and Snodgrass, 92] M. Soo, R. Snodgrass: "Mixed Calendar Query Language Support for Temporal Constraints", Tech.
Rep. TempIS No.29, University of Arizona (1992).
Soo, 93] M. Soo: "Multiple Calendar Support for Conventional Database Management Systems", Proc.
Int.
Workshop on an Infrastructure for Temporal Databases, (1993).
Stonebraker, 90] M. R. Stonebraker: Chapter 7: Extensibility.
Readings in Database Systems, M.R.
Stonebraker (editor), Morgan Kaufman (1990).
Terenziani, 95] P. Terenziani: "Integrating calendar-dates and qualitative temporal constraints in the treatment of periodic events", Tech.
Report 12-95, Dipartimento di Informatica, Universita' di Torino (1995).
Van Eynde, 87] F. Van Eynde: "Iteration, Habituality and Verb Form Semantics", Proc.
3rd Conf.
of European Chapter of the Association for Computational Linguistics, 270-277 (1987).
Property 2.
Our operation of check-intersection does not loose information (this is obvious for composition, which only adds new pieces of information).
Property 1 has been proved by showing that, for each case in the de nitions of check-intersection and composition, the logical description of the "antecedent" part of the de nition (including also the axioms formalising the relation between the I-Times being considered) implies the logical description of the "consequent" part.
For example, we proved the correctness of our de nition of composition in the case where the relation between the two I-Times being composed is 2 by proving that ev1* EACH C1* R1 ev2* ^ ev1* EACH C2* R2 ev3* ^ C1* 2 C2* !
ev2* EACH C1* R ev3* where R is the composition of R1 and R2 in Allen's Interval Algebra and each one of the speci cations has been replaced by the logical axioms describing its meaning.
Property 2 has been proved by showing that, for each case in the de nition of check intersection, the "consequent" part of the de nition, plus the axioms formalising the relation between the I-Times being considered imply the "antecedent" of each de nition.
For instance, in the case where the relation between the two I-Times being composed is temporal equality, we proved that ev1* EACH C1* R ev2* ^ C1* =T C2* !
ev1* EACH C1* R1 ev2* ^ ev1* EACH C2* R2 ev2* (where R is the intersection of R1 and R2, in Allen's Interval Algebra).
It is important to notice that Properties 1 and 2 grant that our operations of check-intersection and composition can be regarded as a compilation of a set of logical inferences that could also be performed (in a less ecient way), e.g, by a theorem prover for the  rst order logic.
Moreover, as a consequence of these properties, Corollary 1 holds.
Corollary 1.
PCforPE is correct and does not loose information (in fact, PCforPE is a pathconsistency algorithm repeatedly applying checkintersection and composition).
The temporal logic and the proofs are not reported in this paper for the sake of brevity, and are presented in Terenziani, 95].
7 The TeMP system  The TeMP system (Temporal Manager of Periodic events) has been realised on the basis of the approach described in this paper.
TeMP is a general purpose  temporal manager dealing with user-de ned calendric de nitions (I-Times) and temporal speci cations about periodic events.
The architecture of TeMP is shown in  gure 2: boxes denote modules and ovals represent data.
Interface Module Manipulation  I-Times definitions KB  Unfolding Manager  Query  Periodic Events KB  I-Times Relations KB  Heuristic Rules Manager  PCforPE  Frame Time Manager  Qualitative Relations Manager  Inversion Check-intersection Composition  Figure 2: Architecture of the TeMP system The Interface Module manages the interaction with the user, allowing the insertion/deletion of ITimes de nitions and temporal speci cations about periodic events (using the formalism described in section 2), as well as queries.
Insertions and deletions build up the knowledge base (KB) of Periodic Events speci cations and the KB of I-Times de nitions.
The Heuristic Rules Manager is an auxiliary module which operates on the de nitions of I-Times provided by the user and gives as output the relation holding between each pair of I-Times, on the basis of a set of heuristic rules (see section 4).
The basic reasoning module is PCforPE, which is based on the algorithm shown in section 5.
PCforPE takes in input the KB of temporal speci cations about periodic events and the KB of temporal relations between I-Times, and check the consistency of the temporal speci cations and infers new speci cations.
In order to operate, the PCforPE module takes advantage of three dierent modules, which operate on the dierent components of our temporal speci cations (i.e., <Frame>, <Qual-Rel> and <ITime> in the syntax (S)).
The Frame Time Manager takes in input two Frame-Times F1 and F2 and provides as output three (possibly empty) sets of FrameTimes: F1-F2 (i.e., the dierence between F1 and   C1 2 C2 (the inverse holds if C2 2 C1)  The result of the composition is ev2* Q C1 R ev3*.
Notice that C1 is the I-Time for the new speci cation between ev2* and ev3*.
For example, the composition of (s11) and (s12) is (s13).
 C1 2inc C2 (the inverse holds if C2 2inc C1) No new speci cation in our formalism can be inferred (the motivation is analogous to that discussed when dealing with ).
For example: a* EACH Christmas* (BEFORE,MEETS) b* @ a* EACH Months* (AFTER) c* {> NO NEW TEMPORAL SPECIFICATION  C1  C2 (the same holds if C2  C1) An inconsistency is reported, since it cannot be the case that ev1* happens exactly once both in C1 and in C2.
For example, consider again the composition of (s12) and (s14).
 C1 ] C2 (C1 and C2 are not comparable) No new qualitative relation can be inferred.
For example: a* EACH Mondays* (BEFORE,MEETS) b* @ a* EACH Tuesdays* (AFTER) c* {> NO NEW TEMPORAL SPECIFICATION  5.3 Reasoning Process  We developed an extension of Allen's path consistency algorithm Allen, 83] in order to reason with a knowledge base of temporal speci cations in our formalism.
Since path-consistency is not complete for Allen's Interval Algebra, a-fortiori it is not complete for our temporal speci cations (which include the Interval Algebra for specifying the qualitative constraints): as in many AI approaches dealing with the Interval Algebra (see, e.g., the survey in Allen, 91]), we chose to loose the completeness of the reasoning process in order to retain tractability.
Dierently from Allen, 83], we must consider multiple temporal speci cations relating the same pair of events.
We denote as Sa (ev1*, ev2*) a temporal speci cation relating ev1* and ev2* "S" is indexed in order to distinguish among dierent speci cations relating ev1* and ev2*.
Figure 1 sketches our path consistency algorithm for periodic events (PCforPE).
In PCforPE, Paths(Sa (ev1*, ev2*)) contains, for each event evh* (ev1* 6= evh* 6= ev2*), all the speci cations relating ev1* and evh* -i.e., Si (ev1*, evh*), for all i and for all event evh*-).
STACK is a stack containing all the new speci cations.
Before the execution of PCforPE, all the input speci cations are pushed onto STACK.
Both check-intersection and composition may generate new speci cations, which are pushed onto the STACK, in order to propagate  FORALL speci cation Sa (ev1*, ev2*) in STACK DO POP Sa (ev1*, ev2*) from STACK FORALL speci cation Si (ev1*, ev2*) DO X <{ Sa (ev1*, ev2*) \ Si (ev1*, ev2*) PUSH the new speci cations in X (if any) onto STACK OD FORALL speci cation Si (ev1*, evh*) in Paths(Sa (ev1*, ev2*)) DO X <{ Sa (ev1*, ev2*) @ Si (ev1*, evh*) PUSH the new speci cations in X (if any) onto STACK OD OD Figure 1: PCforPE algorithm the new constraints they convey.
The algorithm stops when the stack is empty or when an inconsistency is reported (by check-intersection or composition).
Check-intersection and composition do not generate either new bounds (dierent from the upper and lower bounds of the input Frame-Times) nor new ITimes.
Let H and K be the number of bounds and I-Times introduced by the user in the input speci cations.
Thus, at most O(H 	 K ) speci cations may hold between the same pair of periodic events.
Thus, PCforPE considers considers O(H 	 K 	 N 2) dierent temporal speci cations (where N is the number of events in the knowledge base).
Each of them can be pushed onto STACK at most 13 times (due to the fact that an ambiguous qualitative relation is at most a disjunction of 13 basic Interval relations, so that it can be reduced at most 13 times Allen, 83]).
Whenever a speci cation S is pushed onto the stack, PCforPE performs at most O(H 	 K ) check-intersections and O(H 	K 	N ) (the cardinality of Paths(S )) compositions.
Thus, PCforPE operates in polynomial time, performing O(H 2 	 K 2 	 N 2 ) check-intersections and O(H 2 	 K 2 	 N 3 ) compositions.
6 Temporal logic and properties of the approach We provided a logical formalization for the dierent components of our temporal approach and, besides the others, for (i) the basic notions of correlation and association, (ii) the temporal speci cations in our formalism, (iii) the relations between I-Times (e.g., ).
On the basis of our  rst order temporal logic for periodic events we proved the following properties: Property 1.
Our operations of check-intersection and composition are correct  (where Q is the quanti er EACH, C1 and C2 are I-Times and R1 and R2 are qualitative relations in Allen's Interval Algebra) in the intersection d1', d1"] \ d2', d2"] of the Frame-Times is de ned by cases (in the following, R indicates the intersection between the Interval Algebra relations R1 and R2):  C1 =T C2 If R is the null relation, an inconsistency is reported.
Otherwise, the result of checkintersection is ev1* Q C1 R ev2*.
For example: a* EACH Days* (BEFORE,MEETS) b* \ a* EACH Days* (BEFORE,OVERLAPS) b* {> a* EACH Days* (BEFORE) b*  C1  C2 (the inverse holds if C2  C1) If R is empty, an inconsistency is reported.
Otherwise, the result of check-intersection is ev1* Q C1 R ev2* and ev1* Q C2 R ev2* (the intersection of the qualitative temporal relations is selected in the output speci cation).
E.g., a* EACH Mondays* (BEFORE) b* \ a* EACH Weeks* (BEFORE,MEETS) b* {> a* EACH Mondays* (BEFORE) b*, a* EACH Weeks* (BEFORE) b* Notice that both speci cations must be provided in output, since they conjunctively convey the information that a* and b* occur exactly once each Weeks*, and, more speci cally, in the Mondays* part of Weeks*.
 C1 2 C2 (the inverse holds if C2 2 C1) If R is empty, an inconsistency is reported.
Otherwise, the result of check-intersection is ev1* Q C1 R ev2* and ev1* Q C2 R2 ev2*.
In fact, the qualitative relations holding in the restricted ITime must be forced to be compatible those holding in general.
For example, since Mondays* 2 Days*, the result of check-intersection on (s8) and (s7), in the overlapping part of the Frame-Times (i.e., in 1-6-91, 1-1-92]) is the pair of speci cations (s10) and (s11) above.
 C1 2inc C2 (the inverse holds if C1 2inc C1) If R is empty, an inconsistency is reported.
Otherwise, the result is ev1* Q C1 R ev2* and ev1* Q C2 R2 ev2*.
For example: a* EACH Christmas* (AFTER,STARTS) b* \ a* EACH Months* (AFTER,MEETS) b* {> a* EACH Christmas* (AFTER) b*, a* EACH Months* (AFTER,MEETS) b*  C1  C2 (the same holds if C2  C1) In such a case an inconsistency is reported.
For example:  a* EACH Days* (BEFORE,MEETS) b* \ a* EACH Months* (BEFORE) b* gives an inconsistency, since a* cannot happen exactly once a day and exactly once a month.
 C1 ] C2 The temporal speci cations provide two dierent constraints between the same pair of periodic events, holding at "incomparable" I-times.
In such a case no new qualitative constraint can be inferred, and the input constraints are left unchanged (with the implicit meaning that both of them must hold in the intersection -if any- of the instances of the two I-Times).
For example: a* EACH Mondays* (MEETS) b* \ a* EACH Tuesdays* (AFTER) b* {> INPUT SPECIFICATIONS UNCHANGED  5.2 Composition  Composition (@) applies to two speci cations of periodic events as in (s16) (s16) d1', d1"] ev1* Q C1 R1 ev2* @ d2', d2"] ev1* Q C2 R2 ev3* No new information can be inferred as regards the non-intersecting parts of the two Frame-Times.
In the time interval d1', d1"] \ d2', d2"] (if any), the result of composition depends on which relation holds between the I-Times C1 and C2 (in the following, R represents the composition, in Allen's Interval Algebra, of the inverse of R1 with R2).
 C1 =T C2 The composition of the two speci cations is ev2* Q C1 R ev3*.
For example: a* EACH Days* (BEFORE,MEETS) b* @ a* EACH Days* (AFTER) c* {> b* EACH Days* (AFTER) c*  C1  C2 (the inverse holds if C2  C1) In such a case, no new speci cation in our speci cation formalism can be inferred.
For instance, a* EACH Mondays* (BEFORE, MEETS) b* @ a* EACH Weeks* (AFTER) c* {> NO NEW SPECIFICATION Notice that it would not be correct to infer either (i) b* EACH Mondays* (AFTER) c* or (ii) b* EACH Weeks* (AFTER) c*.
In fact, (i) corresponds to arbitrarily assume that c* happens each Mondays*, while (ii) corresponds to arbitrarily assume that b* happens once a week (while we only have that it happens once each Mondays*, so that it could happen also, e.g., on Tuesdays).
Thus, neither (i) nor (ii) are implied by the input speci cations.
Thus, we cannot propose a compact de nition of intersection and composition such as in Allen, 83], Morris et al., 93].
On the other hand, we have to point out a set of basic relations between I-Times (see section 4) and to propose a de nition by cases of check-intersection and composition, on the basis of the relation holding between the I-Times in the speci cations.
4 Relations between I-Times  Given two I-Times C1* and C2*, since we use the quanti er EACH ("exactly once each") in the temporal speci cations, we are interested in the cases where for each instance of C1* there is just a related instance of C2* and vice versa (bijective relation between instances of C1* and of C2*) or in the cases where for each instance of one of the two I-Times (say C1*) there is just a related instance of the other I-Time (say C2*) but not viceversa.
The treatment of the partial relations which do not cover all instances of at least one of C1* and C2* requires an extension of our formalism which is discussed in Terenziani, 95].
Moreover, the cases to be considered for temporal reasoning are those in which C1* and C2* allow one to refer to the same instances of a periodic event, i.e., those cases where there is a relation of temporal containment between the corresponding instances of C1* and C2* (C1* EQUAL C2* is a special case of temporal containment, which must be distinguished since it allows one to draw further inferences).
The disjoint relations =T , , 2 and 2inc (plus inverses) cover these cases.
Given two I-Times C1* and C2*,  C1* =T C2* (read as: C1* and C2* are temporally equal) i there is a bijection between instances of C1* and instances of C2*, and Allen's relation EQUAL holds between each pair of corresponding instances.
 C1*  C2* (C1* is more speci c than C2*) i for each instance of C1* there is exactly one instance of C2* which properly contains it and, conversely, for each instance of C2* there is exactly one instance of C1* which is properly contained in it (bijection) (e.g., Mondays*  Weeks*).
 C1* 2 C2* (C1* is a restriction of C2*) i for each instance of C1* there is an instance of C2* which is temporally equal to it, but not vice versa (e.g., Mondays* 2 Days*).
 C1* 2inc C2* (C1* is an inclusion restriction of C2*) i for each instance of C1* there is an instance of C2* which properly contains it, but not vice versa (e.g., Christmas* 2inc Months*).
Besides these relations, it is important to introduce two further relations.
 C1*  C2* (C1* is more frequent than C2*) characterises the cases where two assertions such as (i) "eventx happens exactly once each C1*" and (ii) "eventx happens exactly once each C2*" are inconsistent in a given FrameTime I.
Roughly speaking, this happens when, in any way we choose a time interval in each instance of C1* in I, at least two of these time intervals intersect the same instance of C2* (e.g., Days*  Weeks* see Terenziani, 95] for a formal de nition of  and of the relations =T , , 2 and 2inc).
 C1* ] C2* (C1* and C2* are temporally incomparable) i none of the above relations (or their inverses) hold between C1* and C2* (e.g., Mondays* ] Tuesdays*).
We devised a set of heuristic rules for determining automatically which one of the 6 relations above holds between two user-de ned I-Times.
For instance, rule (IT) states that a de nition of the form C1*   n / C2* :during: C3* implies C2*  C3*, C1*  C3* and C1* 2 C2* (e.g., from the de nition Aprils*   4 / Months* :during: Years* we have that Months*  Years* and Aprils*  Years* and Aprils* 2 Months*).
Our rules proved to be powerful enough to cover "non exceptional" cases.
However, since the user is completely free in the use of the speci cation language for I-Times, they do not cover all possible cases.
If no relation between a pair of I-Times is determined by the heuristic rules, the relation is asked to the user.
5 Check-Intersection, Composition and Reasoning Process 5.1 Check-intersection  Check-intersection (\) operates on two temporal speci cations involving the same pair of periodic events and works in two steps.
First, the intersection of the two Frame-Times is computed.
If it is empty, then no further operation must be devised, and the original speci cations are left unchanged.
Otherwise, (i) the original speci cations are left unchanged as regards the non-intersecting parts of the Frame-Times and (ii) in the intersecting part of the Frame-Times, check-intersection forces the compatibility of the qualitative temporal relations, depending on the relation between the I-Times.
More speci cally, the value of the check-intersection operation (s15) d1', d1"] ev1* Q C1 R1 ev2* \ d2', d2"] ev1* Q C2 R2 ev2*  (S) <Frame> ev1* <Quant> <I-Time> <Qual-Rel> ev2* where <Frame> is speci ed as the range of time spanning between a starting point and an ending point (e.g.
1-1-90, 1-6-94]), <Quant> is the quanti er "EACH", which stands for "exactly once each", <ITime> is speci ed as in Leban et al., 86] and <QualRel> is a (possibly ambiguous) relation in Allen's Interval Algebra.
For instance, the temporal content of Ex.1 above can be represented by (s4) (given the de nitions of I-Times in (s1-s3)): (s4) 1-1-90, 1-6-94] Sam-visits-oce-X01* EACH First-Monday-of-Aprils* (BEFORE) Sam-goes-to-his-oce* The meaning of a temporal speci cation of the form d1, d2] ev1* EACH C R ev2* is the following: for each instance C' of the I-Time C in the Frame-Time d1, d2], (i) there is one and only one instance ev1' of the periodic event ev1* and one and only one instance ev2' of ev2* associated with C' and (ii) ev1' and ev2' are correlated, and the temporal relation R holds between them.
Our approach also deals with temporal speci cations in which the I-Time is omitted.
For instance, Ex.2 can be speci ed in our formalism by (s5) (s5) (-1, +1) EACH John-works* (BEFORE) Mary-works* with the meaning that, in the Frame-Time (-1, +1), there is a one-to-one correspondence between instances of John-works* and instances of Maryworks*, and the relation BEFORE holds between the temporal extent of each correlated pair of instances.
We also deal with temporal speci cations in which only the I-time of a periodic event is speci ed.
For example Ex.5 can be represented by (s6), Ex.5 "Between 1-1-90 and 1-1-91 John run each Monday" (s6) 1-1-90, 1-1-91] John-runs* EACH Mondays* with the meaning that, between 1-1-90 and 1-1-91, there is exactly one instance of John-runs* associated with each Monday.
For the sake of brevity, in this paper we only consider temporal speci cations expressed according to the schema (S) above.
The complete description of our formalism is proposed in Terenziani, 95].
3 Intersection and Composition of temporal speci	cations  Since our temporal speci cations consider also ITimes and Frame-Times, new problems have to be faced when de ning intersection and composition.
As  regards intersection, for instance, in our approach it is no longer true that at most one speci cation may relate each pair of events.
In fact, dierent temporal speci cations may be involved at dierent FrameTimes, or even in equal or overlapping Frame-Times (consider, e.g., (s7) and (s8)).
(s7) 1-6-91, 1-1-92] mail* EACH Days* (BEFORE, MEETS) visit* (s8) 1-1-91, 1-1-92] mail* EACH Mondays* (BEFORE, AFTER) visit* However, the consistency of the temporal speci cations on the "overlapping parts" of the Frame-Times and of the I-Times must be checked.
For instance, given (s7), the AFTER relation between mail* and visit* asserted in (s8) is not possible on Mondays since 1-6-91 until 1-1-92, and must be ruled out.
Thus, we have to introduce an operation of "checkintersection" (indicated as \), which gives the intersection of two temporal speci cations (or an inconsistency) just in case the temporal speci cations overlap in a given "context" (e.g.
since 1-6-91 until 1-1-92 on Mondays* in (s7) and (s8)).
Check-intersection may give in output more than one temporal speci cation e.g., the application of check-intersection to (s7) and (s8) gives as result (s9), (s10) and (s11): (s9) 1-1-91, 1-6-91) mail* EACH Mondays* (BEFORE, AFTER) visit* (s10) 1-6-91, 1-1-92] mail* EACH Days* (BEFORE, MEETS) visit* (s11) 1-6-91, 1-1-92] mail* EACH Mondays* (BEFORE) visit* Of course, the results of check-intersection crucially depend on the relations between the I-Times of the input speci cations.
For instance, if we put the Itime Tuesdays* in (s7) instead of Days*, the AFTER relation in (s8) has no longer to be ruled out.
Analogously, also composition depends on the relation between the I-Times of the input speci cations.
For instance, the composition of (s11) above and (s12) gives as result (s13): (s12) 1-6-91, 1-1-92] mail* EACH Days* (AFTER) meeting* (s13) 1-6-91, 1-1-92] visit* EACH Mondays* (AFTER) meeting* On the other hand, the composition between (s12) and (s14) reports an inconsistency, since it is not possible that mail* happens exactly once each day and once each week.
(s14) 1-6-91, 1-1-92] mail* EACH Weeks* (AFTER) meeting*  Ex.2 "John always works before Mary" Ex.3 "Bill always works (only) during Mary's work" provides the information that John always works before Bill (see, e.g., Morris, 93]).
However, the approaches in this mainstream deal only with "contextindependent" temporal speci cations, in which no Frame-Time and no I-Time is considered (see, e.g., Ex.2 and Ex.3).
This limitation allow these approaches to propose compact de nitions of composition and intersection (see, e.g., Morris, 93]), but compromises their practical applicability.
Our goal is that of extending the approaches in the second mainstream for dealing also with the "context" (Frame-Time and I-Time) in which periodic events occur (see, e.g., Ex.1), in order to increase their expressiveness and their practical applicability to areas such as scheduling, process-control,  nancial trading, work ow and oce automation.
In section 2, we introduce our formalismfor dealing with I-Times and qualitative relations between periodic events.
In section 3, we discuss some of the main problems in the de nition of intersection and composition of temporal speci cations expressed in our formalism.
In section 4, we distinguish between six dierent types of relations between I-Times.
These relations are then used in section 5 for de ning intersection and composition.
In section 5 we also introduce a path-consistency algorithm which uses intersection and composition for performing temporal reasoning, and discuss its complexity.
In section 6, we sketch some of the properties (e.g., correctness) of our approach, and in section 7 we brie y describe the architecture of TeMP, a temporal manager of periodic events which is based on the approach described in this paper.
2 Temporal Representation of Periodic Events  We assume time to be a linear order on a domain consisting of points.
A time interval I is a convex set of points between a starting and an ending point.
As in Leban et al., 86], we de ne a collection of intervals as an ordered set of non-overlapping time intervals.
Time intervals are the temporal extents in which events take place.
Collections of time intervals represent the collection of the temporal extents upon which the dierent instances (realisations) of a periodic event take place.
The formalism we use for specifying I-Times is that in Leban et al., 86], who introduced a notation for de ning basic calendars and two types of operators on collections of intervals (dicing -e.g., ":during:" in (s1) - and slicing -e.g., "2 /" in (s1)-) for building new user-de ned collections on  the basis of the basic calendars.
For example, given the basic de nitions of Days*, Weeks* and Months*, the collection of the  rst Mondays of April can be incrementally de ned as follows: (s1) Mondays*   2 / Days* :during: Weeks* (s2) Aprils*   4 / Months* :during: Years* (s3) First-Monday-of-Aprils*   1 / Mondays* :during: Aprils* In order to deal with the qualitative temporal relations between periodic events, we adopt the qualitative relations of Allen's Interval Algebra Allen, 83].
However, since Allen's relations hold between pairs of time intervals, and periodic events happen over collections of time intervals, a way for relating pairs of time intervals belonging to dierent collections is needed.
As in Morris et al., 93], we introduce the equivalence relation of correlation between pairs of instances of periodic events, which holds as a result of some contingent relation in the world between them.
For instance, in Ex.1, correlation holds between each corresponding pair of Sam visiting the branch oce X01 and Sam going to his oce, and the temporal relation "before" holds between (the temporal extents of) each pair of correlated instances.
Since we deal with qualitative relations which hold in an I-Time, we consider also the relation of association, which relates each instance of a periodic event with the instance of the I-Time in which it occurs.
In particular, an instance e of a periodic event which occurred in a time interval i is associated with an instance p of an I-Time if and only if i is contained in p. Since the user may introduce more than one speci cation concerning the very same type of event, s/he may want to specify that these speci cations concern the same periodic event (in this case, we say that the dierent speci cations co-designate Morris et al., 93] the same periodic event) or dierent periodic events of the same type.
For example, Ex.4 introduces two dierent periodic events of the same type "John brushing his teeth".
In other words, Ex.4 involves two dierent collections of time intervals in which John brushes his teeth.
Ex.4 "Each day, John brushes his teeth after breakfast and after lunch" In our approach, dierent indexes are used to distinguish between dierent collections of the same type of event (e.g., John ; brush1 and John ; brush2 ).
Indexes will be omitted in the rest of the paper, for the sake of clarity.
2.1 Temporal formalism  In our approach, complex speci cations of periodic events can be provided, according to the syntax (S):  Reasoning about Periodic Events P. Terenziani  Dipartimento di Informatica, Universita di Torino, Corso Svizzera 185, 10149 Torino, Italy E-mail: terenz@di.unito.it  Abstract The paper describes a temporal formalism which deals with both (i) quantitative information concerning the frame of time and the user-de ned calendar-dates in which periodic events are located and (ii) the qualitative relations between periodic events.
The paper de nes the operations of intersection and composition of temporal speci cations, and describes an algorithm which takes advantage of these operations for performing temporal reasoning.
This is the kernel of TeMP, a temporal manager of periodic events.
1 Introduction Periodic events are widely studied in many research areas, such as Arti cial Intelligence (AI) and Temporal Databases (TDB).
In particular, most AI and TDB approaches provided a high-level, powerful and user-friendly formalism for representing periodic events, and (especially in AI) some form of temporal reasoning operating on them.
This work belongs to such a stream of research, and aims at providing a framework in which it is possible to deal also with very rich temporal speci cations, such as Ex.1 "Between 1-1-90 and 1-6-94, each  rst Monday of April Sam visited the branch oce X01 before going to his oce " (following VanEynde, 87], we call Frame-Time the interval which contains all the instances of the event -e.g., "From 1-1-90 to 1-6-94" in Ex.1-, I-Time the periodic time interval over which periodic events take place, which is usually expressed by some calendric expression -e.g., " rst Monday of April" in Ex.1- and e-Time the time in which the actual instance of the periodic event occurred -e.g., "before going to his of ce").
Current AI and TDB approaches do not allow one to deal with such complex speci cations.
In particular, current approaches can be roughly divided  into two mainstreams, depending on the types of temporal information they deal with.
In the  rst mainstream (carried on especially in the TDB community) most attention is devoted to the treatment of I-Times (see, for instance, Leban et al., 86], Chomicki and Imielinsky, 88], Kabanza et al., 90], Soo and Snodgras, 92], Baudinet, 93], Chandra and Segev, 93], Soo, 93]).
These approaches are based on the consideration that, in most cases, periodic events are "context-dependent", in the sense that they take place at speci c periods of time (ITimes).
Moreover, since dierent calendric systems are used for specifying I-Times (depending e.g., from cultural and social factors see e.g., Soo, 93]), most of these approaches stress the necessity of dealing with user-de ned calendars.
This is necessary, e.g., for dealing with temporal information in many areas, such as scheduling, manufacturing, process-control,  nancial trading, work ow and oce automation (see, e.g., the discussions in Stonebraker, 90], Chandra and Segev, 93] as regards  nancial trading, and consider Soo and Snodgrass, 92], Soo, 93] for a more general discussion).
However, the approaches in this mainstream do not consider the possibility of specifying the e-Time of a periodic event in term of its relative position with respect to other periodic events (as, e.g., the qualitative relation "before" in Ex.1).
As a consequence, these approaches devised only limited forms of temporal reasoning.
On the other hand, the approaches in the second mainstream (carried on especially in the AI community) focus on the treatment of quanti ers and eTime, stressing the importance of dealing with qualitative relations between periodic events (see, for instance, Ladkin, 86a, 86b], Poesio, 88], Ligozat, 91], Morris et al., 93]).
This involves, among other things, the development of some form of temporal reasoning.
Following Allen, 83], also temporal reasoning about periodic event has been usually performed using the basic operations of intersection, for checking the consistency of temporal speci cations, and composition, for inferring new speci cations for example, the composition of Ex.2 and Ex.3
An algebraic approach to granularity in time representation Jerome Euzenat INRIA Rhone-Alpes, IMAG-LIFIA 46, avenue Felix Viallet, 38031 Grenoble cedex, France Jerome.Euzenat@imag.fr  Abstract Any phenomenon can be seen under a more or less precise granularity, depending on the kind of details which are perceivable.
This can be applied to time.
A characteristic of abstract spaces such as the one used for representing time is their granularity independence, i.e.
the fact that they have the same structure at different granularities.
So, time "places" and their relationship can be seen under different granularities and they still behave like time places and relationship under each granularity.
However, they do not remain exactly the same time places and relationship.
Here is presented a pair of operators for converting (upward and downward) qualitative time relationship from one granularity to another.
These operators are the only ones to satisfy a set of six constraints which characterize granularity changes.
1 .
Introduction "Imagine, you are biking in a flat countryside.
At some distance ahead of you there is something still.
You are just able to say (a) that a truck (T) is aside a house (H), it seems that they meet.
When you come closer to them (b) you are able to distinguish a bumper (B) between them, and even closer (c), you can perceive the space between the bumper and the house."
This little story shows the description of the same reality perceived at several resolution levels: this is called granularity.
Granularity would not be a problem if different individuals, institutions, etc.
would use the same granularity.
This is not the case and, moreover, these individuals communicate data expressed under different granularities.
There could be a problem if, for instance, someone at position (a), asked "how would you call that which is between H and T?"
because at that granularity, the description of the scene would assume that there is nothing between H and T. The study of granular knowledge representation thus tries to express  how the same phenomenon can, in some sense, be consistently expressed in different manners under different granularities.
This is achieved through operators which, for a situation expressed under a particular granularity, can predict how it is perceivable under another granularity.
coarser (a)  (b) upward  downward  T  H B  (c) finer  Figure 1.
The same scene under three different granularities.
This is taken as a spatial metaphor for time granularity and is used throughout the paper.
Granularity can be applied to the fusion of knowledge provided by sources of different resolution (for instance, agents -- human or computers -- communicating about the same situation) and to the structuring of reasoning by drawing inference at the right level of resolution (in the example of figure 1, the first granularity is informative enough for deciding that the truck driving wheel is on the left of the house -- from the standpoint of the observer).
On one hand, in [10], granularity is expressed granularity between two, more or less detailed, logical theories.
On the other hand, the physical time-space and its representation have been well-studied because many applications require them.
A very popular way to deal with time is the representation of relationships between time intervals [2].
To our knowledge, qualitative time granularity has never been studied before.
Jerry Hobbs [10] introduced granularity in an abstract way  (i.e.
not connected to time) and [11, 4] introduced operators for quantitative time granularity which share a common ground with ours (see SS7).
The paper first recalls some basics about time representation (SS2).
This section can be skipped by those who already know the subject.
Then, the usual interpretations of time and granularity in this context are introduced.
Afterwards, required properties for granularity change operators in the classical time algebra are presented (SS3).
This part is very important since, once accepted the remainder is directly deduced.
The only set of operators (for instant and interval algebra) satisfying the required properties are thus deduced in SS4.
The results concerning the relationship between granularity and inference are then briefly presented (SS5).
The proofs of all the propositions, but the "only" part of the first one, can be found in [6].
The main results (but those of SS5) are from [7].
2 .
Background Classical notions about temporal algebras, neighborhood structures and instant-interval conversions are presented here.
2.1.
Temporal algebra  reciprocal: x2 r-1x1 after (>)  x1/x2  simultaneously (=)  =  Table 1.
The 3 relationships between instants x1 and x2.
x3 > = <  > > > <=>  relation: x1 r x2 before (b) during (d) overlaps (o) starts (s) (and finishes before) finishes (f) (and start after) meets (m)  x1/x2  = > = <  < <=> < <  Table 2.
Composition table between instant relationships.
It is sometimes possible to deduce the relationship between two instants x and z, even if it has not been provided, by propagating the otherwise known relationships.
For instance, if x is simultaneous ({=}) to  reciprocal: x2 r-1x1 after contains overlapped by started by (and finishes after) finished by (and starts before) met by e  equals (e)  There has been considerable work carried out on qualitative time representation.
We recall here several notions about the algebra of topological and vectorial relationships holding between time entities.
An instant is a durationless temporal entity (also called time point by analogy with a point on a line).
It can be numerically represented by a date.
Qualitatively representing these instants requires identifying them and putting them in relation.
There are three possible mutually exclusive relationships between instants.
They are called <<before>> (<), <<after>> (>) and <<simultaneously>> (=).
The set {<, =, >} is called A3.
relation (r): x1 r x2 before (<)  y which is anterior ({<}) to z, then x is anterior to z; this is called composition of temporal relations.
The composition operator x3 is represented by a composition table (table 2) which indeed indicates that =x 3< gives {<}.
A (continuous) period is a temporal entity with duration.
It can be thought of as a segment on a straight line.
A numerical representation of a period is an interval: a couple of bounds (beginning instant, ending instant) or a beginning instant and a duration.
Intervals can be manipulated through a set of 13 mutually exclusive temporal relationships between two intervals (see table 3); this set is called A13.
Table 3.
The 13 relationships between two intervals x1 and x2.
The composition operator x 13 is represented by a composition table [2], similar to the table 2, which allows to deduce, from a set of intervals and constraints between these intervals, the possible relations between any two of these intervals.
2.2.
Extensions of notations Let G be either A1 3 or A3 , [?]
be the logical disjunction and x be the composition operator on G, the following notations are used (in a general manner, <2G [?]
x> is an algebra of binary relationships).
The lack of knowledge concerning the actual position of some temporal entity x with regard to the temporal entity y is expressed by a sub-set r of G which is interpreted as the disjunction of the relations in r: xry =  [?]
xry  r[?
]r  Thus, x{b m}y signifies that the temporal entity x is anterior to or meets the temporal entity y.
The following conventions are used below: * When a result is valid for both algebras, no distinction is made between the temporal entities concerned.
The base sets (A13, A3, and maybe others), as well as the composition x and reciprocity -1 operators are not distinguished;  *  * *  The letter r represents a sub-set of the corresponding base set of relations (r[?
]G); the letter <<r>> represents a relationship.
r -1 represents the set of relations reciprocal of those contained in r: {r-1; r[?]r}.
r1xr2 represents the distribution of x on [?
]:  x+> and y=<y- y+> is expressed by a quadruple (r1, r2, r3, r4) of relationships between the extremities defined as so:  U  considering that x - < x + and y - < y + , each possible relationship between the bounding instants are expressible with such a quadruple (see table 4).
The symbol = is used such that =x is the expression of an interval as a couple of extremities and =r a relationship between intervals expressed as a quadruple.
= is extended towards sets of relations such that =r is a set of quadruples.
Thus:  r1 x r 2 =  r1 x r2 r1 [?
]r1 ,r 2 [?
]r 2  2.3.
Neighborhood structure Two qualitative relations between two entities are called conceptual neighbors if they can be transformed into one another through continuous deformation of the entities [9].
A conceptual neighborhood is a set of relations whose elements constitute a connected subgraph of the neighborhood graph.
DEFINITION (conceptual neighborhood): A conceptual X neighbor relationship is a binary relation NG on a set G X of relations such that NG (r 1 ,r 2 ) if and only if the continuous transformation of an entity o1 in relationship r 1 with another entity o2 can put them in relation r2 without transition through another relation.
>  =  <  d f  (a) b  m (b)  o  s  e si  di  oi  mi  bi  fi  Figure 2.
Neighborhood graphs for (a) instant-to-instant relations, (b) interval-to-interval relations.
The neighborhood graph is made of relations as nodes and conceptual neighborhood as edges (reciprocal relationships are denoted with an "i" added at the end for the sake of readability).
The graph of figure 2a represents the graph of A conceptual neighborhood N3 between instants (the only continuous deformation is translation).
The graph of A Figure 2b represents the conceptual neighborhood N13 for the deformation corresponding to the move of an extremity of an interval (more generally, the deformation corresponds to moving a limit).
Throughout the paper, the only considered transformation A is the continuous move of a limit (called A-neighborhood in [9]).
The influence of this choice is acknowledged when it matters.
2.4.
Conversion from interval to instant formalisms Relationships between intervals can be expressed in function of the relationships between their bounding instants (see table 4): any relationship between x=<x -  <x- x+ > (r1, r2, r3, r4) <y- y+ >  %0 x - r1 y - [?]
x - r2 y + [?]
x + r3 y - [?]
x + r4 y +  [?
]n [?]
< x - x + > [?]
U ( ri1 , ri2 , ri3 , ri 4 ) [?]
< y - y + > [?
]i =1 [?]
{  [?]
}  n  [?]
x - ri1 y - [?]
x - ri2 y + [?]
x + ri3 y - [?]
x + ri 4 y + i =1  xr y  x -r1y -  x -r2y +  x + r3y -  x + r4y +  b d o s f m e m-1 f-1 s-1 o-1 d-1 b-1  < > < = > < = > < = > < >  < < < < < < < = < < < < >  < > > > > = > > > > > > >  < < < < = < = > = > > > >  Table 4.
The 13 relationships between intervals expressed through relationships between interval extremities.
Since any formula representing relationship between four instants x-, x+, y- and y+ respecting the properties of intervals (x-<x+ and y-<y+ ) can be expressed under that form, the inverse operation = is defined.
It converts such an expression between bounding instants of two intervals into a set of relations expressing the disjunction of relations holding between the intervals.
Of course, both operators (= and =) are inverse.
3 .
Requirements for granularity change operators We aim at defining operators for transforming the representation of a temporal situation from one granularity to another so that the resulting representation should be compatible with what can be observed under  that granularity.
The requirements for building such operators are considered here.
The first section concerns what happens to classical models of time and to temporal entities when they are seen through granularity.
The second one provides a set of properties that any system of granularity conversion operators should enjoy.
These properties are expressed in a sufficiently abstract way for being meaningful for instants and periods, time and space.
3.1.
Granularity change operators Time is usually represented under a particular granularity.
Thus, the time representation system presented so far is an adequate representation for time at any granularity (as far as only qualitative properties are considered).
For instance, the three situations of figure 1 can be expressed in the same formalism with objects and qualitative relations between them.
If we only consider the position of the objects along the horizontal line, the three elements (T, B and H) are related to each other in the way of figure 1c by T{m}B (the truck meets its bumper) and B{b}H (the bumper is before the house).
We aim at elucidating the relationship between two representations of the same reality under two different granularities.
As a matter of fact, the situations of figure 1 cannot be merged into one consistent situation: figures 1b and c together are inconsistent since, in (b), B{m}H and, in (c), B{b}H which, when put together, gives B{}H. First, the reasons for these problems are examined before providing a set of properties that granularity change operators must satisfy.
Time is usually interpreted as a straight line, instants as points and intervals as segments.
Under a numerical light, granularity can be defined as scaling plus filtering what is relevant and what is not (discretizing).
However, granularity is a special filter since, as the name indicates, it filters on size.
For the time concern, the granularity of a system can be defined as the duration of the smallest relevant event (relevance being defined independently beforehand).
But what happens to non relevant events?
There are two solutions: * they can vanish; * they can remain with size 0, i.e.
as instants.
In both cases, these solutions share additional consequences (for symbolic representations): if, under a coarse granularity, one observes that some event is connected to another this can be wrong under a finer granularity since a non relevant laps of time could be relevant here.
In another way, when communicating the same observation, it must be taken into account that the short laps of time may be non relevant (and thus that the relationship between the event can be disconnected).
This  is what happened for the relationship between B and H, which is {b}, in Figure 1c, and becomes {m}, in 1b.
In order to account for this situation, which appears to be regular, we need a downward (resp.
upward) operator which, from a relationship observed a some particular granularity, is able to provide a set of relationships at a finer (resp.
coarser) granularity which represents what can be perceived under that last granularity.
The purpose here, is not to design granularity conversion operators which can make events vanish or turn into instants (see [5]) but rather operators which can account for the possibility of having, under a finer granularity, new space between two entities, and, vice-versa, that a space can become non relevant under a coarser granularity.
These operators are called upward and downward granularity conversion operators and noted by the infix g | g' and g'| g operators (where g and g' are granularities such that g is finer -- more precise -- than g', i.e.
that the size of relevant events is smallest in g than in g').
The following g- g' operator will be used for any of them when the property holds for both (then there is no constraint upon g and g').
As usual, the notation g-g' introduced for the conversion of a single relationship is extended towards sets: g-g' r =  U g- g'  r.
r[?
]r  3.2.
Properties for granularity change operators Anyone can think about a particular set of such operators by imagining the effects of coarseness.
But here are provided a set of properties which should be satisfied by any system of granularity conversion operators.
In fact, the set of properties is very small.
But next section shows that they are sufficient for constraining the possibility for such operators to only one (plus the expected operators corresponding to identity and conversion to everything).
Self-conservation Self-conservation states that whatever be the conversion, a relationship must belong to its own conversion.
It is quite a sensible and minimal property: the knowledge about the relationship can be less precise but it must have a chance to be correct.
(1) r [?]
g-g'r (self-conservation) Neighborhood compatibility A property considered earlier is the order preservation property [10] which states (a part of this):  x > y = !
(g - g' x < g - g' y) (order preservation) However, this property has the shortcoming of being vectorial rather than purely topological.
Its topological generalization, is reciprocal avoidance: x r y = !
(g - g' x r-1 g - g' y) (reciprocal avoidance) Reciprocal avoidance, was over-generalized and caused problems with auto-reciprocal relationship (i.e.
such that r=r -1 ).
The neighborhood compatibility, while not expressed in [5] has been taken into account informally: it constrains the conversion of a relation to form a conceptual neighborhood (and hence the conversion of a conceptual neighborhood to form a conceptual neighborhood).
(2) [?
]r, [?]r',r"[?
]g-g'r , [?]r1,...rn[?
]g-g'r such that X  r1=r', rn=r" and [?]i[?
][1,n-1] NG (ri,ri+1) (neighborhood compatibility) This property has already been reported by Christian Freksa [9] who considers that a set of relationships must be a conceptual neighborhood for pretending being a coarse representation of the actual relationship.
(2) is weaker than the two former proposals because it does not forbid the opposite to be part of the conversion, but, in such a case, it constrains whatever be in between the opposite to be in the conversion too.
Neighborhood compatibility seems to be the right property, partly because, instead of the former ones, it does not forbid a very coarse grain under which any relationship is converted in the whole set of relations.
It also seems natural because granularity can hardly be imagined as discontinuous (at least in continuous spaces).
Conversion-reciprocity distributivity An obvious property for such an operator is symmetry.
It is clear that the relationships between two temporal occurrences are symmetric and thus granularity conversion must respect this.
(3) (g-g' r-1) = (g-g' r)-1 (distributivity of g-g' on -1) Inverse compatibility Inverse compatibility states that the conversion operators are consistent with each other, i.e.
that, if the relationship between two occurrences can be seen as another relationship under some granularity, then the inverse operation from the latter to the former can be achieved through the inverse operator.
(4)  r[?]
I g ' | g r ' and r [?]
I g ' | g r '  r '[?
]g | g' r  r '[?
]g | g ' r  (inverse compatibility)  For instance, if someone in situation (b) of figure 1 is able to imagine that, under a finer granularity (say situation c), there is some space between the bumper and the house, then (s)he must be whiling to accept that if (s)he were in situation (c), (s)he could imagine that there is no space between them under a coarser granularity (as in situation b).
Cumulated transitivity A property which is usually considered first is the full transitivity: g- g'*g'- g" r = g- g" r This property is too strong; it would for instance imply that: g- g'*g'- g r = r Of course, it cannot be achieved because this would mean that there is no loss of information through granularity change: this is obviously false.
If it were true anyway, there would be no need for granularity operators: everything would be the same under each granularity.
We can expect to have the cumulated transitivity: g' g" = g" g" g' g" g| *g'| r g| r and |g'* |g r = |g r  However, in a purely qualitative calculus, the amounts of granularity (g) are not relevant and this property becomes a property of idempotency of operators: (5)  |*|=| and |*| = |  (idempotency)  At first sight, it could be clever to have non idempotent operators which are less and less precise with granularity change.
However, if this applies very well to quantitative data, it does not apply for qualitative: the qualitative conversion applies equally for a big granularity conversion and for a small one which is ten times less.
If there were no idempotency, converting a relationship directly would give a different result than doing it through ten successive conversions.
Representation independence Representation independence states that the conversion must not be dependent upon the representation of the temporal entity (as an interval or as a set of bounding instants).
Again, this property must be required: (6)g-g' r = = g-g' =r and g-g' r= = g-g' = r (representation independence) Note that since = requires that the relationships between bounding instants allows the result to be an interval, there could be some restrictions on the results (however, these restrictions correspond exactly to the vanishing of an interval that which is out of scope here).
4 .
The granular system for time relations Once these six properties have been defined one can start generating candidate upward and downward conversion operators.
However, these requirements are so precise that they leave no place for choice.
We are showing below by starting with the instant algebra that there is only one possible couple of operators.
Afterwards, this easily transfers to interval algebra.
4.1.
Conversion operators for the instant algebra The 64(=2 3 .2 3 ) a priori possible operators for converting < and = can be easily reduced to six: the constraint (1) restricts the conversion of < to be {<}, {<=}, {<>} or {<=>} and that of {=} to be in {=}, {<=}, {=>} or {<=>}.
The constraint (2) suppresses the possibility for < to become {<>}.
The constraint (3) has been used in a peculiar but correct way for eliminating the {<=} (resp.
{=>}) solutions for =.
As a matter of fact, this would cause the conversion of =-1 to be {=>} (resp.
{<=}), but =-1 is = and thus its conversion should be that of =.
<\= {<} {<=} {<=>}  {=} Id b d  {<=>} a g no info  Table 5.
The six possible conversion operators for = and <.
There are still six possible conversion operators left (Id, a, b, g, d and NI).
Since the above table does not consider whether the operators are for downward or upward conversion, this leaves, a priori, 36 upwarddownward couples.
But the use of property (4) -- the putative operators must be compatible with their inverse operator (and vice-versa) -- reduces them to 5: Id-Id, ab, g-g, d-d and NI-NI.
The solution Id-Id cannot be considered as granularity since it does not provide any change in the representation.
The solution NI-NI is such that it is useless.
The d-d pair has the major flaw of not being idempotent (i.e.
d *d [?]
d ): as a matter of fact, the composition of d with itself is NI, this is not a good qualitative granularity converter (this violates property 5).
There are two candidates left: the g-g has no general flaw, it seems just odd to have an auto-inverse operator (i.e.
which is its own inverse) since we all know the asymmetry between upward and downward conversion: it could be a candidate for upward conversion (it preserves the equality of equals and weakens the assertions of difference) but it does not fit intuition as a downward  conversion operator (for the same reasons).
Moreover, g does not respect vectorial properties such as orderpreservation (g is just b plus the non distinction between < and >).
Thus the a -b pair is chosen as downward/upward operators.
The main argument in favor of a-b is that they fit intuition very well.
For instance, if the example of figure 1 is modeled through bounding instants (x - for the beginning and x + for the end) of intervals T+ , B-, B+ and H-, it is represented in (c) by T+=B- (the truck ends where the bumper begins), B-<B+ (the beginning of the bumper is before its end), B+<H(the end of the bumper is before the beginning of the house) in (b) by B+ =H - (the bumper ends where the house begins) and in (a) by B-=B+ (the bumper does not exist anymore).
This is possible by converting with the couple a-b which allows to convert B+<H- into B+=H(= [?]
b<) and B-=B+ into B-<B+ (< [?]
a=), but not with the use of g as a downward operator.
Thus the following result is established: PROPOSITION: The table 6 defines the only possible non auto-inverse upward/downward operators for A3.
relation: r < = >  g|  g'r  <= = >=  g|  g'r  < <=> >  Table 6.
Upward and downward granularity conversions between instants.
The operators of table 6 also satisfy the properties of granularity operators.
PROPOSITION: The upward/downward operators for A3 of table 6 satisfy the properties (1) through (5).
4.2.
Conversion operators for the interval algebra By constraint (6) the only possible operators for A13 are now given.
They enjoy the same properties as the operators for A3.
PROPOSITION: The upward/downward operators for A13 of table 7 are the only one to satisfy the property (6) with regard to the operators of A3 of table 6.
PROPOSITION: The upward/downward operators for A13 of table 7 satisfy the properties (1) through (5).
The reader is invited to check on the example of figure 1, that what has been said about instant operators is still valid.
The upward operator does not satisfy the condition (2) for B-neighborhood (violated by d, s and f) and C-neighborhood (o, s and f).
This result holds since the corresponding neighborhoods are not based upon  independent limit translations while this independence has been used for translating the results from A3 to A13.
relation: r b d o s f m e m-1 f-1 s-1 o-1 d-1 b-1  g|  g'r  bm dfse o f-1 s m e se fe m e m-1 f-1 e s-1 e -1 -1 o s f e m-1 d-1 s-1 f-1 e b-1 m-1  g|  g'r  b d o osd d f o-1 bmo o f-1 d-1 s e s-1 d f o-1 o-1 m-1 b-1 d-1 f-1 o d-1 s-1 o-1 o-1 d-1 b-1  Table 7.
Upward and downward conversion operators between intervals.
5.
Granularity and inference The composition of symbolic relationship is a favored inference mean for symbolic representation systems.
One of the properties which would be interesting to obtain is the independence of the results of the inferences from the granularity level (property 7).
The distributivity of g - g ' on x denotes the independence of the inferences from the granularity under which they are worked out.
(7) g- g' (r1 x r2) = (g- g' r1) x (g- g' r2) (distributivity of g-g' over x) This property is only satisfied for upward conversion in A3.
P ROPOSITION : The upward operator for A3 satisfies property (7).
It does not hold true for A13 : let consider three intervals x , y and z such that x b y and y d z , the application of composition of relations gives x{b o m d s}z which, once upward converted, gives x{b m e d f s o f-1}z.
By opposition, if the conversion is first applied, it returns x{b m}y and y{d f s e}z which, once composed, gives x{b o m d s}z.
The interpretation of this result is the following: by first converting, the information that there exists an interval y forbidding x to finish z is lost: if, however, the relationships linking y to x and z are kept, then the propagation will take this into account and recover the lost precision: {b m e d f s o f-1}[?
]{b o m d s}={b o m d s}.
However, this cannot be enforced since, if the length of y is so small that the conversion makes it vanishing, the correct information at that  granularity is the one provided by applying first the composition: x can meet z under such a granularity.
However, if (7) cannot be achieved for upward conversion in A13, we proved that upward conversion is super-distributive over composition.
PROPOSITION: The upward operator for A13 satisfies the following property: (8)  (g|g' r1) x (g|g' r2) [?]
g|g' (r1 x r2) (super-distributivity of g|g' over x)  A similar phenomenon appears with the downward conversion operators (it appears both for instants and intervals).
So let consider three instants x, y and z such that x>y and y=z, on one hand, the composition of relations gives x>z, which is converted to x>z under the finer granularity.
On the other hand, the conversion gives x>y and y{<=>}z because, under a more precise granularity y could be close but not really equal to z.
The composition then provides no more information about the relationship between x and z (x{<=>}z).
This is the reverse situation as before: it takes into account the fact that the indicernability of two instants cannot be ensured under a finer grain.
Of course, if everything is converted first, then the result is as precise as possible: downward conversion is sub-distributive over composition.
PROPOSITION: The downward operators for A13 and A3 satisfy the following property: (9)  g|  g g g' (r1 x r2) [?]
( |g' r1) x ( |g' r2) (sub-distributivity of g|g' over x)  These two latter properties can be useful for propagating constraints in order to get out of them the maximum of information quickly.
For instance, in the case of upward conversion, if no interval vanishes, every relationship must be first converted and then composed.
6 .
Further and ongoing works Category theory which is widely used in programming language semantics has been introduced in knowledge representation [1] in order to account for the relation of approximation between, on the one hand, a knowledge base and the modeled domain, and on the other hand, the many achievements of a knowledge base.
Ongoing works tackle the problem of such a categorical semantics for time representation.
It meets the intuition: granular representation is approximation.
This will provide the advantage of allowing the integration of a specialized time representation into a wider context (e.g.
for adding temporal extension to objects represented as  Ps-terms).
Category theory allows to do so in a general way.
works have been done for extending qualitative granularity from time to space and are reported in [6, 7].
7 .
Related works  Acknowledgments  Jerry Hobbs introduced the concept of granularity from the non discernability of particular terms with regard to a given set of predicates (these terms can be substituted in the range of any of the given predicates without changing their validity).
The main difference here is that the granularity is given a priori in the structure of time and the scaling notion while Hobbs defines a granularity with regard to relevant predicates.
To our knowledge there is no other proposal for integrating granularity into qualitative time representation.
There has been tremendous work on granularity in metric spaces.
One of the more elaborate model is that of [11, 4].
It proposes a quantitative temporal granularity based on a hierarchy of granularities strictly constrained (to be convertible, divisible...) which offers upward and downward conversion operators for instants and intervals (instead of their relationships).
[5] offers a more general (i.e.
less constrained) framework for quantitative relationships and thus achieves weaker properties.
Hence, the properties obtained here for qualitative representation are compatible with the quantitative representation of [11, 4].
Others works [3] considered granularity in a hybrid qualitative/quantitative system.
The same effect as presented here could certainly been achieved through the computation of qualitative relationships from quantitative ones (using [11, 4] but not in a pure qualitative fashion.
[8] presents a systems which shares a great deal with ours: they treat granularity changes between several representations expressed in the same classical temporal logic (just like here, we used the classical A3 and A13) and they map these representations to natural numbers instead of real numbers [5].
However, temporal logics and algebra of relations are not immediately comparable so the results are quite different in nature.
It is expected that the categorical framework sketched in SS6 allows to compare the two approaches in depth.
Many thanks to Hany Tolba who carefully commented a longer version of this paper and to one of the reviewers who pointed out interesting connections.
8 .
Conclusion In order to understand the relationships between several granularities, a set of requirements have been established for conversion operators.
The only possible operators filling these requirements have been defined.
Moreover other properties of the operators have been established (preservation of the relationship between points and interval).
These operators can be used for combining information coming from different sources and overcoming their contradictory appearance.
Further  References 1.
2.
3.
4.
5.
6.
Hassan Ait-Kaci, Andreas Podelski, Towards a meaning of LIFE, Journal of logic programming16(3-4):195-234, 1993 James Allen, Maintaining knowledge about temporal intervals, Communication of the ACM 26(11):832-843 (rep. in Ronald Brachman, Hector Levesque (eds.
), Readings in knowledge representation, Morgan Kaufmann, Los Altos (CA US), pp509-521, 1985), 1983 Silvana Badaloni, Marina Berati, Dealing with time granularity in a temporal planning system, Lecture notes in computer science (lecture notes in artificial intelligence) 827:101-116, 1994 Emanuele Ciapessoni, Edoardo Corsetti, Angelo Montanari, P. San Pietro, Embedding time granularity in a logical specification language for sychronous real-time systems, Science of computer programming 20(1):141-171, 1993 Jerome Euzenat, Representation granulaire du temps, Revue d'intelligence artificielle 7(3):329361, 1993 Jerome Euzenat, Granularite dans les representations spatio-temporelles, Research report 2242, INRIA, Grenoble (FR), 1994 available by ftp from ftp.imag.fr as /pub/SHERPA/rapports/rrinria-2242-?.ps.gz  7.
Jerome Euzenat, An algebraic approach to granularity in qualitative time and space representations, Proc.
14th IJCAI, Montreal (CA), 1995 to appear 8.
Jose Luis Fiadeiro, Tom Maibaum, Sometimes "tomorrow" is "sometime": action refinement in a temporal logic of objects, Lecture notes in computer science (lecture notes in artificial intelligence) 827:48-66, 1994 9.
Christian Freksa, Temporal reasoning based on semi-intervals, Artificial intelligence 54(1):199227, 1992 10.
Jerry Hobbs, Granularity, Proc.
9th IJCAI, Los Angeles (CA US), pp432-435, 1985 11.
Angelo Montanari, Enrico Maim, Emanuele Ciapessoni, Elena Ratto, Dealing with time and granularity in the event calculus, Proc.
4th FGCS, Tokyo (JP), pp702-712, 1992
Decision Time in Temporal Databases Mario A. Nascimento and Margaret H. Eich Department of Computer Science and Engineering Southern Methodist University Dallas, Texas, 75275-0122, USA fmario, eichg@seas.smu.edu  Abstract  Most of the research found in the Temporal Database literature assumes the use of valid time and transaction time as temporal attributes supported by the DBMS.
Some also assume a user-defined time as a temporal attribute which is not supported by the DBMS.
We discuss the reasons for the need of such support, in the context of the relational data model.
We argue that decision time needs to be supported by the DBMS and we elaborate on a data structure that encompasses both decision time and transaction time.
Examples are provided to illustrate our arguments.
Finally, based on the three temporal dimensions a new taxonomy for database operations is sketched.
1 Introduction  The need of temporal support in databases has been well discussed 3, 6, 5], and most of the research has assumed the use of valid time and transaction time as those which need to be supported by the database management system (DBMS.)
However, we believe that additional support from the DBMS is needed, namely for decision time.
The purpose of this paper is to discuss what is meant by DBMS support regarding temporal data and why, from the perspective of a relational DBMS, such support is necessary for better modelling of real world situations.
In addition we discuss why valid time and transaction time are necessary but not sufficient.
Hence we introduce the notion of decision time.
To support our arguments we use as an example an evolving relation, where to preserve the history associated with one (or more) tuple(s) traditional attributes is (are) not sucient.
We shall see that the need for temporal attributes arise naturally.
We also On leave of absence from EMBRAPA - CNPTIA, Campinas, Brazil, and supported by CNPq (Process 260088/92.7), Brasilia, Brazil.
show how such temporal attributes are treated by the DBMS.
This paper is divided as follows.
In the remainder of this section we present our assumptions and a glossary of terms we will be using throughout the paper.
In Section 2 we discuss when and why DBMS support is needed for valid and transaction times.
Additionally we elaborate on the concept of a key throughout that section, basing our discussion on examples.
In Section 3 we present one example where DBMS support for decision time is needed, otherwise there is loss of information.
This, along with some ideas for the corresponding data structure, presents our contribution towards time representation and reasoning within databases, more specifically decision time.
Section 4 presents, rather briey, a taxonomy for classification of operations in a database, regarding the three discussed temporal dimensions.
Finally, in Section 5, we present a summary which encompasses conclusions and outlines future directions.
1.1 Framework and Definitions  We assume the user has available a traditional DBMS.
By traditional we mean that at least the following capabilities are present: ( ) The user is able to create a relation by defining its attributes and defining one (or more) of those attributes as a key for that relation.
( ) The user has available a query language (perhaps an SQL-dialect 1, Chapter 7]) which allows him/her to retrieve and update the contents of a relation.
As we will be discussing several time dimensions shortly, we shall make clear what we mean by each of them.
Where possible we make use of a terminology which appears to be widely accepted in the area 2].
It is worthwhile noting that in 2] the notion of decision time is not mentioned at all.
Object: Any data element in the relational DBMS, for instance, a tuple in a given relation.
Event: The action an object suers, being either its creation (and insertion into the database) or an update (deletion included.)
Valid Time: This is the time range when the object is true in the real world, and includes a  b  start and end time, being thus, a time range.
We denote valid time by =  s e ].
V  V V  Transaction Time: This is the time the object is recorded (which may be an update, insertion or deletion) into the database.
The transaction time is punctual, in contrast to valid time.
We denote transaction time by .
T  User-defined Time: A temporal attribute which is not valid time nor the transaction time.
More than one user-defined temporal attribute may be defined in a relation, however we assume, with no loss in our arguments that only one user-defined time exists per relation.
It is treated in the same way as any other non{ temporal attribute in a relation.
User-defined time is denoted by .
U  Decision Time: This is the time that an event was decided to happen.
As transaction time it is punctual and it is denoted by .
D  In what follows we argue that without DBMS support for valid and transaction time temporal data cannot be properly handled.
We also show that there are situations where the non{existence of another temporal attribute, namely decision time, may cause loss of information.
Therefore we conclude that decision time must be supported in the same way that valid time and transaction time are.
Indeed, the reader may argue that most of the problems we are going to point out may be overcome by adding proper user-defined temporal attributes (which do not require DBMS support.)
However this is not a good option for the following reasons: ( ) the user has to define all temporal attributes beforehand, possibly even before he/she knows what sort of temporal support is needed, or if it is needed at all, otherwise schema modification is required ( ) the user is responsible for maintaining the relation keys.
As we shall see those temporal attributes are to be part of the key, which then makes ( ) even more dicult to handle.
Therefore we believe that DBMS support for temporal data is very much desired and our discussion is grounded upon such a belief.
As a framework for our discussion we use the Job relation to be introduced below and evolve it, i.e.
make a series of operations on it as time evolves.
To illustrate the concepts that follow, consider the following scenario.
There is a relation Job which has one attribute called Position and another one called Id which happens to be the key for this relation.
In the following tables the name of a key attribute is shown in boldface.
Without loss of generality we look, in the examples given hereafter, at time as a time-line made up of integer values equally spaced.
i  ii  i  2 Support for Valid and Transaction Time  Suppose we have one instance of the Job relation as presented in Table 1 at time = 3.
What in fact we most likely visualize, semantically speaking, from that table is the information in Table 2.
Thus, all one can say, while still relying on Table 1, is that either Mary's and John's tuples have s  3 and e = .
We use to denote the current time, thus it is a dynamic value.
Now suppose that at time 5 one wants to update the attribute Position of Mary's tuple in such a way that it reects the information that from now on Mary is to be a Manager.
In this case a standard DBMS would take Mary's tuple to a state very similar to the one shown in Table 1, namely the only difference would be Mary's Position attribute, which would be Manager, instead of Clerk.
Obviously we lost information, as we are not able to see that Mary had been a Clerk in the past.
Now, let us assume that the DBMS is able to (somehow) perform the following.
Once an event occurs on an object it has a valid time range associated to it.
If such range is not specified we assume it defaults to  ], where is the time of the update.
Also, the DBMS is able to record this information in the database as part of the object's attributes.
Then, the user would be able to access the relation as it is shown in Table 3.
Note that the key did change and we shall discuss this in the following.
As history builds up on a tuple, the original key is not valid any longer.
By looking at Table 3 we can see that for a single Id, Mary, there are two Positions.
This calls for a change in the way we see keys in a temporal relation.
When history of a tuple is kept it is obvious that each key will be associated not only to a single tuple but to all versions of such tuple.
Therefore the new actual key is the union of the previous key(s) with the valid time, specifically only s is needed as part of the key.
However, this composite key may be neglected by the user.
When a valid time is not specified either in a query or update request, the latest version of a tuple is taken into account.
Nevertheless the query language should, somehow, also allow access to all versions associated with a given key.
Thus valid time is needed to preserve the history of a tuple.
In fact this is the definition of a historical database 5].
Should valid time not be supported by the DBMS, a query such as \Has Mary ever been a Manager before" or \What was Mary's position at time 3" would not be answered, instead only queries such as \Is Mary a Manager" (which implicitly are concerned with the current status of the database) would be possibly answered.
We are still not able to have overlapping valid times for a given tuple.
For example, consider if Mary had her position changed from Clerk to Assistantt  V  N OW  V  N OW  t N OW  t  V  Id  Mary John  Position  Clerk Clerk  Table 1: Snapshot of the Job relation at time 3  Id  Mary John  Position  Clerk Clerk  Vs  33-  Ve N OW N OW  Table 2: Implicit information of the Job relation at time 3 Manager at time 2, but it is decided at time 5 to be a retroactive promotion from time 2 until time 4.
Suppose however that, for some reason, such a transaction committed only at time = 7.
The new table is depicted in Table 4.
At the same time range 2, 4] Mary would have two Positions.
Hence the query \What was Mary's position at time 3" would have two answers, when just one is being sought for.
Depending on which snapshot of the database one is looking at there is a dierent answer for such query.
The answer depends on where the query is positioned in the time-line, i.e., if the query is posed with respect to the knowledge the database had from the = 0 up to (but not including) time 7 (when Mary's promotion was committed into the database) then the answer would be Clerk, otherwise the answer would be Assistant Manager.
Therefore the answer depends on whether the update transaction had been committed (recorded in the database) or not.
Hence transaction time is also needed to be supported by the DBMS.
Considering this, assume that in addition to attaching valid time to each object once it is updated, the DBMS is also capable of recording the time when the associated transaction committed.
Thus, the DBMS would be providing support for transaction time as well.
We can then refine the default for the valid time: unless it is defined otherwise, we set and e = , where is the actual transs= action's commit time.
In this case the relation Job (as seen at = 7) would be, semantically, as shown in Table 5.
We have just introduced the notion of a temporal database as discussed in 5], which is a database providing support for both transaction time and valid time.
Notice that the original key plus the valid time may be not enough to dierentiate versions of a tuple.
Thus we need to expand on the key again and make it the original key plus valid time plus transaction time, in order to have a proper key for the extended relation.
As we did previously we assume that if no informaT  t  V  T  V  t  N OW  T  tion is given about the query's position in the timeline, its position is the current time, hence the most recent information is to be used.
3 Support for Decision Time  We have discussed why valid time and transaction time is necessary for proper database modeling of reality.
Recall that we assume that the use of userdefined temporal attributes for valid time and decision time handling is not an option.
If decision time is not supported by the DBMS, modelling the real world cannot be accomplished satisfactorily.
We still use the Job relation to illustrate our arguments.
As we saw in the previous section queries are to be based on valid and transaction time.
However the decision to change Mary's position had been made some time before it was actually committed into the database.
Nevertheless queries would be (potentially) misled by the transaction time as it would act as the actual decision time.
Consider Mary's promotion and the fact that although it was decided at time 5, it was actually committed at time 7.
In this particular case could Mary be considered a Manager at time 6, considering (i) the state of the relation Job as of time 5 (or 6) and (ii) the current time to be  7.
The answer would be negative.
Even though at time  7 the transaction had been committed, the Job relation as of time 5 (or 6) did not have \ocially", so to speak, the necessary knowledge.
Hence the incorrect negative answer.
Furthermore, as there is no support for decision time there is no way to know when the decision was taken.
All we are able to know is that such decision was committed into the database at = 7.
We propose that decision time should be another temporal attribute, i.e., whenever one event happens on one object it must have a decision time associated to it (it defaults to the transaction time if none is assigned.)
It would work very much in the same way as valid time, which can also be assigned or left to a default as discussed before, with the exception that valid time is a range and decision time is punctual as t  t  t  Id  Vs  Position  Mary Clerk Mary Manager John Clerk  0 5 0  Ve  4  N OW N OW  Table 3: Information of the Job relation at t = 5, using valid time  Id  Position  Mary Clerk Mary Assistant Manager Mary Manager John Clerk  Vs 0 2 5 0  Ve  4 4  N OW N OW  Table 4: Two valid times of dierent tuple versions overlap transaction time.
Therefore once a decision time is assigned, the DBMS is responsible for recording such information in the database, if none is assigned then it defaults to transaction time.
This would lead Table 5 to the semantically richer Table 6.
The question is how this is to be accomplished.
Although we have not addressed the issue here thus far it is obvious that both valid and transaction times need to be physically recorded and indexed by the DBMS.
Suppose that such indices are v and t .
Then we can design the DBMS to handle relations with valid, transaction and decision times, and thus maintain v , t and a new structure, d , the index for decision time.
We propose the following idea to avoid most of the overhead due to the new additional index d : to incorporate the decision time inside the transaction time index t .
Using v instead is not a good idea because, in general, it indexes a time range, whereas both t and d indexes time points.
Once a transaction is committed v and t are updated as before, but in addition the decision time is recorded in t as well.
Let us call this combined index td .
The dierence between the former t and the new td has to do only with the leaf nodes.+ Notice that we use the terminology assuming a B {tree index 1, Chapter 5].
If only a single data type is indexed, as in the t for instance, the leaf nodes look like the ones depicted in Figure 1(a).
In the td each leaf node indexing time point k has two types of pointers.
The first type is the set d where each pointer points to a record that was decided upon at time k .
The second type is a linked list of pointers t where each one points to a record committed at time k .
Furthermore the leaves are connected by a doubly{linked list.
This is shown in Figure 1(b).
Note that pointers associated to a decision at time d always point to a event committed at time t , where d  t. Also both types of pointers, at any of the leaves.
d and t may be With this structure, and properly manipulation of I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  t  SP  t  P  t  t  t  SP  t  P  t  N U LL  the new composite index, we are able to know that at a given time, previous to the actual transaction time, decisions may have already been made and thus such knowledge may be retrieved.
Also as we made use of an already existing structure, namely t , we reduced the overhead in space of a new indexing structure.
Therefore, we can now retrieve the fact that at time = 5 Mary was a manager, if such query is posed at  7, and this was done without modifying the Job relation schema at all.
For the sake of fairness we should recognize that there is still one problem that this approach does not solve.
Until the transaction is committed there is still no way to know whether decisions regarding the time frame previous to the commit on the database had been made or not.
It is our opinion that this is a problem which cannot be solved, given that until a fact is recorded it is unknown.
Nevertheless we were able to record a decision retroactively (in relation to the actual transaction time) without incurring additional structural overhead.
Recall that before introducing decision time, as a temporal attribute, we could only assume that decision time was equal to transaction time, and thus even for  7 Mary would not be considered a Manager at time 5 (or 6.)
Finally, let us discuss the role of the key whenever decision time is taken into account.
If we assume that one is not allowed to decide on the very same issue twice then decision time does not need to be part of the actual key.
On the other hand if one is not allowed to decide on the same issue twice, we now have the possibility of using only the original key plus decision time as the actual key for a relation.
I  t t  t  4 An Enhanced Temporal Taxonomy Classification  As we discussed before, most research have taken into account only valid time and transaction time, and thus a classification of operations would be reduced  Id  Vs  Position  Mary Clerk Mary Assistant Manager Mary Manager John Clerk  0 2 5 0  T  Ve  4 4  0 7 5 0  N OW N OW  Table 5: Implicit information of the Job relation with transaction time  Id  Position  Mary Clerk Mary Assistant Manager Mary Manager John Clerk  Vs  0 2 5 0  Ve  D  T  4 4  N OW N OW  0 7 5 0  0 5 5 0  Table 6: Implicit information of the Job relation with transaction time and decision time to those two dimensions.
Now that we are convinced that a third temporal dimension is needed, we are able to present Figure 2, which shows how an event may be classified with respect to its decision time, valid time and transaction time.
For the sake of illustration, and brevity we explain only a couple of possible events, others can be derived with the aid of Figure 2.
Retroactive Late Transaction of a Futuristic Decision - As the transaction is retroactive we have , finally a fue  , being late implies turistic decision means that  s , therefore  s  e  which is feasible.
Immediate Instantaneous Transaction of a Past due Decision - An immediate transaction implies s = , as it is instantaneous we have = and the past due decision yields s .
Thus we have which is not feasible.
V  T  D < T D  D  V  V  D  V  V  T  T  T  V  < D  T < D  5 Summary and Future Directions  We have presented, by means of a relation evolving in time, why valid time and transaction time are needed to better model reality.
We went further and argued why decision time is also needed as well.
We have mentioned briey how to use the transaction time index as an underlying structure to actually implement it.
We assumed that relying on user-defined temporal attributes for these three time dimensions would incur too much overhead to the user and therefore those are desired to be supported by the DBMS itself.
We discussed how the original relation key needs to be expanded as dierent temporal attributes enrich the tuple semantics.
Using the three temporal dimensions simultaneously we could realize a novel taxonomy for temporal events.
Further research is being conducted in investigating a suitable indexing data structure containing both transaction and decision time.
Finally, we plan on  investigating on how to combine the three dierent indices, v , t and d , in order to be able to take advantage of all three temporal dimensions, and also how a query language such as TQuel 4] would need to be extended to allow decision time.
I  I  I  Acknowledgments  The authors thank Ashley C. Peltier and Clenio F. Salviano for comments in previous versions of this paper.
Anonymous reviewers' comments and directions to new references are also acknowledged.
References  1] R. Elmasri and S. B. Navathe.
Fundamentals of Database Systems.
Benjamin/Cummings, Redwood City, CA, 2nd edition, 1994.
2] C. S. Jensen et al.
Proposed temporal database concepts.
In Proceedings of the International Workshop on an Infrastructure for Temporal Databases, pages A1{A24, Arlington, TX, June  3] 4] 5] 6]  1993.
N. Pissinou et al.
Towards an infrastructure for temporal databases - Report of an invitational ARPA/NSF workshop.
Technical Report TR 9401, University of Arizona, March 1994.
R. T. Snodgrass.
The temporal query language TQuel.
Transactions on Database Systems, 12(2):247{298, June 1987.
R. T. Snodgrass and I. Ahn.
Temporal databases.
IEEE Computer, 19(9):35{42, September 1986.
A. Tansel et al., editors.
Temporal Databases: Theory, Design and Implementation.
Benjamin/Cummings, Redwood City, CA, 1993.
Tk  Ti  Tj  ...  Tk  Tk  Ti  ...  Tj  SPd Pt  (a) Part of the bottom of a standard B+-tree  ...  Tk Pt  ...  SPd  (b) Part of the bottom of the proposed indexing tree  Figure 1: Tree shaped indices structures for transaction (and decision) time V = Valid Time  V = Valid Time V=T  V=D  Immediate Proactive  Futuristic Current  Retroactive and Proactive  Futuristic and Past due  Past due  Retroactive  D = Decision Time  T = Transaction Time D = Decision Time D=T  Unfeasible Region Instantaneous  Late Late T = Transaction Time  Figure 2: Classication of an event with respect to the three time dimensions

Just one approach for several temporal logics in Computing: the topological semantics Inma P. de Guzman and Manuel Enciso and Carlos Rossi* Universidad de Malaga.
Pza El Ejido s/n.
29013 Malaga {gimac}@tecma1.ctima.uma.es Abstract In this work we present an unified frame for temporal logic that allows us to consider * discrete time and continuous time, * points and intervals jointly, and * the absolute and relative approaches.
in an adequate way for computing.
This unified frame is built up from a new kind of semantics that we introduce in this work: the topological semantics, which avoids the use of first order logic as the base of the semantics and allows to develop natural connectives that relates to common properties in real systems.
We present a point logic over discrete time (the LN logic [5]), a point logic over continuous time (the IRLN logic [7]), and an interval-point logic over discrete time (the LNint logic [13]).
Key words: Temporal logic and ontologies, continuous and discrete time, point and interval logics, semantics, absolute and relative approaches.
1  Introduction  The introduction of special temporal connectives is considered very useful for several applications in Computer Science.
To give a formal account in these applications of the intended meanings of the connectives, first order logic is used as a model theory.
In our opinion, this approach implies several difficulties: * It obscures the nature of the time that we have in mind (and therefore the intuitive meaning of the connectives).
* It makes difficult to consider approaches that treat points and intervals jointly and to combine absolute and relative temporal information.
*  This work has been partially supported by CICYT project TIC94-0847-C02-02.
* Furthermore, it renders difficult the proof of the validity of the formulas and the search for natural and efficient proof systems for temporal logics (particularly, it is difficult to build automatic theorem provers that reflect the semantics and that allow model generation).
Because of the previously mentioned shortcomings, we introduce a new semantics, which we call topological semantics because it reflects the topology of the flow of time.
Using this common semantic framework, we introduce a point logic over discrete time (the LN logic [5]), a point logic over continuous time (the IRLN logic [7]), and an interval-point logic over discrete time (the LNint logic [13]).
The connectives introduced in these logics consider the fact that every application of the temporal logic in Computer Science is intrinsically concerned with the possibility of testing the first (last) occurrence of an event after (before) the instant in which we are speaking--or executing.
So, they reflect the relations of precedence, posteriority and simultaneity.
The well-behavior of our semantic approach has been shown in the proof of the Separation Theorem for LN [5] and in the design of efficient and parallel automatic theorem provers for several systems in temporal logics [6].
2  The LN Logic  The LN logic is a linear temporal logic over discrete time.
Its well formed formulas are built on the following alphabet * An enumerable set O of atoms.
* The usual classical connectives !, [?
], [?].
* The temporal connectives , .
The well formed formulas (wffs) of LN are inductively defined as follows: * An atomic p [?]
O is a wff.
* Let A and B be wffs, then !A, A [?]
B, A [?]
B, A - B, A  B and A  B are wffs.
A  B is read as sometime in the future A, and the next occurrence of A will be before or simultaneous with the next occurrence of B.
A  B is read as sometime in the past A occurred, and the last occurrence of A was after or simultaneous with the last occurrence of B. Topological semantics of LN We define a topological semantics for LN considering (ZZ, fi) as the flow of time, where ZZ is the set of integer numbers with the smaller than relation.
The key concepts of this approach reflect the well - order of ZZ and are the m+ tA and mtA defined below: Definition 1: Given a temporal formula A and an instant of time t [?]
ZZ, we define m+ tA m- tA  =  min{t [?]
ZZ | t > t and A is true at t }  =  max{t [?]
ZZ | t < t and A is true at t }  We will convey that min [?]
= +[?]
and that max [?]
= -[?
], and we shall consider the smaller than relation extended as usual to ZZ [?]{+[?]}
[?]
{-[?]}.
These concepts represent the first instant after t in which A is true (m+ tA ) and the last instant before t in which A was true (m- tA ).
This is why this logic - is called LN (Last-Next).
m+ tA and mtA are powerful tools to obtain the desired advantages: to give the interpretation for the temporal connectives of LN in such a way that directly reflects their intuitive meaning and to facilitates the use of temporal logic in applications.
Definition 2: We define a temporal interpretation for LN to be a function that associates with each atom p [?]
O, a subset h(p) of ZZ: h: O -- 2ZZ Informally, h(p) is to be thought of as the set of time points t [?]
ZZ at which p is true.
The function h can be extended to any wff of LN as follows: 1. h(!A) = ZZ \h(A) 2.
h(A[?
]B) = h(A)[?
]h(B); h(A[?
]B) = h(A)[?
]h(B)   3. h(A - B) = ZZ \h(A) [?]
h(B) 4. h(A  B) = {t [?]
ZZ | m+ tA < +[?]
and + m+ <= m } tA tB 5. h(A  B) = {t [?]
ZZ | m- tA > -[?]
and - m- >= m } tA tB where:   m+ tA = min (t, +[?])
[?]
h(A)  m- tA = max - [?
], t) [?]
h(A) Definition 3: A formula A in LN is valid in a temporal interpretation h, denoted |=h A, if h(A) = ZZ.
A formula A is called valid, denoted |= A, if |=h A for all interpretations h. The formulas A and B are called equivalent, denoted A [?]
B, if A [?
]h B for any interpretation h.  Theorem 1 The logics LN is expressively complete over linear discrete time [5].
This primitive system allows us to derive other connectives which reflect other precedence situations and simultaneity (we present in the table 1 only the future connectives).
3  The IRLN Logic  The choice of a discrete time or a continuous time temporal logic depends on the particular problem we are dealing with.
Since most of these problems are tractable with temporal logics over discrete time, most of the temporal logics have a discrete flow of time [1], [8].
Nevertheless, in our opinion, the temporal logics over continuous time may not be rejected.
In fact we may cite some authors that confirm this assertion: [4], [17], [9], in these works temporal logics over continuous time are presented, and their utility is discussed.
The extension we propose is a further step in the sense that we look for a natural extension of the LN logic to continuous time (the IRLN Logic); i.e., we want a temporal logic over continuous time that verifies the properties of LN: to have natural connectives for specifications and to avoid the use of first order logic in the semantics.
In particular, we think that IRLN is suitable for the specification of the so called reactive systems.
These are systems that continuously interact with their environment.
For instance almost all concurrent, distributed or embedded systems are reactive.
In the reactive systems, a non-trivial mixture of discrete (program-like) and continuous (environmentlike) components are allowed.
There are some works saying that the temporal logics over continuous time and the reactive systems are good partners.
([14] is a pioneer in this area) Finally, we may say that there are reactive systems that have some asynchronous components, and in this situation, the change of state does not occur with the ticks of a global clock (the non-negative integer IN); so, we will need a dense and continuous set to model the flow of time (the real number set IR).
3.1  Syntax of IRLN  The well formed formulas of IRLN are built on the following alphabet * An enumerable set of atoms, O and the usual classical connectives : !, [?
], [?]
and -.
* A set of symbols of temporal connectives, CT , whose elements will be discussed in the rest of this section.
Once the CT set was introduced, the wffs are defined like those presented for LN.
Connective A[?
]B A [?
]+ B AB AB A =+ B  Description strong strict precedence strong simultaneity weak strict precedence weak wide precedence weak simultaneity  Definition A  B [?]
!
(B  A) AB[?
]B A !
(B  A) !
(B [?]
A) AB[?
]B A  Semantics + m+ tA < mtB + m+ = m < +[?]
tA tB + + m+ < m or m tA tB tB = +[?]
+ m+ fi m tA tB + m+ tA = mtB  Table 1: Derived connectives in LN  Topological semantics of IRLN The topological semantics of IRLN is the natural extension of the topological semantics of LN.
We consider (IR, fi) as the time flow, where IR represents the set of real numbers and fi is the smaller than or equal to relation.
The tools m+ and m- of LN must be fitted to the topology of IR: For any wff A and any t [?]
IR we define:    i+ tA = Inf {t [?]
IR | t  t and A is true at t } -   itA = Sup{t [?]
IR | t fi t and A is true at t } We extend as usual the smaller than or equal to relation to the set IR [?]
{-[?
], +[?]}.
If A is true at i+ tA + and i+ tA = t, then we denote it as mtA (it is a max- imum).
Analogously, if A is true at i- tA and itA = t, - then we denote it as mtA (it is a minimum).
This distinction is not only a theoretical constraint, it is a method that represents all the situations in which a statement might occur in real applications.
In our opinion, this distinction is fundamental to applications of temporal logic because we need to represent naturally all the many different patterns of the occurrences of the statements in time and it allows to specify environment facts and program facts in reactive systems.
Definition 4: We define a temporal interpretation of IRLN to be a function, hIR , that associates each atom, p [?]
O, to a subset hIR (p) of IR: hIR : O -- 2IR The extension of hIR to any wff will be defined when we introduce C. 3.2 Choosing Temporal Connectives 3.2.1 Monary Connectives As we have mentioned above, we need to specify naturally both the internal programs facts and the external environmental facts.
Thus, we need monary temporal connectives which adequately reflect both modes of occurrences of an event, this is, the connectives included in the table 2 (we present only the future connectives).  + where: i+ tA = inf (t, +[?])
[?]
hIR (A) and if itA [?]
+ hIR (A), then we note mtA .
The past connectives are defined in the same way - using the topological past-tools i- tA and mtA .
From  now on, we will only introduce the future-related components of our temporal logic.
3.3  Several Systems of Connectives for IRLN We present here three equivalent systems which are fully expressive.
System 1 The philosophy of this system is to extend directly the primitive system of LN: {, } (see table 3).
The connective  is a very strong connective of non-strict precedence.
This name reflects the fact that when we affirm A  B we force the existence of the first future occurrence of A.
We look for a fully expressive set CT for IRLN.
Therefore, we need a weak monary connective.
So, together with , we include a monary connective [7].
The set is now defined as follows: CT1 = {, , Ac+ , Ac- } Nevertheless, CT1 does not reflect the simple and transparent behavior that it had in LN specifications: 1.
There are some precedence situations in the real world that are not directly covered with this too strong precedence.
(If we are not able to determine a first instant on which a proposition will occur.
Examples are showed in [7]).
2.
The precedence and simultaneity connectives do not match properly.
(For example, the semantics of !
(A  B) does not render any connectives of precedence).
This system is a good step in our development, but it is not the end of the road.
System 2 To avoid these problems, we consider a new binary connective which reflects directly the precedence relation on the real line.
The connective is shown in the table 4.
This connective does not require any monary connective to have full expressive power because in their definition the precedence connectives implicitly include the two ways of occurrences.
So, the set CT2 is now {, }.
We may do the following criticism for this system: the precedence and simultaneity connectives match  Connective Ac+ (A) Al+ (A)  Meaning A in the future A will be true at instants arbitrarily close in the future there will be a first accessible instant at which A will be true  Definition hIR (Ac (A)) = {t [?]
IR | i+ tA = t} +    hIR Al+ (A) = {t [?]
IR | m+ tA exists }  Table 2: Monary connectives of IRLN Connective AB  Meaning in the future there will be a first instant at which A will be true, and it will take place before or simultaneously with any future occurrence of B  Definition + hIR (A  B) = {t [?]
IR | m+ tA fi itB }  Table 3: System 1 of IRLN  perfectly because they reflect the precedence and simultaneity relation over the real numbers.
Nevertheless, it does not seem too adequate for real applications.
System 3 In practice, when we reason over continuous time, it is impossible to determine towards the future, when we will reach the instant at which A will happen or the instant at which occurrences of A will accumulate and this limits our reasoning.
Therefore, to apply temporal logic over continuous time we must identify both situations.
This is our new starting point to seek the appropriate system of connectives.
In opposition to system 1, the lack of expressiveness for this system is due to the strong character.
So, we need the monary connective Al+ and the set CT3 is now {Al+ , Al- ,  -,  -} (see table 5).
In our opinion, we have successfully concluded our search for a system of connectives that covers sufficiently our objective.
We may summarize its advantages below: 1.
CT3 has temporal connectives that naturally correspond to the different interpretations that represent properties of interest in real systems.
2.
The situations reflected by the connectives are translated easily to order relations between instants of IR.
In particular !
(A  - B) = B  A.
3.
IRLN is appropriate for applications that require both program and environmental events to be integrated in the specification.
4.
IRLN permits the successful extension of the automated theorem prove methods used in [2] and [6] for propositional classical logic and temporal logic respectively.
3.3.1 Other derivable connectives All of these systems allow us to define simultaneity and severe precedence connectives as naturally as LN permits.
In fact, we can present two definition schemata to introduce this kind of connectives.
Each one of these new connectives has the character of the connective used to define it.
The schemata are: A=B  def  =  AB[?
]BA  A<B  def  A  B [?]
!
(A = B)  =  Where = represent simultaneity, < represent strict precedence and  [?]
{, ,  -},  [?]
{,  -}.
(The strict precedence connective corresponding to  is only a bit more complex because it must consider the Ac+ connective in its definition).
4  A point-interval logic  In this section we present a logic with which we pretend to combine different approaches found in the literature and that we consider artificially opposed.
Firstly, we may refer the classical discussion between points and intervals.
Until now, temporal logic of points has been the most commonly used in computer applications and there are many results and different implementations.
Nevertheless, recently we have seen an increasing tendency to work with interval-based temporal logics because they appear to be more adequate for reasoning about change, for temporal reasoning in general, for scheduling and planning or for natural language processing.
A point-based temporal logic has several advantages, perhaps the most important is that it inherits the well-demonstrated good computational behavior of point algebra ([16]).
On the other hand, when we begin the development of a temporal logic, we have to decide if we will treat time in an absolute or relative way.
The  Connective AB  Meaning if in the future B will occur, then there will be an occurrence of A before or simultaneously with any occurrence of B  Definition hIR (A  B) = {t [?]
IR | i+ tB = +[?]}[?]
+ + + {t [?]
IR | m+ tA fi itB } [?]
{t [?]
IR | itA < itB }  Table 4: System 2 of IRLN Connective A -B  Meaning if in the future B will occur, then there will be occurrences of A before any occurrence of B, or it will be indeterminate if A precedes B or B precedes A  Definition + + hIR (A  - B) = {t [?]
IR | itA fi itB }  Table 5: System 3 of IRLN  absolute approach involves treating temporal assertions according to the date in which they occur as, for example, in the logics proposed by McDermott [11] and Allen [3].
This approach is typically followed by reified logics and logics with temporal arguments.
On the other hand, when we consider relativity, time is not absolutely defined, but the passage of time is marked by occurrences.
Most modal logics, like the USF logic [8], Tempura [12], or that proposed by Halpern and Shoham [10] adopt this relative approach.
However, we conclude that we really did not have to choose between one kind of temporal logic or another because, in practice, each or both logical forms can satisfy naturally the needs of the application we have in mind.
Our approach is to have a logic that treats points and intervals jointly, which allows both a relative and an absolute treatment of time.
4.1  The Logic LNint  LNint is a modal temporal logic for handling points and intervals.
At the syntactic level, we begin considering two components, one to collect assertions about: points, points that belong to intervals and dates; and other collecting events.
We use atomic events in a similar sense to that used by Allen, i.e., expressions about intervals which are true neither at the subintervals nor, more specifically, at the points of the interval over which the expression is affirmed.
These initial components, although separated at first, will be extended later to collect mixed concepts, to reach finally a perfect semantic cohabitation.
This cohabitation takes shape in the following idea: we talk about points or intervals in LNint, but we are always in an (evaluation) point, the current instant or present.
Later, we abstract this instant.
In this way, we can avoid the ambiguous concept of the current interval that the interval modal logics impose.
The concept of current interval is not very intuitive and less com-  putationally manageable and appropriate.
LNint permits a good absolute treatment of time and to handle points and intervals (when needed) like temporal logics with temporal arguments, or like reified logics.
In this way, we obtain a mixture of the absolute and relative approaches to the treatment of time.
The time flow in LNint is (T, +, <=) isomorphic to (ZZ, <=, +).
4.1.1 Syntax of LNint The construction of the language of LNint starts considering three sets of atoms: * The set Op = {p, q, .
.
.
, p1 , q1 , .
.
.
, pn , qn , .
.
.}
of point atoms that formalize statements whose executions take place in an instant (hereditary formulas of Shoham [15]).
* The set T = {t | t [?]
T } of date atoms, used to name instants.
* The set Oint = {a, b, .
.
.
, a1 , b1 , .
.
.
, an , bn , .
.
.}
of atomic events.
Afterwards we define three sublanguages, each one gathering different expressions: I) The "events language" Lint , in which we treat event expressions.
Lint is the inductive closure of Oint under the boolean connectives, the monary temporal confi  nectives ab+ , ab- , beg, beg,end, end, and the absolute connective [m, n].
These six unary connectives are sufficient to express any temporal relation between two intervals [15].
The absolute connective allows us to name dated intervals.
II) The "points language" Lp , in which we treat point expressions.
Lp is based on the language of the LN logic, extended to talk about dates.
So we add to the alphabet of LN the elements of T .
In Lp we can define an absolute connective that links a point formula and a date: A at m = (A [?]
m) [?]
A atnext m [?]
A atlast m This connective tells us if a formula A is true at a given instant m; it is independent of the instant in which we are located, i.e., we deliberately ignore the current instant.
fip .
To avoid III) The "points and events language" L the intrinsic use of the interval language, we need to extend Lp to treat the events by using points.
Of course, although it is true that events in themselves have no sense in the points of the interval over which we affirm them, it is also true that every event must have a start instant and an end instant, and must take place at every point between them.
So we can now characterize events by means of these points.
They are formalized in the extension of Lp which we fip in which we define for each atomic denote as L - event a [?]
Oint , | a, | a and - a to denote the starting instant, the ending instant and a course instant, respectively.
Then we define OIp as follows: - a | a [?]
Oint } OIp = Op [?]
T [?]
{, [?]}
[?]
{|a, |a, - fip as the inductive closure of OI unWe define L p der the boolean connectives,  and .
fip ) of the Finally we extend the definition (in L start instant, the end instant and the course to the well-formed formulas of Lint .
This facilitates the effective manipulation of any event based on its start and end instants.
The language L of LNint.
To establish the final language, L, of LNint, we fip as define a translation function Tr from Lint to L follows: fip Tr : Lint - L - - where Tr(A) =|A [?]
A [?]
|A.
With this function we have available "point versions" of the formulas of Lint , and then we can treat events adequately in terms of points.
fip the Now, we define L adding to the language L following defined formulas: A =def Tr(A) for all formula A [?]
Lint We can define in L the desired connective to achieve an absolute temporal treatment of events: if A is a well-formed formula in the sub-language Lint , we define A atI [m, n] = (|A [?]
|A [?
]+ n) at m A atI [m, n] is read as the event A occurs exactly in the interval [m, n].
4.1.2 Semantics of LNint Since all the expressions of L are really point expressions, the semantics is basically the topological semantics of the logic LN.
The only difference is that the temporal interpretations must satisfy some natural requirements to give coherent truth values to the dates and to the start, end and course points of atomic events.
For example, two of the requirements are: h(t) = {t}, for all t [?]
T - - h(| a)[?
]h(| a) = h(| a)[?
]h(- a ) = h(| a)[?
]h(- a)=[?]
References [1] M. Abadi and Z.
Manna.
Temporal logic programming.
In Symposium of Logic Programming.
IEEE, 1987.
[2] G. Aguilera, I. P. de Guzman, and M. Ojeda.
TAS-D++: Syntactic tree transformations for automated theorem proving.
In European Conference on Logic and Artificial Intelligence JELIA'94, volume 838.
Lecture Notes in Artificial Intelligence, 1994.
[3] J. F. Allen.
Maintaining knowledge about temporal intervals.
Communications of the ACM, pages 832-843, 1983.
[4] H. Barringer, A. Pnuelli, and R. Kuiper.
A really abstract concurrent and its temporal logic.
In POPL.
ACM, 1986.
[5] A. Burrieza and I. P. de Guzman.
A new algebraic semantic approach and some adequate connectives for computation with temporal logic over discrete time.
Journal of Applied non Classical Logic, 2, 1992.
[6] M. Enciso.
Logica temporal y demostracion automatica de teoremas.
Eficiencia y paralelismo.
PhD thesis, Universidad de Malaga.
Espana, 1995.
[7] M. Enciso and I. P. de Guzman.
Topological semantics for specification and verification: the RLN logic.
Submitted to Journal of Applied Non-classical Logics, 1994.
[8] D. M. Gabbay.
The declarative past and imperative future, volume 398 of Lecture Notes in Computer Science.
Springer Verlag, 1989.
[9] D. M. Gabbay.
Temporal Logic: Mathematical Foundations.
Oxford University Press, 1992.
[10] J. Halpern and Y. Shoham.
A propositional modal logic of time intervals.
In First IEEE Symposium on Logic in Computer Science.
Computer Society Press, 1986.
[11] D. V. McDermott.
A temporal logic for reasoning about processes and plans.
Cognitive Science, 6:101-155, 1982.
[12] B. C. Moszkowski.
Executing Temporal Logic Programs.
Cambridge University Press, 1986.
[13] I. P. de Guzman and C. Rossi.
LNint: a temporal logic that combines points and intervals and the absolute and relative approaches.
To appear in Bulletin of the IGPL, 1995.
[14] A. Pnueli.
The temporal logic of programs.
In 18th Symposium on the Foundations of Computer Science.
IEEE, 1977.
[15] Y. Shoham.
Reasoning about Change.
Time and Causation from the Standpoint of Artificial Intelligence.
MIT Press, 1988.
[16] M. Vilain, H. Kautz, and P. van Beek.
Constraint propagation algorithms for temporal reasoning: A revised report.
In Readings on Qualitative Reasoning about Physical Systems, pages 373-381.
Morgan Kaufmann, 1990.
[17] P. Wolper.
ESPRIT Basic Research Action REACT(6021): Building correct reactive systems.
EATCS Bulletin, 50, 1993.
A language to express time intervals and repetition Diana Cukierman and James Delgrande School of Computing Science Simon Fraser University Burnaby, BC, Canada V5A 1S6 fdiana,jimg@cs.sfu.ca  Abstract  We are investigating a formal representation of time units, calendars, and time unit instances as restricted temporal entities for reasoning about repeated activities.
We examine characteristics of time units, and provide a categorization of the hierarchical relations among them.
Hence we dene an abstract hierarchical unit structure (a calendar structure) that expresses specic relations and properties among the units that compose it.
Specic time objects in the time line are represented based on this formalism, including non-convex intervals corresponding to repeated activities.
A goal of this research is to be able to represent and reason eciently about repeated activities.
1 Introduction  The motivation for this work is to ultimately be able to reason about schedulable, repeated activities, specied using calendars.
Examples of such activities include going to a specic class every Tuesday and Thursday during a semester, attending a seminar every rst day of a month, and going to tness classes every other day.
Dening a precise representation and developing or adapting known ecient algorithms to this domain would provide a valuable framework for scheduling systems, nancial systems, process control systems and in general date-based systems.
We search for a more general, and formalized representation of the temporal entities than in previous work.
We explore further the date concept, building a structure that formalizes dates in calendars.
Schedulable activities are based on conventional systems called calendars.
We use as a departure point \calendar" in the usual sense of the word.
Examples of calendars include the traditional Gregorian calendar, university calendars, and business calendars, the last two groups being dened in terms of the Gregorian.
The framework we dene concerns a generic calendar abstract structure, which subsumes the mentioned calendars, and arguably any system of measures based on discrete units.
Calendars can be considered as repetitive, cyclic temporal objects.
We dene an abstract structure that formalizes calendars as being composed of time units, which are related by a  decomposition relation.
The decomposition relation is  a containment relation involving repetition and other specic characteristics.
Time units decompose into contiguous sequences of other time units in various ways.
A calendar structure is a hierarchical structure based on the decomposition of time units.
This structure expresses relationships that hold between time units in several calendars.
We refer to the concept of time unit instances, and distinguish dierent levels of instantiation.
Whereas \month" refers to a time unit class, \June" is referred to as a named time unit.
June is one of the 12 occurrences of the notion of month with respect to year, and viewed extensionally it represents the set of all possible occurrences of June.
Finally, \June 1994" is one specic instance of a month.
2 Related work  Our formalism deals with time units, which are a special kind of time interval with inherent durations.
Therefore we base our work on time intervals as the basic temporal objects 1].
In 21], the time point algebra is developed, based on the notion of time point in place of interval.
Computation of the closure of pairwise time-interval relations in the full interval algebra is NP-complete, whereas the time point algebra has a polynomial closure algorithm.
However the time point algebra is less expressive than the interval algebra: certain disjunctive combinations of relations are not expressible with the time point algebra.
Nonetheless, some applications do not require the full expressive power of the interval algebra, and can benet from ecient (albeit less expressive) representations.
Hence it is of interest to study restrictions of the interval algebra.
The present framework deals with restricted kinds of intervals within a hierarchical structure.
The intent is that the hierarchy provide a basis for obtaining ecient algorithms for certain operations.
(This may be contrasted with 11] which considers a hierarchical structure in the general interval algebra.)
Nonconvex intervals (intervals with \gaps") are employed in 12, 13] when using time units in a repetitive way or when referring to recurring periods.
The time unit hierarchy proposed in our work generalizes 13], in that temporal objects can be represented by any se-  quence of composed time units, as opposed to xed in Ladkin's approach.
Moreover, we are able to combine systems of measurement, and so talk about the third month of a company's business year as corresponding with June in the Gregorian calendar.
13] also does not take into account the varying duration of specic time instances, a matter addressed and formalized in our work.
In addition we address the varying duration of specic time instances.
18] also elaborate on the notions of non-convex interval relations dened in 12, 13] while 16] proposes a generalization of nonconvex intervals.
Leban et.
al.
15] deals with repetition and time units.
This work relies on sequences of consecutive intervals combined into \collections".
The collection representation makes use of \primitive collections" (essentially circular lists of integers), and two basic operators, slicing and dicing, which subdivide an interval and select a subinterval respectively.
Poessio and Brachman 19] are mainly concerned with the implementation of algorithms to detect overlapping repeated activities.
This work relies on temporal constraint satisfaction results and algorithms 8].
19] also introduces the concept of using dates as reference intervals to make constraint propagation further ecient.
We envision our proposed formalism provides a useful framework to follow this idea.
5] exposes a set theoretic structure for the time domain with a calendar perspective.
It formalizes the temporal domain with sets of \constructed intervallic partitions" which have a certain parallel to our decomposition into contiguous sequences of intervals.
However, this formalization is more restricted than ours and can not handle the Gregorian nor general calendars.
3] and 4] propose formalizations that deal with calendars and time units.
These two papers are interestingly related to our research, even though they have evolved independently.
These works are analyzed and compared with our research in the Section 5.
3 Time units and time unit instances  The central element of our formalism is that of a time unit.
Time units represent classes of time intervals, each with certain commonalties and which interact in a limited number of ways.
For example, year and month are time units.
Something common to every year is that it decomposes into a constant number of months.
A characteristic of month is that it decomposes into a non-constant number of days, which vary according to the instance of the month.
Properties that are common to time units determine the time unit class attributes.
In 6, 7], the identier of a time unit class and the (general) duration are presented.
Relative and general durations are compared and the concept of an atomic time unit is introduced.
Time unit instances and their numbering and naming is described as well.
All attributes are formally dened in a functional way.
We here  present a summary of these concepts with examples from the Gregorian calendar.
Identier of a time unit The rst attribute of the time unit class is a unique identier, a time unit name, for example year or month.
We distinguish time unit names when the time units have the same duration but dier in their origin.
For example, years in the Gregorian calendar start in January, but academic years, in university calendars in the northern hemisphere, start in September.
Year and academic year are two dierent time units, which have several properties in common.
Duration Time units inherently involve durations a time unit expressly represents a standard adopted to measure periods of time.
Dierent time unit instances of the same time unit class can have dierent durations.
For example dierent months have different number of days, from 28 to 31.
Accordingly, the attribute representing a duration of a class is a range of possible values.
We represent this range by a pair of integers, the extremes of the range of possible lengths any time unit instance can have.
Thus, month as a class has a duration of (28 31).
A duration referred to as general will be expressed in a basic unit, common to all the time units in one calendar.
For example, using day as a common or basic unit, month has a (general) duration of (28 31) days.
A specic month, for example, February 1994, has a duration of 28 days.
We also dene another kind of duration a duration relative to another time unit.
An important reason why (general) durations of all time units in a calendar are dened based on the same basic measure unit is to be able to compare them.
This notion plays a fundamental role in the partial order among time units in the same calendar or variants of a calendar.
Time units decompose into smaller units up to a nite level at which a time unit is atomic | a non-divisible interval.
When the time interval is non-decomposable it is called a time moment, following 2].
Time units therefore model time in a discrete fashion.
Time units may be considered atomic in one application and decomposable into smaller time units in other applications, depending on the intended granularity for the application 10].
Instance names and numbers The name or number of a time unit instance can be expressed in several ways.
Similar to durations, the name is relative to another time unit, a reference time unit.
For example, a day instance can be named from Sunday to Saturday or numbered 1 to 7 if week is the reference time unit of day.
(Names can be thought of as synonyms for the numbers).
Instances of time units which do not have any reference time unit are numbered relative to a conventional reference point or zero point of the calendar, for example the Christian Era for the Gregorian calendar.
The ordered set of all the possible names the instances of a time unit can take within a reference can be expressed extensionally, by a sequence of names (if there are names associated to the time unit class), or a pair of numbers representing the range of maximum possible instance numbers of a class.
This sequence (or pair of numbers) is an attribute of the time unit class.
A particular instance name or number of a time unit within a reference reects the relative position of the instance within the reference, given a certain origin where counting of instance values starts.
For example, months are counted from 1 to 12, in a year in the Gregorian calendar, starting from 1.
But, in the case of months counted within an academic year, the origin of numbering is not month number 1, but rather the 9th (or September), so the 3rd month of a academic year is the 11th month of all the possible months name sequence (or November).
Circular counting is assumed.
More detail appears in 6].
3.1 Decomposition of time units  The primary relation among time units is that of decomposition.
When A decomposes into B, A will be referred to as the composed time unit, and B will be referred to as the component unit.
For example, a year decomposes into months and a month into days.
Also a month decomposes into weeks, a week into days, etc.
Clearly there are dierent kinds of decompositions: a year decomposes exactly into 12 months, whereas a month decomposes in a non-exact way into weeks, since the extreme weeks of the month may be complete or incomplete weeks.
We propose that all these variations in the decomposition relation can be captured with two dierent aspects of the relation: alignment and constancy.
A time unit may decompose into another in an aligned or non-aligned fashion.
The decomposition is aligned just when the composed time unit starts exactly with the rst component and nishes exactly with the last component.
Consequently, a certain number of complete components t exactly into the composed time unit.
Examples of aligned decompositions include year into months, month into days, and week into day.
Examples of non-aligned decompositions include year into weeks and month into weeks.
Figure 1 shows a graphical picture of aligned and non-aligned decomposition.
A (a) Bx  By A  A  A (b)  Bx  By  Bx  By  Bx  By  Figure 1: Graphical representation of Alignment A time unit may decompose into another in a constant or non-constant fashion.
The decomposition is constant when the component time unit is repeated  a constant number of times for every time unit instance.
Examples of constant decompositions include year into months and week into days.
Examples of non-constant decompositions include month into days and year into days.
There are four possible combinations resulting from these two aspects of alignment and constancy.
These combinations cover examples from the various calendars analyzed.
(Arguably) all the relationships of interest in such systems are covered by the variants resulting from combining alignment and constancy of decomposition.
We analyze the composition (or product), intersection (or sum) and inverse of decomposition relations.
For example, if two aligned decomposition relations are multiplied, the resulting decomposition relation is also aligned.
For example year decomposes into month, month decomposes into day.
These two (aligned) decomposition relations can be composed (or multiplied) to obtain the (aligned) decomposition relation of year into day.
It should be noticed that these three operations (composition, intersection and inverse) are dened among decomposition between time unit classes.
This is to be contrasted to the relational algebras dened in the literature and constraint propagation algorithms, which deal with relations between time intervals in the time line, i.e., at the instance level (for example 1, 18, 14].)
A detailed study of these operations appears in 6].
3.2 Calendar Structures: time unit hierarchies  A calendar structure is a pair: a set of time units and a decomposition relation.
We obtained the following result:  Theorem 1 (Decomposition) The decomposition  relation is a particular case of containment, and constitutes a partial order on the set of time units in a calendar.
Therefore, a calendar structure is dened as a containment time unit hierarchy.
The structure is dened so that variants of a specic calendar can be dened in terms of a basic one.
Such would be the case of a university or business calendar, based on the Gregorian calendar.
Such calendars have the same time units as the basic one, with possible new time units or a dierent conventional beginning point.
For example, many university calendars would have a semester time unit, where the academic year begins in September.
Since calendar structures are partial orders, they can be represented by a directed acyclic graph.
The set of nodes in a calendar structure represents the set of time units.
Edges represent the decomposition relation.
The intended application of use of the calendar structure determines which level of time units is included.
Chains We develop a categorization based on the  subrelations of decomposition, i.e.
considering only constant/aligned decompositions, or aligned only, or constant only.
Calendar substructures, composed of chains result from these subrelations.
The term \chains" appears in 17], however there chains are dened on time intervals on the time line.
In our case we are referring to chains of time unit classes.
Nonetheless, chains of time unit classes are directly related to expressions that represent time unit instances, and inuence greatly in eciency matters.
To give an example, the operation of converting time unit instances from one time unit to another will be more ecient when the time units intervening in the time unit instance expression decompose in a constant/aligned way divisions and multiplications can be done, whereas it is necessary to have some iterative process of additions or subtractions when the decomposition is not exact.
A chain is a consecutive linear sequence of time units, such that each one decomposes into any other in the chain in the same way.
Hence all time units in an aligned chain decompose in an aligned way, etc.
A calendar structure can be organized according to these chains.
There is a dierent subgraph associated to each type of decomposition.
For example, Figure 2 shows the Gregorian calendar structure characterized by two aligned chains: <28-centuries, 4-centuries, century, year, month, day, hour> and <28-centuries, week, day, hour>.
In this gure we can observe there are three common nodes to both chains: 28-centuries,day and hour.
28-Centuries 4-Centuries  Century  Year  Month  Week  Day  Hour  Figure 2: Aligned Gregorian calendar substructure We refer to the subgraph that results from organizing the calendar structures according to a certain type of chain as a calendar substructures of that particular type of decomposition (constant, aligned or constant/aligned).
Therefore a chain of a certain type is composed by all those time units in the hierarchy that form a path in the corresponding calendar substructure.
3.3 A language of the set of time units  We dene a language of time units based on the fact that calendar structures can be organized in constant/aligned, constant or aligned chains.
Together with the organization of calendar structures in chains, we distinguish special time units as primitive.
The set of primitive time units includes one time unit per chain in a minimal way.
Hence, in case that all chains have at least one common node, the primitive set is a singleton.
It was proved 6] that any calendar structure can be extended (i.e.
added time units) so that there is such unique common time unit to all chains, for any type of chain.
For example, if the Gregorian calendar is organized as in Figure 2, there are three minimal sets of primitive time units: fhourg, fdayg or f28-centuriesg.
The main idea of this language is that all time units in the structure can be recursively constructed starting from a set of primitive time units, decomposing or composing them, with decomposition limited to an aspect (for example only aligned).
The symbols that are used in this language are: A set of primitive time units, PRIM = fP P1 P2 : : :g, a set of special symbols, f = ( )g a set of (possibly innitely many) constants, CONS = fsecond minute hour :::g and a language to express durations, DUR = fd j d 2 N + g fi f(d1 d2) j + d1 d2 2 N and d1 < d2g.
The language is dened: 1.
If P 2 PRIM, then P 2 TUS.
2.
If C 2 CONS, then C 2 TUS.
3.
If T 2 TUS and D 2 DUR then (T  D) 2 TUS.
4.
If T 2 TUS and D 2 DUR then (T=D) 2 TUS.
5.
Those are all the possible elements of TUS.
Briey, T = (S  D) when T is composed of a contiguous sequence of D S's and T = (S=D) when D contiguous T's compose S. For example, if we organize the Gregorian calendar with aligned chains, the following is a possible interpretation of the language: PRIM = fdayg month = day  (28 31) year = day  (365 366) = month  12 hour = day=24 twoday = day  2 trimester = month  3: Generally we consider only one time unit as primitive.
It is convenient to include constants into the language.
We abuse notation in that we name constants, which are part of the alphabet of symbols, by the name of the domain elements.
Thus if  is an interpretation function,  : TUS  Calendar ;!
Time units in the calendar.
For example (month Gregorian) = \month", (month  3 University) = \trimester".
As long as it does not lead to ambiguities, we will not make an explicit use of this interpretation function in this paper.
Using the same strings for constants and domain elements appears elsewhere, for example 20].
3.3.1 Named time units  As discussed in previous sections, there is more than one level of time unit instantiation.
For exam-  ple, a subclass of month in the Gregorian calendar could be the fourth month (synonym of April).
This does not represent a specic month yet.
We call April a named-month, and viewed extensionally, it represents the set of all specic instances of the month \April".
A specic instance would be \April 1995".
To be able to express specic instances we dene calendar expressions in the next section.
Named time units are dependent on the time unit, a reference time unit and a position within the reference.
Names and numbers are functions dened on the sets of time units, and some involving instances as well.
We write name(T R X) to represent the Xth \named-T" within the reference time unit R, such that R decomposes into T .
If X is outside permissible values, or if the time unit has no associated names with that reference name(T R X) represents an inconsistent value (?).
Examples of this function include name(month year 3) = \March" name(month academic year 3) = \November" name(day week 8) = ?.
Numbered-time units are dened in an analogous way.
For example number(month year 3) = \3".
A function related to named and numbered time units is the last possible value.
Some cases, as already explained, will not provide a unique value, but a range of last values.
In those cases there exists a unique last value only at the instance level (and not at the class level).
For example, last name(month year) = \December" last number(day month) = (28 31) last number instance(day F ebruary 1994) = 28.
4 Calendar expressions  A goal of our research is to represent and reason with time unit instances, that is, the temporal counterpart of single or repeated activities occurring in the time line.
We want to express these time entities in terms of days, weeks, hours, etc, relative to certain conventional calendars.
Calendar structures above dened provide a formal apparatus of units on which to represent a date-based temporal counterpart of activities.
Intervals, time points and moments in the time line will be represented in terms of these units.
Specic time objects are expressed based on the time units in a calendar structure, via calendar expressions.
A basic calendar expression is dened by a conventional beginning reference point or zero point associated to the calendar, and a nite sequence of pairs: Z 	 (t1 x1) : : : (tn xn)].
Each pair (ti xi) contains a time unit ti from the calendar structure and a numeric expression xi.
Numeric expressions include numbers and variables ranging over the set of integers.
The pairs in the sequence are ordered so that any time unit in a pair decomposes into the time unit in the following pair in the sequence.
We also dene duration expressions.
These are similar to calendar expressions in that they consist of a list of pairs (time unit,value), but have no beginning refer-  ence point nor included variables.
The operation of adding a duration expression to a calendar expression denes a new calendar expression.
4.1 A language for simple and repetitive time unit instances: calendar expressions  The formal denition of the language of calendar expressions is presented next.
The language of duration expressions is not introduced here for space reasons, it follows a similar style as the calendar expressions language.
The decomposition relation is used in the denitions.
The same kind of decomposition used to dene the time units in the calendar structure (and therefore the time units language, TUS) is used in these languages.
The language of calendar expressions CALXS, uses the following: A conventional zero point, Z, a set of special symbols f	 ( )  ] \ fi = Lastg, a set of time units (TUS), a set of variables (VAR), which will be ranging in the set of positive naturals, a set of duration expressions(DURXS), and can be dened as: 1.
If T 2 TUS X 2 N + fi VAR then (Z 	 (T X)]) 2 CALXS.
2.
If T1 T2 2 TUS, such that T1 decomposes into T2 , X2 2 N + fi VAR fi fLastg and Z 	 CX (T1 X1)] 2 CALXS, then (Z 	 CX (T1 X1 ) (T2 X2 )]) 2 CALXS.
3.
If CX(*v ) 2 CALXS, where *v are the variables * * in CX, then (CX( v ) where Exp( v )) 2 CALXS Exp(*v ) is an expression constraining *v .
4.
If CX1 and CX2 2 CALXS then (CX1 fi CX2 ) (CX1 \ CX2 ) (CX1 =CX2) 2 CALXS.
5.
If CX 2 CALXS and DX 2 DURXS then (CX + DX) 2 CALXS.
6.
These are all the possible elements of CALXS.
4.2 Examples of calendar expressions  The following examples express calendar expressions in the Gregorian calendar.
The zero point is the beginning of the Christian Era (CE).
1.
CE 	 (year X) (month 4)].
The fourth month within any year.
(Viewed extensionally, it represents the set of all specic instances of the month \April").
X is a non-quantied variable.
2.
CE 	 (year 1994) (week 17) (day 5)].
The 5th day within the 17th week of the year 1994.
The last time unit in the sequence provides the precision of the time unit instance.
Thus, in Example 1 above, the precision is month.
It can also be observed that a calendar expression with no variables is a single convex interval a time unit instance of the last time unit in the calendar expression.
Consequently, Z 	(t1 x1) : : : (tn xn)] is a (single) tn-instance.
Example 2 above is a (single) day-instance.
A calendar expression with variables represents a  set of time unit instances.
These sets of intervals are  the temporal counterpart of a date-based repeated activity, a specic case of a non-convex interval, as dened in 12] .
Thus, CE 	 (year X) (day 175)] represents a non-convex interval, and the subintervals are the days numbered 175th.
As well, when there are variables in the calendar expression, these can be constrained with logic/mathematical expressions.
This example can be extended to: 3.
CE 	 (year Y ) (day 175)] where (Y > 1992 and Y < 1996).
The 175th day from the beginning of each year after 1992 and before 1996.
Hence starting and ending times of non-convex intervals can be expressed straightforwardly.
Another extension to the language of calendar expressions consists of applying set operations (union, intersection and dierence) to combine non-convex intervals, viewing non-convex intervals as sets of subintervals.
(limit cases could produce an empty set of intervals, which we consider a limit case of calendar expression).
The following illustrate this possibility: 4.
CE 	 (year 1994) (month 11) (day X)] \ CE 	 (year 1994) (week W) (day Tuesday)].
Tuesdays of November 1994.
5.
CE 	 (year 1994) (month 11) (day X)] = (CE 	 (year 1994) (month W) (day Y )] where (Y = 7 or Y = 1) ).
Days of November 1994 which are not Saturdays nor Sundays, i.e., weekdays of November 1994.
Saturday and Sunday are abbreviations of certain days numbers as numbered within week.
= represents set dierence.
Finally we also allow calendar expressions to be moved (or displaced) by adding a certain (convex) interval.
For example, October 1994 plus one month is November 1994.
We use duration expressions to express moved expressions.
For example: 6.
CE 	 (year 1994) (month 11) (day 5)] + (day 5)].
The 5th day of November is moved 5 days, resulting in the 10th day of November 1994.
7.
CE 	 (year 1994) (month X) (day 5)] + (day 5)] The 10th day of every month in 1994.
It is worth noticing that (month 1)] is a duration expression representing one month.
On the contrary, CE 	 (year Y ) (month 1)], stands for the non-convex interval of all January's.
Named time units are precluded as a valid duration.
The following examples show how to represent slightly more elaborated periodicity patterns.
8.
CE 	 (year 1994) (week  2 W ) (day 1)].
Every other Monday of 1994.
9.
CE 	 (year 1994) (month M) (day Last)].
Last day of every month.
\Last" is based on the last number function above presented.
Examples 1, 2 and 8 above are accounted by rules 1 and 2 of the language.
Rule 3 allows to have calendars with variables that are constrained, as in example 3.
Examples 4 and 5 are covered by rule 4.
Finally examples 6 and 7 correspond to rule 5.
4.3 Semantics of calendar expressions  A set-based semantics is proposed for these languages.
Interpretation of duration expressions and calendar expressions is based on an interpreted calendar structure.
Recall that we conventionally name constants in the language of the time units (TUS) as the elements in the domain, i.e.
as the time units in the calendar.
Therefore, for the sake of a more clear presentation we will omit the application of an interpretation function when referring to interpreted time units whenever this does not lead to ambiguities.
Examples will be made within the Gregorian calendar.
Hence the zero point Z will be interpreted as year zero of the Christian Era (CE).
A duration expression is interpreted as a convex interval in the time line.
Comparison, addition and subtraction of duration expressions are dened.
Addition and subtraction can be viewed as translations or displacements in the time line.
Limit cases, circular counting, dierence in precision, etc, are taken into account.
This is not presented in this paper.
Calendar expressions are interpreted as sets of intervals in the time line.
Let !
be a function interpreting calendar expressions.
(We omit here the specication of the calendar to simplify this presentation, and again will provide examples from the Gregorian Calendar), then !
: CALXS ;!
2time intervals.
1.
!
(Z 	 (T X)]) = a.
If X 2 N + , the singleton with the X th occurrence of the interval T starting from !(Z).
For example, !
(Z 	 (year 1995)]) = fyear 1995g.
b.
If X is a variable, Sthe set of time intervals T starting from !
(Z), i.e.
1 X =1 !
(Z 	 (T X)]).
For example, !
(Z +	 (year Y )]) = set with every year since CE = N .
2.
!
(Z 	 CX (T1 X1 ) (T2 X2 )]) = a.
If X2 2 N + , the set of X2th subintervals T2 within each interval in the set !
(Z 	 CX (T1 X1)]).
For example, !
(Z 	 (year Y ) (month April)]) = fApril 1, April 2, : : : g. b.
If X2 = Last, the set of Lth subintervals T2 within each interval in the set !
(Z 	 CX (T1 X1 )]), where L = last number(T2 T1) as dened in Section 3.3.1 above.
For example !
(Z 	 (year Y ) (month Last)]) = f December 1, December 2, : : : g. In case T1 decomposes into T2 in a non-constant way, L depends on X1 and possibly on CX.
For example !
(Z 	 (year 95) (month M) (day Last)]) = fJanuary 31 1995, February 28 1995, : : : g. c. If X is a variable, the set of all time intervals T2 within eachS interval in the set !
(Z 	 CX (T1 X1 )]), i.e.
LX =1 !
(Z 	 CX (T1 X1)]), where L = last number(T2 T1).
For example, !
(Z 	  (year 1995) (month M)]) = fJanuary 1995, February 1995, : : : , December 1995 g. 3.
!
(CX(*v ) where Exp(*v )) = S* * !
(CX(*v )).
v j Exp( v ) For example, !
(Z 	 (year 95) (month M)] where (M > 3 and M < 6) ) = fApril 1995, May 1995g.
4.
!
(CX1 fi CX2 ) = !
(CX1 ) fi !
(CX2 ) !
(CX1 \ CX2 ) = !
(CX1 ) \ !
(CX2 ) !
(CX1 =CX2 ) = !
(CX1 )=!
(CX2 ) See examples in Section 4.2 above.
5.
!
(CX = S T + DX) (DX ) (Interval) 8 Interval 2 !
(CX) Addition of a duration expression is interpreted as a translation (T) or displacement of all the subintervals denoted by the calendar expression, eventually a single one (!
(CX)), by a distance dened by the duration expression (!(DX)).
See examples in Section 4.2 above.
5 Alternative proposals dealing with calendars  3] by Chandra et.al.
present a language of \calendar expressions" to dene, manipulate and query what they call \calendars".
In terms of terminology used, their and our proposals coincidentally refer to \calendars" and \calendar expressions".
The notions however are of a dierent nature the two researches have evolved completely independently.
We refer to \calendar" in the usual sense, where the Gregorian calendar or a university calendar are possible examples.
On the other hand, their \calendars" are structured collections of intervals, as dened in 15].
The dierence between both \calendar expressions" is more subtle.
Both calendar expressions dene, in dierent ways, lists of points and intervals in the time line.
However, our expressions are of a more declarative nature, whereas their expressions are closer to a procedure to obtain such intervals.
For example, we express the collection of Fridays as: CE 	 (year Y ) (week W ) (day 5)].
The calendar expressions in 3] are meant as a way of creating and manipulating their \calendars" (i.e., collections of intervals).
5]/DAYS:during:WEEKS expresses the Fridays collection in their language.
There are two operations involved here: selection and the strict foreach operation (or dicing, as dened in 15]) with the operator during.
We have continued further a formal denition of the domain and languages.
On the other hand, Chandra et al.
's work includes a description of an algorithm to parse their expressions and generate a procedural plan to produce an evaluation plan, implemented in Postgress.
As well, they outline how to incorporate calendar expressions to temporal rules in a temporal data base.
We have not dealt with temporal  databases however it is perceived as an area in which to apply our framework.
Ciapessoni et al.
4] dene a many sorted rst order logic language, augmented with temporal operators and a metric of time.
The language is an extension of a specication language TRIO 9].
We will focus the present discussion on the semantics of their system, specially in the extension they provide to embed time granularities in the language.
At this stage of our research, this is where we see the two works as being comparable.
The semantics of the language in 4] includes a temporal universe which consists of a nite set of disjoint and dierently grained \temporal domains".
Each temporal domain contains temporal instants expressed in the corresponding granularity.
Time domains relate with a granularity (or coarseness) relation and a disjointedness relation.
Very briey, our \time units classes" are related to their \time domains" our \decomposition relation" to their \granularity relation" our \repetition factor" to their \conversion factor", our \aligned decomposition" to their \disjointedness" relation.
Another similar characteristic in both proposals is the distinction between time moments and durations (or displacements in their terminology).
We deal with a more general concept of repetition factor.
In fact we extensively address non-constant decomposition.
We emphasize how to express temporal entities based on dates with the conventions of the Gregorian and other calendars.
At the present stage we specially address the formal representation and reasoning about repetitive temporal terms.
As well, we believe that our abstraction of decomposition with only two characteristics (constancy and alignment) denes the relations of interest between the time unit classes in a more general way, as compared to the distinction of dierent relations in 4].
6 Discussion  The goal of this work is to be able to represent and reason eciently about repeated activities using the formalism.
We have dened a hierarchical structure of time units, emphasizing on decomposition among the time units.
We take into consideration the distinction between time unit classes, named time units and specic time unit instances, (for example \month", \August", and \August 1994").
A characterization of the hierarchical structure considers subrelations of decomposition, according to the dierent aspects of decomposition: aligned, constant, and constant/aligned.
A path in such a substructure is referred to as a chain, where all time units decompose in the same way.
Chains constitute the basis we use to dene a formal language to characterize time units.
We also introduce a language to represent time unit instances, called calendar expressions.
These ex-  pressions provide for a straightforward way of representing the temporal counterpart of repeated activities.
We envision that reasoning with calendar expressions based on time units in one chain will produce very ecient operations.
Constant/aligned chains are expected to be particularly ecient.
Currently we are working on further formalizing useful operations between calendar expressions.
A set based semantics has been provided for such expressions.
We are investigating alternative semantic characterizations.
We are also considering adding more expressive power to the language to consider for example expressions that add uncertainty, as for example \twice a week".
Important future research includes studying algorithms that would best t with this formalism, so that we may obtain ecient inferences when reasoning about repeated activities.
Algorithms developed for qualitative and/or quantitative temporal constraint satisfaction problems, or variations, could be considered in this matter.
A language which allows one to dene time units of a variant calendar, such as a university calendar, in terms of a basic calendar, such as the Gregorian, is under study also.
Finally, we believe that the formalism dened represents a generic approach, appropriate to ultimately reason eciently with repeated events within any measurement system based on discrete units that relate with a repetitive containment relation, such as the Metric or Imperial systems.
References  1] J. F. Allen.
Maintaining knowledge about temporal intervals.
Communications of the ACM, 26(11):832{843, 1983.
2] J. F. Allen and P. J. Hayes.
Moments and points in an interval-based temporal logic.
Computational Intelligence, 5:225{238, 1989.
3] R. Chandra, A. Segev, and M. Stonebraker.
Implementing calendars and temporal rules in next generation databases.
In Proc.
of the International Conference on Data Engineering, ICDE94, pages 264{273, 1994.
4] E. Ciapessoni, E.Corsetti, A. Montanari, and P. San Pietro.
Embedding time granularity in a logical specication language for synchronous real-time systems.
Science of Computer Programming, 20:141{171, 1993.
5] J. Cliord and A. Rao.
A simple, general structure for temporal domains.
In C. Rolland, F. Bodart, and M. Leonard, editors, Temporal aspects of information systems, pages 17{28.
Elsevier Science Publishers B.V. (North-Holland), 1988.
6] D. Cukierman.
Formalizing the temporal domain with hierarchical structures of time units.
M.Sc.
Thesis.
Simon Fraser University, Vancouver, Canada, 1994.
7] D. Cukierman and J. Delgrande.
Hierarchical decomposition of time units.
In Workshop of Temporal and Spatial reasoning, in conjunction with AAAI-94, pages 11{17, Seattle, USA, 1994.
8] R. Dechter, I. Meiri, and J. Pearl.
Temporal constraint networks.
Articial Intelligence, 49:61{ 95, 1991.
9] C Ghezzi, D. Mandrioli, and A. Morzenti.
Trio, a logic language for executable specications of real-time systems.
Journal of Systems and Software, 12(2), 1990.
10] J. Hobbs.
Granularity.
In Proc.
of the Ninth IJCAI-85, pages 1{2, 1985.
11] J.
A. G. M. Koomen.
Localizing temporal constraint propagation.
In Proc.
of the First Inter-  national Conference on Principles of Knowledge Representation and Reasoning, pages 198{202,  12] 13] 14] 15] 16] 17] 18] 19] 20]  21]  Toronto, 1989.
P. Ladkin.
Time representation: A taxonomy of interval relations.
In Proc.
of the AAAI-86, pages 360{366, 1986.
P. B. Ladkin.
Primitives and units for time specication.
In Proc.
of the AAAI-86, pages 354{359, 1986.
P. B. Ladkin and R. D. Maddux.
On binary constraint problems.
Journal of the ACM, 41(3):435{469, 1994.
B. Leban, D. D. McDonald, and D. R. Forster.
A representation for collections of temporal intervals.
In Proc.
of the AAAI-86, pages 367{371, 1986.
G. Ligozat.
On generalized interval calculi.
In Proc.
of the AAAI-91, pages 234{240, 1991.
S. A. Miller and L. K. Schubert.
Time revisited.
Computational Intelligence, 6(2):108{118, 1990.
R. A. Morris, W. D. Shoa, and L. Khatib.
Path consistency in a network of non-convex intervals.
In Proc.
of the IJCAI-93, pages 655{660, 1993.
M. Poesio and R. J. Brachman.
Metric constraints for maintaining appointments: Dates and repeated activities.
In Proc.
of the AAAI-91, pages 253{259, 1991.
R. Reiter.
Towards a logical reconstruction of relational database theory.
In M. L. Brodie, J. Mylopoulos, and J. W. Schmidt, editors, On Conceptual Modeling, pages 191{238.
SpringerVerlag, 1984.
M. Vilain and H. Kautz.
Constraint propagation algorithms for temporal reasoning.
In Proc.
of the AAAI-86, pages 377{382, 1986.
Generating Explanations with the Help of Temporal Constraints Margo Guertin  Computer Science Dept., Boston University, Boston, MA.
02215 guertin@cs.bu.edu  Abstract This paper describes Holmes, a new system for generating explanations for cyclic events using temporal constraints.
The domain is electrical conduction in the heart, with the temporal constraints coming from the trace of an electrocardiogram (ECG), and an explanation being a laddergram | a diagram of the paths of the beat impulses through the heart which could have produced that ECG.
I have combined a method for propagating temporal constraints with an object-oriented model of heart conduction, and it has resulted in a remarkably quick and ecient search of the vast space of possible explanations.
This type of system, which combines temporal reasoning and abductive inference, may well point the way to much fruitful research in the future.
1 Introduction  My original motivation for developing the Holmes system was to build a program which could use ECG information to automatically trace the paths of beats through the heart.
This is something which is currently done by hand when a doctor suspects heart conduction problems.
As I worked on the system, I realized it was also an interesting and promising hybrid of temporal reasoning and abductive inference.
It is this aspect I concentrate on here.
Abduction, the inference of the best explanation for the observed evidence, is a small but steadily growing 	eld of AI research.
A clear and thorough discussion of it appears in John and Susan Josephson's recent book on abductive inference 3], in which they describe past and current systems for doing automated abduction.
All these systems must deal with the problem of an enormous search space of explanations which might account for a body of evidence.
Since there are far too many possible hypotheses to look at them all, much eort has been expended on ways of directing the search towards the hypotheses most likely to produce the \best" explanations.
Josephson points out that a system can avoid these problems of search complexity if it has enough of the right kind of information and an eective strategy for using it.
2] I believe that the temporal intervals which represent the start, stop and duration times of events in the Holmes system are an example of this right kind of information.
Almost all of the vast search space of hypotheses are ruled out quickly by a form of quantitative temporal constraint propagation, based on Isaac Kohane's Temporal Utility Package (TUP)4], a domain independent utility which makes use of interval arithmetic to propagate quantitative temporal constraints.
TUP, in turn, is based upon upon James F. Allen's interval-based logic 1].
In my domain, a hypothesis is a chronologically ordered list of events, each of which may account for one of the \bumps" on the ECG.
For example, the event of the left ventricle of the heart being activated by an impulse from the adjacent left bundle branch at time could be used to explain an ECG spike (or QRS wave) at time .
An explanation is a causal network of events which can account for all the ECG evidence.
Each attempted explanation is generated incrementally and in order, incorporating one event at a time, beginning at the time of the 	rst piece of evidence and ending with the last piece of evidence, until an inconsistency is detected or an explanation has been successfully completed.
Any inconsistency which arises is either because an event necessary to the particular explanation being attempted is inconsistent with the evidence or with some event(s) previously incorporated into it.
Because events are incorporated in chronological order, failure can only arise from a conict with the evidence or previous events already part of the explanation being formed.
This means that if an ordered hypothesis of events A, B, C, D, E, F, fails on the attempt to incorporate D, then all hypotheses whose 	rst four events are A, B, C, and D can be ruled out.
In a search space where each hypothesis consists of many events, a failure that occurs early in the hypothesis list allows a shallow cuto in the search tree, eliminating a significant number of hypotheses from consideration.
One is particularly likely to encounter this kind of ecient pruning in a cyclic domain where the pieces of evidence are often repetitive.
In such a domain, the x  x  :::  chances are high that any inconsistency which may occur near the end of a list of hypothesized events, is a repeat of the same inconsistency occuring much earlier in the list, where the pruning is more drastic.
Thus, in the domain of heart conduction, it is possible to take full advantage of the cyclic temporal information available, propagate temporal constraints to rule out almost all of the possible explanations early, and consider only those remaining.
This ability to generate all the explanations would be important in any scenario where more evidence might be gathered later which could rule out what was formerly rated the \best" explanation.
It would also be important to any system meant to discover new explanations, not normally considered.
In the following sections, I discuss the domain of heart conduction in more detail, give an overview of its control with a brief example of how it uses temporal constraint propagation, and 	nally give my preliminary statistics on search eciency from generating explanations for 	ve dierent (normal) ECGs.
2 The Heart Conduction Domain  The information on heart physiology and electrocardiography below comes primarily from three sources 5, 6, 7].
The heart can by thought of as a 	st-sized electric pump for moving blood through the body (see Fig.
1) .
When all is going well, the sino-atrial SA node Atria AV node & Bundle of His Right Bundle Branch  Left Bundle Branch Right Ventricle  atrioventricular node (AV node) and the \Bundle of His", and the current travels through them to the left and right bundle branches.
From the bundle branches it travels into the ventricles and the septum between them, normally activating them all simultaneously.
As the current travels through the ventricles, they contract powerfully, forcing the blood just received from the atria on out into the body.
When any part of the heart is activated, it begins to depolarize , which has the eect of spreading the current throughout, and soon to adjacent parts.
Once the process of depolarization is complete, the longer process of repolarization begins.
This can be thought of as a recovery process, during which the heart part cannot be activated by any impulse.
Once the repolarization process is complete, the part is in its initial resting state, ready to receive and carry an impulse comes along.
Each heart part can be thought of as a little 	nite state machine (FSM), and the whole heart as a bunch of parallel, interacting FSMs.
In the Holmes heart model, an event is de	ned to be the activation of one heart part, either by an adjacent part, or occasionally, by itself.
(It is possible for any heart part to activate itself by producing its own electrical impulse spontaneously, although normally this only happens in the SA node.)
The representation of an event in the model includes temporal interval parameters for the start of its depolarization, duration of depolarization, start of repolarization, duration of repolarization, and start of its rest state.
The electrocardiogram (see Fig.
2) is a trace of the heart's electrical activity as measured from dierent points on the surface of the body.
It provides incomplete evidence of the impulse paths through the heart, because some heart parts produce a current strong enough to be measured from the outside, while others do not.
The atria depolarizing give rise to the P wave, the ventricles depolarizing produce the QRS complex, and the T wave is the result of the ventricles repolarizing.
All other events are invisible to the ECG and must be inferred by Holmes.
Left Ventricle  QRS  QRS T  P  T P  Septum  Figure 1: Diagram of the main parts of the heart node (SA node) at the top generates a steady electrical pulse which is conducted down through the other parts of the heart.
The SA node's current 	rst activates the upper chambers of the heart (the atria), which contract and squeeze the blood into the lower chambers (the left and right ventricles).
Very soon after the SA node activates the atria, it activates the  Figure 2: Two heartbeats in a typical ECG A hypothesis is a list of assumed events, each of which could account for one bump on the ECG.
In Fig.
2, for instance, each P wave could be due to the atria being activated by the SA node, or the atria activating themselves spontaneously.
Each QRS could be the left ventricle being activated by the left bun-  dle branch, the left ventricle being activated by the septum, or the left ventricle activating itself spontaneously.
So, even for a 2-beat ECG, we would have 2 3 2 3, or 36 4-event hypotheses.
In this domain, an explanation is a complete causal network of events which could have resulted in the ECG.
Following is an example of a possible explanation for a P wave, followed by a QRS complex, followed by a T wave.
The SA node generated a beat which activated the atria and the AV node and Bundle of His.
Then the Bundle of His activated the left and right bundle branches, and soon after, the left bundle branch activated the left ventricle and septum, while the right bundle branch activated the right ventricle.
Any explanatory network must be internally consistent, and include all the assumed events of the hypothesis, as well as all invisible events (within the ECG's time span) which either lead to or resulted from the hypothesis events.
All events in the explanation must, of course, be consistent with the ECG evidence.
3 The Holmes System 3.1 Control  The Holmes system, shown in Fig.
3, contains a general model of the heart which is individualized with information from the ECG.
For example, normal heart rates range anywhere from 55 beats per minute to 120 beats per minute, but if the ECG shows a normal heart rate, in this case varying from, say, 80 to 95, the interval range for the heart rate parameter is restricted accordingly.
As many parameters as possible are tightened using the ECG information.
The hypothesis former, a set of rules for forming all possible hypotheses from an ECG, swings into action next, producing a very large number of hypothesis lists which go into the search space.
(In the case of an apparently normal -beat ECG, 6n lists of length 2 are produced.)
From this point on, all hypotheses in the search space (which have not been pruned away) are fed to the explanation, one at a time, until the search space is exhausted.
The explanation developer attempts to form all explanations possible for each hypothesis it receives.
(There can be several explanations for the same hypothesis because the hypothesis is a list of only visible events, and there can be a number of unique causal networks of events which include the same set of visible events.)
For each attempted explanation, the explanation developer starts a chronologically ordered list of events to be incorporated, which initially contains only the events of the hypothesis, but is expanded, with each event successfully incorporated, to include all events which would result from it.
Each event incorporated must be consistent with the ECG and with the events already incorporated in the explanation.
When it is added, any constraints it imposes are propagated back through the model.
If failn  n  ure occurs, the explanation is abandoned and its failure point noted.
All successful explanations resulting from the hypothesis are put into the pool of completed explanations.
If all attempts to explain the current hypothesis fail, the latest hypothesis event responsible for failure is noted and sent on to the search space pruner, which prunes the search space accordingly.
3.2 Simplied Example of Temporal Constraint Propagation  Following is a simpli	ed example of constraint propagation through a very small causal network of four events | the activations of four adjacent heart parts, say, A, B, C, and D. It is exactly this kind of thing which happens, over and over, as Holmes tries to develop an explanation, and each event being incorporated in the explanation is checked for internal consistency with those already there.
Given: 1.
The activations of heart parts A, B, and C do not show up on the ECG the activation of part D does.
2.
It has been inferred from the model and the assumptions of the current hypothesis that the activation of part A occurred some time between 0 and 20 msec.
We represent this by the interval (0, 20).
3.
Part A is adjacent to part B.
4.
It takes (20, 40) for a current to travel from A to B.
5.
Part B is adjacent to parts C and D (as well as A).
6.
It takes (5, 15) for a current to travel from B to C. 7.
It takes (10, 20) for a current to travel from B to D. So initially: 1.
A is activated at (0, 20).
2.
B is activated at (20, 60).
3.
C is activated at (25, 75).
4.
D is activated at (30, 80).
But then we see from the ECG that part D was activated at 45 (or (45, 45) in interval notation).
This restricted start time for the activation of D constrains the start of B, which in turn constrains the starts of A and C. Using interval arithmetic 4], we end up with the following constrained information: 1.
A is activated at (0, 15).
2.
B is activated at (25, 35).
3.
C is activated at (30, 50).
4.
D is activated at (45, 45).
ECG (Evidence)  General Heart Model  Hypothesis Former  Parameter Learner  Search Space of Hypotheses  Explanation Developer  Partial Explanation Search Space Pruner  Individualized Heart Model  Constraint Propagator  failure  success Completed Explanations  Figure 3: Architecture of the Holmes System  4 Results and Conclusions  cyclic domains of medical and environmental science.
At this point in time, the Holmes system is fully operational for normal (i.e.
healthy) ECGs, and I have run it successfully on 	ve such ECGs so far.
I am now in the process of 	ne tuning the general heart model to allow it to handle abnormal ECGs as well, since one of my main motivations in developing this system was to make a physicians' tool for diagnosing heart conduction problems.
I have gotten good results with the 	ve ECGs I have tried.
For each, Holmes has come up with the simplest explanation (that of a normal sinus rhythm with no aberrations).
For two of them, it has also found some more | for ECG #1, three other explanations, and for ECG #5, two others.
While these alternative explanations are much less likely, it is quite interesting to be able to see what aberrant events could could be occuring in a heart's conduction system, when a normal-looking ECG trace is being produced.
The eciency of performing a complete search is very high.
I measure eciency with the ratio of the number of hypotheses which need to be examined in a complete search to the search space size (about 6n for an -beat ECG).
In my 	rst 	ve runs, the eciency ratio is 34 : 610, in the least ecient search, and 135 : 617, in the most ecient.
I think that these results are due, in large part, to the pruning enabled by temporal constraint propagation, and are further improved by the cyclic nature of the domain.
I hope they will encourage more researchers to incorporate temporal information and constraint propagation techniques into their models, and to start work on some of the many unexplored  Acknowledgments  n  This paper has bene	tted greatly from the insightful comments and questions of John Josephson and Michael Weintraub on an earlier draft.
I am also very grateful to Bipin Indurkya and Scott O'Hara for their time and thoughtful criticism throughout its development.
References 1] James F. Allen.
Maintaining knowledge about temporal intervals.
Communications of the ACM, 26(11):832{843, 1983.
2] John R. Josephson.
Personal communication.
3] John R. Josephson and Susan G. Josephson.
Abductive Inference: Computation, Philosophy, Technology.
Cambridge University Press, 1994.
4] Isaac S. Kohane.
Temporal Reasoning in Medical Expert Systems.
PhD thesis, Boston University, 1987.
5] Henry J. L. Marriott.
ECG/PDQ.
Williams & Wilkins, 1987.
6] Lionel H. Opie.
The Heart: Physiology and Metabolism, chapter 5, pages 102{126.
Raven Press, New York, second edition, 1991.
7] Allen M. Scher.
The electrocardiogram.
In Harry D. Patton, Albert F. Fuchs, Bertil Hille, Allen M. Scher, and Robert Steiner, editors, Textbook of Physiology: Circulation, Respiration, Body Fluids, Metabolism, and Endocrinology, chapter 38, pages 796{819.
W.B.
Saunders  Company, 21st edition, 1989.
Experimental Evaluation of Search and Reasoning Algorithms, 1994.
11] Debasis Mitra, Padmini Srnivasan, Mark L. Gerard, and Adrian E. Hands.
A probabilistic interval constraint problem: fuzzy temporal reasoning.
In World Congress on Computational Intelligence (FUZZ-IEEE'94), 1994.
12] Raul E. Valdes-Perez.
The satisability of temporal constraint networks.
In Proceedings of AAAI, 1987.
13] Peter van Beek.
Ph.D. dissertation: Exact and Approximate Reasoning about Qualitative Temporal Relations.
University of Alberta, Ed-  monton, Canada, 1990.
5 Conclusion In this article we have described some advantages of having the capability to generate all possible consistent singleton models (CSM), from a given set of temporal intervals and some incomplete constraints between them.
The problem of nding all consistent singleton models is plagued with two sources of expected exponential growth with respect to the number of temporal entities in the data base.
First, the problem of nding one consistent model is itself NP-complete.
Second, intuition suggests that the number of models should increase exponentially with respect to the number of nodes.
The latter apprehension is possibly not appropriate, at least from our preliminary experimental results with randomly generated temporal networks.
Number of models actually reduces as constraining level increases with the number of nodes6].
Ladkin's experiment4] has also allayed the fear about hardness of the problem of nding one consistent model.
An algorithm is described here which systematically searches and prunes search space to achieve not only one CSM, but all CSM's.
The systematic nature of its searching (and pruning) seem to be the key to its eciency.
Signicance of this algorithm in temporal reasoning and its capability to nd all CSM's in reasonable amount of time is the focus of this article.
Theoretical implications involve the capability of studying global consistency problem empirically at much greater depth than has been done so far.
Our mechanism also makes us capable to handle non-monotonicity (of a particular type) in temporal reasoning.
Other theoretical implications are also discussed in this article.
There are quite a few implications of this algorithm from the point of view of applied research.
Some of the most important implications involve temporal data base, critical-domain application areas, prediction/explanation, generating all feasible (interval-based) plans, plan recognition, and image recognition There are many other implications which have not been discussed in this article (for example, parallelizability of the algorithm and associated speed up).
Generating all CSM's were considered to be a very hard problem.
This is the main reason why no one (as far as we know) has looked into the signicance of having all CSM's available.
Higher eciency of our algorithm makes such modeling feasible, and hence studying the signicance  of such modeling necessary.
References 1] J. F. Allen.
Maintaining knowledge about temporal intervals.
Communications of the ACM, 26(11):510{521, 1983.
2] J. F. Allen.
Towards a general theory of action and time.
Arti cial Intelligence, 26(2), 1984.
3] Peter Cheeseman, Bob Kanefsky, and William M. Taylor.
Where the really hard problems are.
In Proccedings of International Joint Conference on Arti cial Intelligence, pages 331{337, 1991.
4] Peter B. Ladkin and Alexander Reineeld.
Eective solution of qualitative interval constraint problems.
Art cial Intelligence, 57:105{124, 1992.
5] David Mitchell, Bart Seleman, and Hector Levesque.
Hard and easy distributions of sat problems.
In Proceedings of the Tenth National Conference of Am.
Asso.
of Arti cial Intelligence, 1992.
6] Debasis Mitra.
Ph.D. dissertation: Eciency issues in temporal reasoning.
University of  Southwestern Louisiana, Lafayette, 1994.
7] Debasis Mitra and Rasiah Loganantharaj.
All feasible plans using temporal reasoning.
In Proceedings of AIAA/NASA conference on intelligent robotics in  eld, factory, services and space, '94, 1994.
8] Debasis Mitra and Rasiah Loganantharaj.
Efcent exact algorithm for nding all consistent singleton labelled models.
In Proceedings of IEEE Robotics and Automation conference, Nice, France, 1991.
9] Debasis Mitra and Rasiah Loganantharaj.
Weak-consistency algorithms for intervalbased temporal constraint propagation.
In AAAI'94 workshop note on Spatial and Temporal reasoning, 1994.
10] Debasis Mitra, Nabendu Pal, and Rasiah Logananthatraj.
Complexity studies of a temporal constraint propagation algorithm: a statistical analysis.
AAAI'94 workshop note on  example, we may study how such models grow in planning domain with respect to the structural parameters mentioned above.
(c) Studying real-life problems: Instead of studying average number of models (or average-case complexity) for randomly generated networks, as suggested in previous paragraphs, one can study networks from classes of real life of temporal consistency-problems, e.g., planning, scheduling and diagnosis.
(d) Topological distance between local consistency problems and global-consistency problem: Local consistency problems and global-consistency problem form a hieararchy9].
An ecient globalconsistent algorithm makes it feasible to empirically study the topological distance6] between global-consistency problem and any other localconsistency problem (e.g.
3-consistency), for which ecient polynomial algorithm already exists.
Such study will expose a quantitative measure of the usefulness of these local consistency algorithms.
(e) Predictability of complexity: In a recent set of experimental studies we are trying to have the capability of predicting run-time of an algorithm based on the structure of a problem instance10].
In addition to predicting time-complexity, now we could predict the number of CSM's a problem instance can have, which is more related to the problem-structure.
In order to gain such statistical prediction power, we need to experiment with our algorithm, in line with such experiments for empirically studying time-complexity.
(f) Non-monotonic temporal reasoning: The algorithm requires maintenance of all CSM's at each stage of updating the network.
This enables one to go back and redo the reasoning from any stage, in case some information on any old arc gets changed in future.
This non-monotonicity is not feasible unless all CSM's are preserved at each stage.
4 Practical implications of the algorithm Practical implication of being able to incrementally developing models, and having all such models available in a data base are immense.
Some of them are discussed in this section.
(a) Temporal data base: A temporal data base will grow incrementally as and when a new temporal assertion comes to the knowledge of the user.
So far temporal reasoning community has not paid much attention to this fact.
The present algorithm is particularly tuned to such application for ecient incremental growth, making it a very strong candidate for utilization in temporal data bases.
(b) Critical applications: In any critical application temporal queries on multiple arcs have to be answered in real-time.
Most of the temporal reasoning scheme of present day are not suitable for handling query involving multiple temporal relations in real time.
Having all CSM's available in data base makes this feasible.
(c) Prediction of possible scenarios: Having all CSM's available again makes one capable of nding out a suitable model for the purpose of prediction, such that the chosen model is consistent with the currently supplied partial model.
(d) Explanation: Similar to prediction, developing scenarios involving past temporal assertions can act as explanations for some phenomena which are occurring at present or which have occurred in the past.
(e) All feasible plans: A special case of predictive capability of temporal reasoning system is that of generating all feasible plans7].
(f) Plan recognition: A special advantage of having a data base of all feasible plans available, is to recognize a plan based on executed operations up to a `present' time.
(g) Image recognition: If images are represented in terms of spatial relations (a two dimensional extension of temporal relations) between objects in the image (as number of possible models derived from incomplete information), then image recognition problem becomes a problem of search within models.
(h) Incorporating uncertainty in temporal reasoning: Some work has been done recently in studying propagation of fuzzy plausibility values of temporal constraints while running 3-consistency algorithm11].
In the temporal reasoning system for nding all CSM's, presented in this article, such uncertainty propagation will result in an ordering on all possible models, and possibly eliminating some weak models, thus gaining further pruning power for the sake of eciency, paying for the overhead incurred in uncertainty propagation.
2 Consistent Singleton Modeling Algorithm The algorithm described here for nding all consistent temporal models is based on forward pruning technique which we have developed sometime back8].
The formalism used is based on constraint propagation technique, which was rst developed for interval based-temporal reasoning by Allen1].
In this formalism propositional temporal entities (on time-intervals) are represented as nodes in a graph, and relations between them are represented as directed arcs between them.
Labels on each arc express possible temporal relations between two end nodes.
There are 13 primitive temporal relations feasible between time intervals1].
Disjunction of them forms an incomplete information between two temporal entities.
Three types of such labels are of special interest.
(1) Disjunction of all 13 primitive relations signies lack of any information (or constraint) between two nodes.
(2) A null relation as any label in the network signies contradiction of constraints, or an inconsistent network.
(3) Single primitive relation on each of the labels signify a singleton model of the temporal data base.
Our algorithm adds each node one by one to the data base of temporal intervals.
Thus the n-th node is added to the database of (n -1) nodes.
The data base of (n -1) nodes consists of a set of consistent singleton models (CSM).
New constraints from nth node to all other previously added nodes are used to nd n-node consistent singleton models.
This is done for all CSM's available in the old data base.
New data base consists of n-node CSM's.
At each stage an approximate algorithm is run rst as preprocessor to prune primitive relations from labels which do not have any possibility of forming a CSM.
After running the preprocessor, the forward pruning (FP) algorithm is run.
FPalgorithm systematically picks up singletons (primitive relations) from the next new arc from an ordered set of new arcs, and updates labels on all remaining new arcs by standard temporal interval{ constraint updating technique2].
If in any updating stage a null relation is generated it is considered to be a contradiction, and the algorithm backtracks over, rst the primitive relations on current arc (we call it sidetrack), and if it is exhausted, then over the previous arc in the ordered set (of new arcs).
If backtracking exhausts all primitive relations from label on the rst picked up arc, then the present  old-CSM can not nd a progeny.
A new n-node CSM is found when a primitive relation is picked up nal updated label of on the last arc of the set.
A new CSM is added for each primitive relation on the last arc to the new data base.
This process is repeated for all old CSM's.
If no model is found for all old CSM's then the new given constraints on n-th node is not consistent with respect to old data base.
Using this dynamic programming technique, the algorithm improves upon the unsystematic approach of backtracking which have been used for complete interval-based reasoning algorithms developed before.
The cost of this improved time eciency is in the additional space requirement of storing list of singletons at each stage.
The data structure design for the implementation becomes quite important in achieving space and time eciency.
The algorithm is already implemented in its preliminary form.
3 Theoretical implications of the algorithm In this section some theoretical signicance of having this algorithm for completely solving the problem of interval-based temporal constraint propagation is discussed.
(a) Empirical studies of temporal constraint satisfaction problem (TCSP): Recently a new approach of empirically studying NP-complete problems3, 5] is taking shape.
Apart from gaining insight into the structure of the problem, such empirical studies also help in developing more efcient algorithms.
Our algorithm should provide an ecient means to empirically study the temporal constraint propagation problem, which also happened to be NP-complete problem.
This algorithm, being more ecient and powerful (in providing all models), should yield better insight into the problem, with respect to some identied problem structures like average number of constrained arcs, average number of primitive relations on those arcs, or higher moments of their distributions10].
(b) Average number of models: Since our algorithm is particularly suitable for generating all CSM's, it gives an opportunity to study average number of models with respect to dierent problem structures.
Such studies could be very valuable in acquiring domain-specic information.
For  Theoretical and practical implications of an algorithm for nding all consistent temporal models Debasis Mitra dmitra@ccaix.jsums.edu Department of Computer Science 1400 Lynch St, P.O.
Box 18839 Jackson State University, Jackson MS 39217  Abstract In this article we have discussed an algorithm for nding all consistent temporal models (with interval-based representation) in an incremental fashion, and signicance of having this algorithm.
Reasoning with time in interval-based representation is NP-complete problem.
This has deterred researchers from studying the signicance of having a data base of all consistent models.
Advantage of having such models is multi-fold, as we have tried to show here.
Our algorithm is ecient enough so that obtaining all consistent models become practicable.
This makes such study necessary.
Signicance elaborated here spreads over both theoretical as well as practical aspects.
1 Introduction Reasoning with time is an important part of most cognitive activities.
Detecting consistency for a given set of temporal constraints is an NP-complete problem, when temporal entities are considered as intervals in time.
So far there exist only four algorithms for completely detecting temporal consistency.
First one, based on dependency-directed backtracking was proposed by Valdes-Perez12].
It was not complete, and there is no reported implementation of it.
Second one, forwarded by Peter van Beek13] is based on rst isolating a polynomially solvable (pointizable) subset of the problem, next solving it using polynomial algorithm, then converting the result back to original domain, and on failure, backtracking to nd dierent subset of the original problem.
Third algorithm, based on an unsystematic approach of backtrack, was developed and implemented by Ladkin et al4].
Experimentation with their algorithm has shown that, on an average, the problem is not a very hard one.
Almost at the same time, we have developed another algorithm which not only detects consistency, but also nds all possible consistent (temporal) models.
Ladkin et al's algorithm nds one consistent model as a side eect.
Although, if sucient time is given, their depthrst search algorithm also can be extended to nd all consistent models, our dynamic programmingbased algorithm is particularly geared towards efciently nding all consistent models.
Its forwardpruning heuristic prunes the search space in each stage, thus gaining eciency on average case.
Another aspect of our algorithm is that it adds each temporal entity individually, thus developing the data base of the temporal entities incrementally.
Other algorithms are not particularly geared towards such incremental development of data base.
Inferred Validity of Transaction-Time Data Cristina De Castro  Maria Rita Scalas  C.I.O.C.-C.N.R.
and Dipartimento di Elettronica, Informatica e Sistemistica Universita di Bologna Viale Risorgimento 2, I-40136 Bologna, Italy Tel.
+ 39 (51) 644.
{3542,3544} Fax: +39 (51) 644.3540 E-mail: {cdecastro,mrscalas}@deis.unibo.it Abstract: Temporal databases can support at least two kinds of independent time dimensions: transaction-time, which tells when an event is recorded in a database, and valid-time, which tells when an event occurs, occurred or is expected to occur in the real world.
According to the semantics of transaction-time, when data are retrieved from a transaction-time relation, the only temporal information that can be associated to such data is the time when they were stored, updated or deleted.
No conjecture on their validity in the real world is straightforward.
Nevertheless, when transaction-time data are used, a validity is implicitly assigned to them.
In this paper we propose three possible interpretations of the validity of transaction-time data.
Such proposals can be useful in a temporal heterogeneous environment, where relations of different temporal format must interoperate: as a matter of fact, the proposed solutions allow a conversion of temporal data to the bitemporal format, and thus they provide a common format for the execution of the operations.
All the three proposed solutions assume that data can be considered valid at least since they were stored.
Such semantics is intrinsic of transaction-time DBs, which are accordingly named historical, thus we can say that, when data validity is not specified, the knowledge actually stored in the database is taken into account.
The second assumption is that any conjecture on the  effective beginning of the validity of data preceding their insertion time would create false information and must thus be avoided.
These criteria fix the beginning of the inferred validity.
The end of validity can be defined in three distinct ways, depending on how much it can span along the valid-time axis.
A comparison among the three types of inference is provided at the end, on the basis of the possible use of such data, because the inferred validity can be used for the translation of data both to the valid-time or the bitemporal format.
The differences among the three inferences look remarkable.
Keywords: Transaction-Time, Inferred Valid-Time Extension  Valid-Time,  1.
Introduction and Notation In current bibliography on temporal databases there is a common agreement for the support of at least two kinds of time dimensions: transactiontime, which tells when an event is recorded in a database, and valid-time, which tells when an event occurs, occurred or is expected to occur in the real world [Soo91,TSC+93].
Further proposals include the temporal dimension event-time [KC93], which allows the distinction between retroactive and proactive updates, impossible with transaction and valid-time only.
In this paper we are concerned with the possible deductions that can be made when only transaction-time is  supported and thus, for the beginning, we do not take into account event time.
If we consider transaction- and valid-time, according to the temporal dimensions they support, temporal databases can be classified as monotemporal (transaction- or valid-time) , bitemporal or snapshot [JCE+94].
Transaction-time DBs record all the versions of data inserted, deleted or updated in successive transactions (current and non current versions).
The temporal information of a transaction-time relation concerns the time when data are recorded, updated or deleted from the database.
In this sense, transaction-time is the database time.
Valid-time DBs maintain the most recently inserted versions of data, each relative to a distinct valid-time interval (current versions only).
The temporal information of a valid-time relation concerns the time when events actually happen in the real world.
In this sense, valid-time is the time of the miniworld to represent.
Bitemporal DBs support both transaction and valid-time and thus maintain all the valid-time versions recorded in successive transactions (current and non current versions).
Snapshot DBs do not support time: they maintain only the most recently inserted (current) version.
When data are retrieved from a database where their validity is not represented, the user implicitly assigns them a validity interval.
For instance, when you get the latest version of the telephone book, you can use the telephone numbers and thus you consider them valid.
This process is, of course, risky, but reflects commonly used deductions made on every type of available data.
In this paper we consider a transaction-time database and propose three distinct ways for inferring data validity from their represented transaction-time.
Even if transaction- and validtime are independent (orthogonal) dimensions, the above considerations justify this type of inference.
Not only the proposed methods give criteria for making inferences on the validity of data, but also they allow the conversion of data to the bitemporal format.
This use is necessary in a temporal heterogeneous environment, where the interoperability is required of relations of different temporal format [DGS94,DGS95].
The temporal representation adopted in this paper is the Bitemporal Conceptual Data Model (BCDM in [JCE+94]), where time-stamps are represented by Temporal Elements.
In the following, we provide a concise description of the BCDM formalism and of the adopted notations.
Time is represented by means of temporal elements, which consist of sets of chronons.
As in [JCE+94] a chronon is a non-decomposable time interval of some fixed, minimal duration.
A duration p represents the chosen granularity of time.
A (bi)temporal chronon is an ordered pair of unitary-length chronons, one relative to transaction-time, the other to valid-time: for instance (ti, tj) is a bitemporal chronon, where ti denotes transaction-time, and tj denotes validtime.
Bitemporal elements are sets of bitemporal chronons of the type tb = {(ti, tj), ... , (tl, tm)}.
In general, the symbol tX denotes a transaction-time (tt), valid-time (tv) or bitemporal (tb) element.
A monotemporal element can always be represented by the union of disjoint component subsets, each represented by its endpoints (IN, OUT for transaction-time, FROM, TO for validtime) and containing contiguous chronons only: every chronon tj, such that tmi <= tj <= tni, belongs to the component subset {tmi .. tni}.
For instance, the general representation of a transaction-time temporal element is tt = [?
]i tti = [?
]i {tmi .. tni}.
The paper is organized as follows: in section 2 we present the three inferences, discuss the criteria on which they are based and provide some examples.
In section 3 we carry on the discussion and focus the attention on the different results obtained when the data produced by each inference are selected at distinct transaction-time instants.
2.
Three Inferences on the validity of transaction-time data In this section we present the three different inferences, named Square Inference, Stripe Inference and L-Shaped Inference respectively, and discuss the criteria on which each of them is based.
A comparison among the three is carried on.
The common assumption concerns the  beginning of validity of data: transaction-time (historical) data can be considered valid from the instant they were recorded.
Probably such validity precedes this instant, but, in the absence of further information, it would be absolutely unsafe and arbitrary to make further conjectures.
The difference among the three solutions is in the extent data validity is allowed to span along the valid-time axis.
The following notation is adopted: if the non-temporal attributes are denoted by r and [?]
denotes the operation of tuple concatenation, a version of an object along the temporal dimension X can be expressed as rX = r [?]
(tX).
*  Transaction-time semi-axis: {T0 ..
T[?]}
*  Valid-time semi-axis: {t0 ..
t[?]}
*  Current transaction-time: Tnow  *  If rt is a transaction-time record, rb' , rb'' and rb''' will denote the results in the bitemporal format obtained by using the first, second and third inference made on the validity of rt.
The transaction-time relation T-Employee in Tab.1 will be used in the examples.
For the sake of simplicity, in the examples the transaction-time temporal elements have a single component.
The granularity of time chosen for the examples is one year, thus, for instance, {90 .. 92} starts at the beginning of 1990 and finishes at the end of 1992.
2.1.
Square Inference The criterion on which the Square Inference is based is that when data are retrieved from a transaction-time relation they can be considered valid no less and no more than in their transaction-time interval.
This type of inference is represented in Fig.1.
The Square-inferred valid-time pertinence equals the transaction-time temporal element of each record: the transaction-time record r [?]
(tt) is thus transformed to the bitemporal format as follows:  r [?]
(tt) - r [?]
(tt) [?]
(tv) inferred validity is:  where the tv [?
]def tt  If tt is the union of disjoint intervals tt = [?
]i tti, the above definition must be applied to each component transaction-time temporal element tti: r [?]
{[?
]i tti} - r [?]
( [?
]i tti) [?]
( [?
]i tvi ) where tvi [?
]def tti 2.2.
Stripe Inference The criterion on which the Stripe-Inference is based is that when data are retrieved from a transaction-time relation they can be considered valid since they were stored and indefinitely valid in their transaction-time interval.
The Stripe inference reconstructs the bitemporal pertinence of each record taking into account that every record was current before it was updated.
Therefore, when a record had not been archived yet, it could be considered undefinitely valid.
This type of inference is represented in Fig.2.
The Stripe-inferred valid-time pertinence spans the whole valid-time axis starting from the minimum chronon of the transaction-time temporal element of each record: the transactiontime record r[?
](tt) is thus transformed as follows: where r [?]
(tt) - r [?]
(tt) [?]
(tv) def tv [?]
{min {tt} ..
t[?]}
Again, if tt = [?
]i tti , the above definition must be applied to eachtti: r [?]
( [?
]i tti) - r [?]
( [?
]i tti) [?]
( [?
]i tvi) tvi [?
]def {min {tti} ..
t[?]}
where  2.3.
L-Shaped inference A transaction-time tuple r [?]
(tt) is said to be current if Tnow [?
]tt (in this case OUT = T[?
]); it is said to be archived if Tnow >= min{tt}+1 (in this case OUT < T[?]).
A current tuple r [?]
({IN ..
T[?]})
(e.g.
r3 in Fig.3) can be considered valid since it  NAME  JOB  SALARY  tt  Ann  Engineer  2800  {85 .. 90}  Ann  Manager  3000  {91 ..
T[?]}
John  Engineer  1500  {90 .. 92}  John  Engineer  2000  {93 ..
T[?]}
Table 1: transaction-time relation T-Employee Valid-Time Axis T0 r0  T[?]
t2 = T2 t3 = T3  r 0 'b  T1  T1 r1  T2 T3  t1= T1  T0  T2 r2  r 1 'b  T3  r3  r 2 'b r 3 'b  T[?]
T[?]
Transaction-Time Axis a)  b)  Figure 1: (a) transaction-time pertinence; (b) corresponding Square-inferred validity  NAME  JOB  SALARY  tb = tt x tv  Ann  Engineer  2800  {85 .. 90} x {85 .. 90}  Ann  Manager  3000  {91 ..
T[?]}
x {91 ..
t[?]}
John  Engineer  1500  {90 .. 92} x {90 .. 92}  John  Engineer  2000  {93 ..
T[?]}
x {93 ..
t[?]}
Table 2: Square-inferred validity of T-Employee  Valid-Time Axis T0  T0  t1 = T1  r0  r 0 ''  T1  b  T1 r1  r 1 ''  T2  b  T2 r2  T3  T[?]
t2 = T2 t3 = T3  r 2 ''  T3  b  r3  r 3 '' b  T[?]
T[?]
Transaction-Time Axis a)  b)  Figure 2: (a) transaction-time pertinence; (b) corresponding Stripe-inferred validity  NAME  JOB  SALARY  tt x tv  Ann  Engineer  2800  {85 .. 90} x {85 ..
t[?]}
Ann  Manager  3000  {91 ..
T[?]}
x {91 ..
t[?]}
John  Engineer  1500  {90 .. 92} x {90 ..
t[?]}
John  Engineer  2000  {93 ..
T[?]}
x {93 ..
t[?]}
Table 3: Stripe-inferred validity of T-Employee  T0  T0 r0  T1  r 0 ''' b T1  r1 T2 T3  t1 = T1  Valid-Time Axis t2 = T2 t3 = T3 T[?]
T2 r2  T3  r3 T[?]
Transaction-Time Axis a)  r 1 ''' b r 2 ''' b r 3 ''' b  T[?]
b)  Figure 3: (a) transaction-time pertinence; (b) corresponding L-shaped inferred validity  Valid-Time Axis T0  t1= T1  T0 r0  T [?]
t2 = T2 t3 = T3  r 0 'b  T1  T1  T = T* r1 T2 T3  r 1 'b  T2 r2  r3 T = Tnow T [?]
Transaction-Time Axis  r 3 'b T [?]
T0  T0  r 2 'b  T3  t1 = T1  r0  r 0 ''b  T1  T1  T [?]
t2 = T2 t3 = T3  T = T* r 1 '' b  r1 T2  T2 T3  r2  r 2 ''b  T3  r 3 ''b  r3 T = Tnow T [?]
T [?]
T0  T0 r0  T1  t1 = T1  T[?]
t2 = T2 t3 = T3  r 0 ''' b T1  T = T* r1 T2 T3  T2 r2  T3  r 1 ''' b r 2 ''' b  r3 T = Tnow T[?]
r 3 ''' b T[?]
Figure 4: projection along the valid-time axis of the inferred validity at T = T* < T[?]
and at T = Tnow  was stored and indefinitely valid, since it cannot be forecasted if an update transaction would ever occur and archive such tuple.
An archived tuple r [?]
({IN .. OUT}) can be considered as created by a transaction with effect in [IN ..
T[?])
x [IN ..
T[?])
and modified by a successive transaction with effect in [OUT ..
T[?])
x [OUT ..
T[?]).
The update transaction cuts the initial time pertinence of the tuple to an ``L-shaped'' region (e.g.
r0b''', r1b''', r2b''' in Fig.3 and their corresponding ones).
This inference criterion was proposed in [DGS93] and fits the bitemporal view of a diagonal user (for the concept of user see [BG93]).
As in the Stripe-inferred case, the L-shaped inferred valid-time pertinence spans the whole valid-time axis starting from the minimum chronon of the transaction-time temporal element of each record; The difference is that this second criterion takes into account that, before a record was updated it were unknown if it would have been updated and when.
As a consequence, before the update, not only could the valid-time interval be interpreted as covering the whole valid-time axis from the time IN, but the same hold for the transaction-time pertinence of the record itself (OUT = T[?]
before the update).
The transactiontime record r [?]
(tt) is transformed as follows: r [?]
(tt) - r [?]
(tb), where tb [?
]def {min{tt} ..
T[?]}
x {min{tt} ..
t[?]}
{max{tt}+1 ..
T[?
]}x {max{tt}+1 ..
t[?]}
When r [?]
(tt) is current, {max{tt}+1 ..
T[?
]}x {max{tt}+1 ..
t[?]}
= [?]
Again, if tt = [?
]i tti , the above definition must be applied to each tti: r [?]
( [?
]i tti) - r [?]
( [?
]i tbi) where tbi [?
]def {min{tti} ..
T[?
]}x {min{tti} ..
t[?]}
- {max{tti}+1 ..
T[?
]}x {max{tti}+1 ..
t[?]}
3.
Further Discussion and Conclusions  In this paper we have considered how data validity can be inferred in transaction-time databases.
We proposed three distinct solutions: the Square Inference, the Stripe Inference and the L-Shaped Inference.
The Square Inference reflects only the knowledge of the database: data are valid within their transaction-time interval.
The StripeInference takes into account that archived data were current before they were updated and, before the update, data could be considered indefinitely valid.
A further deduction is made in the L-shaped solution: a portion of data is considered as still current.
As far as the use of the inferred data is concerned, a distinction must be made between their use in the bitemporal or in the valid-time format.
It can be noticed that the Square Inference can be used not only for the bitemporal view of data, but also for the valid-time view, since the Square-inferred valid-time intervals do not overlap; on the contrary, the other two solutions can be used only in the bitemporal format, since the Stripe- or Lshaped- inferred valid-time intervals overlap on the valid-time axis.
Furthermore, if we consider the bitemporal representations in Figs.1, 2, 3 and project (see Fig.4) the valid-time data inferred in each solution at T = T* < T[?]
and at T = Tnow, we find out another important difference.
At the generic time T = T* < T[?
], the Square Inference and the Stripe Inference return only the data whose original transaction-time pertinence contains T*, i.e.
they return the information which was available in the considered transaction-time interval; the L-shaped inference returns all the data whose transactiontime pertinence contains or precedes T*, i.e.
they return all the information which was available at time T*, independently of the transaction-time pertinence of such data.
If the projection is performed at the current time Tnow, the Square inference and the Stripe inference return only the current data, whereas the L-shaped inference returns all the original data, both current and archived.
These considerations can guide the use of the data produced in each type of inference.
The Square method is most in  harmony with the semantics of both transactionand valid-time, thus it allows to use the inferred data for the translation to both the bitemporal- and the valid-time format in quite a safe way, just being aware that the validity were not explicitly defined.
Also the Stripe method returns, for each transaction-time instant, only the data which were current at that time.
In this sense, it is in harmony with the semantics of transaction-time.
On the other hand, the inferred data can not be used with no transaction-time reference, because they overlap along valid-time.
The L-shaped inference, if used with no transaction-time reference, is the most risky of the three.
Its use can be justified by the fact that transaction-time can only grow, thus no retroactive insertion is possible.
As a consequence, the original transaction-time data are, in each interval, the only one version which was ever recorded and thus, in this sense, the most recently inserted one.
References: [BG93] Bhargava G., Gadia S.K., ``Relational Database Systems with Zero Information Loss'', IEEE Trans.
on Knowledge and Data Engineering, Vol.
5, No.
1, Feb. 1993.
[DGS94] De Castro C., Grandi F., Scalas M.R.
: ``Semantic Interoperability of Multitemporal Relational Databases'', in Entity-Relationship Approach - ER '93, Lecture Notes in Computer Science, Vol.
823, Springer-Verlag, 1994.
[DGS94] De Castro C., Grandi F., Scalas M.R.
: ``Meaning of Relational Operations in Temporal Environment'', accepted for presentation at the Basque International workshop on Information Technology (BIWIT 95), to be held in San Sebastian (Spain), July 1995.
[JCE+94] Jensen C., Clifford J., Elmasri R., Gadia S.K., Hayes P., Jajodia S. (editors), Dyreson C., Grandi F., Kafer W., Kline N., Lorentzos N., Mitsopoulos Y., Montanari A., Nonen D., Peressi E., Pernici B., Roddick J.F., Sarda N.L., Scalas M.R., Segev A., Snodgrass R., Soo M.D., Tansel A., Tiberio P., Wiederhold G.: ``A Consensus Glossary of Temporal Database Concepts", SIGMOD RECORD, Vol.
23, No.
1, March 1994 [KC93] Kim S.K., Chakravarthy S.: ``Modeling Time: Adequacy of Three Distinct Time Concepts for Temporal Databases", Proc.
of 12th International Conference on  Entity-Relationship Approach, Arlington, December 1993, also in Lecture Notes in Computer Science, Springer-Verlag.
[Soo91] Soo M., ``Bibliography on Temporal Databases,'' ACM SIGMOD Record, Vol.
20, No.
1, Mar.
1991.
[TSC+93] Tansel A., Snodgrass R., Clifford J., Gadia V., Segev A.
(eds), Temporal Databases: Theory, Design and Implementation, The Benjamin/Cummings Publishing Company, Redwood city, California, 1993.
Using Temporal Logic to Control Search in a Forward Chaining Planner Fahiem Bacchus Dept.
Of Computer Science University Of Waterloo Waterloo, Ontario Canada, N2L 3G1 fbacchus@logo.uwaterloo.ca Abstract: Over the years increasingly sophisticated planning algorithms have been developed.
These have made for more efficient planners, but unfortunately these planners still suffer from combinatorial explosion.
Indeed, recent theoretical results demonstrate that such an explosion is inevitable.
It has long been acknowledged that domain independent planners need domain dependent information to help them plan effectively.
In this work we describe how natural domain information, of a "strategic" nature, can be expressed in a temporal logic, and then utilized to effectively control a forwardchaining planner.
There are numerous advantages to our approach, including a declarative semantics for the search control knowledge; a high degree of modularity (the more search control knowledge utilized the more efficient search becomes); and an independence of this knowledge from the details of the planning algorithm.
We have implemented our ideas in the TLP LAN system, and have been able to demonstrate its remarkable effectiveness in a wide range of planning domains.
1  Introduction  Planners generally employ search to find plans, and planning research has identified a number of different spaces in which search can be performed.
Of these, three of the most common are (1) the forward-chaining search space, (2) the backwardchaining search space, and (3) the space of partially ordered plans.
The forward-chaining space is generated by applying all applicable actions to every state starting with the initial state; the backward-chaining space by regressing the goal conditions back through actions that achieve at least one of the subgoals; and the space of partially ordered plans by applying a collection of plan modification operators to an initial "dummy" plan.
Planners that explore the backward-chaining space or the space of partially ordered plans have an advantage over those that explore the forward-chaining space in that the latter spaces are generated in a "goal directed" manner.
Hence, such This research was supported by the Canadian Government through their IRIS project and NSERC programs.
Fahiem Bacchus is currently on sabbatical leave from the University of Waterloo, Canada.
Froduald Kabanza Dept.
De Math Et Informatique Universite De Sherbrooke Sherbrooke, Quebec Canada, J1K 2R1 kabanza@dmi.usherb.ca planners are intrinsically goal directed: they need never consider actions that are not syntactically relevant to the goal because the spaces they explore do not include such actions.
Partial-order planners have an additional advantage over simple backward chaining planners in that the objects in their search space are partially ordered plans.
This allows these planners to delay ordering actions until they detect an interaction between them.
Linear backward or forward-chaining planners, on the other hand, might be forced into backtracking because they have prematurely committed to an ordering between the actions.
However, both backward-chaining and partial-order planners search in spaces in which knowledge of the state of the world is far less complete than in the forward-chaining space.
For example, even if a backward-chaining planner starts with a completely described initial world and actions that preserve the completeness of this description, it will still have only incomplete knowledge of the world state at the various points of its search space.
Partial order planners also suffer from this problem.
The points of their search space are incomplete partially ordered plans, and at the various stages of an incomplete plan we have only limited knowledge of the state of the world.
On the other hand, the points in the forward-chaining space are world descriptions.
Such descriptions provide a lot of information about the world state, even if the description is incomplete.
As we will demonstrate in this paper, such knowledge can be effectively utilized to control search in this space.
The choice between the various search spaces has been the subject of much recent inquiry [BW94; MDBP92], with current consensus seemingly converging on the space of partially ordered plans,1 mainly because of its goal-directness and least commitment attitude towards action ordering.
However, these studies have only investigated simple heuristic search over these spaces, where domain independent heuristics, like counting the number of unsatisfied sub-goals, are utilized.
Domain independent heuristics cannot take advantage of structural features that might be present in a particular domain.
Theoretical work [ENS92; Sel94] indicates that for the traditional S TRIPS actions used by almost all current planners, 1  Although, see [VB94] for an refreshing counterpoint.
finding a plan is, in general, intractable.
This means that no domain independent planning algorithm can succeed except in very simple (and probably artificial) domains.
More importantly, however, is that there may be many domains where it is feasible to find plans, but where domain structure must be exploited to do so.
This can be verified empirically; e.g., the partial order planner implemented by Soderland et al.
[SBW90] cannot effectively generate plans for reconfiguring more than 5 blocks in the blocks world using domain independent heuristic search.
Nevertheless, the blocks world does have sufficient structure to make it easy to generate good plans in this domain [GN92].
One way of exploiting domain structure during planning is to use domain information to control search.
Hence, a more practical evaluation of the relative merit of various planning algorithms and search spaces would also take into account how easy it is to exploit domain knowledge to control search in that space.
The idea of search control is not new, e.g., it is a prominent part of the P RODIGY planing system [CBE+ 92].
Our work, however, makes a number of new contributions to the notion of search control.
In particular, we demonstrate how search control information can be expressed in a first-order temporal logic, and we develop a method for utilizing this information to control search during planning.
By using a logic we gain the advantage of providing a formal semantics for the search control information.
Furthermore, we would claim that this semantics is quite natural and intuitive.
This differentiates our mechanism for search control from classical state-based heuristics and from the control rules employed by the P RODIGY system.
P RODIGY control rules are implemented as a rule-based system.
Various rules are activated dependent on the properties of the node in the search space that is currently being expanded.
These rules are activated in a particular order and have various effects.
This means that any attempt to give a semantics to these rules would require an operational semantics that makes reference to way in which the rules are utilized.
By using the forward-chaining search space we end up searching in the space of world descriptions.
This allows us to utilize search control knowledge that only makes reference to the properties of these worlds, i.e., to properties of the domain.
Thus the search control knowledge used can be considered to be no different from the description of the domain actions: it is part of our knowledge of the dynamics of the domain.
In contrast, P RODIGY control rules include things like binding selection rules that guide the planner in deciding how to instantiate actions.
Such rules have to do with particular operations of the P RODIGY planning algorithm, and to compose such rules the user must not only possess domain knowledge but also knowledge of the planning algorithm.
Finally, the language in which we express search control knowledge is richer than previous approaches.
Hence, it can capture much more complex control knowledge.
In particular, the control strategies are not restricted to considering only the current state, as are P RODIGY control rules and statebased heuristics.
They can, e.g., consider past states and pass  information forward into future states.
All of these features make the control information employed by our system not only easier to express and understand, but also more effective, sometimes amazingly effective, as we will demonstrate in Section 4.
Using the forward-chaining search space is not, of course, a panacea.
It does, however, seem to better support effective search control.
We have already mentioned two reasons for this: we have access to more information about the world state in this space, and it allows us to express search control information that is independent of the planning algorithm.
We have also found a third advantage during our experiments.
In many domains, humans seem to possess strategies for achieving various kinds of goals.
Such strategies seem to be most often expressed in a "forward-direction".
This makes their use in controlling forward-chaining search straightforward, but exploiting them in the other search spaces not always so.
Due to its support of effective search control, we have found that forward-chaining can in many domains yield planners that are more effective than those based on partial order planning or backwards-chaining regression.
The forward-chaining search space still suffers from the problem that it is not goal directed, and in many of our test domains we have found that some of the search control information we added was designed to recapture goal-directedness.
Much of this kind of search control knowledge can be automatically inferred from the operator descriptions, using ideas like those of [Etz93].
Inferring and learning search control in the form we utilize is an area of research we are currently pursuing.
One of the key advantages of using a logic to express search control knowledge is that it opens the door to reasoning with this knowledge to, e.g., generate further control knowledge or to verify and prove properties of the search control knowledge.
But again this avenue is a topic for future research.
In the rest of the paper we will first describe the temporal logic we use to express domain strategies.
We then describe how knowledge expressed in this language can be used to control forward chaining search.
We have implemented our approach in a system we call TLP LAN, and we describe some of our empirical results with this system next.
We close with a summary of our contributions and a description of some of the extensions to our approach we are currently working on.
2 First-order Linear Temporal Logic We use as our language for expressing strategic knowledge a first-order version of linear temporal logic (LTL) [Eme90].
The language starts with a standard first-order language, L, containing some collection of constant, function, and predicate symbols.
LTL adds to L the following temporal modalities: U (until), 2 (always), 3 (eventually), and (next).
The standard formula formation rules for first-order logic are augmented by the following rules: if f 1 and f2 are formulas then so are f1 U f2 , 2f1 , 3f1 , and f1 .
Note that the first-order and temporal formula formation rules can be applied in any order, so, e.g., quantifiers can scope temporal modalities al-  lowing quantifying into modal contexts.
Our planner works with standard S TRIPS operators and world descriptions, and it takes advantage of the fact that these world descriptions support the efficient testing of various conditions.
In particular, worlds described as lists of positive literals support the efficient evaluation of complex first-order formulas via model-checking [HV91].
Hence, we can express complex conditions as first-order formulas and evaluate their truth in the worlds generated by forward-chaining.
Part of our TLP LAN implementation is a first-order formula evaluator, and TLP LAN allows the user to define predicates by firstorder formulas.
These predicates can in turn be used in temporal control formulas, where they act to detect various conditions in the sequence of worlds explored by the planner.
To ensure that it is computationally effective to evaluate these first-order formulas and at the same time not limit ourselves to finite domains (e.g., we may want to use the integers in our domain axiomatization), we use bounded instead of standard quantification.
In particular, instead of the quantifiers 8x or 9x, we have 8 x: ] and 9 x: ] , where  is an atomic formula2 whose free variables include x.
It is easiest to think about bounded quantifiers semantically: 8 x: ]  for some formula  holds iff  is true for all x such that  (x) holds, and 9 x: ]  holds iff  is true for some x such that  (x) holds.
Computational effectiveness is attained by requiring that in any world the set of satisfying instances of  be finite.
3 The formulas of LTL are interpreted over models of the form M = hs0 fi s1 fi : : :i, i.e., a sequence of states.
Every state si is a model (a first-order interpretation) for the base language L. In addition to the standard rules for the first-order connectives and quantifiers, we have that for a state si in a model M and formulas f1 and f2 :  fi hMfi sii j= f1 U f2 iff there exists j  i such that hMfi sj i j= f2 and for all k, i  k < j we have hMfi sk i j= f1 : f1 is true until f 2 is achieved.
fi hMfi sii j= state.
f1 iff hMfi si+1 i j f1 : f1 is true in the next =  fi hMfi sii j= 3f1 iff there exists j  i such that hMfi sj i j= f1 : f1 is eventually true.
fi hMfi sii j= 2f1 iff for all j  i we have hMfi sj i j= f1 : f1 is always true.
Finally, we say that the model M satisfies a formula f if hMfi s0i j= f .
First-order LTL allows us to express various claims about the sequence of states M. For example, on(Afi B ) asserts that in state s2 we have that A is on B .
Similarly, 2  We also allow to be an atomic formula within the scope of a (described below).
That is, instead of allowing x to range over the entire domain, we only allow it to range over the elements satisfying .
Thus, the underlying domain may be infinite, but any particular quantification over it is finite.
Also we allow formulas of the form 9 x: ] where  is implicitly taken to be TRUE.
GOAL modality 3  2:holding C asserts that we are never in a state where we are holding C , and 2 on Bfi C ) on Bfi C U on Afi B asserts that whenever we enter a state in which B is on C it remains on C until A is on B , i.e., on Bfi C is preserved until we achieve on Afi B .
Quantification allows even greater expressiveness, e.g., 8 x clear x clear x asserts that ev(  )  (  (  )  (  (  (  (  )  (  )))  )  )  :  (  )]  ( )  ery object that is clear in the current state remains clear in the next state.
We are going to use LTL formulas to express search control information (domain strategies).
Search control generally needs to take into account properties of the goal, and we have found a need to make reference to requirements of the goal in our LTL formulas.
To accomplish this we augment the base language L with a goal modality.
In particular, to the base language L we add the following formula formation rule: if f is a formula of L then so is GOAL(f ).
If the agent's goal is expressed by the first order formula , then semantically this modality is a modality of entailment from  and any state constraints.
That is, GOAL (f ) is true iff  ^  j= f , where  are the set of conditions true in every state, i.e., the set of state constraints.
However, testing if GOAL (f ) is true given an arbitrary goal formula  is intractable (in general, it requires theorem proving).
Our implemented planning system TLP LAN allows the goal modality to be used only when goals are sets of positive literals, and it computes goal formulas by assuming that these literals describe a "goal world" using a closed world assumption.
For example, if the goal is the set of literals fon(Afi B ), on(Bfi C )g then these are assumed to be the only positive literals that are true in the goal world, every other atomic formula is false by the closed world assumption.
For example clear(A) is false in the goal world.
Intuitively, it is not a necessary requirement of the goal.
TLP LAN can then use its firstorder formula evaluator over this goal world, so the formula GOAL (9 y:on(Afi y )] ) will also evaluate to true.
If, however, we had as our goal fP (A)g and the state constraint that in all states P (A) ) Q(A), then, in its current implementation, TLP LAN will incorrectly (according to the above semantics) conclude that GOAL (Q(A)) is false.
That is, TLP LAN cannot currently handle state constraints over the goal world.
3 Expressing Search Control Information Any LTL formula specifies a property of a sequence of states: it is satisfied by some sequences and falsified by others.
In planning we are dealing with sequences of executable actions, but to each such sequence there corresponds a sequence of worlds: the worlds we pass through as we execute the actions.
These worlds act as models for the language L. Hence, we can check the truth of an LTL formula given a plan, by checking its truth in the sequence of world visited by that plan using standard model checking techniques developed in the program verification area (see, e.g., [CG87]).4 Hence, if 4 LTL formulas  actually require an infinite sequence of worlds as their model.
In the context of standard planning languages, where a plan consists of a finite sequence of actions, we can terminate every finite sequence of actions with an infinitely replicated "do nothing"  we have a domain strategy for the goal fon(Bfi A)fi on(Cfi B )g like "if we achieve on(Bfi A) then preserve it until on(Cfi B ) is achieved", we could express this information as the LTL formula 2(on(Bfi A) ) on(Bfi A) U on(Cfi B )) and check its truth against candidate plans, rejecting any plans that violate this condition.
What we need is an incremental way of checking our control strategies against the partial plans generated as we search for a correct plan.
If one of our partial plans violates our control strategy we can reject it, thus pruning all of its extensions from the search space.
We have developed a mechanism for doing incremental checking of an LTL formula.
The key to this method is the progression algorithm given in Table 1.
In the algorithm quantified formulas are progressed by progressing all of their instances.
This algorithm is characterized by the following theorem: Theorem 3.1 Let M = hs0 fi s1 fi : : :i be any LTL model.
Then, we have for any LTL formula f , hMfi si i j= f if and only if hMfi si+1i j= f + .
The progression algorithm admits the following implementation strategy, used in TLP LAN.
Every world generated during our search of the forward-chaining space is labeled with an LTL formula f , with the initial world being labeled with a user supplied LTL control formula that expresses a control strategy for this domain.
When we expand a world w we progress its formula f through w using the given algorithm, generating a new formula f + .
This new formula becomes the label of all of w's successor worlds (the worlds generated by applying all applicable actions to w).
If f progresses to FALSE , (i.e., f + is FALSE ), then Theorem 3.1 shows that none of the sequences of worlds emanating from w can satisfy our LTL formula.
Hence, we can mark w as a dead-end in the search space and prune all of its successors.
The complexity of evaluating Clause 2 of the progression algorithm depends on the form of the world descriptions.
It requires us to test an atemporal formula in the current world.
Hence, its complexity depends on two things, the complexity of the atemporal formula and the form of the world description.
If the worlds are incompletely described and represented simply as a first-order formula that characterizes some of its properties, then this clause will require theorem proving to evaluate.
If the world is described as a set of atomic formulas and a collection of horn clauses, and if the formula is quantifier free then evaluating the formula in the world might still be tractable.
The efficiency of this step is important however, as we must use this algorithm at every world expanded during plan search.
In our current implementation of TLP LAN we use worlds that are described as sets of positive literals, and we employ a closed world assumption: every positive literal not in this set is assumed to be false.
In this way we need make no restrictions on the atemporal formulas that can appear in our LTL control formula.
In particular, we can use arbitrary first-order action.
This corresponds to infinitely replicating the final world in the sequence of worlds visited by the plan.
Inputs: An LTL formula f and a world w (generated by forward-chaining).
Output: A new formula f + , also expressed as an LTL formula, representing the progression of f through the world w.  Algorithm Progress(f ,w) 1.
Case 2. f =  2 L (i.e.,  contains no temporal modalities): f + := TRUE if w j= ffi FALSE otherwise.
3 f = f1 ^ f2 : f + := Progress(f1 fi w) ^ Progress(f2 fi w) 4. f = :f1: f + := :Progress(f1 fi w) 5. f = f1 : f + := f1 6. f = f1 U f2 : f + := Progress(f2 fi w) _ (Progress(f1 fi w) ^ f ) 7. f = 3f1: f + := Progress(f1 fi w) _ f 8. f = 2f1 : f + := V Progress(f1 fi w) ^ f 9. f = 8 x: ] f1 : f + := fc:wj=(c)g Progress(f1 (x=c)fi w) W 10. f = 9 x: ] f1 : f + := fc:wj=(c)g Progress(f1 (x=c)fi w) Table 1: The progression algorithm.
formulas in our LTL control.
With worlds described by (assumed to be) complete sets of positive literals, we can employ model-checking instead of theorem proving to determine the truth of these formulas in any world, and thus evaluate clause 2 efficiently.
4 Empirical Results Blocks World.
Our first empirical results come from the blocks world, which we describe using the four operators given in table 2.
If we run our planner with the vacuous search control formula 2TRUE 5 , and exploring candidate plans in a depth-first manner, rejecting plans with state cycles, we obtain the performance given in Figure 1.
Each data point represents the average time required to solve 5 randomly generated blocks world problems (in CPU seconds on a SUN1000), where the initial state and the goal state were independently randomly generated.
The same problems were also run using S NLP, a partial order planner [MR91; SBW90], using domain independent heuristic search.
The graph demonstrates that both of these planners hit a computational wall at or before 6 blocks.6 S NLP failed to solve 4 of the six block problems posed; the times shown on the graph include the time taken by the runs that failed.7 5 This formula is satisfied by every sequence, and hence it provides no pruning of the search space.
6 We can note that with 5 blocks and a holding predicate there are only 866 different configurations of the blocks world.
This number jumps to 7057 when we have 6 blocks, and to 65990 when we have 7 blocks.
7 That is, S NLP exceeded the resource bounds we set (on the number of nodes in the search tree).
Note that by including these times in the data we are making S NLP's performance seem better than it really is: S NLP would have taken strictly more time to solve these problems than the numbers indicate.
The same comment applies to the tests described below.
Operator pickup (x) putdown (x) stack (xfi y) unstack (xfi y)  Preconditions and Deletes ontable(x), clear(x), handempty.
holding(x).
holding(x), clear(y).
on(xfi y), clear(x), handempty.
Adds holding(x).
ontable(x), clear(x), handempty.
on(xfi y), clear(x), handempty.
holding(x), clear(y).
Table 2: Blocks World operators.
400 SNLP TLPlan  Time (CPU sec.)
350 300 250 200 150 100 50 0 0  1  2  3  4  5  6  Number of Blocks  Figure 1: Performance of blind search in the blocks world This shows that domain independent heuristic search does not work well in this domain, even for the sophisticated S NLP algorithm.
Domain independent heuristics have difficult exploiting the special structure of blocks world.
Nevertheless, the blocks world does have a special structure that makes planning in this domain easy [GN92], and it is easy to come up with effective control strategies.
A basic one is that towers in the blocks world can be build from the bottom up.
That is, if we have built a good base we need never disassemble that base to achieve the goal.
We can write a firstorder formula that defines when a block x is a good tower, i.e., a good base that need not be disassembled.
4  goodtower(x) = clear(x) ^ goodtowerbelow(x)  4  goodtowerbelow(x) = (ontable(x) ^ : GOAL (9 y :on(xfi y )] _ holding(x))) _ 9 y:on(xfi y)] :GOAL(ontable(x) _ holding(x)) ^ :GOAL(clear(y)) ^ 8 z :GOAL(on(xfi z ))] z = y ^ 8 z :GOAL(on(zfi y))] z = x ^ goodtowerbelow(y) A block x satisfies the predicate goodtower(x) if it is on top of a tower, i.e., it is clear, and the tower below it does not violate any goal conditions.
The various tests for the violation of a goal condition are given in the definition of goodtowerbelow.
If x is on the table, the goal cannot require that it be on another block y nor can it require that the robot be holding x.
On the other hand, if x is on another block y, then x should not be required to be on the table, nor should the robot be required to hold it, nor should y be required to be clear, any block that is  required to be below x should be y, any block that is required to be on y should be x, and finally the tower below y cannot violate any goal conditions.
Our planner can take this first-order definition of a predicate (rewritten in Lisp syntax) as input.
And we can then use this predicate in an LTL control formula where during the operation of the progression algorithm (Table 1) its first-order definition will be evaluated in the current world for various instantiations of its "parameter" x.
Hence, we can use a strategy of preserving good towers by setting our LTL control formula to  2 8 x clear x (  :  (  )]  goodtower(x) )  goodtowerabove(x))fi  (1)  where the predicate goodtowerabove is defined in a manner that is symmetric to goodtowerbelow.
In any world the formula will prune all successor worlds in which a good tower x is destroyed, either by picking up x or by stacking a new block y on x that results in a violation of a goal condition.
Note also that by our definition of goodtower, a tower will be a good tower if none of its blocks are mentioned in the goal: such a tower of irrelevant blocks cannot violate any goal conditions.
Hence, this control rule also stops the planner from considering actions that unstack towers of irrelevant blocks.
What about towers that are not good towers?
Clearly they violate some goal condition.
Hence, there is no point in stacking more blocks on top of them as eventually we must disassemble these towers.
We can define: badtower(x)  4 clear x  =  (  )  ^ :goodtower(x)  And we can augment our control strategy to prevent growing  bad towers, by using the formula:    2 8 x clear x :  ( )]  goodtower(x) ) goodtowerabove(x) ^ badtower(x) ) (:9 y:on(yfi x)] )  (2)  This control formula stops the placement of additional blocks onto a bad tower.
With this control formula only blocks on top of bad towers can be picked up.
This is what we want, as bad towers must be disassembled.
However, a single block on the table that is not intended to be on the table is also a bad tower, and there is no point in picking up such a block unless its final position is ready.
Adding this insight we arrive at our final control strategy for the blocks world:    2 8 x clear x :  ( )]  goodtower(x) ) goodtowerabove(x) ^ ;badtower(x) ) (:9 y:on(yfi x)] ) ^ ontable(x)  ^ 9 y:GOAL(on(xfi y))]:goodtower(y) ) (:holding(x))  (3)  The performance of our planner with these three different control formulas is shown in Figure 2.
As in Figure 1 each data point represents the average time taken to solve 5 randomly generated blocks world problems.
This figure also shows the performance of the P RODIGY planner on these problems.
This planner, like TLP LAN, was run with a collection of hand written search control rules.
In this domain our method proved to be more effective than that of P RODIGY.
In fact, it is not difficult to show that the final control rule yields an O(n2) blocks world planner (where n is the number of blocks in the world).
In particular, the planner can find a near optimal plan (at most twice the length of the optimal) using depth-first search without ever having to backtrack.
Furthermore, there is always an optimal plan admitted by this control formula.
Hence, if we employ breadth-first search our planner will find an optimal plan.
However, finding an optimal plan is known to be NP-hard [GN92].
P RODIGY employed 11 different control rules some of which have similar intuitive content to our control formulas, but with others that required an understanding the P RODIGY planning algorithm.
We would claim that our final control formula is easily understood by anyone familiar with the blocks world.
P RODIGY does end-means analysis, so at every node in its search space it has available a world description.
It would be possible to use our control formulas on this world description.
However, most of the search performed by P RODIGY is a goal regressive search to find an action applicable to the current world.
It is this part of the search that seems to be hard to control.
Bounded Blocks World.
We have also implemented a bounded blocks world in which the table has a limited amount of space.
Dealing with resource constraints of this kind is  fairly easy for a forward-chaining planner, but much more difficult for partial order planners.8 A strategy capable of generating good plans in polynomial time can be written for this domain also, but it is more complex than our strategy for the unconstrained blocks world.
But even very simple strategies can be amazingly effective.
With no search control at all TLP LAN took an average of 470 CPU seconds to solve 10 different randomly generated six block problems when the table had space for 3 blocks.
When we changed the strategy to a simple "trigger" rule (trigger rules are rules that take advantage of fortuitous situations but do not attempt to achieve these situations) the average dropped to 0.48 seconds!
The particular rule we added was: whenever a block is clear and its final position is ready, move it there immediately.
This rule is easily expressed as an LTL formula.
Figure 3 plots the average time taken by TLP LAN to solve 10 random n block reconfiguration problems in the bounded blocks world where the table has space for 3 blocks.
The graph shows TLP LAN's performance using no control knowledge, the simple trigger rule given above, and a complete backtrack-free strategy.
Schedule World.
Another domain we have tested is the P RODIGY scheduling domain.
This domain has a large number of actions and the branching factor of the forward chaining space is large.
Nevertheless, we were able to write a conjunction of natural control formulas that allowed TLP LAN to generate good schedules without backtracking.
In this domain the task is to schedule a collection of objects on various machines to achieve various machining effects.
One example of the control formulas we used is, 8 x:object(x)] 2(polish(x) ^ :polish(x)) ) 2:polish(x), where polish is true of an object x if x is being polished in the current world.
This formula prohibits action sequences where an object is polished twice (basically the transition from being polished to when polishing stops prohibits future polishing).
Another use of the control formulas was to detect impossible goals.
For example, 8 x:object(x)] 8 s:GOAL(shape(xfi s))] s = cylindrical _ 2shape(xfi s).
In the domain the only shape we can create are cylinders.
This formula, when progressed through the initial state, checks every object for which the goal mentions a desired shape to ensure that the desired shape is cylindrical.
If it is not then the object must have that shape in the current state (i.e., in the initial state) and in all subsequent states.
Hence, when we pose a goal such as shape(Afi cone), the planner will immediately detect the impossibility of achieving this goal unless A starts off being cone shaped.
The performance of TLP LAN and S NLP (using domain independent heuristics) is shown in Figure 4.9 The data points in the figure show the average time taken to solve 10 random problems.
The x-axis plots the number of new features the goal requires (i.e., the 8  Resource constraints are beyond the capabilities of S NLP, but see [YC94] for a partial order planner with some ability to deal with resource constraints.
9 We were unable to obtain the P RODIGY control rules for this domain; hence, we omitted P RODIGY from this comparison.
400  Time (CPU sec.)
350 300 250 200 150 Prodigy 4.0 Control Formula 1 Control Formula 2 Control Formula 3  100 50 0 0  5  10  15  20  25  30  35  40  45  50  Number of Blocks  Figure 2: Performance of search control in the blocks world 400  Time (CPU sec.)
350 300 250 200 150 100  No Control Tigger Rule Complete Strategy  50 0 0  5  10  15  20  25  30  35  40  Number of Blocks  Figure 3: Performance of TLP LAN in the bounded blocks World number of machining operations that must be scheduled).10 In the experiments the number of objects are just sufficient to allow goals involving that number of new features (in this domain we can only add a limited number of new features to each object).
TLP LAN was able to solve all of the problems at each data point.
However, S NLP failed to solve 4 of the 3 new feature problems.
It is important to note that we are not using our experiments to claim that TLP LAN is a superior planner to S NLP (or P RODIGY).
After all, in the experiments described above, TLP LAN is being run with extensive control knowledge while S NLP was not.
Hence, it should be expected to outperform S NLP.
What we are claiming is that (1) domain independent heuristic search in real domains is totally inadequate, (2) planning can be effective in these domains with the addition of natural domain dependent search control knowledge, and (3) the TLP LAN approach to search control knowledge in particular is an effective way to utilize such knowledge.
These points are borne out by the data we have presented.
We have only incomplete evidence, given from the blocks world, that our approach to specifying search control knowledge is superior to 10 In this test we did not pose any impossible goals; so did not take advantage of the TLP LAN control formulas designed to detect impyossible goals.
P RODIGY's.
However, we are currently engaging in a more systematic comparison.
What is very clear at this point however, is that our LTL control formulas are much easier to understand, and their behavior easier to predict, than P RODIGY control rules.
In conclusion, we have demonstrated that search control knowledge for forward chaining planners can be expressed in a logical formalism and utilized effectively to produce efficient planners for a number of domains.
We are currently working on extending our planner so that it can plan for temporally extended goals (e.g., maintenance goals) and quantified goals.
TLP LAN is not yet capable of generating plans that satisfy all such goals.
This is due to its handling of eventuality goals like 3p.
If we pose the goal q, then our current implementation will search for a plan whose final state satisfies q.
The temporal formula 3p will be used only as a search control formula, and since it involves only an eventuality that can be postponed indefinitely, this will have no pruning effect.
Hence, our implementation could return a plan that achieves q in the final state but never passes through a state satisfying p. A more sophisticated implementation is required to ensure that eventualities are eventually satisfied and not postponed indefinitely.
In addition, we are working on applying our planner to some practical problem domains.
Finally,  400  Time (CPU sec.)
350 300 250 200 150 100  SNLP  50  TLPlan  0 0  5  10  15  20  25  30  Number of new features  Figure 4: Performance in the Schedule world.
as mentioned above, we are working on automatically generating search control knowledge from operator descriptions along the lines of [Etz93], and on a more systematic comparison with the P RODIGY system.
Acknowledgments: thanks to Adam Grove for extensive discussions; David McAllister for insights into inductive definitions; John McCarthy for the idea of trigger rules; Manuela Veloso and Jim Blythe for help with P RODIGY; and Hector Levesque, Nir Friedman and Jeff Siskind, David Etherington, Oren Ertzioni, and Dan Weld for various useful insights and comments on previous versions.
References [BW94]  A. Barrett and D.S.
Weld.
Partial-order planning: evaluating possible efficiency gains.
Artificial Intelligence, 67(1):71-112, 1994.
[CBE+ 92] J.G.
Carbonell, J. Blythe, O. Etzioni, Y. Gill, R. Joseph, D. Khan, C. Knoblock, S. Minton, A. Perez, S. Reilly, M. Veloso, and X. Wang.
Prodigy 4.0: The manual and turorial.
Technical Report CMU-CS-92-150, School of Computer Science, Carnegie Mellon University, 1992.
[CG87]  E. M. Clarke and O. Grumberg.
Research on automatic verification of finite-state concurrent systems.
In Joe F. Traub, Nils J. Nilsson, and Barbara J. Grozf, editors, Annual Review of Computing Science.
Annual Reviews Inc., 1987.
[Eme90]  E. A. Emerson.
Temporal and modal logic.
In J. van Leeuwen, editor, Handbook of Theoretical Computer Science, Volume B, chapter 16, pages 997-1072.
MIT, 1990.
[ENS92]  K. Erol, D.S.
Nau, and V.S.
Subrahmanian.
On the complexity of domain-independent planning.
In Proceedings of the AAAI National Conference, pages 381-386, 1992.
[Etz93]  Oren Etzioni.
Acquiring search-control knowledge via static analysis.
Artificial Intelligence, 62(2):255-302, 1993.
[GN92]  N. Gupta and D.S.
Nau.
On the complexity of blocksworld planning.
Artificial Intelligence, 56:223-254, 1992.
[HV91]  J. Y. Halpern and M. Y. Vardi.
Model checking vs. theorem proving: a manifesto.
In Proceedings of the International Conference on Principles of Knowledge Representation and Reasoning, pages 325-334, 1991.
[MDBP92] S. Minton, M. Drummond, J. Bresina, and A. Phillips.
Total order vs. partial order planning: Factors influencing performance.
In Proceedings of the International Conference on Principles of Knowledge Representation and Reasoning, pages 83-82, 1992.
[MR91]  D. McAllester and D. Rosenblitt.
Systematic nonlinear planning.
In Proceedings of the AAAI National Conference, pages 634-639, 1991.
[SBW90]  S. Soderland, T. Barrett, and D. Weld.
The SNLP planner implementation.
Contact bug-snlp@cs.washington.edu, 1990.
[Sel94]  B. Selman.
Near-optimal plans, tractability and reactivity.
In Proceedings of the International Conference on Principles of Knowledge Representation and Reasoning, pages 521-529, 1994.
[VB94]  M. Veloso and J. Blythe.
Linkability: Examining causal link commitments in partial-order planning.
In Proceedings of the Second International Conference on AI Planning Systems, 1994.
[YC94]  Q. Yang and A. Chan.
Delaying variable binding committments in planning.
In Proceedings of the Second International Conference on AI Planning Systems, 1994.
Generating Scenarios from Specications of Repeating Events Robert A. Morris and Lina Khatib Florida Institute of Technology Computer Science Program Melbourne, FL 32901 morris@cs.
t.edu  Abstract  This paper addresses the problem of reasoning about events that recur over time, especially when knowledge about those events may be incomplete or qualitative.
A constraint-based framework is proposed to solve the problem of mapping a speci cation of temporal information about recurring events to a scenario, a qualitative representation of an assignment of times to those events.
The main result of this paper is to reduce the problem of generating recurrence scenarios to the problem of generating a set of scenarios for convex interval relations.
1 Introduction  Many temporal reasoning problem involve a concept of recurrence.
For example, consider the following speci cation: Oce hours (OH) happen twice a week.
Faculty meetings (FM) happen at least once a week.
The CS-1 class (CM) meets twice a week.
Preparation time (PT) only precedes or meets class meetings.
Class meetings, in general, are always before, or 	nished by, lab research meetings (LR), and once a week the class meetings are 	nished by the lab meetings.
Preparation time is always and only during of	ce hours.
Unless otherwise indicated, each occurrence is disjoint from every other.
This speci cation contains qualitative and quantitative knowledge about recurring events.
Gerard Ligozat  LIMSI, Universite Paris-Sud, B.P.
133 91403 Orsay, Cedex, France ligozat@limsi.fr The statements in this speci cation can be classi ed into three kinds, viz., expressing: cardinality constraints (faculty meetings happen at least once) recurring relationships: (preparation time only precedes or meets classes) and global constraints: (every occurrence is disjoint from every other).
A scenario oers a  nite, qualitative abstraction of an in nite set of solutions, assignments of times to events.
A graphical representation of a scenario for the example speci cation is found in Figure 1.
Each event occurrence is indicated by a horizontal line, and the temporal relationships by their relative positions.
The meaning of the rectangles and arrows in the  gure are explained below.
The aim of this paper is to describe a complete framework for representing the temporal information found in speci cations of the kind just illustrated, and to describe a reasoning mechanism which, applied to this knowledge, is capable of generating scenarios for recurring events.
The paper extends the framework found in 5], 6].
2 Recurrence Relationships Relational contexts involving plural noun phrases describing recurring events (e.g.
class meetings) can be interpreted as a form of collective predication (i.e.
predication over collections of elements from some domain).
Thus, the domain of discourse for reasoning about recur-  rence can be viewed as collections of the standard time units, either intervals or points.
For example, if intervals are viewed in the normal way as ordered pairs of minimal time units, then the set  I = fhI1;  I1+i hI2; I2+i : : :  hIn; In+ig is a potential interpretation of a plural noun phrase describing a  nitely recurring event1.
For the sake of completeness, we review the proposed semantics for predicative contexts involving collections of intervals.
Given a temporal relation such as follows, it is possible to express a recurrence relationship by pluralizing: thus, the sentence games follow national anthems expresses a recurring relationship between recurring events.
Contextual knowledge about games and national anthems indicates a one-to-one mapping between occurrences.
In the  rst-order representation, annotations can be added to quanti ers to express this type of mapping.
For example, (8i : games)(9d!j : N:A:)follow(i j ) represents a one-one functional mapping, where 9d!
is read \there exists a unique, distinct".
Other contexts are compatible with dierent mappings for example, the sentence parallel sessions precede lunches suggests a many-to-one mapping between sessions and lunches.
Applying temporal adverbs such as \always", \only" and \sometimes" to these contexts re nes their meaning.
Consider, for example games only follow national anthems.
The truth condition of this sentence implies a notion of \corresponding pairs" of occurrences.
In previous accounts, such an I is called an ninterval.
1  Again following the previous analysis, corresponding components will be said to be correlated.
Formally, correlation is an equivalence relation over its domain.
The sentence Preparation time only precedes or meets classes in the example speci cation can be expressed in  rst-order logic as the following conjunction:2 (8i 2 PT )(9d!j 2 CM )COR(i j )^fp mg(i j )  ^(8i 2 PT )(8j 2 CM )COR(i j ) !
fp mg(i j )]  Here, the variables range over interval components, and the expression \COR(i j )" expresses the correlation of components.
If a time frame is speci ed, relational expressions such as \WEEK (i j )" (i and j happen during the same week) can be added to the representation.
In this paper, we ignore the issue of reasoning with dierent time frames.
This class of  rst order formulas will be abbreviated as second-order relational expressions.
The relations which provide the interpretations of these expressions will be called Q9-relations.
This secondorder representation will be expressed in the form QR(R) where QR stands for a Q9-relation, and R stands for an interval temporal relation (e.g.
an Allen relation between pairs of intervals).
Rather than review the formal semantics of these contexts, illustrative examples are provided in Table 1.
I and J stand for recurring events.
The table translates an English context involving recurrence relations into a second-order formula exThe thirteen atomic interval relations will in this paper be abbreviated as follows: precedes (p), preceded by (pi), meets (m), met by (mi), equals (eq), starts (s), started by (si), nished (f ), nished by (fi), during (d), contains (di), overlaps (o), overlapped by (oi).
Following custom, the set notation is used to abbreviate contexts involving disjunctions of these relations.
2  pressing a 89-relation, and its  rst-order equivalent.
Certain collections of recurrences relations form the domain elements of an interval algebra 7].
In 6], a recurrence algebra RA is de ned based on a set of recurrence relations of the form discussed briey above.
This algebra is closed under the operators inverse, intersection and composition.
Intersection of recurrence relations results in the formation of relations which are best expressed as a form of conjunction.
For example, the sentence meetings always follow  or meet lunches furthermore, they sometimes meet lunches expresses a  relation which can be viewed as the intersection of two recurrence relations: always follow or meet and sometimes meet.
We call the sometimes operator in this context the re	nement operator, and the relations that result the re	ning relations the relation being re ned will be called the leading relation.
Following the earlier notation 5], the symbol  will stand for furthermore.
Furthermore can be generalized to arbitrary  nite conjunctions of simple relations.
We de ne a normal form for recurrence relations in RA as QR(R)  99d!
(S1)  : : :  99d!
(Sn) where 1.
R is one of the 213 atomic interval relations 1] 2.
QR is either 89od!, 89d!oi, 89d!, or 89d!ouoi (see Table 1) 3.
Si  R i = 1 : : : n and 4. each Si is atomic, i.e., consisting of a single temporal relation.
Remark 1 Let RA be the set of recurrence relations in normal form.
For pairs  R S 2 RA, let inverse R;1 , intersection R u S , and composition R 	 S be de	ned as in  6].
The set RA is closed under these operations.
The proof appeared in 6].
3 Reasoning About Recurring Relations  First, we extend the notion of temporal constraint network for the purpose of storing information about recurring events: De nition 1 (RA Networks) An RA network is a network of binary relations where 1. variables represent collections of convex intervals 2. for each variable I , DI , the domain of I is the set of all sets of nonoverlapping convex intervals and 3. the binary relations between variables are elements of RA.
The restriction of elements of the domain to sets of non-overlapping intervals means that the elements within the set can be totally ordered.
This restriction is not necessary, but is typical for the sorts of applications under consideration.
The next de nition generalizes the corresponding notion within the interval calculus: De nition 2 (Instantiation) An instantiation of m variables in a set I representing collections of intervals is an mtuple of sets of intervals representing an assignment of elements of DI to each I 2 I .
Given a set of binary temporal relations from RA between elements of I , a consistent instantiation is an instantiation that satis	es all the relations in the set.
An RA-network is consistent  if such an instantiation exists otherwise, the network is inconsistent.
The next concept formalizes the various patterns of recurrence corresponding to a given recurrence relation: De nition 3 (Concretization of a Recurrence Relation) A recurrence relation  R = QR(R)  99(S1)  : : :  99(Sn) de	nes a set of concretizations Rc taken from a set of elements A, called the alphabet for R. A contains R, as well as possibly other elements, depending on the value of QR, as follows:  1.
If QR = 89d!o then A contains a symbol representing occurrences of an element in range(R), and each concretization in Rc contains zero or more occurrences of this symbol 2.
If QR = 89d then A contains a symbol representing occurrences of an element in dom(R), and each concretization in Rc contains zero or more occurrences of this symbol  !oi  3.
Finally, A contains 0, and each concretization in Rc contains zero or more occurrence of this symbol.
Terminology: if R is a recurrence relation between I and J , then i and j will be the the symbols representing arbitrary elements of I and J , respectively.
Each element in Rc is called a concretization.
Each concretization r must contain an occurrence of each Si  i = 1 : : : n. A k concretization is a 	nite concretization of length k .
Example 1 (Concretization) The sequence hp m p m o o : : :i represents an in	nite recurrence of temporal relations between pairs of collections of intervals.
It is a concretization of many recurrence relations among them  89d!ouoifp m og.
To properly explain the notion of concretization, and its corresponding set A, out of which concretizations are built, it is necessary to revisit the notion of correlated occurrences.
A recurrence scenario can be viewed as a partition of a set of occurrences based on the relation COR.
Formally: De nition 4 (C-partition).
Given a set I of 	nite sets of intervals and I J 2 I , a c-partition of I is the structure hfXi ] : i = 1 : : : kg CORi, for some kS, such that fXi]g is a partition of S I 2I  Ij 2I Ij ] determined by an equivalence relation COR, where Xi] = Xj ] if and only if COR(Xi  Xj ) Each Xi ] will be called a stage in the cpartition.
A c-partition will be said to be well-behaved if there exists a total ordering <COR of the elements of fXi ]g de	ned by  Xi] <COR Xj ] if and only if (8Ij 2 Xi])(8Jp 2 Xj ])Ij < Jp For example, Figure 1 represents a wellbehaved c-partition of a set of repeating events.
Each pair of intervals i j in the cpartition (represented by the rectangles) are correlated.
Examples of correlated intervals are represented by the arrows.
Any permutation of this order is also a c-partition.
Not all c-partitions are wellbehaved, as a later example will illustrate.
It may not, in fact, be possible to generate a well-behaved c-partition, given a speci cation of recurrence information.
An ordered c-partition corresponds to a concretization r as follows.
Let ri] be the ith element of a concretization r of a recurrence relation between I and J .
This element will typically denote a temporal relation between some Im 2 I and some Jn 2 J in the ith stage of the ordered c-partition of the set containing I and J .
Notice however that, in general, it is not required for a stage to contain an element from each recurring event.
For example, in the second stage of the cpartition in Figure 1, there is no occurrence of either OH or PT , hence no temporal relation between occurrences.
In addition, there is an occurrence of FM but not of CM in the same partition.
We will say that if rm] =i (j), then an occurrence of some element of I (J ) appears in the mth stage of a c-partition of a set containing I and J .
If ri] = 0, then neither I nor J has an element in the ith stage of the c-partition that corresponds to r. As a  nal preliminary to a formalization of the notion of recurrence scenario, we associate instantiations of a set of interval collections to concretizations.
For simplicity, and without loss of generality, we assume that a set of concretizations have a  xed length k: De nition 5 (C-satisfaction) Let I be a set of variables standing for recurring events.
An instantiation of every element of this set c-satis es a set of k concretizations frIJ : I J 2 SIg if Sthere exists a c-partition fXi ]g of I 2I  Ii 2I ] into k disjoint, non-overlapping sets such that, for each m = 1 : : : k exactly one of the following holds for each Xi] i = 1 : : : k and each concretization rIJ : 1. if rIJ i] is an atomic interval temporal relation, there exists a pair Iq] Jp] such that Xi] = Iq] =  Jp] and hIq  Jpi satis	es the relation rIJ m] 2. if rIJ m] is i, there exists an Iq ] such that Xi ] = Iq] and there exists no Jp] such that Iq ] = Jp] 3. if rIJ m] is j there exists a Jp ] such that Xi] = Jp] and there exists no Iq] such that Jp] = Iq] 4. if rIJ m] is 0, then for no Iq ] Jp] is it the case that Xi] = Jp] or Xi] =  Iq ] .
Example 2 Consider the instantiation: I = fh0 1i h2 4ig J = fh0 10i h12 13ig.
This corresponds to the following scenario:  I  J  This instantiation c-satis	es the concretization hs pi, as well as hi d ji.
Notice that no well-behaved c-partition exists for this instantiation.
There are weaker, \natural" c-partitions which are not well-behaved.
Intuitively, natural cpartitions are any that partition on the basis of temporal proximity.
They are natural because they correspond to the manner in which recurrence patterns are organized in thought and communicated.
3.1 Generating a Recurrence Scenario  A scenario for a set of n repeating events I can be viewed as a set C = r1 : : : rn(n;1) of concretizations of recurrence relations, one for each pair of elements of I , that satisfy certain properties.
Intuitively, for each i = 1 : : : n(n ; 1), the set fri]1 : : :ri]n(n;1)g contains consistent temporal information.
The  computational problem of interest, then, is that of generating a consistent set of concretizations from a speci cation.
The notion of a concretization network aids in solving this problem: De nition 6 (Concretization Network) Given a RA-network, M , M c is a conc of M c cretization of M if each edge MIJ is labeled with a concretization rIJ of the recurrence relation RIJ which labels the edge MIJ of M .
This leads,  nally, to the generalization of the notion of a consistent scenario: De nition 7 (k-Scenario) A concretization network M c of RA-relations is a k-scenario of a network M if every edge in M c is labeled by a concretization of length k of the corresponding relation in M .
M c is a consistent k-scenario if there is an instantiation of all the variables in M c that c-satis	es all the labels on the edges of M c .
A simple method transforms a concretization network into a set of interval relation networks.
De nition 8 (I-transformation of a 	nite concretization network).
Given a 	nite concretization network M c , and index p = 1 : : : k, where k is the maximum length of any concretization in M c, the pth I-transformation of M c is a network M p] de	ned as follows: 1.
Each vertex of M p] represents an element of some I in M for which the pth element of the concretization rIJ (denoted by rp]IJ ) on the arc MIJ (equivalently: on the arc MJI ) consists of either an atomic interval relation or i 2.
For each pair of vertices I , J , the arc M p]IJ is labeled as follows:   M p]IJ = rp]IJ , if rp]IJ is an  atomic interval relation otherwise  M p]IJ =??
where ??
denotes the universal interval relation (i.e., the relation that all pairs of intervals satisfy).
M is an I-transformation of a concretization network M c if, for some p, M is M p], the pth I -transformation of M c.  Remark 2 An I-transformation is a convex interval relation network.
It is now possible to reduce the problem of generating a consistent scenario from a concretization network to the problem of generating a set of solutions to interval networks.
This reduction is based on the following result, whose proof follows immediately from the preceding de nitions:  Theorem 1 A concretization network M c is a (	nite) m-scenario of an RA-  network M if:  c of M c is labeled by 1.
Each edge MIJ a member of the set RcIJ of concretizations of the relation RIJ on arc MIJ  and  2.
For each k = 1 : : : m, the Itransformation M k] of M c has a consistent scenario.
Example 3 Consider the following RA-network:  fi fi    89d!oi(fp pig)  99d!
(fpig) I - J BMB  BB  o BB  99d!
(fpigB)  89d!
(fmg)  BB  B   fi    K  A concretization network for this RAnetwork is the following:  J Y HHH hpi pi H C     HHH HH  CC  CC hm mi C     CC  > fi fi fi CCW fi fifi hi pii K  I  This network is a 2-scenario of the previous network.
The theorem suggests that the problem of generating k-scenarios from speci cations of recurrence relations stored as a RA-network M can be reduced to the problem of taking a set I = I1 : : :  In of repeating events and their n(n ; 1) relations and generating an n(n ; 1)  k matrix such that  The set of values in each column of the matrix de ne relations which can be I-transformed into a scenario for a set of intervals and  Each row of the matrix de nes a concretization for the recurrence relation for some pair of repeating events.
An algorithm based on this reduction has been designed it will be discussed in future work.
The algorithm is similar to, and in fact utilizes, a technique similar to that employed by Ladkin 3] for generating a scenario of interval temporal relations.
4 Summary  The objective here has been the development of a framework for reasoning about the recurrence of temporal relations.
To meet this objective, we focus on speci cations of recurrence whose counterparts in natural language involve the application of an adverbial modi er to a prepositional phrase describing a temporal order.
These contexts are mapped into second-order unary relational expressions interpreted over pairs of collections of intervals.
The class RA of recurrence relation is a subset of the Q9-class of relation, which were found to make up the logical structure of relation recurrence.
Knowledge expressible as an RA-relation can be stored and manipulated within a constraint-based framework.
In particular, the main result of this paper was showing how recurrence scenarios can be generated from speci cations of recurring events.
References  1] Allen, J.
(1983) Maintaining Knowledge About Temporal Intervals.
In Brachman, R., and Levesque, H., (eds.)
Readings in Knowledge Representation, (San Mateo:Morgan Kaufman), 510- 521.
2] Ladkin, P.B.
and Maddux, R. (1988) The Algebra of Constraint Satisfaction and Temporal Reasoning.
Technical Report, Kestrel Institute, Palo Alto, CA.
3] Ladkin, P.B., On Binary Constraint Problems.
Journal Of the ACM Vol.
41, Number 3, 435-469.
4] Ligozat, G., On Generalized Interval Calculi.
In Proceedings of the Ninth National Conference on Arti	cial Intelligence (AAAI-91), 234240.
5] Morris, R., Shoa, W., Khatib, L., Path Consistency in a Network of Non-Convex Intervals.Proceedings of the 13th International Joint Conference on Arti	cial Intelligence (IJCAI-93), Chambery, France (1993), pp.
650-655.
6] Morris, R., Shoa, W., and AlKhatib, L., (1994) Reasoning about Recurrence.
Forthcoming in The Journal of Computational Intelligence.
7] Van Beek, P. (1990).
Exact and Approximate Reasoning about Qualitative Temporal Relations.
Technical Report TR 90-29, University of Alberta, Edmonton, Alberta, Canada.
OH PT  BN  CM  HH j  LR FM  ?
QQs  ?
PPP PPq  Figure 1: A scenario for the academic scheduling example  English Context I only before J  Second-order Form First-order Equivalent I 89d!o(fpg) J (8i 2 I )(9 !j  I always before J  I 89d!oi(fpg) J  d  I always and only before J I 89d!ouoi(fpg) J I before J I sometimes before J  I 89d!
(fpg) J  2  J )COR(i j )^ fpg(i j )^ (8i 2 I )(8j 2 J )COR(i j ) !
fpg(i j )] (8j 2 J )(9d !i 2 I ) COR(i j ) ^fpg(i j ) ^  (8i 2 I )(8j 2 J )COR(i j ) !
fpg(i j )] (8j 2 J )(9d !i 2 I ) COR(i j ) ^ fpg(ij ) ^ (8j 2 J )(9i 2 I ) COR(i j ) ^fpg(i j ) ^  (8i 2 I )(8j 2 J )COR(i j ) !
fpg(i j )] (9i 2 I )(9d !j 2 J ) COR(i j ) ^fpg(i j ) ^  (8i 2 I )(8j 2 J )COR(i j ) !
fpg(i j )] (9i 2 I )(9d !j 2 J ) COR(i j ) ^ fpg(i j )  I 99d!
(fpg) J Table 1: Interpreting Recurrence
Speeding up temporal reasoning by exploiting the notion of kernel of an ordering relation Luca Chittaro, Angelo Montanari Dipartimento di Matematica e Informatica Universita di Udine, Via delle Scienze, 206 33100 Udine - ITALY {chittaro|montana}@dimi.uniud.it  Iliano Cervesato Dipartimento di Informatica Universita di Torino, Corso Svizzera, 185 10149 Torino - ITALY iliano@di.unito.it  1 Introduction In this paper, we consider the problem of expediting temporal reasoning about partially ordered events in Kowalski and Sergot's Event Calculus (EC).
EC is a formalism for representing and reasoning about events and their effects in a logic programming framework [7].
Given a set of events occurring in the real world, EC is able to infer the set of maximal validity intervals (MVIs, hereinafter) over which the properties initiated and/or terminated by the events maximally hold.
Event occurrences can be provided with different temporal qualifications [1].
In this paper, we suppose that for each event we either specify its relative position with respect to some other events (e.g., event e 1 occurs before event e2) or leave it temporally unqualified (the only thing we know is that it occurred).
Database updates in EC provide information about the occurrences of events and their times [6] and are of additive nature only.
We assume here that the set of events is fixed, and the input process consists in the addition of ordering information.
We will show how the introduction of partial ordering heavily increases the computational complexity of deriving MVIs.
Then, we will provide a precise characterization of what EC actually does to compute MVIs, and propose a solution to do it efficiently when only incomplete information about event ordering is available.
The paper is organized as follows.
In Section 2, we introduce the basic features of EC with relative times and partial ordering.
In Section 3, we analyze its computational complexity, that turns out to be exponential.
Moreover, we show how complexity can be reduced to polynomial (O(n5)) by adopting a graph marking technique that speeds up search.
In Section 4, we provide a deeper analysis of how EC derives MVIs, and we formally introduce the notion of kernel of an ordering relation.
Then, we show how the notion of kernel can be usefully applied to further reduce the complexity of computing MVIs.
Section 5 discusses an example, also contrasting the set of MVIs with the sets of necessarily and possibly true MVIs.
Section 6 concludes the paper.
2 The Event Calculus with relative times and partial ordering EC takes the notions of event, property, time-point and time-interval as primitives and defines a model of change in which events happen at time-points and initiate and/or terminate time-intervals over which some property holds.
Time-points are unique points in time at which events take place instantaneously.
Time-intervals are represented as pairs  of time-points.
EC embodies a notion of default persistence according to which properties are assumed to persist until an event occurs which terminates them.
In this paper, we focus our attention on situations where precise temporal information for event occurrences is not available.
We represent the occurrence of an event e of type tye by means of the clause: happens(e,tye).
The relation between types of events and properties is defined by means of initiates and terminates clauses: initiates(tye, p1).
terminates(tye, p2).
The initiates (terminates ) clause relates each type of event tye to the property p it initiates (terminates).
The plain EC model of time and change is defined by means of the axioms: holds(period(Ei,P,Et)):happens(Ei,TyEi), initiates(TyEi,P), happens(Et,TyEt), terminates(TyEt,P), before(Ei,Et), not broken(Ei,P,Et).
(1.1)  broken(Ei,P,Et):(1.2) happens(E,TyE), before(Ei,E), before(E,Et), (initiates(TyE,Q);terminates(TyE,Q)), (exclusive(P,Q);P=Q).
The holds axiom states that a property P maximally holds between events Ei and Et if Ei initiates P and occurs before Et that terminates P, provided there is no known interruption in between.
The negation involving the predicate broken is interpreted using negation-as-failure.
The broken axiom states that a given property P ceases to hold if there is an event E that happens between Ei and Et and initiates or terminates a property Q that is exclusive with P .
The exclusive( P , Q ) predicate is a constraint to force the derivation of P to fail when it is possible to conclude that Q holds at the same time.
Finally, the condition P = Q constrains interferences due to incomplete sequences of events relating to the same property.
It indeed guarantees that broken succeeds also when an initiating or terminating event for property P is found between the pair of events Ei and Et starting and ending P respectively.
The exclusive facts have obviously to be defined for each specific application (e.g.
exclusive(p,q).
).
Finally, knowledge about the relative ordering of events is expressed by means of facts of the form beforeFact( e 1 , e 2 ) .
The predicate b e f o r e used in h o l d s and b r o k e n is defined as the transitive closure of beforeFact :  before(E1,E2):beforeFact(E1,E2).
(1.3)  before(E1,E2):beforeFact(E1,E3),before(E3,E2).
(1.4)  The ordering information is entered through the predicate updateOrder( e 1 , e 2 ) : updateOrder(E1, E2) :assert(beforeFact(E1, E2)).
(1.5)  We assume that the set of ordered pairs is always consistent as it grows.
This means that before is supposed to represent a relation that is irreflexive, anti-symmetric and transitive.
The axioms of EC, shown as clauses (1.1-5), will be referred as program 1 in the following.
3  A complexity analysis  In the case of EC with absolute times and total ordering, the worst case complexity of deriving all the MVIs for a given property has been proven to be O(n3), where n is the number of recorded events [3].
In this section, a worst case complexity analysis will be carried out for EC with relative times and partial ordering.
We consider an EC database consisting of a set of events E={e1,...,en} and a set w of elements (ei,ej) whose transitive closure w + is a strict ordering relation on ExE.
The cost is measured as the number of accesses to the database to unify facts during the computation.
Some hashing mechanism is assumed so that fully-instantiated atomic goals are matched in one single access to a sequence of variable-free facts in case of success, and do not need any access in case of failure.
The complexity is given as a function of the number n of recorded events.
3.1  The complexity of EC with relative times and partial ordering  Queries have the form h o l d s ( p e r i o d ( E i , p , E t ) ) , where Ei and Et are variables and p can be either a variable or a constant.
The update predicate is always called with ground arguments.
For each predicate, we now analyze the cost of finding all its solutions.
u p d a t e O r d e r ( e 1 , e 2 ) : a call to this predicate has unitary cost since it only results in asserting a new fact in the database.
happens(Ei,TyEi) and happens(Et,TyEt) : each of this goal succeeds n times, since n events are recorded in the database.
So, the cost of each is O(n).
i n i t i a t e s ( T y E i , P ) and t e r m i n a t e s ( T y E t , P ) : the cost of these predicates is constant for a ground TyEi (or TyEt) even when they are called with P uninstantiated (as in clause 1.2).
exclusive(P,Q) : this predicate is always called ground and it thus can be matched against at most one fact in the database.
The cost is therefore constant.
beforeFact(E1,E2) : When called ground, as in clause (1.3), the query cost is constant.
In clause (1.4) instead, the call results in instantiating a variable.
The complexity is given as the maximum number of matching facts in the  database.
Having n nodes, at most n-1 edges can start from a given node.
Thus, when called with one variable argument, this predicate has cost O(n).
before(Ei,Et) : we will show that the standard twoclauses definition of before (clauses 1.3-4) has a worst case complexity that is at least exponential.
Consider n>=4 events arranged as shown in figure 1: all the n events but two (en-1 and en) participate in a total order between e1 and en2 .
All the transitive pairs, but the pair ( e 1 , e n ) are explicitly specified by means of beforeFact .
We assume that beforeFact(e 1 ,e n-1 ) textually follows any other fact of the form beforeFact(e 1 ,e i ) , for i = 2 ..n-2 .
en-2  Due to the operational behavior of PROLOG, EC thus tries to prove before(e1,en)  m = n-3  looking for a path that passes through e n - 2 e 1 e n-1 before attempting the en (only possible) path Figure 1 that includes e n - 1 .
Backtracking due to the failure in proving before(e n-2 ,e n ) causes every path from e 1 to e n-2 to be unsuccessfully attempted.
The resulting cost is computed as follows.
Let m=n-3 be the length of the longest path between e 1 and e n-2 .
We have the following recursive relation, where Cbf(m) represents the cost of finding all the solutions to the goal before(e',e") in case a total order of length m exists between e' and e" (the value of m highlighted in Figure 1 concerns the pair (e1,en-2)): Cbf (1) = 1;  m= 1 m -1  Cbf ( m ) = m + [?]
Cbf (i )  m >1  i =1  Indeed, there are m edges (represented by beforeFact ) starting from e' , which we can traverse to go towards e" .
One of these edges directly reaches e" (clause 1.3).
If m>1, each of the remaining m-1 edges leads to a node e (clause 1.4), reducing the problem to size i, where i is the length of the longest path between e and e".
In order to give an analytical form for Cbf(m), let us unfold the expression for Cbf(m): Cbf(m)=m+Cbf(m-1)+ Cbf(m-2) + Cbf(m-3) +...  +Cbf(1)=  =m +  (m-1)+2Cbf(m-2) +2Cbf(m-3) +... +2Cbf(1)=  =m+  (m-1) +  =m+ 20(m-1)+  2(m-2) +4Cbf(m-3) +... +4Cbf(1)= 21(m-2) +  22(m-3) +... +2m-2(1)  We can summarize this formula as: m- 1  Cbf (m ) = m +  m- 1  m -2  [?]
2i-1 ( m - i ) >= [?]
2i -1 = [?]
2 j = 2 m-1 - 1  i =1  i =1  j =0  Therefore the cost is at least O(2 m ).
Since we set m=n-3, before with both arguments instantiated turns out to be at least exponential in the worst case.
broken(ei,p,et) : since the definition of this predicate contains calls to before , this goal has an exponential  complexity.
holds(period(Ei,p,Et)) : reasoning as for broken , we obtain an exponential cost too.
The major results of the preceding analysis are the constant cost of updating ordering information (updateOrder ) and the exponential query complexity (holds).
3.2  The addition of marking The exponential cost resulting from the previous analysis makes EC with relative times not appealing for practical computations.
It is interesting to note that this very high cost origins from the calls to b e f o r e .
Can this predicate be re-implemented with a lower cost?
We use before to check whether a pair of nodes belongs to the transitive closure of a cycle-free relation.
Well-known algorithms for this kind of operations have polynomial complexity in the number of nodes.
Unfortunately, these algorithms are more suited to be implemented using traditional programming languages rather than logic programming.
We will now present a PROLOG program implementing a marking algorithm.
We foresee that it is written using extra-logical features of PROLOG.
Therefore, it will hardly be classified as a logic program.
Nevertheless, this program can be considered acceptable: once we have proven that the two versions of before behave coherently, we can see the non-declarative procedure as an actual implementation of the logical one.
before(E1, E2) :markingBefore(E1, E2), !, unmarkAll.
before(E1, E2) :unmarkAll, fail.
markingBefore(E1, E2) :beforeFact(E1, E2), !.
markingBefore(E1, E2) :beforeFact(E1, E3), happens(E3, _, unmarked), mark(E3), markingBefore(E3, E2).
(2.1)  unmarkAll :happens(E, _, marked), !, unmark(E), unmarkAll.
unmarkAll.
unmark(E) :retract(happens(E, TyE, marked)), assert(happens(E, TyE, unmarked)).
mark(E) :retract(happens(E, TyE, unmarked)), assert(happens(E, TyE, marked)).
happens(E,TyE):happens(E,TyE,_).
(2.5)  (2.2) (2.3) (2.4)  (2.6) (2.7) (2.8) (2.9)  Program 2 The idea is very simple: during the search, nodes are marked as they are visited, and only edges leading to not yet marked nodes are analyzed.
We need to change the arity of the predicate happens to support marking: the third argument contains either marked or unmarked with the obvious meaning.
Initially all the nodes are unmarked.
The purpose  of clause (2.9) is to maintain the one parameter interface to happens.
Let us now prove that the two versions of before compute the same relation, given the same factual database.
The declarative program consisting of clauses (1.3-4) yields before(e',e") if and only if the database contains a path leading from e' to e" , where the edges are represented by beforeFact (a simple inductive proof can be found in [2]).
We will now prove that the database contains that path if and only if program 2 derives m a r k i n g B e f o r e ( e ' , e " ) , and consequently before(e',e").
First suppose that the database contains at least a path p=( e' ,e 1 ),(e 1 ,e 2 ),...,(e k-1 ,e k ), (e k ,e" ) of length k+1 that links nodes e' and e" passing through k intermediate nodes and that all nodes are initially unmarked (the validity of this condition after each execution of before is guaranteed by the execution of the unmarkAll predicate occuring in clauses (2.1) and (2.2)).
If there is more than one path from e' to e" , let p be that path whose first edge comes first in the listing of beforeFact (if there is more than one path from e' to e" with this edge as its first edge, then let p be the path whose second edge comes first in the listing of beforeFact ; and so on).
In other words, this selected path is the first path from e' to e" considered by the PROLOG control strategy.
Let us prove that markingBefore(e',e") is derivable from program 2, and only nodes e 1,e 2,..,e k of the selected path are marked during this process.
The proof is inductive in the length of the selected path.
The case where the length of the path is 1, i.e.
there are 0 intermediate nodes and p=(e',e"), is caught by clause (2.3).
We assume, as inductive hypothesis, that the statement holds for length k (i.e.
k-1 intermediate nodes), and we prove that it holds for length k+1 (i.e.
k intermediate nodes).
Let us consider an instance of clause (2.4) where E 1 = e ' and E 2 = e " .
The first subgoal matches beforeFact(e',e 1 ) in the database, instantiating E3 to e 1 .
By hypothesis, all nodes are initially unmarked and e 1 can not have been marked up to now: if e1 were marked, then it would have been already reached by traversing another path, but this would contradict the hypothesis that p (to which e1 belongs) is the first path from e' to e" considered by the PROLOG control strategy.
Therefore, the second subgoal, happens(e 1 ,_,unmarked) , is immediately provable.
Moreover, the presence of this fact in the database causes the subgoal mark(e1) to mark e1 by clause (2.8).
By inductive hypothesis, markingBefore(e 1 ,e") is derivable and nodes e2,..,ek of the selected path are marked as a by-product of the process.
Thus, the overall clause succeeds and proves the goal markingBefore(e',e") .
Conversely, suppose that m a r k i n g B e f o r e ( e ' , e") is derivable from program 2.
We proceed by induction on the height h of the resolution tree for markingBefore(e',e") .
If h=1, then clause (2.3) has been applied, and the database contains the fact  beforeFact(e',e").
Otherwise, we assume, as inductive hypothesis, that the statement holds for every tree of height lower than h and prove its validity for trees of height h. Since h>1, clause (2.4) must have been selected at the first step.
Thus, b e f o r e F a c t ( e ' , e 1 ) a n d markingBefore(e 1 ,e") have successful derivations for some node e1.
By inductive hypothesis, the derivability of the former goal implies that there is a path p'=(e1,e2),...,(ek,e") between nodes e1 and e".
Considering (e',e1) together with p', we obtain the desired path.
We will now evaluate the cost of b e f o r e as implemented by program 2, and show the impact on EC of substituting clauses (1.3-4) of program 1 with program 2.
Let us first compute the cost of m a r k i n g B e f o r e and unmarkAll .
It is easy to show that the latter is always called.
Thus, the complexity of before is equal to the sum of the costs of the two.
The predicate m a r k i n g B e f o r e is designed to be called with both arguments instantiated.
Since clause (2.4) marks a node before the recursive call and since the number of nodes (initially all unmarked) is n, this predicate is called at most n times.
Moreover, each execution of m a r k i n g B e f o r e involves at most n -1 accesses to a b e f o r e F a c t fact.
Therefore, the overall cost for this predicate is O(n2).
Since at most n nodes have been marked by m a r k i n g B e f o r e , u n m a r k A l l has cost O ( n ) .
Therefore, before costs at most O(n2).
This upper bound is reached in the situation of Figure 1.
After integrating this version of before into EC, the cost of holds becomes polynomial, dropping from O(2n) to O(n5).
Indeed, if there are k events initiating p and h events terminating p in the database, the call to h a p p e n s ( E i , T y E i ) in the body of h o l d s , with E i unbound, can succeed n times but initiates(TyEi,P) will succeed only for k of the identified events.
Analogously, h a p p e n s ( E t , T y E t ) succeeds n times but terminates(TyEt,P) retains only h events.
As a result, k*h pairs of events are allowed to reach before(Ei,Et) .
Since k+h <= n, the product k*h is maximum for k=h=n/2, resulting in a quadratic number of pairs.
For each pair, both b e f o r e ( E i , E t ) and not broken(Ei,P,Et) will be considered in the worst case.
The cost of the former has been proved to be O(n 2 ) above, while the cost of the latter is evaluated as follows: the first goal in broken is called with an uninstantiated argument and can succeed n times; for each success, at worst two calls to before (2*O(n 2 )) and three constant cost calls (initiates and terminates with the first argument bound, and exclusive ) are performed.
Therefore, the cost of b r o k e n turns out to be cubic (O(n)*O(n2)).
Returning to holds, we obtain a cost equal to O(n2)*O(n3)=O(n5).
4 What the Event Calculus actually does and how to do it efficiently In this section, we aim at getting a better understanding  of what EC actually does when computing MVIs.
This insight allows to design more efficient versions of EC.
The representation of the ordering of events is of primary importance.
Let E = { e 1 ,...,e n } be the set of events, represented in EC by the predicate happens .
The ordered pairs beforeFact( e i , e j ) contained into the database constitute a set w[?
]ExE .
Notice however that the main axioms of EC never access w (i.e.
beforeFact facts) directly.
Instead, they rely heavily on the predicate before that models the transitive closure of w. Let us denote as w+ the transitive closure of w. w + is a strict order on E, i.e.
a relation that is irreflexive, asymmetric and transitive.
w is a subset of w+ and can indeed be viewed as a specification of it.
It is easy to prove that w+ can contain a quadratic number of edges; indeed the maximum number of edges n*(n-1)/2 is reached when w + is a total order.
From a graph-theoretic point of view, w corresponds to a directed acyclic graph G on E , whose nodes are event occurrences and such that there exists an edge from node ei to ej if and only if the pair (ei,ej) belongs to w., while w+ corresponds to its completion G+.
In order to compute the set of MVIs for a property p, the predicate holds in program 1 considers every pair of events e i , e j such that e i initiates p and e j terminates p, checks whether (ei,ej) belongs to w +, i.e.
if G + contains an edge from ei to ej, and ascertains that no interrupting event e occurs in between, i.e.
that G + does not contain any node e associated to a property q exclusive with p (or associated to the property p itself) such that (ei,e)[?
]G+ and (e,ej)[?]G+.
This approach presents two drawbacks.
First, EC blindly picks up every pair consisting of an event initiating p and an event terminating p, and only later looks for possible interruptions.
We would like instead to include the determination of possibly interrupting events within the search of candidate MVIs for p (i.e.
pairs (ei,ej) such that ei initiates p and ej terminates it).
Second, since G+ is implicit in G, we showed in the previous section that the cost of checking whether an edge belongs to G + by means of the marking version of before is proportional to the number of edges in G, i.e.
to the number of beforeFact in the database.
Both problems can be solved by shifting the emphasis from the transitive closure w + of w to its antitransitive closure, or kernel, w -.
w - is the least subset of w such that (w-)+=w+ and it can be obtained by removing every pair (e i,e j) from w such that (e i,e)[?
]w + and (e,e j)[?
]w + for some event e. The notion of kernel induces a subgraph G- of G that does not contain any transitive edge.
The number of edges in G - is strictly lower than n*(n-1)/2 and is indeed linear in most cases (as in the example reported in Section 5).
The results of Section 3 call for optimization in those cases (we expect them to be the most frequent) where the critical operation is querying for the validity of a certain property.
In these circumstances, the upper bound that comes out from the previous analysis is still not acceptable.
The solution we propose in the following operates on the representation of ordering information in the database.
We  will first introduce the proposed technique in the case where a single property is involved.
Then, the solution will be generalized in order to account for multiple properties.
4.1  Storing and updating the kernel In order to store and update the kernel of the ordering relation, clause (1.5) of program 1 must be replaced with the following program: updateOrder(E1, E2) :(3.1) not before(E1, E2), assertOP(E1,E2).
assertOP(E1, E2) :beforeFact(EA, EB), retractInBetween(E1,EA,EB,E2).
(3.2)  assertOP(E1, E2) :assert(beforeFact(E1, E2)), !.
(3.3)  retractInBetween(E1,EA,EB,E2) :(E1=EA; before(EA, E1)), !, (EB=E2; before(E2, EB)), !, retract(beforeFact(EA, EB)), fail.
(3.4)  Program 3 When the edge (e1,e2) is already entailed by the current ordering relation, it is not added to the representation, in order to maintain minimality.
This case is caught by clause (3.1) through the negative call to before.
Co-operating clauses (3.2-4) deal with the complementary case, i.e.
edge (e1,e2) is not subsumed by the current ordering.
Edge (e1,e2) is added to the representation by clause (3.3).
Before doing this, edges becoming redundant because of transitivity must be located and retracted from the database.
As shown by figure 2 (where the thin lines represent sequences of zero or more chained instances of beforeFact ), adding the edge (e1,e2) can close a transitive relation between nodes eA (possibly e1 ) and eB (possibly e2).
This is problematic when there exists already a direct link between these two nodes: this previously e1 e2 inserted link has now become redundant and eB eA must be removed.
This is done by Figure 2 clauses (3.2) and (3.4).
Notice that there may exist several pairs of the above kind, and all of the corresponding edges must be retracted.
This is achieved through backtracking by forcing the failure of clause (3.4).
When every possibility has been examined, the execution finally backtracks to clause (3.3) that succeeds asserting (only once) the added edge.
The cost of these operations is the following: retractInBetween(E1,EA,EB,E2) : this predicate is called ground and its cost thus corresponds to the cost of two ground calls to before - 2O(n2) - plus the constant cost of performing the retraction.
Notice that no backtracking is allowed within this clause.
The resulting cost is therefore quadratic.
assertOP(E1,E2): this predicate calls retractInBetween for each element of the kernel of the ordering relation (clause 3.2) and then asserts the ordered pair of interest (clause 3.3).
We showed that an upper bound for  the number of edges of the kernel is at most O (n 2 ) .
Therefore, a call to this predicate can cost at most O(n4).
updateOrder(E1,E2) : this predicate is called ground and its cost is given by the cost of one negative call to b e f o r e , and one to a s s e r t O P , resulting in O (n 4 ) complexity.
4.2  Single property In the case of a single property p, once only the kernel of the ordering relation is retained in the database, clause (1.1) can be simplified as follows: holds(period(Ei, p, Et)) :happens(Ei,TyEi), initiates(TyEi, p), happens(Et,TyEt), terminates(TyEt, p), beforeFact(Ei, Et).
Indeed, whenever p is the only property represented in the database, an event e interrupting a candidate MVI (e i,e j), where ei initiates p and ej terminates it, must either initiate or terminate p. Therefore (ei,ej) is an MVI for p if and only if ei is an immediate predecessor of ej in the current ordering, in the sense that no other event is recorded between the two.
It is worth noting that the predicate broken is not needed in this case.
Computing the complexity of this restricted version of EC is trivial.
In fact, the cost of holds(period(Ei,p,Et)) consists in the two calls to happens .
Since there are n events in the database, a call to h o l d s costs O (n 2 ).
Finally, notice that a further improvement in efficiency (at least in the average case) can be obtained by eliminating the calls to happens and by rearranging the atomic goals in its body as follows: holds(period(Ei, p, Et)) :beforeFact(Ei, Et), happens(Ei,TyEi), initiates(TyEi, p), happens(Et,TyEt), terminates(TyEt, p).
The complexity becomes equal to the number of beforeFact in the database, i.e.
the cardinality of the kernel.
4.3  Multiple properties We now generalize the technique to the case of multiple properties.
We maintain the minimality of the ordering information, as in the single property case, and implement a graph search algorithm for the query predicate holds.
This algorithm is implemented by the program given in the Appendix, where holds is clause (4.1).
The search is started from a specific initiating event.
Let ei be this event and p the corresponding property.
If ei is not instantiated in the query, all events in the graph will be processed.
The idea is to examine all the successors of e i searching for events terminating p. The search starts from its immediate successors, and proceeds breadth-first.
Exhausting a layer before examining nodes in the next is indeed crucial for the soundness of the algorithm.
As depicted in Figure 3, the nodes in the first layer after ei, are partitioned in three categories: terminating events for p, events interfering with p (i.e.
other initiating events for p, or  for properties incompatible with p) and independent events.
The nodes in the first category are terminating events in the MVIs returned to the user, and their successors are marked since there is no need to keep them into consideration during further processing.
<p] Nodes in the second termP category are simply marked as well as their successors since [p> initP [p> they cannot be ei contained in a successful path for Others otherP the user query.
The nodes in the third category are used to determine the next Figure 3 layer to explore, which is obtained by collecting all of their unmarked immediate successors.
The procedure repeats recursively until the most distant layer from ei is examined.
With reference to the code in the Appendix, findTerm (clause 4.2) finds the elements in the first layer after a given node, f i n d T e r m i n a n t s (clause 4.3) is the predicate which processes a layer and partitions the nodes, termP (clauses 4.4-6) is used to identify the events in the first category, nodes in the second category are processed by means of the predicates initP together with the auxiliary predicate initPorEx (clauses 4.7-11), and the remaining nodes are processed by the predicate otherP (clauses 4.12-13).  }
} }  5  Analyzing example  the  beverage  dispenser  We will now introduce an example, comment on the results of executing the basic and enhanced versions of EC on it, and further characterize the set of MVIs computed on this example by contrasting it with the set of possibly true and the set of necessarily true MVIs, respectively.
Consider the functioning of the simple beverage dispenser depicted in Figure 4: by setting the selector to the apple or to the orange position, apple juice or orange juice is obtained, respectively.
Choosing the stop position terminates the output of juice.
We model this knowledge as follows: initiates(selectApple,supplyApple).
initiates(selectOrange,supplyOrange).
terminates(selectStop, supplyApple).
terminates(selectStop, supplyOrange).
exclusive(supplyApple,supplyOrange).
exclusive(supplyOrange,supplyApple).
We consider an actual situation where six events happened: e1, e2, e3, e4, e5, and e6.
The following PROLOG factual knowledge associates events to their types: happens(e1,selectApple,unmarked).
happens(e2,selectStop,unmarked).
happens(e3,selectOrange,unmarked).
happens(e4,selectStop,unmarked).
happens(e5,selectApple,unmarked).
happens(e6,selectStop,unmarked).
These facts are intended for the marking implementation of EC, as it can be seen from the arity of the predicate happens .
The PROLOG code for the standard case is analogous and differs only for the absence of the third argument.
In the intended final ordering, events are ordered according to their indices.
Therefore, the final situation is represented in figure 5.
In our example, we will consider the following sequence of ordered pairs, which arrive one at a time: (e 1 ,e 4 ) - (e 1 ,e 6 ) - (e 2 ,e 4 ) - (e 1 ,e 2 ) - (e 3 ,e 4 ) - (e 4 ,e 5 ) (e2,e3) - (e2,e6) - (e5,e6).
e  1  supplyApple  e  2  e  e  3  supplyOrange  4  e  e  5  6  supplyApple  Figure 5 This sequence has been devised so that the complete situation shown in Figure 5 can be fully derived only after the last update.
The 9 ordered pairs are entered into the PROLOG database by running the following goals, in sequence.
????
?-  updateOrder(e1,e4).
updateOrder(e1,e6).
updateOrder(e2,e4).
updateOrder(e1,e2).
updateOrder(e3,e4).
???
?-  updateOrder(e4,e5).
updateOrder(e2,e3).
updateOrder(e2,e6).
updateOrder(e5,e6).
Table 1 shows the evolution of the Apple STOP Orange computation: each row corresponds to the addition of one of these ordered pairs to the database.
The first column shows which update is being performed.
The second column gives a visual account of the content of the database.
In particular, the kernel is represented in solid lines while deleted edges are drawn as dotted lines.
The third column contains the list of the MVIs derived by EC, i.e.
the result of running a generic query of the form ?h o l d s ( X ) .
For conciseness, we represented p e r i o d ( e i , s u p p l y A p p l e , e t ) as Figure 4 the more compact a ( e i , e t ) and period(ei,supplyOrange,et) as the more compact o(ei,et).
The experimental data (number of nodes visited in the resolution tree) obtained with this example show a more efficient behavior for the enhanced version of EC in the query phase.
This fact becomes more and more evident as the number of ordered pairs into the knowledge base grows: if the enhanced EC is only slightly faster (40 nodes analyzed versus 47) when there is no ordering information, after adding the last ordered pair the first answer is retrieved 4.2 times faster (86 nodes examined instead of 363) and the basic implementation explores 5 times more nodes (889 versus 178) to find all the MVIs.
Of course, the update operation is more expensive in the enhanced case, since just one node is explored in the basic implementation.
Anyway, this extracost is acceptable considering the benefits produced in the query phase and the fact that the overall cost (update and query) of the enhanced version is lower.
w  w visually  X:  w  w visually  X:  holds(X)  e1  e5  e3  e2  holds(X)  e4  e3  + (e 3 ,e 4 )  e6  a(e 1 ,e 2 ) a(e 1 ,e 6 ) o(e 3 ,e 4 )  e4  e2 e1 e6  + (e 1 ,e 4 )  e1  e4  e3  e5  e2  e6  e5  e3  + (e 4 ,e 5 )  e5  e4  e2  a(e 1 ,e 2 ) a(e 1 ,e 6 ) o(e 3 ,e 4 )  e1 e6  + (e 1 ,e 6 )  e4 e1  e3  e6  e5  e5  e2  a(e 1 ,e 6 )  + (e 2 ,e 3 )  e4  e3  a(e 1 ,e 2 ) a(e 1 ,e 6 ) o(e 3 ,e 4 )  e2 e1 e6  e2  + (e 2 ,e 4 )  e4  e3  e1 e6  a(e 1 ,e 6 )  e5  + (e 2 ,e 6 )  e3  e4 e2  e3  e6  e5  e1  a(e 1 ,e 2 ) a(e 1 ,e 6 )  + (e 5 ,e 6 )  a(e 1 ,e 2 ) o(e 3 ,e 4 )  e2 e1  + (e 1 ,e 2 )  e5  e4  e1  e6  e2  e3  e4  e5  e 6 a(e 1 ,e 2 ) o(e 3 ,e 4 ) a(e 5 ,e 6 )  Table 1 MVIs derived by EC  Necessary MVIs  Possible  MVIs  [?]
[?]
a(e1,e2),a(e1,e6),o(e3,e4),a(e5,e2),a(e5,e6)  ?- updateOrder(e1,e4).
[?]
[?]
a(e1,e2),a(e1,e6),o(e3,e4),a(e5,e2),a(e5,e6)  ?- updateOrder(e1,e6).
a(e1,e6)  [?]
a(e1,e2),a(e1,e6),o(e3,e4),a(e5,e2),a(e5,e6)  ...  ...  ...  ...  ?- updateOrder(e2,e3).
a(e1,e2),a(e1,e6),o(e3,e4)  [?]
a(e1,e2),a(e1,e6),o(e3,e4),a(e5,e6)  ?- updateOrder(e2,e6).
a(e1,e2),o(e3,e4)  a(e1,e2)  a(e1,e2),o(e3,e4),a(e5,e6)  ?- updateOrder(e5,e6).
a(e1,e2),o(e3,e4),a(e5,e6)  a(e1,e2),o(e3,e4),a(e5,e6)  a(e1,e2),o(e3,e4),a(e5,e6)  Table 2 Further insights on the behavior of EC with partially ordered events can be obtained by comparing it with the behavior of the Skeptical EC and Credulous EC, two variants of EC we proposed [4] to compute the MVIs which are derivable in all the total orders consistent with the given partial order (Skeptical EC), and those derivable in at least one of the total orders (Credulous EC).
As an example, Table 2 considers  again the beverage dispenser example and provides a comparison of the MVIs computed by the calculus presented in this paper, contrasted with those computed by the Skeptical and the Credulous calculus.
Dean and Boddy [5] studied the task of deriving which facts must be or can possibly be true over certain intervals of time in presence of partially ordered events (in our context, which MVIs must  necessarily hold and which possibly hold), focusing on the computational complexity of the task and showing that it is intractable in the general case.
In [2,4], we focused on providing the task with a modal logic formulation.
While [5] develop polynomial algorithms to compute supersets of the set of possible facts and subsets of the set of necessary facts, the calculus presented in this paper polynomially computes a set in between, mimicking some behaviors of human reasoners.
6 Conclusions The paper analyzed in detail the process of computing MVIs for properties in EC, and proposed a revision of the calculus that strongly increases its efficiency when dealing with partial ordering information.
The resulting calculus models events and their ordering relations in terms of a directed acyclic graph, and incorporates a marking technique to speed up the visit of the graph during the computation of validity intervals.
Moreover, it provides an alternative solution to the problem of supporting default persistence that further improves its performance.
Instead of the expensive generateand-test approach of the original calculus, it restricts a priori the search space by exploiting the kernel of an ordering relation.
Since we did not determine a lower bound for the complexity of the problem of deriving MVIs with partially information about event ordering, the possibility of further improving the achieved results can not be excluded.
References [1] I. Cervesato, A. Montanari, A. Provetti 1993.
"On the Non-monotonic Behavior of Event Calculus for Deriving Maximal Time Intervals", I n t e r v a l Computations 2, 83-119.
[2] I. Cervesato, L. Chittaro, A. Montanari 1995.
"A Modal Calculus of Partially Ordered Events in a Logic Programming Framework".
Proc.
ICLP '95, Tokyo, Japan, MIT Press.
[3] L. Chittaro, A. Montanari 1996.
"Efficient temporal reasoning in the Cached Event Calculus", to appear in Computational Intelligence Journal.
[4] L. Chittaro, A. Montanari, A. Provetti 1994.
"Skeptical and Credulous Event Calculi for Supporting Modal Queries", in Proc.
ECAI '94, Amsterdam, The Netherlands, Wiley and Sons Publishers, 361-365.
[5] T. Dean, M. Boddy 1988.
"Reasoning about Partially Ordered Events", Artificial Intelligence 36, 375-399.
[6] R. Kowalski 1992.
"Database Updates in the Event Calculus", in Journal of Logic Programming 12, 121146.
[7] R. Kowalski, M. Sergot 1986.
"A Logic-based Calculus of Events", in New Generation Computing,  4, Ohmsha Ltd and Springer-Verlag, 67-95.
Appendix holds(period(Ei, P, Et)) :(4.1) happens(Ei, TyEi, unmarked), initiates(TyEi, P), findTerm(Ei, P, Et).
findTerm(Ei, P, Et) :(4.2) findSucc(Ei, Es), findTerminants(Es, P, Res),unmarkAll,!, member(Et, Res).
findTerminants(Es, P, Res) :termP(P, Es, LessEs, ResTerm), initP(P, LessEs, FewerEs), otherP(P, FewerEs, ResOther), append(ResTerm, ResOther,Res).
(4.3)  termP(P,[E|Tail], NonTerm,[E|Term]) :(4.4) happens(E,TyE),terminates(TyE, P), markAll(E),termP(P, Tail, NonTerm, Term).
termP(P,[E|Tail],[E|NonTerm],Term) :(4.5) happens(E,TyE), not terminates(TyE, P), termP(P, Tail, NonTerm, Term).
termP(P, [], [], []).
(4.6) initP(P, [E|Tail], NonP) :initPorEx(E, P), markAll(E), initP(P, Tail, NonP).
initP(P, [E|Tail], [E|NonP]) :not initPorEx(E, P), initP(P, Tail, NonP).
initP(P, [], []).
(4.7)  (4.8) (4.9)  initPorEx(E, P) :(4.10) happens(E,TyE), initiates(TyE, P).
initPorEx(E, P) :(4.11) happens(E,TyE), (initiates(TyE,Q);terminates(TyE,Q)), exclusive(P, Q).
otherP(P, [E|Es], Res) :listSucc([E|Es], SuccEs), findTerminants(SuccEs, P, Res).
otherP(P, [], []).
(4.12)  findSucc(E, Es) :setof(NextE,(beforeFact(E, NextE), happens(NextE, _, unmarked)),Es), !.
findSucc(E, []).
(4.14)  listSucc([E|Es], SuccEEs) :findSucc(E, SuccE), listSucc(Es, SuccEs), listUnion(SuccE, SuccEs, SuccEEs).
listSucc([], []).
(4.13)  (4.15) (4.16)  (4.17)  markAll(E) :mark(E), findSucc(E, SuccE), markAllIn(SuccE).
(4.18)  markAllIn([E|Es]) :markAll(E), markAllIn(Es).
markAllIn([]).
(4.19)  listUnion([E|L1], L2, L3) :member(E, L2), !, listUnion(L1, L2, L3).
listUnion([E|L1], L2, [E|L3]) :listUnion(L1, L2, L3).
listUnion([], L, L).
(4.20) (4.21) (4.22) (4.23)
Rules for Simple Temporal Reasoning  Maroua Bouzid  Peter Ladkin  Maroua.Bouzid@loria.fr  ladkin@techfak.uni-bielefeld.de  CRIN-CNRS & INRIA Lorraine Technische Fakultt Bfitiment LORIA, BP 239 Universitt Bielefeld, Postfach 10 01 31, 54506 Vanduvre-Ls-Nancy, France 33501 Bielefeld, Germany  Abstract Simple practical reasoning with propositions whose truth depends on time is a matter of logical engineering.
We show that for Boolean logic a reied logic is more appropriate than its non-reied equivalent when time references are interpreted as union-of-convex intervals (UoCI).
1 Introduction  It is an elementary observation that propositions may be true at one time and false at another.
Any sort of real-world database must consider such a possibility, and there are various ways of timestamping entries in relation tables (i.e., atomic formulae) to reect temporal dependencies.
Drawing conclusions from data is the domain of logic.
How may logic help us in maintaining temporallydependent information in a current state, and in using this information easily?
This is not a new subject, either in AI or CS in general.
However, sometimes one can discover important features of reasoning by looking again at simple cases.
We want to nd rules capturing a large range of simple but useful inferences for temporally-irreective propositions, and to implement these rules in a system which attempts to maintain temporally-dependent data.
(We were thinking of applications to an ATMS 4] which annotates propositions with their periods of validity.)
We nd that using  union-of-convex intervals is more expressive than other proposals, but that one must use a reied logic rather than a non-reied logic, in contrast to the suggestion of Bacchus et al.
3].
1.1 Temporally-Irreective Propositions  We consider only propositions which are temporally-irrefiective: intuitively those that in a natural language would be expressed in the present tense with no temporal adverbs or other such explicit reference to time other than the present.
Propositions such as `Fred Smith is employed by Jones CatCleaning Industries, Inc.' are temporallyirreective: temporal qualiers such as `yesterday', `next week' do not occur.
The intuition is that a temporally-irreective proposition has a truth value which depends on time, but not on temporal indexicals in the proposition itself.
This feature validates certain persistence rules: downward persistence (DP), that if a proposition is true over a period of time, it's true over any subperiod of that time and limited upward persistence (UP), that if a proposition is true over two periods of time, then it's true over the `union' period of the two.
For purposes of precision, we dene a temporally-irrefiective proposition to be a proposition that satises (DP) and (UP).
If p and q are temporally irreective, it seems intuitively to be the case that Boolean combinations of them are temporally irreective also.
We treat here only Boolean com-  binations of temporally-irreective atomic propositions.
1.2 Motivation from Application  We must ensure that the hypotheses constraining the inquiry are plausible.
We originally considered enhancing an ATMS with temporal qualication to the truth of its propositions.
This application has three features which we adopted as constraints: (1) A focus on syntax.
Any information concerning the relations between propositions, intervals and truth must be reduced to syntactic information and syntactic inference from axioms.
(2) Inference is quantier-free.
Propositional inference alone is used, and inference follows so-called forward-chaining (i.e., use of Modus Ponens on axioms which are conditionals).
(3) Propositions typically are temporally irreective.
The rst feature suggests saying `proposition p is true on interval i' within the object-language, rather than as a metastatement about a logical system.
The second suggests searching for axioms of the form (V hypotheses ) conclusion) the third that we can assume (DP) and (UP).
Designing a temporal ATMS is beyond the scope of this paper.
We mention it simply to motivate the three constraints.
2 Time  What structure is needed to represent realworld temporal information?
Suppose the CEO of your company has two jackets, one red with orange polka dots and the other orange with red polka dots.
You naturally want to keep a daily record of his dress, in order to explain how it contributed either to your company's meteoric rise or to its complete misreading of the market, but in either case to make millions with your book on it.
The CEO wears his red jacket on Mondays, Wednesdays and Fridays, and his orange jacket on Tuesdays and Thursdays, or the other way round,  depending on the week.
According to some mathematicians, and some AI researchers, his jacket is red at all points in the set of points from the rst point at which he starts work on Monday until the point at which he leaves work on Monday.
According to others who simplify, it's red from the rst point .... to the point at which he leaves.
However, being ordinary mortals we just want to say it's red for the workday on Monday.
But we also want to say that this is true on other days too, with gaps of orange in between.
The rst simplication leads to an ontology of convex intervals of time as the temporal reference of truth values of propositions and the second to an ontology of objects which are unions of separated convex intervals, which we call union-of-convex intervals (UoCIs).
Truth over explicit convex intervals and related structures has been studied in 7, 1, 2, 18].
8, 9, 16] have studied propositions and reasoning over UoCIs 13, 14] has further studied the mathematics of representations of convex intervals as sequences of points of varying length.
2.1 Choosing a Representation UoCIs correspond to no xed number of points - one UoCI may have four components (maximal convex subintervals) with thus 8 points, and another six components (12 points).
However, if we represent UoCIs directly, rather than via points, we utilise one temporal argument only in an assertion of truth (the UoCI), as in the convex case.
This move allows us to use a standard logical language to write temporally-qualied assertions.
Concerning the UoCI data structure, Ligozat (op.
cit.)
represents a UoCI as a sequences of real numbers (representing component endpoints).
We choose the TUS.
This allows us to use logical reasoning within a theory of UoCIs, as well as numerical calculation.
2.2 Can We Use an Existing Truth-Over-Intervals Theory?
2.3 Applicable Reasoning With Intervals  We show in 5] the inadequacy of Humberstone's formulation 7].
Shoham's reied logic 17] adds axioms simulating propositional reasoning in the arguments to the predicate TRUE, to enable the usual propositional inferences to be performed, e.g.
TRUE(i p)&TRUE(i q)  ) TRUE(i p  q)  ( & )  It is somewhat inelegant to have to add all these rules.
A Bacchus-style propositional logic, which subsumes Shoham's 3], treats an atomic proposition as having an extra argument which is a UoCI, and in contrast to Shoham's, doesn't allow Boolean combinations as arguments in atomic formulae.
Thus we write p(I ) instead of TRUE(i p), and the formula corresponding to Shoham's atomic TRUE(i (p&q)) is the conjunction p(I ) & q(I ), which also corresponds to TRUE(i p) & TRUE(i q), showing how the reied rule is absorbed by the underlying logic in the non-reied formulation.
We need to add a temporal theory (Bacchus et al.
don't propose any)  we pick UoCIs over the rationals, and varying temporal references in some of the axioms, so one can infer the truth of propositions over di erent intervals from those they came with (else adding temporal references wouldn't give us anything over propositional logic!).
In addition, a reied theory needs to add simulation rules as above for propositional reasoning.
Using (the theory of) UoCIs over the rationals allows us to employ a single temporal argument in either reied or Bacchustype predicate symbols.
The atomic formula models(I p) (which we write I j= p) asserts that proposition p is true over interval I .
The logic of interval reasoning is considered in 18], in particular the rst-order theory of convex intervals over a dense unbounded linear order is proved countably categorical.
Special reasoning techniques, using methods described in 11], have been developed for some quantier-free formulas 1, 12].
McKenzie has noted that the theory of nite-UoCIs (fUoCI, a UoCI with nitely many components) on the rationals is decidable via the decision procedure for S2S (which is superexponential!)
15] Ligozat has shown how to perform some quantier-free reasoning with them 13, 14] and Morris, Shoa and Khatib have adapted the methods of convex-interval reasoning to some special cases 16].
These works all treat the intervals as objects of a mathematical structure.
They show that we can full some desiderata by choosing fUoCIs as our temporal reference.
But what about the data?
One can represent real convex intervals of time as clock-like sequences year, month, day, hour, ....] of all possible nite lengths and 10] showed that, with the appropriate interval relations between them, these form a notation for the convex rational interval structure.
This notation is called the BTU (Basic Time Units) (called TU in 10]) and is the foundation of the TUS (Time Unit System), which includes non-convex intervals formed from BTUs by application of the operators periodify 8], and conglom and intersect (see Section 2.4).
2.4 Conglomeration and Intersection  In 5], we dene the conglomeration operation of two intervals I and J , denoted conglom(I J ) in the code and I + J in math notation, to be the `union' of I and J , i.e.
that interval which is the set of components of I and J , except that those di erent components which have some common subinter-  val or those which meet are merged into one component 8].
Conglomeration is an associative and commutative operation, and so generalises to an arbitrary set of interval arguments.
A formal denition of conglomeration is a straightforward formalisation of the intuitive denition.
Similarly, the intersection of two intervals, intersect(I J ) or I  J , is that interval which consists precisely of those subintervals which are common to both I and J (i.e.
the overlapping parts of their components).
Algorithms for computing both these operations in time linear in the number of components are given in 5].
It's easy to show that the fUoC rational intervals form a distributive lattice 6] under + and `'.
Relaxing the niteness requirement, a general UoCI may have innitely many components.
The general UoC rational intervals form a complete lattice (arbitrary sums and products exist) under the generalisation of + and  to arbitrary sets of intervals.
2.5 Some Reasoning Principles  If we add the two points at innity to the rationals, we can add the empty interval hi and the full line 1 = h;1 1i.
They satisfy the following laws: for any interval I , I + hi = I I  hi = hi I + 1 = 1 I  1 = I .
It would also make sense under this supposition to allow intervals to have components which are half-innite convex intervals in the set: Half = fh;1 aij a 2 Qg  fhb 1ij b 2 Qg where Q is the rational numbers.
Call these intervals the extended-rational UoCIs.
It's easy to show that every extended-rational UoCI I has a complement: an interval I such that I + I = 1 I  I = hi.
A structure on which the binary operations +  form a distributive lattice, with constants hi, 1 and unary operation , all satisfying the stated laws, is a Boolean algebra.
Thus the extended-rational UoCIs form a Boolean algebra under these operations.
How may we use this observation to explicate the relation  between intervals and formulas?
We have taken the following rule of downward-persistence 17] as part of the denition of temporally-irreective propositions: (DP) : (I j= p) & (J  I ) ) (J j= p)  where "" denotes the interval-containment relation.
Interval containment is dened for convex intervals as S  F  D: a UoC I is contained in J just in case each component of I is contained in some component of J (it is straightforward to formalise this informal denition).
For the interval structure we use, interval-containment is denable from `+' using the composition principle below.
Similarly, we have taken the following rule of limited upward-persistence to be part of the denition of temporal irreectivity: (UP) : (I j= p) & (J j= p) ) (I + J j= p)  With a proposition p we may associate the interval  Ip  =  conglom(fJ j J j= pg)  when it exists.
In the complete lattice of extended-rational UoC intervals, which includes arbitrary sums and products and thus some `innite-UoC' intervals, Ip always exists, whereas in the extended-rational fUoCIs it may sometimes not.
In the complete lattice, we may generalise upward-persistence to the rule of complete upward persistence (CUP) : Ip j= p  so Ip is thus the maximal interval on which p is satised, in this lattice.
The composition principle (CompP) that  8J  I 9K disjoint fromJ :  :  J +K =I  although false for convex intervals over the rationals (for example, take I = h1 4i and J = h2 3i), is easily provable for UoC rational  intervals (take K = fh1 2i h3 4ig).
One may prefer to dene  in terms of +, namely that  J  I = 9K : J + K = I 4  In the presence of this denition, the composition principle is equivalent to the existence of a complement for every interval (complements require also hi and 1).
In the lattice of fUoCIs over the extended rationals, it follows from (DP) and (UP) along with (CompP) that the persistence condition is satised, namely that  I j= p , 8J  I : J j= p  The rule Ip j= p conjoined with downward persistence is equivalent to this persistence condition over the complete lattice.
We can take persistence in the complete lattice to mean either of these equivalent formulations.
The following rules are intuitively plausible for temporal irreectivity: (I j= p & I j= q ) , I j= (p & q ) (I j= p _ I j= q ) ) I j= (p _ q ) The following two more general rules follow directly from these and (DP).
(&-I) (I j= p&J j= q) ) (I  J ) j= (p & q)  (_-I) (I j= p _ J j= q) ) (I  J ) j= (p _ q) It easily follows from (&-I) and (_-I) that in the complete lattice:  Ip&q = Ip  Iq and Ip_q = Ip + Iq  In order to obtain propositional reasoning within the reied form, we add the principle that tautologies are true over any interval: (Taut) I j= p for any tautology p  which may be expressed in the complete lattice in the presence of downward-persistence as: 1 j= p for any tautology p.  Consider now the law of non-contradiction, (NonCon) I 6= hi  ) I 6j  p :p) In the complete lattice, it follows from the fact that p _ :p is a tautology, from (Taut), persistence, and (NonCon) that I:p = (Ip).
Thus the mapping p 7!
Ip is an embedding (a one-to-one homomorphism) of the Boolean algebra of propositional logic (the free Boolean algebra on countably many generators) into the complete extended-rational UoC intervals.
This makes temporal inference very easy!
But we don't yet have all required rules.
Fix interval I , and suppose I j= p. How may we infer that all the propositional consequences of p also hold on I ?
That is, suppose I j= (p !
q).
The intuition behind temporal irreectivity along with persistence would lead us to infer I j= q.
Thus, we need the rule: (MP) I j= (p !
q) & I j= p ) I j= q However, using Ip comes at a high price.
Adding hi and the points at innity destroy the uniformity property that for any Allen relation R and convex interval i, there exists a j such that iRj : consider meets and the interval i = h2 1i.
There is no interval j such that iM j .
Similarly there is no j such that hi M j .
This in turn destroys the validity of the Allen composition table and renders the relation algebra IA of intervals much more complex (indeed possibly innite).
If the algebra is innite, path-consistency computations may no longer terminate (example in 11]), and so on.
Thus we choose to stay with inference rules over the basic lattice or the basic complete lattice, and not include hi, 1, or Half.
This a ects the rules concerning negation, and prevents us from simplifying (Taut).
However, we retain as axioms all the rules mentioned, modifying (NonCon) as follows: =( &  (NC) I 6j= (p & :p)  Name  Rule  Justication  DP UP  j= p & J  I ) J j= p j= p & J j= p ) (I + J ) j= p I j= p _ J j= p ) (I  J ) j= p I j= :p ) :(I j= p) :(I j= (p&:p)) I j= p for every tautology p I j= (:p) & J  I ) :(J j= p) I j= p & I j= q , I j= (p&q ) I j= p _ I j= q ) I j= (p _ q ) I j= (p !
q ) & I j= p ) I j= q I j= p & J j= q ) (I  J ) j= (p&q ) I j= p & J j= q ) (I + J ) j= (p _ q ) I j= p _ J j= q ) (I  J ) j= (p _ q ) I j= (p !
q ) & J j= p ) (I  J ) j= q  Axiom Axiom DP, PrLogic NC, &-I, PrLogic Axiom Axiom NC, DP Taut, MP (&-I) MP, PrLogic Axiom Axiom Taut, MP, UP Axiom MP, DP  NC Taut  MP &-I _-P _-I EMP  I I  Table 1: `Inference Rules' and Their Justication An argument that these rules su#ce (in the sense that no more can be reasonably added) may be found in 5].
2.6 Converting to Bacchus Form  For Bacchus-form, we rewrite I j= p as p(I ) and we have noted that one writes I j= (p & q) as p(I ::) & q(I ::).
3 Reied Logic Wins  Table 2 shows the axioms converted into Bacchus-form.
(NC), (Taut) and (MP) become tautologies.
(&-I) and (_-I) become three formulas in Bacchus-form, all of which follow by propositional logic from (BDP).
We're left with the persistence rules (BDP) and (BUP).
How elegant!
Since we regarded these rules as dening the temporallyirreective propositions, it may seem that the extra rules we introduced for the reied case are there only because of reication, and that Bacchus-form is preferable.
But hold on  in fact, Bacchus-form turns out to be simply less expressive.
Table 2 shows the Bacchus translation of the rules of Table 1, which are either  Name Rule  Justication  BDP BUP  Axiom Axiom BDP, PrLogic Tautology Tautology Tautologies False!!
Tautology Tautology Tautology Axiom Axiom False!!
Axiom New Axiom !!
( ) & J  I ) p(J ) ( ) & p(J ) ) p(I + J ) p(I ) _ p(J ) ) p(I  J ) :p(I ) ) :p(I ) :(p(I )&:p(I )) p I p I  (Tautologies)  &-I-L &-I-R _-U-I _-I BEMP  :p(I ) & J  I ) :p(J ) p(I ) & q (I ) ) p(I ) & q (I ) p(I ) _ q (I ) ) p(I ) _ q (I ) p(I ) !
q (I ) & p(I ) ) q (I ) p(I ) & q (J ) ) p(I  J ) p(I ) & q (J ) ) q (I  J ) p(I ) & q (J ) ) p(I + J ) _ q (I + J ) p(I ) _ q (J ) ) p(I  J ) _ q (I  J ) p(I ) !
q (I ) & p(J ) ) q (I  J )  Table 2: The Rules in Bacchus Form axioms or derived from axioms (the Justication column).
The Bacchus-form of the derived rules yields some anomalies.
1.
The rule (NC) turns into a false statement in Bacchus-form.
The reason is straightforward  the reied logic distinguishes between a negation of a proposition being true over an interval I , and it not being the case that the proposition is true over I .
This distinction may not be made in Bacchus-form.
2.
The rule (_-P), which is derivable in the reied logic, corresponds to (_-U-I) in Bacchus-form.
(_-U-I) is false, and we don't see a way to translate (_-P) well into Bacchus-form.
3.
The Bacchus-form of (EMP) is a new rule (BEMP) which isn't derivable from (BDP) or (BUP), even though (EMP) is derivable from (MP) (whose Bacchusform is a tautology) and (DP).
(BEMP) may look as though it should be derivable from propositional modus ponens  and (BDP), but in fact it's not, since although one can conclude p(I  J ) from p(I ) using (BDP), from p(I ) !
q(I ) there's no rule which would enable one to conclude p(K ) !
q(K ) for K  I (and then we would use K = I  J ).
The hypothesis p(I ) has been weakened  of course one could infer p(I ) !
q(K ) but this doesn't help.
The implication is a compound formula composed from two formulas evaluated on intervals, whereas in the reied case, it's one compound formula evaluated on a single interval, and thus (DP) is applicable.
So Bacchus-form needs three rules, (BDP), (BUP) and (BEMP), not just two as we'd originally thought.
Crucially, it cannot easily e ect the important distinction between a negation being true on an interval and it not being the case that a proposition is true on the interval neither does it seem that the upward persistence of disjunction from a conjunction, (_-U-I), can be expressed in the simple form of an implication.
A reied logic yields discriminations that appear not to be obtainable easily with Bacchus-form for UoCIs.
Even supposing an equivalent form of the rules could be found, they may not have the form V hypotheses ) atomic-formula suitable for forward-chaining, and they may involve formulas other than Boolean combinations of qualied temporally-irreective atomic predicates.
We conclude that a reied logic is more suitable for evaluating propositions on UoC intervals.
4 Conclusions  We considered the simple propositional logic of temporally-irreective propositions whose truth varies with time.
Under the supposition of linear time, we proposed a temporal ontology of UoCIs, and proposed to represent these directly, using the TUS with conglom-  eration and intersection, as in 5].
Given this structure, we considered the issue of reied logic versus a Bacchus-style non-reied logic for evaluating propositions over UoCIs, and concluded in favor of the reied logic.
References 1] J.F.
Allen.
Maintaining knowledge about temporal intervals.
Communications of the ACM, 26(11):832843, November 1983.
2] J.F.
Allen.
Towards a general theory of action and time.
Articial Intelligence, 23:123154, 1984.
3] F. Bacchus, J. Tennenberg, and J.A.
Koomen.
A non-reied temporal logic.
Articial Intelligence, 52:87108, 1991.
4] M. Bouzid, F. Charpillet, P. Marquis, and J.-P. Haton.
Assumption-based truth maintenance in presence of temporal assertions.
In Proceedings of the 6th IEEE Tools with Articial Intelligence, pages 492498.
IEEE Press, 1994.
5] M. Bouzid and P. Ladkin.
Simple reasoning with time-dependent propositions, 1995.
Submitted for publication.
Also in  http://www.techfak.uni-bielefeld.de/ techfak/persons/ladkin/.
6] B.A.
Davey and H.A.
Priestley.
Introduction to Lattices and Order.
Cambridge University Press, 1990.
7] I.L.
Humberstone.
Interval semantics for tense logic: Some remarks.
Journal of Philosophical Logic, 8:171196, 1979.
8] P.B.
Ladkin.
Primitives and units for time specication.
In Proceedings of AAAI'86, pages 354359.
Morgan Kaufmann, 1986.
9] P.B.
Ladkin.
Time representation: A taxonomy of interval relations.
In Proceedings of AAAI'86, pages 360366.
Morgan Kaufmann, 1986.
10] P.B.
Ladkin.
The completeness of a natural system for reasoning with time intervals.
In Proceedings of IJCAI'87, pages 462467.
Morgan Kaufmann, 1987.
11] P.B.
Ladkin and R.D.
Maddux.
On binary constraint networks.
Journal of the ACM, 41(3):435469, May 1994.
12] P.B.
Ladkin and A. Reinefeld.
E ective solution of qualitative interval constraint networks.
Articial Intelligence, 57(1):105124, September 1992.
13] G. Ligozat.
Weak representations of interval algebras.
In Proceedings of AAAI'90, pages 715720.
AAAI Press, 1990.
14] G. Ligozat.
On generalised interval calculi.
In Proceedings of AAAI'91, pages 234240.
AAAI Press, 1991.
15] R.N.
McKenzie, 1987.
Personal Communication.
16] R. Morris, W. Shoa , and L. Khatib.
Path consistency in networks of nonconvex intervals.
In Proceedings of IJCAI'93, pages 655660.
AAAI Press, 1993.
17] Y. Shoham.
Reasoning About Change: Time and Causation from the Standpoint of Articial Intelligence.
MIT Press, 1987.
18] J.F.A.K.
van Benthem.
The Logic of Time.
D. Reidel, second edition, 1992.

Parallel Temporal Resolution Clare Dixon Department of Computer Science University of Manchester Manchester M13 9PL, UK.
Michael Fisher and Rob Johnson Department of Computing Manchester Metropolitan University Manchester M1 5GD, UK.
dixonc@cs.man.ac.uk  fM.Fisher,R.Johnsong@doc.mmu.ac.uk  Abstract Temporal reasoning is complex.
Typically, the proof methods used for temporal logics are both slow and, due to the quantity of information required, consume a large amount of space.
The introduction of parallelism provides the potential for significant speedups together with the increased memory size required to handle larger proofs.
Thus, we see the effective utilisation of parallelism as being crucial in making temporal theorem-proving practical.
In this paper we investigate and analyse opportunities for parallelism within a clausal resolution method for temporal logics.
We show how parallelism might be introduced into the method in a variety of ways, providing a range of options for the parallel implementation of proof procedures for this important class of logics.
1 Introduction Temporal logics are non-classical logics that have been specifically developed for reasoning about properties that vary over time [10].
Varieties of temporal logic have been used in both computer science and AI to represent and reason about dynamic systems and systems dealing directly with temporal information.
In many of these areas, some form of proof or validation is required.
However, as complex systems are often represented by correspondingly large temporal formulae, proofs about such systems are usually long and computationally intensive.
A further problem is that, in general, deciding whether a temporal formula is valid or not is difficult.
Although the worstcase complexity for temporal proof methods may not arise frequently, a large amount of temporal information needs to be handled even in the average-case.
We see the practical viability of large-scale temporal theorem proving as being dependent upon the effective utilisation of parallel architectures.
The successful implementation of parallel proof methods appears to be the only means whereby these large amounts of temporal information can be handled tractably.
Although parallel techniques cannot always be applied productively, certain proof methods that have been developed for temporal logics, particularly clausal resolution and semantic tableau,  show potential for parallelisation.
The exploitation of parallelism is, unfortunately, far less straightforward for temporal logics than in the case of classical logics since the additional complication of partitioning graphs, together with other graph manipulation operations, must be introduced.
It is a variety of approaches to tackling this problem that we seek to address in this paper.
1.1  Parallel Theorem-Proving  The parallel implementation of a theorem-prover, whilst not affecting the worst-case complexity of the underlying proof procedure, often enables us to handle much larger formulae, and to process them much faster.
In classical logics, parallel implementations of various proof procedures have been developed, including resolution [7], semantic tableau [13], and model elimination [21].
The parallel implementation of these theorem-provers is closely related to the development of parallel implementations of logic programming systems, where potential for parallelism occurs at several different levels of granularity within logical formulae (e.g., formula, clause, or literal) and where several alternative evaluation strategies (e.g., AND/OR parallelism) can be utilised [15].
1.2  Temporal Theorem-Proving  What makes temporal theorem-proving more difficult than the classical case, and why can't we directly use the systems already developed?
The answer to both questions lies in the fact that temporal logics, particularly the logic we study in this paper, provide more expressive power than their classical counterparts.
As discrete temporal logics essentially encode a simple form of induction, then decision procedures for such logics usually involve two forms of search -- tree search within non-temporal formulae and graph search within temporal formulae [11].
Thus, the parallelisation of such decision procedures will involve the integration and parallel implementation of two, distinct, types of search.
While the first of these is equivalent to the problems encountered in mechanising classical logic, the necessity of implementing additional graph search mechanisms means that temporal theoremproving is fundamentally different to classical theoremproving.
1.3 Structure of the Paper We begin, in SS2, by outlining the temporal logic that we wish to mechanise.
In SS3, the clausal resolution method that we use for proofs in this logic is described [11].
This description includes not only an outline of the proof method, but also an overview of the areas within this algorithm where parallel implementation is either possible or particularly beneficial.
The subsequent sections describe the potential for parallelism within these areas in more detail, examining translation to the clausal form (SS4), non-temporal resolution (SS5), simplification (SS6), temporal resolution (SS7), and the possibility of executing several of these elements in parallel (SS8).
Finally, in SS9, we present a summary and outline our future work in this area.
2 A Propositional Temporal Logic In recent years, temporal logics have been used for representing time-dependent information [3], for the specification and verification of reactive systems [17], and for direct execution [5].
Within the broad category of temporal logics, a range of different models of time have been utilised, including discrete, dense, linear, branching and interval-based models [10].
In this paper, we will consider a discrete, linear model of time, and will outline a propositional temporal logic based on this model.
This logic is simply called PTL.
Rather than give the syntax and semantics of the full logic, we will present a brief description of the elements of the language, together with their semantics, required for representing the clausal resolution proof method [11].
2.1 Syntax of PTL The set of well-formed formulae of PTL (WFFp ) is defined as follows.
* Any proposition symbol is in WFF p .
* If A and B are in WFFp , then so are !A A  }  A[?
]B A  A[?
]B wA  A=B gA f e d c  AW B  Here, the temporal operators used consist of the futuretime temporal operators, ' ' and ' ', and the past-time g'.
f e temporal operators, ' w' and ' cd  }  2.2 Semantics of PTL Intuitively, the models for PTL formulae are based on discrete, linear structures having a finite past and infinite future, i.e.
sequences such as s0 , s1 , s2 , s3 , ..., where each si , called a state, provides a propositional valuation.
We can visualise this as representing a sequence of 'moments' in time.
Thus, the semantics of a proposition is defined by the valuation given to it at a particular state.
While the semantics of the standard propositional connectives are as in classical logic, the semantics of the future time temporal operators are as follows: A is satisfied at a particular state if A is satisfied at some state in the future; A is  }  satisfied at a particular state if A is satisfied at all states in the future; A W B is satisfied at a particular state if A is satisfied unless a state where B is satisfied occurs.
We are also able to refer to properties in the past.
As temporal formulae are interpreted at a particular stateindex, i, then indices less than i represent states that are 'in the past' with respect to state si .
The two past-time operators have similar semantics but, as there is a unique start state, termed the beginning of time, they have slightly different behaviour when interpreted at this state.
Thus, gA are satisfied at a particular state if A is f e both wA and cd satisfied at the previous state, but wA is satisfied, while gA is not, when interpreted at the beginning of time.
In f e cd particular, wfalse is only satisfied when interpreted at the beginning of time.
Note that the following equivalence relates these two last-time operators.
g!A f e cd  = !
wA  Although this implies that only one of these operators need be defined, we will see later that it is useful, particularly when defining the normal form, to utilise both operators.
Note that a variety of other temporal operators are available but, as these will be represented within the normal form discussed in the next section simply in terms of the above operators, we will omit the definitions of these derived operators.
2.3  Proof Methods for PTL  Discrete temporal logic essentially characterises the combination of a classical logic system with a minimal form of induction.
Decision procedures for such temporal logics are correspondingly more complex, being PSPACE complete in the worst-case.
Several proof methods for propositional discrete temporal logics, such as PTL, have been developed, the most widely used being semantic tableau or automata-based methods [23, 12].
More recently, proof methods have been developed based on a translation to classical logics [18] and on resolution, both clausal [11] and non-clausal [1].
Several of these proof techniques, particularly clausal resolution and semantic tableau, show potential for parallelisation.
The parallelisation of the temporal tableau method is discussed in [14], while the parallelisation of the resolution method is the subject of this paper.
3  Clausal Temporal Resolution  The temporal resolution system that we investigate here is presented in [11].
The basic idea behind it is to translate an arbitrary PTL formula into a set of formulae in a normal form.
This normal form, which corresponds to clausal form in classical resolution, is called Separated Normal Form (SNF).
Formulae in SNF are of the form  ^(P n  i  i=1  = Fi ) .
Here, each Pi is a strict past-time temporal formula and each Fi is a non-strict1 future-time formula.
Each of the 'Pi = Fi ' (called rules) is further restricted to the following:  _q m  wfalse  ^p  =  l  g f e cd  b  (an initial  b  (a global  _q m  a  =  a=1  -rule)  b=1  wfalse  =  }s  (an initial  ^p  =  }s  (a global  l  g f e cd  -rule)  b=1  a  }-rule)  }-rule)  a=1  where each pa , qb or s is a literal.
Once the translation to SNF has been carried out, then all temporal statements within PTL are represented as sets of such rules.
Thus, if we are to derive a contradiction, there are effectively only two ways to achieve this, given a set of SNF rules.
The first is to apply resolution within a state (i.e., between -rules), the second is to apply resolution to -rules.
The former is equivalent to classical resolution within a state, the latter involves a search for sets of rules which, together, represent a -formula that complements the selected -formula.  }
}  3.1 Sequential Resolution Method Given an arbitrary temporal formula, ph , that we wish to show is unsatisfiable, the clausal temporal resolution method [11] proceeds through the following steps: 1. rewrite ph into Separated Normal Form (SNF), giving a set of rules phs (note that this transformation preserves satisfiability); 2. perform non-temporal (effectively classical) resolution on pairs of rules within ph s -- if false is derived, terminate, noting that ph is unsatisfiable, otherwise continue; 3. perform simplification and subsumption; 4. choose an eventuality that appears in the set of SNF rules, e.g., !p, and look for sets of rules forming "loops in p" (i.e., sets of rules representing ' p'), add resolvents (rewritten into SNF) for all the loops found;  }  5. if any new formulae are generated, rewrite them into SNF, add them to phs and go to 2; 6. terminate, noting that ph is satisfiable.
Before examining the potential for parallelism in each step of the proof process, we will consider which steps involve the greatest complexity.
1  Here, 'non-strict' means "including the present".
3.2 Complexity of the Procedure The complexity of the current (sequential) resolution procedure [9] is: * translation to SNF -- although used many times throughout the proof process, this transformation is not expensive, usually involving only a linear increase in the length of formula and a linear increase in the number of symbols; * non-temporal resolution -- as in classical case, i.e.
NP-complete; * temporal resolution -- exponential for each formula.
In practice, the translation to SNF is relatively quick, the non-temporal resolution step can be slow, while the temporal resolution step is usually slow.
As this suggests, the main effort has involved investigating the parallelisation of the temporal resolution step.  }
3.3 Opportunities for Parallelism There are several places in the sequential algorithm where parallel techniques can (possibly) be exploited: 1. in the translation from an arbitrary formula to SNF; 2. in the non-temporal resolution step; 3. during simplification and subsumption; 4. in the application of temporal resolution to distinct eventualities; 5. when searching for loops.
We will consider the potential for parallelising these parts of the process in the subsequent sections2 .
In SS8, we consider the coarser-level parallelism available by executing some of these components in parallel.
4  Conversion to SNF  Although parallelisation is possible in the conversion of an arbitrary formula into SNF, we note that 1. this translation is relatively efficient anyway, and, 2. even if parallel techniques are applied, the bottleneck of ensuring that instances of the same formula are identically renamed, is a (potentially severe) restriction upon parallelism.
Thus, we will not consider the parallelisation of this step (although, in the journal paper, we will outline approaches to this problem).
5  Non-Temporal Resolution  Non-temporal resolution is a variation on classical resolution, being given as the rule: gA f e cd gB f e cd  g(A [?]
B) f e cd  = F[?]
l = G [?]
!l = F[?
]G  2 Apart from (4), which will be considered in more detail in the journal version of this paper.
The exploitation of parallelism is possible here and, indeed, many systems have been developed with this in mind.
This enables us to utilise the parallel resolution schemas that have been examined by others for classical logics.
For example, the following elements have already been developed within parallel resolution systems.
7  Temporal Resolution  The general temporal resolution rule, written as an inference rule, can be described as gA f e cd  wtrue  * The generation of new literals (ROO [16]).
* OR-parallelism with spanning sets (DelPhi [8]).
* OR+AND parallelism (the Andorra model [22]).
* OR-independent AND parallelism (PEPSys [4]).
* Pipelined parallelism (POPE [6]).
Rather than describe any of these options in more detail, we simply note that our non-temporal resolution step is able to utilise such advances in parallel resolution for classical logic.
We will give an agent-oriented (see SS8) description of such non-temporal resolution in the journal paper.
gP = falseg - f e Simp3 : f cd    = !A [?]
!Q  where 'L is a last-time operator.
Here, the first resolvent shows that both A and Q cannot be satisfied together, while the second that once Q has occurred then !p must occur (i.e.
the eventuality must be satisfied) before A can be satisfied.
The full temporal resolution rule is given by expanding gA = f e the ' cd p' rule into its constituent parts composed of global -rules, as follows.
gA f e cd  0 = F0 ... ... gA f e cd n = Fn LQ = !p  }  wtrue  6.1 Simplification The simplification rewrite rules used in the resolution method for PTL are: Simp2 : fP = trueg - fg;  p  }!p  LQ = (!A) W (!p)  6 Simplification and Subsumption  gfalse = Ag - fg; f e Simp1 : f cd  =  LQ =  = !Q [?]
LQ = (  ^ !A n  i  ^ !A ) W !p n  i=0 i  i=0  wfalse gtrue f e d c  fi = !P = !P  The first two remove valid formulae, while the third is the basic mechanism for transferring constraints between states.
Since the pattern of the left-hand sides of each of these three simplification rewrite rules is different we may concurrently apply each to the ruleset (R).
If we attribute a heterogeneous simple process to the application of each rule Simpi they may all operate in parallel on R. Furthermore homogeneous copies of each simplification process may execute concurrently.
Therefore we may (theoretically) utilise a potential 3 x |R| processors in the simplification process at this level of granularity.
6.2 Subsumption As in the classical case, a particular bottleneck for parallel resolution is in the subsumption checking for newly generated resolvents.
The formulation of processes for this portion of the algorithm can be tailored according to the required granularity.
We may combine all simplification and subsumption elements together.
Alternatively we may consider two processes, one dealing with simplification, and the other with subsumption.
In the journal paper, we will expand this section, outlining the finer structure of processes for simplification and subsumption (and actually defining them in terms of 'agents').
with side conditions that for all i such that 0 <= i <= n, 1.
` Fi = p, and, 2.
` Fi =  _A .
n  j  j=0  Thus, the side conditions ensure that each -rule makes p true and the right hand side of each -rule ensures that the left hand side of one of the -rules will be satisfied.
So if any of the Ai are satisfied then p will be always be satisfied, i.e.,  _A = n  g f e cd  k  p.  k=0  Such a set of rules are known as a loop in p. Thus, the temporal resolution step essentially consists of a search for a set of rules which together represent a -formula, complementary to the -formula to which the resolution is applied.
It is this search that is usually the most costly element of the whole resolution process.
Fortunately, it is also an element with a significant potential for parallelisation.
There are (currently) two general ways to implement the temporal resolution search, given a set of rules and a -formula.
1.
Merge all possible SNF rules to give SNFm rules3 , then search for strongly connected components (SCCs) within the graph defined by the SNF m rules [11].  }
}  3  SNFm is effectively a form of SNF where the right-hand side of -rules are in Disjunctive Normal Form (DNF), rather than just being a disjunction of literals.
2.
Attempt to lazily construct portions of the above SNFm structure directly from the SNF rules, searching for SCCs during this construction [9].
In the worst case, the second approach has the complexity of the first.
The complexity of the first approach is an exponential step (SNF - SNFm ) followed by a linear one (SCC detection using Tarjan's algorithm [2]).
We will term the first approach the naive search, and the second approach the lazy search.
7.1 Parallelising Naive Search The basic bottleneck in the naive search is the SNF to SNFm translation.
This involves conjoining all possible subsets of the set of SNF rules and rewriting the new rules' right-hand sides in to DNF.
Potentially many parallel processes can be employed in order to achieve this.
The main problem that occurs is that of merging all the generated sets of rules back in to one set (and removing subsumed rules).
One approach that may alleviate this is by using an intelligent merge, where the sets of rules generated are first sorted before merging.
We will not consider the parallelisation of the naive search in any more detail here, as it is the lazy search that has been the focus of development for the sequential proof method.
Its advantage in the sequential case, namely the minimal space consumed, ensures that this approach is also (initially) more desirable in the parallel case.
Thus, it is this lazy search that we concentrate upon.
7.2 Parallelising Lazy Search Within the lazy search approach, there are two basic algorithms: 1. depth-first search through the rule-set looking for a 'loop'; 2. breadth-first search through the rule-set looking for a 'loop'.
As in the sequential case, as the breadth-first algorithm detects all loops for a particular eventuality whereas the depth-first search described in [9] finds one at a time, we will predominantly consider the parallelisation of the former.
However, we should mention that, as the depthfirst search is an example of independent search with backtracking, we can utilise many of the techniques developed for standard OR-parallel search.
In particular multiple depth-first searches could be invoked in parallel, with the first successful process being the loop considered.
An important element of this would be the sharing of results between independent branches, which would avoid several searches 'discovering' the same loops.
Returning to the breadth-first algorithm, we first give a brief outline of its operation, assuming we wish to detect a loop in the literal p. The breadth-first algorithm builds a graph structure consisting of nodes labelled by formulae in disjunctive normal form (DNF).
When building each node we attempt to use all possible combinations of SNF rules that satisfy the expansion conditions rather than picking one and backtracking as in the depth-first algorithm.
The  top node, N0 , is the disjunction (and simplification), of the conjunction of literals appearing on the left-hand side of rules that ensure p in the next moment in time.
A new node Ni+1 is constructed from node Ni by using all possible SNFm rules where, for each rule, the conjunction of literals on the left-hand side implies the top node and whose right-hand side implies the previous node N i .
The former makes sure that the rule is guaranteed to give p in the next moment in time, and the latter is for constructing the looping we require.
Termination occurs either when the node we have just constructed is equivalent to the previous node (or equivalent to true), or when we have been unable to construct a new node.
If the former, then we have detected a loop, if the latter then we have not found a loop to resolve with the eventuality that we are considering.
More formally, we can describe the breadth-first loopsearch algorithm as follows.
For each rule of the form LQ = !p (where L is either of the last-time operators) do the following.
gT = p, (called f e 1.
Search for all the rules of the form cd i start rules), disjoin the left hand sides, simplify and make the top node N0 equivalent to this i.e.  }
N0 =  _T .
i  i  If ` N0 = true we terminate having found a loop.
2.
Given node Ni = Dk  _ k  where Dk is a conjunction of literals, to create node Ni+1 for i = 0, 1, ... look for rules or combinations of gA = B where ` B = N and f e rules of the form cd j j j i ` Aj = N0.
Disjoin the left hand sides so that Ni+1 =  _A  j  j  and simplify as previously.
3.
Repeat 2. until (a) ` Ni = true.
We terminate having found a breadth-first loop and return true.
(b) ` Ni = Ni+1 .
We terminate having found a breadth-first loop and return the DNF formula Ni .
(c) The new node is empty.
We terminate without having found a loop.
We next consider a range of strategies for parallelising this algorithm.
Although this list is not meant to be exhaustive, it does provide a basis for comparison between approaches.
Indeed the prototype implementations of some of these strategies have shown them to be relatively successful.
7.2.1 Strategy 1: Top-level partition of disjuncts This strategy essentially consists of examining the first node, partitioning the disjuncts within this node, and performing breadth-first search separately on each of these new nodes as normal.
Advantages If we pick a good subset of disjuncts we can produce a breadth-first graph for this subset that not only finds the loop, but has fewer nodes and contains fewer literals (just by totaling the number of occurrences of each literal in the graph) than the full graph.
Thus, if we can apply some heuristic to detect a good subset then we can detect a solution more quickly than with full breadth-first search.
Disadvantages If we pick a bad subset of disjuncts we may produce a breadth-first graph that finds the loop, contains the same number of nodes, but contains more literals (since we have had to carry out more combinations of SNF rules) than the full breadth-first graph.
If a loop is detected each solution (i.e., a DNF formula) will imply the solution from the full breadth-first graph.
However, even though this holds, we may obtain a less general solution than with the full breadth first search.
This means that if the less general solution does not give specific enough resolvents to produce a contradiction we may have to do another round of temporal resolution to obtain a more general solution.
Perhaps most importantly, we do not yet have a suitable heuristic for identifying good partitions, although work on developing heuristics to discard rules that will never form part of the loop (in the sequential algorithm) may be applicable here.
Also, since a smaller set of disjuncts is not guaranteed to lead to a solution, then, to retain completeness, we must always ensure that the full breadth-first search runs in parallel.
7.2.2 Strategy 2: Parallel node construction Here, we carry out a normal breadth-first search but, when constructing the next node, apply the expansion of each disjunct in parallel.
Advantages Recall that we are searching for combinations of rules where the right hand side implies the previous node, and the conjunction of literals on the left hand side implies the top node.
Thus, for each disjunct di,j in the previous node we look for sets of SNF rules to combine where each right hand side contains a literal in di,j .
We combine them so that every literal in d i,j is covered, making sure that the right hand side of the combined rule we have built does actually imply the previous node.
By expanding each disjunct separately we will obtain a DNF formula representing the disjunction of the left-hand side of the combined rules used to expand each di,j as long as the right-hand side of the combined rule has no disjunctions in it.
By carrying this out in parallel we will construct the (unsimplified) new node more quickly than in the sequential version.
Further, as the same disjunct may appear in several nodes, and therefore need expansion several times, by storing the DNF formula obtained by expanding a particular disjunct we can avoid repeating calculations to expand the same disjunct several times.
Disadvantages By expanding each disjunct in parallel we may merge the same combinations of rules more than once to cover two or more disjuncts when the combined rule we are applying has disjuncts on the right-hand side (i.e.
we may repeat work by looking at each disjunct separately).
Having expanded each disjunct in node Ni we may end up with several (large) DNF formulae which we must now disjoin and simplify to build node N i+1 .
In the sequential version simplification is carried out as we use a new rule to expand a node so that the DNF formula does not become excessively large.
7.2.3  Strategy 3: Search re-use  Here, the basic approach is to take some subset of the set of disjuncts from the top node (as strategy 1), try to build a breadth-first graph and, if we fail, add new disjuncts to the subset from the original top node and extend the graph, saving what we had before.
Advantages Appears to allow the incremental construction of the breadth-first graph by reusing parts of the graph that have previously been computed.
Disadvantages Unfortunately, in most practical cases we have to amend what has been saved so extensively it is just as costly as building the full breadth-first structure in the first place.
Specifically, when trying to reuse parts of the graph saved from a previously failed search we must check that 1. we have not overcombined the required rules, and, 2. we have combined the rules enough times.
This occurs simply due to the fact that we have added disjuncts to the top node and now have extra conjunctions of literals that will generate the required literal.
However, there seems to be no way to easily test whether, for the disjuncts already constructed, it was necessary to add in more disjuncts, "uncombine" them, or just leave them as they were.
A further disadvantage is that, while the basic breadthfirst search algorithm only requires that the top and previous nodes be kept, this strategy requires that all the constructed nodes be kept for re-use.
Thus, the use of this approach in practice would have major storage implications.
In summary, strategy 2 seems to have the most potential, strategy 1 also seems worth pursuing, while strategy 3 appears to have too many drawbacks.
In fact, the most productive approach may be to use a combination of the first two strategies.
In the journal paper we will include a more detailed comparison of these approaches, as well as relative timings for specific examples.
8 Combining Agents The temporal resolution algorithm contains a number of opportunities for parallelisation.
These opportunities vary in their granularity and impact.
We can examine the potential parallelism in the algorithm by deconstructing it in order to extract the fundamental operations involved, and to identify any necessary inter-dependencies between components.
Following deconstruction the fundamental operations may be composed in order to carry out the same function as the original algorithm.
However, the aim of the deconstruction is to liberate parallelism wherever possible by identifying the parts of the algorithm that may execute in parallel without affecting the overall soundness or completeness of the algorithm.
In the journal paper we derive the collection of finegrained subtasks (fundamental operations) that compose the steps of the algorithm.
Each subtask has a processing object associated with it.
We shall refer to these objects as simple agents.
We subsequently reformulate the steps of the algorithm by providing cohesive arrangements of simple agents that reflect the coarser-grained processes evident in the algorithm.
We refer to these coarser-grained objects as process agents.
Given differing arrangements of simple agents and process agents we may reflect the potential parallelisations available to us with varying task grain sizes.
We will describe the constraints and necessary synchronisation points (dependencies) in the algorithm that inhibit parallelism.
These dependencies will generally affect the soundness or the completeness of the proof method and hence must not be disregarded during the reformulation process.
We will deconstruct the steps of the algorithm with a view to identifying subtasks that may be carried out by simple agents executing in parallel.
When considering forms of parallelism it is useful to consider the activities of homogeneous and heterogeneous agents.
Heterogeneous agents carry out different tasks in a domain in parallel, whereas homogeneous agents carry out the same task but on different data.
These agent schemas may be viewed (broadly speaking) as mechanisms for describing control and data parallel activities respectively.
Finally, we note that pipelined parallelism may be employed between several of the steps, for instance new constraints produced by temporal resolution may be rewritten as soon as they are derived.
Similarly there is no reason to wait for the completion of temporal resolution before commencing non-temporal resolution operations with the rewritten rules produced previously.
9 Conclusions and Future Work Parallelism seems to provide the potential for efficiently implementing temporal reasoning methods.
While we have outlined some of the opportunities for parallelism within one particular proof method, it is important to note that, as there has been very little work in the specific area of parallel temporal theorem proving, it is not yet clear what type of parallel architecture is best suited for this  application.
It appears that some implementation of the system on different machines will be necessary to determine the "best fit".
In the longer paper we present a decomposition of the approach we describe here such that component parts of the proof process are identified to enable the reconstruction of the algorithm as an agent-based design.
9.1 Future Work Obvious future work includes the further investigation of alternative strategies for parallelising both the temporal resolution method as a whole, and the temporal resolution step within this.
The approaches outlined within this paper are currently being developed and implemented -- work will continue on these.
In the journal paper, a deeper comparison of the approaches outlined here will be presented.
This will incorporate both correctness and efficiency issues.
As several of the graph search algorithms described in SS7 have already been implemented, comparative timings will be provided.
We also hope to utilise advances in state-space search [20], in order to improve the tree-based search procedures, and parallel graph algorithms, such as [19], in order to improve the graph-based search procedures.
Finally, we note that, developing efficient parallel algorithms is a difficult task.
For example, it is sometimes the case that algorithms considered naive in the sequential case turn out to be optimal given the correct parallelisation.
Thus, it is possible that the algorithms considered unsuitable for temporal theorem-proving may be extended and, possibly, generalised, in order to develop appropriate parallel versions.
Acknowledgements This work was partially supported by a SERC Research Studentship and under SERC Research Grant GR/J48979.
References [1] M. Abadi and Z.
Manna.
Nonclausal Deduction in First-Order Temporal Logic.
ACM Journal, 37(2):279-317, April 1990.
[2] A. Aho, J. Hopcroft, and J. Ullman.
The Design and Analysis of Computer Algorithms.
Addison-Wesley, 1974.
[3] J. Allen and P. Hayes.
A Common Sense Theory of Time.
In Proceedings of the International Joint Conference on Artificial Intelligence, pages 528-531, Los Angeles, California, August 1985.
[4] U. Baron, J. de Kergommeaux, M. Hailpern, M. Ratcliffe, M. Roberts, J-Cl.
Syre, H. Westphal.
The parallel ECRC Prolog System PEPSys: An Overview and Evaluation Results.
In Future Generation Computing Systems, 1988.
[5] H. Barringer, M. Fisher, D. Gabbay, G. Gough, and R. Owens.
METATEM: A Framework for Programming in Temporal Logic.
In Proceedings of REX Workshop on Stepwise Refinement of Distributed Systems: Models, Formalisms, Correctness, Mook, Netherlands, June 1989.
(Published in Lecture Notes in Computer Science, volume 430, Springer Verlag).
[6] J.
Beer and W. Giloi.
POPE - A Parallel Operating Prolog Engine.
In Future Comp.
Systems, 3:83-92, 1987.
[7] R. Butler, I.
Foster, A. Jindal, and R. Overbeek.
A High-Performance Parallel Theorem Prover.
Lecture Notes in Computer Science, 449:649-650, 1990.
[8] W. Clocksin.
Principles of the DelPhi Parallel Inference Machine.
In Computer Journal vol.31, no.3, 1987.
[9] C. Dixon, M. Fisher, and H. Barringer.
A graphbased approach to temporal resolution.
In First International Conference on Temporal Logic (ICTL), July 1994.
[10] E. Emerson.
Temporal and Modal Logic.
In J. van Leeuwen, editor, Handbook of Theoretical Computer Science, pages 996-1072.
Elsevier, 1990.
[11] M. Fisher.
A Resolution Method for Temporal Logic.
In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence (IJCAI), Sydney, Australia, August 1991.
Morgan Kaufman.
[12] G. Gough.
Decision Procedures for Temporal Logic.
M.Sc.
Thesis, Department of Computer Science, University of Manchester, U.K., October 1984.
[13] R. Johnson.
Concurrent Theorem Proving with Tableaux and the Connection Method using Strand over a Distributed Network.
technical report 538, Department of Computer Science, Queen Mary and Westfield College, University of London, July 1991.
[14] R. Johnson.
A Blackboard Approach To Parallel Temporal Tableaux.
In Proceedings Artificial Intelligence, Methodologies, Systems, and Applications (AIMSA), World Scientific, 1994.
[15] F. Kurfess.
Parallelism in Logic.
Vieweg, 1991.
[16] E. Lusk and W. McCune.
Experiments with ROO: A Parallel Automated Deduction System In Proceedings of Parallelization in Inference Systems, International Workshop.
Springer-Verlag.
1990.
[17] Z.
Manna and A. Pnueli.
The Temporal Logic of Reactive and Concurrent Systems: Specification.
Springer-Verlag, New York, 1992.
[18] D. Plaisted and S-J.
Lee.
Inference by clause matching.
In Z. Ras and M. Zemankova, editors, Intelligent Systems, chapter 8, pages 200-235.
Ellis Horwood, Chichester, England, 1990.
[19] V. Ramachandran and J. Reif.
An Optimal Parallel Algorithm for Graph Planarity.
In Proceedings of the 30th Annual IEEE Symposium on Foundations of Computer Science, pages 282-287, 1989.
[20] V. Saletore and L. Kale.
Consistent Linear Speedups to a First Solution in Parallel State-Space Search.
In Proceedings of the Eighth National Conference on Artificial Intelligence (AAAI), pages 227-233, Boston, Massachusetts, 1990.
MIT Press.
[21] J. Schumann and R. Letz.
PARTHEO: A HighPerformance Parallel Theorem Prover.
Lecture Notes in Computer Science, 449:40-56, 1990.
[22] D. Warren.
The SRI model for OR-parallel execution of Prolog-abstract design and implementation issues.
In International Symposium on Logic Programming, pp92-102, 1987.
[23] P. Wolper.
The Tableau Method for Temporal Logic: An overview.
Logique et Analyse, 110-111:119- 136, June-Sept 1985.
Formalizing Actions in Branching Time: Model-Theoretic Considerations Munindar P. Singh  Microelectronics and Computer Technology Corporation 3500 W. Balcones Center Drive Austin, TX 78759-5398 USA msingh@mcc.com  Abstract  The formalization of actions is essential to AI.
Several approaches have been proposed over the years.
However, most approaches concentrate on the causes and effects of actions, but do not give general characterizations of actions themselves.
A useful formalization of actions would be based on a general, possibly nondiscrete, model of time that allows branching (to capture agents' choices).
A good formalization would also allow actions to be of arbitrary duration and would permit multiple agents to act concurrently.
We develop a branching-time framework that allows great exibility in how time and action are modeled.
We motivate and formalize several coherence constraints on our models, which capture some nice intuitions and validate some useful inferences relating actions with time.
1 Introduction  Actions and time have drawn much attention in arti	cial intelligence (AI).
Whereas much progress has been made in modeling time, corresponding progress has not been made in modeling actions.
Temporal approaches run the gamut from discrete to continuous, point-based to interval-based, and linear to branching.
By contrast, approaches to formalizing actions tend to be restricted to discrete models, typically linear and with additional assumptions such as that exactly one action happens at a time, and all actions have the same duration.
Reasoning about actions focuses on the possible causes and e ects of the actions, but not on their structure.
This work is undoubtedly of value.
However, we submit that its full potential can be realized only if actions themselves are formalized in a general framework.
What are the properties that intuitively acceptable and technically feasible models of actions and time should support?
We  address this question here.
A general model of actions would provide the underpinnings for work on concepts|such as intentions, ability, and know-how|that supervene on actions.
When actions are modeled restrictively, the technical results obtained on the above concepts end up with potentially superuous or even pernicious restrictions.
It is easy to obtain spurious results on the above concepts that rely on some irrelevant aspect of the underlying model of actions Singh, 1992].
Our main interest is in formalizing the above concepts, but we must model actions properly to succeed Singh, 1994].
We introduce this formalization to the temporal representation community here.
Our framework allows (a) time to branch to model agents' choices, (b) multiple agents to act simultaneously, (c) actions to be of varying durations relative to one another, and (d) time to be nondiscrete.
Thus choice and control can be properly captured in this framework.
Our approach is nonrei	ed Bacchus et al., 1989], as is common in the non-AI literature Emerson, 1990].
Time can variously be modeled as linear or branching.
Allen presents an interval-based linear-time theory of actions in 1984].
Turner (p. 88) and Shoham 1988, ch.
2] show that Allen's theory is not conceptually clear, especially with regard to intervals.
Shoham too restricts his models to be linear.
Allen (p. 131) and Shoham (p. 36) argue that branching time is unnecessary since the agents' ignorance can be modeled in other ways.
However, branching into the future is not a matter of ignorance, but of choice.
That is why, ignorance apart, the past can be linear but the future must branch.
(Sometimes, eciency may be gained by treating even the past as branching: we allow this.)
Galton 1990] improves over Allen's approach in some respects but does not address constraints on actions per se.
McDermott's approach, like ours, is point-based and involves branching-time 1982].
But, McDermott requires his models to be dense also, clock values are essential to his semantics.
McDermott notes, correctly, that an action cannot hap-  pen over overlapping intervals: we capture this differently.
But a lot more must be said that his and other approaches do not say.
Related research includes Thomason & Gupta, 1981 van Frassen, 1981 Haddawy, 1990 Dean & Boddy, 1988], but it does not model actions as motivated here.
We present our formal language and model next and discuss our key operators informally.
Then we motivate and formalize a number of coherence constraints on our models that are required for various useful properties.
We use these to prove some important results relating actions and time.
We close with a discussion of some open problems.
2 Technical Framework  The proposed formal model is based on a set of moments with a strict partial order, which denotes temporal precedence.
Each moment is associated with a possible state of the world, which is identi	ed by the atomic conditions or propositions that hold at that moment.
A scenario at a moment is any maximal set of moments containing the given moment, and all moments in its future along some particular branch.
 .. .. ..     q X HX  HX  XX .
.
.
t 1 HX  H   q .
.
.
HH .
.
.
a k c  a k d t2    XHXHXXX b k c  qq .. .. ..   t0 HH XXX   HHH XXXr X HHXXXX q .
.
.
HH t3HX bkd HH q .
.
.
HHH q .
.
.
t4  does action a, then whether t1 or t2 becomes the case depends on whether the second agent does c or d. Intuitively, actions are package deals.
They correspond to the granularity at which an agent can make his choices.
In Figure 1, the 	rst agent can choose between t1 and t2, on the one hand, and between t3 and t4 , on the other hand.
However, he cannot choose between t1 and t2 , or between t3 and t4 .
Both choice and limited control are thus captured.
2.1 The Formal Language  We use a qualitative temporal language, L, based on CTL* Emerson, 1990].
This captures the essential properties of actions and time that are of interest.
Formally, L is the minimalset closed under the following rules.
Here L is the set of \scenario-formulae," which is used as an auxiliary de	nition.
is a set of atomic propositional symbols, A is a set of agent symbols, B is a set of basic action symbols, and X is a set of variables.
L1.
 2  implies that  2 L L2.
p Wq 2 L and a 2 B implies that p ^ q, :p, Pp, ( a : p) 2 L s  LL L4.
p q 2 L , x 2 A, and a 2 B implies that p ^ q, :p, pUq, xa]p, xhaip, xjhaijp 2 L L5.
p 2 L implies that Ap 2 L W L6.
p 2 (L ; L) and a 2 X implies that ( a : p) 2 L 2.2 The Formal Model A model for L is a four-tuple, M = (T < A  ] ).
Here T is a set of possible moments ordered by <.
A assigns agents to di erent moments i.e., A : T 7!
}(A).  ]
is described below.
The relation < is a L3.
s  s  s  s  s  s  strict partial order:  	 Transitivity: (8t t  t 2 T : (t < t ^ t < t )) t < t ) 	 Asymmetry: (8t t 2 T : t < t ) t 6< t) 	 Irreexivity: (8t 2 T : t 6< t) 0  00  Figure 1: The Formal Model Figure 1 shows a schematic picture of the formal model.
Time may branch into the future and, in any interesting application, does.
It may be taken as linear in the past, although nothing hinges upon this.
The agents' ignorance about the past, as about anything else, is captured by beliefs (not discussed here).
Each agent inuences the future by acting, but the outcome also depends on other events.
Figure 1 is labeled with the actions of two agents.
The 	rst agent can constrain the future to some extent by choosing to do action a or action b.
If he does a, then the world progresses along one of the top two branches out of t0  if he does b, then it progresses along one of the bottom two branches.
However, the agent cannot control what exactly transpires.
For example, if he  00  0  0  00  0  0  0  A scenario at a moment t is any single branch of the relation < that includes t and all moments in some future of t that is a linear subrelation of <.
Di erent scenarios correspond to di erent ways in which the world may develop, as a result of the actions of agents and events in the environment.
Formally, a scenario at t is a set S  T that satis	es the following.
Rootedness: t 2 S 	 Linearity: (8t  t 2 S : (t = t ) _ (t < t ) _ 0  (t < t )) 00  00  0  00  0  00  0  	 Relative Density: (8t  t 2 S t 2 T : (t < t < t )) t 2 S ) 0  000  00  000  00  000  0  	 Relative Maximality: (8t 2 S t 2 T : (t < t )) (9t 2 S : (t < t ) ^ (t 6< t ))) 0  00  000  0  00  000  000  0  00  It is possible to extend S (here to t ), then it is extended, either to t (when t = t ), or along some other branch.
By itself, this does not entail that time be eternal.
S is the set of all scenarios at moment t. Since each scenario at a moment is rooted at that moment, the sets of scenarios at di erent moments are disjoint, that is, t 6= t ) S \ S = .
If t is such that t < t , then for every scenario, S 2 S , there is a scenario, S , such that S  S and S 2 S .
Conversely, for every scenario S 2 S , for each moment t 2 S , there is a scenario S 2 S , such that S  S .
S  t t ] denotes a period on scenario S from t to t , inclusive, i.e., the subset of S from t to t .
Thus, if S0 t t ]  S1 , then S0  t t ] = S1  t t ].
However, in general, S0 t t ] 6= S1 t t ].
For notational simplicity, S  t t ] presupposes t t 2 S and t  t .
00  00  000  00  t  0  t0  t  0  0  0  t0  0  t  t  0  t0  0  0  0  0  0  0  0  0  0  0  0  0  0  2.3 Semantics For p 2 L, M j= p expresses \M satis	es p at t." For p 2 L , M j= p expresses \M satis	es p at moment t on scenario S " (we require t 2 S ).
We say p is satisable i  for some M and t, M j= p. The t  s  St  t  satisfaction conditions for the temporal operators are adapted from those in Emerson, 1990].
It is assumed that each action symbol is quanti	ed over at most once in any formula.
Below, pj is the formula resulting from the substitution of all occurrences of a in p by b.
Two useful abbreviations are false  (p ^ :p), for any p 2 , and true  :false.
Formally, we have: M1.
M j=  i  t 2  ] , where  2  M2.
M j= p ^ q i  M j= p and M j= q M3.
M j= :p i  M 6j= p M4.
M j= Ap i  (8S : S 2 S ) M j= p) M5.
M j= Pp i  (9t : t < t and M j= p) W M6.
M j= ( a : p) i  (9b : b 2 B and M j= pj ), where p 2 L W M7.
M j= ( a : p) i  (9b : b 2 B and M j= pj ), where p 2 (L ; L) q and M8.
M j= pUq i  (9t : t  t and M j= (8t : t  t  t ) M j= p)) M9.
M j= xa]p i  (8t 2 S : S  t t ] 2  a] implies that (9t : t < t  t and M j= p)) M10.
M j= xhaip i  (9t 2 S : S  t t ] 2  a] and (9t : t < t  t and M j= p)) M11.
M j= xjhaijp i  (9t 2 S : S  t t ] 2  a] and (9t : t < t  t and (8t : t < t  t implies that M j= p))) M12.
M j= p ^ q i  M j= p and M j= q M13.
M j= :p i  M 6j= p M14.
M j= p i  M j= p, where p 2 L a b  t t  t  t  t  t  t  t  0  t  St  0  t0  t  : c :  00  x  x  0  0  0  St  St  St 00  0  0  St  x  0  0  00  0  St0  St00  0  St  Figure 2: Actions: Nonsynchronized and of Varying Durations Basic actions can be of arbitrary durations.
Multiple agents may act simultaneously.
The set of actions available to an agent can be di erent at di erent moments.
For example, the actions of moving a block may take more or less time than the action of turning a knob.
This case is diagramed in Figure 2, which also shows that actions may begin and end arbitrarily.
The intension,  ] , of an atomic proposition is the set of moments at which it is true.
The intension of an action symbol a is, for each agent symbol x, the set of periods in which an instance of a is performed by x.
Thus t 2  p] means that p is true at moment t and, S  t t ] 2  a] means that agent x is performing action a from moment t to moment t .
When S  t t ] 2  a] , t corresponds to the ending of a, but t does not correspond to the initiation of a.
This is because a may already be in progress before t. Constraints C1 and C2 of section 3 pertain to this aspect.
All basic actions take time.
That is, if S  t t ] 2  a] , then t < t .
a b  s  00  b = move a block a = turn a knob  0  St  a b  b 9 : 9 9 a XXyXXXXX XyXXXXb XXXXX zXXX yXXXXXXXXXX a XXX z XXXXX XdX zX X  t  00  0  00  0  St00  0  00  x  x  0  0  St00  0  00  0  0  000  x  000  00  St000  St  St  St St  St  St  t  2.4 Temporal and Action Operators: Discussion  pUq is true at a moment t on a scenario, i  q holds at a future moment on the given scenario and p holds on all moments between t and the selected occurrence of q. Fp means that p holds sometimes in the future on the given scenario and abbreviates trueUp.
Gp means that p always holds in the future on the given scenario it abbreviates :F:p. Pp denotes p held at  some moment in the past.
The branching-time operator, A, denotes \in all scenarios at the present moment."
Here \the present moment" refers to the moment at which a given formula is evaluated.
A useful abbreviation is E, which denotes \in some scenario at the present moment."
In other words, Ep  :A:p. For example, in Figure 1, EFr and AFq hold at t0 , since r holds on some  moment on some scenario at t0 and q holds on some moment on each scenario.
L also contains operators on actions.
These are based on operators in dynamic logic, but are given a linear rather than a branching semantics.
For an action symbol a, an agent symbol x, and a formula p, xa]p holds on a given scenario S and a moment t on it, i , if x performs a on S starting at t, then p holds at some moment while a is being performed.
The formula xhaip holds on a given scenario S and a moment t on it, i , x performs a on S starting at t and p holds at some moment while a is being performed.
These de	nitions require p to hold at any moment in the (left-open and right-closed) period in which the given action is being performed.
Thus they are weaker than possible de	nitions that require p to hold at the moment at which the given action completes.
It is essential to allow the condition to hold at any moment in the period over which the action is performed.
This is because we are not assuming that time is discrete or that all actions are of equal durations and synchronized to begin and end together.
Intuitively, if we insisted that the relevant condition hold at the end of the action, then an agent could e ectively leap over a condition.
In that case, even if a condition occurs while an action is performed, we may not have xhaip.
For example, if p is \the agent is at the equator," and the agent performs the action of hopping northwards from just south of the equator, he may end up north of the equator without ever (of	cially) being at it.
That would be quite unintuitive.
For this reason, the present de	nitions are preferred although as a consequence, the operators h i and  ] are not formal duals of each other.
But this is made up for by having a more intuitive set of de	nitions, which also enable the right relationship between the action operators and F, G, and U to be captured.
Recall from above that pUq considers all moments between the given moment and the 	rst occurrence of q, not just those at which di erent actions may end.
Further, xjhaijp holds on a scenario S and moment t if x performs action a starting at t and p holds in some initial subperiod of the period over which a is performed.
This operator is necessary to relate actions with time for the following reason.
We allow actions to happen over periods which contain moments between their endpoints.
Such cases can arise even in discrete models if all actions are not unit length.
Consequently, if a is performed at t and q holds at an internal moment of a and p holds throughout, then pUq holds at t. But absent the jh ij operator, we cannot characterize pUq recursively in terms of actions.
One useful characterization is given in section 4: this helps in giving the 	xed point semantics of the temporal operators, which is essential to computing them eciently.
The above action modalities yield scenarioformulae, which can be combined with the branchingtime operators A and E. Axa]p denotes that on all  scenarios S at the present moment, if a is performed on S , then p holds at some moment on S between the present moment and the moment at which a is completed.
Similarly, Exhaip denotes that a is being performed on some scenario at the present moment and that on this scenario p holds at some moment between the present moment and the moment at which a is completed.
In other words, Axa]p corresponds to the necessitation operator and Exhaip to the possibility operator in dynamic logic.
Existential quanti	cation over basic actions is a useful feature.
Of the several basic actions that an agent may do at a given moment, we would often like to talk restrictively of the subset of actions that have some interesting property.
Indeed, we need something like this to formally express the idea of choice: an agent may be able to do several actions, but would, in fact, choose to do one, e.g., one of those that ensure success.
3 Coherence Constraints  For the above models to be coherent and useful, further technical constraints are required.
These are motivated and formalized below.
z z  t0 t1  a }|  }|a  { {  t3  t2  Figure 3: Case Disallowed by Action Uniqueness (1) z  t0  }|a  {  t1  z  a}|  t2  {  t3  Figure 4: Case Disallowed by Action Uniqueness (2) C1.
Uniqueness of Termination of Actions:  Starting at any given moment, each action can be performed in at most one way on any given scenario.
In other words, for any action a, scenario S , and moments t0  t1 t2 t3 in S , we have that S  t0 t2] 2  a] and S  t1 t3] 2  a] implies that, if t0  t1 < t2 , then t2 = t3.
This is needed to exclude ill-formed models in which an action does not have a unique moment of ending (see Figures 3 and 4).
If an agent performs an action and then repeats it, the repetition counts as a separate instance, because it has a distinct starting moment.
This constraint permits di erent actions with possibly distinct endpoints to happen simultaneously.
In discrete models with unit length actions, both endpoints are necessarily unique here only the termination point is assumed to be unique.
a }|  z  t  ...  t  00  z  }|  in Figure 6, would allow a condition to be inevitable and yet unreachable though any 	nite sequence of actions.
It is important that this not be the case for inevitability to relate properly with know-how.
This constraint always holds in discrete models.
{  a  {  t  ... S  0  Figure 5: Actions in Progress  Actions in Progress: It helps in relating moments with actions to require that S  t t ] 2  a] ) (8t : t  t < t ) S  t  t ] 2  a] ).
This allows us to talk of an agent's actions at any moment at which they are happening, not just where they begin.
However, in accordance with constraint C1, actions begun at a moment still have a unique ending moment.
As a result of this constraint, the actions operators behave properly.
For example, if an agent can achieve a condition by performing some action, then he can also achieve it while in the process of performing that action (until it happens).
This constraint holds vacuously in discrete models.
Figure 5 shows how this constraint causes the intension of an action to be 	lled out by suxes of the period over which it is performed.
The period S  t  t ] is not added to  a] , since that would lead to a violation of our assumption that S  t t ] 2  a] implies that t < t .
This would cause ambiguity between an action instance ending at t and another beginning there.
C3.
Passage of Time: Something must be done by each agent along each scenario in the model, even if it is some kind of a dummy action.
This assumption ensures that time does not just pass by itself, and is needed to make the appropriate connections between time and action.
Formally, (8t 2 T x 2 A(t) S 2 S ) ((9t 2 S )) (9t 2 S a : S  t t ] 2  a] ))).
C2.
00  0  0  00  0  0  0  0  0  0  t  x  0  t  t  0  0  t  ... S  00  Figure 6: Limit Sequences Disallowed by Reachability of Moments C4.
S0  0  00  Reachability of Moments: For any scenario and two moments on it, there is a 	nite number of actions of each agent that, if performed on that scenario starting at the 	rst moment, will lead to a moment in the future of the second moment.
Formally, (8S : (8t t 2 S : t < t ) (9t : t  t and (9a1  .
.
.
 a and S  t t ] 2  a1 .
.
.
 a ] )))).
This condition is intended to exclude models in which there are moments that would require in	nitely long action sequences to reach.
Such models, e.g., as 0  0  00  00  0  00  n  n  t  tX 0    a  X-XXXX XXXXX t1 S1  Figure 7: Actions Cannot be Partially Performed on any Scenario  Atomicity of Basic Actions: If an agent is performing an action over a part of a scenario, then he completes that action on that scenario.
This makes sense since the actions in the model are basic actions, performed with one atomic choice by their agent.
If an action in some domain can in fact be chopped into a pre	x and sux such that the sux is optional, then it should be modeled as two separate basic actions, the 	rst of which completes entirely and the second of which may not be begun at all.
Formally, let t t  t1 2 T, such that t < t < t1.
Let S0  S1 2 S , such that S1  t t ] 2 S0 .
Then S1 t t1] 2  a] implies that (9t0 2 S0 : S0 t t0] 2  a] ).
Intuitively, S1 t t1] 2  a] means that x is performing a from t to t1.
Therefore, he must be performing a in any subperiod of that, including S1 t t ], which is the same as S0  t t ].
Thus, a must be completed on S0 .
Higher-level actions do not satisfy this.
For example, Al may be crossing the street (on a scenario) even if he did not cross it successfully on that scenario, e.g., by being run over by a bus.
Our models represent physical systems, albeit nondeterministic ones.
The actions available to the agents and the conditions that hold on di erent scenarios leading from a given state are determined by that state itself.
Constraints on agent's choices, abilities, or intentions can thus be exibly modeled.
A well-known alternative characterization of models of time is by the set of all scenarios at all states.
We relate moments and states as follows.
De	ne a relation  to indicate the state-equivalence of moments and periods.
The state at a moment is precisely characterized by the atomic propositions that hold at that moment.
For moments, t and t , we de	ne t  t i  f 2 jt 2  ] g = f 2 jt 2  ] g. For sets of C5.
0  0  t  0  x  x  x  0  0  0  0  0  moments, L and L , we de	ne L  L in terms of an order-isomorphism, f .
Given two sets L and L with an order <, a map f from L to L is an orderisomorphism i  (a) f is onto, (b) (t 2 L i  f (t) 2 L ), and (c) (8t t0 2 L : t < t0 i  f (t) < f (t0 )).
We can now de	ne L  L as L  L i  (9f : f is an orderisomorphism and (8t 2 L) t  f (t))).
Observation 1  is an equivalence relation 2 Thus, t  t means that the same physical state occurs at moments t and t .
Thus, states are the equivalence classes of  on moments.
L  L means that the moments in L and L represent the same states occurring in the same temporal order.
In other words, L and L represent the same trajectory in state-space.
For a model to represent a physical system and be speci	able by a transition relation among di erent states, the corresponding set of scenarios, S, must satisfy the following closure properties Emerson, 1990].
We generalize these from discrete time.
Sux closure: If S 2 S, then all suxes of S belong to S. 	 Limit closure: If for a set of states T = ft0 .
.
.
t .
.
.g, scenarios containing t0 .
.
.
t , for n  0 are in S, then a scenario S such that T  S is also in S. 	 Fusion closure: If S0 = S0  t  S0 and S1 = S1  t  S1 in S include the same state t, then the scenarios S0  t  S1 and S1  t  S0 formed by concatenating the initial and later parts of S0 and S1 also belong to S ( indicates concatenation).
Lemma 2 By construction, S derived from our models satis	es sux and limit closures.
2 0  0  0  0  0  0  0  0  0  0  0  0  n  n  p  p  f  p  z  f  f  p  a  ...  }|  ...  f  {      t0    0  0  t  x  x  0  t0  4 Results on Time and Actions  It is helpful in intuitively understanding formal de	nitions to attempt to prove some technical results that should follow from them.
For this reason, we state and discuss some consequences of the above model and semantic de	nitions next.
We believe constraint C1 is what McDermott intends by requiring that actions do not overlap.
But, that also eliminates C2, which is essential, e.g., so that Fp can be concluded at all moments which precede p (Observation 6).
Constraints C3 and C4 are required for Observation 6 and related results about G and U.
We also use the fact that x:a]:p means that a is performed and p holds throughout a.
Observation 4 (xhaip)!
Fp 2  Observation 5 (xhaiFp)!
Fp 2 Observation 6 Fp!
p _ (W a : xhaiFp) 2 Observation 8 Gp!
(W a : x:a]:Gp) 2  XXXX  a  t  0  0  Observation 7 Gp!
p 2  XXXXX XXXX6X 6 XXXX6 XXX  t  Weak Determinism: If two moments satisfy exactly the same atomic propositions, then the fragments of the model rooted at those moments must be isomorphic with respect to the temporal precedence relation and the atomic propositions in the formal language.
Thus, we can de	ne weak determinism as the following constraint.
(8x 2 A a 2 B t t  t0 2 T S0 2 S : t  t ) (S0  t t0] 2  a] ) (9S1 2 S  t1 : S1 t  t1 ] 2  a] and S0  t t0]  S1  t  t1]))) Lemma 3 Under weak determinism, S derived from our models satis	es fusion closure.
2 C6.
?z .
.
.
}| ?
.
.
.
{ ?t1 XXXXX XXXXX XXXX XXXXX XX Figure 8: Weak Determinism  However, fusion closure is not satis	ed in general.
We show next how to satisfy it.
Observation 9 (p ^ x:a]:Gp)!
Gp 2 Observation 10 (p ^ q)!
pUq 2 Observation 11 (p ^ x:a]:(pUq))!
pUq 2 Observation 12 (p ^ xjhaij(pUq))!
pUq 2 Observation 13 pUq!
((p ^ q)_W W (p ^ ( a : x:a]:(pUq))) _ (p ^ ( a : xjhaij(pUq)))) 2  Observation 14 In discrete models with unit length actions, xhaip  x:a]:p and xhaip  xjhaijp.
Thus one action operator suces in such models.
2  5 Conclusions and Open Problems  Actions and time are crucial to several subareas of AI.
We sought to generalize the formalization of actions, so that several important properties are not excluded.
These include the actions being of di erent durations, the actions being performed concurrently by di erent agents.
the underlying notion of time being variously continuous or discrete, and the underlying notion of time allowing branching into the future.
We stated various coherence constraints that capture the intuitive properties of actions in di erent cases of interest.
Or model can thus serve as an underpinning for further research on notions such as intentions and know-how.
Previous research on these concepts has been shackled by poor models of time and action, thereby leading to spurious results Singh, 1992].
The logic CTL* was designed over a decade ago for reasoning about programs.
Usually, its models are discrete with unit length actions performed one at a time.
We extended CTL* with new operators and gave our language a more general semantics that allows time to be discrete, dense, or continuous.
One of our concerns was that our de	nitions specialize to each case properly.
This is useful since AI models must often function at multiple levels of abstraction.
We also discovered that several constraints must be stated on models to capture the AI notion of basic actions.
The sole traditional constraint of no overlap McDermott, 1982] says too little and sometimes is too strong.
Even though several decision procedures are known for CTL*, no closed-form axiomatization is still known.
This is an important open problem, as is determining an axiomatization for our language, L. Further research is required to determine the role of past time operators for AI purposes.
Such operators are known to make formulae drastically compact in some cases, but they also raise the complexity of the required decision procedures signi	cantly.
Would it help for AI to augment L with operators such as since ?
For models with exclusively unit length actions, one action operator is enough (instead of three).
Are there other interesting classes of models for which L can be simpli	ed?
We have focused here on representational issues.
We have not explored the tradeo s between expressiveness and computational complexity.
Clearly, eciency can be gained by simplifying the formal language and model.
One class of reasoning techniques that is likely to prove of much value is the one developed in the qualitative reasoning community, which routinely deals with continuous phenomena and looks for ways to express them in terms of signi	cant transitions Kuipers, 1986 Sandewall, 1989].
Model checkers (programs which check whether a given model satis	es a given formula) have drawn much attention lately Burch et al., 1990].
One such  can fairly easily be constructed for L by generalizing the ones known for CTL*.
The recursive characterizations of the temporal operators in terms of actions go a long way in designing this.
Instead of points in discrete models, we have to maintain periods in our model.
Clearly, if a model is 	nitely speci	able in terms of periods, we can compute on it in 	nite time using standard techniques.
However, ecient, specialized data structures for periods would be essential in practice.
References  Allen, 1984] Allen, James F. 1984.
Towards a general theory of action and time.
Articial Intelligence 23(2):123{154.
Bacchus et al., 1989] Bacchus, Fahiem Tenenberg, Josh and Koomen, Johannes A. 1989.
A nonrei	ed temporal logic.
In First Conference on Knowledge Representation and Reasoning.
2{10.
Burch et al., 1990] Burch, J. R. Clarke, E. C. McMillan, K. L. Dill, D. L. and Hwang, L. J. 1990.
Symbolic model checking: 1020 states and beyond.
In LICS.
Dean & Boddy, 1988] Dean, Thomas and Boddy, Mark 1988.
Reasoning about partially ordered events.
Articial Intelligence 36:375{399.
Emerson, 1990] Emerson, E. A. 1990.
Temporal and modal logic.
In Leeuwen, J.van, editor, Handbook of Theoretical Computer Science, volume B. North-Holland Publishing Company, Amsterdam, The Netherlands.
Galton, 1990] Galton, Antony 1990.
A critical examination of Allen's theory of action and time.
Articial Intelligence 42:159{188.
Haddawy, 1990] Haddawy, Peter 1990.
Time, chance, and action.
In Sixth Conference on Uncertainty in AI.
Harper et al., 1981] Harper, William L. Stalnaker, Robert and Pearce, Glenn, editors.
IFS: Conditionals, Belief, Decision, Chance, and Time.
D. Reidel, Dordrecht, Netherlands.
Kuipers, 1986] Kuipers, Benjamin J. 1986.
Qualitative simulation.
Articial Intelligence 29:289{338.
McDermott, 1982] McDermott, Drew 1982.
A temporal logic for reasoning about processes and plans.
Cognitive Science 6(2):101{155.
Sandewall, 1989] Sandewall, Erik 1989.
Combining logic and di erential equations for describing realworld systems.
In Principles of Knowledge Representation and Reasoning.
Shoham, 1988] Shoham, Yoav 1988.
Reasoning About Change: Time and Causation from the Standpoint of AI.
MIT Press, Cambridge, MA.
Singh, 1992] Singh, Munindar P. 1992.
A critical examination of the Cohen-Levesque theory of intentions.
In 10th European Conference on Articial Intelligence.
Singh, 1994] Singh, Munindar P. 1994.
Multiagent Systems: A Theoretical Framework for Intentions, Know-How, and Communications.
Springer Ver-  lag, Heidelberg, Germany.
Thomason & Gupta, 1981] Thomason, Richmond H. and Gupta, Anil 1981.
A theory of conditionals in the context of branching time.
In Harper et al., 1981].
299{322. van Frassen, 1981] van Frassen, Bas C. 1981.
A temporal framework for conditionals and chance.
In Harper et al., 1981].
323{340.
Reasoning with Sequences of Point Events* (An Extended Abstract) R. Wetprasit and A. Sattar  L. Khatib  School of Comp.
and Infor.
Tech.
Griffith University Nathan, Brisbane, 41 11 AUSTRALIA  Computer Science Program Florida Institute of Technology 150 W. University Blvd., Melbourne, F1.
32901, USA 2  Abstract We propose to model recurring events as multipoint events by extending Vilain and Kautz's point algebra [7].
We then propose an exact algorithm based on van Beek's exact algorithm) forfinding feasi le relations for multi-point event networks.
The complexity of our method is compared with previously known results both for recurring and non-recurring events.
We identify the special cases for which our multi-point based algorithm can find exact solution.
Finally, we summarise our paper with brief discussion on ongoing and future research.
A multi-point event (MPE) is a collection of points, when each point represents the related subevents.
A MPE is in normal form if all points are totally ordered, or the relation between the ith point to the i l t h point of the MPE is '<'.
Given two MPEs: I of size n and J of size m, R f I , J ) is the multi-point relation (MPR) between MPE I and J .
An element R(Ij,J j ) is a disjunction of point relation defined in point algebra [7] representing the relationship between the ith point of I and the j t h point of J .
For computational purpose, R ( I ,J ) is represented by using a A,,, matrix relation, when the rows of matrix represent the points of I and the columns represent the points of J .
Solving reasoning tasks for multi-point events in our framework is based on constraint satisfaction techniques.
A binary constraint network of k MPEs, each contains n point subevents, consists of le nodes where each node represents an individual MPE.
The domain of each node is a set of real numbers each representing a point subevent: Di = {aI,a?, ..., a, : aj < aj+l}.
The labels on the arcs are matrix relations representing constraints between two nodes.
An instantiation of a node is a k-tuple ( ~ 1 ~ x..,, 2 xk) , when zj E Dj.
The minimal label or the matrix of feasible relations between two MPEs is the matrix in which each element is consisting of all and only the feasible relations.
A scenario is a set of atomic relations between pairs of MPEs.
Each atomic relation corresponds to a matrix label for each arc.
6  1  +  Introduction  In this paper, we consider the events that are sequences of time points as multa-point events by extending Vilain and Kautz's point algebra.
Each multi-point event could be a finite sequence of recurring instantaneous actions, or a finite set of the beginning or ending of interval events that occur repeatedly.
It is straightforward to transform a multi-point event network into corresponding point algebra network.
We can then apply the existing PA algorithms to the reasoning tasks in questions.
Intuitively, we believe that reasoning with multi-point event networks should have better performance than the traditional PA network because computation on implicit relations of the same multipoint event could be omitted.
Even though, the non-convex interval model [3] can represent and reason about the recurring interval events, obtaining an exact solution for interval-based problem is a hard problem which requires exponential time algorithms [l, 31.
However, if the numbers of subintervals are known and we restrict the relations between subintervals to pointizable relations (SIA) [6], by translating to multi-point event networks, our algorithm computes the exact solutions for the original non-convex interval networks in polynomial time.
3  Reasoning with MPE Networks  The path-consistency checking for MPE network is done in two levels: between two MPEs and three MPEs.
The canonical form is defined to ensure pathconsistency between three points being in two different MPEs.
The path-consistency between three MPEs obtains from the compositzon operations over two matrix relations (defined in Section 3.2).
*A full version of this paper will appear in Proceedings of Canadian AI Conference 1996 (CSCSI-96).
36 0-8186-7528/96 $5.00 0 1996 IEEE  Multi-Point Events and Their Relations  3.1  Canonical Form  of four vertices called a forbadden subgraph.
We define the forbidden subgraph for MPE network in terms of points in MPEs as follow:  A matrix relation A,,, is said to be in the canonical form if the path-consistency conditions 51 among their neighbours as following are satisfied more detail of the algorithm for converting a matrix into the canonical form is in [3]):  t  1.
Ai,J-i  E A,,jo > ( V l 5 i 5 n, 1 < j 5 m )  2.
C > OAi,, ( V I 5 i < n , 1 5 j 5 m )  Definition 3 (A forbidden subgraph) Gwen a MPE subgraph of any four nodes: V , W , S , and T .
Let V be (Vi < ... < V, , W be < .. < Wn), S be (SI< ... < So), and d b e (Ti < ... < Tp),where m, n, o and p are the numbers of points in V,W ,5' and T respectauely.
The subgraph as called a forbidden MPE subgraph, af there exast v , w,s, and t , whach are any valid indices of V , W , S , and T , such that the followang condataons are satasjied:  3.
A;,j+l E A;,jo < (Vl 5 i 5 n, 1 5 j < m ) 4.
A ; - I , ~5 < oA;,j (V1 < i 5 n , 1 5 j 5 m )  R(V,,WW) R(V,,S,) R(W,,S,) R(V,,Tt) R(WW,Tt) R(S,,Tt)  These conditions could be used to approximate the path-consistency of the MPE network.
In our algorithm for finding feasible relations, we first transform the given matrices into canonical form.
This is to reduce the elements in the matrices and also maintain the path-consistency between two MPEs.
Lemma 1 Given a MPE network with IC MPEs (k 2 l), each contains n points (n 2 2).
The MPE network has a corresponding PA network.
3.2 Matrix Relations Operations The operations on two matrix relations, which are necessary in solving reasoning tasks, are defined by adopting the operations on constraint matrices from 41.
Table 1 defines the result of matrix operations $, @,*, 0,and') on two MPRs in terms of basic opX , -, 0 , erations in point algebra.
The symbols and correspond to union, intersection, complement, composition and inverse operators defined in PA, respectively.
+,  Complement Inverse ComDosition  = = = = =  '#;  '2 '2'  '<'  '7' -  '5'  The infeasible relation in the forbidden MPE subgraph is R(S,,Tt), which is '5'.This relation causes inconsistency among those four nodes.
However, if we don't allow '=' between S, and T,, the subgraph becomes 4-consistent [a].
If S is identical to T , the relation between S, and must be '<' because all MPEs are in normal form (either '<' or '>' allowed between points in MPE).
Thus, we don't need to take this case into consideration.
Similarly, we can show that other pairs of MPE nodes in the forbidden subgraph cannot be identical.
Therefore, the set of four MPE nodes considered as a forbidden subgraph are mutually different.
The following algorithm using the canonical form matrices and the matrix operations to solve the minimal label problem in polynomial time.
We found that we do not need to check the pathconsistency again after performing the procedure FIND-SUBGRAPHS-MPE since the network is still path-consistent.
Theorem 2 Suppose a MPE network has two nodes I and J.
Let A be the matrix relation between I and J. I f A is in canonical form, then the corresponding PA network is path-consistent.
i  =  Algorithm FEASIBLE-MPE Input: A MPE network represented as a matrix C where entry CIJ is the label on the arc from nodes I to J.
Each C I J is a matrix relation R [ I ,J],where an entry of R [ I ,J ] is an internal relation between points in MPEs I and J Output: The set of feasible relations for C I J , I , J = 1 , 2 , .., k begin PATH-CONSISTENCY-MPE FIND-SUBGRAPHS-MPE end  C = A" iff Ci,j = -Ai,i C = A iffci i = -Aji C = A 0B iff  Table 1: The Multi-Point Event Operations  Procedure PATH-CONSISTENCYXPE begin For each matrix relation R [ I ,J ] do Canonical-Conv (R[I, J1) Q := { ( I ,I(, J ) I 1 1 I-< k, 1 2 IS 5 k, K # I , J} While Q is not emDtv do selek and de1eie"a path ( I ,I(, J ) from Q  3.3 The Algorithm The algorithm to solve the minimal labels problem for MPE network is based on van Beek's exact algorithm for PA network [6].
He has shown that the pathconsistency algorithm alone is not sufficient for PA network by pointing out a counter-example consisting  j's  37  4  Temp := R[I, J ] 63 (R[I, I<]0 R[K, 51) If (Temp # R[I, J ] ) then Canonical-Conv(Temp) R[I, J ] := Temp R[J, I] := TLmp (inverse of Temp) Q := Q U RELATEDPATHS(I, J )  Conclusion  The main contribution of this framework is an extension of point-based representation to reason with the recurring events that are considered as collections of point subevents.
The algorithm we proposed correctly finds all feasible relations in the MPE network.
The complexity of this algorithm is O ( m a z ( n 5 k 3 , m n 2 k 2 ) where ), k is the number of MPEs with maximum n points and m is the number of 'f' internal relations in the network.
The complexity for finding the same solutions for non-recurring PA network with the same data (nk point events) is O(,max(n3k3,mn'k')), where m is also the number of # relations in the network [6].
For non-convex interval network, the minimal labels of the network can be approximately achieved in O(n5k3)complexity algorithm, or 0 n3k3) when considering only the affected relations [3 .
However, if we restrict the internal relations between subintervals to be pointizable interval relations (SIA) [6] and transform into MPE network, our algorithm yields the exact solutions.
The complexity of our algorithm is proportional to the number of '#' edges, the worst case would seldom occur in the real world domain.
Currently, we are working on improving Complexity of the algorithm we presented in this paper by avoiding the redundant computation of the unconstrained relations, and on constructing the algorithm for finding a consistent scenario for the MPE network.
We are implementing our algorithms and comparing the performance to other non-multi-point event approaches.
end  Procedure RELATED-PATHS/I.
J ) Return {(I, J , IC), ( K ,I , J ) ( 1 ' d K 6 k , I< # I, I< # J } Procedure FIND-SUB GRAPHS-MPE begin For each matrix relation such that R[V,W ] , = ' # I (1 5 V < W 5 k ) and (1 5 U ,w 5 n ) do Initialize P1, P 2 , P I Q1, Q2, Q to empty set For each MPE K (1 5 K 5 k , IC # V,W ) do P1 = P1 U adj-MPEs(>, Vu,IC) P 2 = P2 U a d j - M P E s ( 2 , W,, K ) Q1 = Q1 U adj-MPEs <, Vu,IC) Q2 = Q2 U adj-MPEs <, W,, K ) P =PlnP2, Q = QnnQ2 For each S, E P do For each Tt E Q do If S # T then R [ S , T ] , , ,:= ' < I R[T,S]t,,:= ( > I end  \  i  Our algorithm for computing feasible relations consists of two main tasks: checking path-consistency between three MPEs throughout the network (procedure PATH-CONSISTENCY-MPE), and eliminating the infeasible relations from the forbidden subgraphs (procedure FIND-SUBGRAPHS-MPE).
We maintain the consistency between two MPEs by calling procedure Canonical-Conv (detailed in [3]).
This procedure transforms the matrix relations into canonical form before inserting to the database.
Function a d j - M P E s ( L , V,, IC) in procedure FIND-SUBGRAPHS-MPE returns the set of elements I < k l in which R(Vu,I < k ) = '>' by checking the relations between V, and I - ,, (1 5 i 5 n ) .
Here are some technical results:  References J. Allen, "Maintaining Knowledge about Temporal Intervals," Communication of the A CM, Vol.
26:11, pp.
832-843, 1983.
E.C.
Freuder, "A Sufficient Condition for Backtrack-Free Search."
Journal of ACM.
Vol.
29, pp.
24-32, 1982.
L. al-Khatib, Reasonzng wath Non- Convex Tame Intervals.
PhD dissertation, Florida Institute of Technology, Melbourne, Florida.
1994.
P. Ladkin and R.D.
Maddux, "On Binary Constraint Problems."
Journal of ACM.
Vol.
41:3, pp.
435-409, 1994.
Lemma 4 Changzng the label R(Ss, Tt) of the forbadden subgraph defined an Definztaon 3 wall not lead to path znconszstency  A.K.
Mackworth, "Consistency in Networks of Relations."
Artzjkzal Intellagence.
Vol.
8 , pp.
99-118, 1 n-7  131 1 .
Theorem 5 The closure of a MPE network, calculated by the algorzthm FEASIBLE-MPE, corresponds to a path conszstent PA network  P. van Beek, Exact and Approximate Reasoning about Qualatataue Temporal Relations.
Technical Report TR-90-29, University of Alberta, Edmonton, Alberta, Canada.
1990.
Theorem 6 The algorathm FEASIBLE-MPE correctly finds the manzmal labels for all znternal relatzons an the MPE network.
M. Vilain and H. Kautz, "Constraint Propagation Algorithms for Temporal Reasoning."
Proceedings of AAAI-86, San Mateo; Morgan Kaufman.
1986.
Theorem 7 The algorathm FEASIBLE-MPE, for k MPE nodes an the constraant network and each node contaans at most n poants, has a tame complexaty of O ( m a x ( n 5 k 3m, n 2 k 2 ) ) where , there are m f-relataons.
38
Proceedings of TIME-96  1  Gaining Efficiency and Flexibility in the Simple Temporal Problem Amedeo Cesta  Angelo Oddi  IP-CNR National Research Council of Italy Viale Marx 15, I-00137 Rome, Italy amedeo@pscs2.irmkant.rm.cnr.it  Dipartimento di Informatica e Sistemistica Universita di Roma "La Sapienza" Via Salaria 113, I-00198 Rome, Italy oddi@assi.dis.uniroma1.it  Abstract The paper deals with the problem of managing quantitative temporal networks without disjunctive constraints.
The problem is known as Simple Temporal Problem.
Dynamic management algorithms are considered to be coupled with incremental constraint posting approaches for planning and scheduling.
A basic algorithm for incremental propagation of a new time constraint is presented that is a modification of the Bellman-Ford algorithm for Single Source Shortest Path Problem.
For this algorithm a sufficient condition for inconsistency is given based on cycle detection in the shortest paths graph.
Moreover, the problem of constraint retraction from a consistent situation is considered and properties for repropagating the network locally are exploited.
Some experiments are also presented that show the usefulness of the properties.
1  Introduction  Knowledge-based architectures for planning and scheduling based on constraint propagation, e.g.
[5, 3, 8, 2], perform incremental constraint posting and retraction on a current partial solution.
A complete plan is created by efficiently searching in partial plans space, and, in other cases, it is adapted to new situations by partially removing parts of the solution.
A module for temporal constraint management that supports plan space search and current solution maintenance should be extremely efficient because is called into play at any modification (monotonic or not) of the current plan.
Such an efficiency is usually guaranteed by restricting the expressive power of the temporal representation.
Usually the so called Simple Temporal Problem (STP) [7] is used that allows the representation of binary quantitative constraints without disjunction.
In spite of the restriction of expressivity, also for STP it results useful to consider how the efficiency of manipulation primitives may be improved.
In our research, we have been investigating possible  algorithms for managing temporal information that: (a) allow dynamic changes of the constraint set for both incremental constraint posting and retraction; (b) exploit the localization of effects of any change in a subnetwork of the whole constraint graph; (c) do not compute the minimal network as done in [7] but just check for consistency.
A previous paper [1], in the same line of [6], has concerned the specialization of arc-consistency algorithm to the STP.
The choice of arc-consistency to propagate temporal constraints was motivated by the good trade-off wrt space and time complexity.
In the same paper some properties were given that were shown experimentally to improve the performance of the algorithm in the average case.
The present paper contains a further step in the direction of gaining efficiency in the solution of the STP.
After presenting the essentials of STP (Section 2), it presents dynamic algorithms based on the well known Bellman-Ford algorithm for computing Single Source Shortest Paths (Section 3).
It also introduces (Section 4) the concept of dependency that computes a particular spanning tree on the constraint graphs that allows the definitions of a sufficient condition for inconsistency detection (Section 5) and an algorithm for local constraint retraction (Section 6).
Some experiments (Section 7) show the usefulness of the properties.
2  The Temporal Problem  A Simple Temporal Problem is defined in [7] and involves a set of temporal variables {X1 , .
.
.
, Xn }, having continuous domains [lbi , ubi ] and a set of constraints {aij <= Xj - Xi <= bij }, where aij >= 0, bij >= 0 and aij <= bij .
A special variable X0 is added to represent the origin of the time (the beginning of the considered temporal horizon) and its domain is fixed to [0, 0].
A solution of the STP is a tuple (xi .
.
.
xn ) such that xi [?]
[lbi , ubi ] and every constraint aij <= Xj - Xi <= bij is satisfied.
An STP is inconsis-  Proceedings of TIME-96 tent if no solution exists.
In order to find the set of possible values [lbi , ubi ] for every variable Xi , a direct constraint graph Gd (Vd , Ed ) is associated to the STP, where the set of nodes Vd represents the set of variables {X1 , .
.
.
, Xn } and the set of edges Ed represents the set of constraints {aij <= Xj - Xi <= bij }.
Given a constraint aij <= Xj -Xi <= bij , we can rewrite it as a pair of inequalities: * Xj - Xi <= bij * Xi - Xj <= -aij For every linear inequality Xj - Xi <= wij (with wij equal to bij or -aij ) we have an edge (i, j) in Gd (Vd , Ed ) labeled with the weight wij .
Each path in Gd from the node i to the node j, i = i0 , i1 .
.
.
im = j induces between the variables Xj and Xi the constraint Xj - Xi <= lij , where lij is the sum of weights along the path, that is lij = w01 +w12 +.
.
.+w(m-1)m .
Considering the set of all paths between the nodes i and j, these paths induce a constraint Xj - Xi <= dij , where dij is the length of a shortest path between the nodes i and j.
Finally a cycle on the graph Gd is closed path i = i0 , i1 .
.
.
im = i and a negative cycle is a cycle with associated a negative length (lii < 0).
In [7] some useful properties of an STP are given and reported in the following theorems.
Theorem 1 [7] A Simple Temporal Problem is consistent iff Gd does not have negative cycles.
Defining d0i as the length of a shortest path on the graph Gd from the origin 0 and the node i and di0 as the length of a shortest path from the node i to the origin 0 we can also have the other following theorem.
Theorem 2 [7] Given a consistent Simple Temporal Problem, the set [lbi , ubi ] of feasible values for the variable Xi is the interval [-di0 , d0i ].
Theorem 2 shows that the Simple Temporal Problem is a Shortest Paths Problem and precisely we have to calculate two sets of shortest paths length: (a) the set of shortest paths from the node 0 (that represent the variable X0 ) to the nodes 1 .
.
.
n; (b) and the set of shortest paths from the nodes 1 .
.
.
n to node 0.
3  An Algorithm for the STP  To solve the basic STP we use the Bellman-Ford algorithm for the Single Source Shortest Paths Problem [4] giving an incremental version of the algorithm named Propagation, which accepts as an input the graph Gd and a new constraint Cij (where Cij = aij <= Xj - Xi <= bij ) and produces in output a new set of feasible values [-di0 , d0i ] for every variable Xi or a value fail in the case the new constraint induces a inconsistent situation.
To understand the algorithm, shown in Figure 1, some  2 simple definitions are useful: given a node i of the graph Gd we define EdgesOut(i) as the set of edges which leave from the node i and EdgesIn(i) as the set of edges which arrive to the node i. T and F are the boolean constants T rue and F alse.
The algorithm has two differences wrt the standard implementation on Bellman-Ford with a queue.
First, it calculates at the same time two sets of shortest distances.
Second, the algorithm has an internal test which detects negative cycles on the graph Gd which contain the reference node X0 .
In addition, every node u [?]
Vd has two boolean marks: LB(u) and U B(u).
This marks are useful in order to distinguish the two types of propagation in the graph Gd , that is, respectively U B(u) = T and LB(u) = T when a node is modified by the propagation process for the distance d0i and the distances di0 .
The Propagation calculates the set of distances {d0i } between Steps 6 and 14 and the set of distances {di0 } between Steps 16 and 24.
This last section of the algorithm, in order to calculates the set of distances di0 , (that is, the length of the shortest paths on the graph Gd between the nodes 1 .
.
.
n and the node 0) considers the set of direct edges in Gd as oriented in the opposite direction.
In this way when a shortest path between the nodes 0 and i is found, it is actually a shortest path in the opposite direction.
Finally, the tests at Steps 10 and 20 check for negative cycles in the graph Gd when they contain the node 0.
The algorithm calculates also two shortest path trees.
In fact Steps 11 and 21 respectively update the predecessor function pu, which represents the shortest path tree of the distances {d0i } and the predecessor function pl, which represents the shortest path tree of the distances {di0 }.
The complexity of the algorithm, as well known, is O(EN ).
Where N and E are respectively the number of nodes and the number of edges in Gd .
negative cycles  4  Focusing on Dependency  The temporal meaning of shortest path trees on the Gd graph is simple.
Every bound {d0i } (or {di0 }) is induced by the set of temporal constraints in the shortest paths between the origin 0 and the node i (or between the node i the origin 0).
The following definitions are useful: Definition 1 Let Gd a consistent distance graph.
The tree DTub of the shortest paths from the origin 0 to the nodes 1 .
.
.
n is called Upper Bounds' Dependency Tree.
Definition 2 Let Gd a consistent distance graph.
The tree DTlb of the shortest paths from to the nodes  Proceedings of TIME-96 Propagation (Gd , Cij ) 1. begin 2.
Q - {i, j} 2a.
LB(i) ::= T ; U B(i) ::= T 2b.
LB(j) ::= T ; U B(j) ::= T 3.
While Q 6= [?]
do begin 4. u - P op(Q) 5. if U B(u) then 6.
Foreach (u, v) [?]
EdgesOut(u) do 7. if d0u + wuv < d0v 8. then begin 9. d0v ::= d0u + wuv 10. if d0v + dv0 < 0 then exit(fail) 11. pu(v) ::= u 12.
U B(v) ::= T 13. if v 6[?]
Q then Q - Q [?]
{v} 14. end 15. if LB(u) then 16.
Foreach (u, v) [?]
EdgesIn(u) do 17. if du0 + wvu < dv0 18. then begin 19. dv0 ::= du0 + wvu 20. if d0v + dv0 < 0 then exit(fail) 21. pl(v) ::= u 22.
LB(v) ::= T 23. if v 6[?]
Q then Q - Q [?]
{v} 24. end 25.
LB(u) ::= F 26.
U B(u) ::= F 27. end 28. end Figure 1: Propagation algorithm  1 .
.
.
n to origin 0 is called Lower Bounds' Dependency Tree.
If a given graph Gd is consistent then the trees DTub and DTlb are always defined.
In fact, without negative cycles, the distances {d0i } and {di0 } are always defined.
In general, the trees DTub and DTlb may not be single.
In fact, the graph Gd may contain several paths with the same length.
A relevant situation is verified when the graph Gd contains at least a negative cycle.
In this case, the following Theorem holds.
Theorem 3 Give a distance graph Gd .
If during the update process of the Propagation algorithm the predecessor function pu (pl) represents a graph containing at least a cycle then the graph Gd is inconsistent.
3 Proof.
We give the proof for the distances {d0i }, but an analogous proof can be given for the distances {di0 }.
Suppose by hypothesis that during the update process of the algorithm, a dependency path exists between the nodes i and j named p1 : i = i0 , i1 .
.
.
ir = j, that is, a path such that pu(ik ) = ik-1 , with k = 1 .
.
.
r. If we sum the weights along this path, we have the following relation: d0j - d0i = w01 + w12 + .
.
.
+ w(r-1)r .
(1)  If successively the Propagation algorithm builds a dependency path p2 : j = j0 , j1 .
.
.
js = j, we can write the following relation: dnew - d0j = w01 + w12 + .
.
.
+ w(s-1)s .
0i  (2)  Where dnew is the new value of the distances d0i 0i updated along the path p2 .
If we sum the relations 1 and 2 we obtain the length of the cycle lii : lii = dnew - d0i .
0i  (3)  Observing that the link of two paths p1 and p2 is a cycle and dnew < d0i , then the length lii is negative 0i and this proves the inconsistency of the graph Gd .
2  5  Cycle Detection  In order to use the property expressed by Theorem 3 few changes are introduced in the Propagation algorithm.
Each edge (i, j) in the graph Gd have three new boolean marks: N EW ((i, j)), LBP ((i, j)) and U BP ((i, j)).
The mark N EW is useful in order to distinguish the new edges introduced in Gd , by the new temporal constraint Cij .
In fact, if in the graph there is at least a negative cycle, then it must contain at least one of the new edges introduced.
Instead, the two marks LBP ((i, j)) and U BP ((i, j)) are used to check when a bound changes two times as explained in the next Theorem 4: Theorem 4 Let Gd a consistent distance graph and Cij = aij <= Xj - Xi <= bij the new constraint added.
If during the propagation process the distance d0j (di0 ) changes two times, then the constraint Cij is inconsistent with the other constraints represented in Gd .
Proof.
We give the proof for the distances {d0j }, but an analogous proof can be given for the distances {dj0 }.
If the constraint represented by the edge (i, j) changes the distance d0j a first time, this means every new shortest paths built by the Propagation algorithm will contain the node j.
If the distances is changed a second time, then the algorithm has built a closed dependency path and for the Theorem 3 the graph Gd is inconsistent.
2  Proceedings of TIME-96 Figure 2 shows the modified version of the algorithm to check for cycle detection.
It is interesting to notice the complexity of the algorithm with cycles detection is the same of the Propagation algorithm.
In fact, the only difference with the previous algorithm is the check of the boolean marks N EW ((i, j)) LBP ((i, j)) and U BP ((i, j)).
Propagation-cd (Gd , Cij ) 1.
- 9. as in the Propagation algorithm 10a.
if d0v + dv0 < 0 10b.
then exit(fail) 10c.
else if N EW ((u, v)) 10d.
then if U BP ((u, v)) 10e.
then exit(fail) 10f.
else U BP ((u, v)) ::= T 11.
- 19. as in the Propagation algorithm 20a.
if d0v + dv0 < 0 20b.
then exit(fail) 20c.
else if N EW ((u, v)) 20d.
then if LBP ((u, v)) 20e.
then exit(fail) 20f.
else LBP ((u, v)) ::= T 24.
- 28. as in the Propagation algorithm  Figure 2: Differences introduced by cycle detection the average time  6  Retraction of Temporal Constraints from a Consistent Context  This paragraph deals with the problem of removing temporal constraints from a consistent graph Gd (a graph without negative cycles).
A basic way to do this consists of: physically removing the constraint from the graph Gd ; setting every distance {d0i } and {di0 } to the value +[?
]; finally, running the Propagation algorithm on the whole graph.
As a matter of fact, this method is not very efficient.
In fact, when retracting a constraint from the time map a lot of distances are likely not to be affected by the removal.
The dependency information may be used to focalize the part of the network actually affected by the removal and to run the Propagation algorithm on that part of the graph.
To state same properties some definitions are useful.
Given an upper bounds' dependency tree DTub (VDTub , EDTub ), each sub-tree STub [i](VSTub , ESTub ) of root i [?]
VDTub is called an Upper Bounds' Dependency Sub-tree.
Given a lower bounds' dependency tree DTlb (VDTlb , EDTlb )  4 every sub-tree STlb [i](VSTlb , ESTlb ) of root i [?]
VDTlb is called a Lower Bounds' Dependency Sub-tree.
Given a a distance graph Gd (VGd , EGd ) and a node i [?]
VGd , IN (i) is the set of start nodes of the edges which enter in the node i (in the edge (j, i), j is the start node and i is the end node).
The next Proposition explains the real effects of a removal constraints from a graph Gd and it is a starting point to write a new algorithm to remove temporal constraints from Gd .
Proposition 1 Let Gd be a consistent graph and DTub (VDTub , EDTub ) its upper bounds' dependency tree (DTlb (VDTlb , EDTlb ) its lower bounds' dependency tree).
The retraction of an edge (i, j) [?]
EDTub ( (i, j) [?]
EDTlb ) modifies at most the distances of the nodes k [?]
VSTub [j] ( k [?]
VSTlb [j] ).
No distances are modified when (i, j) 6[?]
EDTub ( (i, j) 6[?]
EDTlb ).
Proof.
We give the proof for the distances {d0i }, but an analogous proof can be given for the distances {di0 }.
The removal of an edge (i, j) [?]
EDTub can't modify a node's distance {d0k } in the case k 6[?]
VSTub [k].
In fact the removal of (i, j) does not change the shortest path between the origin 0 and the node k. If (i, j) 6[?]
EDTub then no distance is changed because no shortest path is changed.
2 The basic idea to write an efficient removal algorithm is run the Propagation algorithm on the only part of the Gd graph affected by the removal of the constraint.
The next Theorem formalize this concept and explains how to initialize the Propagation algorithm.
Theorem 5 Let Gd be a consistent distance graph.
To remove the effects of the constraint represented by the edge (i, j) [?]
EDTub ( (i, j) [?]
EDTlb ) the queue Q of the Propagation algorithm and the set of distances {d0i } ({di0 }) in the graph Gd need of the following initialize operations.
S S 1.
Q - k[?
]VST [j] IN (k) (Q - k[?
]VST [j] IN (k)) ub lb 2. d0u ::= +[?
], u [?]
VSTub [j] (du0 ::= +[?
], u [?]
VSTlb [j]) Proof.
We give the proof for the distances {d0i }, but an analogous proof can be given for the distances {di0 }.
By Proposition 1, for every node k [?]
VSTub [j], the distance {d0k } can change after the removal.
The Propagation algorithm have to rebuild the new shortest paths for every node k [?]
VSTub [j].
In order to update these distances to the new values, it is necessary to initialize them to the maximum possible value +[?].
In fact, it is not known what the new values will be and the Propagation algorithm can only reduce the bounds.
In addition, we have to put in the queue Q all  Proceedings of TIME-96 the nodes of the constraints (i, j) which enter in the set S of updated nodes.
That is, the nodes in the set k[?
]VSTub [i] IN (k).
In fact, these are the only nodes of the graph from which can start the new shortest paths of the nodes k [?]
VSTub [j].
2 The Remove algorithm is shown in Figure 3.
It accepts as an input a graph Gd and a constraint Cij which have to be removed from Gd and return the graph Gd updated.
At the step 13 is used the RePropagation algorithm that is similar to the Propagation algorithm but accepts as an input a list of nodes Q instead of an edge Cij .
The parameter Q is used as an initialization for the internal queue.
Moreover RePropagation does not check for the consistency of a modification because the removal of one or more constraints, relax the STP holding the consistency property.
Remove (Gd , Cij ) 1. begin 2.
Vm - [?]
3.
Q-[?]
4. if (i, j) [?]
EDTub 5. then Vm - Vm [?]
VSTub [j] 6. else if (j, i) [?]
EDTub 7. then Vm - Vm [?]
VSTub [i] 8. if (i, j) [?]
EDTlb 9. then Vm - Vm [?]
VSTlb [i] 7. else if (j, i) [?]
EDTlb 8. then Vm - Vm [?]
VSTlb [j] 9.
Foreach u [?]
Vm do begin 10.
Q - Q [?]
IN (u) 11. end 12.
EGd - EGd - {(i, j), (j, i)} 13.
RePropagation(Gd , Q) 14. end Figure 3: Remove algorithm  7  Performance Evaluation  In order to get some realistic evaluations of the algorithms, we have used a scheduling system described in [3] and the time network generated by the scheduler.
This scheduler solves instances of the Deadline Job Shop Scheduling Problem (DJSSP) by incremental precedence constraint posting between the activities until any conflict in the use of resources is resolved.
In the DJSSP, each activity in a job can request only one resource and a resource is requested only once in a job.
The sequence of resources requested by the activities in a job is random.
Every job has a  5 fixed release date and a due date.
More details on the random problem generator are described in [3].
All the evaluations are given as number of time points explored by the algorithms.
This choice is motivated from the fact that such number is both proportional to the time of computation and machine independent.
We have built two different types of time networks from the resolution of two different DJSSPs: the 8x8x8 (named P 8) and the 10x10x10 (named P 10), where the first number indicate the number of jobs, the second one the number of activities in a job and the third one the number of resources.
The data are obtained running ten instances of each type of problem.
Table 1 shows the number of time points N , the maximum number of distance constraints Emax and maximum connectivity Cmax for each problem.
The connectivity is defined as the ratio between the number of distance constraints E and the number of time points N .
The value N is two times the number of activities plus two (the origin point and horizon point).
The value Emax represents the maximum number of distance constraints which can be contained in a time network associated to the solution of the instance of the DJSSP.
Emax is obtained by the sum of the maximum values of the number of precedence constraints for each resource and the number of constraints before the scheduling algorithm starts to find a solution.
Table 2 and Table 3 present the perfor-  Table 1: Number of time points and maximum connectivity for the experimental time networks Problem P8 P 10  N 130 202  Emax 333 661  Cmax = Emax /N 2.56 3.27  mance of the Propagation algorithm when a modification is either consistent or inconsistent respectively.
This values are shown as a function of the average connectivity Av-conn, that is, every row of the table represents the average value obtained in the interval Av-conn +-0.25.
In order to get several values of the connectivity we have built a solution of an instance of a DJSSP and progressively reduced the number of edges and selected a time constraint Cij in random way.
In order to get the results showed in Table 2, we have modified the distance constraint selected Cij = aij <= Xj - Xi <= bij , in the constraint  Proceedings of TIME-96  6  Cij = aij + (dij - aij )U [0.05, 01] <= Xj - Xi <= bij .
Where U [x, y] represents a random value r with uniform distribution such that x <= r <= y and dij is minimal temporal distance between the nodes i and j on the Gd graph.
In this case, it is possible to make a comparison between the number of nodes scanned by the Propagation algorithm (Loc-prop values) and the number of nodes scanned by an algorithm which works from scratch (Scratch values).
In order to get the results showed in Table 3, we have induced an inconsistent situation by modifying the constraint Cij in the constraint dij (1 + U [0.05, 01]) <= Xj - Xi <= bij In this other case, it is possible make a comparison between the number of nodes visited by the Propagation algorithm which uses the property expresses by Theorem 4 (Cycle-det values) and the number without the previous property (No-cycle-det values).
Table 2: Incremental vs scratch propagation Problem P8  P 10  Av-conn 1.25 1.75 2.25 2.75 1.25 1.75 2.25 2.75 3.25  Loc-prop 38.76 52.33 34.08 39.51 51.42 67.20 64.34 57.00 63.92  Scratch 652.67 1111.47 1641.19 2048.28 1108.38 1928.54 2876.22 3817.79 4388.71  a solution; then we have reduced progressively the number of time constraints by using the Remove algorithm.
In this case, is possible to make a comparison between the average number of nodes scanned by the Remove algorithm (Loc-rem values) and the number of nodes scanned in the same case by a scratch algorithm (Scratch-rem values).
The scratch algorithm eliminates first the constraint from the time map; then puts all the bounds of the time points to the value +[?
]; finally updates all the network.
Table 4: Incremental vs scratch remove Problem P8  P 10  Av-conn 1.25 1.75 2.25 2.75 1.25 1.75 2.25 2.75 3.25  Loc-rem 2.37 35.34 56.02 96.35 2.69 33.12 55.06 70.58 156.97  Scratch-rem 652.67 1111.47 1641.19 2048.28 1108.38 1928.54 2876.22 3817.79 4388.71  Acknowledgments This research is partially supported by: ASI - Italian Space Agency, CNR Special Project on Planning, CNR Committee 04 on Biology and Medicine.
References [1] Cervoni, R., Cesta, A., Oddi, A., Managing Dynamic Temporal Constraint Networks, Proceedings of the Second International Conference on AI Planning Systems (AIPS94), AAAI Press, 1994.
Table 3: Propagation with and without cycle detection Problem P8  P 10  Av-conn 1.25 1.75 2.25 2.75 1.25 1.75 2.25 2.75 3.25  Cycle-det 3.02 2.92 2.47 1.92 3.21 2.78 2.68 2.55 2.63  No-cycle-det 114.42 77.30 43.87 9.75 199.45 133.81 86.85 27.15 14.58  Finally, Table 4 presents the performance of the Remove algorithm.
These results are obtained in the same way as the previous ones.
First we have built  [2] Cesta, A., Oddi, A., DDL.1: A Formal Description of a Constraint Representation Language for Physical Domains, Proceedings of the 3rd European Workshop on Planning (EWSP95), IOS Press, 1996.
[3] Cheng, C. Smith, S.,F., Generating Feasible Schedules under Complex Metric Constraints, Proceedings of the 12th National Conference on Artificial Intelligence (AAAI-94), AAAI Press, 1994.
[4] Cormen, T.H., Leierson, C.E., Rivest, R.L., Introduction to Algorithms, MIT Press, 1990.
[5] Currie, K., Tate, A., O-Plan: the open planning architecture, Artificial Intelligence, 52, 1991, 49-86.
[6] Davis, E., Constraint Propagation with Interval Labels, Artificial Intelligence, 32, 1987, 281-331.
[7] Dechter, R., Meiri, I., Pearl, J., Temporal constraint networks.
Artificial Intelligence, 49, 1991, 61-95.
[8] Ghallab, M, Laruelle, H., Representation on Control in IxTeT, a Temporal Planner, Proceedings of the Second International Conference on AI Planning Systems (AIPS94), AAAI Press, 1994.

Combining Simultaneous Values and Temporal Data Dependencies Avigdor Gal & Dov Dori Information Systems Engineering Department Faculty of Industrial Engineering and Management Technion - Israel Institute of Technology Haifa, 32000, Israel  Abstract  In temporal databases there are situations where multiple values of the same data item have overlapping validity times.
In addition to the common case of multi-valued properties, there are several possible semantics to multiple values with overlapping validity times of the same data item.
We refer to such data items as having simultaneous values.
This paper presents a polynomial algorithm for ecient handling of simultaneous values in a database with temporal data dependencies|integrity rules that dene relationships among values of dierent data items in a temporal database.
The algorithm is demonstrated using a case study from the game theory area.
An implementation of the algorithm is integrated in a prototype of a temporal active database.
keywords: temporal databases, simultaneous values, uncertainty, temporal data dependencies, action reasoning  1 Introduction and Motivation  A temporal database is a database that supports some aspects of time [5].
One of the basic temporal aspects supported by many temporal databases is the valid time, representing the time a data-item is considered to be true in the modeled reality [5].
There are situations where multiple values of the same data-item have overlapping valid times.
The multi-valued property is the most common case, where several values are grouped into a single property [4].
For example, a property that contains the languages that a person speaks, can have a set of values grouped into a single property.
There are situations, however, where multiple values with overlapping valid times of the same data-item exist in the database, but with dierent semantics than the multi-valued case.
We refer to these data-items as having simultaneous values.
While in the multi-valued case all values whose valid time include t are deemed to be valid in the modeled reality, it is possible that only part of the candidate values, i.e.
the values that were assigned to a data-item at time t, represent the data-item's value in the real world.
For example, a data-item that contains a spouse name is limited by law to be single The work was conducted while the author was in the Technion.
He is currently at the Department of Computer Science, University of Toronto, Toronto, Ontario, M5S 3H5 CANADA.
valued.
When there are several alternatives for the spouse name due to uncertain information, then only one value of the set is the data-item's value.
Each value of the set is possibly the data-item's value.
In temporal databases, change of decisions about the value and valid time of a data-item may cause a situation where two values of the same data-item have overlapping valid times.
For example, a value val1 valid during [Jan 1994, Mar 1994) of a data-item  is augmented at time point Aug 1993 by a value val2 valid during [Feb 1994, Apr 1994).
 has more than one value in the interval [Feb 1994, Mar 1994).
In some cases, the value that was inserted later corrects an erroneous value that was inserted earlier.
In other cases, both values are possibly correct, each with respect to a dierent time point.
Simultaneous values enable dierent semantics in mapping the stored values to the modeled reality values.
It is particularly useful in applications where a data-item may have multiple values representing the existence of dierent alternatives.
For example, if knowledge arrives from various sources, then no apriori selection of a single value should be enforced.
Instead, for each database retrieval operation, the user can choose the appropriate value, values or any aggregation of those values.
Handling simultaneous values in a temporal database requires the use of optimized update and retrieval mechanisms.
The maintenance problem of simultaneous values becomes more arduous in temporal active databases [2], where temporal data dependencies are enforced.
A temporal data dependency is a tool that supports rules for manipulating data-items which may have a variety of temporal characteristics.
Temporal data dependencies can be viewed as a type of integrity rules of the temporal active database.
Violating them, activates database operations that react to restore the database integrity.
As an example for the use of temporal data dependencies, we can consider decision support systems [3]|systems that model decisions about actions that should be performed in a target system.
Such systems consist of decision models that are rooted in the operations research or articial intelligence disciplines, and of a database, that stores the necessary data to support the decision models.
Decision support systems can benet signicantly from the temporal active paradigm.
As a concrete motivating case study, we present the following application of a decision support system, based on the Cournot game [7]; [1].
Three instant coffee manufacturers|Bilbo, Frodo and Gandalf, decide each month about the quantity of coee to be produced in the next month.
Each manufacturer bases the decision about its manufactured quantity upon estimation of the quantities manufactured by the other two manufacturers, its own strategy (maximum revenue, a certain market share, etc.
), and general knowledge about the market behavior.
Each manufacturer has its own deadline for making the production quantity decision.
We assume a single market price for the manufactured type of instant coee, which is determined periodically as a function of the total quantity produced in that period.
The relationships between the market price and the total quantity is modeled by the constraint Total ?
Quantity  Market ?
Price = Market ?
Constant (1) Each manufacturer attempts to estimate the best decision to be taken, based on its own competition strategy and the two competitors' decisions and competition strategies.
For example, Bilbo may assume that Frodo and Gandalf have an objective of maximum prot. Consequently, each manufacturer would like to produce as much coee as possible without lowering the price to a level that decreases its total prot. By assuming the competition strategy of both Frodo and Gandalf, Bilbo can estimate their production decisions and determine the optimal production level, based on the following temporal data dependency: Production-Decision q Market-Constant :=  Competitors-Total-Estimation ?
Unit-Cost Competitors-Total-Estimation  This temporal data dependency is a periodical result of maximizing the prot function of a single manufacturer.
The derivation of the temporal data dependency is given in Appendix A.
Other strategies would yield dierent temporal data dependencies.
Competitors-Total-Estimation is the sum of the production estimations of the other two manufacturers.
Since this information is often misleading, each manufacturer should collect as much estimations as possible on each one of the competitors.
The temporal dependency graph can be evaluated each time there is a change in one of the data-items MarketConstant, Competitors-Total-Estimation or the manufacturer's Unit-Cost.
Alternatively, it can be evaluated once each period, just before the manufacturer has to decide about the quantity to be produced for the following period.
Figure 1 presents the data over time of the dataitems Market-Constant, Competitors-Total-Estimation, Unit-Cost, and the resulting Production-Decision of Bilbo.
As the gure shows, the temporal validity of each value is bounded.
For example, Market-Constant has the value 10000 during the interval [Feb 94, June 94).1 The resulting values of Production-Decision are 1  We use a single month granularity, hence this interval is  Market Constant  10000 9500 9000 Feb 94  June 94  Sep Oct Dec Feb Apr 94 94 94 95 95  Aug 95  t  Aug 95  t  Aug 95  t  Aug 95  t  630  Competitors Total Estimations  600 570 540 Feb 94  June 94  Sep Oct Dec Feb Apr 94 94 94 95 95  Unit Cost  7 6 Feb 94  June 94  Sep Oct Dec Feb Apr 94 94 94 95 95  405 400  Production Decision  385 380 375 316 293  Feb 94  June 94  270  Sep Oct Dec Feb Apr 94 94 94 95 95  Figure 1: Exemplary data of Bilbo in the coee manufacturers case study valid during the time intervals as computed using all data-items that determine Production-Decision and shown in the bottom graph of Figure 1.
For example, a Market-Constant of 10000, a CompetitorsTotal-Estimations of 600, and a Unit-Cost of 6, yield a Production-Decision of 400.
Since during the interval [Jan 94, June 94), the value of Market-Constant is 10000, Competitors-Total-Estimations is either 600 or 570, and Unit-Cost is 6, Production-Decision in that interval is either 400 or 405.
As Figure 1 demonstrates, the number of values and their associated temporal intervals resulting from the computation of a single temporal data dependency may be very large.
These values are a subset of the Cartesian product of all the possible values of each data-item.
A naive approach would consider all the possible combinations in the Cartesian set (342=24 combinations in the example of Figure 1), yielding an algorithm with high time complexity.
However, due to the bound temporal validity of values, usually only a small subset of the combinations in the Cartesian set should be considered.
For example, in Figure 1 only interpreted as all the days from February 1, 1994 to May 31, 1994 (June 1, 1994 is not included).
A time interval is dened in [5] as \the time between two insatnces" and can be represented as either close or semi-open intervals.
8 combinations out of the 24 possible ones should be considered.
This work presents an algorithm that eciently computes temporal data dependencies.
Our approach for ecient evaluation of temporal data dependencies is based in part on previous works on computing temporal aggregates, including [8] and [6].
An aggregate function, such as selecting the minimumvalue of a set, is applied to a set of values (e.g.
relations in the relational database model) and yields a scalar value.
In temporal databases, the aggregate function is, in general, time dependent, i.e.
the result of the aggregate function is applied to a set of values, each possibly having a dierent temporal validity.
The calculation of temporal data dependencies is an extension of aggregate computing with temporal grouping, where the resulting values are grouped by time.
To carry out such calculation, it is necessary to know which values have overlapping validity intervals, and to consider each value in its own validity interval.
The approach proposed in [8] rst determines constant intervals as intervals within which there is no change in the data-item value.
It then selects tuples that overlap each of these constant intervals and calculates the result.
The work in [6] is based on a tree data structure for the time axis partition.
Extending these approaches to solve the problem of evaluating temporal data dependencies, we present a polynomialalgorithm for ecient evaluation of temporal data dependencies with simultaneous values.
The computation is not necessarily an aggregate operation that involves a single type of data-item with several values.
Rather, it is a formula that may involve several types of data-items, each of which may consist of simultaneous values.
The algorithm constructs a list sorted according to the time validity of data-items values, and then calculates the result as a function of the values in the list elements.
Section 2 presents the algorithm for calculating temporal data dependencies with simultaneous values, while the properties of the algorithm are discussed in Section 3.
2 Evaluation of temporal data dependencies with simultaneous values  In this section we provide an outline of the algorithm for evaluating temporal data dependencies with data-items that consist of simultaneous values.
The algorithm consists of two phases, namely generating a constant interval list and computing the value for each combination of each constant interval element, as follows.
The constant interval list is sorted by the starting time points of the constant intervals.
The algorithm generates a partition of the valid time interval within which the temporal data dependency is to be determined.
Each element of the constant interval list has a valid time , and it consists of all the values whose validity covers .
Initially, a constant interval element is generated s , te ), where ts and with a valid time interval of [t te are the start and end time points of the interval within which the temporal data dependency is to be  evaluated, respectively.
Each value is processed with respect to an interval that consists of its starting time point, as follows.
Let [ts , te ) be a valid time interval of a value val, and [tis , tie ) a constant time interval associated with a constant interval element cii .
If [ts, te )\[tis, tie )6= ;, then there are six possible relationships between [ts , te ) and [tis, tie ), which are listed below along with the corresponding actions taken by the algorithm.
1. ts = tis and te < tie : replace cii with two constant interval elements, ci with [ts , te ) and ci with [te, tie ).
ci and ci receive the values of cii , and val is added to ci .
2. ts = tis and te = tie: add val to cii .
3. ts = tis and te > tie: add val to cii , and process val again with a valid time of [tie , te ).
4. ts > tis and te < tie : replace cii with three constant interval elements, ci with [tis, ts), ci with [ts , te ) and ci with [te, tie ).
ci , ci and ci receive the values of cii , and val is added to ci .
5. ts > tis and te = tie : replace cii with two constant interval elements, ci with [tis, ts ) and ci with [ts, te ).
ci and ci receive the values of cii , and val is added to ci .
6. ts > tis and te > tie : replace cii with two constant interval elements, ci with [tis, ts ) and ci with [ts, te ).
ci and ci receive the values of cii , and val is added to ci .
In addition, process val again with a valid time of [tie, te ).
As an example of the activation of the rst part of the algorithm, consider the data set of Figure 1, and assume that the interval within which the temporal data dependency is to be evaluated is [Feb 94, Aug 95).
The initial element of the list would be h[Feb 94, Aug 95), Market-Constant=, Competitors-TotalEstimations=, Unit-Cost=i.
The market-Constant values were processed rst, then the CompetitorsTotal-Estimations values, and nally the Unit-Cost values.
The full constant interval list is presented in Figure 2.
To enhance comprehension, the gure presents all the elements that were generated throughout the process, in a form of a tree.
Each node in the tree (except the root node) is an element of the list that was generated as a result of processing a value.
A value at the bottom of a node represents the value whose processing resulted in splitting the node.
The nal Constant Interval List (CIL) is the set of all leaf nodes of the tree, represented in Figure 2 by bold rectangles.
All other nodes were deleted during the process.
The Production-Decision values, shown in Figure 1, are shown within the constant interval rectangles in Figure 2.
0  0  00  00  0  0  00  0  00  00  0  0  00  0  00  00  00  0  00  00  Constant interval: Market Constant: Competitors Total Estimation: Unit Cost: 10000  Constant interval: Market Constant:  Constant interval: 10000  Market Constant:  Competitors Total Estimation:  600, 570  Competitors Total Estimation:  Unit Cost:  6  Unit Cost:  9500  Constant interval:  Constant interval: Market Constant:  Market Constant: 9000  9500  Competitors Total Estimation:  Competitors Total Estimation:  Unit Cost:  Unit Cost:  600  Constant interval:  540  Constant interval:  Constant interval:  Constant interval:  Market Constant:  9500  Market Constant:  Market Constant:  9000  Market Constant:  9000  Competitors Total Estimation:  600, 570, 540  Competitors Total Estimation:  Competitors Total Estimation:  540, 630  Competitors Total Estimation:  630  Unit Cost:  Unit Cost:  Unit Cost:  6  9500  7  Unit Cost:  7  570  Constant interval: Market Constant:  3 Algorithm properties  Constant interval:  9500  This section discusses the algorithm complexity (Section 3.1) and the correctness of the algorithm (Section 3.2).
Market Constant: 9500  Competitors Total Estimation:  570, 540  Competitors Total Estimation:  Unit Cost:  6  Unit Cost:  540  6  Constant interval:  94).
The next constant interval element is scanned, and the same combination is found.
Therefore, the valid time of the combination is set to be [June 94, Oct 94).
The same combination is found again in the subsequent constant interval element, and the valid time of the combination is set to be [June 94, Dec 94).
At this point, the process is terminated since the following constant interval element does not consist of this combination.
The result of the second phase of the algorithm, applied on the data set of Figure 1 is the set of values of Production-Decision, which are also shown in Figure 1.
After deciding on the appropriate interval, the values are used for calculating the derived value for that interval.
For example, in the previous example, the derived value is calculated to be 385, valid during [June 94, Dec 94).
Constant interval:  Market Constant:  9500  Market Constant:  9500  Competitors Total Estimation:  540  Competitors Total Estimation:  540  Unit Cost:  6  Unit Cost:  7  Figure 2: The constant interval list (CIL) generation process A single constant interval element may consist of more than a single value for a data-item.
For example, the constant interval element with the constant interval [Feb 94, June 94) in Figure 2 has two sets of values: h10000, 600, 6i, and h10000, 570, 6i.
Each such set of values is a dierent combination for the calculation of the temporal data dependency, giving rise to a dierent value for the same interval.
The constant interval list may also be over split with respect to a combination, i.e.
it may have consecutive constant interval elements with identical sets of values.
For example, all the constant interval elements with the constant intervals [June 94, Sep 94), [Sep 94, Oct 94), and [Oct 94, Dec 94) consist of the combination h9500, 540, 6i.
This is a result of several overlapping values of the same data-item.
In this case, [June 94, Sep 94) and [Sep 94, Oct 94) are separate constant intervals, since the value 600 of CompetitorsTotal-Estimation is valid only during [June 94, Sep 94).
The second phase of the algorithm uses each of the possible value combinations in the constant interval elements to compute the new values.
The combinations are scanned for each constant interval element, starting from the constant interval element with the minimal valid time.
Subsequent constant interval elements are scanned to nd identical combinations.
If an identical combination is found, the valid time of the constant interval element is added to the valid time of the combination.
This process is repeated until no more identical combinations can be found.
For example, consider the example given in Figure 2.
The combination h9500, 540, 6i is selected from the constant interval element with the valid time of [June 94, Sep  3.1 Complexity  Let n be the number of processed values.
A value with a valid time interval of [ts, te ) can add two constant interval elements at the most, if for a constant interval element cii , ts > tis and te < tie , or if there are two constant interval elements cii and cij such that ts > tis and te < tje .
Consequently, if the number of processed values is n, then the upper bound on the number of constant interval elements is 2n.
For each value, the location of the rst constant interval element to be processed is searched.
This search is bounded by log2(2n).
In the worst case, a valid time of a value covers the valid times of all of the constant interval elements, resulting in 2n comparisons.
Hence, the time complexity of the CIL generation phase is bounded by O(n(log2 (2n) + 2n)) =O(n2 ).
The time complexity of the second phase of the algorithm is bound by O(m3 ), where: m is the number of all the valid combinations of a single value from all the data-items, i.e.
all the combinations for which values have valid time overlapping.
For example, in Figure 1, m=8 and the eight combinations are: h10000, 600, 6i, h10000, 570, 6i, h9500, 600, 6i, h9500, 570, 6i, h9500, 540, 6i, h9500, 540, 7i, h9000, 540, 7i, h9000, 630, 7i.
2n is the maximal number of constant interval elements.
for example, 18 is the maximal number of constant interval elements in Figure 1 since the number of state-elements is 9.
However, The actual number of constant interval elements is 7, as shown in Figure 2.
2mn is the maximal number of possible combinations in the list.
The algorithm generates all of the list combinations (2mn).
At each iteration of the algorithm, a single combination is processed.
The worst case is when at each iteration all the remaining combinations are scanned.
Thus, the worst case complexity is O(m2 n).
Since at each  constant interval element there is at least one combination, m  n. Therefore, the worst case complexity is bounded by O(m3 ).
From the discussion above we can conclude that the worst case complexity of the two phases of the algorithm is bounded by O(m3 ).
3.2 Correctness  Proposition 1 The partition of the time interval:  The entire set of constant interval elements constitute a partition of the evaluated interval.
The proof of Proposition 1 is done using induction on the number of constant interval elements.2 At each iteration we verify that each of the six possible relationships between a valid time of a value and a constant interval element results in a new constant interval list that maintains the partition assertion.
This proposition ensures the correctness of the list construction phase, where each value should allocate a single constant interval element.
Proposition 2 Algorithm correctness: The algorithm generates a correct result.
Given a set of values and a temporal data dependency, a correct result ensures that for each combination of values with overlapping valid times there is a value which is the result of applying the temporal data dependency on this combination, and its valid time consists of the intersection of the overlapping valid times of the values in the combination.
A simple algorithm, in which each combination of the Cartesian product is evaluated, can achieve a correct result but at a high computational cost.
The proof of Proposition 2 shows that if a combination is not processed by the algorithm, then it should not be in the resulting set.
4 Conclusion and future research  We have proposed an algorithm for ecient evaluation of temporal data dependencies in temporal databases with simultaneous values.
The algorithm consists of two phases, the rst generates a constant interval list and the second computes the value for each combination of each constant interval element.
A single constant interval element may consist of more than a single value for a data-item.
The constant interval list may also be over split with respect to a combination, i.e.
it may have consecutive constant interval elements with identical sets of values.
The time complexity of the calculation algorithm is bound by O(m3 ), where m is the number of all the valid combinations of a single value from all the data-items.
This result is less expensive, computation wise, than the 2 Full denitions and proofs of propositions in this paper can be obtained via anonymous ftp to ftp.technion.ac.il under directory/usr/local/servers/ftp/pub/supported/ie.
The le is called proofs.tex.
It is produced using LaTEX.
The proofs can also be obtained through the author's WWW home page, http://www.cs.toronto.edu/avigal.
result of scanning the Cartesian product (O(ni=1 mi ), where mi is the number of values of the i-th dataitem and n is the number of data-items involve in the calculation.
A prototype of a system that implements the algorithm exists on the basis of MAGIC 5.6 for DOS, under DOS 6.2.
Further research is aimed at a more general form of temporal data dependencies, where the valid time is dened indirectly through constraints, or relative to other time points.
References  [1] A. Cournot.
Researches into the Mathematical Principles of the Theory of Wealth.
Macmillan, New York, N.Y., 1897.
[2] O. Etzion, A. Gal, and A. Segev.
Temporal active databases.
In Proceedings of the International Workshop on an Infrastructure for Temporal Database, June 1993.
[3] K.M.
Van Hee, L.J.
Somers, and M. Voorhoeve.
A modeling environment for decision support systems.
Decision Support Systems, 7:241{251, 1991.
[4] R. Hull and R. King.
Semantic database modeling: Survey, application and research issues.
ACM Computing Surveys, 19(3):201{260, Sep 1987.
[5] C.S.
Jensen, J. Cliord, S.K.
Gadia, A. Segev, and R.T. Snodgrass.
A glossary of temporal database concepts.
ACM SIGMOD Record, 21(3):35{43, 1992.
[6] N. Kline and R.T. Snodgrass.
Computing temporal aggregates.
In Proceedings of the International Conference on Data Engineering, pages 223{231, Mar 1995.
[7] J. Tirole.
The Theory of Industrial Organization.
the MIT press, 1989.
[8] P.A.
Tuma.
Implementing historical aggregates in TempIS.
Master thesis.
Wayne State University, Nov. 1992.
Appendix A: The production decision temporal data dependency In this section we present the derivation of the temporal dependency graph given in Section 1.
The following notation is used: A  Prot B  Revenue C  Cost D  Market-Price E  Fixed-Cost F  Unit-Cost G  Market-Constant H  Total-Quantity I  Competitors-Total-Estimation X  Production-Decision We assume that Bilbo's strategy is to produce the amount that would maximize A, as follows.
A = B?C= X  D ?
(E + X  F) = X G H ?
(E + X  F) = X  X G+ I ?
(E + X  F) max (A) =) A0 = 0 =) G  (X(X+ +I) I)?2 X  G ?
F = 0 2 =) G  (X + I) ?
(XX + GI)?2 F  (X + I) = 0 =) F  X2 + 2  F  I  X + F  I2 ?
G  I = 0 p ?
2  F  I  4  F2  I2 ?
4  F  (F  I2 - G  I) =) X = = 2F qG  I ?I F  X is non-negative.
q  =) X = max( GF I ?
I, 0)
Temporal Resolution: A Breadth-First Search Approach Clare Dixon Department of Computing Manchester Metropolitan University Manchester M1 5GD United Kingdom C.Dixon@doc.mmu.ac.uk  Abstract  An approach to applying clausal resolution, a proof method for classical logics suited to mechanisation, to temporal logics has been developed by Fisher.
The method involves translation to a normal form, classical style resolution within states and temporal resolution between states.
The method consists of only one temporal resolution rule and is therefore particularly suitable as the basis of an automated temporal resolution theorem prover.
As the application of this temporal resolution rule is the most costly part of the method, involving search amongst graphs, it is on this area we focus.
A breadth-rst search approach to the application of this rule is presented and shown to be correct.
Analysis of its operation is carried out and test results for its comparison to a previously developed depth-rst style algorithm given.
1 Introduction  Temporal logics have been used extensively for the specication and verication of properties of concurrent systems, see for example 18, 19, 15, 11, 3, 21, 2, 14].
Important computational properties such as liveness, deadlock and mutual exclusion can be expressed easily and simply in temporal logic making it useful for specication.
Verifying that a temporal logic specication satises a temporal property usually requires some form of theorem proving.
Model checking approaches have been the most popular, particularly based on tableau 25] or automata 23].
However, standard model checking approaches are limited as only nite state problems can be handled and, even then the number of states required soon becomes large due to the combinatorial explosion.
Alternatively one can adopt a resolution based approach 22] which is is not limited to nite state problems and has a signicant body of work on heuristics to control search (see This work was supported partially by an EPSRC PhD Studentship and partially by EPSRC Research Grant GR/K57282  standard texts such as 6] for example).
Decision procedures based on resolution have been developed for temporal logics in 5, 24, 1], however in many cases they are unsuitable for implementation either because they only deal with a small number of the temporal operators or because of problems with proof direction due to the large numbers of resolution rules that may be applied.
In this paper we present a breadth-rst search style algorithm which enables practical implementation of the resolution method for temporal logics developed by Fisher 9].
The resolution procedure is characterised by translation to a normal form, the application of a classical style resolution rule to derive contradictions that occur at the same points in time (termed step resolution), together with a new resolution rule, which derives contradictions over time (termed temporal resolution).
This paper is structured as follows.
In x2, a description of the propositional temporal logic used and the normal form required for the temporal resolution method is given.
An outline of this temporal resolution method is given in x3, while the BreadthFirst Search algorithm to implement the temporal resolution step is described in x4.
The output of the Breadth-First Search algorithm is examined in x5, results comparing it with a previously described algorithm and conclusions are drawn in x6.
2 A linear temporal logic  Here we summarise the syntax and semantics of the logic used and describe the normal form required for the resolution method.
2.1 Syntax and semantics  The logic used in this report is Propositional Temporal Logic (PTL), in which we use a linear, discrete model of time with nite past and innite future.
PTL may be viewed as a classical propositional logic augmented with both future-time and past-time tempo-  ral operators.
Future-time temporal operators include `}' (sometime in the future ), ` ' (always in the future ), ` g' (in the next moment in time ), ` U ' (until ), ` W ' (unless or weak until ), each with a corresponding past-time operator.
Since our temporal models assume a nite past, for convenience, two last-time operators are used namely ` v' (weak last) and ` bcdef' (strong last).
When evaluated at any point other than the beginning of time, both bcdefA and vA are true if and only if A was true at the previous moment.
However, for any formula A, bcdefA is false, when interpreted at the beginning of time, while vA is true at that point.
Particularly, vfalse is only true when interpreted at the beginning of time.
The weak lasttime operator may be dened in terms of the strong last operator as follows  A:  v  :A:  f e d bc  Models for PTL consist of a sequence of states, representing moments in time, i.e.,   = s0 fi s1 fi s2 fi s3 fi : : : Here, each state, si , contains those propositions satised in the ith moment in time.
As formulae in PTL  are interpreted at a particular moment, the satisfaction of a formula f is denoted by (fi i) j= f where  is the model and i is the state index at which the temporal statement is to be interpreted.
For any well-formed formula f , model  and state index i, then either (fi i) j= f or (fi i) 6j= f .
For example, a proposition symbol, `p', is satised in model  and at state index i if, and only if, p is one of the propositions in state si , i.e., (fi i) j= p i p 2 si : The semantics of the temporal connectives used in the normal form or the resolution rule are dened as follows (fi i) j= vA i i = 0 or (fi i ; 1) j= A (fi i) j= bcdefA i i > 0 and (fi i ; 1) j= A (fi i) j= }A i there exists a j > i s.t.
(fi j ) j= A (fi i) j= A i for all j > i then (fi j ) j= A (fi i) j= A U B i there exists a k > i s.t.
(fi k) j= B and for all i 6 j < k then (fi j ) j= A (fi i) j= A W B i (fi i) j= A U B or (fi i) j= A: The full syntax and semantics of PTL will not be presented here, but can be found in 9].
2.2 A normal form for PTL  Formulae in PTL can be transformed to a normal form, Separated Normal Form (SNF), which is the basis of the resolution method used in this paper.
SNF was introduced rst in 9] and has been extended to rst-order temporal logic in 10].
While the translation from an arbitrary temporal formula to SNF will not be described here, we note that such a transformation preserves satisability and so any contradiction generated from the formula in SNF implies a contradiction in the original formula.
Formulae in SNF are of the general form  ^ Ri i  where each Ri is known as a rule and must be one of the following forms.
false  v  f e d bc  ^g ka  a=1  false  v  f e d bc  ^g ka  a=1  ) ) ) )  _r lb  (an initial  _ lb  (a global  b=1 r  b=1  }l }l  {rule) {rule)  (an initial }{rule) (a global }{rule)  Here ka , lb , and l are literals.
The outer ` ' operator, that surrounds the conjunction of rules is usually omitted.
Similarly, for convenience the conjunction is dropped and we consider just the set of rules Ri .
We note a variant on SNF called merged-SNF (SNFm ) 9] used for combining rules by applying the following transformation.
A B f(A ^ B ) e d bc f e d bc  f e d bc  ) ) )  F G F ^G  The right hand side of the rule generated may have to be further translated into Disjunctive Normal Form (DNF), if either F or G are disjunctive, to maintain the general SNF rule structure.
3 The resolution procedure  Here we present a review of the temporal resolution method 9].
The clausal temporal resolution method consists of repeated applications of both `step' and `temporal' resolution on sets of formulae in SNF, together with various simplication steps.
3.1 Step resolution  `Step' resolution consists of the application of standard classical resolution rule to formulae representing constraints at a particular moment in time, together with simplication rules for transferring contradictions within states to constraints on previous states.
Simplication and subsumption rules are also applied.
Pairs of initial {rules, or global {rules, may be resolved using the following (step resolution) rule where L1 and L2 are both last-time formulae.
L1 ) L2 ) (L1 ^ L2 ) )  A_r B _ :r A_B  Once a contradiction within a state is found using step resolution, the following rule can be used to generate extra global constraints.
P  f e d bc  true  v  false  ) ) :P  This rule states that if, by satisfying P in the last moment in time a contradiction is produced, then P must never be satised in any moment in time.
The new constraint therefore represents :P (though it must rst be translated into SNF before being added to the rule-set).
The step resolution process terminates when either no new resolvents are derived, or false is derived in the form of one of the following rules.
false true  v  f e d bc  ) )  false false  3.2 Temporal resolution During temporal resolution the aim is to resolve a }{rule, LQ ) }l, where L may be either of the last-  time operators, with a set of rules that together imply :l, for example a set of rules that together have the eect of bcdefA ) :l. However the interaction between the ` g' and ` ' operators in PTL makes the denition of such a rule non-trivial and further the translation from PTL to SNF will have removed all but the outer level of {operators.
So, resolution will be between a }{rule and a set of rules that together imply an {formula which will contradict the }{ rule.
Thus, given a set of rules in SNF, then for every rule of the form LQ ) }l temporal resolution may be applied between this }{rule and a set of global {rules, which taken together force :l always to be satised.
The temporal resolution rule is given by the following  A0 ::: fA e d bc n LQ f e d bc  true  v  ) ) )  )  F0 ::: Fn }l n ^ :Ai :Q _ n i ^ ( :Ai ) W l =0  LQ )  with side conditions for all 0   i   n and  8> < >:  i=0  ` `  Fi ) :l _n Fi ) Aj j =0  9> = >	  where the side conditions ensure that the set of rules fA ) F together imply e d bc :l. In particular the i i rst side condition ensures that each rule, bcdefAi ) Fi , makes :l true now if bcdefAi is satised.
The second side condition ensures that the right hand side of each rule, fA ) F , means that the left hand side of one of e d bc i i the rules in the set will be satised.
So once the left hand side of one of these rules is satised, i.e.
if Ai is satised for some i in the last moment in time, then :l will hold now and the left hand side of another rule will also be satised.
Thus at the next moment in time again :l holds and the left hand side of another rule is satised and so on.
So if any of the Ai are satised then :l will be always be satised, i.e., f e d bc  _n Ak )  k=0  :l:  Such a set of rules are known as a loop in :l.  3.3 The temporal resolution algorithm  Given any temporal formula  to be shown unsatisable the following steps are performed.
1.
Translate  into a set of SNF rules s .
2.
Perform step resolution (including simplication and subsumption) until either (a) false is derived - terminate noting  unsatisable or (b) no new resolvents are generated - continue at step 3.
3.
Select an eventuality from the right hand side of a }{rule within s , for example }l. Search for loops in :l and generate the resolvents.
4.
If any new formulae have been generated, translate the resolvents into SNF add them to the ruleset and go to step 2, otherwise continue to step 5.
5.
Terminate declaring  satisable.
Completeness of the resolution procedure has been shown in 16].
3.4 Loop search  As it is the application of the temporal resolution rule, i.e.
the search for a set of rules that together imply :l, assuming we are resolving with }l, that is the most dicult part of the problem it is on this we concentrate in the rest of the paper.
Dierent approaches to detecting such loops have been described in 7] and in particular a depth-rst search style algorithm was outlined in 8].
With this algorithm, rules are used as edges in a graph and nodes represent the left hand sides of rules.
The Depth-First Search algorithm uses SNFm rules to try build a path of nodes, where every path leads back into the set of nodes already explored.
The SNFm rules are applied one at a time in a depth-rst manner, as several SNFm rules may be used to expand from a particular node, backtracking when a dead end is reached.
The rules governing expansion from a node ensure that the desired looping occurs and, assuming we are resolving with }l, that the required literal :l, is obtained.
In the next section we give an alternative, BreadthFirst Search algorithm, for detecting loops together with an example of its use.
4 Breadth-First Search  Using the Breadth-First Search Algorithm, rules are only selected for use if they will generate the required literal at the next moment in time and their right hand side implies the previous node.
The algorithm operates on SNF rules, only combining them into SNFm when required.
With Breadth-First Search all possible rules (but avoiding the duplication of information) are used to expand the graph, rather than just selecting one rule.
The graph constructed using this approach is a sequence of nodes that are labelled with formulae in Disjunctive Normal Form.
This represents the left hand sides of rules used to expand the previous node which have been disjoined and simplied.
If we build a new node that is equivalent to the previous one, using this approach, then we have detected a loop.
However if we cannot create a new node then we terminate without having found a loop.
4.1 Breadth-First Search Algorithm For each rule of the form LQ ) }l carry out the  following.
1.
Search for all the rules of the form bcdefXk ) :l, for k = 0 to b (called start rules), disjoin the left hand sides and make the top node H0 equivalent to this, i.e.
H0 ,  _b Xk :  k=0  Simplify H0 .
If ` H0 , true we terminate having found a loop.
2.
Given node Hi , build node Hi+1 for i = 0fi 1fi : : : by looking for rules or combinations of rules of the form bcdefAj ) Bj , for j = 0 to m where ` Bj ) Hi and ` Aj ) H0 .
Disjoin the left hand sides so that  Hi+1 ,  _m Aj  j =0  and simplify as previously.
3.
Repeat (2) until (a) ` Hi , true.
We terminate having found a Breadth-First loop and return true.
(b) ` Hi , Hi+1 .
We terminate having found a Breadth-First loop and return the DNF formula Hi .
(c) The new node is empty.
We terminate without having found a loop.
Algorithms to limit the number of rule combinations required have also been developed and are given in 7].
Input to the Breadth-First Search will be a set of SNF rules and these algorithms show how to test whether rules can be used as they are for node expansion, require combination only with start rules, or must be combined with other rules.
Similarly, when rules are combined together an algorithm is given to make the number of combinations required as few as possible.
Nodes (in DNF) are kept in their simplest form by carrying out simplication and subsumption.
4.2 Example  Breadth-First Search is used to detect the loop in the set of rules given below.
Assume we are trying to resolve with the rule LQ ) }l where L is either of the last-time operators, and the set of global -rules is fe e d bc 1: bcdefa ) :l 5: ) e f e d c b f e d c b 2: b ) :l 6: (a ^ e) ) a fb e d bc 3: bcdefc ) :l 7: ) b f e d bc f e d bc 4: d ) :l 8: c ) b  To create a new node Hi , we examine each rule in turn, disjoining the conjunction of literals on the left hand side of the new node if the rule satises the criteria given.
The new node is then simplied where necessary.
1.
The rules 1{4 have :l on their right hand side.
We disjoin their left hand sides and simplify (although in this case no simplication is necessary) to give the top node  H0 = a _ b _ c _ d: 2.
To build the next node, H1 , we see that rules 6, 7 and 8 satisfy the expansion criteria in step (2) of the Breadth-First Search Algorithm (i.e.
the right hand side and the literals on the left hand side of each rule implies H0 ) but rule 5 does not.
Note if we combine rule 5 with any of the other rules to produce an SNFm rule that satises the expansion criteria, its left hand side will be removed through simplication.
So we disjoin the literals on their left hand sides of rules 6, 7 and 8 to obtain node  H1 = (a ^ e) _ b _ c: 3.
Rules 7 and 8 satisfy the expansion criteria and so do rules 5 and 6 when combined together to give the rule bcdef(a ^ e) ) a ^ e: Thus node H2 becomes H2 = (a ^ e) _ b _ c: As H2 , H1 we terminate having detected a loop.
The Breadth-First loop we have found is (a ^ e) _ b _ c. The graph constructed using Breadth-First Search for this set of rules is shown in Figure 1.
4.3 Resolvents  W  The Breadth-First Search algorithm returns a DNF formula i Di that means (assuming we are resolving with LQ ) }l) if this formula is satised in the previous moment in time then :l always holds, i.e.
f e d bc  _ Di ) i  :l:  Rather than try reconstruct a set of rules with which to apply the temporal resolution rule we use the output from Breadth-First Search directly (as if each disjunct were the left hand side of a rule).
In the previous example the loop output was (a ^ e) _ b _ c and the resolvents obtained are  true  v  ) :Q _ :((a ^ e) _ b _ c) LQ ) :((a ^ e) _ b _ c) W l  and must be further translated into SNF.
4.4 The main processing cycle  The Breadth-First search procedure nds all loops (see x5) for a particular eventuality.
Each time round the main processing loop an eventuality is taken with which to resolve.
Termination, declaring the formula satisable is only allowed if no more new loops may be found (and step resolution produces no new rules).
Given Li , for i = 1 to n, loops detected previously we can ensure that a new loop L provides no new information by checking whether n _ L ) Li : i=1  This is because any new resolvents produced from L will be subsumed by those already generated from loops Li .
4.5 Correctness issues H0 H1  a_b_c_d (  a ^ e) _ b _ c  Figure 1 Breadth-First Search Example  Soundness, completeness and termination of the Breadth-First Search algorithm are proved in 7].
5 Breadth-First Search loops  The two main characteristics of loops output by the Breadth-First Search are that all loops are detected for an eventuality and that paths into the loop are also detected.
These are described below and compared with the output of the Depth-First Search algorithm.
Finally we consider the memory implications for Breadth-First Search.
5.1 Detection of all the loops The Breadth-First Algorithm nds all the loops for a particular }{rule.
Considering the example given in x4.2 it is clear that the three SNFm rules,  M1 M2 M3  (a ^ e)  f e d bc  b fc e d bc  f e d bc  ) ) )  (a ^ e ^ :l) (b ^ :l) (b ^ :l)  satisfy the criteria for a loop together (and in any combination of subsets of these three rules where if rule M 3 occurs then rule M 2 also occurs).
We could combine these SNFm rules further to give other SNFm rules for example combining rule M 1 and M 2 we obtain  M4  (a ^ b ^ e)  f e d bc  )  (a ^ b ^ e ^ :l):  Performing Breadth-First Search we return the formula (a ^ e) _ b _ c as shown previously (because of simplication we will never return (a ^ e) _ b _ c _ (a ^ b ^ e) for example).
However performing a Depth-First Search we detect loops one at a time i.e.
we would only ever return a loop relating to either rule M 1 or rule M 2 or rule M 4.
5.2 The lead into the loop  Breadth-First Search nds disjuncts representing rules that are never detected in Depth-First Search.
An example of this is the disjunct c representing the use of the SNFm rule bcdefc ) (b ^ :l): This rule represents the path into the loop (there are no rules that make this rule `re').
Both systems are still complete but having found the Depth-First loop bcdefb ) (b ^:l)fi for example, we will have to carry out further applications of the step resolution rule to generate rules equivalent to the complete set of resolvents from BreadthFirst Search.
5.3 Memory considerations  Usually a disadvantage of breadth-rst search algorithms is the amount of memory required to construct the search space.
The Breadth-First Search Algorithm described here does attempt to use all the rules or combinations of rules to construct the next node so, in the worst case, memory requirements may be a problem.
However, such problems are avoided in many cases for the following reasons.
Firstly, the BreadthFirst Search Algorithm only requires the storage of the top node, H0 , and the last node constructed, Hi , to enable the construction of node Hi+1 and to test for termination.
The full search space constructed between these two points is not required and therefore after the construction of node Hi+1 , and failure of the termination test, the memory required for the  storage of node Hi may be released.
Secondly each node constructed is a DNF formula kept in its simplest form so any redundant information is removed from the node.
Finally, although the number of rules in which we search for loops may be large, containing many propositions, typically, the loops will relate to only a few of these rules containing a subset of the total number of propositions.
6 Results and conclusions  Results comparing the Breadth-First Search Algorithm with a previous loop search algorithm are given and conclusions are then drawn.
6.1 Results  A prototype implementation performing the temporal resolution method has been built.
Two dierent loop search programs have been provided|one for Breadth-First Search, the other for the DepthFirst Search algorithm.
The programs are written in SICStus Prolog 4] running under UNIX and timings have been carried out on a SPARCstation 1 using compiled Prolog code.
The test data is a set of valid temporal formulae taken from 13] chosen as it is a reasonably sized collection of small problems to be proved valid.
An example of the type of formula being shown valid (we actually shown the negation is unsatisable) is  }w1 ^ }w2 ) }(w1 ^ }w2 ) _ }(w2 ^ }w1 ): We note that, although not presented in the results given here, larger examples have also been tackled, for example Peterson's Algorithm 17, 20].
The full set of results, including timings for each eventuality and example, are given in 7] however, due to space restrictions, we only present a summary of the data here.
Table 1 shows the number of times the total loop search for Depth-First Search (DFS) was less than or equal to, or greater than that for Breadth-First Search (BFS), for each eventuality and example.
The gures in brackets are the values as percentages.
Values are given for the full data set and for those examples where at least one of the times is greater than 60 and then 100 milliseconds.
By considering these categories we hope to eliminate inaccuracies from very low timings.
The gures in Table 1 indicate that BreadthFirst Search performs better in signicantly more examples than Depth-First Search.
The increase in the percentage of examples where Breadth-First Search is quicker than Depth-First Search as we move from the examples with (at least one) time over 60 milliseconds to (at least one) time over 100 milliseconds suggests  Subset of Data DFS 6 BFS BFS < DFS (i) Full data set 20 (40 %) 30 (60 %) (ii) Time > 60 10 (37 %) 17 (63 %) (iii) Time > 100 6 (27 %) 16 (73 %)  Table 1 Summary of Comparative Timings that Breadth-First Search performs better on larger examples.
Further, the raw data shows that timings for Depth-First Search have a larger range of values than Breadth-First Search.
This is what we would expect from depth-rst search style algorithms because if the correct search path is chosen rst the solution can be detected more quickly than breadth-rst search type algorithms.
However, if a large amount of time is spent exploring fruitless paths then the overall time may be far greater than that for Breadth-First Search.
Table 2 shows the same data displayed in columns representing the number of calls made by each example to the Depth-First Search algorithm, i.e.
how many times we have had to search for a loop using DepthFirst Search for each example.
Recall that BreadthFirst Search nds all loops for an eventuality where as Depth-First Search nd them one at a time.
The rst row in the table gives the total number of examples for the respective number of calls.
The second and third rows give the number and percentage, respectively, of these examples where the time for detecting the loop with Breadth-First Search was less than the total time spent in loop search by Depth-First Search.
The column headed 3 has been omitted as there were no examples that required 3 calls to the Depth-First Search Algorithm.
1 2 4 5 Total 25 22 2 1 BFS < DFS (No.)
13 14 2 1 BFS < DFS (%) 52% 64% 100% 100%  Table 2 Summary by Number of Calls The table indicates that for this set of examples the more calls to the loop nding section that Depth-First Search makes the more likely it is that it is quicker to do a Breadth-First Search than a Depth-First Search.
If we take the greater number of calls to the DepthFirst loop nding program as an indication of the size of the example, then this matches the observation that  was made previously that Breadth-First Search performs better on larger examples.
6.2 Conclusions  A Breadth-First Search algorithm for implementing Fisher's temporal resolution method has been described and given.
Output from this algorithm is analysed and compared with a previously described depthrst approach.
An prototype implementation has been produced and run on a variety of valid temporal formulae.
Test results suggest that breadth-rst search does perform better than this alternative on larger examples.
Although we have only considered the propositional version of the logic it may be possible to extend the resolution method to rst-order temporal logic (and even to other temporal logics).
Indeed a rst-order version of the normal form exists 10].
However, as full rst-order temporal logic is undecidable 12] we must rst consider subsets to which the resolution method can successfully be applied.
The suitability of Fisher's temporal resolution method to mechanisation, the algorithms developed for the temporal resolution step and prototype implementation together means that temporal resolution provides a viable option for automated temporal theorem proving.
6.3 Acknowledgements  Thanks to my supervisors Professor Howard Barringer and Dr. Michael Fisher for their guidance and encouragement during this work.
Thanks also to Graham Gough for many helpful comments and advice.
References  1] M. Abadi and Z.
Manna.
Nonclausal Deduction in First-Order Temporal Logic.
ACM Journal, 37(2):279{317, April 1990.
2] H. Barringer.
Using Temporal Logic in the Compositional Specication of Concurrent Systems.
In A. P. Galton, editor, Temporal Logics and their Applications, chapter 2, pages 53{90.
Academic Press Inc. Limited, London, December 1987.
3] H. Barringer, R. Kuiper, and A. Pnueli.
Now You May Compose Temporal Logic Specications.
In Proceedings of the Sixteenth ACM Symposium on the Theory of Computing, 1984.
4] M. Carlsson and J.
Widen.
SICStus Prolog User's Manual.
Swedish Institute of Computer Science, Kista, Sweden, September 1991.
5] A. Cavalli and L. Fari~nas del Cerro.
A Decision Method for Linear Temporal Logic.
In R. E.  Shostak, editor, Proceedings of the 7th International Conference on Automated Deduction, volume 170 of Lecture Notes in Computer Science, pages 113{127.
Springer-Verlag, 1984.
6] C. L. Chang and R. C. T. Lee.
Symbolic Logic and Mechanical Theorem Proving.
Academic Press, 1973.
7] C. Dixon.
Strategies for Temporal Resolution.
PhD thesis, Department of Computer Science, University of Manchester, 1995.
8] C. Dixon, M. Fisher, and H. Barringer.
A GraphBased Approach to Resolution in Temporal Logic.
In D. M. Gabbay and H. J. Ohlbach, editors, Temporal Logic, First International Conference, ICTL '94, Proceedings, volume 827 of Lecture Notes in Articial Intelligence, Bonn, Germany, July 1994.
Springer-Verlag.
9] M. Fisher.
A Resolution Method for Temporal Logic.
In Proceedings of the Twelfth International Joint Conference on Articial Intelligence (IJCAI), Sydney, Australia, August 1991.
Morgan Kaufman.
10] M. Fisher.
A Normal Form for First-Order Temporal Formulae.
In Proceedings of Eleventh International Conference on Automated Deduction (CADE), volume 607 of Lecture Notes in Computer Science, Saratoga Springs, New York, June 1992.
Springer-Verlag.
11] B. T. Hailpern.
Verifying Concurrent Processes Using Temporal Logic, volume 129 of Lecture Notes in Computer Science.
Springer-Verlag, 1982.
12] W. Hussak.
Decidability in Temporal Presburger Arithmetic.
Master's thesis, Department of Computer Science, University of Manchester, February 1987.
13] Z.
Manna and A. Pnueli.
Verication of Concurrent Programs: The Temporal Framework.
In Robert S. Boyer and J. Strother Moore, editors, The Correctness Problem in Computer Science, pages 215{273.
Academic Press, London, 1981.
14] Z.
Manna and A. Pnueli.
The Temporal Logic of Reactive and Concurrent Systems: Specication.
Springer-Verlag, New York, 1992.
15] S. Owicki and L. Lamport.
Proving Liveness Properties of Concurrent Programs.
ACM Transactions on Programming Languages and Systems, 4(3):455{495, July 1982.
16] M. Peim.
Propositional Temporal Resolution Over Labelled Transition Systems.
Unpublished Technical Note, Department of Computer Science, University of Manchester, 1994.
17] G. L. Peterson.
Myths about the Mutual Exclusion Problem.
Information Processing Letters, 12(3):115{116, 1981.
18] A. Pnueli.
The Temporal Logic of Programs.
In Proceedings of the Eighteenth Symposium on the Foundations of Computer Science, Providence, November 1977.
19] A. Pnueli.
The Temporal Semantics of Concurrent Programs.
Theoretical Computer Science, 13:45{60, 1981.
20] A. Pnueli.
In Transition From Global to Modular Temporal Reasoning about Programs.
In Krysztof Apt, editor, Logics and Models of Concurrent Systems, pages 123{144, La Colle-surLoup, France, October 1984.
NATO, SpringerVerlag.
21] A. Pnueli.
Applications of Temporal Logic to the Specication and Verication of Reactive Systems: A Survey of Current Trends.
In J.W.
de Bakker, W. P. de Roever, and G. Rozenberg, editors, Current Trends in Concurrency, volume 224 of Lecture Notes in Computer Science.
Springer-Verlag, August 1986.
22] J.
A. Robinson.
A Machine{Oriented Logic Based on the Resolution Principle.
ACM Journal, 12(1):23{41, January 1965.
23] M. Y. Vardi and P. Wolper.
An AutomataTheoretic Approach to Automatic Program Verication.
In Proceedings IEEE Symposium on Logic in Computer Science, pages 332{344, Cambridge, 1986.
24] G. Venkatesh.
A Decision Method for Temporal Logic based on Resolution.
Lecture Notes in Computer Science, 206:272{289, 1986.
25] P. Wolper.
The Tableau Method for Temporal Logic: An overview.
Logique et Analyse, 110{ 111:119{136, June-Sept 1985.
In Proc.
of the Third International Workshop on Temporal Representation and Reasoning (TIMEa96) May 19-20, 1996, Key West, Florida.
IEEE Computer Society Press, pp.
144-151.
Guiding and Rening Simulation using Temporal Logic Giorgio Brajnik  Daniel J. Clancy  Dip.
di Matematica e Informatica Universita di Udine 33100 Udine, ITALY  Department of Computer Sciences University of Texas at Austin Austin, TEXAS 78712  Abstract  We illustrate TeQSIM, a qualitative simulator for continuous dynamical systems.
It combines the expressive power of qualitative dierential equations with temporal logic by interleaving simulation with model checking to constrain and rene the resulting predicted behaviors.
Temporal logic expressions are used to specify constraints that restrict the simulation to a region of the state space and to specify trajectories for input variables.
A propositional linear{time temporal logic is adopted, which is extended to a three valued logic that allows a formula to be conditionally entailed when quantitative information specied in the formula can be applied to a behavior to rene it.
We present a formalization of the logic with theoretical results concerning the adopted model checking algorithm (correctness and completeness).
We show also an example of the simulation of a non{autonomous dynamical system and illustrate possible application tasks, ranging from simulation to monitoring and control of continuous dynamical systems, where TeQSIM can be applied.
1 Introduction  Reasoning about change across time is a common problem within articial intelligence and computer science in general.
For systems with discrete state spaces, temporal logic (TL) provides a broad class of formalisms that have been used for such tasks as reasoning about the eect of actions, specifying and verifying correctness of reactive computer programs and synthesizing or analyzing discrete event systems [13, 6, 2, 8].
Qualitative reasoning [10] has been used to reason about continuous change within the eld of dynamical systems in the presence of incomplete knowledge.
The expressiveness of the models used within qualitative simulation, however, is often limited to structural equations constraining the potential values for related variables.
The modeler is unable to express behavioral information about the trajectory of a variable or relationships between the trajectories of interconnected variables.
Such an information allows the modeler to restrict the simulation to a region of the state space and to specify trajectories for input variables.
 The research reported in this paper has been performed while visiting the Qualitative Reasoning Group at the University of Texas at Austin.
The Temporally Constrained QSIM (TeQSIM, pronounced tek'sim) algorithm combines the expressive power of these two paradigms by interleaving temporal logic model checking with the qualitative simulation process.
Temporal logic is used to specify qualitative and quantitative trajectory information that is incorporated into the simulation to constrain and rene the resulting behaviors.
Qualitative simulation constructs a set of possible behaviors consistent with a model of a dynamical system represented by a qualitative dierential equation (QDE).
The QSIM algorithm [10] represents the behavior of a dynamical system by an incrementally generated tree of qualitative states.
Each state describes the system at either a time{point or over a time{interval between two points by a tuple of qualitative values for the variables specied within the QDE.
Each qualitative value is described by a magnitude and a direction of change: the direction of change represents the sign of the variable's time derivative while the magnitude is dened upon a totally ordered set of distinctive landmark values and is either a landmark value or an interval between two landmark values.
The simulator uses the constraints specied within the QDE along with continuity to derive a branching{ time behavioral description.
Each path within the tree represents a potential behavior of the system: branches result from inherent ambiguity within the qualitative description.
Semi{quantitative simulation incorporates quantitative information into the qualitative simulation in the form of numeric ranges for landmark values and bounding envelopes for functions.
TeQSIM interleaves temporal logic model checking with the qualitative simulation process to obtain two major benets.
Behavior ltering tests each partial behavior against the set of temporal logic expressions representing trajectory constraints as the set of behaviors is incrementally generated.
A behavior is eliminated from the simulation when it can be shown that all of its possible completions fail to model the set of temporal logic expressions.
Thus, the space of the behavioral description is restricted to include only behaviors that can satisfy the temporal logic expressions.
Behavior renement integrates numeric information contained within the temporal logic expressions into the qualitative simulation to provide a more precise numerical description.
This process restricts an individual behavior to include only those real valued inter-  Simulation & Model Checking  Preprocessing Extended TL Expressions Event List  QDE & Initial State  Event Replacement  QDE Modifier  TL Expressions  Modified QDE & Initial State  TLaGuide  Filtered and Refined Behavior Tree  QSIM  Figure 1: TeQSIM architecture.
pretations which model the set of temporal logic expressions and therefore improves the precision of the prediction.
The integration of temporal logic model checking and qualitative simulationwas initially investigated by Kuipers and Shults [11].
They use a branching{time temporal logic to prove properties about continuous systems by testing the entire behavioral description against a temporal logic expression.
The appropriate truth value is returned depending upon whether or not the description models the expression.
Our work focuses on constraining the simulation as opposed to testing a simulation after it is completed.
The next section provides an overview of the TeQSIM algorithm.
Section 3 provides an example along with a discussion of some of the applications of this technique while section 4 describes the formal syntax and semantics of our temporal logic.
Soundness and completeness theorems are presented for the TL{ guide algorithm.
Finally, section 5 provides a discussion of some of the extensions that will be investigated in the future.
2 TeQSIM Overview  TeQSIM has been designed to provide the user with a mechanism for specifying trajectory constraints to guide and rene the qualitative simulation process.
By trajectory of a set of variables over a time interval (a; b)  < we mean a mapping from (a; b) to variable values ( < ).
Trajectory constraints on a set of variables restrict the possible trajectories for those variables.
Figure 1 provides an overview of the system architecture.
In general, the algorithm can be divided into two main components: a preprocessing stage that combines the trajectory information provided by the modeler into the qualitative model and generates the appropriate TL expressions; and a simulation and model checking stage that integrates model checking into the simulation process by ltering and rening qualitative behaviors according to a set of temporal logic expressions.
Trajectory information is specied in the form of an event list and a set of extended temporal logic statements.
An event is a time{point distinguished within the simulation.
The event list is a sequence of named, quantitatively bounded events not represented within the QDE that are incorporated into the simulation.
An extended temporal logic statement is simply a tem-  poral logic formula which may include direct references to events occurring within the event list.
These references are replaced by the appropriate formulae.
The simulation provides a complete temporal ordering between these externally dened events and other events dened within the model.
Model checking and behavior renement is performed by TL{guide.
Each time QSIM extends a behavior by the addition of a new state, the behavior is passed to TL{guide.
The behavior is ltered if there is sucient information within the partially formed behavior to determine that all completions of the behavior fail to model the set of TL expressions.
If the behavior can potentially model the set of TL expressions, then it is rened by the incorporation of any relevant quantitative information contained within the TL expressions.
Otherwise the behavior is retained unchanged.
3 Problem solving with TeQSIM  Incorporating temporal logic model checking into the qualitative simulation process allows the modeler to control the simulation by restricting the behavior of input and dependent variables.
Trajectory constraints can be used for a variety of tasks, including simulation of non{autonomous systems, incorporating observations into simulation, analysis of continuous control laws and performing goal oriented simulation.
The TeQSIM algorithm has been tested on a range of examples demonstrating each of the tasks above.
The following example demonstrates the use of TeQSIM to simulate a non{autonomous system incorporating information obtained via observation.
The system being simulated is a very simple one, which generalizes to the real{world problem of water supply control in the domain of lakes, rivers and dams [7].
Consider an open tank, with an uncontrolled inow u, a regulated outow y, valve opening v and amount of liquid A in the tank.
The system is modeled by the dierential equation A_ = u ?
y; y = f (A; v) where f (A; v) is an unknown monotonically increasing function on both arguments, numerically bounded.
This model can be straightforwardly specied using the QSIM QDE language.
Figure 2 shows the trajectory constraints used by TeQSIM for simulating the regulated tank.
Part (a) of the gure shows a totally ordered event list that provides quantitative temporal bounds for external events corresponding to two opening actions on the outow valve.
Four events are dened corresponding to the beginning and the end of each of these actions.
Part (b) contains the trajectory constraints that specify the behavior of the outow valve along with constraints on inow and level (up to the end of the rst opening action; the second action, not shown, is similarly specied): i. the variable Inflow is constant3 and its value is within the range [200, 220] cm /s, ii.
the valve opening is constant at 0.5 until the beginning of the rst opening action, iii.
between events b-open1 and e-open1 (i.e.
the duration of the rst opening action) the valve  (event (event (event (event  b-open1 e-open1 b-open2 e-open2  :time :time :time :time  (30 30)) ; sec (35 36)) (150 150)) (153 155))  (a) External event declaration.
i)  (always (and (qvalue InFlow (nil std)) (value-in InFlow (200 220))) ii) (until (and (value-in Valve (0.5 0.5)) (qvalue Valve (nil std))) (event b-open1)) iii) (between (event b-open1) (event e-open1) (and (qvalue Valve (nil inc)) (qvalue Level (nil inc)))) iv) (occurs-at (event e-open1) (value-in Valve (0.65 0.7)))  (b) Trajectory constraints (rst opening action only).
Figure 2: Input to TeQSIM.
opening is increasing and Level is observed to increase, and iv.
valve reaches a value in [0.65, 0.7] at the end of the rst opening action.
Figure 3 shows the result of the simulation.
TeQSIM yields a single behavior where Level increases and reaches a new equilibrium value, well below the High threshold.
Correctness properties of TeQSIM guarantee that this is the only possible outcome of any real plant that is validly described by the QDE model, the initial state and the trajectory constraints.
This example is very simple but it shows one possible use of trajectory constraints to extend the scope of qualitative simulators (like QSIM, that are limited to simulation of initial value problems only).
A brief description of other tasks to which TeQSIM can be applied along with a discussion of how each task can be demonstrated on the model described above is contained in gure 4.
Additional examples are contained in [5].
4 Guiding and rening simulation  TeQSIM guides and renes the simulation based upon a specication formulated using a variation of propositional linear time logic (PLTL).
PLTL combines state formulae, that express information about an individual state, with temporal operators such as until, always, and eventually to extend these state formulae across time and represent change.
We have extended PLTL by using a three valued logic that allows an expression to be conditionally entailed when quantitative information contained within the expression can be applied to a behavior to rene the description.
A renement condition species numerical bounds extracted from the TL expressions.
Application of these conditions to the behavior eliminates the region of the  state space that extended beyond the quantitative information specied in the TL expression.
In addition, the TL{guide algorithm is designed to handle the incremental nature of a qualitative simulation.
An undetermined result occurs whenever the behavior is insuciently determined to evaluate the truth of a TL expression.
4.1 TL specication language  This section provides a formal description of the syntax and semantics for the TL language.
The syntax and semantics are derived from work done by Emerson [6] and Kuipers and Shults [12].
A discussion of the TL{guide algorithm along with soundness and completeness theorems are presented in the following subsections.
Proofs of these theorems along with additional lemmas and corollaries can be found in [5].
Syntax.
The syntax and semantics for the state for-  mulae are described in gure 5(a).
Path formulae are derived by applying the temporal operators until and strong-next along with boolean operators to state formulae.
Path formulae P are formally dened as P ::= Sj (and P P )j(not P )j(strong-next P )j(until P P ), where S is the set of state formulae.
Informally, (until p q) is true for a path if p holds in all states preceding the rst one where q holds, while (strong-next p) is true for a path if p holds in the second state of the path which must exist.
Figure 5(b) gives a set of useful abbreviations.
We require that formulae are in positive normal form, i.e.
(i) until, releases and strong-next are the only temporal operators in the formula, (ii) for every not in the formula, its scope is an atomic proposition, and (iii) such a scope does not include any proposition constructed using value-<=, value->= or value-in.
The rst two requirements do not restrict the expressiveness of the language since the abbreviations shown above can be used to transform a formula to satisfy these conditions.
The latter requirement is due to the specic representation of numeric information in QSIM, which does not allow open numeric intervals.
Semantics.
Temporal logic formulae are given meaning with respect to the interpretation structures dened below.
These structures are extended from their typical denition (e.g.
[6]) in order to accommodate the renement process.
Path formulae are interpreted against an interpretation structure M = <S; ; ?
; I ; C ; M> where:  S is a set of states;  : S !
S is a partial function, mapping states  to their successors (we are dening a linear{time logic, hence each state has at most one successor);  I : S  S !
ft; f; ?g is an assignment of truth values to propositions and states (?
denotes the \unknown" truth value);  T1  T2  T3  T4  ..a T5 [153 +INF] .
... .... a  HIGH [75 75]  ... L-20 [30.8 37.4] .
..... ..... ..... ..... ..... ..... ..... ..... .. Adeg .... a a a a a a a a a a L* [30 35] 0 [0 0]  0 [0 0] T0  INF  TOP [100 100]  1 [1 1]  ..... ..... ....Adeg Adeg Adeg V-56 [0.900 0.950] ..a. .
.
.
V-28 [0.650 0.700] .. ..... ..... ... Adeg Adeg Adeg ..a ... ..... ..... V* [0.500 0.500] Adeg Adeg Adeg  MINF  T5  T0  VALVE  T1  T2  T3  T4  T5  ..a .
... .... a ...a .
.
.
.... a  .....a ...a ..a.. .
.
.
.
.... a a T0  T1  T4 [153 155] T3 [150 150] T2 [35 36] T1 [30 30] T0 [0 0]  T2  T3  T4  T5  TIME  LEVEL  Figure 3: Output of TeQSIM.
 ?
is a set of renement conditions.
?
is closed with respect to the standard boolean operators f^; :g and contains the distinguished item TRUE;  C : S  S !
?
is a function (condition gener-  ator) that maps state formulae and states into  renement conditions that disambiguate a formula truth value; we require that C ('; s) = TRUE i I ('; s) = t and C ('; s) to be dened when I ('; s) =?.
 M: ?
 S !
S is a function (state modier) that maps a condition and a state into a rened state.
For any state s, M(TRUE; s) = s and M(:TRUE; s) = ?.
We require that if ' is an atomic proposition then renement conditions are necessary and sucient for resolving the ambiguity, i.e.
if C = C ('; s) then I ('; M(C; s)) = t and I ('; M(:C; s)) = f (unless C = TRUE, in which case M(:C; s) = ?).
As customary, a path x is a sequence of states x = <s0 ; s1 ; : : :> such that for any pair of consecutive states (s ; s +1) we have that (s ) = s +1 .
The length of a path is denoted by jxj, which for innite paths is 1.
For all non negative integers i < jxj, x denotes the sub{path <s ; : : :> and x(i) denotes s .
A full{ path extension of a nite path x, denoted with xb, is an innite path formed by concatenating x with an innite sequence of states.
Finally, M is naturally extended to paths in order to rene paths.
In the specic case of QSIM, I may give ?
only for propositions including value-<=, value->= and value-in (as illustrated in g. 5(a)).
A renement condition is an inequality between the partially known numeric value of a variable in a state and an extended real number (or a boolean combination of conditions).
The condition that the QDE variable X in state s has to be less than 5 is written \X < 5".
Notice that ambiguity is not purely a syntactic property, but it depends on state information.
For example, (value-<= X .3) will be (unconditionally) true on a state s where R(X; s) = [0; 0:25], but only conditionally true on s0 where R(X; s0 ) = [0; 1:0].
Because of ambiguity, to dene the semantics of formulae we need to introduce two entailment relations.
The rst one, called models (j=), will be used to characterize non{ambiguous true formulae; the second one, i  i  i  i  i  i  i  s  called conditionally{models ( j=? )
will characterize formulae that are ambiguous.
We will say that an interpretation M and a state s falsify (6j=) a formula ' when neither s j= ' nor s j=? '
hold.
Figure 5(c,d) gives the semantics of the language.
To simplify the analysis of the renement process, the usage of ambiguous formulae must be restricted.
The problem is that an arbitrary ambiguous formula may yield several alternative renement conditions.
A disjunction of renement conditions cannot be applied to states without requiring a change in the successor function  and the introduction of a new behavior which is qualitatively identical to the original behavior in the tree.
Two dierent types of disjunction can result from certain ambiguous formula.
A state disjunction results from a disjunction of ambiguous state formulae.
For example, when interpreted against a particular state (or (value-<= X 0.5) (value->= Y 15)) may yield the condition (X  0:5 _ Y  15).
When applying such a condition to a state, M(C; s) yields two states { s0 in which X is restricted to be  0:5 and s00 where Y  15.
A path disjunction, on the other hand, occurs when an ambiguous formula is included in a path formula in such a manner that a sub{formula can be conditionally true for more than one sub{path.
For example, in the path formula (until p (value-<= X 0.5)) a disjunction occurs across sub{paths regarding when (value-<= X 0.5) should be conditionally true.
The following denitions restrict the syntax to formulae that are well{behaved.
A potentially ambiguous formula is any TL formula that (i) is an atomic proposition constructed using one of the following operators value-<=, value->= or value-in, or (ii) is a path formula which contains a potentially ambiguous sub{formula.
Admissible formulae are those formulae ' that satisfy the following conditions: 1. '
is in positive normal form, and 2. if '  (until p q) then q is not potentially ambiguous, and 3. if '  (releases p q) then p is not potentially ambiguous, and 4. if '  (or p q) then at most one of p and q is potentially ambiguous.
It can be proved that for all admissible formulae ' s  s0  s00  s  Continuous feedback control  A control law is expressed in terms of a set of formulae relating the value of the monitored variable (say Level) with the required value, or trend, of the control variable (say Valve).
The resulting closed{loop behaviors can then be analyzed with respect to the controller's goal.
The following trajectory constraint provides a partial specication of a control law to avoid overowing the tank.
It states that the valve opening must be increasing whenever the magnitude of Level is greater than high and the valve hasn't yet reached its maximum opening max.
(always (between (qvalue Level ((high nil) nil)) (qvalue Level (high dec)) (implies (qvalue Valve ((min max) nil)) (qvalue Valve (nil inc)))))  Continuous feed{forward control  The control law is expressed in terms of a set of formulae relating a predicted value of the monitored variable to the current value or trend of the control variable.
The following trajectory constraint species that if the tank can potentially overow then the valve opening should be increased until it reaches its maximum value or level becomes smaller than high.
(always (implies (eventually (qvalue Level (top nil))) (until (qvalue Valve (nil inc)) (or (qvalue Level (high dec)) (qvalue Valve (max nil))))))  Goal Oriented Simulation  The statements reported below can be used to check whether the tank will overow within a specied time frame.
Since TeQSIM is sound, if no behaviors are produced then the modeled system can not violate these constraints (assuming that the QDE model is valid).
The following trajectory constraint limits the simulation to behaviors in which the tank Level reaches high within 150 seconds.
(and (event horizon :time 150) (before (qvalue Level (high nil)) (event horizon)))  Figure 4: Applying TeQSIM to other tasks using the regulated tank model.
and for any interpretation M and path x, if x j=? '
then any necessary and sucient condition C for rening x into a model for ' (i.e.
M(C; x) j= ' and M(:C; x) 6j= ') is either a single condition or a conjunction of conditions.
Even though the restriction to admissible formulae reduces expressiveness, such a restriction does not hinder the practical applicability of TeQSIM.
As long as important distinctions are qualitatively represented (using landmarks or events), most trajectory constraints can be cast into admissible formulae.
For example, the constraint that \until level goes above 50 the input ow rate has to be below 200" could be expressed with the following non{admissible formula (until (value-<= InFlow 200) (value->= Level 50)) where the two distinctions (200 and 50) do not correspond to qualitative landmarks.
By adding a landmark to the quantity space of Level corresponding to the value 50, the formula can be rewritten in an admissible form (i.e.
(until (value-<= InFlow 200) (qvalue Level (lm-50 nil)))).
QSIM computes in nite time a set of behaviors, each representing a class of trajectories of the system being simulated.
Although a QSIM behavior is a nite structure, it may represent innite trajectories of the simulated system.
In fact, quiescent states are nite descriptions of xed{point trajectories1 .
For-  mally, a QSIM behavior b is a nite sequence of non repeating states <s0 ; : : :; s > such that 8i; 0  i < n : (s ; s +1 ) belongs to the QSIM relations successor or transition.
A behavior b is closed i QSIM detected that s is a quiescent state or that s is a transition state that has no possible successor.
1 QSIM may identify cyclic behaviors as well and represent them throughcycles in a directed graph.
The use of quantitative information, however, only makes sense if time is not cyclic;  furthermore the renement of behaviors requires that they do not share sub{behaviors.
For these reasons cycle detection is disabled within TeQSIM.
n  i  i  n  n  4.2 Model checking  The model checking algorithm is designed to evaluate a behavior with respect to a set of admissible formulae as the behavior is incrementally developed.
This allows behaviors to be ltered and rened as early as possible during the simulation.
Our algorithm is derived from the one described in [11]; however, it has been modied to deal with conditionally true formulae and to cope with behaviors which are not closed.
A detailed discussion of the algorithm is provided in [5].
The algorithm computes the function  : P  Behaviors !
fT; F; Ug Conditions: A denite answer (i.e.
T or F) is provided when the behavior contains sufcient information to determine the truth value of the formula.
For example, a non{closed behavior b, for all interpretations M , will not be suciently determined with respect to the formula ' (eventually p) if p is not true for any sux of b, since p may become true in the future.
A behavior is considered to be suciently determined with respect to a formula whenever there is enough information within the behavior to determine  (a) Syntax and semantics of state formulae.
In the denitions of the most important state formulae given below, v denotes a QSIM variable, R(v; s) the range of potential values for v in state s, and vs the unknown value of v in s. n, ni denote extended real numbers.
(qvalue v (qmag qdir )) where qmag is a landmark or open interval dened by a pair of landmarks in the quantity space associated with v, and qdir is one of finc, std, decg.
NIL can be used anywhere to match anything.
Such a proposition is true exactly when the qualitative value of v in the state s matches the description (qmag qdir ).
(value-<= v n) is true i 8x 2 R(v; s) : x  n; it is false i 8x 2 R(v; s) : n < x; it is unknown otherwise.
In such a case the renement condition is that the least upper bound of the possible real values of v is equal to n (i.e.
vs  n).
(value->= v n) is similar.
(value-in v (n1 n2 )) is true i R(v; s)  [n1 ; n2 ]; it is false i R(v; s) \ [n1 ; n2 ] = ;.
It is unknown otherwise, and the renement condition is that the greatest lower bound is equal to n1 and the least upper bound is equal to n2 (i.e.
n1  vs ^ vs  n2 ).
Non{atomic propositions are dened using standard boolean operators (and, not); standard propositional abbreviations are also allowed (true, false, or, implies, iff).
(c) Entailment relations for state formulae.
(a ranges over atomic propositions, p and q over S ): s j= a i I (a; s) = t s j=?
a i I (a; s) =?
s j= (and p q) i s j= p and s j= q s j=?
(and p q) i s j=?
p and s j= q, or s j= p and s j=?
q, or s j=?
p and s j=?
q s j= (not p) i s 6j= p s j=?
(not p) i s j=?
p  (b) Path formulae abbreviations.
(or p (releases p (before p (eventually (always (never (starts p (follows p (occurs-at (between p  q) q) q) p) p) p) q) q)           p q) q r)     (not (and (not p) (not q ))) (not (until (not p) (not q ))) (not (until (not p) q )) (until true p) (releases false p) (always (not p)) (releases p (implies p (always q ))) (releases p (implies p (strong-next (always q )))) (releases p (implies p q )) (releases p (implies p (strong-next (until r q ))))  The intuitive meaning for some of these forms is: p q): q is true up until and including the rst state in which p is true.
(starts p q ): q holds from the rst occurrence of p. (follows p q ): similar to starts, but q should hold just after the rst occurrence of p. (occurs-at p q ): q is true at the rst occurrence of p. (between p q r ): r holds in the open time interval between the rst occurrence of p and the subsequent rst of q.
(releases  (d) Entailment relations for path formulae.
(p 2 S and '; 2 P ): x j= p i x(0) j= p x j=?
p i x(0) j=?
p x j= (strong-next ') i jxj > 1 and x1 j= ' x j=?
(strong-next ') i jxj > 1 and x1 j=? '
x j= (until ' ) i 9i  0 : xi j= and 8j < i : xj j= ' x j=?
(until ' ) i 9i  0 : (xi j= or xi j=? )
and ? ')
and 8j < i : (xj j= ' or xj j= ?
occurs at least once j= The semantics of (and ' ) and (not ') is similar to the propositional case.
Figure 5: Syntax and semantics of the language.
a single truth value for all completions of the behavior.
If a behavior is not suciently determined for a formula, then U is returned by  and the behavior is not ltered out by TL{guide.
Notice that indeterminacy is a property independent from ambiguity: the former is related to incomplete paths, while the latter deals with ambiguous information present in states of a path.
The following recursive denition characterizes determinacy.
Given an interpretation M , a path x is suciently determined for a positive normal formula ' (written x  ') i one of the following conditions is met: 1. x corresponds to a closed behavior or ' is a proposition or x j= ' or x j=? '
2. '
 (strong-next p) and jxj > 1 and x1  p  3. '
 (until p q) and 9i: i < jxj and x 6j= p and x  p and 8j  i: x 6j= q and x  q 4. '
 (releases pq) and 9i: i < jxj and x 6j= q and x  q and 8j  i: x 6j= p and x  p 5. '
 (and p1 p2) and 9i: x 6j= p and x  p 6. '
 (or p1 p2 ) and 8i: x 6j= p and x  p We will write x = ' to signify that x is not suciently determined for '.
We are now ready to state the main correctness and completeness theorem on model checking.
i  i  j  j  i  j  j  i  i  i  i  i  Theorem 1 ( is sound and complete) For any  admissible formula ' and for any QSIM behavior b the following statements hold:  1.
('; b) = (T; TRUE) () there exists an interpretation M such that b j= '.
2.
('; b) = (T; C ) and C 6= TRUE () there exists an interpretation M such that b j=? '
and if b0 ; b00 exist such that b0 = M(C; b) and b00 = M(:C; b) then b0 j= ' and for all full{path extensions bb00 of b00: bb00 6j= '.
3.
1 ('; b) = F () for all interpretations M and for all full{path extensions bb: bb 6j= ' and b  '.
4.
('; b) = (U; C ) () for all interpretations M , b = ' and if b0 = M(:C; b) exists then for all full{path extensions bb0 : bb0 6j= '.
Proof is based on induction on formula length.
It follows by a case{by{case analysis of the algorithm.
4.3 Guiding and rening the simulation  When given a behavior b and an admissible formula ', TL{guide computes  ('; b) = (v; C ).
The behavior b is refuted i v = F; it is retained unmodied i v 6= F and C = TRUE; and it is rened into M(C; b) i v 2 fT; Ug and C 6= TRUE.
The following theorem justies our use of temporal logic model checking for guiding and rening the simulation.
Theorem 2 (TL{guide is sound and complete)  Given a QSIM behavior b and an admissible formula ' then TL{guide: 1. refutes b i for all interpretations M and for all full{path extensions bb: bb 6j= ' and b  '.
2. retains b without modifying it i (a) there exists an interpretation M such that b j= '; or (b) for all interpretations M , b  = ' and there is0 no necessary condition C such that if b = M(:C; b) exists then for all full{path extensions bb0: bb0 6j= '.
3. replaces b with b0 i (a) there exists an interpretation M such that b j=? '
and exists C such that b0 = M (C; b) j= '00 and C is necessary (i.e.
exists b00 such that b = M(:C; b) and for all full{path extensions bb00: bb00 6j= '); or (b) for all interpretations M , b  = ' and there is a necessary condition C such that b0 = M(:C; b) and for all full{path extensions bb0: bb0 6j= '.
Proof follows almost directly from the previous theorem.
5 Discussion and future work  TeQSIM is designed to provide a general methodology for incorporating trajectory constraints into the qualitative simulation process.
The current trajectory specication language is, however, insucient to express certain constraints relevant to dynamical systems (e.g.
stability requirements for controllers).
Three dierent extensions to the language are currently being investigated.
Limited rst order expressiveness - The temporal logic used is limited to PLTL and is unable to quantify over attributes of states.
Certain trajectory constraints require the ability to refer to values across states within the behavior.
For example, the specication of a decreasing oscillation requires the ability to compare the magnitude of a variable across states.
A limited form of rst order logic may provide a suciently expressive language while still giving satisfactory performance with respect to complexity.
Metric temporal logic - Due to the introduction of landmarks during the simulation process, QSIM behaviors are potentially innite structures.
Getting a denite answer for formulae such as (eventually p) is not always possible when potentially innite behaviors are encountered since it is always possible for p to occur in the future.
Metric temporal logic [1] allows the denition of a horizon for a temporal logic expression.
This would allow statements such as \within 50 seconds the tanks level reaches 70 inches."
This statements is only expressible within our logic using an external predened event.
Such an extension oers the modeler more exibility to express relevant constraints.
Functional envelopes - Semi{quantitative reasoning [3] within TeQSIM uses interval bounds and static functional envelopes for monotonic functions to derive quantitative information about a behavior.
NSIM [9] derives dynamic envelopes describing a variable's behavior with respect to time.
Currently, only interval information can be specied within TeQSIM trajectory constraints.
Extending the language to include information about bounding envelopes with respect to time would increase the precision of solutions computed by TeQSIM.
Finally, the current algorithm for incremental model checking is inecient if compared to the on{ the{y model checker algorithm developed by Bhat and colleagues [4].2 We plan to incorporate it within TeQSIM.
6 Conclusions  Qualitative simulation and temporal logic provide two alternative formalisms for reasoning about change  2 In all the examples we have run so far, the practical time{ complexity of a TeQSIM simulation is denitely dominated by other operations(like quantitativeinferences)rather than model checking.
across time.
TeQSIM integrates these two paradigms by incorporating trajectory information specied via temporal logic into the qualitative simulation process.
Behaviors that do not model the set of temporal logic expressions are ltered during simulation.
Numeric information specied within the TL expressions can be integrated into the simulation to provide a more precise numerical description for the behaviors which model these expressions.
The correctness of the TL{guide algorithm along with the correctness of QSIM guarantee that all possible trajectories of the modeled system compatible with the QDE, the initial state and the trajectory constraints are included in the generated behaviors.
In addition, the completeness of TL{guide ensures that all behaviors generated by TeQSIM are potential models of the trajectory constraints specied by the modeler.
Acknowledgments  We would like to thank Benjamin Shults for letting us use part of his program to implement TeQSIM, and to the Qualitative Reasoning Group for many fruitful discussions.
Thanks also to a careful anonymous referee.
QSIM and TeQSIM are available for research purposes via anonymous ftp at ftp.cs.utexas.edu in the directory /pub/qsim.
These and other results of the Qualitative Reasoning Group are accessible by World-Wide Web via http://www.cs.utexas.edu/users/qr.
This work has taken place in the Qualitative Reasoning Group at the Articial Intelligence Laboratory, The University of Texas at Austin.
Research of the Qualitative Reasoning Group is supported in part by NSF grants IRI-9216584 and IRI-9504138, by NASA grants NCC 2760 and NAG 2-994, and by the Texas Advanced Research Program under grant no.
003658-242.
References  [1] R. Alur and T. Henzinger.
Real{time logics: complexity and expressiveness.
Information and Computation, 104(1):35{77, 1993.
[2] M. Barbeau, F. Kabanza, and R. St-Denis.
Synthesizing plant controllers using real-time goals.
In Proc.
of IJCAI{95, pages 791{798.
IJCAI, Morgan Kaufman, August 1995.
[3] D. Berleant and B.J.
Kuipers.
Using incomplete quantitative knowledge in qualitative reasoning.
In Proc.
of the Sixth National Conference on Articial Intelligence, pages 324{329, 1988.
[4] G. Bhat, R. Cleaveland, and O. Grumberg.
Ecient on{the{y model checking for CTL*.
In Proc.
of Conference on Logic in Computer Science (LICS{95), 1995.
[5] G. Brajnik and D. J. Clancy.
Temporal constraints on trajectories in qualitative simulation.
Technical Report UDMI{RT{01{96, Dip.
di Matematica e Informatica, University of Udine, Udine, Italy, January 1996.
[6] E.A.
Emerson.
Temporal and modal logic.
In J. van Leeuwen, editor, Handbook of Theoretical Computer Science, pages 995{1072.
Elsevier Science Publishers/MIT Press, 1990.
Chap.
16.
[7] A. Farquhar and G. Brajnik.
A semi{quantitative physics compiler.
In Tenth International Conference on Applications of Articial Intelligence in Engineering, Udine, Italy, July 1995.
Presented also at the Eighth International Workshop on Qualitative Reasoning on Physical Systems, 1994, Nara, Japan.
[8] D. Jonescu and J.Y.
Lin.
Optimal supervision of discrete event systems in a temporal logic framework.
IEEE Transactions on Systems, Man and Cybernetics, 25(12):1595{1605, Dec. 1995.
[9] H. Kay and B.J.
Kuipers.
Numerical behavior envelopes for qualitative models.
In Proc.
of the Eleventh National Conference on Articial Intelligence.
AAAI Press/MIT Press, 1993.
[10] B.J.
Kuipers.
Qualitative Reasoning: modeling and simulation with incomplete knowledge.
MIT Press, Cambridge, Massachusetts, 1994.
[11] B.J.
Kuipers and B. Shults.
Reasoning in logic about continuous change.
In J. Doyle, E. Sandewall, and P. Torasso, editors, Principles of Knowledge Representation and Reasoning, San Mateo, CA, 1994.
Fourth International Conference (KR{94), Morgan Kaufmann.
[12] B. Shults and B. J. Kuipers.
Qualitative simulation and temporal logic: proving properties of continuous systems.
Technical Report TR AI96{244, University of Texas at Austin, Dept.
of Computer Sciences, January 1996.
[13] J.G.
Thistle and W.M.
Wonham.
Control problems in a temporal logic framework.
International Journal on Control, 44(4):943{976, 1986.
Reasoning about Concurrent Actions within Features and Fluents Choong-ho Yi Department of Computer Science University of Karlstad S-651 88 Karlstad, Sweden E-mail: Choong-ho.Yi@hks.se  Abstract  trast to the case for sequential actions where several entailment criteria, e.g.
chronologically maximal ignorance [Sho88], have been proposed for selecting intended models, no such method has been tested for dealing with concurrent actions.
The work presented in this paper is an approach '[in that direction" which has been done based directly on Sandewall.
By making necessary generalizations we have introduced concurrent actions into his framework which was restricted to sequential actions.
The resulting formalism is capable of reasoning about interdependent as well as independent concurrent actions.
Then as a first step, we picked out the simplest entailment criteria PCM (prototypical chronological minimization of change) and PCMF (filtered PCM) of Sandewall, allowed independent concurrent actions into the classes of reasoning problems for which he had proven PCM and PCMF to be correct respectively, and proved that these criteria are still correct for the respective extended classes.
Sandewall proposed a systematic assessment method for temporal logics.
In favour of the assessment of logics, we have introduced concurrency into his framework.
The resulting formalism is capable of reasoning about interdependent as well as independent concurrent actions.
We have then applied the entailment criteria PCM and PCMF t o selecting intended models of common sense theories where concurrent actions are allowed, and proved that the criteria lead to only intended models for respective subsets of such theories.
1  Introduction  With restriction to the case where actions are assumed to occur sequentially, a number of nonmonotonic logics have been proposed in AI.
In the meanwhile, research has advanced and one began to investigate model-theoretically whether a logic at hand produces conclusions correctly for a given theory.
In his Features and Fluents [San941 Sandewall proposes a new approach in this context.
For each of the major logics presented by then he identified, in a systematical way, a corresponding class of reasoning problems for which the logic is proved to obtain exactly the intended conclusions, i.e.
the range of applicability of the logic.
On the other hand, logics have been suggested by, e.g.
Kowalski & Sergot [KS86], Allen [AllSl], Pelavin [Pe191],Lansky [LanSO], and Georgeff [Geo86] which, directly or indirectly, allow concurrency.
As was the case in reasoning with sequential actions, the importance and the need to identify the range of applicability of a given logic could not be emphasized too much even when concurrent actions are allowed.
By this time, however, there has not been reported any systematic result in that direction.
Actually, in con-  2  describes a systematic approach to common-sense reasoning, where he first defined several classes of reasoning problems, formalised different nonmonotonic logics, and then assessed the correctness of each logic on the classes of reasoning prob1ems.l In his approach common-sense reasoning is understood on the basis of an underlging semantics which views the interaction between the ego of an intelligent agent and a world as a game, and which characterizes the actions the agent may evoke during the game in terms of a trajectory semantics.
The inertia problem is approached by building inertia into the underlying semantics, i.e.
the world has inertia so that features lThe presentation in this section is mainly based on [SanSS].
6 0-8186-7528/96 $5.00 0 1996 IEEE  The Features and Fluents In the Features and Fluents formalism Sandewall  results of an action, but also about its trajectories.
Since it is the world which performs actions, the pair (Inf 1,Trajs) characterizes a world.
Later, ego and world will be defined exactly in a trajectory semantics generalized for concurrency.
remain unchanged unless actions which override the inertia are performed.
2.1  The Game  The game is made in terms of a finite development  (8, M ,R, -4,  2.3 Commonsense Scenarios A commonsense theory is expressed as a tuple  B is a set of integers representing the time points at which the ego and the world alternate in the game and the largest member n of the set is "now".
M assigns values to temporal constants and object constants.
R is a mapping from a set (0,. .
.,n} of time points to a set R of states, i.e.
R is a history of the world up to n. The pair  (6,n, SCD,OBS).
0 is an object domain.
I3 is a set of formulae describing the effects of actions, e.g.
Cs,tl Openwindow 3 [tl WindowsOpen  ( M ,R)  which means if the Open Window action happens over the time interval s to t , then the feature Window rep-  then constitutes an interpretation for a given object domain 6.
A is a set of tuples (s,E , t ) where s and t are start respective end time of the action E and s < t 5 n, i.e.
a set of actions which have been terminated at time n. C is a set of tuples ( s , E ) where s is start time and s 5 n, i.e.
a set of actions which have been started but not terminated yet at time n. The tuple (a, M,R , A, C) works as a "game board" in the game; the ego and the world alternate and extend it such that, roughly, the world executes the actions which axe evoked by the ego.
2.2  resenting the openness of the window has value Open at time t. Actually, this set is an exhaustive description of Traj s in logical formulae.
SCD represents the actions scheduled to be performed, and is a set of action statements, e.g.
[3,51 Open Window,  and time statements, e.g.
The Trajectory Semantics and Nondeterminism  tl < 52.
The effect  The trajectory semantics characterizes actions in terms of two functions.
The function  [SI Window&Open  Inf1(E,r )  of performing [3,51Open Window is then obtained by applying the action statement to II.
The result of  represents a set of features which may be affected if the action E is performed in the state r .
The function  replacing each action statement in SCD by the effects specified by l7 will be written as ~ ( s c D ) .
OBS is a set of observation statements, i.e.
any formulae not containing action statements.
Trajs(E,r) represents a set of possible trajectories of E initiated in r , where a trajectory, written as U , is expressed as a finite sequence  of partial states ri (1  5 i 5 k) each of  2.4  Intended Models  If a scenario T = (O,II,SCD,OBS)is given, then the set of intended models of Y is defined as follows.
First, select an arbitrary world which is exactly characterized by II, select an arbitrary ego, an arbitrary initial state and an arbitrary initial mapping for temporal and object constants.
Let  which as-  signs values to exactly those features appearing in  Infl(E,r).
This sequence is a trajectory of the action of the form [s,s + kl E , where s is start point in time and s + k end point, and describes the effects of E successively for ecah time point during the execution period.
Therefore, in the trajectory semantics one cares not only about the nondeterministic  Mod(T) be a set of completed developments ( B ,M , R, A, C) obtained from games between them over Y such that there is a 1:l correspondencebetween members of the  7  nation Kp-IsAn represents a set of reasoning problems satisfying the restrictions IsAn and Kp.
Such combinations are used for identifying the applicability of different logics.
That is, the correctness of a logic is defined for a class of reasoning problems in terms of equality between the set of intended models and the set of preferred models.
set A and those of SCD (i.e.
all of the scheduled actions have been performed successfully), and all formulae in SCD U OBS are true in (M, R) having 0 as object domain.
Then,  is the set of intended models of Y.
2.5 Taxonomy of Reasoning Problems One of the characteristics of Sandewall's systematic approach is the use of taxonomy of reasoning problems.
The taxonomy is obtained by making explicitly ontological assumptions about actions and world and epistemological assumptions about knowledge about the actions and the world to be reasoned with.
For example, the ontological characteristic I represents that inertia holds; A represents "alternative results", i.e.
the effects of an action are conditional on the starting state; C represents that concurrent actions are allowed; D represents dependencies between features, i.e.
change in one feature implies possibility of immediate change in another feature; and so on.
The classical frame problem is then denoted as IA, and the ramification problem is in the IAD ontological family.
In addition, for a more precise specification, he provides sub-characteristics which are additional constraints within characteristics and which are written with small letters.
For example, Is represents the subfamily of I where all actions take a single time step; An denotes the subfamily of A where all features which are allowed to be influenced as result of an action in a given state should change their value if the action is performed in that state: if a feature with three possible values red, yellow, green is influenced by an action, then the action is allowed to nondeterministically change the value from red to yellow or red to green, but it is not allowed to choose between switching from red to green or keeping it red; and so on.
All of the sub-characteristics can be defined precisely in terms of the trajectory semantics.
In order to characterize the epistemological assumptions, a list of epistemological characteristics is provided.
For example, K: denotes complete and correct knowledge about actions.
K p represents that in addition there are no observations about any time point after the initial one.
Therefore Icp denotes pure prediction problem.
The ontological and the epistemological descriptors are then combined and characterize a class of sys-  2.6  PCM and PCMF  The entailment criteria PCM has been formalized by Sandewall as follows.
Let 1 = ( M ,R) be an interpretation, then the breakset of I at time t is defined as a set of features which change value from time t - 1 to t; formally  breakset(I,t) = {fil R(fi,t - 1) # R(f;,t ) } .
Definition [San941 Let I = ( M , R ) and I' = (W,RI) be interpretations, then I is said to be PCMpreferred over I/,written as I <<pcm I f , iff M = MI and there is some time point t such that 0  R ( f ,t ) = R'(f, t ) for all features f in v and for all time points t < t , and breakset(1,t) c brealcset(I',t).
Sandewall has shown that PCM guarantees only intended models for reasoning problems within the class Xp-IsAn described above.
However, the applicability can be improved by combining PCM with the entailment technique filtering into the new criterion PCMF.
The idea with filtering [San891 is to separate the premises in the sets SCD and OBS.
Let denote the set of classical models for the set I?, then the set of PCM-minimal models is  Min((<,,,,  I[n(SCD) U OBS]).
The PCMF-minimal models set is instead M~+Z,,,,  pwD)n)  n  wn,  so that the PCM-minimization is performed before the observations.
PCMF is correct for a larger class IC-IsAn of reasoning problems, i.e.
the restriction that only initial observations are allowed, is now removed.
For the detailed discussion and the full proofs, please refer to [San94].
3  Concurrency in the Trajectory Semantics  In Sandewall [San941 the trajectory semantics was defined with the restriction that only sequential actions are allowed, i.e.
at most one action is considered  tems or reasoning problems.
For example, the combi-  8  at a time.
However, in dealing with concurrent actions new problems arise which were not there for sequential actions.
Concurrent actions imply that at least two actions are involved at a given time, and, consequently, that interactions may arise between them.
Therefore, the semantics must be modified.
3.1 Concurrent Interactions In a broad sense, concurrent actions may be interacting or noninteracting, and, if they interact, they may interact interferingly or noninterferingly.
Given two or more actions, one cannot say unconditionally whether they interact or not, and if they do, whether they interfere or not.
The behaviour of individual actions in these respects is dependent on their start states and the trajectories chosen for them.
The set of features influenced by executing an action in a state may be different if the action occurs in another state.
What it means is that any two overlapping actions E1 and E2 which influence no feature in common if they start in state r1 and r2 respectively, i.e.
reasoned about effectively.
What is missing in this function is to represent such conditions for every trajectory U E Traj s ( E ,r).
Definition Let E be an action and T a state, then each trajectory 2r E Trajs(E,r) is defined now as a pair ((r~,...,ri),(r~,...,r~))  of finite sequences of partial states where  is a trajectory description and is our "old" trajectory, and (ry,.. .
,r!)
is trajectory preserving condition.
If the action E is started at time point s, each ry (1 5 i 5 k) in the trajectory preserving condition specifies conditions to hold at time point s + i in order for the action E to proceed as described by ( r i ,.
.
.
,r i ) .
The trajectory description will be written as d, and the trajectory preserving condition as pc.
The trajectory preserving condition is a generalization of the prevail condition of Sandewall & Rnnquist [SR86] in that both two refer to conditions that should hold during an action performance.
The difference is that their prevail condition only represents conditions that should hold during the whole duration of an action, while the trajectory preserving condition can freely refer to conditions not solely for the whole duration but also for some parts of it, or even for a time point.
Infl(E1,rl)n 1nfl(E2,rz)= 0 , can easily show different effects if performed in different states, e.g.
such that  InfI ( & , r1) n Inf l(E2,r3) # 0.
For another example, let  {v1,v2} E. Trajs(E1,r;) (213,214) C Trajs(E2,rj).
3.3  Concurrent Interactions in terms of  Inf 1 and p c In our formalism, concurrent interactions are considered at the level of concurrent trajectories, i.e.
the trajectories of concurrent actions.
Concurrent trac jectories can interact in two ways, namely, by influencing some feature in common, or by influencing a feature which appears in the preserving condition of the other trajectory.
The formal definition follows.
For the forthcoming discussion, we introduce some notations first.
For a given trajectory  Then it may be the case that the trajectory 211 interacts with 213, and, also with 214 but differently than with 213, while there is no interaction between v2 and 9J4.
However, ils will be discussed when we define concurrent interactions formally, the interactions are relative to the start time points of actions as well.
3.2 Trajectory Preserving Condition As mentioned in our previous discussion, the function Trajs(E,r) captures the set of possible trajectories of the action E w.r.t.
its starting state r. In the case of sequential actions, it was enough to say merely that there are several ways for a given action to go.
In discussing about concurrent actions, one also needs to know the conditions under which each trajectory proceeds as such, since, unless these conditions are available, interactions between trajectories of concurrent actions cannot be represented and  d(IC) and pc(IC) shall be the k:th (1 5 IC 5 m) element of the trajectory description d and that of the trajectory preserving condition pc, respectively.
Similarly, d(f,k) and pc(f,k) will be used to express the value of a feature f defined in d(k) respectively p c ( k ) .
Let v = (d,pc), then Zength(v) shall be the length of time  9  period over which the trajectory v proceeds, i.e.
m. In addition, by 3 ( r ) we denote the set of features which are defined in a given state r .
Definition Given two arbitrary actions of the form Csl ,tilE1 and Cs2, t 2 l E2 such that m a z ( s ~s2) , < min(tl,ts), i.e.
they are concurrent actions, let R be an arbitrary history defined over [O,sl where s 2 m a z ( s l , s 2 ) , and, for 1 5 i 5 2, let R(s;)= r,, let U ; = (d;,pcz) be a member of Trajs(E,,r,) of length t, - s,, let x, = muz(s1,sz) - sa + 1 and yz = min(t1,t2)-s,, i.e.
z, and yz are intended to represent the first respectively the last moment at which U, might interact with the other trajectory.
Then v1 and 212 are said to 0  Inf 1-interact iff  e  pc-interact iff  Figure 1: A cart in front of the curb.
time points satisfying x1 5 tl 5 y1, 2 2 5 k2 5 y2 and SI kl = s2 k2.
Then the trajectories 211 and u2 are said to  +  +  e Inf 1-interfere iff  they Inf 1-interact through some feature f and dl (f,k1) # d2 (f,k2) for some k1 and k2;  pc-interfere iff  Notice that Inf l(E1,T I ) and Inf l(Ez,r2) represent the set of features defined in the elements of dl respectively d2, and that the interactions in the above definition are relative to the choice of s1 and s 2 .
Additionally, let  they pc-interact through some feature f and  (dl(f, k)# pc2(f, k2) v &(f,h ) # pc1 (f,h))for some ICl and Az.
fl E Infl(El,rl)n Infl(Ez,r2), f2 E (Infl(E1, TI) n F ( p c ~ ( k 2 ) ) )U  In addition, independent actions are defined trivially such that they neither Infl-interact nor pcinteract.
n F(pc1(k1))).
(1nfV2,r2)  Then, for any f1 and f2, VI and 212 are said to Inf 1interact through fl and p i n t e r a c t through f 2 , respectively.
Therefore, concurrent trajectories may Inf 1-,or pc-interact, or both of the two.
Based on these concepts, we can go on and identify clearly interferences too.
Inf 1-interacting trajectories interfere iff they assign at some overlapping time point different values to some feature through which they Inf 1-interact.
And the pc-interacting interfere iff one trajectory assigns at some overlapping time point a different value from the trajectory preserving condition of the other to some feature through which they pc-interact.
The precise definition is as follows.
As an example, consider a situation shown in Figure 1 where a cart is standing in front of the curb, and we want to move it over the curb.
In order to do that, you should first press down the handle while the front wheels go over the curb and then lift the handle while the back wheels go over.
The vertical position of the front wheels and the back wheels is represented in relation to the curb by the features VpF repectively VpB which have as their value domain  Definition Let the same assumptions be given as in the previous definition, and let A1 and IC2 be any  Therefore, the wheels may be on the ground, under or over the curb, or lifted higher than the curb so  3.4 An Example  { on, lifted}.
10  as to pass it freely.
Similarly, HpF and HpB whose value domain is  where Inf l(PC,r2) = { HpF, H p B }  {before, passed, elsew},  d2 = ({HpFGbefore,HpBlelsew}, { HpFPpassed, HpBGeZsew }, { Hp F &pass ed ,HpB before}), pc2 = ({ V p F 3 i f t e d } , { VpF+lifted},0).
state the horizontal position of the front and the back wheels in relation to the curb.
before means that the front or the back wheels are directly before the curb and ready to go over, passed they passed it, and elsew the wheels are elsewhere before the curb but not directly in front of it.
Let PH denote the action "press handle" whose intended effect is to lift the front wheels.
The action PC, "push cart", has the effect of moving the cart over the curb while its wheels are lifted.
By restricting to these actions and some states which would characterize the actions well, let us briefly consider about concurrent interactions in the trajectory semantics.
Let r1 be any state which satisfies TI Z I  According to the trajectory v2, pushing the cart in r2 would proceed as follows; if the front wheels continue to be held lifted over the curb over Cs 1, s 21 where s is start time of P C , i.e.
+ +  pc2 ( V p F , 1 ) = pc2 ( VpF, 2 ) = lzfted, then the cart rolls on back wheels so that the front wheels pass the curb at [s 21, i.e.
+  dz(HpF, 2 ) = passed,  { VpFGon, V p B A o n } ,  and the back wheels reach the curb at Es  and let  + 31, i.e.
I n f l ( P H , r l ) = { VpF}.
dz(HpB, 3) = before.
It means, pressing down the handle of the cart when the wheels are on the ground, can influence the vertical position of the front wheels.
Then consider a trajectory  Here, too, 212 is assumed to be the only member of Traj s ( P C , r 2 ) , Now, let r be the initial state  VI  { V p F A o n ,HpFGbefore, V p B A o n , HpBPelsew}  = (d1,wl) E Trajs(PH,rl)  pictured in Figure 1, and let 0 be initial time point.
In addition suppose that we press the handle of the cart over the interval [0,4] and push the cart concurrently over C1,41.
Since r satisfies the condition of r1, u1 E T r a j s ( P C , r ) .
Let u1 be chosen for [0,41 P H .
(We need to and will discuss in detail about choosing trajectories for given actions in next section.)
By starting the trajectory v1 from time 0, the front wheels are lifted at succeeding time point 1 , and this is the only change caused by vi at time 1; recall d l ( 1 ) = { VpFLlifted).
Therefore the state of the world is changed from r to 7-2 over CO, 11.
And so, let the trajectory 212 be selected for [1,41PC.
The concurrent trajectories V I and w2 interact, namely pcinteract through the feature VpF, but not interfere.
Actually, u1 enables w2 such that the trajectory preserving condition pc2, i.e.
pcz(1) and pc2(2), is satisfied by d l ( 2 ) and d l ( 3 ) .
where  dl = ({ VpFAlifted}, { VpF%fted}, { V p F 3 i f t e d } ,{ V p F % f t e d } ) PCl  = (0,0,0,@.
That is, a possible trajectory of the action PH initiated in r1 is that it proceeds over 4 time units and holds the front wheels lifted over the interval, i.e.
dl(VpF,l) =...=dl(VpF,4)=Zzfted.
No trajectory preserving condition is required.
Of course, T r a j s ( P H ,r l ) may also contain other trajectories.
For convenience, however, we assume that V I is its only member.
Next, suppose a state r2 such that  { VpF +lifted, HpF bef ore, V p B A on, HpB A elsew}.
4  Then, for the action PC we consider in a similar way a trajectory  Trajectory Semantics World and Ego  For dealing with concurrency in the trajectory semantics, the single-timestep ego-world game where  v2 = (d2,pcz) E T r a j s ( P C , r z )  11  Definition Let a development (B, M , R, A, C) be given where max(B) = n and R is defined over CO, n] .
Then a compatible-trajectories combination, written as c, for C is defined as any set of trajectories v, = (d,,pci) E Trajs(E,,R(si)) for some or all members (si,E,) of C which satisfy  the world advances time by exactly one time step at a time, is adopted.
It offers a clear and simple underlying semantics, and reduces the technical complexity.
Following Sandewall, trajectory semantics world and ego are defined as follows.
Formal definitions are given in [Yi95].
4.1 Trajectory Semantics World in a Single-timestep Ego-World Game As mentioned previously, the ego-world game is performed in terms of a finite development (B,M , R, A, C ) .
Let a world description (Trajs, Inf 1) be given, and let (23, M , R, A, C ) be a development given for a singletimestep game between a trajectory semantics world and a trajectory semantics ego where the "now" time, i.e.
maa:(B),is n, and the history R is defined over CO, nl .
Assume that the world takes over the control now, and that the world modifies the development into (B',M', R', A',C').
As we will see, the modification is made differently according to whether the current action set C is empty or not.
However, the following hold irrespective of it; B' = Bun+ 1 , i.e.
the now-time is increased by one time point, M E MI, and the restriction of R' to the period [O,n] equals R. If C = 0, then it means that no action is going on at time n. It's because all actions being processed have "died out", and because the ego has decided not to start any new action at this moment.
In this case, the world extends history such that R'(n+l) = R(n), and A' and C' are set to A and 0 respectively.
On the other hand, at n there may be an arbitrary number of actions to be considered, i.e.
C has an arbitrary number of members (sa, E;) where s; is start time of E; and si 5 n. Here, it's not sure whether all members of C can be performed concurrently.
For example, it may be that some actions are evoked at n but interfere with other actions which have been started previously.
When conflict arises between concurrent actions, one may choose to perform as many actions as possible, or break the game there, or abandon all interfering ones, or save earlier actions first, and so on.
However, rather than choose a specific "policy" among them, we leave our underlying semantics open and more general on the question, and simply find some combination of compatible actions containing some or all members of C. Actually, it's a combination of mutually compatible trajectories which are in accordance with the history R. There may be more than one such combination.
The precise definition follows.
length(vi) > n - si, if n 15  then d;(k)U p c ; ( k ) R(si + k) for all L 5 n - si, i.e.
U; agrees with the previous  > s;,  history,  vi interferes in no way with any other member of c and the trajectory preserving condition pc; is satisfied by other trajectories in c or by applying inertia.
o Let c be a compatible-trajectories combination selected by the world.
Then the world extends the history as follows.
R'(n + 1) = R(n) CB  U  d;(n - S;  + 1)  (de ,pcz)Ec  where is Sandewall's "override" operation over states such that the value of a feature f in [T @ T ' ] equals that in state T' if f is defined there, otherwise that in state r. Using this CB operation inertia is interwoven into the semantics.
Notice that, since the trajectories in c do not interfere, the partial states at time n 1, d,(n - s, l), which are obtained from the trajectories, can be "put together" into a union without causing any conflict.
On the other hand, the trajectory preserving conditions at that time point, pc,(n - s, l),do not participate in the history extension.
They are expected to be satisfied by other trajectories d j ( n - sI + 1) or inertia.
For some trajectory {d,,pc,) E c, if d,(n - sI 1) is the last element of d3, then it means that the trajectory vI has been performed successfully and is terminated at n + 1.
Therefore C' is obtained by first making a set of corresponding tuples (s,, Ei) for each member v 2 of c and then removing from the set all of the "terminated" members ( s j , E j ) .
On the other hand, tuples of the form ( s j , Ej, n 1) are added to the past action set A for the completed actions Ej, and A' is set to the resulting set.
+  +  +  +  +  12  4.2 Trajectory Semantics Ego In the game it is the ego that activates one or more actions in its turn.
Let a development (B,M , R , A, C ) be given where "(8) = n, then for each action Ei  [Geo86] Michael P. Georgeff.
The representation of events in multigent domains.
In AAAI, pages 70-75, 1986.
[KS86] Robert Kowalski and Marek Sergot.
A logicbased calculus of events.
New Generation Computing, 4:67-95, 1986.  which is started by the ego at time n, a corresponding tuple (TI,&) shall be added to the C component.
If the ego passes on the control to the world without evoking any new actions, then no change is made for C. This definition does not need to be restricted to the single-timestep games.
5  [LanSO] Amy L. Lansky.
Localised representation and planning.
In J. Allen, J. Hendler, and A. Tate, editors, Readings in Planning, pages 670-674.
Morgan Kaufmann, 1990.
Reapplying PCM and PCMF to  [Pel911 Richard N. Pelavin.
Planning with simultaneous actions and external events.
In J. F. Allen, H. A. Kautz, R. N. Pelavin, and J. D. Tenenberg, editors, Reasoning about Plans, pages 127-211.
Morgan Kaufmann, 1991.
Concurrent Actions As a subset of concurrent actions, we have defined into Sandewall's taxonomy the class Ci where all trajectories of concurrent actions are mutually independent.
Then we have introduced Ci into the classes ICp-IsAn and IC-IsAn and extended them to Kp-IsAnCi respectively IC-IsAnCi.
By generalizing Sandewall's proofs of the correctness of PCM for Kp-IsAn and PCMF for IC-IsAn, we have proven that PCM and PCMF are still correct for the new classes Kp-IsAnCi and IC-IsAnCi, respectively.
For want of space, the full proofs and details are reserved in Pi951.
6  [San891 Erik Sandewall.
Filter preferential entailment for the logic of action in almost continuous worlds.
In IJCAI, 1989.
[San931 Erik Sandewall.
The range of applicability of nonmonotonic logics for the inertia problem.
In IJCAI, 1993.
[San941 Erik Sandewall.
Features and Fluents, A  Sgstematic Approach t o the Representation of Knowledge about Dgnamical Systems.
Ox-  Conclusion and Future work  ford University Press, 1994.
This work gives a base for analyzing the range of applicability of logics for concurrency.
The result of our work implies that Sandewall's systematic approach can easily be extended to concurrency, and that most of the results shown by him for the case of sequential actions may be reobtained similarly for concurrent actions as well after necessary modifications.
Considering the necessity of concurrent actions in common sense reasoning, and also, the importance of identifying the applicability of a given logic, it is urgently required to extend such assessment for concurrent actions.
In addition, we need to lift the restriction of independent actions in next step, so that even interdependent actions are considered when assessing applicabilities.
[Sho88] Y .
Shoham.
Reasoning about Change: Time and Causation from the Standpoint of Art$cia1 Intelligence.
MIT Press, Massachusetts, 1988.
[SR86] Erik Sandewall and Ralph Ronnquist.
A representation of action structures.
In AAAI, pages 89-97, 1986.
[Yi95] Choong-Ho Yi.
Reasoning about concurrent actions in the trajectory semantics.
Licentiate Thesis, 1995.
Department of Computer and Information Science, Linkijping University.
References [A11911 James F. Allen.
Temporal reasoning and planning.
In J. F. Allen, H. A. Kautz, R. N. Pelavin, and J. D.Tenenberg, editors, Reasoning about Plans, pages 1-68.
Morgan Kaufmann, 1991.
13
ficha MOKHTARI Institut dInformatique USTHB BP 32 El Alia Alger Algdrie mokhtari @ist .ibp.dz  Daniel KAYSER LIPN URA 1507 du CNRS Institut GalilCe Universitk Paris-Nord 93430 Villetaneuse France Daniel.Kayser@ural507.univ-paris 13.fr  Abstract  inferences can be achieved without ever making that choice (see e.g.
[4]), we adopt in this paper a pointbased point of view, for reasons which will appear in the next section.
However, it is likely that an interpretation in terms of intervals of what we call below time points would not change drastically the core of our approach.
The next section motivates our choices, in order to satisfy the kind of reasoning we wish to capture.
We then go on to provide a formalism intended to link an incomplete description with any normal course of the world: we give preliminary notations and definitions, then describe a theory of action, and compare it with related works.
We conclude with a discussion about current and future work.
This paper discusses the temporal aspect of a causal theory based on an "interventionist" conception of causality, i.e.
a preference to select causes among a set of actions which an agent has the ability to perform or not to perform (free will).
Casting causal reasoning in this framework leads to explore the problem of reasoning about actions, generally considered as a nonmonotonic temporal reasoning.
Most of the works on nonmonotonic temporal reasoning have used simple temporal ontologies, such as situation calculus, or temporal logic with discrete time.
The theory presented in this paper also has a simple temporal ontology based on "time points" organized on branching "time lines ", with the possibility of modelling continuous evolutions of the world for various fitures (prediction) or pasts (diagnostic).
2.
Actions, effects, and time points Actions and effects will be the only temporal propositions considered in our framework.
An effect can be, among other things, an event or a fact.
But as argued in [13], a fine distinction is unnecessary.
In order to introduce temporal aspects, some choices must be made.
The first one concerns the basic temporal element: point or interval ?
Intervals can be related in various ways: "I1 is completely before 12"; "I1 abuts I2", "I1 overlaps I2", etc.[l].
All these relations are possible between a cause and an effect, but they are subsumed by the general principle, according to which effects never precede their causes; therefore we find it simpler to use only t i m e points.
We do not mean to reduce causation merely to a temporal relationship: what we present below shows the contrary.
Let us mention, in addition, that most approaches choose discrete sets, isomorphic to integers, to represent time.
In order to better reflect our intuitions on continuity, we prefer to take reals, but we do not consider this choice as critical,  1.
Introduction A definition of the concept of cause, if at all possible, would involve deep philosophical questions ; we do not need to tackle them, however, in order to use a practical notion of cause.
Intuitively, this notion is necessary in our everyday reasoning, both to anticipate what should happen if we decide to perform an action, and to diagnose what might have happened to yield a given state of affairs.
We propose to prune the collection of propositions which might be considered as causes by preferring to take as causes the result of the free will of an agent, i.e.
hidher ability to opt for performing or not performing a given action.
Casting causal reasoning in this framework leads to explore the problem of reasoning about actions, generally considered to be a nonmonotonic (i.e.
defeasible) temporal reasoning.
Temporal reasoning is said to require a choice among two ontologies : point-based or intervalbased.
Although we consider that many non-trivial  14 0-8186-7528/96 $5.00 0 1996 IEEE  the result of causal relations.
The possibility of having a branching past implies that the "interesting propositions" do not include "historical" statements, since otherwise different pasts could never lead to the same time point.
Having time points defined both by the subset of true propositions and by the date allows to distinguish several occurrcnces of the same state of affairs.
If we need to represent cyclic phenomena, where this distinction is useless, we may add an equivalence relation on time points: t l E t2 if they differ only by their date, and then reason on the quotient set.
The alternative, i.e.
define time points only by the subset of true propositions, does not allow to restore the notion of date when needed.
To extract the date of a time point, we define a function date : T I+ 93 which maps every time point on a real number representing its date.
To simplify, we write date(t) as dt.
especially as our examples need only to consider a finite number of time points.
Our second choice amounts to select either a linear or a branching model of time; time is intuitively linear.
However, as we deal with choice making, the very representation of a choice immediately suggests the use of branching time.
Among the time points, some particular ones, called choice points, are intended to represent states of affairs where an agent can take the decision of performing or not an action: obviously, not all time points are choice points, since the conditions allowing for the action are not always satisfied.
The decision of the agent is represented as a time line splitting into two futures, or more accurately, as two distinct time lines having the same time points up to the choice point.
This is consonant with most systems, which have a branching future [lo].
But we consider as well a branching past [14], because we often need to examine two different courses of events leading to the same situation.
We also allow time lines to meet again in the future (case of an action without long-range effects, for example).
Fig.
1 below will provide an illustration.
We now present more formally the general framework corresponding to our choices.
Definition 2: We call time line 1 (somehow similarly to McDermott's "chronicle" [IO]) a set of time points in bijection with the set of dates, meant to represent a possible evolution of the universe.
A time line hence conveys the complete evolution of the truth value of the "interesting propositions".
The time points of a time line are supposed to comply with the general principle: "there is no effect without a cause".
Their propositions are then the result of cause-effect relations governed by causal rules.
The set of causal rules is gathered in a rule base called BR.
The time points of a time line are totally ordered by a precedence relation written "I",where tKt2 means that time point t2 does not precede t l , hence whenever tlSt2 we have dtlSdt2, but the converse does not hold (see Fig.1).
3.
Temporal ontology Definition 1: A time point t is a "snapshot" state of the universe defined by a subset of true propositions at a certain date and by this date.
T is the set of time points.
The subset mentioned in the definition is not arbitrary: we sometimes refer to it as the set of "interesting propositions", i.e.
in our framework,  15  dO  d2  dl  Figure 1: the structure of branching time in the past and in the future.
The thick line represents a time line, I, including among others time points to, t i , t2.
The other curved lines represent other time lines.
tO c tl c t2 holds, as do tO < t"2 and t i < t'2, but there is no relation between t i and t"2, although dtl< d t y is true.
- L is the set of time lines, 4.
The language  -  3 is the set of real numbers, if t has exactly as  - t E 1 is true in the model  its true propositions the set I(l,dt), - v(p,l,dt) is true in the model iff p E Z(l,dt), i.e.
proposition p is true at the time point determined by time line 1 and date dt.
It follows that nocc(p,l,dt,A) is true in the model iff (b't') ((t'E1 A dtld&+A) p 6i Z(1,dtt)).
The proposed langage A!
is defined at two levels: * the first level is meant to represent static information.
It is a plain propositional language in which: P is a set of propositions we are interested in, A, subset of P, is a set of actions, and E, subset of P, is a set of effects, with A n E = 0 and A v E = P. the second level expresses dynamic information.
It contains predicates with time variables.
If p is a formula of the first level, I a time line, t E 1 a time point, a formula of the second level has the form: - v(p, I , d t ) with the intended meaning that formula p is true in 1 at the date of time point t, and: - nocc(p,l,d,A) with the intended meaning that p is never true in line 1 from the date of time point t on, during the delay A.
In other words, nocc(p,l,dt,A) is a short-hand for : (Vt') ((t'E I A O 2 d t d f < A) 3 -~V(p,Z,df))  =I  The dependence of the effect on the cause may vary according to the context.
We shall therefore introduce a subset of preferred time lines and augment the language with an operator denoted "a'' meaning normally implies (in a more comprehensive presentation, see [ 111, we also have an operator "-+" meaning implies in all cases).
The intuitive idea behind these two new notions is as follows: when an agent chooses to perform an action, he or she does not anticipate every possible outcome of his or her choice: several circumstances, unknown to, or neglected by, the agent at the moment of the choice, may alter the predictable effect of the action.
Informally, the "preferred" time lines are the futures that the agent normally "had in mind" when he or she opted to perform the action.
The effects which are present in all "preferred time lines" following an action are said to be "normally implied" by the action.
The idea of "normal implication" is inspired by the work of  We are going to extend gradually this language, but first let us define its semantics.
The associated model theory is a generalization of Kripke [8] possible world semantics.
In this model, an interpretation is defined as function I mapping the Cartesian product L X % into a subset of propositions, i.e.
Z : L X 93 I+ 2 p , where:  16  Delgrande [3].
We defer a more precise explanation until we introduce some more notions.
To make sense of the notion of "normality" requires to reason with uncertain information: in the absence of specific information, we are entitled to believe that things behave normally.
This brings us to a problem similar to the well-known "frame problem", which is inherent to any theory of change.
We must therefore take into account: the preconditions of an action a , i.e.
represent what is reasonable to assume whenever performing a is considered; the hindrances of an action a, i.e.
represent the effects of other actions that can inhibit the effects of a; the persistence of states, corresponding to the fact that some propositions continue to hold true for some duration, unless an external event entails their falsity.
All these aspects generally require the use of a nonmonotonic reasoning.
That is the reason why we devote next section to this issue.
time line, as we introduced it in 54.
Its definition requires the preliminary notion of coincidence, viz.
Definition 3: Two time lines  11 and 12 coincide up to time point t, property written coincide(ll,12,t) ifffor every time point t'preceding t, t' E 11 = t' E 12.
In other words, a model satisfies coincide (Zi, 12,t) iff (b't')(t'lt 2 Z(ll, dti)=Z(12,dtl))  Definition 4: The set of preferred time lines for line 1 ut time point t, noted Lp(1,dt) is a subset of L obtained by a function Lp :L X 3'I+ 2L such that: (Vl,l',t) (I' E Lp(l,dt) 3 coincide(l,l',t)) We are now in position to define formally normal implication:  Definition 5: Action a normally implies effect e within the delay A, noted a 3 e [A], ifs (Vl,t ) ( C l A C2) where C1 and C2 stand for the following conditions: C1 {tv(a,Wt) A (YP)(pcnorm(a)3 v ( p , W t ) ) l 3 { Vl') (l'sLp(1,dt)I> [(dt') (t'sl' A dtldt.ldt+A A v(e,l',dt~))v (3e',t") (e'EUR inhibit(e,a) A t"s1' A v(e',l ',dy)A dt.GQGit+A)l)} C2 { v ( - r a , l , d t ) I> (31') ( I ' E L p ( l , d t ) A noccte,l',dt,A))J  5.
Nonmonotonicity In [ l l ] , we show how to include implicit premises in a normal inference.
We suppose the existence of a function, called norm, to define the normal conditions under which an action is executed.
Technically, norm : A I+ 2p is such that for any action a , n o r m ( a ) contains those propositions (preconditions) which are true unless otherwise specified when an agent considers to perform a.
Extending the domain of norm to our "first-level" language, i.e.
defining compositionally norm (a op a') where op is a boolean operator is not a trivial task, if we want to remain compatible with our intuitions.
We will not treat this problem here.
As we saw the importance of defining "hindrances", we suppose similarly the existence of a function inhibit that, for any couple <e,a> where e is a normal effect of a, determines the events which are liable to prevent e from following a .
Technically, inhibit : E X A + 2E is such that inhibit(e,a) is the subset E' of E where e' E E' iff whenever e' occurs during the delay after a where e should turn true, e may actually remain false.
Notice that action a' causing e' may happen before, with, or after a.
Similarly, extending the domain of inhibit to couples of formulas instead of couples of atoms is a very thorny issue.
We turn now to what we mean by "normal case".
This notion is often attached to a preference ordering, but we notice that the definition of a socalled "correct order" is rather difficult; therefore, we find it more convenient to use the notion of preferred  This rather intricate definition calls for some explanations: C1 tells that whenever a occurs under normal conditions, in all preferred futures, either there exists a subsequent occurrence of event e within the delay A, or there is an occurrence of event e' known to inhibit the effect of a ; e' must then occur after t within the prescribed delay (notice that, even in this case, e may become true); C2 checks that if a is not executed, there exists at least one preferred future in which e will not occur within the specified delay A.
This condition reflects the implicit counterfactuality always present in causation: we are not ready to say that a normally implies e if we think that, even without performing a, e will nevertheless occur in every likely future.
We now turn to the last problem related with the "frame problem", namely persistence.
Example 1: Suppose that the following facts and rule are given.
They represent the well-known "Yale Shooting Problem" (Y.S.P.)
according to our no tation: - v(Fred is alive,l,dto) - v(gun is loaded,l,dtl)  17  one in which the action is performed, the other in which it is not.
The free will of the agent is exactly histher ability to choose which of these two lines will correspond to reality.
- v(shoot at Fred,l,dt2) - shoot at Fred lalive Fred [A]  *  with dtoldt11dt2 and A: a few seconds.
Two problems have to be considered in relation with the persistence of a given event e: 1. temporal nonmonotonicity, i.e.
the possibility that an external event prevents e from remaining true, and 2. the estimation of the duration of the persistence of a fact, i.e.
how long, after e has begun to be true, is it likely that it is still true ?
Suppose that we heve the answer to point 2., and let a be the "normal duration" of fact e. We can define persistence as follows:  Definition 7: t is called a choice point relative to action a, among lines 11 and 12 {noted pchoix(a,11,12,t)) iff ( V t ' ) ((t'<t 3 coincide (11,12,t')) A v(a,li,dt) A v(-a,L2,dt)) The set of causal rules BR and the definitions provided so far allow us to define the set of voluntary causes of an effect e observed on time line I' at time point t':  Definition 6 (persistence): We note persist(e,d)  Definition 8: the voluntary causes are defined as a partial function: causev .
E x L x T + 2A defined only if v(e,l',dtf)holds.
Then, causev (e,l',t') is the subset A ' of A such that U E A ' iff a satisfies conditions C K 4 (the scope of t and 1 extends from C2 to C4, and of A includes C1 and C2): C l ( 3 A ) (a 3 e[A] E BR) C2 (3t,1,1") (pchoix(a,1, l",t) A dt_<ti_<t+RelevantDelay), where: if ( 33) (persist(e, 3) E TP) then Relevant-Delay = A + delse Relevant-Delay = A, c3 1'E Lp(1,dt) C4 v( T e ,1,d,) .
the fact that, without any external influence, event e is believed to remain true for a duration d. We have: (t'E1 A persist(e,d) 2 {Vto,l) ((v(e,l,dto)A (3') nocc(e,l,t',dt,-dtI))) 3 {Vl',t) ( ( t e l l A C1 A C2 A C3) 3 v(e,l',dt))) where C l , C2, and C3 abbreviate the following conditions: C1 I'E Lp(1,dtO) C2 dtoldtldto+ d 7 e [ A ] ) ~ B RI> nocc(a',l',dto-A, C3 { Va',A) ((a' dt-dto+A)) to is the time point where fact e becomes true in time line 1; C1 expresses that persistence is predicted at least in the preferred futures; C2, that persistence lasts at least for (without prohibiting it beyond this duration), provided that no action a', known in B R to make e false, occurs during the relevant lapse (C3).
(We cannot use the function inhibit here, since we want the persistence of an event to be defined without reference to its cause, while inhibit defines a set of events related to both a cause and its effect).
In the same way as implications are gathered in a rule base BR, the known persistences are collected in a "table of persistences" TP.
We now have all the prerequisites necessary to investigate which set of actions can reasonably be held as causally responsible for a given event e.  C1 selects the set of causal rules of B R containing the effect e in their right part (we examine in [ 111 the possibility of exploring what we call closure(BR) instead of BR, in order to take into account the actions which are known to cause an effect e ' , of which e is a tautological consequence); C2 means that the agent had the choice (at a time which is relevant for the observation of e ) between doing and not doing action a, and that he or she chose to do a ... C3 ... in a time line 1 for which time line I' (where event e has actually occurred) is among the preferred futures at the time where the choice has been made; C4 specifies the relevance of the action to the observed event: e must not already be true at the moment of executing a in 1.  a  6.
Explanation The reader should remember that the notion of action is essential in our theory, since we decided to privilege, when asked to find the causes of a state of - which we take to be actions executed by agents in virtue of their free will - over "natural laws".
We that a choice point is a time point from which stem (at least) two different time lines,  Suppose that we have a description of the evolution of a world by of a set D L of statements using the predicates and nocc.
The above definition can be used to solve the explanation problem, if we also have at our disposal general information such as BR and TP, and provided that some assumptions concerning the completeness of  18  DL are accepted.
[ll] gives further results, and  simultaneously providing the same result) and the fact that we require the actions to be instantaneous.
extends Definition 8 to the case where an operator + for "implies in all cases" is added.
8.
Conclusion  7, Related works  In this paper, we have developped:  - a simple temporal ontology,  Recent publications [ 12,151 contain thorough discussions of other approaches, namely chronological minimization [6,7,13] or causal minimisation [9].
They show why such approaches fail to handle adequately prediction, explanation, or ramification problems.
The aim of this section is not to restart these discussions.
However, we would like to show briefly how we tackle the central problem illustrated by the already mentionned Y.S.P.
To DL and BR given in section 5, we add TP containing the facts that alive and loaded persist indefinitely; we add also the fact that inhibit( - d i v e , shoot) contains facts like deviation-of-bullet and so on.
The conflict of persistence between alive and loaded does not arise in our approach.
As a matter of fact, the assumption of completeness on DL enables us to derive that no inhibiting fact prevents - d i v e to occur, once shoot has been done; therefore, we predict lalive.
If we also have a rule like shoot 4oaded in BR, definition 6 cannot be used to predict the persistence of loaded.
Knowing what belongs to inhibit(Tloaded, shoot) - or assuming this set to be empty, in the absence of any information on the subject -, we predict lloaded as well.
We now consider backward reasoning, adding to D L a statement such as v(Fred is alive,l,dt~)and dt2+A<dt3 : if we have to explain this anomalous state, our approach will consider two possible tracks to follow: the persistence of loaded has stopped (the gun has somehow become unloaded between dtl and dt2) or some inhibiting effect (e.g.
deviation-ofbullet) has occurred between the action shoot and its normal effect lalive, that is between dt2 and dt3.
However, we cannot prefer one track over the other, nor can we guess exactly when, in the intervals defined, the anomalous fact took place.
In contrast to this intuition, the chronological systems tend to prefer the sequence of world states where the gun becomes unloaded just before shooting, because this sequence postpones the change as long as it is consistent to do so.
More recent approaches [2,12,151 do not present these anomalies, but they should be augmented with the possibility of inhibiting effects after the action; otherwise, they are not able to propose the second kind of explanations.
Finally, the solution advocated for in this paper also runs into some difficulties.
An important one concerns concurrent actions (two or more actions  - the role of an agent in the evolution of the  world.
This approach seems to provide an intuitively correct analysis of the main problems encountered in the A.I.
literature: the explanation problem, the prediction problem, and the ramification problem (see [ 111 for examples).
Our approach should easily extend to the case where the first-level language is first-order.
We do not anticipate too many difficulties to take into account the duration of actions: instead of a threeplace predicate v(a,t,dt), we might abbreviate the formula: ( V t ) ( t l < t < t 2 3 v ( a , l , d t ) ) into v(a,1,dtl ,dt2), a four-place predicate.
This should also help us to handle the case of concurrent actions.
As another direction of further research, we are taking advantage of a cospus of car-crash reports, which is currently studied in our Laboratory [ 5 ] .The goal is to determine what should be put in BR and TP in order to find intuitively correct answers to questions concerning the causes of the accident.
As it is often the case in Artificial Intelligence, real-size problems reveal issues which are not even visible in toy problems, such as those which illustrate the present paper.
REFERENCES [l] James ALLEN: Towards a general theory of action and time.
Artificial Intelligence vo1.23 pp.
123-154,  1984 [2] A.B.BAKER: A simple solution to the Yale Shooting Problem.
Intern.
Con$ on Knowledge Representation and Reasoning pp.
11-20, 1989 [3] James P.DELGRANDE: A first-order conditional logic for prototypical properties.
Artificial Intelligence vo1.33 pp.105-130, 1987 Frangoise GAYRAL, Philippe GRANDEMANGE: Evtnements : ponctualitt et durativitt.
81hA F C E T RFIA Congress pp.905-910, Lyon (F), Nov. 1991 [5] Frangoise GAYRAL, Philippe GRANDEMANGE, Daniel KAYSER, FranGois LEVY: InterprCtation des constats d'accidents : reprksenter le rCel et le potentiel Approches se'maiztiques t.a.1.
vo1.35 n"1 pp.65-81, 1994 [6] B.A.HAUGH: Simple causal minimization for  [PI  temporal persistence and projection.
AAA1 pp.218223.
1987  19  [7] Henry A.KAUTZ: The logic of persistence.
Y h National Conference on Artificial Intelligence pp.401-  [12] Erik SANDEWALL: The range of applicability of nonmonotonic logics for the inertia problem.
13th IJCAI pp.738-743, Chambery, 1993 [I31 Yoav SHOHAM: Reasoning about change: time and causation from the standpoint of Artificial Intelligence.
M. I. T.Press 1988 [14] Yoav SHOHAM: Time for Action : On the Relation Between Time, Knowledge and Action.
l l t hIJCAI pp.954-959 & 1173, Detroit, 1989 [ 151 Lynn A.STEIN, Leora MORGENSTERN: Motivated action theory: a formal theory of causal reasoning.
Artificial Intelligence voI.7 1 pp.
1-42, 1994  405, 1986 [8] Saul A.KRIPKE: Semantical considerations on modal logic.
Acta philosophica fennica vo1.16 pp.8394, 1963 [9] Vladimir LIFSHITZ: Computing Circumscription.
9th IJCAI pp.121-127, Los Angeles, 1985 [ l o ] Drew V.McDERMOTT: A Temporal Logic for Reasoning about Processes and Plans.
Cognitive Science vo1.6 pp.101-155, 1982 [ 111 Aicha MOKHTARI: Action-based causal reasoning.
Applied Intelligence to appear  20
A Theory of Time and Temporal Incidence based on Instants and Periods* Lluis Vila+  Eddie Schwalb  Information and Computer Science Dept.
University of California, Irvine Abstract Time is fundamental in representing and reasoning about changing domains.
A proper temporal representation requires characterizing two notions: (1) time itself, and (2) temporal incidence, i.e.
the domainindependent properties for the truth-value of Juents and events throughout time.
There are some problematic issues such as the expression of instantaneous events and instantaneous holding of fluents, the specification of the properties for the temporal holding of Juents and the Dividing Instant Problem.
This paper presents a theory of tame and temporal incidence which is more natural than its predecessors and satisfactorily addresses the issues above.
Our theo y of time, called Z P , is based on having instants and periods at equal level.
We define a theory of temporal incidence upon at whose main original feature is the distinction between continuous and discrete fluents.
1  time.
Commonly there is a distinction between propositions describing the state of the world (fluents) and those describing the occurrences that make the world change (events).
For these frameworks, that claim to have a wide range of practical applicationsin AI, an appropriate and precise theory of time is a fundamental component.
To enjoy a proper temporal representation, two notions need to be defined: 1. time itself, and 2. temporal incidence, i.e.
the domain-independent properties for the truth-value of fluents and events throughout time.
The aim of this paper is to provide a natural theory of time and temporal incidence that supports the formalization of changing domains where discrete and continuous phenomena occur.
There are a number of problematic issues that have been encountered by previous attempts.
Namely, the expression of instantaneous events and instantaneous holding of jluents, the specification of the properties for the temporal holding of fluents and the Dividing Instant Problem.
This paper presents a theory of time and temporal incidence which can be used as a formal ground t o model changing domains.
Our theory of time is based on both instants and periods2 together at equal level.
We call it Z'P.
We define a theory of temporal incidence upon 2P.
Its major original feature is the distinction between continuous and discrete fluents.
Although the difference between them is something commonly agreed, there is no previous attempt were the specific features where they differ and its close relation with the theory of time have been accounted properly.
It is a simple idea yet combined with the instant/period ontology turns out to be sufficient to satisfactorily address the problems encountered by previous approaches.
Section 2 presents some problematic issues to be considered.
Section 3 discusses the shortcomings of previous approaches.
Section 4 presents ZP.
Section 5 presents our categorization of propositions and the theory of their temporal incidence.
In section 6 we discuss how the above problems are satisfactorily addressed and section 7 presents an example where the  Introduction  In order for an intelligent system to interact with the real world it needs to be able to reason about the changes that happen in it and the events and actions that originate them.
Consider, as an illustrative example, the so-called hybrid systems [lo].
These are systems that involve both discrete and continuous change.
For instance many electro-mechanical devices exhibit both continuous (e.g.
the charge in a battery) and discrete behavior (e.g.
a digital signal), and involve events that can be viewed as instantaneous (e.g.
to close a relay) and others that take time (e.g.
recharge the battery)'.
The notion of Time has been recognized as a fundamental in formalizing change and action.
Many theories for change and action are built upon a theory of time [17, 1, 14, 25, 20, 15, 4, 6, 7, 19, 13, 241.
In these systems, the domain at hand is formalized by expressing how propositions are true or false throughout *Contact address: Lluis Vila, 444 CS UCI, Irvine CA 92717; vila@ics.uci.edu;http://www.ics.uci/ vila tPartially supported by a MEC of Spain grant (EX94 77909683).
'Hybrid systems are interesting because m a n y real systems can be modeled as such.
2By period we merely mean a time interval.
21 0-8186-7528/96 $5.00 0 1996 IEEE  Non-atomic fluents.
In some cases, axiomatizing the holding of non-atomic fluents such as negation, conjunction or disjunction of atomic fluents may not be straight forward [20, 71.  behavior a hybrid model is described using our approach.
Finally section 8 summarizes our contribution.
Problematic Issues  2  Dividing Instant Problem (DIP).
Assuming that time is made of instants and periods, we need to determine the truth-value of a fluent f (e.g.
"the light is on") at an instant i, given that f is true on a period p 1 ending at i and it is false at a period p2 beginning at it (see figure 2) [8, 21, 1, 71.
Instantaneous events.
There are many events like "turn off the light", "shoot the gun", "start moving" which intuitively are viewed as instantaneous.
Modeling them can be controversial since null duration time elements seem to be needed to talk about them.
Moreover, some problems arise when we need to model complex sequences of them occurring in presence of continuous change (we discuss it in detail in section 7).
propositions  -ff  Instantaneous fluent holding.
Modeling continuous change, i.e.
taking account of fluents whose value is continuously changing, involves representing parameters whose value may hold for only a single instant.
A simple, representative example is the parameter "speed" of a ball tossed upwards in what we call it the Tossed Ball Scenarzo (TBS) (see figure 1).
,  p; f?
P2  I I  i  e  time  Figure 2: The Dividing Instant Problem (DIP).
If we want to be logically consistent the problem arises.
If intervals are closed then f and lf are both true at i .
If they are open we might have a "truth gap" at i .
The other two options are open/closed and closed/open intervals.
An arbitrary decision here is artificial.
Take, for instance, the fluent "being in contact with the floor".
It seems that p1 is closed/open interval whereas p 2 is open/closed.
I Figure 1: The Tossed Ball Scenario (TBS).
3  There must be an instant where the speed of the ball is zero, being not zero for a period before and a period after it.
To model it we require the ability of talking about the holding of fluents at instants.
However, it may lead to the dividing instant problem below.
In this section we briefly summarize the main criticisms to previous approaches in terms of the issues above.
Instant-based approaches [17, 20, 41 They have been criticized for not being natural ("our direct experience is with phenomena that take time"), for being 121 and for getting in trouble with  Non-instantaneous fluent holding.
Some difficulties arise when defining the properties of temporal incidence for non-instantaneous fluents such as the following:  Allen's interval-based approach [l]  Homogeneity: If a fluent is true on a piece of time it must hold on any subtime [l,71.
e  Related Work  Neither instantaneous holding of fluents nor instantaneous events can be represented, the reason being that there are no instants.
Concatenativity3: If a fluent is true on two consecutive pieces of time it must be true on the piece of time obtained by concatenating them.
Notice that there may be different views for the meaning of "consecutive" .
Characterizing holding of continuously changing fluents exhibits semantical problems.
Specificly, the axiom for "homogeneity of fluent holding" (axiom H.2) and the axiom for "holding of negated fluents" (axiom H.4) do conflict.
This and the previous problem are precisely presented in [7].
3Thisproperty, in addition to being a semantic issue, may be important for computational efficiency since it allows to have a compact representation of fluents holding throughout numerous consecutive or overlaping periods.
22  4  Allen and Hayes [2] They propose a theory of intervals but they suggest various ways of deriving instants from them.
One type is defined as nests of intervals.
These "instants", however, are only used for instantaneous events but not for fluents.
They write ([4,section 4) ".
.
.resolutely refusing to allow fluents to hold at points.
One could define a notion of a fluent X being true at a point p by saying that X is true at p just when there is some interval I containing p during which X is true".
This is again not satisfactory for modeling continuous fluents (consider the ('zero speed" fluent in the TBS).
The other type, called moment, is defined as an indivisible period.
This is not adequate either because, although very short, it takes some time.
In the TBS, for example, we cannot use a moment to talk about the time where the ball speed is zero because then the periods p l and p2 would not meet.
This would entail that the ball is hovering in the air for a while.
The technique of change of granularity[9] applied to fix this shortcoming is problematic too since it leads to unintuitive and non-uniform models of time and introduces unnecessary technical complications [22].
Time  In this section we present our theory of time, called  ZP, based on the idea of having instants and periods  at equal We are lead by the intuition that a period is characterized by its two endpoints.
Our language for time has two sorts of symbols, the instants sort (2) and the periods sort ( P ) , which are formed by two infinite disjoint sets of symbols, and three primitive binary relation symbols, 4 :Z x Z and begin, end : z x p .
The first-order logical formulation of ZP consists of the following axioms: '(i 4 i) i 4 i' -(it 4 i)  *  i 4 it A it 4 i" + i 4 i" i 4 i' V i 3 i' V i = i' 3i' (i' 4 i) 3i' (i 4 i')  +  begin(i,p) A end(i',p) i 4 i' 3 i begin(i, p) 3i end(i,p) begin(i,p) A begin(i',p) i = i' end(i,p) A end(i',p) j 2 = i' i 4 i' 3 p (begin(i,p) A end(i',p)) begin(i,p) A end(i',p) A A begin(i,p') A end(i',p') =+-p = p'  *  Galton's instant-period theory [7] Galton's approach presents two central characteristics: 1.
It is based on a time ontology with instants and periods on the same footang.
The theory of time is not based on the classical set-theoretic interval constructions in order to avoid the DIP.
IPltIP4are the conditions for 4 to be an strict linear order -namely irreflexive, asymmetric, transitive and linear- relation over the instants6 IPS imposes unboundness on this ordered set.
IP6 is intended to order the extremes of a period.
This axiom rules out durationless periods which are not necessary since we have instants as a primitive.
Thus an instant cannot be identified as a null duration period.
The pairs of axioms IP7.- and IPS.- formalize the intuition that the beginning and ending instants of a period always exist and are unique respectively.
Conversely, axioms IPS and 1 9 0 are intended to ensure the existence and uniqueness of periods t o close the connection between instants and periods.
We now characterize the models by following the simple intuition of an interval being an ordered pair.
2.
It diversifies fluents into instantaneous/durable and states of positionlstates of motion: a state of position can hold at isolated instants; if it holds during a period it holds at its limits (e.g.
"a quantity taking a particular value"); a state of motion cannot hold at isolated instants (e.g.
"a body's being at rest").
The main shortcomings of this approach are the following:  1.
The theory formed by the axioms is too weak for a proper account of the relations between instants and periods.
We discuss it in detail in section 4.
2.
It's not clear that Galton's new types of fluents are useful.
Let us try to formalize the TBS: we consider the fluent speed # 0; if we model it as a state of position then we get in trouble; speed J.
0 holds on p1 and p z which must contain the limiting instants including i where speed= 0; If we consider it an state of motion then we are not allowed to say that lspeed # 0 is true at the isolated instant i.
Definition 1 (ZP-structure) A n ZP-structure 2s a tuple Zd, P d , <d, begind,endd) where Z d and P d are sets o instants and periods respectively, <d is a binary relation on Zd and begind,endd are binary relations On z d , P d .
/  3.
While states of position are concatenable states  We merely view periods as ordered pairs of instants.
We show that the elements and the pairs of an unbounded linear order S form a model for ZP.
of motion are not always.
It looks somewhat counter-intuitive: it seems that states of position should not be concatenable since the quantity they represent may have a different value at the meeting point.
Since it is not the case for states of motion it seems that they should be concatenable.
In section 5 we shall follow this intuition.
4Very much with in the same spirit of [7] and [5].
5Results on Z?
'are a summary of those presented in [22, 231.
'Notice that I 4 is actually redundant since it can be derived from IPz.
We include it for clarity.
23  The reason of this weakness is the too loose connection between instants and periods.
It can be properly extended to fit the idea of having an instant at the meeting points [22].
The resulting theory is equivalent to ZPdense.
Theorem 1 (a model) Given an infinite set S and an unbounded strict Iinear order < on it then the ZPstructure ( S ,pairs(S),<,Jirst, second) forms a model of Z P .
Corollary 1 Every model of T P is characterized b y an injinite set S and a nunbounded strict Iinear order < on it.
In this section we present a theory of the ascription of fluents and occurrences in time.
The key issue to consider is the temporal incidence of a fluent at the extremes of a period.
Our approach is based on the following ideas:  The subtheory of dense models, we call it ZPdenSe, is axiomatized by adding the following density axiom: IPll  i 4 i'  j 3i"  (i 4 i" 4 i')  1.
We allow fluents to hold at poznts.
We demonstrate that this does not, in fact, create any problem.
Corollary 2 (dense models) The models of ZPdenseare characterized b y the set of elements and the set of ordered pairs of distinct elements of an unbounded, "dense", strict linearly ordered set.
Hence ZPdepzseisan axiomatization of T h ( ( I N T ( Q )Q , )).
2.
We dzstznguish between contznuous and discrete jluents.
We diversify fluents according to whether the change on the parameter they model is continuous or dzscrete.
Let us see how our theory relates to previous ones.
The original Allen and Hayes's theory (let us call it I A N ) [a] is exclusively based on intervals.
For the sake of comparison, we can introduce instants using the following technique used by Ladkin [16]: we take pairs of intervals one meeting the other, apply the equivalence relation "having the same meeting point" and associate a instant to each class Let us call this extension TAX,,  Temporal Incidence  5  Remark 1 Z P accepts both dense and discrete models of time.
We use a standard reified temporal first-order language with equality (as in [17, 1, 71).
The decisions made regarding temporal representation are the following: e  Time theory: We take ZP.
We define the instant-to-period (such as within) and period-toperiod (such as Meets) relations upon 5 , begin and end for convenience.
e  Reified propositions: We use standard firstorder language forms although we shall not deal with quantified expressions.
Propositions are classified into three classes: contanuous fluents, discrete fluents' and events.
e  Temporal Occurrence Predicates (TOPS).
We shall introduce a different TOP for each combination of temporal proposition and time unit (similar to [14, 71):  .
We obtain a theory whose class of models is the same as our instant-period axiomatization, i.e.
the theories are equivalent.
Theorem 2 2 7 E Z A ~ , , Galton proposes [7] a theory of instants and periods "on the same footing".
The axioms seem appropriate to avoid DIP-like criticisms and to specify the properties of the various types of fluents introduced.
However, the theory looks very artificial.
It is difficult to figure out the intuition that some of they intend to capture and it complicates the proofs of theorems.
A more serious criticism is the fact that the theory is too weak.
It is easy to identify examples of counterintuitive models accepted by the theory:  H o L D s ~ ~ ( ~ , ~ The ) continuous fluent f holds throughout the period p The discrete fluent f , H o L D s $ ( ~ , ~G ) holds throughout the period p The continuous fluent f H o L D s ~ ~i)( ~ , holds at the instant z WOLDS$(~, i) = The discrete fluent f holds at the instant i OccmSon(e,p)E The event e occurs on the period p Occmsat(e,2) E The event e occurs at the instant i  Example 1 Let us take a basic model M composed of an infinite set of periods and Allen's relations satisfying interval axioms plus an infinite set of instants which make M satisfy I1 -for example I N T ( Q ) as periods and Q as instants: e  Example model 1: M plus a single instant i Q which limits a certain period p in M and only that one.
In particular it does not limit a n y of those periods that meet or are met b y p .
'We use the equality relation to express a fluent representing a parameter taking a certain value.
E.g.
the speed of a ball bep ) .
We omit ing positive on p is expressed as HoLDs(speed = necessary axioms imposing the exclusivity among the different values of a parameter.
i$Q which limits a certain period p in M and as not within a n y period.
I n particular it is not within any of those periods that overlap p .
e Example model 2: M plus a single instant  +,  24  Terminology.
Henceforth we use the following Non-atomic fluents.
notational shorthands.
We may use begin and end in functional form (e.g.
i = begin@)).
H O L D S ~ ~ Negation: stands for both HOLDS, and HOLDS$ and H O L D S , ~ stands for both HOLDS" and HOLDS>.
We define within(i,p) E begin(ilat, p ) A end(i",pj i' + i 4 i" and, given p i p' such that Meets(p,p'), p" = Conjunction: Concat(p,p') begin(p'/) = begin(p) A end(p") = end(p').
We use its functional form too.
5.1 Properties of Temporal Incidence Since instants and periods are defined at the same Disjunction: primitive ontological level in ZP, we are not forced to accept any assumption on the relation between the HOLD^(^ v f',z) ++ H o L D s , ~ ( ~2), V H o r " ( f ' , holding of a fluent on a period and its holding at the period endpoints.
A fluent holds during a period if and only if it holds at the inner instants: HoLDs~~(e ~ , ~ )(within(i,p) + HOLD^(^,^)) (1)  6  Nothing can be inferred about its holding at its endpoints.
Either the fluent or its negation may hold at them.
Our opinion is that this is domain-dependent.
The fluent holding at the extreme point can go either way: (1) It can be the fluent finishin a t that point (an example is perhaps "shoot the gun'$, (2) the fluent starting at it (an example might be "start moving"), or a different fluent representing the state of changing e example here could be "turn on the light").
By keeping them independent we already avoid some of the problems in section 2 as we shall see in a moment.
2)  (5)  Revisiting the Problems  Let us now see how the problems presented in section 2 are addressed.
Instantaneous events.
Since we have instants in our ontology, we can express them straight forwardly using OCCURS,~ a t an instant.
In the DIP, for instance, we have OCCURs,l(switchoff, i).
In section 7 we discuss the issues regarding sequences of events.
Instantaneous holding.
Having instants in our ontology allows us to talk about instantaneous holding of a fluent.
The underlying view of "open intervals" (axiom 1) ensures that there will be no conflict with the holding of that fluent on meeting periods.
For discrete fluents it only can happen if it holds on a meeting period (as enforced by axiom 2).
In the case of a continuous fluent there is ncrestriction and it can be expressed using the HOLDS,~ predicate.
In the TBS, for instance, the speed of the ball is a continuous fluent and the scenario is simply expressed as:  Continuous fluents.
A continuous fluent may hold both during a period and at a particular instant without any restriction.
This is not the case for discrete ones.
Discrete fluents.
The genuine property of discrete fluents is that they cannot hold at an isolated anstant: HoLDs$(~, i ) + 3p ( H o L D s , ~ , ( ~ , ~ ) A  +,  HoLDSLn(speed = p l ) HoLDSgt(speed = 0, i) HoLDS,,(speed = -, p 2 )  (within(i,p) V begin(z,p) V end(2,p)))  (2)  Our distinction between continuous and discrete events do not fit with Galton's distinction between states of position and states of motion.
However discrete fluents correspond to Shoham's concatenable proposition types.
Identifying it as a key property in modeling changing domains is one of this paper's contributions.
end(p1) = i = begin(p2)  Non-instantaneous fluent holding.
Some of the fundamental properties of previous approaches are either theorems of our theory or they are very easy to specify.
For instance, Allen's Homogeneity H O L D S ~ ~ ( ~ , ~ )  In(p',p)  + I - I O L D S ; ~ ( ~ , P ' ) is  easy to prove.
The properties for concatenativity are as follows:  Events.
Unlike previous approaches, our proposal does not include any specific axiom governing the occurrence of events.
It corresponds to the intuition that events represent an accomplishment which may concurrently happen.
For example, one may accomplish producing a software module on the period "Jan/lstDec/3lst" and accomplish producing another module on the period "Jul/lst-Jul/3lst".
Theorem 3 (Concatenativity of discrete fluents) IfMeets(p,p') then HoLDs$(~,P) A H o L D s , ~ , ( ~ , P ' ) ++ HoLDs;(~, Concat(p, p'))  25  (6)  Theorem 4 (Concatenativity of continuous fluents) If Meets(p,p') then  Relay Solar  L+ T I  H o L D s ~ ~ ( (A~H, O ~ L) D S ; ~ ( ~ , P 'A) HoLDs,~(~, end(p) H O L D S ~ ( Concat(p,p'))) ~,  I  I  I  I  I  I  L -  (7)  Figure 3: The hybrid circuit example.
The Dividing Instant Problem.
The proposition in the DIP can be regarded as a discrete fluent.
Then the scenario is formalized as:  CO: "If the sun is shining and the relay is closed, the solar array is a source of current and the battery accumulates charge."
HoLDs$(light = on,pl) Meets(p1,pz) HOLDs$((light = off, p a )  D1: "If the relay is closed, when the signal from the controller goes high, then the relay opens."
Note that given this information only, the query HOLDS; (light = on, end(p2)) cannot be answered.
Answering it requires additional domain-dependent information.
In some cases we will like to regard period p l as closed at the end.
For instance, imagine the fluent f="being in contact with the floor" for a ball being lifted up.
At the dividing instant one probably want to have f=true.
There are other examples for which the closed period is pa.
In the light example, it might be the case that the most appropriate is viewing the fluent light as having three possible values {on, changing, off}.
We believe that this is a domain-dependent issue and our approach does not make any commitment about that.
However it provides the means to specify "safely" what happens at the dividing instant.
7  D2: "If the relay is open, when the signal from the  controller goes low, then the relay closes."
D3: "If the signal is low, when the controller detects that the charge level in the battery has reached the threshold q 2 , then the controller turns on the signal to the relay " Now, let us consider a particular envisionment or predicted qualitative behaviour.
Typically it is described as a sequence of states that hold alternatively at a point and during an interval.
When discrete events happen ".
.
.we would like to model them as being instantaneous" (Iwasaki).
The problems arise at the point where sequences of them need to be described, for instance "the signal goes high and smmedaately a f t e r the relay closes".
Given the initial state of our example where the szgnal is low, the relay is closed and the sun is shining, the intended envisionment would be the following sequence of states:  Example: Modelling Hybrid Systems  In this section we illustrate the application of our time theory by means of an example in qualitative modeling.
A (qualitative) model of a system is usually the result of an abstraction intended to simplify the analysis.
In hybrid models this abstraction produces discontinuous or discrete behaviors together with continuous ones.
There are many instances of Hybrad systems: most of electro-mechanical devices are, e.g.
photocopiers, cars, stereo sets, video cameras, etc.
There have been several attempts of introducing discrete changes into a continuous model in the area of qualitative modeling [18, 6, 11, 101.
Some semantical problems have been encountered because of the different nature of discrete changes and continuous change.
We shall see that an adequate theory of time and temporal incidence help in overcoming them.
Let's consider a particular example to illustrate these problems and how are they handled in our proposal.
The following example, the qualitative model, the intended envisionment and the tentative solutions are from [lo].
Figure 3 shows a simple circuit in which electric power is provided to a load either by a solar array or a rechargeable battery.
Its behavior is described as follows:  QBA  = 42  Q B A =?
s2.2  sa'gnuklow signakhigh signakhigh  rehy=chsed rehy=closed relay=open  The state S2.1 is produced by the signal going high and S2.2 by the relay closing.
It is not clear how to model the times of S2, S2.1 and S2.2 or the time spans and the discrete events between them.
Assuming that discrete events and changes take no time leads to logical contradiction because of their specification.
It is common for a change to take place only if the effect is not already in place.
In the case of D1, for example, the signal is low and goes high, but if the change is instantaneous both values for the signal would be true at the same time which is obviously contradictory.
The alternative is to assume that discrete changes take a very little time interval.
It is problematic too since the value of continuous variables changing concurrently becomes unknown after a sequence of actions.
In the example, the charge would keep continuously increasing for a short while.
After a number of discrete events these small variations accumulate and complicate the value computation.
26  Nevertheless, we have seen that instants are required to express instantaneous holding of fluents and instantaneous events.
We have shown that the DIP is not necessarily a problem.
0  We diversify fluents according to whether the change on the parameter they model is continuous or discrete.
We have shown that both features are required to adequately model discrete and continuous change.
The theory here presented should be of interest as a firm foundation for time to theoretical works on representation of time and to practical works on modeling changing domains.
References [l]J. Allen.
Towards a general theory of action and time.
Artzjiczal Intellzgence, 23:123-154, 1984.
S1 s2 S3  (ti, t2) tz ( t z , -)  QBA  < q2  QBA= qz QBA  < qZ  signatlow szgnak?
signathigh  [a] J.  Allen and P. Hayes.
Moments and points in an interval-based temporal logic.
Computational Intelligence, 5:225-238, 1989.  relay=closed relay=?
relay=open  [3] R. Alur, C. Courcoubetis, T. Henzinger, and P. Ho.
Hybrid automata: an algorithmic approach to the specification and analysis of hybrid systems.
In Proc.
Workshop on Theory of Hybrid Systems, pages 209-229, 1993.
[4] F. Bacchus, J .
Tenenberg, and J. Koomen.
A non-reified temporal logic.
In Proc.
KR'89, pages 2-10, 1989.
[5] A. Bochman.
Concerted instant-interval tempoOCCuRs,t(open( relay)),end(p3))  end(p2) = end(p3)  HOLDS;( relag, = open, p5)  Meets(p3,p5)  ral semantics i: Temporal ontologies.
Notre Dame Journal of Formal Logic, 31(3):403-414, 1990.
[6] K. Forbus.
Introducing actions into qualitative simulation.
In Proc.
IJCA1'89, pages 1273-1278, 1989.
The behavior the system in our example can also be formalized using our theory and this modelling method.
The rules are given in appendix A.
[7] A. Galton.
A critical examination of Allen's theory of action and time.
Artificrad Intelligence, 42:159-188, 1990.
Conclusion  8  [8] C. Hamblin.
Instants and intervals.
In J. Fraser, editor, The Study of Time, pages 325-331.
Springer-Verlag , 1972.
We discussed some problematic issues that arise in temporal representations of changing domains.
Namely, the expression of znstantaneous events and instantaneous holdzng of fluents, the specification of the properties for the temporal holding of fluents and the Divading Instant Problem.
We presented a simple theory of time and temporal incidence that satisfactorily overcomes the problems encountered by previous approaches.
Its main features are: 0  [9] J. Hobbs.
Granularity.
In Proc.
IJCA1'85, 1985.
[lo] Y. Iwasaki, A. Farquhar, V. Saraswat, D. Bobrow, and V. Gupta.
Modeling time in hybrid ssytems: How fast is 'instantaneous'?
In Proc.
IJCAI'S5, 1995.
[ll]Y. Iwasaki and C. Low.
Device modeling environment: An integrated model-formulation and simulation environment for continuous and discrete phenomena.
In Proc.
of Conf.
on Intellagent Systems Engzneerzng, 1992.
The time ontology is composed of both instants and periods, where a period is merely interpreted as an ordered pair of points.
Having instants is said to cause semantical problems due to the DIP.
27  A  H. Kamp.
Events, instants and temporal reference.
In R. Bauerle, U. Egli, and A. von Stechow, editors, Semantics from Diflerent Points of View, pages 376-417.
Springer-Verlag, 1979.
M. Koubarakis.
Foundations of Temporal Constraint Databases.
PhD thesis, National Technical University of Athens, Athens, Greece, 1994.
Rules formalizing the Behavior of the Circuit Example D1  D2  R. Kowalski and M. Sergot.
A logic-based calculus of events.
New Generation Computing, 3, 1986.
D3  B. Kuipers.
Qualitative simulation using timescaled abstraction.
Artificial Intelligence an Engineering, 3(4): 185-19 1, 1988.
P. Ladkin.
Models of axioms for time intervals.
In Proc.
AAAI'87, pages 234-239, 1987.
D. McDermott.
A temporal logic for reasoning about processes and plans.
Cognitive Scaence, 6:lOl-155, 1982.
T. Nishida and S .
Doshita.
ReasoninK about discontinuous change.
In Proc.
AAAI'8fpages 643648, 1987.
E. Schwalb, K. Kask, and R. Dechter.
Temporal reasoning with constraints on fluents and events.
In Proc.
AAAI'94, 1994.
Y. Shoham.
Temporal logics in AI: Semantical and ontological considerations.
Artificial Intelligence, 33:89-104, 1987.
J .
van Benthem.
The Logzc of Time.
Kluwer Academic, Dordrecht, 1983.
L. Vila.
I P An instant-period-based theory of time.
In R. Rodriguez, editor, Proc.
ECAI'94 Workshop on Spatial and Temporal Reasoning, 1994.
[23] L. Vila.
On Temporal Representation and Reasonang in Knowledge-based Systems.
PhD Thesis on Computer Science, Dept.
of Computer Languages and Systems, Technical University of Catalonia, C/ Pau Gargallo 5, 08028 Barcelona, 1994.
[24] L. Vila and H. Reichgelt.
The token reification approach t o temporal reasoning.
Artzficzal Inteldzgence, 82, (to appear) 1996.
[25] B. C. Williams.
Doing time: Putting qualitative reasoning on firmer ground.
In Proc.
AAAI'86, pages 105-112,1986.
28  HoLDS&(relay = closed,p) A A OCCURS,t(open(rehy), end(p)) + j HoLDS&(Telay = open),$) A Heets(p,p') HOLDS&(relay = closed,p) A A OCCURS,t(close(relay), end(p)) j HoLDs&(re[ay = closed),p') A Heets(p,p') HOLDS(signal = low, p ) A onA HOLDS,~(QBA2 qz,end(p)) j OCCURS,t(tunz-on( signal), end(p))  +  *
