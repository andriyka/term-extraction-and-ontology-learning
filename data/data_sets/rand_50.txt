Temporal Reasoning with Classes and Instances of Events Paolo Terenziani DISTA, Univ.
del Piemonte Orientale "Amedeo Avogadro" Corso Borsalino 54, Alessandria, Italy Phone: +39 0131 287447 -- terenz@mfn.unipmn.it  Abstract Representing and reasoning with both temporal constraints between classes of events (e.g., between the types of actions needed to achieve a goal) and temporal constraints between instances of events (e.g., between the specific actions being executed) is a ubiquitous task in many areas of computer science, such as planning, workflow, guidelines and protocol management.
The temporal constraints between the classes of events must be inherited by the instances, and the consistency of both types of constraints must be checked.
In this paper, we propose a general-purpose domain-independent knowledge server dealing with these issues.
In particular, we propose a formalism to represent temporal constraints, we show two algorithms to deal with inheritance and to perform temporal consistency checking, and we study the properties of the algorithms.
Keywords: Temporal constraints between classes and instances of events, Inheritance, Consistency, Prediction  1 Introduction The need to represent and reason with classes (i.e., sets of individuals), instances (specific individuals) and inheritance is ubiquitous in many fields of Computer Science (and, in particular, of Artificial Intelligence -AI) and in many applications.
Thus, a lot of works in AI focused on this problem, in order to provide once and for all systems dealing with these phenomena [14].
KL-ONE [3] and KL-ONE-like hybrid knowledge representation systems (henceforth HKRS) are probably the most popular examples of these types of systems (see, e.g., the survey in [11]).
HKRS were usually conceived as task and domain independent knowledge servers, providing other systems and problem solvers with facilities for storing and reasoning with classes, instances and inheritance [9].
This showed to be very advantageous both from the conceptual and from the engineering point of view: instead of having to deal from scratch with classes, instances and inheritance, programmers and knowledge engineers could use a HKRS to this purpose, and focus on the specific problems of their task/application domain.
In HKRS, a terminological component (called T-Box) is used to  represent the description of classes, and an assertional one (A-Box) is used to deal with the instances of the terminological classes.
Classification is used to determine the exact place of each class in the class taxonomy.
Inheritance of properties is supported, as well as integrated reasoning between instances and classes: the realization process [11] determines (considering inheritance of properties and the description of classes and instances) all the most specific classes of which a given assertional entity is an instance.
HKRS are widely and fruitfully applied in different fields of AI and Computer Science (see, e.g., in [15] a survey on some paradigmatic applications).
There is an obvious temporal counterpart to the problem of dealing with classes, instances and inheritance: in many areas, such as planning, workflow management, protocol/guidelines management and so on one usually wants to specify the actions (henceforth, we use the cover term event to denote all types of actions -e.g., agentive or not) needed in order to achieve a given task, and the temporal constraints between them.
Notice that an event in a general plan (or workflow, or protocol, or guideline) represents a class (set) of instances of events, in the sense that it has specific instantiations for specific executions of the plan itself.
On the other hand, while executing (instantiating) a general plan, one has specific instances of the classes of events in the plan, which must inherit the temporal constraints from their super-classes.
Obviously, the instances must respect (i.e., be consistent with) the constraints they inherit from their super-classes.
Moreover, general plans (or workflows, or protocols, or guidelines) may have a predictive role, since they state that a given action has to be executed within a give range of time.
However, surprisingly enough, this temporal counterpart has been quite neglected in the AI literature, in the sense that no general-purpose domainindependent knowledge server for hybrid (i.e., classes plus instances) temporal reasoning has been built (to the best of our knowledge), so that the temporal issues mentioned here have been and are still faced almost from scratch by programmers/researchers in different tasks and applications.
In fact, although since the beginning of the 80's many general purpose knowledge servers (the so called temporal managers) have been built to deal with different types of qualitative and/or  Proceedings of the Ninth International Symposium on Temporal Representation and Reasoning (TIME'02) 1530-1311/02 $17.00 (c) 2002 IEEE  1  quantitative temporal constraints (see, e.g., the surveys in [1,19]), none of them supports an explicit treatment of both classes and instances constraints, with the treatment of inheritance and consistency.
In the paper, we sketch a hybrid temporal approach which overcomes such a limitation providing users with a temporal corresponding of HKRS.
In section 2, we discuss the phenomena to be addressed by an hybrid temporal manager.
In section 3, we introduce two languages to deal with temporal constraints between instances and temporal constraints between classes respectively.
Since the main goal of this paper is that of focusing on the integration of constraints between classes and between instances, we deliberately chose two languages which are based on a well-known constraint framework (i.e., STP [6]), whose properties are well known.
In section 4, we deal with constraint inheritance and hybrid (classes/instances) temporal reasoning to check consistency in case the observations on instances are not complete.
In section 5, we extend consistency checking to the case where observations are compete (i.e., when all the events which actually occurred have been observed).
Finally, in section 6, we further carry on the parallelism between our approach and HKRS approaches, enlightening future research issues in the fields of knowledge representation and temporal reasoning.
2 Temporal constraints between classes and between instances of events 2.1Classes and Instances of Events In the introduction, we sketched the temporal counterpart of the well known dichotomy between classes and instances.
"Classes of events" may correspond to terminological classes (T-Box concepts), and "instances of events" to (A-Box, i.e., assertional) instances.
For example, in a general guideline or plan (e.g., in the clinical field), one may represent the event (action) of "performing a laboratory test".
Such an event stands for a class (of events), since it represents a set of individual occurrences of "performing a laboratory test", taking place at definite intervals of time.
A specific person may execute, at a given time, a specific laboratory test.
This is, of course, an instance event (i.e., a specific occurrence) of the class of events above.
This can be graphically represented as in Figure 1, where LT1, LT2, ... LTk represent specific instances (Instance-of arcs) of the event class "Lab_Tests" occurring at specific intervals of time.
2.2Temporal Constraints Inheritance Usually, general plans (guidelines, protocols, workflows) contain temporal constraints between (classes of) events.
For example, in the CLASSES part of Figure 1, we graphically represent in a simplified way part of a  guideline for the management of laboratory tests in an hospital.
The general guideline represents the fact that the reservation (RS) of each test must be done between 1 and 7 days before the lab-test (LT), and that the results of the tests are reported (RP) within 1 and 48 hours after the end of the test.
Of course, these are temporal constraints between classes of events, which might be instantiated many times, for different instantiations of the classes of events (see the INSTANCES part of Figure 1).
However, it is important to notice that the temporal constraints at the class level are "relational" constraints, in the sense that they have to be inherited only by "corresponding" pair of instances of the related classes.
For example, in Figure 1, the constraint between RS and LT states that each instance of Reservation must be 1-7 days before the corresponding instance of Lab-Test (and not before all LT instances!).
In general, a temporal constraint R between two classes C1 and C2 of events involves an underlying relation pairing instances of the two classes.
This correspondence relation has been recognised, e.g., by [10,16], who called it correlation.
Correlation is symmetric and transitive [10,16].
Its nature depends on the problem and the context.
Even in our simple clinical example, correlation may be specified in different ways1.
Thus, in general, different rules could be devised to infer whether two instances of events are correlated or not, depending on the specific context and domain.
Modelling correlation is outside the goals of this paper.
Further discussions on correlation are in the conclusions and in [10,16].
In the example in Figure 1, we suppose that RSi is correlated to LTi which, in turn, is correlated to RPi, 1<=i<=k.
CLASSES Reservation (RS)  RS1  1-7 days  before  1-48 hours  Lab_Test (LT)  RSk  RS2  LT1  LT2  Report (RP)  before  RP1  LTk RP2  RPk  INSTANCES  Correlation: {<RS1,LT1>,<RS2,LT2>,.., <RSk,LTk>, <LT1,RP1>, <LT2,RP2>, ..., <LTk,RPk>}  temp.
constraints Instance_of  Figure 1 Temporal constraints between classes of events and between instances.
1Even in our simple example, correlation may depend on the level of detail used to describe events, and on assumptions on the specific application.
E.g., an instance R of Reservation may be correlated to an instance L of Lab_Test if (1) both L and R refer to the same patient code and to the same type of test (in case a patient cannot have multiple tests of the same type) (2) both L and R refer to the same patient and to the same type of test in the same date (if a patient cannot have two tests of the same type exactly at the same time) (3) both L and R refer to the same "unique code" (this is the extreme case: the correspondence is given by a code, unique for each execution of a lab test).
Proceedings of the Ninth International Symposium on Temporal Representation and Reasoning (TIME'02) 1530-1311/02 $17.00 (c) 2002 IEEE  2  2.3 Reasoning: consistency checking While most KL-ONE-like approaches support classification and realization [11], in the temporal case one is usually interested in checking the consistency of temporal constraints.
In particular, temporal consistency can be checked on the classes alone, on the instances alone, or to the merge of the constraints on classes and the constraints on instances, (i.e., considering inheritance).
2.4 Reasoning: prediction In KL-ONE-like systems, the descriptions of classes play a predictive role, in the sense that they predict a set of properties and property value restrictions for the instances.
Analogously, in the temporal domain, a plan (or protocol, or guideline) is "predictive", in the sense that, if one has observed a given action E1 which is an instance of a class of events E in a plan, and the class E' follows E in the general plan, one expects to observe an instance of E' in a time consistent with the temporal constraints between the classes of events E and E' in the plan.
In domains where one is certain to have a full and complete observability of events, the consistency check of the temporal constraints must take into account "prediction", since not having observed a given instance of event in a given range of time may indicate an inconsistency.
2.5 Hybrid Temporal Manager To summarize, the goal of the work described in this paper is to propose a general purpose knowledge server (temporal manager) which offers a support for - explicitly representing (i) temporal constraints between classes and instances of events, (ii) instance-of relations, and (iii) correlations -reasoning about inheritance of temporal constraints, and performing consistency checking and prediction.
On the other hand, in this paper we do not deal with the representation of the internal description of events (which are considered as "primitive" entities; see also the discussions in section 6).
3 A hybrid approach to temporal reasoning 3.1 Language for temporal constraints between instances of events (ITL) The basic notion in our temporal ontology are time points.
A time interval I is a convex set of points between a starting (Start(I)) and an ending (End(I)) point.
Different types of temporal constraints can involve instances of events.
Dates locate instances of events in time and can be precise (see Ex.1) or imprecise (Ex.2,3).
(Ex.1) RS1 started on 10/1/98 at 10:00 and ended on 10/1/98 at 10:05  (Ex.2) RS2 started on 10/1/98 at 10:10-10:15 and ended on 10/1/98 at 10:20 (Ex.3) LT1 started on 15/1/98 at 9:00-9:40 and ended on 15/1/98 at 10:00 Dates can be expressed in our language for temporal constraints between instances of events (called ITL) using the predicate date(E,L1,U1,L2,U2), stating that the starting point of E is between L1 and U1 and its ending point is between L2 and U2.
Also durations can be precise or not (e.g., Ex.4) (Ex.4) LT2 lasted at least 1 hour Durations are represented in ITL by the predicate duration(E,L1,U1), stating that L1 and U1 are the minimum and maximum durations of E respectively.
Delays represent (in a precise or imprecise way) the temporal distance between pairs of instances (more precisely, between two of their endpoints; see, e.g., Ex5) (Ex.5) RS2 started 4-5 minutes after the end of RS1 Delays are represented in ITL by the predicate delay(P1,P2,L1,U1), stating that L1 and U1 are the minimum and maximum delay between P1 and P2, where P1 and P2 are endpoints of events.
On the other hand, qualitative temporal constraints do not involve any metric of time, allowing one to deal with the relative position of two instances of events (ex.6).
Currently, ITL considers the qualitative constraints of the Continuous Interval Algebra i.e., the subset of relations of Allen's Interval Algebra which can be mapped onto conjunctions of constraints between points, excluding disequality [19].
We chose such a subset because it has very interesting computational properties, and nevertheless it proved to be very important in many practical applications [5,18]2.
(Ex.6) LT1 was before LT2 E.g., the constraints in (Ex.1-Ex.6) can be represented in ITL as shown by (ITL1).
(ITL1): date(RS1,10/1/98 at 10:00,10/1/98 at 10:00,10/1/98 at 10:05,10/1/98 at 10:05), date(RS2,10/1/98 at 10:10,10/1/98 at 10:15,10/1/98 at 10:20,10/1/98 at 10:20), date(LT1,15/1/98 at 9:00,15/1/98 at 9:40,15/1/98 at 10:00,15/1/98 at 10:00), duration(LT2,1 hour,[?
]), delay(End(RS1),Start(RS2),4 min,5 min), before(LT1,LT2) All the constraints in ITL above can be easily mapped onto distances between time points, or, better into bounds on differences (b.o.d.)
constraints of the general form L<= X - Y<= U where L, U are real numbers where X and Y represent time points and L and U their minimum and maximum temporal distance (i.e., onto the STP framework).
The semantics of ITL predicates can be  2Notice that arbitrary disjunctions of temporal constraints (as in "LT1 was during LT2 or LT2 lasted at least 1 hour") cannot be specified in ITL, as well as some disjunctive relations in Allen's algebra such as "LT1 before or after LT2".
Proceedings of the Ninth International Symposium on Temporal Representation and Reasoning (TIME'02) 1530-1311/02 $17.00 (c) 2002 IEEE  3  specified in terms of b.o.d.
constraints on the distance between points as follows: date(E,L1,U1,L2,U2) = (L1 <= Start(E) - X0 <= U1) L (L2 <= End(E) - X0 <= U2) duration(E,L1,U1) = L1 <= End(E) - Start(E) <= U1 delay(P1,P2,L1,U1) = L1 <= P2 - P1 <= U1 Notice that dates are represented by distances from a reference time point X0 for the whole knowledge base, and that P1 and P2 are starting/ending points of events.
As examples of qualitative relations, let us consider before and during between two time intervals E1 and E2 before(E1,E2) = 0 < Start(E2) - End(E1) during(E1,E2) = (0 < Start(E1) - Start(E2)) L (0 < End(E2) - End(E1)) Bounds on differences (and the STP framework) have been widely used in the AI literature in order to represent and reason with temporal constraints (consider, e.g., [5,6,7]).
Correct and complete reasoning on b.o.d.
can be performed efficiently using an all-to-all shortest path algorithm which provides an inconsistency or the upper and lower bounds for the distance between each pair of time points (also called minimal network), and which operates in a time that is cubic in the number of time points [6].
A simple test in the all-shortest-path algorithm allows it to detect inconsistencies, at no additional cost [6].
For example, reasoning on the b.o.d.
corresponding to (ITL1) finds their consistency and infers that, e.g., RS2 started at 10:10 and LT2 started after 15/1/98 at 10:00.
The temporal high-level language we described until now is very similar to the ones of many STP-based temporal managers in the AI literature (see, e.g., [1,5,6,19]).
In order to be able to integrate temporal constraints between classes and between instances, we must extend ITL.
We introduce the predicate Instance_of(E1,C1) to state that E1 is a specific instance of the class of events C1.
In the following, we suppose that we have the classes in Figure 1, and to have observed only the instances RS1 and RS2 (of Reservation) and LT1, LT2 (of Lab_Tests).
The class/instance relations can be represented in ITL as shown by (ITL2): (ITL2): Instance_of(RS1, Reservation), Instance_of(RS2, Reservation), Instance_of(LT1, Lab_Tests), Instance_of(LT2, Lab_Tests) Predicate COR is introduced to represent correlations between instances of events.
In our example, we assume (as in Figure 1) that LT1 is correlated to RS1 and LT2 is correlated to RS2.
This can be expressed in ITL by (ITL3): COR(RS1,LT1), COR(RS2,LT2) Finally, it is useful to indicate the set IKB_Elements of all the instances e.g., as shown in (ITL4) (ITL4): {RS1,RS2,LT1,LT2} In ITL, a Knowledge Base of temporal constraints between instances (IKB for short) is a quadruple  <IKB_Elements, IKB_Instance_of, IKB_COR, IKB_Constraints>, where IKB_Elements is a set of instances of events, IKB_Instance_of is a set of Instance_of assertions, IKB_COR a set of correlations and IKB_Constraints a set of temporal constraints on instances of events.
In our example, we have IKB = <ITL4,ITL2,ITL3,ITL1>.
Axioms (Ax1) and (Ax2) (and the logical formulae in section 3.2) are introduced to make explicit our intended semantics of an IKB: IKB is a representation of the instances of events which have been observed until NOW (where NOW is the system time when a call to the temporal manager is done).
(Ax1) states that if one instance x of event has been observed (i.e., x[?
]IKB_Elements), then it has been observed to start before (or equal to) NOW.
Of course, the temporal reasoning algorithms we describe in the following sections have to respect such a semantics (in other words, they can be seen as a procedural implementation of such a semantics).
(Ax1) [?
]x x[?
]IKB_Elements = Start(x) - NOW <= 0 If we hypothesize that observations are complete, the fact that an instance x of event has not been observed (i.e., x[?]
IKB_Elements) implies that it did not start until NOW (see Axiom Ax2).
(Ax2) [?
]x x[?
]IKB_Elements= NOT(Start(x) - NOW <= 0)  3.2 Language for temporal constraints between classes of events (CTL) In general, all the types of temporal constraints discussed above can also be expressed between classes of events.
For instance, considering again the example in Figure 1, one could assert the following temporal constraints: (Ex.7) Laboratory tests are made between 1 and 7 days after the reservation (Ex.8) Laboratory tests last between 30 minutes and 48 hours (Ex.9) Results are reported between 1 and 48 hours after the end of the tests (Ex.10) Results are reported after the tests Thus, we used the same predicates as above to express them into our temporal language for classes of events (CTL for short).
Ex.7-Ex.10 are represented in CTL as follows: (CTL1): Cdelay(End(Reservation),Start(Lab_Tests),1day, 7day), Cduration(Lab_Tests, 30 min, 48 hour), Cdelay(End(Lab_Tests),Start(Report), 1 hour, 48 hour), Cafter(Report,Lab_Tests) The predicates on classes are basically the same as for instances (we put the prefix C to distinguish them); however, when applied to classes, durations, delays (here we consider just the delays between the starting points of two classes; the other cases are analogous) and qualitative relations have a different meaning, as shown below.
Proceedings of the Ninth International Symposium on Temporal Representation and Reasoning (TIME'02) 1530-1311/02 $17.00 (c) 2002 IEEE  4  Cduration(C,L1,U1) = [?]
E Instance_of(E,C) = L1 <= End(E)- Start(E) <= U1 Cdelay(Start(C1),Start(C2),L1,U1) = ([?]
C1',C2' (Instance_of(C1',C1) L Instance_of(C2',C2) L COR(C1',C2')) = (L1 <= Start(C2') - Start(C1') <= U1) L ([?]
C1' Instance_of(C1',C1) = ([?]
C2' Instance_of(C2',C2) L COR(C1',C2')))) As example of qualitative relations, let us consider the relation "before": Cbefore(C1,C2) = ([?]
C1',C2' (Instance_of(C1',C1) L Instance_of(C2',C2) L COR(C1',C2')) = (0 < Start(C2') - End(C1')) L ([?]
C1' Instance_of(C1',C1) = ([?]
C2' Instance_of(C2',C2) L COR(C1',C2')))) While durations are simply inherited by all instances, qualitative relations and delays are only inherited by correlated pairs of instances (see section 2).
The second conjuncts in the definition of Cdelay and Cbefore formalize the "predictive" character of delays and qualitative relations between classes of events.
For example, given the constraint between classes Cbefore(C1,C2), the observation of an instance of C1 implies the later occurrence of a correlated instance of C2.
Finally, the predicate EventClass is introduced in CTL in order to declare the classes of events being considered.
Thus, in the example in Figure 1, we would have (CTL2) below (CTL2): EventClass(Reservation), EventClass(Lab_Tests), EventClass(Report) Thus, in our language CTL, a KB of temporal constraints between classes of events (CKB for short) can be defined as a pair <CKB_EventClass, CKB_Constraints> (<CTL2,CTL1> in our example).
4 Hybrid consistency prediction)  checking  (no  If one has only constraints between classes, the fact that they are classes is irrelevant from the point of view of temporal reasoning; they can be interpreted as primitive (individual) events and standard temporal reasoning can be performed on them (see, e.g., [2] as regards temporal constraints in general plans).
On the other hand, hybrid temporal reasoning takes in input a KB of temporal constraints between classes and a KB of temporal constraints between instances, and gives as output the upper and lower bounds on the distance between each pair of starting and ending points of instances (i.e., the minimal network) or an inconsistency.
The procedure Integrated_Reasoning in Figure 2 deals with the case (common in many applications) in which observations are incomplete, i.e., instances of events can occur and not be observed (i.e., not be present in the IKB).
Before performing hybrid temporal reasoning, temporal constraints in the high-level language are translated into the corresponding b.o.d.
constraints (steps (1) and (2)).
In step (3), Set_NOW updates the constraints in IKB_Constraints adding the constraint represented in Axiom (Ax1)3.
Then, temporal reasoning is performed separately on instances and on classes (using the allshortest-path algorithm on b.o.d.
constraints [6], called here Temporal_reasoning) to check whether each one of them is independently consistent and to infer the implied temporal constraints separately (see steps (4) and (5); let BOD_IKB_Con' and BOD_CKB_Con' the resulting sets of constraints).
Step (6) performs the transitive closure of correlation relations.
The rest of the procedure deals with the integration of the two levels of constraints.
The basic idea is that of inheriting (accordingly with the semantics specified is section 3.2) the temporal constraints between classes on the instances of events, and then performing temporal reasoning on instances (applying again the allshortest-path algorithm) on the union of the inherited plus the instances constraints.
Step (7) implements the inheritance of durations of events.
All distances t <= End(E) - Start(E) <= u between the ending point and the starting point of an event class E must be inherited by all the instances of the class.
Thus, they are added to the constraints in BOD_IKB_Con'.
For each pair of correlated instances E1 and E2, Step (8) deals with the inheritance of qualitative relations and delays from the corresponding classes of events.
Finally, step (9) performs integrated reasoning at the level of instances, considering also the constraints inherited from the classes of events.
The procedure stops reporting an inconsistency if a call to Temporal reasoning (steps 4, 5, and 9) finds it.
Procedure Integrated_Reasoning (<CKB_EventClass, CKB_Constraints>, <IKB_Elements,IKB_Instance_of, IKB_COR, IKB_Constraints>) (1) BOD_IKB_Con := Transform(IKB_Constraints); (2) BOD_CKB_Con := Transform(CKB_Constraints); (3) BOD_IKB_Con := Set_NOW(BOD_IKB_Con,NOW); (4) BOD_IKB_Con' := Temporal_reasoning(BOD_IKB_Con); (5) BOD_CKB_Con':= Temporal_reasoning(BOD_CKB_Con); (6) IKB_COR := Closure(IKB_COR); (7) Forall C1 \ EventClass(C1) [?]
CKB_EventClass L t <= End(C1) - Start(C1) <= u [?]
BOD_CKB_Con' do Forall E \ Instance_of(E,C1) [?]
IKB_Instance_of do BOD_IKB_Con':= BOD_IKB_Con' [?]
{t <= End(E) - Start(E) <= u} od od;  3In our example, we have observed (the beginning of) LT2, but there is only the constraint before(LT1,LT2) concerning LT2 in IKB.
Thus, in the mapping on b.o.d., we have Start(LT2) - X0 < [?
], and the effect of Set_NOW is to change this constraint into Start(LT2) - X0 <= NOW.
Proceedings of the Ninth International Symposium on Temporal Representation and Reasoning (TIME'02) 1530-1311/02 $17.00 (c) 2002 IEEE  5  (8) Forall E1,E2 [?]
IKB_Elements, E1[?
]E2\ COR(E1,E2) Let C1[?]
CKB_EventClass and C1[?]
CKB_EventClass the corresponding classes /* i.e., Instance_of(E1,C1) and Instance_of(E2,C2) hold */ Istantiate on E1 and E2 the constraints in CKB_Constraints between C1 and C2 (9) Minimal_Network := Temporal_reasoning(BOD_IKB_Con'); Figure 2.
Procedure Integrated_Reasoning  For example, let us apply Integrated_Reasoning to <CTL2,CTL1> and <ITL4,ITL2,ITL3,ITL1> described above, supposing that NOW=18/1/98 at 18:00.
Step (7) inherits the constraints on the duration of LT1 and LT2 (which must last between 30 minutes and 48 hours).
Step (8) inherits the delay of 1-7 days between correlated pairs of instances of Reservation and Lab_Tests.
In the example, and taking minutes as the basic granularity, this corresponds to adding the constraints 30 <= End(LT1)Start(LT1) <= 2880, 30 <= End(LT2)-Start(LT2) <= 2880, 1440 <= Start(LT1)-End(RS1) <= 10080, and 1440 <= Start(LT2)-End(RS2) <= 10080 into the temporal constraints between instances of events.
The final application of Temporal_reasoning does not detect any inconsistency and provides, among the others, the constraints that: LT1 starts on 15/1/98 at 9-9:30; LT2 starts between 15/1/98 at 10:00 and 17/1/98 at 10:20 and ends between 15/1/98 at 11:00 and 19/1/98 at 10:20.
More generally, the following property holds: Property 1 The procedure Integrated_Reasoning is correct with respect to the logical semantics of the temporal language we introduced in subsections 3.1 and 3.2.
Proof (Sketch) The proof is based on the fact that all and only the temporal constraints specified by the semantics of the constructs in CTL (Cduration, Cdelay etc.)
are inherited at the level of instances of events (steps (7) and (8)), and then correct and complete temporal reasoning is performed at the level of instances of events via the all-toall shortest path algorithm (step 9).
Given the proof sketched above, Integrated_Reasoning is complete as regards consistency checking on the classes in CKB plus the instances in IKB.
However, it does not consider the "predictive" part in the logical semantics of delays and qualitative relations between classes, since it does not add the predicted (correlated) instances into the IKB.
However, this is reasonable in many applications.
For example, in all the applications where observations are incomplete (i.e., where Axiom Ax2 does not hold), prediction has no impact on consistency checking.
In fact, even if the predicted events should have occurred in the past (i.e., before NOW), not having them in the IKB does not imply  an inconsistency: maybe they occurred and were not observed (inserted in the IKB).
Thus, Property 2 holds: Property 2 In the case of incomplete observations, the procedure Integrated_Reasoning checks consistency in a correct and complete way with respect to the logical semantics of the temporal language.
5 Hybrid consistency checking (complete observations) In many applications, the "predictory" part of temporal constraints between classes must be considered.
For example, in applications where one can hypothesize that observations are complete (i.e., Axiom Ax2 holds), "prediction" must be used to detect inconsistency.
In fact, in such a case, the absence of the observation of an instance of an event which, according to the constraints among classes, should have already happened (and be observed), gives an inconsistency.
This can be coped with as in Procedure Integrated_Predictory_Reasoning in Figure 3.
The procedure first calls Integrated_Reasoning (step 1) and then consider "predictions".
In the procedure, we denote by CKB_Connected(C) the set of all classes in CKB that can be reached (directly or indirectly) from the class C via some temporal constraint (a delay or a qualitative relation; i.e., CKB_Connected(C) represent the set of classes correlated to C).
This can be easily computed a-priori by navigating the graph of constraints between classes (see Figure 1).
Step (2) implements the "prediction" of new instances.
For each instance E in IKB, it considers all the classes in CKB_Connected(CE) which are connected to the class CE of which E is an instance.
For each one of these classes (say C), it looks whether there is an instance of C which is correlated to E in IKB.
If there is not, in step (2.1.1) Add_Instance(C, IKB_Elements, IKB_Instance_of, IKB_COR,E) returns a new instance I' of C and inserts: I' into IKB_Elements, the relation Instance_of(I',C) into IKB_Instance_of, and COR(E,I') in IKB_COR; this amounts to create a new instance I' of C correlated to E, according to the "prediction" part of the semantics of constraints between classes.
Then, in step (2.1.3), the transitive closure of COR is computed, and in 2.1.4 Inh_constr_inst is invoked to consider all the constraints concerning (the starting and ending points of) C in CKB, and to let I' inherit them (inheritance here is analogous to steps (7) and (8) of the procedure Integrated_Reasoning).
Step (3) executes temporal reasoning on the resulting set of constraints.
Finally, step (4), for each one of the new instances I introduced into the IKB (the instances in NEW_INST), checks whether the resulting constraints in the IKB imply that I should have started necessarily before NOW.
In such a case, an inconsistency is reported.
NEC(KB,test) checks whether test is necessarily true given the constraints in KB (i.e., if test is logically implied by KB;  Proceedings of the Ninth International Symposium on Temporal Representation and Reasoning (TIME'02) 1530-1311/02 $17.00 (c) 2002 IEEE  6  considering the minimal network of a KB of bounds on differences constraints, this can be done in a time linear in the time points in test [4]).
Procedure Integrated_Predictory_Reasoning (<CKB_EventClass, CKB_Constraints>, <IKB_Elements,IKB_Instance_of, IKB_COR, IKB_Constraints>) (0) NEW_INST := [?
]; (1) Integrated_Reasoning (CKB,IKB); (2) Forall E [?]
IKB_Elements do Let CE [?]
CKB_EventClass the class such that Instance_of(E,CE) [?]
IKB_Instance_of Let CKB_Connected(CE) the set of all classes in CKB_EventClass connected to CE via temporal constraints (2.1) Forall C [?]
CKB_Connected(CE) do if NOT (Exists E' such that Instance_of(E',C) [?]
IKB_Instance_of L COR(E',E) [?]
IKB_COR) then begin (2.1.1) I' := Add_Instance(C, IKB_Elements, IKB_Instance_of, IKB_COR, E); (2.1.2) NEW_INST := NEW_INST [?]
{I'}; (2.1.3) IKB_COR := Closure(IKB_COR); Forall bod [?]
BOD_CKB_Con concerning C (2.1.4) do Inh_constr_inst(bod,BOD_IKB_Con') od end od od; (3) Minimal_Network := Temporal_Reasoning(BOD_IKB_Con'); (4) Forall I [?]
NEW_INST do If NEC(BOD_IKB_Con', Before(start(I),NOW)) then INCONSISTENT; od; Figure 3.
Procedure Integrated_Predictory_Reasoning Let us consider again our example.
The procedure above inserts two instances RP1 and RP2 of Report into the IKB.
RP1 is correlated to RS1 and LT1, and RP2 to RS2 and LT2.
Considering the inherited temporal constraints, we infer that RP1 should start between 1 and 48 hours after the end of LT1, i.e., between 15/1/98 at 11:00 and 17/1/98 at 10:00, and RP2 should start between 15/1/98 at 12:00 and 21/1/98 at 10:20.
In particular, the starting point of RP1 must be between 15/1/98 at 11:00 and 17/1/98 at 10:00 and thus it is necessarily before NOW (18/1/98 at 18:00 in our example).
Thus, an inconsistency is detected.
In general, Property 3 holds: Property 3 The procedure Integrated_Predictory_Reasoning checks consistency in a correct and complete way with respect to the logical semantics of the temporal language we introduced in subsections 3.1 and 3.2.
Proof (Sketch) The proof is based on the fact that Integrated_Reasoning is correct, and its incompleteness is  only due to the fact that it does not consider the "predictive" part of the semantics of constraints between classes, which is dealt with by step (2) of Integrated_Predictory_Reasoning.
Then correct and complete temporal reasoning is performed at the level of instances of events via the all-to-all shortest path algorithm (step 3).
Finally, step 4 is needed to force the fact that observations are complete (see Ax2), checking whether some predicted instance should have been observed necessarily before now.
6 Conclusions and Developments In planning, workflows, guidelines, protocols and so on, checking whether the temporal constraints in a general plan (protocol, guideline, workflow) are respected by the plan (protocol, guideline, workflow) instantiation is a fundamental task.
Such a task involves integrated temporal reasoning considering both the temporal constraints between the classes of events and the (observed) temporal constraints between their instances.
The approach in this paper is, to the best of our knowledge, the first one proposing a general-purpose and domain-independent knowledge server supporting such a task, thus providing a temporal corresponding of HKRS systems.
We think that the parallel between our approach and HKRS systems [11] could give rise to new interesting topics of research in temporal reasoning.
For example, a main issue in HKRS concerns the relation between the expressiveness of the terminological and the assertional components [11], ranging from KL-ONE [3], in which the terminological component is very powerful and expressive and the assertional one very limited, to BACK [12], where the two components are balanced from both the expressive and computational point of view.
Considering this issue, our approach is close to a balanced BACK-like approach.
However, as in the research about HKRS's, a lot of work should be done in order to extend the classes and/or the instances languages and the temporal reasoning features (for the sake of simplicity, we currently adopted very simple STP-based languages, whose expressive limitations are well known within the temporal reasoning community), and considering the trade-off between expressive power and the complexity of (complete) reasoning.
Moreover, some HKRS have been extended with an Inferential Box, containing formulae or rules operating on the assertional component (consider, e.g., the I-Box in BACK [13]).
Analogously, in our approach, one could introduce a further component, which manages the (domain and application dependent) rules which specify the correlation relations between instances of events.
Finally, the integration of our approach with a classical HKRS (e.g., BACK) to represent the internal description of classes and instances of events (using concepts and roles) and to exploit the classification and realization facilities would be interesting.
In such an extended approach, one could use (in the I-Box rules) the  Proceedings of the Ninth International Symposium on Temporal Representation and Reasoning (TIME'02) 1530-1311/02 $17.00 (c) 2002 IEEE  7  descriptions of the classes of events in T-Box in order to infer, e.g., correlation4.
Finally, it would be interesting to extend the approach in this paper in order to cope with cases where a one-toone correspondence between events cannot be assumed (e.g., coming back to the example in Figure 1, where the same reservation can be used for more than one laboratory test).
Furthermore, we think that our approach is suitable to be extended in order to use constraints between classes as basic knowledge to be evaluated apriori, and to be used to check consistency in an incremental way whenever new (constraints on) instances are added (e.g., to deal with least commitment temporal planning).
To conclude, we conceive our hybrid temporal reasoner as a domain and task independent knowledge server to be loosely coupled with other systems and problem solvers to deal with different problems in different areas (following the lines which have been pointed out by many applications of HKRS [11,15] and e.g., by [1,5] as regards applications of temporal managers dealing with instances only).
Currently, we are studying to loosely couple our hybrid temporal manager with GLARE, a system for managing clinical guidelines we developed in cooperation with the physicians of Azienda Ospedaliera S. Giovanni Battista of Torino, Italy [8,17].
References [1] J. Allen, "Time and Time again: the Many Ways to Represent Time", Int'l J.
Intelligent Systems, vol.
6, no.
4, pp.
341-355, July 1991.
[2] J. Allen, "Planning as Temporal Reasoning", Proc.
KR91, 3-14, 1991.
[3] R. Brachman, and J. Schmolze, "An Overview of the KL-ONE Knowledge Representation System", Cognitive Science, vol.
9, No.
2, pp.
171-216, AprilJune 1985.
[4] V. Brusoni, L. Console, and P. Terenziani.
"On the computational complexity of querying bounds on differences constraints", Artificial Intelligence 74(2):367-379, 1995.
[5] V. Brusoni, L. Console, B. Pernici, P. Terenziani, "LaTeR: Managing Temporal Information Efficiently", IEEE Expert 12(4), 56-64, 1997.
[6] R. Dechter, I. Meiri, J. Pearl, "Temporal Constraint Networks", Artificial Intelligence 49, 61-95, 1991.
[7] E. Davis, "Constraint Propagation with Interval Labels", Artificial Intelligence 32, 281-331, 1987.
[8] A. Guarnero, M. Marzuoli, G. Molino, P. Terenziani, M. Torchio, K. Vanni, "Contextual and Temporal Clinical Guidelines", Journal of the American Medical Informatics Association, 683687, 1998.
[9] H.J.
Levesque and R.J. Brachman, "Expressiveness and Tractability in Knowledge Representation and Reasoning", Computational Intelligence 3, 78-93, 1987.
[10] R.A. Morris, W.D.
Shoaff, and L. Khatib, "Path Consistency in a Network of Non-convex Intervals", Proc.
thirteenth Int'l Joint Conf.
on Artificial Intelligence, pp.
655-660, Chambery, France, 1993.
[11] B.Nebel, Reasoning and Revision in Hybrid Representation Systems, LNCS 422, SpringerVerlag, 1990.
[12] B. Nebel and K. von Luck, "Hybrid Reasoning in BACK", In Z.W.
Ras and L. Saitta eds., Methodologies for Intelligent Systems 3, North Holland, 260-269, 1988.
[13] J. Quantz and C. Kindermann, "Implementation of the BACK System Version 4", KIT-REPORT 78, Technische Universitat Berlin, December 1990.
[14] E. Rich, K. Knight, Artificial Intelligence, McGraw Hill, 1991.
[15] J. Schmolze, W.Mark "The NIKL Experience", Computational Intelligence 6, 48-69, 1991 [16] P. Terenziani, "Integrating calendar-dates and qualitative temporal constraints in the treatment of periodic events", IEEE Trans.
on Knowledge and Data Engineering 9(5), 1997.
[17] P. Terenziani, G. Molino, and M. Torchio, "A modular approach for representing and executing clinical guidelines", Artificial Intelligence in Medicine 23, 249-276, 2001.
[18] P. VanBeek, "Temporal Query Processing with Indefinite Information", Artificial Intelligence in Medicine, 3(6), 325-339, 1991.
[19] L.Vila, "A Survey on Temporal Reasoning in Artificial Intelligence", AI Communications 7(1), 428, 1994.
4Considering for example footnote 1, the rule (2) of correlation could be implemented in the following way.
One could describe Reservation (RS for short) and Lab-tests (LT) giving them (among the others) the attributes (slots) patient code (PC), type of test code (TTC) and date of the examination (EXAM_DATE), and insert in I-Box a rule such as [?]
I1,I2 Instance_of(I1,RS) L Instance_of(I2,LT) L I1.PC=I2.PC L I1.TTC=I2.TTC L I1.EXAM_DATE = I2.EXAM_DATE = COR(I1,I2) Proceedings of the Ninth International Symposium on Temporal Representation and Reasoning (TIME'02) 1530-1311/02 $17.00 (c) 2002 IEEE  8
Generating Explanations with the Help of Temporal Constraints Margo Guertin  Computer Science Dept., Boston University, Boston, MA.
02215 guertin@cs.bu.edu  Abstract This paper describes Holmes, a new system for generating explanations for cyclic events using temporal constraints.
The domain is electrical conduction in the heart, with the temporal constraints coming from the trace of an electrocardiogram (ECG), and an explanation being a laddergram | a diagram of the paths of the beat impulses through the heart which could have produced that ECG.
I have combined a method for propagating temporal constraints with an object-oriented model of heart conduction, and it has resulted in a remarkably quick and ecient search of the vast space of possible explanations.
This type of system, which combines temporal reasoning and abductive inference, may well point the way to much fruitful research in the future.
1 Introduction  My original motivation for developing the Holmes system was to build a program which could use ECG information to automatically trace the paths of beats through the heart.
This is something which is currently done by hand when a doctor suspects heart conduction problems.
As I worked on the system, I realized it was also an interesting and promising hybrid of temporal reasoning and abductive inference.
It is this aspect I concentrate on here.
Abduction, the inference of the best explanation for the observed evidence, is a small but steadily growing 	eld of AI research.
A clear and thorough discussion of it appears in John and Susan Josephson's recent book on abductive inference 3], in which they describe past and current systems for doing automated abduction.
All these systems must deal with the problem of an enormous search space of explanations which might account for a body of evidence.
Since there are far too many possible hypotheses to look at them all, much eort has been expended on ways of directing the search towards the hypotheses most likely to produce the \best" explanations.
Josephson points out that a system can avoid these problems of search complexity if it has enough of the right kind of information and an eective strategy for using it.
2] I believe that the temporal intervals which represent the start, stop and duration times of events in the Holmes system are an example of this right kind of information.
Almost all of the vast search space of hypotheses are ruled out quickly by a form of quantitative temporal constraint propagation, based on Isaac Kohane's Temporal Utility Package (TUP)4], a domain independent utility which makes use of interval arithmetic to propagate quantitative temporal constraints.
TUP, in turn, is based upon upon James F. Allen's interval-based logic 1].
In my domain, a hypothesis is a chronologically ordered list of events, each of which may account for one of the \bumps" on the ECG.
For example, the event of the left ventricle of the heart being activated by an impulse from the adjacent left bundle branch at time could be used to explain an ECG spike (or QRS wave) at time .
An explanation is a causal network of events which can account for all the ECG evidence.
Each attempted explanation is generated incrementally and in order, incorporating one event at a time, beginning at the time of the 	rst piece of evidence and ending with the last piece of evidence, until an inconsistency is detected or an explanation has been successfully completed.
Any inconsistency which arises is either because an event necessary to the particular explanation being attempted is inconsistent with the evidence or with some event(s) previously incorporated into it.
Because events are incorporated in chronological order, failure can only arise from a conict with the evidence or previous events already part of the explanation being formed.
This means that if an ordered hypothesis of events A, B, C, D, E, F, fails on the attempt to incorporate D, then all hypotheses whose 	rst four events are A, B, C, and D can be ruled out.
In a search space where each hypothesis consists of many events, a failure that occurs early in the hypothesis list allows a shallow cuto in the search tree, eliminating a significant number of hypotheses from consideration.
One is particularly likely to encounter this kind of ecient pruning in a cyclic domain where the pieces of evidence are often repetitive.
In such a domain, the x  x  :::  chances are high that any inconsistency which may occur near the end of a list of hypothesized events, is a repeat of the same inconsistency occuring much earlier in the list, where the pruning is more drastic.
Thus, in the domain of heart conduction, it is possible to take full advantage of the cyclic temporal information available, propagate temporal constraints to rule out almost all of the possible explanations early, and consider only those remaining.
This ability to generate all the explanations would be important in any scenario where more evidence might be gathered later which could rule out what was formerly rated the \best" explanation.
It would also be important to any system meant to discover new explanations, not normally considered.
In the following sections, I discuss the domain of heart conduction in more detail, give an overview of its control with a brief example of how it uses temporal constraint propagation, and 	nally give my preliminary statistics on search eciency from generating explanations for 	ve dierent (normal) ECGs.
2 The Heart Conduction Domain  The information on heart physiology and electrocardiography below comes primarily from three sources 5, 6, 7].
The heart can by thought of as a 	st-sized electric pump for moving blood through the body (see Fig.
1) .
When all is going well, the sino-atrial SA node Atria AV node & Bundle of His Right Bundle Branch  Left Bundle Branch Right Ventricle  atrioventricular node (AV node) and the \Bundle of His", and the current travels through them to the left and right bundle branches.
From the bundle branches it travels into the ventricles and the septum between them, normally activating them all simultaneously.
As the current travels through the ventricles, they contract powerfully, forcing the blood just received from the atria on out into the body.
When any part of the heart is activated, it begins to depolarize , which has the eect of spreading the current throughout, and soon to adjacent parts.
Once the process of depolarization is complete, the longer process of repolarization begins.
This can be thought of as a recovery process, during which the heart part cannot be activated by any impulse.
Once the repolarization process is complete, the part is in its initial resting state, ready to receive and carry an impulse comes along.
Each heart part can be thought of as a little 	nite state machine (FSM), and the whole heart as a bunch of parallel, interacting FSMs.
In the Holmes heart model, an event is de	ned to be the activation of one heart part, either by an adjacent part, or occasionally, by itself.
(It is possible for any heart part to activate itself by producing its own electrical impulse spontaneously, although normally this only happens in the SA node.)
The representation of an event in the model includes temporal interval parameters for the start of its depolarization, duration of depolarization, start of repolarization, duration of repolarization, and start of its rest state.
The electrocardiogram (see Fig.
2) is a trace of the heart's electrical activity as measured from dierent points on the surface of the body.
It provides incomplete evidence of the impulse paths through the heart, because some heart parts produce a current strong enough to be measured from the outside, while others do not.
The atria depolarizing give rise to the P wave, the ventricles depolarizing produce the QRS complex, and the T wave is the result of the ventricles repolarizing.
All other events are invisible to the ECG and must be inferred by Holmes.
Left Ventricle  QRS  QRS T  P  T P  Septum  Figure 1: Diagram of the main parts of the heart node (SA node) at the top generates a steady electrical pulse which is conducted down through the other parts of the heart.
The SA node's current 	rst activates the upper chambers of the heart (the atria), which contract and squeeze the blood into the lower chambers (the left and right ventricles).
Very soon after the SA node activates the atria, it activates the  Figure 2: Two heartbeats in a typical ECG A hypothesis is a list of assumed events, each of which could account for one bump on the ECG.
In Fig.
2, for instance, each P wave could be due to the atria being activated by the SA node, or the atria activating themselves spontaneously.
Each QRS could be the left ventricle being activated by the left bun-  dle branch, the left ventricle being activated by the septum, or the left ventricle activating itself spontaneously.
So, even for a 2-beat ECG, we would have 2 3 2 3, or 36 4-event hypotheses.
In this domain, an explanation is a complete causal network of events which could have resulted in the ECG.
Following is an example of a possible explanation for a P wave, followed by a QRS complex, followed by a T wave.
The SA node generated a beat which activated the atria and the AV node and Bundle of His.
Then the Bundle of His activated the left and right bundle branches, and soon after, the left bundle branch activated the left ventricle and septum, while the right bundle branch activated the right ventricle.
Any explanatory network must be internally consistent, and include all the assumed events of the hypothesis, as well as all invisible events (within the ECG's time span) which either lead to or resulted from the hypothesis events.
All events in the explanation must, of course, be consistent with the ECG evidence.
3 The Holmes System 3.1 Control  The Holmes system, shown in Fig.
3, contains a general model of the heart which is individualized with information from the ECG.
For example, normal heart rates range anywhere from 55 beats per minute to 120 beats per minute, but if the ECG shows a normal heart rate, in this case varying from, say, 80 to 95, the interval range for the heart rate parameter is restricted accordingly.
As many parameters as possible are tightened using the ECG information.
The hypothesis former, a set of rules for forming all possible hypotheses from an ECG, swings into action next, producing a very large number of hypothesis lists which go into the search space.
(In the case of an apparently normal -beat ECG, 6n lists of length 2 are produced.)
From this point on, all hypotheses in the search space (which have not been pruned away) are fed to the explanation, one at a time, until the search space is exhausted.
The explanation developer attempts to form all explanations possible for each hypothesis it receives.
(There can be several explanations for the same hypothesis because the hypothesis is a list of only visible events, and there can be a number of unique causal networks of events which include the same set of visible events.)
For each attempted explanation, the explanation developer starts a chronologically ordered list of events to be incorporated, which initially contains only the events of the hypothesis, but is expanded, with each event successfully incorporated, to include all events which would result from it.
Each event incorporated must be consistent with the ECG and with the events already incorporated in the explanation.
When it is added, any constraints it imposes are propagated back through the model.
If failn  n  ure occurs, the explanation is abandoned and its failure point noted.
All successful explanations resulting from the hypothesis are put into the pool of completed explanations.
If all attempts to explain the current hypothesis fail, the latest hypothesis event responsible for failure is noted and sent on to the search space pruner, which prunes the search space accordingly.
3.2 Simplied Example of Temporal Constraint Propagation  Following is a simpli	ed example of constraint propagation through a very small causal network of four events | the activations of four adjacent heart parts, say, A, B, C, and D. It is exactly this kind of thing which happens, over and over, as Holmes tries to develop an explanation, and each event being incorporated in the explanation is checked for internal consistency with those already there.
Given: 1.
The activations of heart parts A, B, and C do not show up on the ECG the activation of part D does.
2.
It has been inferred from the model and the assumptions of the current hypothesis that the activation of part A occurred some time between 0 and 20 msec.
We represent this by the interval (0, 20).
3.
Part A is adjacent to part B.
4.
It takes (20, 40) for a current to travel from A to B.
5.
Part B is adjacent to parts C and D (as well as A).
6.
It takes (5, 15) for a current to travel from B to C. 7.
It takes (10, 20) for a current to travel from B to D. So initially: 1.
A is activated at (0, 20).
2.
B is activated at (20, 60).
3.
C is activated at (25, 75).
4.
D is activated at (30, 80).
But then we see from the ECG that part D was activated at 45 (or (45, 45) in interval notation).
This restricted start time for the activation of D constrains the start of B, which in turn constrains the starts of A and C. Using interval arithmetic 4], we end up with the following constrained information: 1.
A is activated at (0, 15).
2.
B is activated at (25, 35).
3.
C is activated at (30, 50).
4.
D is activated at (45, 45).
ECG (Evidence)  General Heart Model  Hypothesis Former  Parameter Learner  Search Space of Hypotheses  Explanation Developer  Partial Explanation Search Space Pruner  Individualized Heart Model  Constraint Propagator  failure  success Completed Explanations  Figure 3: Architecture of the Holmes System  4 Results and Conclusions  cyclic domains of medical and environmental science.
At this point in time, the Holmes system is fully operational for normal (i.e.
healthy) ECGs, and I have run it successfully on 	ve such ECGs so far.
I am now in the process of 	ne tuning the general heart model to allow it to handle abnormal ECGs as well, since one of my main motivations in developing this system was to make a physicians' tool for diagnosing heart conduction problems.
I have gotten good results with the 	ve ECGs I have tried.
For each, Holmes has come up with the simplest explanation (that of a normal sinus rhythm with no aberrations).
For two of them, it has also found some more | for ECG #1, three other explanations, and for ECG #5, two others.
While these alternative explanations are much less likely, it is quite interesting to be able to see what aberrant events could could be occuring in a heart's conduction system, when a normal-looking ECG trace is being produced.
The eciency of performing a complete search is very high.
I measure eciency with the ratio of the number of hypotheses which need to be examined in a complete search to the search space size (about 6n for an -beat ECG).
In my 	rst 	ve runs, the eciency ratio is 34 : 610, in the least ecient search, and 135 : 617, in the most ecient.
I think that these results are due, in large part, to the pruning enabled by temporal constraint propagation, and are further improved by the cyclic nature of the domain.
I hope they will encourage more researchers to incorporate temporal information and constraint propagation techniques into their models, and to start work on some of the many unexplored  Acknowledgments  n  This paper has bene	tted greatly from the insightful comments and questions of John Josephson and Michael Weintraub on an earlier draft.
I am also very grateful to Bipin Indurkya and Scott O'Hara for their time and thoughtful criticism throughout its development.
References 1] James F. Allen.
Maintaining knowledge about temporal intervals.
Communications of the ACM, 26(11):832{843, 1983.
2] John R. Josephson.
Personal communication.
3] John R. Josephson and Susan G. Josephson.
Abductive Inference: Computation, Philosophy, Technology.
Cambridge University Press, 1994.
4] Isaac S. Kohane.
Temporal Reasoning in Medical Expert Systems.
PhD thesis, Boston University, 1987.
5] Henry J. L. Marriott.
ECG/PDQ.
Williams & Wilkins, 1987.
6] Lionel H. Opie.
The Heart: Physiology and Metabolism, chapter 5, pages 102{126.
Raven Press, New York, second edition, 1991.
7] Allen M. Scher.
The electrocardiogram.
In Harry D. Patton, Albert F. Fuchs, Bertil Hille, Allen M. Scher, and Robert Steiner, editors, Textbook of Physiology: Circulation, Respiration, Body Fluids, Metabolism, and Endocrinology, chapter 38, pages 796{819.
W.B.
Saunders  Company, 21st edition, 1989.
An Approach to Model and Query Event-Based Temporal Data Elisa Bertino, Elena Ferrari Dipartimento di Scienze dellaInformazione UniversitaE degli Studi di Milano - Italy fbertino,ferrarie g@dsi.unimi.it Giovanna Guerrini Dipartimento di Informatica e Scienze dellaInformazione UniversitaE degli Studi di Genova - Italy guerrini@disi.unige.it  Abstract Temporal database systems support all functions related to the management of large amounts of constantly changing data.
However, current temporal database systems support a flat view of the history of data changes, in that all the changes are considered equally relevant and are, therefore, all stored in the database.
However, many applications, such as monitoring and planning applications, call for more flexibility.
Monitoring applications, in particular, may require that the history of a data item is stored only whenever a certain event occurs.
For other applications the history of data changes may be less important than the event causing the changes.
In this paper we propose an event-based temporal object model which allows to keep track of selected values within the history of a data object attribute.
The portions, within the history of a data object, which are actually stored into the database are identified by relating them to events.
In the paper, besides defining the data model, we investigate the problem of querying a database with incomplete temporal information.
1 Introduction A temporal database [13] typically stores the entire history of data over time, rather than storing only current data as conventional databases.
Some models support, in addition, static attributes [3], that is, attributes for which the changes in time are not meaningful with respect to the given application domain and, thus, need not to be stored.
Such approach avoids storing irrelevant old values of attributes [12].
There are, however, many applications, such as monitoring and planning applications, for which it would be useful to selectively record values of data objects, that is,  to store the past values of data objects only whenever certain situations occur or certain conditions are met.
Monitoring applications may require that whenever a certain event occurs on a certain data object, the history of the data object (or of some of the objectas attributes) is recorded in the database from that moment on.
Consider as an example the price of a stock; a monitoring application could need to record all the price variations of the stock only when the price of the stock increases over a given amount.
Similar examples can be found in the medical application domain, where for example the temperature variations for a patient need only to be recorded when the temperature becomes higher than 36C degrees.
Planning applications may require to keep the history of a data object after certain modifications on the object have been performed, in order to subsequently analyze the consequences of the modifications.
Object-oriented database systems supporting object evolution may also require a selective recording of object histories [5].
For example, it could be meaningful to store the value of the salary of an employee when he is promoted to a manager, or when he retires, or when he works in a certain division or on a certain project.
As pointed out by Dean and McDermott in [9], time can be seen as a map, where not all instants or time intervals are equally relevant.
Some instants or time intervals may be more important than others, as they indicate relevant application events.
Current temporal DBMSs do not provide support for event-based temporal histories.
One possibility would be supporting such histories through application programs on top of a temporal DBMS.
Such an approach, however, has many drawbacks.
The applications would have to be in charge of detecting events and determining when an old value of a data item must be kept or must be purged.
As with integrity constraints, relying on application programs makes it difficult, if at all possible, the specification of events and  the management of event-based temporal histories.
Versioning mechanisms suffer from the same drawbacks because it is up to the users or applications to explicitly require the creation of a new version of a given data object.
Therefore, no information is kept into the database concerning events that caused the new version to be created (unless some specific code for doing so is added to all application software).
A relevant issue in a data model supporting selective temporal histories is related to queries.
Only queries accessing attributes at time instants in which a value for that attribute is stored can be exactly solved.
The other queries can be rejected, or alternately, can be approximated by using an appropriate approximation method.
Such issue, however, has not been so far addressed.
In this paper we develop an event-based temporal object data model by extending the T Chimera temporal object model [2] with the possibility of selectively storing the past values of object attributes by associating with attributes snapshot conditions, expressed in a rich event language.
When the snapshot condition associated with an attribute occurs, the history of the attribute is updated by inserting the current value of the attribute.
The notion of snapshot condition and its use in event-based temporal histories has been first proposed in [3].
However, in [3] a very simple language for expressing snapshot conditions was considered.
In this paper, we consider a powerful event language, allowing to combine database operations, conditions on the database, temporal and periodic events by means of several operators.
Our event specification language has almost the same expressive power of the event specification language Snoop [8].
The main difference is that we provide a more powerful formalism to express periodic time, since in our language expressions with level 2 periodicity can be specified, like Each working day between 9 a.m. and 12 a.m.
Besides defining the data model, we address the issue of queries, which was not dealt with in [3].
We consider temporal attribute accesses requiring the value of a certain attribute for an object at a specified time, and we investigate conditions ensuring that the access can be exactly solved.
For accesses that cannot be exactly solved, we introduce the notion of approximate query, to supply a value obtained through an approximation method.
Moreover, we show how our event language support meta-queries, that is, queries to retrieve all the objects or attributes whose changes have been recorded because of an arbitrary event occurrence.
This is a relevant issue since often knowing the event which has changed the state of an object is more important than the change itself.
Work on this direction has been carried on in the context of relational databases [10].
However, to our knowledge, we are the first addressing these issues in the context of the object-oriented model.
The remainder of this paper is organized as follows.
Section 2 introduces the event specification language.
Section  3 deals with event containment.
Section 4 presents the data model, whereas Section 5 deals with queries.
Finally, Section 6 concludes the paper and outlines future work.
The event language syntax and semantics are reported in Appendix A.
2 Event Language In this section we give an informal description of the language we provide to specify events.
The formal syntax and semantics are reported in Appendix A.
The language supports the specification of two main categories of events: basic events and composite events, built by applying a set of predefined event constructors to basic or composite events.
Basic events are of two main types: database events and temporal events.
Database events denote database operations such as the migration of an object from a class to another or the increase of an attribute value up to a given threshold.
Database events can be either instantaneous (i.e., they last only one instant), or persistent (i.e., they last over a time period).
For instance, migrate is an instantaneous event, whereas (salary > 60K) is a persistent event, since it lasts from the first instant on which the salary exceeds 60K till the first instant on which the salary becomes again lesser than 60K.
We use the term starting time to denote the instant on which an event begins to occur.
Similarly, the ending time of an event is the time on which the event finishes.
We use the notation mm/dd/yy:hh:mm:ss to represent time instants.
For example, the notation 1/1/94:08 represents 8 a.m. on 1/1/94.
When 1/1/94 is used as a minimum, it denotes the first instant of the first day of January 1994, while, as a maximum, it denotes the last instant of 1/1/94.
Note moreover that an event can occur more than one time during the database history.
Example 2.1 Consider the event (salary > 60K) and suppose that salary = 30K during the period [1/1/94,1/1/95].
Suppose that on 1/2/95 the salary increases up to 70K and remains unchanged till 1/1/96 when it assumes the value 20K.
Suppose moreover that the salary remains unchanged till 4/1/96 when it is set equal to 80K.
Therefore the event (salary > 60K) occurs from the first instant of 1/2/95 to the last instant of 12/31/95 and from the first instant of 4/1/96 up to now.
The first instants of 1/2/95 and 4/1/96 represent the starting times of the event (salary > 60K), whereas the last instant of 12/31/95 represents the ending time of the event.
Temporal events can be either absolute, as the first hour of a specific day, periodic, as each working-day between 9 a.m. and 12 a.m., or relative, as five hours after the salary  of a given employee has been modified.
To represent periodic time we adopt the symbolic formalism proposed by Niezette and Stevenne in [11], based on the notions of calendar and periodic expression.
We postulate the existence of a set of predefined calendars containing at least the calendars Hours, Days, Weeks, Months, and Years, and we consider Hours as our finest granularity calendar.
Calendars are combined to represent more general sets of periodic intervals, not necessarily contiguous, as, for instance, the set of Mondays or the set of The third hours of the first day of each month.
These complex sets of periodic intervals are represented by means of periodic expressions.
Table 1 illustrates a set of periodic expressions and their meaning.
More details can be found in [11].
Moreover, the language provides a set of event constructors from which complex events can be defined.
A first set of constructors, called event modifiers, consists of a number of unary operators that transform an arbitrary event into one or more related events.
We support four different types of event modifiers.
Let E be an event, begin(E ) and end(E ) occur on the starting and ending times of E , respectively; begin on(E ) occurs from the first occurrence of E up to now, whereas end on(E ) occurs from the first time E ends up to now.
Example 2.2 With reference to Example 2.1, begin(salary > 60K) occurs on 1/2/95 and 4/1/96, end(salary > 80K) occurs on 12/31/95, begin on(salary > 60K) occurs from 1/2/95 up to now, whereas end on(salary > 60K) occurs from 12/31/95 up to now.
Additionally, the language provides the following constructors (in the following, E1 and E2 denotes two events):      Disjunction.
E1 OR E2 occurs when E1 or E2 occurs; Conjunction.
E2 occur;  E1 AND E2 occurs when both E1 and  Sequence.
This constructor conditions the occurrence of an event to the temporal relations occurring between the starting times of two arbitrary events.
We provide four different forms of the sequence constructor: sequence(E1 ; E2 ,tmin ,tmax ), which occurs on the starting time of E2 , provided that the ending time of E1 occurs tmax tmin instants before the starting time of E2 ;1 sequence(E1 ; E2 ,tmin ,null) which occurs at the starting time of E2 , provided that the ending time of E1 occurs at least tmin instants before the starting time of E2 ; sequence(E1; E2 ,null,tmax) which  1 Note that, as a particular case, sequence(E1 ; E2 ,0,0) occurs on the starting time of E2 , provided that the starting time of E2 is immediately after the ending time of E1 .
occurs at the starting time of E2 , provided that the ending time of E1 occurs no more than tmax instants before the starting time of E2 ; sequence(E1 ; E2 ,*) which occurs on the starting time of E2 , provided that E1 has already occurred.
    Occurrence.
This constructor relates the occurrence of an event to the multiple occurrences of another event in a set of time intervals bound by the starting/ending times of two arbitrary events.
There are two variants of the occurrence constructor: happen(E1 ; n; E2 ; E3 ) occurs on the starting time of the n-th occurrence of event E1 in the closed interval determined by the starting time of E2 and the ending time of E3 ; happen(E1 ; ; E2 ; E3 ) occurs on the starting time of any occurrence of event E1 in the closed interval determined by the starting time of E2 and the ending time of E3 .
Non-occurrence.
This constructor relates the occurrence of an event to the non occurrence of another event in a set of time intervals bound by the starting/ending times of two arbitrary events.
There are two variants of the non-occurrence constructor: not happen(E1 ; n; E2 ; E3 ) occurs on the ending time of E3 provided that the number of occurrences of E1 in the closed interval determined by the starting time of E2 and the ending time of E3 is lesser than n; not happen(E1; ; E2 ; E3 ) occurs on the ending time of E3 provided that E1 does not occur in the closed interval determined by the starting time of E2 and the ending time of E3 .
In the following, E denotes the set of events that can be expressed in our language.
Example 2.3 The following are events in E : generalize: it occurs each time an object in the database migrates to a superclass; salary > 60K AND status =aapart timeaa: it occurs each time the salary of a part time employee is greater than 60K; ([1/1/94,1],Mondays): it occurs each Monday from 1/1/94; happen(increase(salary), 5, status = aapart timeaa, status = aafull timeaa): it occurs if the salary of an employee has increased 5 times during the period he was a part time employee.
3 Containment between Events In this section we investigate the property of event containment, which will be useful for characterizing a number of conditions in our temporal data model.
Intuitively, given  periodic expression Weeks + 2,6 .Days Months + 20.Days Years + 7.Months 3.Months Weeks + 2,: : :,6 .Days + 10.Hours  f g f   g   3.Hours  meaning Mondays and Fridays The twentieth day of every month (Pay-days) Summer-time Each Working day between 9 a.m. and 12 a.m.  Table 1.
Example of periodic expressions two events E and E 0 , E is contained in E 0 if each time E occurs, E 0 occurs too.
To formally define the notion of containment, we need first to introduce some preliminary notations.
We model the database as an history  db history = h[t0 ; t1 ); db0 i ?
!E1 h[t1 ; t2 ); db1 i ?
!E2 : : : : : : ?
!En h[tn ; now]; dbn i where, ti , i = 1; : : : ; n, is a time instant, Ei , i = 1; : : : ; n, is an update event which arose at time ti and now denotes the current time.
The db history models that the database has evolved from its initial state db0 through a sequence of states dbi and each transition arises because of an update event Ei .
Moreover, we make use of a function f : E  T IME 2 !
ftrue ; false g that, given an event E 2 E and a time instant t, returns true if E occurs on t; it returns false otherwise.
This function, modeling event semantics, is specified in Appendix A.
Definition 3.1 Let E and E 0 be two events in E .
E is contained in E 0 , denoted as E  E 0 , if and only if 8db history: ft j f (E; t) = trueg  ft0 j f (E 0 ; t0 ) = trueg.
Example 3.1 The following are examples of containment relationships between events:   f[1/1/94,1/1/95],[1/1/96,1/2/97]g  f[1/1/93,1/2/97]g;  salary > 60K AND status = aapart timeaa  salary > 60K  salary > 20K.
An algorithm to test containment between events in E  has been developed.
4 Data Model The temporal data model we propose allows to selectively keep track of data modifications.
The idea is to associate a snapshot condition, expressed in the language introduced in Section 2, with an attribute, so that the history of the attribute is updated whenever the snapshot condition is  2 The set T IME = f0; 1; : : : ; nowg is our temporal domain.
true.
The data model we propose here extends the objectoriented temporal data model T Chimera [2] with the possibility of associating snapshot conditions with attributes.
However, the idea of selectively keeping track of modifications to data is highly independent from T Chimera and can be applied to any object-oriented or relational temporal data model.
In the following, we denote with OI a set of object identifiers, with CI a set of class identifiers (i.e., class names), with AN a set of attribute names, and with V the set of T Chimera legal values.
T Chimera supports the notion of type.
A finite set of basic predefined types is provided by the language, containing in addition to the usual (nontemporal) types, a time type whose domain is the set T IME = f0,1,: : :,nowg and which represents transaction time.
T Chimera also supports structured types such as sets, lists and records, and allows the use of class names in the definition of structured types.
Finally, temporal types are supported: for each type T , a corresponding temporal type, temporal(T ), is defined.
Instances of type temporal(T ) are partial functions from instances of type time to instances of type T .
Temporal types can be used in the definition of structured types.
In our data model, attributes have temporal types as domains and their values are thus partial functions from T IME to the set of legal values for the attribute.3 Throughout the paper we represent the value of a temporal attribute of type temporal(T ) as a set of pairs fh1 ; v1 i; : : : ; hn ; vn ig, where v1 ; : : : ; vn 2 V are legal values for type T , and 1 ; : : : ; n 2 T IME  T IME are time intervals such that the attribute assumes the value vi for each time instant in i , i = 1; : : : ; n. Moreover, given the value v of a temporal attribute and a time instant t, v (t) denotes the value taken by function v on input t, according to the usual notation for function application.
A snapshot condition in E can be associated with each attribute, specifying the conditions upon which the attribute value is stored.
If no snapshot condition is associated with an attribute, the entire history of attribute changes is recorded in the database.
Given a class c 2 CI , A(c) denotes the set of attributes of instances of that class, whereas dom(a; c) and "(a; c) denote the domain and the snapshot condition  3 Actually, T Chimera supports temporal, static and immutable attributes.
In this paper we only consider temporal attributes.
of attribute a in class c, respectively.
Example 4.1 Class employee and its subclass manager are examples of T Chimera classes:  A(employee) = fname, salary, status, division, manager,  g  w hours ,  dom(name,employee) = temporal(string), dom(salary,employee) = temporal(integer), dom(status,employee) = temporal(string), dom(division,employee) = temporal(string), dom(manager,employee) = temporal(manager), dom(w hours,employee) = temporal(integer), "(salary,employee) = (create,1, Pay-Days), "(manager,employee)= sequence(decrease(salary), increase(w hours),*)  A(manager) = A(employee)[fdependents, official carg, dom(dependents,manager) = temporal(set-of(employee)),  dom(official car,manager) = temporal(string) "(dependents,manager) = count(dependents)  5, "(official car,manager) = update(salary) The value of attribute salary is recorded the twentieth day of every month (i.e., the apay-daya) starting from the time of the object creation; the history of attribute manager is updated only upon a salary reduction followed by an increase of the working hours.
Manager dependents are recorded only when the dependents are more than five, whereas manager official car is recorded only whenever the manager salary is updated.
For all the other attributes, the entire history of changes is maintained.
For a proper redefinition of attributes in subclasses, we impose that: (i) the domain of an attribute in a subclass is refined in a subtype of the domain in the superclass; (ii) the snapshot condition associated with an attribute in a subclass must contain the snapshot condition associated with the attribute in the superclass.
These conditions, formally stated by the following rule, ensure the substitutability of the subclass instances with respect to the superclass.
Rule 4.1 Let c1 and c2 2 CI such that c2 is a subclass of c1 .
Then, 8a 2 A(c1 ), the following conditions must be satisfied: i) dom(a; c2 ) T dom(a; c1 ); ii) "(a; c1 )  "(a; c2 ), where T denotes the subtype relationship [2].
An object o is characterized by an object identifier i, a lifespan and a record value which represents its state, as formally stated by the following definition.
Definition 4.1 An object o is a 3-tuple (i; lifespan; v ), where i 2 OI is the oid of o; lifespan 2 (T IME  T IME ) is the lifespan of o; v 2 V is a record value (a1 : v1 ; : : : ; an : vn ), where a1 ; : : : ; an 2 AN are the names of the attributes of o, and v1 ; : : : ; vn 2 V are their corresponding values.
Example 4.2 Consider the classes of Example 4.1, and suppose that i1 ; : : : ;i7 2 OI .
Objects o1 and o2 specified as follows are examples of T Chimera objects :4  i =i1 lifespan = [1/1/97,now] v = f(name: fh[1/1/97,now],aAlan Smithaig), (salary: fh1/20/97,20Ki; h2/20/97,15000ig), (status: fh[1/1/97,now],afull-timeaig), (division: fh[1/1/97,1/31/97],aDisksai; h[2/1/97,now],aPrintersaig), (manager: fh3/1/97,i4 ig), (w hours: fh[1/1/97,2/28/97],38 i; h[3/1/97,now],40ig)g i = i4 lifespan = [2/1/97,now] v = f(name: fh[2/1/97,now], aMary Doleaig), (salary: fh2/20/97,30Kig), (status: fh[2/1/97,now],afull-timeaig), (division: fh[2/1/97,now],aPrintersaig), (w hours: fh[2/1/97,now],35 ig), (dependents: fh[2/15/97,now], fi1 , i3 , : : :, i7 gig)g  Note that, no value for attributes manager and official car is recorded in object o2 , since there does not exist an instant in which the corresponding snapshot conditions are verified.
In order to ensure object consistency, we require that for each attribute of an object, a value is stored for each instant satisfying the associated snapshot condition.
This requirement is formalized by the following rule.
Rule 4.2 Let o be an object, and let c be the class to which o belongs at time t. Then, 8a 2 A(c) the following condition must be satisfied: o:v:a(t) is defined , f ("(a; c); t) =  true:  Moreover, the value must be of the appropriate type, as usually in data models.
5 Queries Because in our model, only selected portions in an attributeas history can be recorded, a query could be issued requiring the value of the attribute at a time belonging to a non-recorded portion.
An important question concerns which accesses can be exactly answered and how to provide a value also for the time instants for which no value exists in the database.
In this section we address such issues.
For the sake of simplicity we do not introduce a complete query language.
Rather, we focus on temporal attribute accesses, which are the basis of any object-oriented query language.
We consider temporal attribute accesses of the form o:a # E , where o is an object reference (e.g.
4 In the example, by abuse of notation, we use t to denote the interval  [t; t].
a variable) denoting an object of a given class c (e.g.
the type of the variable), a is an attribute of class c, and E is a temporal specification, that is, an expression specified in the event language introduced in Section 2.
Note that other languages could have been considered as well, but we refer to the event language both for the sake of uniformity and because it is convenient for expressing meta-queries.5 However, we restrict ourselves to event expressions denoting single time instants, though temporal attribute accesses involving time intervals could be easily handled [4].
We do not consider them here since they complicate the discussion without bringing in any relevant issue.
Thus, we consider the subset of events introduced in Section 2 which occur on specific time instants (that is, instantaneous events).
This means that we only consider event expressions E such that ft j f (E; t) = trueg is a singleton set ftg.
Given an event E , let tE denote this unique time instant.
Finally, note that, as a particular case, E can be a time instant t (i.e., the time of the attribute access can be explicitly denoted).
Example 5.1 Given a variable X of type employee, X.salary#1/1/94:08 is an example of temporal attribute access requiring the value of attribute salary of the employee object denoted by variable X at 8 a.m. on 1/1/94.
A further example of temporal attribute access is X.salary # end(salary > 80K).
Referring to the db history of Example 2.1, the above temporal attribute access is equivalent to the access X.salary # 12/31/95.
5.1 Exact Queries: Static Conditions In this subsection we consider a temporal attribute access o:a # E and we deal with the problem of (statically) deciding whether the value of attribute a of the object denoted by o is available at the time denoted by E .
Intuitively, an attribute access o:a # E can be exactly solved, that is, a value for attribute a of object o at time tE is available, if the snapshot condition of attribute a in the class of o, is true at time tE .
The value denoted by o:a # E , if available, is the non-temporal value o:v:a(tE ), that is, the value of the (partial) function o:v:a on tE .
Thus, given an object reference o of type c and an attribute a of class c, the attribute access o:a # E denotes an available value, and, thus, can be exactly answered, if and only if f ("(a; c); tE ) = true.
Example 5.2 Consider the objects of Example 4.2.
Let X be a variable of type employee and Y be a variable of type manager, denoting object o1 and o2 , respectively.
The attribute accesses X.name # 2/1/97, X.salary # 1/20/97, X.manager # 3/1/97, Y.salary # 2/20/97 and Y.dependents # 2/16/97 can be exactly answered and they denote the values Alan Smith,  5 We will elaborate on this in Subsection 5.3.
20K, i4 , 30K, and fi1 ,i3 ; : : :,i7 g, respectively.
By contrast, the attribute access X.salary # 1/21/97 cannot be exactly answered since the event (create,1,PayDays) did not occur on 1/21/97.
If the value of attribute a for object o at the time denoted by E is not available, the query could be rejected, or could be approximated.
Query approximation will be dealt with in the following subsection.
However, a user can explicitly require that the query must be exactly answered.
Determining whether a query cannot be exactly answered, without executing the query, would avoid many unnecessary database accesses.
It is however not always possible to statically detect whether an attribute access can be exactly solved.
For instance, if "(a; c) is a database event (either an update event or a value event) and E is a time instant t we cannot decide at query compile-time whether f ("(a; c); t) is true, since this decision depends on the specific db history .
Example 5.3 Given the database event salary > 60K and the time instant 1/1/94:08 we cannot decide, independently from the db history , whether f (salary > 60K,1/1/94:08) = true.
More precisely, we can ensure that an attribute access  o:a # E can be exactly answered, if E is contained6 in the snapshot condition associated with a, that is, if E  "(a; c).
In this case, indeed, by definition of event expression containment, for each db history : tE 2 ft j f ("(a; c)) = trueg.
Example 5.4 The temporal attribute access X.salary # begin(salary > 60K) can be exactly answered in a database where the snapshot condition salary > 60K is associated with attribute salary in class employee since begin(salary > 60K)  salary > 60K.
5.2 Approximate Queries: Approximation Methods In this subsection we deal with attribute accesses that cannot be exactly solved, that is, accesses of the form o:a # E when the value of attribute a of the object denoted by o is not available at time tE .
This can be a common situation in our model, since we allow a partial recording of attribute temporal histories.
When the value at a given instant is not available, different options can be taken with respect to which value should be returned.
The most intuitive and easy solution is to return a null value.
However, several situations can be devised in which it could be more appropriate to return an approximate value.
For instance, suppose that a user requests the salary of a given employee  6 Containment between events has been discussed in Subsection 3.  wavg() = v  v = [(o:a # t; last())  (t ?
time(o:a # t; last()))+ (o:a # t; next())  (time(o:a # t; next()) ?
t)]= (time(o:a # t; next()) ?
time(o:a # t; last())): Figure 1.
Example of approximation method  at time t and suppose that the salary is recorded in the database once a month.
If no salary amount is stored at time t, it is reasonable to return the latest stored value for the salary attribute.
This is equivalent to assume that the value of the salary attribute persists in time until a new value is explicitly stored in the database [6].
Other alternatives could be returning the next explicitly given value, or the average of the last and the next explicitly given values.
More sophisticated approximation methods can be adopted, such as returning the average of the last n values.
We therefore allow an approximate value to be returned for queries that cannot be exactly solved.
This is achieved by assuming the existence of a pre-defined set AM of approximation methods which can be used in a query.
Such methods define how to derive implicit information from that explicitly stored.
Approximation methods are functions that given an instant t and an attribute a of type T return a value of type T representing the value to be returned as the value of a at time t, when the value of a at time t is not available.
This value is computed by appropriately combining the available values for attribute a.
We assume that AM contains at least the method last() which given an instant t and an attribute a returns the more recent value among those stored before time t, the method next() which returns the first value for a, if any, stored after time t, and the method avg() which returns the average of the values returned by last() and next().
Attribute accesses for which an approximate value should be returned are called approximate attribute accesses and are formally defined as follows.
Definition 5.1 An approximate attribute access is a pair (o:a # E ,app), where o is an object reference of type c, a is an attribute of class c, E is a temporal specification, and app is an element of AM.
Example 5.5 Suppose that the value of attribute salary is 60K on 12/30/93 and 80K on 2/1/94.
Suppose moreover that no other value for attribute salary is recorded in the period [12/30/93,2/1/94].
Then the answer to (X.salary#1/1/94:08,last()) is 60K, the answer to (X.salary#1/1/94:08,next()) is 80K, whereas the answer to (X.salary#1/1/94:08,avg ()) is 70K.
Note moreover that user-defined approximation methods can be specified as well, in addition to methods provided by the system.
User-defined approximation methods are used  to specify ad-hoc approximation methods for certain attributes and accesses.
Though several languages could be used to express them, we consider approximation methods expressed in a deductive style, that is, through rules which may contain in their bodies temporal attribute accesses and the predefined approximation methods last(), next() and avg().
Example 5.6 The approximation method wavg () illustrated in Figure 1 computes a weighted average of the values returned by approximation methods last() and next().
It makes use of a function time that, applied to an approximate access, returns the time instant at which the access is approximated.
Approximation methods which can appear in an approximate attribute access depend on the type of the attribute.
Approximation methods denoting aggregate functions, such as avg (), make sense only when they apply to numerical values, such as an integer or a real; in the case of nonnumerical values, such as object identifiers, or for structured values containing non-numerical components, approximation methods that can be applied are those returning one of the stored values for the attribute, like for instance last() and next().
Moreover, approximation methods also depend on the semantics of the attribute.
For instance, it is reasonable to associate with attribute status of Example 4.1 the approximation method last(), since the status of an employee could be reasonably assumed to be the last status recorded, whereas the approximation method wavg () of Example 5.6 could reasonably be associated with attribute salary of Example 4.1, if we assume that the salary of an employee can be approximated by a linear function.
5.3 Meta-queries In the above subsections we have dealt with queries requiring the value of an attribute at a given time instant.
However, our language also support meta-queries.
A metaquery contains conditions on event occurrences.
In particular, we support two different types of meta-queries: 1.
Attribute meta-queries that, given an event and an object, return all the attributes of the object whose temporal histories have been updated because of the event occurrence.
2.
Object meta-queries that, given an event, retrieve all objects whose state has been modified because of the event occurrence.
Note that two different interpretations are possible for the above meta-queries.
Consider an attribute meta-query on an event E .
Under a strong interpretation, the query retrieves only the attributes of the specified object with E as snapshot condition.
Under a weak interpretation the query retrieves all the object attributes whose associated snapshot condition contains E and such that there exists at least an instant, among the ones for which a value for the attribute is stored, in which E occurs.
A similar distinction applies to object meta-queries.
Example 5.7 Consider object o2 of Example 4.2, and the attribute meta-query requiring all the attributes of o2 whose temporal histories have been recorded because of the occurrence of the event count(dependents)  6.
Under a strong interpretation, the query does not return any value, since there is no attribute of object o2 with count(dependents)  6 as snapshot condition.
By contrast, under a weak interpretation the query returns the attribute dependents, since the snapshot condition count(dependents)  5, associated with attribute dependents, contains the event specified in the query, and 8t 2 [2/15/97,now] f (count(dependents)6,t) = true.
Finally, if the event associated with the meta-query is count(dependents)  8, no attribute is returned under both the weak and the strong interpretation.
In what follows the weak interpretation is always assumed.
However, the treatment can be easily extended to the strong interpretation.
The notion of meta-query is formalized by the following definitions.
Definition 5.2 Let o be an object of type c, and let E be an event.
The attribute meta-query o jj E jj returns all the attributes a 2 A(c) which satisfy the following conditions: i) E  "(a; c); ii) 9t such that f (E; t) = true.
Definition 5.3 Let o be an object, and let E be an event.
The object meta-query jj E jj returns all the objects o such that o jj E jj6= ;.
Example 5.8 Consider the objects of Example 4.2 and suppose that they are the only objects in the database.
jj  o1 sequence(decrease(salary),increase(w hours),*)  jj  = manager, o2 count(dependents) count(dependents) 5 = o2 .
jj   jj   6 jj = dependents,  jj  6 Conclusions and Future Work In this paper we have presented an event-based temporal object data model which allows to record selected portions within the history of an object attribute.
The portions which are actually stored are identified by relating them to events.
We have also investigated the problem of querying a database with incomplete temporal information.
We plan to extend this work along several directions.
First, we are currently developing a complete query language based on our data model.
The language, defined as a temporal extension of the OQL language [7], will also support meta-queries.
We also plan to extend the current model to support multiple temporal granularities and integrity constraints.
Finally, implementation issues are being investigated; in particular, we are implementing the proposed model on top of the Ode active OODBMS, by extending with event-based selective attribute recording the existing prototype implementation of T Chimera [1].
References [1] Bertino, E., Bevilacqua, M., Ferrari, E. and Guerrini, G. Approaches to Handling Temporal Data in ObjectOriented Databases.
TR 192-97, Department of Computer Science, University of Milano, 1997.
[2] Bertino, E., Ferrari, E. and Guerrini, G. A Formal Temporal Object-Oriented Data Model.
In Proc.
5th Intal Conf.
on Extending Database Technology, pages 342a356, 1996.
[3] Bertino, E., Ferrari, E. and Guerrini, G. T Chimera: A Temporal Object-Oriented Data Model.
Theory and Practice of Object Systems, 3(2):103a125, 1997.
[4] Bertino, E., Ferrari, E. and Guerrini, G. Navigational Accesses in a Temporal Object Model.
IEEE Trans.
on Knowledge and Data Engineering, to appear.
[5] Bertino, E., Guerrini, G. and Rusca, L. Object Evolution in Object Databases.
In Dynamic Worlds: From the Frame Problem to Knowledge Management.
Kluwer, 1998, to appear.
[6] Bettini, C., Wang, X.S., Bertino, E. and Jajodia, S. Semantic Assumptions and Query Evaluation in Temporal Databases.
In Proc.
of the ACM SIGMOD Conference, pages 257a268, 1995.
[7] Cattel, R. The Object Database Standard: ODMG-93.
Morgan-Kaufmann, 1996.
[8] Chakravarthy, S., Krishnaprasad, V., Anwar, E. and Kim, S.K.
Snoop: An Expressive Event Specification  heventi  hdb eventi hupdate eventi hupdate attr eventi hpath expri hvalue eventi hsimple value eventi hcomp opi hopi htemporal eventi htime speci hfreq speci  ::=  ::= ::= ::= ::= ::= ::=  ::= ::= ::=  ::= ::=  heventi AND heventi j heventi OR heventi j sequence(heventi, heventi,htime speci, htime speci) j sequence(heventi, heventi,h*i) j happen(heventi, hfreq speci, heventi, heventi) j not happen(heventi, hfreq speci, heventi, heventi) j begin onheventi j end onheventi j beginheventi j endheventi j hdb eventi j htemporal eventi hupdate eventi j hvalue eventi generalizej specializej createj delete j migrate j hupdate attr eventi j hmeth namei update(hpath expri) j increase(hpath expri) j decrease(hpath expri) hattr namei j hclass namei.hpath expri hsimple value eventi hcomp opi hsimple value eventi hvalue i j hpath expri j count(hsimple value eventi) j hsimple value eventi hopi hsimple value eventi > j < j  j  j = j 6= j 2 j 62 +j-j*j/j[j\jn hset of time instantsi j htime instanti j hset of time intervalsi j (htime intervali,hperiodic expri) j (heventi,heventi, hperiodic expri) j (heventi,htime instanti) htime instanti j null hnat numberi j * Figure 2.
Event language syntax  Language for Active Databases.
Data & Knowledge Engineering, 14:1a26, 1994.
[9] Dean, T. L. and McDermott, D. V. Temporal Data Base Management.
Artificial Intelligence, 32(1):1-55, April 1987.
[10] Jensen, C.S.
and Mark, L. Queries on Change in an Extended Relational Model.
IEEE Trans.
on Knowledge and Data Engineering, 4(2):192a200, April 1992.
[11] Niezette, M. and Stevenne, J.
An Efficient Symbolic Representation of Periodic Time.
In 1st International Conference on Information and Knowledge Management, 1992.
[12] Ozsoyoglu, G. and Snodgrass, R.T. Temporal and Real-Time Databases: a Survey.
IEEE Trans.
on Knowledge and Data Engineering, 7(4):513-532, August 1995.
[13] Tsotras, V.J.
and Kumar, A. Temporal Database Bibliography Update.
SIGMOD-RECORD, 25, 1996.
A Event Language Syntax and Semantics In the following we illustrate the syntax and semantics of our event language.
A.1  Syntax  The syntax in BNF form of our event language is reported in Figure 2.
Non terminal symbols hvaluei, hattr namei, and hclass namei represent elements of the domains V , AN , and CI , respectively.
hmeth namei denotes a method name.
Finally, non terminal symbols htime instanti, hset of time instantsi, hset of time intervalsi, and hnat numberi represent elements of the domains IN [ 1, 2IN[1 , 2IN[1IN[1 , and IN, respectively.
A.2  Semantics  The semantics of events in E is given by their interpretation I () in first order logic and is reported in Table 2.
In defining I () we make use of two functions start() and end(), that receive as input an event and return the starting and ending times of E , respectively.
Formally, let E 2 E : start(E ) = ft j f (E; t) ^ :f (E; t ?
1)g end(E ) = ft j f (E; t) ^ :f (E; t + 1)g7 Moreover, we make use of function card which computes the cardinality of a given set.
P denotes a generic periodic expression.
(P) denotes the set of time instants represented by P.8 In addition, in the semantics of value events, db j= E denotes that the value event E evaluates  7 Function f has been defined in Subsection 3.
8 We refer to [11] for the formal definition of ().
Event  E = update event,a E 6= increase, decrease E = value event E = increase(p)/decrease(p) E = t1 E = ft1 ; : : :, tn g E = f[t1 ,t2 ]; : : :, [tn ,tn+1 ]g E = ([t1 ,t2 ],P) E = (E1 ; E2 ,P) E = (E1 ; t ) E =begin(E1 ) E =end(E1 ) E =begin on(E1 ) E =end on(E1 ) E = E1 OR E2 E = E1 AND E2 E =sequence(E1; E2 ,tmin ,tmax) 0  E =sequence(E1; E2 ,tmin ,null) E =sequence(E1; E2 ,null,tmax) E =sequence(E1; E2 ,*) E =happen(E1 ; n; E2 ; E3 ) E =happen(E1 ; ; E2 ; E3 ) E =not happen(E1 ; n; E2 ; E3 ) E =not happen(E1 ; ; E2 ; E3 )  I () 8t(9t ; 9t (h[t ; t); dbi ?
!E h[t; t ); db i 2 db history) !
f (E; t)) 8t(9t ; 9t (h[t ; t ]; dbi 2 db history ^ db j= E ^ t 2 [t ; t ]) !
f (E; t)) 8t(9t ; 9t ; 9E (h[t ; t); dbi ?
!E h[t; t ); db i 2 db history) ^ E =update(p) ^hdb; db i j= E ) !
f (E; t)) 8t(t = t1 !
f (E; t)) 8t(t = t1 _ : : : _ t = tn !
f (E; t)) 8t(t1  t  t2 _ : : : _ tn  t  tn+1 !
f (E; t)) 8t(t1  t  t2 ^ t 2 (P) !
f (E; t)) 8t(9t1 ; 8t (t1 2 start(E1 ) ^ t1  t ^ (t1  t < t !
t 62 end(E2 ))^ ^(t1 < t  t !
t 62 start(E1 )) ^ t 2 (P)) !
f (E; t)) 8t(9t1 ; 8t (t1 2 start(E1 ) ^ t = t1 + t ^ (t1 < t  t !
t 62 start(E1 )) !
f (E; t)) 8t(9t1 (t1 2 start(E1 ) ^ t1 = t) !
f (E; t)) 8t(9t1 (t1 2 end(E1 ) ^ t1 = t) !
f (E; t)) 8t(9t1 (t1 2 start(E1 ) ^ t1  t) !
f (E; t)) 8t(9t1 (t1 2 end(E1 ) ^ t1  t) !
f (E; t)) 8t(f (E1 ; t) _ f (E2 ; t) !
f (E; t)) 8t(f (E1 ; t) ^ f (E2 ; t) !
f (E; t)) 8t(9t1 (8t (t1 2 end(E1 ) ^ t 2 start(E2 ) ^ t = t1 + tmax ?
tmin ^ ^(t1  t < t !
t 62 start(E2 )) ^ (t1 < t  t !
t 62 end(E1 )))) !
f (t; E )) 8t(9t1 (8t (t1 2 end(E1 ) ^ t 2 start(E2 ) ^ t1 + tmin  t^ ^(t1  t < t !
t 62 start(E2 )) ^ (t1 < t  t !
t 62 end(E1 )))) !
f (t; E )) 8t(9t1 (8t (t1 2 end(E1 ) ^ t 2 start(E2 ) ^ t  t1 + tmax  t^ ^(t1  t < t !
t 62 start(E2 )) ^ (t1 < t  t !
t 62 end(E1 )))) !
f (t; E )) 8t(9t1 8t (t1 2 end(E1 ) ^ t 2 start(E2 ) ^ t1 < t)^ ^(t1  t < t !
t 62 start(E2 )) ^ (t1 < t  t !
t 62 end(E1 )))) !
f (t; E )) 8t(9t1 (8t (t1 2 start(E2 ) ^ t 2 start(E1 ) ^ t1  t^ card(ft j t 2 start(E1 ) ^ t1  t < tg) = n ?
1 ^ (t1  t < t !
t 62 end(E3 ))^ ^(t1 < t  t !
t 62 start(E2 )))) !
f (t; E )) 8t(9t1 (8t (t1 2 start(E2 ) ^ t 2 start(E1 ) ^ t1  t^ ^(t1  t < t !
t 62 end(E3 )) ^ (t1 < t  t !
t 62 start(E2 )))) !
f (t; E )) 8t(9t1 (8t (t1 2 start(E2 ) ^ t 2 end(E3 ) ^ t1  t ^ ^card(ft j t 2 start(E1 ) ^ t1  t  tg) < n ^ (t1  t < t !
t 62 end(E3 ))^ ^(t1 < t  t !
t 62 start(E2 )))) !
f (t; E )) 8t(9t1 (8t (t1 2 start(E2 ) ^ t 2 end(E3 ) ^ t1  t ^ (t1  t  t !
t 62 start(E1 ))^ ^(t1  t < t !
t 62 end(E3 )) ^ (t1 < t  t !
t 62 start(E2 )))) !
f (t; E )) Interpretation 0  00  0  0  00  0  0  00  00  0  00  0  0  0  0  00  00  0  0  0  0  0  0  0  0  00  00  0  00  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  a If E is a method invocation then f (E; t) is true whenever the method is being executed, that is, start(E ) denotes the time of method invocation, whereas  end(E ) denotes the time of return from method invocation.
Table 2.
Semantics of events to true on the database state db, whereas hdb; db0 i j= E denotes that E evaluates to true with respect to the state transition h[t1 ; t2 ); dbi ?
!E h[t2 ; t3 ); db0 i.
Efficient Aggregation over Moving Objects* Peter Revesz Yi Chen Computer Science and Engineering Department University of Nebraska-Lincoln Lincoln, NE68588, USA {revesz,ychen}@cse.unl.edu Abstract We study two types of aggregation queries over a set S of moving point objects.
The first asks to count the number of points in S that are dominated by a query point Q at a given time t. The second asks to find the maximum number of points in S that are dominated by a query point at any time.
These queries have several applications in the area of Geographic Information Systems and spatiotemporal databases.
For the first query and any fixed [?
]dimension d, we give two different solutions, one using O( N ) time and O(N ) space and another using O(log N ) time and O(N 2 ) space, where N is the number of moving points.
When each of the points in S is moving piecewise linearly along the the same line and the total number [?]
of pieces is O(N ), then we can do the count query in O( N ) time and O(N ) space.
For the second query, when all objects move along the xaxis we give a solution that uses O(log N ) time and O(N 2 ) space in the worst case.
Our solutions introduce novel search structures that can have other applications.
1.
Introduction Aggregation operators are frequently used in database queries.
The efficiency of database queries with aggregate operators is well understood and studied in the context of traditional relational data.
However, aggregation operators are also important for more complex data that cannot be represented in relational databases.
Example 1.1 Suppose that a large company has a number of manufacturing plants P1 , P2 , P3 , .
.
.. Each plant produces four different products X1 , X2 , X3 and X4 .
The profit at each plant for each product changes over time as shown in Table 1.1.
* This research was supported in part by NSF grant EIA-0091530 and a Gallup Research Professorship.
Table 1.
Profits for various plant and product combinations.
Id 1 2 3 4 5 .. .
X1 t + 2t + 10 t3 - 8t - 10 t2 - 50 t4 - 16 t3 + 81 .. .
2  X2 80 10t 3t 7t 4t .. .
X3 t + 30 t2 - 2t 5t - 10 5t2 3 t - 21 .. .
X4 5t - 10 t3 - 3t + 4 t - 10 t - 30 t + 10 .. .
T t t t t t .. .
The company has the opportunity to buy a new plant Q where profits are rising rapidly.
The board of directors would approve the buy only if five years from now Q will be more profitable for each product than 10 of the current plants.
In this case, the input relations P (Id, X1 , X2 , X3 , X4 , T ) and Q(X1 , X2 , X3 , X4 , T ) form a constraint database [10, 12, 16].
Therefore, we can find out how many plants are less profitable in 2007 by the following SQL query: select count(Id) from P, Q where P.X1 < Q.X1 and P.X2 < Q.X2 and P.X3 < Q.X3 and P.X4 < Q.X4 and P.T = 2007 and Q.T = 2007; Suppose that the company has a long-term plan to eliminate all products except X1.
Therefore, the board of directors gives an approval for the purchase plan subject to the following extra condition: Q should have the potential to some day be more profitable on product X1 than 20 of their current plants.
We can find out the maximum number  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  of plants that will be less profitable than Q by the following SQL query: select count(Id) from P, Q where P.X1 < Q.X1 and P.T = Q.T group by T having count(Id) >= all (select count(Id) from P, Q where P.X1 < Q.X1 and P.T group by T);  taking pictures of the ground, which is represented as the rectangular area in Figure 1.
Given a time instance, find out how many cars will be covered in the picture at that time.
= Q.T  While Example 1.1 can be extended to any higher dimension, many practical aggregation queries use only 1, 2 or 3-dimensional moving objects.
Example 1.2 Consider a set of ships moving on the surface of the ocean.
The locations of these ships are known by an enemy submarine which moves secretly underwater at constant depth.
If the submarine fires, it calls attention to itself.
Hence the submarine wants to wait until the maximum number of ships are within its firing range (which is some rectangle with the submarine in the center) before firing at as many ships as possible.
Let us assume that we have the relations Ship(Id, X, Y, T ) and Range(X, Y, T ), which describe the ships and the firing range of the submarine, respectively.
A ship is in the firing range at a time instance if its (X, Y ) location is equal to a point in the Range at the same time instance.
Hence the above can be expressed by the following SQL query using a maximum aggregation operator.
select max(ship-count)) from (select count(Id) as ship-count from Ship, Range where Ship.X = Range.X and Ship.Y = Range.Y and Ship.T = Range.T group-by T); There are many alternatives to express in SQL the same query.
For example, the above SQL query could be also expressed in by another SQL query that has a structure similar to the second SQL query in Exercise 1.1.
It is a practical problem to recognize that these different SQL structures both express max-count queries, which we will define below.
In this paper, we do not deal with the parsing problem.
Example 1.3 We show in Figure 1 three cars driving along three path, which can be represented by piecewise linear constraints.
We assume each car travels at a constant speed in each line segment.
Assume a plane flying in the air keeps  Figure 1.
Aggregations on piecewise linearly moving points  Examples 1.1 and 1.2 are both cases of a group of frequently occurring aggregation problems where the input data can be visualized as a set S of N number of kdimensional moving points.
In Example 1.1 each point represents one plant and the value of the ith dimension represents the profit of the i-th product at that plant.
In Example 1.2 each point represents one ship in 2-dimensions using latitude and longitude.
In Example 1.3, the speed and direction of the cars change as they enter new line segments, but the movement can still be represented by piecewise linear constraints.
We say that point Pi dominates point Pj if and only if Pi has a larger value than Pj has for each dimension.
Then the queries in Examples 1.1 and 1.2 can be generalized as follows: Count: Given a moving point Q and a time instance t, find the number of points in S dominated by Q at time t. Max-Count: Given a moving point Q, find the maximum number of points in S that Q can dominate at any time.
In this paper we focus only on the above two aggregation queries, because several other more complex aggregation queries can be reduced to them or can be solved similarly.
For example: Range-Count: Given a time instance t and two moving points Q1 and Q2 , find the number of points in S located in the hyper-rectangle defined by Q1 and Q2 .
(This reduces to a sequence of count queries.)
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  Max-Time: Given a moving point Q and time instance t, find out whether Q dominates at time t the maximum possible number of points in S. (This reduces to testing whether the results of a count and a max-count query are the same.)
Sum: Assign a value to each moving point.
Then given a moving point Q and time instance t, find the sum of the values of the points in S dominated by Q at time t. (This requires only minor changes in the index structures that we develop for count queries.)
Aggregation queries can be evaluated in O(N log N ) time and O(N ) space (see Appendix).
However, this performance is not acceptable in applications where the input data is large and the query evaluation speed is critical, like in Example 1.2.
The goal of this paper is to develop novel indexing structures that can greatly speed up count and maxcount aggregate query evaluation.
There are some indexing structures for moving objects [1, 2, 5, 11, 17].
One may use these indices to answer the count and the range-count query by first finding the set of points S  [?]
S dominated by a new point Q or being within a hyper-rectangle defined by Q1 and Q2 , and then counting the number of points in S  .
However, the counting may require linear time in the size of S  .
Our goal is to find the count in logarithmic time.
Further, these indices cannot be used to answer the max-count and max-time queries.
As shown by Zhang et al.
[20], if we have a static set of points, then the range-count problem can be solved by generalizing some earlier work on dominance by Bentley [3].
Zhang and Tsotras [19] also considered the max-count aggregation problem for static sets of points in S. However, these methods are not easily generalizable to moving points, which is our focus in this paper.
Lazaridis and Mehrotra [13] , Choi and Chung [6] and Tao et al.
[18] study the approximation of aggregate queries for spatial and spatiotemporal data.
In contrast to them, our algorithm produce precise answers without a significant loss in performance.
This paper is organized as follows.
In Section 2, we review some basic concepts, including partition trees for indexing moving objects proposed by Agarwal et al.
[1].
In Section 3, we consider two different methods for answering count aggregation queries.
The first method extends partition trees, to partition aggregation trees.
The second method uses a novel data structure called dominance-time graphs.
Dominance-Time graphs are faster than partition aggregation trees and they can also be used when the position of moving points are represented by polynomial functions of time.
In Section 4 we consider max-count aggregation queries.
Finally, in Section 5 we discuss some open problems.
Our main results are summarized in Table 2,  Table 2.
Computational complexity of aggregation on moving objects.
Query Count Count Max  I/O [?]
N log N log N  S  D  Function  Method  N N2 N2  d d 1  linear polynomial linear  PA tree DT graph Dome subdiv  where D means dimensions and S means space requirements.
2.
Basic Concepts We review two basic concepts.
Duality [8] allows mapping k-dimensional moving points into 2k-dimensional static points.
Partition Trees proposed by Agarwal et al.
[1] are search trees for moving points.
2.1 Duality Suppose the positions of the moving points in each dimension can be represented by linear functions of time of the form f (t) = a*t+b, which is a line in the plane.
We may represent this line as a point (a, b) in its dual plane.
Similarly, a point (c, d) can be represented as a line g(t) = c*t+d in its dual plane.
Suppose line l and point P have dual point L and dual line p respectively.
Then, l is below P if and only if L is below p .
Lemma 2.1 Let P = aP *t+bP and Q = aQ *t+bQ be two moving points in one dimensional space, and P  (aP , bP ) and Q (aQ , bQ ) be their corresponding points in the dual plane.
Suppose P overtakes Q or vice versa at time instance t, then bP - bQ t=- aP - aQ Let Slope(P  Q ) denote the slope of the line P  Q .
Then we have t = -Slope(P  Q ), that is, t is equal to the negative value of the slope of the line P  Q .
Hence, given a time instance t, the problem of finding how many points are dominated by Q reduces to the problem of finding how many points are below l, where l is a line crossing Q in the dual plane with the slope -t. Definition 2.1 Let S be a set of N points and l be a line in the plane.
We define the function CountBelow(l) as follows.
If l is a vertical line with r1 points on the left and r2 points on the right, then CountBelow(l) = max(r1 , r2 ).
Otherwise, if r number of points are below l, then CountBelow(l) = r.  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  [?]
we can answer the count query in O( N ) time.
Then we describe a more novel data structure, called an dominancetime graph, that needs only logarithmic time.
l1  3.1 Partition Aggregation Trees l2  (A)  (B)  Figure 2.
Rank of a line.
Note that Definition 2.1 is logical, because if l is a vertical line, then we can always tilt it slightly left or right to get another line that has the same value of CountBelow as we defined.
Example 2.1 Figure 2 shows a set of points and two lines l1 and l2 .
There are four points below l1 , hence CountBelow(l1 ) = 4.
There are five points to the left and one point to the right of l2 , which is a vertical line.
Hence CountBelow(l2 ) = 5.
2.2 Partition Trees Given a set S of N points in two dimensional space, we represent a simplicial partition of S as P = {(S1 , [?
]1 ), (S2 , [?
]2 ), ..., (Sm , [?
]m )}, where Si 's are mutually disjoint subsets of S whose union is S, and [?
]i is a triangle that contains all points of Si .
For a given parameter r, 1 <= r < N , we say this simplicial partition is balanced if each subset Si contains between N/r and 2N/r points.
Figure 3(A) shows an example of balanced simplicial partition for 35 points with r = 6.
The crossing number of a simplicial partition is the maximum number of triangles crossed by a single line.
The following is known about crossing numbers: Theorem 2.1 (Matousek[14]) Let S be a set of N points in the plane, and let 1 < r <= N/2 be a given parameter.
For some constant a (independent of r), there exists a balanced simplicial partition P of size r, such that any line crosses at most cr1/2 triangles of P for a constant c. If r <= N a for some suitable a < 1, P can be constructed in O(N log r) time.
Using Theorem 2.1, it is possible to recursively partition a set of points in the plane.
This gives a partition tree.
3.
Count Aggregation Queries In this section, we first make a simple extension of partition trees, described in Section 3.1.
With the modification,  Definition 3.1 Let S be a set of N points in k dimensional space and T be a multi-level partition tree for S. Let vi be an internal node in T , which stores a triangle [?
]i .
We attach a new value Ai to node vi , such that Ai is the number of points in Si .
We call the new tree structure Partition Aggregation Tree (PA Tree).
Theorem 3.1 PA Tree is a linear [?]
size data structure that answers the count query in O( N ) I/Os.
Example 3.1 Figure 3(B) shows a partition tree with four top level triangles A, B, C and D. The query line q crosses two top level triangles A and B.
There are three second level triangles A4, B2 and B3 that are crossed by q.
Figure 3(C) shows the structure of the PA-tree.
For simplicity, we only show for each node the triangle name and the count of the points contained in that triangle.
To find CountBelow(q), we start from the root of the PA-tree, load all top level triangles into memory and compare them to the query line q.
Since both triangles C and D are below the line, we add the precomputed value to the result CountBelow(q) = 12 + 17 = 29.
For the triangles A and B, we traverse their children recursively.
In this case, triangle B4 is below q, then we have CountBelow(q) = CountBelow(q) + CountIn(B4) = 29 + 4 = 33, where CountIn(B4) is the number of points in the subset associated with B4.
When we reach the leaf nodes of the PA-tree, we compare each point in the node with q, and add the number of points below q.
There is one point in triangle B3 that is below q.
Finally, the answer to the aggregation problem is 34.
In Figure 3(C), we indicate using double sided rectangles those nodes that are accessed by this algorithm.
In Example 1.3, the movement of a car can be represented by piecewise linear functions.
When the direction or speed changes, we may consider the car to be replaced by a new car with different direction or speed.
We have the following theorem for the piecewise linearly moving points in one dimensional space: Theorem 3.2 Let S be a set of piecewise linearly moving points with N number of pieces in one dimensional space.[?
]The dominance-sum problem of S can be answered in O( N ) I/Os with O(N ) space.
The above talks about one dimensional space.
That may occur when each car is going on a straight highway, but each car may slow down in certain intervals due to road construction or heavy traffic, and they change direction only if they  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  A  B  A2  B1  A3  B2 q  A1 B3 B4  A4  C C1 C2  D1 D2  D  C3 (A) D3 D4 (B)  S  A1 5  A  16  A2 4  A3 4  B  A4 3  61  16  C  B1 B2 B3 B4 4 4 4 4  12  C1 C2 C3 4 4 4  D 17  D1 D2 D3 D4 4 4 4 5  (C)  Figure 3.
A partition aggregation tree.
make U-turns.
It is an open problem to find a similarly efficient solution for two or higher dimensional space.
true for time instance t that is within any of the open intervals.
Note that any real number and -[?]
and +[?]
are allowed as interval endpoints.
3.2 Dominance-Time Graph Partition aggregation trees are limited because they only work when the points are moving linearly.
In this section we introduce dominance-time graphs, a novel index data structure that can handle polynomial functions of time.
Definition 3.2 For two k-dimensional moving points P = (f1 , ..., fk ) and Q = (g1 , ..., gk ), we say P dominates Q at time t, denoted as dom(P,Q,t), if and only if fi (t) > gi (t) for 1 <= i <= k. If P does not dominate Q at time t, then we write ndom(P,Q,t).
Definition 3.3 Let S be a set of N moving points in k dimensional space.
The dominance-time graph G(V, E) for S is a directed labeled graph, such that for each point in S, there exists a corresponding vertex in V , and there is an edge in G from P to Q labeled by the set of disjoint intervals {(a1 , b1 ), ..., (am , bm )}, if and only if dom(P, Q, t) is  Example 3.2 Suppose that we are given the following set of two dimensional moving points: P1 = (t + 10, t - 5) P2 = (2t, 2t - 10) P3 = (3t + 5, 3t - 15) P4 = (4t - 5, 0) The dominance-time graph of these moving points is shown in Figure 3.
Note that for any time instance t [?]
(5, 10) the condition dom(P3 , P4 , t) is true.
Hence the edge from P3 to P4 is labeled {(5, 10)}.
The labels on the other edges can be found similarly.
Definition 3.4 Let P and Q be two moving points and t0 and t be two time instances such that t0 < t. We say that  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  P2  , 5)  )  8  (-  , -5)  8  (5,+  , 2.5)  8  (-  (-  8  P1  )  8  (10, +  (2.5, 5) )  8  (5, +  P3  P4 (5, 10) Figure 4.
A dominance-time graph.
between t0 and t an increment event happens to P with respect to Q if ndom(P, Q, t0 ) and dom(P, Q, t).
Similarly, we say that between t0 and t a decrement event happens to P with respect to Q if dom(P, Q, t0 ) and ndom(P, Q, t).
Definition 3.5 Let Rank(P,t) be the number of points that are dominated by P at time t. Lemma 3.1 An increment event happens to P with respect to Q if and only if there is an outgoing edge from P that has a label in which no interval contains t0 and some interval contains t. Similarly, a decrement event happens to P with respect to Q if and only if there is an outgoing edge from P that has a label in which some interval contains t0 and no interval contains t. Lemma 3.2 Let t0 and t be two time instances such that t0 < t. Let P be any vertex in a dominance-time graph.
Let m (and n) be the number of increment (and decrement) events that happen to P with respect to different other vertices between t0 and t. Then the following is true: Rank(P, t) = Rank(P, t0 ) + m - n Example 3.3 Table 3 shows the rank of each point of Example 3.2 at time instances t = -8 and t = 12.
Note that dom(P2 , P3 , -8) and ndom(P2 , P3 , 12) are both true.
Hence, an increment event happened to P2 between time t = -8 and t = 12.
Similarly, ndom(P2 , P1 , -8) and dom(P2 , P1 , 12) are also both true.Hence a decrement event happens to P2 between the same times.
Thus, according to Lemma 3.2, we have Rank(P2 , 12) = Rank(P2 , -8) + 1 - 1 = 1  Table 3.
Location and rank of points at times t = -8 and t = 12.
Point P1 P2 P3 P4  Location t = -8 (2, -13) (-16, -26) (-19, -39) (-37, 0)  Rank t = -8 2 1 0 0  Location t = 12 (22, 7) (24, 14) (41, 21) (43, 0)  Rank t = 12 0 1 2 0  3.3 Time and Space Analysis In this section we describe the basic structure of dominance-time trees and show how to use them to answer count aggregation queries in O(log mN ) I/Os, where N is the number of moving points and m is the maximum degree of the polynomial functions used to represent the position of the points.
A dominance-time tree for point P is a B-tree to index the consecutive time intervals: (-[?
], t1 ), (t1 , t2 ), .
.
.
, (ti , ti+1 ), .
.
.
, (tn , +[?])
such that during each interval (ti , ti+1 ), the rank of P remains unchanged.
The rank of P during these intervals and the ti endpoints of these intervals can be precomputed and stored in the B-tree.
Therefore, the B-tree can find the rank of P for any time instance in (-[?
], +[?]).
Let S be a set of N moving points.
For any point P in S, we may compute (precisely for polynomials up to degree 5  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  Theorem 3.3 Let S be a set of N moving points in kdimensional space.
Let m be a fixed constant and assume that the position of each moving point in each dimension is represented by a polynomial function of degree at most m. Given a point P in S and a time instance t, the DominanceTime Tree for each P [?]
S requires O(N ) space.
Hence the count aggregation problem can be done in O(logB N ) I/Os using a total of O(N 2 ) space.
e1 e2 e3 e4 5  9  18 22  30 35  t  Figure 5.
A time line.
and approximately for higher degree polynomials) a set of n time instances ti (1 <= i <= n) such that during each interval (ti-1 , ti ) the rank of P remains unchanged.
Example 3.4 Suppose in a dominance-time graph, there are four outgoing edges, e1 , e2 , e3 and e4 for a point P .
They are labeled as the following respectively: e1 : (5, 18), (22, 35) e2 : (9, 30) e3 : (0, 9), (22, +[?])
e4 : (0, 22) Figure 5 shows the intervals contained in the labels with thick line segments.
In this case, the B-tree contains the time instances 0, 5, 9, 18, 22, 30, 35 and the following time intervals: (-[?
], 0),(0, 5),(5, 9),(9, 18),(18, 22), (22, 30),(30, 35), (35, +[?])
Definition 3.6 Suppose G is a dominance-time graph for a set of moving points and P is a vertex in G. A DominanceTime Tree TP is a data structure based on a B-tree, which indexes all end points of time intervals contained in the labels of outgoing edges from P .
The leaf node of the dominance-time tree contains a list of consecutive time instances, t1 , t2 , ..., tb , and b + 1 data fields v1 , v2 , ..., vb+1 where b is chosen according to the size of the disk pages.
For each field vi for 1 <= i <= b we store the precomputed rank of P during the interval (ti-1 , ti ).
Given a time instance t, the rank of P can be found by searching the dominance-time tree until we find the leaf node with the interval that contains t. Now we can prove the following.
The preprocessing of the dominance-time tree structure involves computation of polynomial functions.
However, for a moving point which is represented by a polynomial function, it is not difficult to use piecewise linear functions to approximately represent its trajectory.
Using this approximation method, the number of time intervals when the rank of a particular point remain unchanged will remain unchanged.
4.
Max-Count Aggregation Queries Our max-count aggregation algorithm uses a novel data structure built on the concept of domes, which we introduce here as a new type of spatial partition of the dual plane of a set of one-dimensional moving points.
We start this section with a few definitions.
Definition 4.1 Let S be any set of points in the plane.
For any new point Q, we define MaxBelow(Q) to be the maximum number of points below any line that passes through Q.
Definition 4.2 Let S be any set of points in the plane.
Let L be the set of lines that cross at least two points in S or cross at least one point in S and are vertical.
For 0 <= i <= N , we define Li = {l [?]
L|CountBelow(l) + CountOn(l) >= i}, where CountOn(l) is the number of points in S crossed by line l. Definition 4.3 For any line l, let Below(l) be the halfplane below l, or if it is a vertical line, then the half-plane on that side of the line that contains more points.
Let Below(Lk ) be the intersection of the half-planes associated with the lines in Lk .
Let k-dome, denoted as dk , be the boundary of the region Below(Lk ).
The intuition is that any point above dk has a line through it with at least k points below.
Definition 4.4 Layer(k)= {Q|Q [?]
Below(Lk+1 ) and Q [?]
Below(Lk )}.
Example 4.1 We show in Figure 6 a set of seven points.
In this case, L7 is composed of the dotted lines (i.e., the lines crossing P2 P3 , P3 P4 ,P4 P5 and the two vertical lines  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  crossing P2 and P5 ), while L6 is composed of the union of the dotted and dashed lines (i.e, the lines crossing P2 P7 , P3 P5 , P4 P6 , P4 P7 and the two vertical lines crossing P3 and P6 ).
The two thick polygonal lines in the figure are d7 and d6 , respectively, and Layer(6) is the area between them.
Now we prove some properties of the above concepts.
Lemma 4.1 For any i and j such that i <= j, the following hold.
(1) Li [?]
Lj .
(2) Below(Li ) [?]
Below(Lj ).
(3) no point of dome di is above any point of dome dj .
Lemma 4.2 Layer(k) consists of those points that are strictly outside dk and on or inside dk+1 .
Lemma 4.3 Each point belongs to only one layer.
We can now show the following characterization of layers.
Theorem 4.1 Q [?]
Layer(m) - M axBelow(Q) = m. Theorem 4.1 implies that the layers partition the plane in such a way that there is a one-to-one correspondence between any element of the partition and the M axBelow value of the points in that element.
We can use this theorem to build a data structure for efficiently identifying which element of the partition a new point is located in, using the following well-known result from computational geometry.
Theorem 4.2 [15] Point location in an N-vertex planar subdivision can be effected in O(log N ) time using O(N ) storage, given O(N log N ) preprocessing time.
Lemma 4.4 Any dome dk has O(N ) edges.
Lemma 4.5 Let S be any set of N points in the plane and Q a query point.
Then we can find in O(log N ) time using an O(N 2 ) space data structure M axBelow(Q) = m. Lemma 4.6 Let S be a set of N points and Q a query point moving along the x axis.
Let S  and Q be the duals of S and Q, respectively.
Then the following hold.
(1) For any time instance t the moving point Q dominates CountBelow(l) number of points in S, where line l crosses Q and has slope -t. (2) The maximum number of points that Q dominates is M axBelow(Q ).
Finally, we have the following theorem.
Theorem 4.3 The Max-Count aggregation query can be answered using an O(N 2 ) size data structure in O(log N ) query time and O(N 2 log N ) preprocessing time.
The above considers only objects that exist at all times.
Suppose that objects only exist between times t1 and t2 .
That means that only lines passing Q and having slopes be(t ,t ) tween -t2 and -t1 are interesting solutions.
Let Li 1 2 be the modification of Li that allows only lines that have slopes between -t2 and -t1 and cross two or more points or cross only one point and have slopes exactly -t2 or -t1 .
With this modification, we can correspondingly modify the definition of layers.
Then Theorems 4.1 and 4.3 still hold.
5.
Further Work There are several interesting open problems.
We list below a few of these.
1.
Are there count or max-count aggregation algorithms that are more efficient in time or space than our algorithms, or can a tight lower bound be proven for these aggregation problems?
2.
Can the count aggregation algorithm for piecewise linear moving points in one dimension be [?]
extended to higher dimensions while keeping the O( N ) time and O(N 2 ) space in the worst case?
3.
Can the max-count aggregation algorithm in one dimension be extended to higher dimensions while keeping the O(log N ) time and O(N 2 log N ) space in the worst case?
4.
How can we make the data structures dynamic, that is, allow efficient deletions and additions of new moving points?
We have partial solution to this problem when only insertions are considered.
5.
What is the average case of the count and max-count algorithms?
6.
Can the algorithms be improved by considering approximations?
As described in Section 1, approximations for the count aggregation query were considered in the work of [13, 6, 18].
However, there are no approximation algorithms for the max-count aggregation problem.
7.
Moving objects can be represented not only by moving points but also by parametric rectangles [4], by geometric transformation objects [7, 9], or by some other constraint representation [12, 16].
These constraint representations are more general because they also represent the changing (growing, shrinking) shape of the objects over time.
It is possible to consider count and max-count aggregation queries on these more general moving objects.
Is it possible to solve these queries within the same time complexity?
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  P4 P3 P5  P7 P2 P6  P1 layer(6)  d6  d7  Figure 6.
Layer(6) for seven points.
We are also interested in implementations of these algorithms and testing them on real data, for example, aviation data sets, and truck delivery data sets.
References  International Converence on Management of Data, pages 57-64, 2000.
[6] Y.-J.
Choi and C.-W. Chung.
Selectivity estimation for spatio-temporal queries to moving object s. In SIGMOD, 2002.
[1] P. K. Agarwal, L. Arge, and J. Erickson.
Indexing moving points.
In Symposium on Principles of Database Systems, pages 175-186, 2000.
[7] J. Chomicki and P. Revesz.
A geometric framework for specifying spatiotemporal objects.
In Proc.
International Workshop on Time Representation and Reasoning, pages 41-6, 1999.
[2] J. Basch, L. J. Guibas, and J. Hershberger.
Data structures for mobile data.
In SODA: ACM-SIAM Symposium on Discrete Algorithms (A Conference on Theoretical and Experimental Analysis of Discrete Algorithms), 1997.
[8] M. de Berg, M. van Kreveld, M. Overmars, and O. Schwarzkopf.
Computational Geometry: Algorithms and Applications.
Springer Verlag, Berlin, 1997.
[3] J. L. Bentley.
Multidimensional divide-and-conquer.
Communications of the ACM, 23(4), 1980.
[9] S. Haesevoets and B. Kuijpers.
Closure properties of classes of spatio-temporal objects under Boolean set operations.
In Proc.
International Workshop on Time Representation and Reasoning, pages 79-86, 2000.
[4] M. Cai, D. Keshwani, and P. Revesz.
Parametric rectangles: A model for querying and animating spatiotemporal databases.
In Proc.
7th International Conference on Extending Database Technology, volume 1777, pages 430-44.
Springer-Verlag, 2000.
[5] M. Cai and P. Revesz.
Parametric r-tree: An index structure for moving objects.
In Proc.
10th COMAD  [10] P. C. Kanellakis, G. M. Kuper, and P. Z. Revesz.
Constraint query languages.
Journal of Computer and System Sciences, 51(1):26-52, 1995.
[11] G. Kollios, D. Gunopulos, and V. J. Tsotras.
On indexing mobile objects.
In ACM Symp.
on Principles of Database Systems, pages 261-272, 1999.
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  [12] G. Kuper, L. Libkin, and J. Paredaens.
Constraint Databases.
Springer Verlag, 2000.
[13] L. Lazaridis and S. Mehrotra.
Progressive approximate aggregate queries with a multi-resolution t ree structure.
In SIGMOD, 2001.
[14] J. Matousek.
Efficient partition trees.
Discrete Comput.
Geom., 8:315-334, 1992.
[15] F. P. Preparata and M. I. Shamos.
Computational Geometry: An Introduction.
Springer Verlag, New York, 1985.
[16] P. Revesz.
Introduction to Constraint Databases.
Springer Verlag, 2002.
[17] S. Saltenis, C. S. Jensen, S. T. Leutenegger, and M. A. Lopez.
Indexing the positions of continuously moving objects.
In SIGMOD Conference, pages 331-342, 2000.
[18] Y. Tao, J.
Sun, and D. Papadias.
Selectivity estimation for predictive spatio-temporal queries.
In ICDE, 2003.
[19] D. Zhang and V. J. Tsotras.
Improving min/max aggregation over spatial objects.
In ACM-GIS, pages 88-93, 2001.
[20] D. Zhang, V. J. Tsotras, and D. Gunopulos.
Efficient aggregation over objects with extent.
In Symposium on Principles of Database Systems, pages 121-132, 2002.
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE
Towards a Theory of Movie Database Queries Bart Kuijpers University of Limburg (LUC) Department WNI B-3590 Diepenbeek, Belgium bart.kuijpers@luc.ac.be  Jan Paredaens University of Antwerp (UIA) Dept.
of Math.
& Computer Science Universiteitsplein 1 B-2610 Antwerpen, Belgium pareda@uia.ua.ac.be  Dirk Van Gucht Indiana University Computer Science Dept.
Bloomington, IN 47405-4101, USA vgucht@cs.indiana.edu  Abstract We present a data model for movies and movie databases.
A movie is considered to be a 2-dimensional semialgebraic figure that can change in time.
We give a number of computability results concerning movies: it can be decided whether a frame of a movie is only a topologically transformation of another frame; a movie has a finite number of scenes and cuts and these can be effectively computed.
Based on these computability results we define an SQLlike query language for movie databases.
This query language supports most movie editing operations like cutting, pasting and selection of scenes.
1.
Introduction We present a data model for movies and movie databases.
We consider a movie to be an infinite sequence of 2dimensional figures that evolve in time.
Each figure consists of a possibly infinite number of points in the 2-dimensional plane.
A recent and much acclaimed method for effectively representing infinite geometrical figures is provided by the constraint database model, that was introduced by Kanellakis, Kuper and Revesz in their 1990 seminal paper [10] (an overview of the area of constraint databases can be found in [13]).
In this model, a 2-dimensional geometrical figure is finitely represented by means of a Boolean combination of polynomial equalities and inequalities.
These involve polynomials with two real variables that represent the  spatial coordinates of a point in the plane.
The set of points on the upper half of the unit circle, for instance, is in this context given by         fi    fi   In more mathematical terminology, these figures are called semi-algebraic sets and for an overview of their properties we refer to [3, 6].
This way of representing fixed figures can easily be adapted to describe figures that change.
Indeed, we can add a time dimension and consider geometrical objects in 3-dimensional space-time that are described by polynomial equalities and inequalities that also have a time variable fi.
This gives us a data model for movies.
Figure 1 gives an example of a movie, in particular a potential scene from Star Trek.
In this short movie the starship Enterprise remains at a constant position in space and can therefore be described   fi    by formula  fifi   fi  fi     in which fi is lacking.
A fired photon torpedo follows the dotted line (between the moments fi   and fi  fi) an then explodes (depicted as increasing dotted circles, between fi  fi and fi  ).
At the bottom of Figure 1 three frames of the movie are shown: at fi  fi fi and .
The complete movie can be described by the set     fi    fifi      	 fi 	   (      fi   	 fi 	 fi       	 fi   fi  fi  fi 	 )   The movie of Figure 1 can be used to illustrate a number of properties that all movies in this model have in common.
       Figure 1.
USS Enterprise firing a photon torpedo at a (cloaked) Klingon vessel.
For instance, between fi  fi and fi   the movie frames change continuously and all frames (fi  fi 	 ) are, topologically seen, the same.
We will call such a sequence of frames a scene of the movie.
Moments in which the movie changes discontinuously are referred to as cuts in the movie.
In the movie of Figure 1 there is, for instance, a cut at fi  fi: a point changes into an increasing circle.
The movie of Figure 1 has five scenes and six cuts (start and end of the movie included).
We remark that our notion of scene is finer that the cinematographic notion of scene.
We will show that the number of cuts and scenes in a semi-algebraic movie is always finite and that a representation of them by means of polynomial constraints can be effectively computed.
A key ingredient in this computation is a decision procedure for testing whether two movie frames can be topologically transformed into one another, i.e., whether they are homeomorphic.
Although deciding whether two 2-dimensional semi-algebraic sets are homeomorphic is a result that belongs to the mathematical folklore, a written proof of it is not to be found in the mathematical literature [15, 18].
We give a decision procedure and we also generalize it to parameterized frames: there is an algorithm that, given two movie frames that depend on time parameters fi	 and fi , produces a formula built with conjunction and disjunction from formulas of the form   fi  	 and fi    ( 	   constants and   fi ) that expresses, in function of fi 	 and fi , whether the frames are homeomorphic.
Finally, we define an SQL-like language to query movie databases.
This language is based on the above computability results and on a well-known language to query databases in the constraint model, namely the relational calculus augmented with polynomial inequalities [10, 16, 13].
It follows from a result by Tarski that the latter language is also  effective [20] (although variables range over the real continuum).
Our query language supports all basic movie editing operations like selecting scenes that satisfy some condition, composing several scenes into a movie, removing scenes, etc.
It also allows for the manipulation of single scenes and even of single frames.
This paper is organized as follows.
In Section 2, we formally define the notion of movie, frame, scene and cut.
Procedures to decide homeomorphism of frames and to compute the scenes and cuts of a movie are given in Section 3.
In Section 4, we present a query language and discuss expressibility issues.
2.
Movies, Frames, Scenes and Cuts We denote the set of the real numbers by .
In the following we will consider planar figures that change in time.
A moving figure is described by means of an (often infinite)  , where and  represent set of tuples    fi in the spatial coordinates of a point in the 2-dimensional real and fi represents the time coordinate in .
We first plane define the notion of a movie.
Definition 2.1 A movie is a set       fi       	 fi 	 	     fi  where  and 	 are real algebraic numbers and where    fi is a formula built with the logical connectives    from atomic formulas of the form    fi  , with    fi a polynomial with real algebraic coefficients and real variables   fi.
The numbers  and 	 are called the beginning and end of the movie respectively and are de and .
noted by A movie database is a finite set of movies.
   fi       A set    fi    fi       fi is a scene of  if  is a maximal open interval in fi   	 fi 	 	 such that  is continuous in each fi     .
   A point fi  in which  is not continuous is called a cut   in .
fi    fi    	  Figure 2.
An example of a movie.
Figure 2 depicts the movie    fi     fi 	 	  (  fi 	 fi     fi     fi  fi 	 	     fi      fi  fi  	) in the space  .
This movie shows at its beginning (i.e., at fi   fi) a single point in the origin.
Then it shows a disk whose radius increases and later decreases and ends in a point at moment fi  fi, followed by a circle whose radius increases, decreases, increases and then shrinks to a point.
Finally, for   fi 	 	, this movie shows nothing.
fi 	  Definition 2.2 Let  be the movie    fi     	 fi 	 	     fi and let  	 fi  	 	.
The set        fi   is called the frame of the movie  at the moment fi  and is denoted by  Az .
  For the movie of Figure 2, for instance, the frame  A 	 is the origin,   is the closed unit disk and   is the empty set.
We can use this same example to illustrate the notion of a scene.
For  fi  fi  fi, the movie of Figure 2 shows a disk on which is zoomed into and then zoomed out of.
This continuous sequence of frames will be called a scene.
Also for fi  fi  , we have a scene in which a circle is continuously deformed.
Scenes are separated by cuts.
In the following definition these concepts are formalized.
The notion of continuity that we will give may seem rather involved.
It corresponds, however to the intuitive notion of acontinuously changing,a as illustrated by the above example.
   fi   	fi	 Definition 2.3 Let   	     fi be a movie and let  	 fi   	 	.
  is continuous on the right in fi   if there exists an    and a continuous (in fi) series               fi  	 fi 	 fi    of homeomorphisms of such that  Az    for all fi  fi   fi   .
is continuous on the left in fi   is defined similarly.
And  is continuous in fi   if it is continuous both on the right and on the left in fi   .
The movie of Figure 2 is continuous in every fi in the open intervals  fi fi, fi  and  	.
These intervals therefore determine the three scenes of the movie.
There are four cuts in the movie of Figure 2: in fi   fi, fi,  and 	.
Remark that the beginning and the end of a movie are always cuts.
3.
Computability Results In this section, we present two computability results concerning movies.
First, we show that it is decidable whether two (parameterized) frames are homeomorphic.
Next, we will show that a movie has a finite number of scenes and cuts and that formulas describing them can be computed from the formula that defines the movie.
A key lemma in this context is the following.
Lemma 3.1 It is decidable whether two movie frames are isotopic1 , and also whether they are homeomorphic.
Proof (sketch).
Let  and  be two movie frames.
 and  are homeomorphic if and only if  is isotopic to  or to a reflection of  (see, e.g., [7, 14, 19]).
It therefore suffices to prove that it is decidable whether  is isotopic to  .
The algorithm to decide isotopy first computes for   , respectively  the labeled planar graph embedding   as follows.
The nodes of   are the asingular pointsa of  , i.e., the points that do not belong to the topological interior of  or the complement of  , nor to a topologically smooth border of  (see [12] for a formal definition) together with the lowest left most points on each closed curve of the topological border of  on which there is no singular point.
As an illustration, we take the frame  shown in (a) of Figure 3.
The singular points of  are  	   and  .
The closed polygon in the right upper corner of  does not contain a singular point and has more than one most left point.
Of these the lowest is picked:   .
We remark that the singular points can be computed by means of a first-order formula in the theory of the real numbers.
It was shown by Tarski that this theory is effective [20], and symbolic algorithms for the first-order theory of the reals [1, 5, 17] can effectively compute the nodes.
The computation of the other nodes can be performed via a Cylindrical Algebraic Decomposition (CAD) [5] (see also [1]).
1 Isotopic means homeomorphic by an orientation preserving homeomorphism.
fi  fi    fifi fi   fi    fi  fi  fi fifi  A"      fi  fi  fi  (a)  (b)  Figure 3.
A frame of a movie  (a) and its graph  (b).
In the graph   (see (b) in Figure 3) these nodes are labeled with typed labels:   for nodes that belong to  and A  labels for nodes that do not belong to  .
Next, the connected components of the intersection of  with the topological border of  minus the labeled points are computed.
These form edges of   and are labeled with labels of type   .
Similarly,  A  labels are given to the connected components of the border that does not belong to  .
The topological border of a frame can be computed in the first-order theory of the reals.
The computation of connected components of a frame is described in [4, 8, 9].
Finally, the areas formed by the graph embedding are computed and labeled   , respectively A  depending on their containment in  .
Let the sets of labeled nodes, edges and areas be called  ,  and  , respectively.
From results in [11] it follows that  and  are isotopic if and only if there exist bijections 	   fi   , 	  fi   , and 	fi  fi   that map -labels to -labels and  -labels to  -labels and that preserve the clockwise occurrence of edges and areas around each of the labeled nodes.
These conditions can be verified.
It is there fore decidable whether two movie frames are isotopic.
It should be remarked (details omitted) that it can be decided whether two frames  and  are isotopic in polynomial time (in the size of the polynomial constraint formulas that describe  and  ).
Theorem 3.1 Let 	 and  be two movies.
There is an algorithm that on input these two movies produces a formula AA" AAz fi	  fi  built with conjunction and disjunction from formulas of the form   fi  	 and fi    ( 	   constants and   fi ) that expresses, in function of fi 	 and fi , whether the two frames 	A" and Az are isotopic (the same is true for homeomorphic).
Proof (sketch).
Let  be the movie    fi     	 fi 	 	     fi Collins proves that from the formula  	 fi 	 	     fi a Cylindrical Algebraic Decomposition (CAD)  of    can be computed such that  each cell in  entirely belongs to  or to the complement of  [5] (see also [1]).
 induces a CAD   of the  fiplane which in its turn induces a CAD   of the fi-axis.
For the movie of Figure 2 this is illustrated in Figure 4.
The cells of  are points, lines, curves, 2-dimensional surfaces and 3-dimensional areas that are built as stacks on the different cells of  .
The cells of  are the (black and grey) dots, arcs and patches of white space in Figure 4 (compare with Figure 2).
  consists of the grey dots on the fi-axis and the open intervals determined by them as shown in Figure 4.
The cells of  are stacks built on these points and intervals.
It can be easily shown that the movie  is continuous in each fi that belongs to one of the open intervals of   .
So,  remains isotopic in these intervals.
The only possible cuts of the movie are therefore the points of   (grey dots).
The algorithm we seek could therefore work as follows.
Compute, using Collinsas CAD algorithm, both for  	 and  the representatives of all the cells in the induced CAD  .
Let these be 	        	 and 	          respectively.
Decide, using Lemma 3.1, for each pair   in the set 	      	  	        , whether 	 is isotopic to  .
If they are, and if  represents, letas say an interval   fi	  	 of the induced CAD of the fi-axis in  	 and if  represents, letas say a point   in the induced CAD of the fi-axis in  , then we add the conjunction   fi 	  	  fi    as a disjunct to the formula  AA" AAz fi	  fi .
 To illustrate the previous theorem, we give  AA fi	  fi for the movie  of Figure 2:  (    A"       A"    A"   (    A"     A"  fi    Az     Az  fi    A"   fi          Az    A"  Az         Az     Az  fi  )  Az  fi  )  fi   Theorem 3.2 A movie has a finite number of scenes and cuts.
Polynomial constraint formulas describing them can be effectively computed.
   fi    Proof (sketch).
Let  be the movie  	 fi 	 	     fi Consider again a CAD of ,     fi    	  Figure 4.
The induced CADs  (black and grey) and  (grey only) for the movie of Figure 2. as in the proof of the previous theorem.
From the proof of the previous theorem it is clear that there are only a finite number of cuts and scenes.
The cuts are among the points of the induced CAD   .
Not all these points must be cuts, however.
For the example of Figures 2 and 4, the grey dots at fi   fi fi  and 	 are cuts.
The one at fi  	, however, is not.
For what concerns the computation of the scenes and cuts, we first compute the candidate points for cuts (namely, the points in  ).
It remains to be tested whether  is continuous in these points.
Let fi   be a point in  different from the beginning or end of .
The test for continuity of  in fi  is two-fold: 1. test whether the frame  Az is isotopic to two frames Az A  and Az  in the neighboring intervals; 2. test for each   fi      (resp.
 ) whether for each small enough    and each A   there exist a points Az   Az  and AzAz   AzAz  at distance at most A from    such that Az   Az  fi       (resp.
 ) and AzAz  AzAz  fi        (resp.
 ).
The first test can be performed using the techniques outlined in Lemma 3.1.
The second test can be expressed as a sentence in the first-order theory of the reals and is therefore effective [20].
Condition 1 is clearly necessary for continuity.
It is, however, not sufficient.
At a cut, a frame can, for instance, just jump to a different location.
This would not violate Condition 1.
Condition 2 guarantees that there is not a jump.
When it is decided which of the points of   are cuts, the computation of the scenes is straightforward.
Collinsas algorithm produces polynomial constraint formulas for all the cells in a CAD.
The scenes and cuts can therefore also be given by means of their defining polynomial   constraint formula.
4.
Querying Movie Databases In this section, we present an effective SQL-like language to query movie databases: SQL.
This language is based on the computability results of the previous section and on a well-known query language for databases in the constraint databases model, namely the relational calculus augmented with polynomial constraints.
First, we define Definition 4.1 A movie database query is a mapping that   maps every -tuple of movies to a movie.
In the following, we will consider queries that have parameters 	      	 .
The calculus: We will use the relational calculus augmented with polynomial constraints, the calculus for short, as an essential part of SQL.
The calculus was introduced and studied in, e.g., [10, 16] (see also [13]).
A calculus formula    	          	      	   is built from the atomic formulas    fi (  fi     ) and  	         ( a polynomial), the logical connectives    and the quantifiers  .
The calculus formula               (                	   fi), for instance, defines the moments when there appears a circle (as a subset) in movie 	 .
The language SQL: An elementary query in SQL is of the form  fi  	      fi          	 fi 	 	  ,  where  and 	 are real algebraic numbers and  is a condition that can be expressed by means of the usual logical  connectives and quantifiers, calculus expressions, other elementary SQL queries and the following primitives:    	     fi   	 fi 	 	     fi 	      	    , with  a calculus formula and  and  computable 2 functions that work on inputs 	      	  and  	      	  ( a natural number), respectively, and return natural numbers.
This primitive returns the movie consisting of the scenes and cuts in the movie defined by  	 fi 	 	     fi 	      	  whose sequence numbers are  fi 	      	    	      	        	      	  	      	  (in that order and consecutive);          fi   	 fi 	 	     fi 	      	  , with  a calcuIt relus formula and  a natural number.
turns the -th cut in the movie defined by  	 fi 	 	     fi 	      	 ; fi	     fi   	 fi 	 	  	   fi 	      	 A"     fi   	 fi 	 	     fi 	      	 Az , with 	 and  calculus formulas.
It expresses the condition on fi 	 and fi that tells us when the two given frames are isotopic (as discussed in Theorem 3.1);    fi     fi   	 fi 	 	     fi 	      	  with  a calculus formula.
Also     fi   	 fi 	 	     fi 	      	 , with  a calculus formula.
They express the beginning and end of the movie defined by  	 fi 	 	     fi  	      	 .
Furthermore, from elementary queries more complicated queries can be constructed by composition, which we denote by A.
For two movies  	 and  , 	 A  is defined to be the movie consisting of  	 without its last frame, immediately followed by  .
The result of an SQL query is a movie.
The meaning of these queries is the obvious one and will be illustrated by the consequent examples.
The function of the -part of an elementary query is to indicate which of the input movies are under consideration.
It should be noted that some of these primitives are redundant and are only added for ease  of use.
From the proof of Theorem 3.2 it follows that can be expressed in terms of .
Finally, we remark that  fi	     Theorem 4.1 SQL queries are computable.
2 This means computable on the polynomial constraint formulas that define the movies.
Proof (sketch).
Let  be a SQL query.
Given the polynomial constraint formulas of the input movies  	      	 , we can, by Theorems 3.1 and 3.2 replace all the occur,  and  in  by conrences of crete constraint formulas.
Also the beginning and end of movies can be computed (from a CAD, for instance).
 and  can be reTherefore all occurrences of placed by concrete constraint formulas.
This way we obtain a first-order formula over the reals, that can possibly contain quantifiers.
From Tarskias quantifier elimination theorem it follows that these can be eliminated [20].
This yields a polynomial constraint formula for the output of the query .
The output is guaranteed to be a movie by the syntactic condition  	 fi 	 	 that appears in the 	  part of elementary SQL queries.
   	    fi	  fi    We give some examples of SQL queries, that illustrate that all the basic movie editing operations can be performed in SQL: Example 1: aGive all the frames of the movie  	 that are homeomorphic to a circlea is expressible by the elementary query  fi  	      fi  	  fi 	  	 fi  fi 	  	   	   fi  fi	  !   !
 fi 	.
  Example 2: aGive all the scenes of the movie  	 of which all frames are homeomorphic to a circlea is a variation on the query of Example 1 and it is expressible by the elementary query  fi  	      fi  	  fi 	  	 fi  fi 	  	  	 	      fi  fi	  !   !
 fi 	 	   ,   where  is the function that returns the number of scenes of the input movie and  is the identity function of the natural numbers.
Example 3: aRemove scene 2 from movie  	 a is expressed by  fi  	      fi  	  fi 	  	 fi  fi 	  	  	 	     fi    fi  where  is the function that maps 1 to itself and all   to   fi.
fi  Example 4: aGive me the first scene of movie  	 followed by the second scene of movie  followed by movie   a is a query that manipulates complete scenes.
It can be expressed as the composition of the elementary queries given by the following three expressions:     fi  fi  	   	  fi  	   fi 	 	 fi fi 	 fi  fi 	  	 	  fi fi  	 	 fi fi   fi    fi  fi  	     fi 	   fi  	 fi  fi 	  	   fi   	   fi    fi    fi  fi  	   fi  	 fi  fi 	         fi  Example 5: A query of particular interest for the Star Trek movie of the Introduction: aGive all frames of  	 that contain a photon torpedo (i.e., an isolated point)a can be expressed as     fi  	  fi 	 	 fi  fi 	  	   	   fi        (	       fi       !
 	  ! fi        !
          !
   )  Example 6: The following query manipulates the frames of a scene.
It also reverses a complete scene.
aPlay movie  	 at double speed followed by the reversed play of movie  turned upside downa is expressed as the composition of the movies expressed by the following elementary queries  fi  	      fi  	  fi 	 	 fi  fi 	  	   	   fi  fi  	      fi      fi	     	 fi  fi          fi     fi  	  	 	  fi fi   fi  fi 	 	 fi fi  fi  fi  fi 	 	  fi fi  	 	  fi fi  Example 8: The next example manipulates complete scenes without touching their individual frames.
aReturn the movie consisting of the scenes of movie  	 but played in reversed ordera is expressed by  fi  	     where the numbers fi and  stand for the constant functions to fi and .
fi  	   Example 7: The next example manipulates one scene.
aReturn the first half of the first scene of movie  	 a is expressed by     fi  	  fi 	  	 fi  fi 	  	   	 	       fi  where  is the function that maps  to      fi.
Example 9: The last example returns a certain computable scene.
aReturn the middle scene of movie  	 a is expressed by  fi  	      fi  	  fi 	  	 fi  fi 	  	   	 	  fi     fi  where  is the function that maps every natural number to   fi if the number of scenes is odd and to  if it is even.
5.
Discussion and conclusion We have presented a data model for movies and movie databases, in which a movie is considered to be a 2dimensional semi-algebraic figure that can change in time.
We have given a number of computability results concerning movies: homeomorphism and isotopy of movie frames are decidable; a movie has a finite number of scenes and cuts and these can be effectively computed.
Based on these computability results we have defined an SQL-like query language for movie databases.
This query language supports most movie editing operations like cutting, pasting and selection of scenes.
We remark that the presented model is very elementary and that it can be extended and made more suitable for practical applications in many ways.
For instance, the movies that we consider here have frames that are purely black and white (even without variations of grey and certainly without colors).
Movies have one single aimagea, whereas many  movies (e.g., cartoon movies) are multi-layered.
The presented model can be extended to cope with this.
Finally, we remark that our model cannot straightforwardly be applied to existing movies.
The problem of converting cinematographic movies, for instance, into the proposed model is beyond the scope of this paper.
We remark however that a number of 3D animation tools and virtual reality environments work with data that can be readily converted into the constraint model.
3D Studio Max [21] and Virtual Reality Modeling Language (VRML) [2] are examples of such environments.
Acknowledgements.
The authors are grateful to Sofie Haesevoets for helpful comments and suggestions to improve the paper.
References [1] D.S.
Arnon.
Geometric reasoning with logic and algebra.
Artificial Intelligence, 37, pages 37a60, 1988.
[2] G. Bell, A. Parisi, and M. Pesce.
The Virtual Reality Modeling Language.
www.vrml.org/VRLM1.0/vrml10c.html [3] J. Bochnak, M. Coste, and M.-F. Roy.
G eEomeEtrie AlgeEbrique ReEelle.
Springer-Verlag, 1987.
[9] J. Heintz, M.-F. Roy, and P. SolernoE.
Description of the Connected Components of a Semialgebraic Set in Single Exponential Time.
Discrete and Computational Geometry, 6, pages 1a20, 1993.. [10] P.C.
Kanellakis, G.M.
Kuper, and P.Z.
Revesz.
Constraint query languages.
Journal of Computer and System Sciences, 51(1), pages 26a52, August 1995.
[11] B. Kuijpers, J. Paredaens, and J.
Van den Bussche.
Lossless Representation of Topological Spatial Data.
In M. Egenhofer and J.
Herring, editors, Advances in Spatial Databases, 4th International Symposium, SSDa95, volume 951 of Lecture Notes in Computer Science, pages 1a13, Springer-Verlag, 1995.
[12] B. Kuijpers, J. Paredaens, and J.
Van den Bussche.
On topological elementary equivalence of spatial databases.
In F. Afrati and Ph.
Kolaitis, editors, 6th International Conference on Database Theory (ICDT a97), volume 1186 of Lecture Notes in Computer Science, pages 432a446, Springer-Verlag, 1997.
[13] G. Kuper, L. Libkin, and J. Paredaens.
Constraint databases.
Springer-Verlag, 2000.
[14] E.E.
Moise.
Geometric Topology in Dimensions 2 and 3, volume 47 of Graduate Texts in Mathematics.
Springer-Verlag, 1977.
[15] A. Nabutovsky.
Personal communication.
June 1997.
[4] J.
Canny, D. Grigoraev, and N.N.
Vorobjov jr. Finding Connected Components of a Semialgebraic Set in Subexponential Time.
Applicable Algebra in Engineering, Communication and Computing, 2, pages 217a238, 1992.
[5] G.E.
Collins.
Quantifier elimination for real closed fields by cylindrical algebraic decomposition.
In volume 33 of Lecture Notes in Computer Science, pages 134a183.
Springer-Verlag, 1975.
[6] M. Coste.
Ensembles semi-algeEbriques.
In G eEometrie AlgeEbrique ReEelle et Formes Quadratiques, volume 959 of Lecture Notes in Mathematics, pages 109a138.
Springer-Verlag, 1982.
[7] R.H. Cromwell and R.H. Fox.
Introduction to Knot Theory, volume 57 of Graduate Texts in Mathematics.
Springer-Verlag, 1977.
[8] J. Heintz, T. Recio, and M.-F. Roy.
Algorithms in Real Algebraic Geometry and Applications to Computational Geometry.
DIMACS Series in Discrete Mathematics and Theoretical Computer Science, Volume 6, pages 137a163, 1991.
[16] J. Paredaens, J.
Van den Bussche, and D. Van Gucht.
Towards a theory of spatial database queries.
In Proceedings 13th ACM Symposium on Principles of Database Systems, pages 279a288.
ACM Press, 1994.
[17] J. Renegar.
On the computational complexity and geometry of the first-order theory of the reals.
Journal of Symbolic Computation, 13, pages 255a352, 1989.
[18] M.-F. Roy.
Personal communication.
May 1997.
[19] J. Stillwell.
Classical Topology and Combinatorial Group Theory, volume 72 of Graduate Texts in Mathematics.
Springer-Verlag, 1980.
[20] A. Tarski.
A Decision Method for Elementary Algebra and Geometry.
University of California Press, 1951.
[21] 3D Studio MAX.
http://www.max3d.com/.
University of Nebraska - Lincoln  DigitalCommons@University of Nebraska - Lincoln CSE Conference and Workshop Papers  Computer Science and Engineering, Department of  1-1-2003  A New Efficient Algorithm for Solving the Simple Temporal Problem Lin Xu University of Nebraska - Lincoln, lxu@cse.unl.edu  Berthe Y. Choueiry University of Nebraska - Lincoln, choueiry@cse.unl.edu  Follow this and additional works at: http://digitalcommons.unl.edu/cseconfwork Part of the Computer Sciences Commons Xu, Lin and Choueiry, Berthe Y., "A New Efficient Algorithm for Solving the Simple Temporal Problem" (2003).
CSE Conference and Workshop Papers.
Paper 164. http://digitalcommons.unl.edu/cseconfwork/164  This Article is brought to you for free and open access by the Computer Science and Engineering, Department of at DigitalCommons@University of Nebraska - Lincoln.
It has been accepted for inclusion in CSE Conference and Workshop Papers by an authorized administrator of DigitalCommons@University of Nebraska - Lincoln.
Xu & Choueiry in 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL 03).
Pages 212-222.
Editors, Mark Reynolds and Abdul Sattar.
Copyright 2003,IEEE Computer Society Press.
Used by permission.
A New Efficient Algorithm for Solving the Simple Temporal Problem Lin XU and Berthe Y. CHOUEIRY Constraint Systems Laboratory Department of Computer Science and Engineering University of Nebraska-Lincoln lxu|choueiry @cse.unl.edu   1 Abstract  ment [12].
Further, an efficient STP solver is a crucial component for solving the Temporal Constraint Satisfaction Problem (TCSP) because the search process designed by Dechter et al.
[8] for solving the TCSP requires solving an STP at each node expansion.
Thus, the performance of the overall process depends heavily on the performance of solving an STP.
In this paper, we propose a new algorithm, STP-solver, for solving the STP and demonstrate empirically that it constitutes a dramatic improvement over previously used algorithms.
We achieve this by first combining the results developed by Bliek and Sam-Haroud [1] for general Constraint Satisfaction Problems (CSPs) with a new strategy for constraint propagation, which restricts the propagation effort to the triangles of the triangulated constraint network instead of its edges.
Then, we apply the resulting mechanism to solve the STP.
The triangulation of the graph and the convexity of the constraints in the STP guarantee that STP-solver is complete and sound for proving the consistency of the STP and for finding the minimal (and decomposable) network.
This paper is structured as follows.
Section 3 recalls the main properties of a CSP and shows how we use them in our study.
Section 4 discusses the algorithms for solving the STP and explains the advantages of the STP-solver.
Section 5 describes our experiments and results, and summarizes our observations.
Section 6 concludes this paper.
In this paper we propose a new efficient algorithm, the STP-solver, for computing the minimal network of the Simple Temporal Problem (STP).
This algorithm achieves high performance by exploiting a topological property of the constraint graph (i.e., triangulation) and a semantic property of the constraints (i.e., convexity) in light of the results reported by Bliek and Sam-Haroud [1], which were presented for general CSPs and have not yet been applied to temporal networks.
Importantly, we design the constraint propagation in STP-solver to operate on triangles instead of operating on edges and implicitly guarantee the decomposition of the constraint graph according to its articulation points.
We also provide extensive empirical evaluations of all known algorithms for solving the STP on sets of randomly generated problems.
Our experiments demonstrate significant improvements of STPsolver, in terms of number of constraint checks and CPU time, over previously reported algorithms such as the Floyd-Warshall algorithm (F-W) [5; 8], Directed-Path Consistency (DPC) [8], and Partial Path-Consistency (PPC) [1].
          2 Introduction    Many critical applications in planning and scheduling rely on an efficient handling of temporal information represented as a Simple Temporal Problem (STP) [6; 8; 3].
The efficiency of the constraint propagation in such a network is particularly crucial in autonomous space applications as demonstrated by the Deep Space 1 Remote Agent experi-  3 Background A Constraint Satisfaction Problem (CSP) is defined as follows.
Given a set of variables, each 1  termines whether or not a network is path consistent without necessarily producing a tight network as with PC.
Since PPC operates on the edges of the triangulated graph (fewer than those of the complete graph), it realizes significant computational savings, especially in sparse networks.
Minimality: Minimality, the central problem in CSPs, is a property stronger than path consistency.
It guarantees that all the binary constraints are as explicit (i.e., tight) as possible [11].
Decomposability: Decomposability is stronger than minimality and guarantees that a solution to the CSP can be found backtrack-free.
This is a highly desirable property and guarantees the tractability of the CSP.
Consistency: In contrast to the above, the consistency property guarantees only the existence of a solution.
Note that decomposability is a sufficient condition for consistency.
Decomposition into biconnected components: The decomposition of the constraint graph into its biconnected components according to its articulation points3 is a known technique for enhancing the performance of solving a CSP in general.
It provides an upper bound, in the size of the largest biconnected component, to the search effort [9].
We establish that the new solver we introduce, STP, implicitly decomposes the constraint graph into its biconnected components without using articulation point.
This important observation justifies its high performance.
with a set of possible values defining its domain, and a set of constraints that restrict the combinations of values that the variables can be assigned at the same time, the task is to assign a value to each variable such that all constraints are simultaneously satisfied.
Path consistency, as we discuss below, is an important property of a CSP.
Recently Bliek and Sam-Haroud [1] proposed the Partial Path Consistency (PPC) algorithm, which determines whether or not a network is path consistent.
Since PPC operates on the triangulated constraint graph 1 , it realizes significant computational savings over previously known algorithms, especially for sparse networks.
In this paper, we first improve the propagation mechanism of the PPC algorithm by making it operate on triangles instead of individual edges.
We then use the improved version to solve the STP.
In the next section, we recall the main properties of a CSP and discuss them in light of the STP.
3.1 Main CSP properties The general properties of constraint graphs and the main algorithms for achieving them are outlined below.
Path consistency: This property ensures that given two values for any two variables that satisfy the constraint between these variables, we can find values for variables in any path of any length (possibly infinite) that satisfy the constraints along the path [11].
In general CSPs, path-consistency algorithms PC (e.g., PC-1 [11] and PC-2 [10]) are used to enforce path consistency by tightening the binary constraints.
(They also tighten the domains, thus enforcing strong path-consistency.)
Montanari established that these algorithms, which consider only paths of length two, on a complete graph2 guarantee a path-consistent network [11].
The Directional Path-Consistency (DPC) algorithm, which achieves path consistency along a given ordering of the variables in the search process, was proposed by Dechter [7] as an efficient approximation of PC; it guarantees path consistency only in the direction that matters, which is that of search.
Recently Bliek and Sam-Haroud [1] proposed the Partial Path Consistency (PPC) algorithm, which de-    3.2 Properties of the STP  fi 		        '&  1 A graph is triangulated if every cycle of length strictly greater than 3 possesses a chord.
2 If the graph is not complete, it is made so by adding universal constraints between non-adjacent edges.
    A Simple Temporal Problem (STP) is defined by a graph where is a set of vertices representing time points; is a set of edges representing constraints between two time points and ; and is a set of constraint labels for the edges; see Figure 1 (left).
A constraint label of edge is a unique interval , , and denotes a constraint of bounded difference ( .
A Temporal Constraint Satisfaction Problem (TCSP) is defined by a similar graph = , where each edge label = , , is a set of disjoint intervals denoting a disjunction of              	 "!
$#&%  ( ' )  *+	",	 -  .0/2714 365 	/)719 85 ::-: /)714>;<5 =  3 An articulation point of a graph is a vertex whose removal disconnects the graph.
A graph with an articulation point is separable, otherwise it is biconnected.
2      constraints of bounded differences between and , see Figure 1 (right).
We assume that the intervals in 2  e1,2 1  I1,2 = [3, 5]  e1,2 1  component can be solved independently.
If all the components are found to be consistent, then the entire network is consistent.
If any of the components is not consistent, then the overall temporal network is not consistent.
The minimal network of the original problem is obtained by the union of the minimal networks of the individual biconnected components.
When the constraint graph is sparse, this property is particularly attractive.
This allows us to process the components in parallel, by independent agents.
Thus, decomposition into biconnected components is particularly attractive in the case of STPs, especially for large problems with sparse graphs.
We show that this decomposition is implicit and automatic in our STP-solver.
2  I1,2 = {[3, 5], [6, 9], ...}  Figure 1.
Left: STP.
Right: TCSP.
a label are ordered in a canonical way.
In this paper we focus on STPs, but we are integrating our results into an algorithm for solving TCSPs.
Below, we show how we exploit the properties of Section 3.1 in the context of the STP.
Triangulation of network and convexity constraints.
In addition to proposing PPC, Bliek and Sam-Haroud also showed that when the constraints are convex, the PC algorithm (operating on the complete graph) and the PPC algorithm (operating on the triangulated graph) yield the same labeling for the edges common to both graphs.
This important feature of the PPC algorithm has never been exploited before in the context of STPs, in which the constraintsa linear inequalitiesaare indeed convex.
Our STP-solver exploits this result and yields significant savings of the computational efforts over previously available techniques for establishing path consistency of the STP.
Distribution of composition over intersection.
The two operators on binary constraints for establishing path consistency are constraint composition and constraint intersection  .
Montanari showed that when constraint composition is distributive over constraint intersection, PC guarantees not only path consistency but also minimality and decomposabilility [11].
In the case of the STP, constraint composition is interval addition, and constraint intersection is interval intersection, which verify the distributivity as noted by Dechter et al.
[8].
Therefore we can deduce that the PPC algorithm and the STP-solver, guarantee the minimality and decomposability of the STP.
DPC does not guarantee the path-consistency, minimality or decomposability of the constraint network, however, and this is an important feature, it can be used to determine the consistency of the STP.
Decomposition into biconnected components.
In the special case of the TCSP, and a fortiori the STP, Dechter et al.
[8] showed that each biconnected    4 STP algorithms Here we discuss four different algorithms to solve STPs.
The first two solvers, F-W and DPC, have been extensively studied.
However, their performance in combination with a decomposition strategy according to articulation points has never been compared before.
The third STP solver we study is PPC, which has never before been used on temporal reasoning problems.
Finally, we introduce our new solver, STP.
    4.1 F-W & DPC with articulation points The Floyd-Warshall (F-W) algorithm for computing all-pairs shortest-paths is a special case of the PC algorithm.
F-W is applied to the distance graph of an STP to compute its minimal network in  fi .
As discussed in Section 3, DPC is a single pass algorithm and weaker than PC.
It does not necessarily yield a path consistent, minimal, or decomposable network, but it determines if the STP is consistent.
DPC can be more efficient than F-W; instead of  fi , DPC can determine the consistency of STP in  fi	  , where   is the maximum number of parents that a node has in the induced graph along the ordering , which can be substantially smaller than  .
We modify the F-W and DPC algorithms to exploit the existence of articulation points in the temporal network.
First, we identify the biconnected components [5], then we execute a particular STP solver on each component, independently.
This          3     8        yields two algorithms, F-W+AP and DPC+AP, respectively.
It is easy to show that F-W+AP and DPC+AP never check more constraints than F-W and DPC.
In fact, for a sparse network, our experiments show that they check substantially less.
We also show empirically that, even in the absence of articulation points, F-W+AP and DPC+AP almost never require more CPU time than the original algorithms; when they do, the difference is insignificant due to the overhead for finding the articulation points.
PPC was introduced for general CSPs by Bliek and Sam-Haroud [1] who showed that the pathconsistency property can be determined in constraint graphs by triangulating them instead of completing them.
They showed a significant improvement in performance in comparison to PC in sparse networks.
They also established that, for convex constraints, both PPC and PC compute the same labeling for the edges common to both graphs.
Since the constraints in the STP (constraints of bounded difference) are convex, we apply for the first time PPC to solve a continuous domain problem and compute the minimal network of the STP.
As specified in Figure 2, the PPC algorithm starts by triangulating the constraint graph , then iterates over a queue  of all edges, including those edges added to the temporal graph by the triangulation process.
It pops an arbitrary edge fi in from the queue, recovers all triangles  which participates, and updates its label by composing the intervals and and intersecting the result of this composition with the interval .
We slightly modify the original algorithm to allow it to update all three edges at once and to terminate when the queue is empty or inconsistency is found.
The distributivity property of interval addition over interval intersection guarantees that running PPC on an STP results in the tightest possible labeling (i.e., minimal) of the existing edges.
    4.3    ;   	      	   Do    	 ;       ! ;  "     1      5 	     and Enqueue &     When  $% # 1 5     	      Then    '  1 5                 %    (  #     When and Enqueue     1 5  '   	  '    'Then      	 1 5           '    '    '    % When '  #   Then 	 '  and Enqueue 1 	  False5 When   ,   or '  is empty Then consistency  Figure 2.
The PPC algorithm, slightly improved to consider simultaneously all three edges in a triangle.
is not as tight as it could be, given the labels of the other edges in the triangle, the label is tightened accordingly.
This process may require tightening the other edges in the triangle as shown in Figure 3.
In B  B  [2, 6]  [6, 9]  [6, 9]  A  B [2, 7]  [2, 7]     ;  	    Return consistency End  4.2 PPC algorithm for STPs       PPC ( ): Begin consistency True Triangulate ( ) edges in While consistency Do Dequeue( ) is a subgraph of Forall such that  C [2, 12]  A  [6, 9]  C  [8, 12]  A  C  [8, 12]  Figure 3.
An example of updating edges.
The      	  	    label of edge BC then that of AC are updated.
this example we can see that it is worth considering all three edges of a given triangle simultaneously and updating them sequentially.
This observation is the basis of our first improvement to PPC, and is already integrated in the algorithm of Figure 2.
When the label of an edge in a given triangle is updated, PPC triggers constraint propagation over all the triangles in which any of the edges of the original triangle participate.
This is clearly an overkill since only the triangles in which the updated edges participate need to considered.
This observation was the motivation for our new algorithm.
While all existing methods consider the temporal network as composed of edges, our new algorithm considers the STP as composed of triangles (see Figure 4).
The graph of the temporal network  STP algorithm  The goal of PPC is to make the labels of the edges of the triangulated constraint graph as tight as possible.
When the label of an edge in a triangle 4  c  b  A = <a, b, c>  work, using for example the algorithm devised in [13], which may result in new edges.
We add these edges to the original constraint graph as universal   constraints setting their label to .
Then we put all the triangles into a queue, 	 , of size     )4 .
We check every triangle in the fi  is not minimal, queue.
If a given triangle  then we update one or more of its edges.
We then retrieve all the adjacent triangles that contain any of the updated edges and add them to 	 if they are not already there.
Finally, we remove  fi  from the queue, and repeat this process until 	 is empty or inconsistency is found.
C = <b, c, e> d a  E = <c, d, e>  f e  B = <a, b, e>  D = <a, c, e>    Figure 4.
The temporal graph as a graph of triangles.
  4.4 Features of  "	  	    STP  We summarize the features of STP as follows: STP has the same pruning power as F-W with less effort.
STP achieves minimality on the triangulated graph, without requiring the completion of the graph, which is necessary for F-W.
This yields dramatic gains in the computational effort.
STP automatically decomposes the graph into its biconnected components.
The decomposition of the graph into its biconnected components is an effective technique to bind the search effort and enhance the performance of solving a CSP.
Our experiments of Figure 7 and 8 and Table 2 and 3 show how such a strategy can improve the performance of the F-W algorithm, even when the articulation points must be explicitly identified.
Because constraints propagate through triangles, PPC and consequently STP implicitely exploit the decomposition into biconnected components.
Consider a triangulated temporal network composed of two sets of nodes !
= #" , $ , $ , , $&% and ' = " , ( , ( , , () , and " is the articulation point.
Suppose that edges exist only between nodes in either !
or ' .
Since no edges connect these two sets, there obviously are no triangles that connect them.
All triangles are either in set !
or in set ' .
As shown in Figure 4, two triangles in the graph of triangles can only be connected by a common edge.
Therefore, no triangle in set !
is connected to a triangle in set ' .
When PPC and consequently STP propagate constraints through neighboring triangles, no updates in set !
may affect triangles in set ' .
As a result, PPC and STP implicitly guarantee that ar      Proposition 4.1.
A tree-structured STP is decomposable and consistent, and its edge labels are minimal.
  We call our new algorithm STP, although it is applicable to general CSPs and would more correctly be called PPC.
The new algorithm is shown in Figure 5.
First, we triangulate the temporal net      STP ( ): Begin consistency True Triangulate the graph of all triangles in While consistency Do empty list First( )  	    	 	         	        	 ;   	      !
 "     1      5 	     and Enqueue      When  $% # 1 5     	      Then    '  1 5               %   and Enqueue 1       5 When   #  '   	  '    'Then      	    '1   Then  '  5 	  '   and Enqueue  '     When '  % #    1 	 False5 When   ,   or '  is empty Then consistency When consistency     fi Do For 	 fi   	  all triangles  fi  	 fi    containing       For    Do    )            Then Enqueue(     ,    	 Unless  Remove(  ) Return consistency ;    8 ::-: =  .
3 8 ::-: =  .
3    End  Figure 5.
The   6   	  	  is replaced by a graph of triangles.
Each triangle is represented by a node, and two nodes are connected if and only if the triangles they represent have a common edge.
Thus STP appears as an AC3-like algorithm [10] on this graph of triangles.
If an edge of the original constraint graph is not a part of any triangle, it is omitted from the graph of triangles.
Indeed, an edge that does not appear in any triangle has no effect on the constraint propagation in the STP and thus can be safely omitted from the graph of triangles.
Consequently:    ( 	     STP algorithm.
4 Note  5  that  *,+.-0/2143//6587,99:+<;  ticulation points in the graph (if any), are expoited, as if the network was decomposed into its biconnected components without actually decomposing it.
STP is cheaper than PPC.
STP and PPC use the same idea of Bliek and Sam-Haroud [1]; however, STP is more careful about how updates are propagated and thus exploits triangulation of the graph more effectively than PPC.
Although propagation of PPC occurs through triangles, PPC does not have a mechanism to record which triangles really need to be checked.
This inability causes some unnecessary constraint checks and a waste of CPU time.
Our improvement in solving the STP directly benefits the task of solving the TCSP.
TCSP is NP-hard and is solved with backtrack search.
Every node expansion in the search tree needs to solve an STP.
Thus a good STP solver is crucial for solving the TCSP.
We are currently demonstrating this idea and showing how the decomposition into independent components is particularly useful in this context.
  ment point.
The results, measured in terms of the number of constraint checks and CPU time, were averaged over the number of instances and showed a precision of 5%.
The detailed data of the above experiments on the instances generated by GenSTP1 and SPRAND are shown in Table 2 and 3.
The CPU time measurements are made in msec, with a clock resolution of 10 msec.
    5.1 Experiments conducted Using the 50-node problems generated by GenSTP-1, we conducted the following experiments: Managing the queue in STP.
The manner in which triangles are inserted in the queue affects the performance of STP.
We tested three heuristics for adding the triangles to the queue: at the front of queue ( STP-front), at the end of queue ( STP-back), and random insertion into the queue ( STP-random).
All three strategies resulted in the same output (i.e., the same label of the edges).
The results in terms of constraint checks are presented in Figure 6.
The results show that STP-back consistently performs the least number of constraint checks.
This can be informally in          5 Empirical evaluations    We implemented the following six algorithms in Common Lisp.
Floyd-Warshall (F-W), DirectedPath Consistency (DPC), and in combination with a mechanism for detecting and exploiting articulation points, F-W+AP and DPC+AP, Partial Path Consistency (PPC), and our new triangle-based solver ( STP).
We used three generators of random STPs: GenSTP-1, SPRAND, and GenSTP-2.
GenSTP1 is our own generator.
We designed it to guarantee that graphs are connected and that at least 80% of the generated instances are consistent.
SPRAND is one class of STPs generated by the public domain library SPLIB, [4].
All the problems we generate with SPRAND have a cycle connecting all the nodes (i.e., a structural constraint).
This guarantees strong connectivity and the absence of any articulation points.
Finally, GenSTP-2 is a generator given to us by Ioannis Tsamardinos and was used in [14].
GenSTP-2 does not enforce the existence of a structural constraint.
The density of  the temporal     fi	      fi  fi     .
network is defined as  fi( Table 1 summarizes the characteristics of the problems tested, including the size of the instances and the number of samples generated for each measure-    GenSTP-1: 50 nodes    'STP-front  &RQVWUDLQWVFKHFNV           'STP-random   'STP-back                          'HQVLW\  Figure 6.
Constraint   Checks for STP-back and    STP-front, STP-random.
terpreted as follows.
It is more effective to propagate the constraints as early as possible across the network, in a asweepinga manner.
Interestingly, we noticed that quiescence was consistently reached in 7 or fewer iterations.
We use STP-back in the rest of our study.
Computing the minimal network.
F-W, F-W+AP,        6  Table 1.
Parameters of problems generated.
Generator  #Nodes  GenSTP-1  50, 100 50, 100 50 100 100 257 513 256 512  SPRAND  GenSTP-2  Problem size Density Range Step [0.01, 0.1] 0.01 [0.2, 0.9] 0.1 [200, 2000] 200 [400, 1400] 200 [1600, 2800] 400 0.016 0.008 0.016 0.008  #Edges Range Step  768 1536 3 256 = 768 3 512 = 1536    Samples per point 100 100 100 100 100 5 5 5 5  Results Table 2 and Figure 6, 7, 8 Table 3  Figure 9 Figure 9    GenSTP-1: 50 nodes  F-W    GenSTP-1: 50 nodes    F-W      &38WLPHV  &RQVWUDLQWVFKHFNV    F-W+AP      PPC    PPC F-W+AP     'STP        'STP                                        'HQVLW\              'HQVLW\  Figure 7.
Constraint Checks (left) and CPU time (right) for F-W, F-W+AP, PPC, and    STP.
5.2 Observations  PPC and STP (but not DPC) result in the labels of the common edges, the minimal labels.
Figure 7 shows that STP clearly and significantly dominates all others, for all values of density.
  From the above experiments, we draw the following observations: Using articulation points.
Dechter et al.
[8] showed that decomposing the temporal network into its biconnected components is particularly effective in enhancing the performance of search.
It is worth recalling that this decomposition does not affect the quality of the solution: the same edge labels are found with and without decomposition.
Figure 7 and 8 show that only F-W realizes significant savings when the density is low.
In contrast, decompostion into biconnected components does not benefit the DPC solver to the same extent.
This can be explained by the fact that the cost of DPC is bounded by  	   , where  is the maximum number of parents that a   node has in the induced graph.
Decomposition does not significantly change the induced width    ; the total cost of solving the subproblems is not sig-    Saving on the constraint checks.
DPC does not necessarily yield the minimal network, but it can determine whether or not the network is consistent in significantly fewer constraint checks than F-W.
Figure 8 shows that STP, which is more powerful in terms of pruning power and yields the minimal network, dominates DPC-like strategies when density is less than 50%.
  Effect of problem size.
In order to compare the performance of these different solvers on larger problems, we tested them on larger problems generated by SPRAND and GenSTP-2.
Figure 9 and 10 show the ratio of the number of constraint checks and that of the CPU time needed for all six strategies in reference to the values needed for F-W.     7     8      Table 2.
Experimental results for STP solvers on random STP generated by GenSTP-1.
Density 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9  F-W CC CPU (s) 122200.5 0.822 123001.5 0.8347 120339.99 0.8389 120044.01 0.8063 117382.5 0.7935 120075.49 0.8209 120940.51 0.8637 116800 0.7862 115321.5 0.7778 116336.5 0.7947 108926.5 0.7335 120195.99 0.8113 106959.5 0.7213 108896.5 0.7487 109074.99 0.7376 109592 0.7502 107428.51 0.7298 108566.5 0.741  Random STP generated by GenSTP-1 with 50 nodes F-W+AP DPC DPC+AP CC CPU (s) CC CPU (s) CC CPU (s) 29924.05 0.2091 1777.03 0.1168 744.44 0.0307 59091.93 0.4026 3572.7 0.1304 2364.62 0.0683 79195.61 0.5297 4769.95 0.1376 3833.36 0.0945 90934.63 0.6029 6411.11 0.1547 5525.58 0.1176 99076.94 0.6591 8106.14 0.161 7510.24 0.1394 108975.06 0.7251 10204.46 0.1804 9746.2 0.1679 113426.05 0.756 11487.391 0.189 11175.431 0.1818 112267.63 0.7598 11715.94 0.1894 11447.12 0.181 112951.92 0.7525 13024.311 0.1976 12915.95 0.1986 114676.23 0.7617 14072.08 0.2115 13975.311 0.207 108852.99 0.7342 21203.27 0.2717 21203.262 0.2705 120195.99 0.8019 28912.988 0.347 28912.988 0.3442 106959.5 0.7147 27121.85 0.3313 27121.85 0.3252 108896.5 0.732 29731.49 0.3506 29731.49 0.3514 109074.99 0.7314 31533.85 0.3732 31533.85 0.3692 109592 0.7294 32002.16 0.3795 32002.16 0.3725 107428.51 0.7116 32391.83 0.3816 32391.83 0.3719 108566.5 0.7207 33249.992 0.3925 33249.992 0.3796  976155.06 955417 944883 920881.06 931483.06 886372.94 916842 924955.94 935953 895177 883597 860400 833850 879490.06 891914.06 866636 847892 854969  486223.66 737264.25 855073.25 879589.9 918906.56 879934.7 914465.9 924361.94 935805.6 894583 883597 860400 833850 879490.06 891914.06 866636 847892 854969  8.3611 8.2284 7.9927 7.8254 7.8308 7.5324 7.7882 7.907 7.9439 7.7186 7.5387 7.4074 7.1203 7.554 7.6565 7.4485 7.3271 7.3954  Random STP generated by GenSTP-1 with 100 nodes 4.088 21574.19 1.0156 14401.68 0.5275 6.2037 45044.293 1.3432 39022.73 0.9329 7.142 71363.34 1.3655 67750.06 1.2528 7.3463 89384.945 1.4859 87387.805 1.4347 7.71 115620.83 1.7429 114994.93 1.7121 7.3403 116526.336 1.6933 116144.984 1.6616 7.6159 145073.03 1.9288 144846.11 1.9396 7.7039 148479.61 1.9416 148393.72 1.9335 7.7978 167192.17 2.1092 167192.17 2.1225 7.4615 165887.34 2.086 165803.48 2.0561 7.3604 218225.31 2.4666 218225.31 2.4527 7.1667 232372.25 2.5446 232372.25 2.5384 6.9936 240254.4 2.6094 240254.4 2.5553 7.3287 262964.03 2.8133 262964.03 2.7976 7.4904 276184.53 2.9108 276184.53 2.8815 7.3051 267027.4 2.8092 267027.4 2.8233 7.2994 258738.61 2.733 258738.61 2.6769 7.3383 266861.47 2.8032 266861.47 2.7704    PPC CC CPU (s) 273.97 0.0039 837.9 0.0109 1532.55 0.02 2529.68 0.03 3766.13 0.0433 5207.57 0.0599 6679.19 0.0782 7861.92 0.0879 9240.66 0.1031 10857.08 0.1247 23677.2 0.2624 41404.09 0.4637 43483.79 0.4958 53446.668 0.6162 57422.24 0.6662 62265.727 0.7224 64625.727 0.7439 67977.31 0.7931  CC 125.75 409.64 761.71 1270.41 1910.97 2622.19 3445.79 4109 4800.74 5705.62 12631.6 22206.16 23388.791 28504.24 30716.22 33464.38 34257.42 36429.34  CPU (s) 0.0025 0.0045 0.0091 0.0115 0.0188 0.0269 0.0358 0.0424 0.0531 0.0649 0.1533 0.2676 0.291 0.3553 0.4083 0.4269 0.443 0.4616  4424.22 14764.17 31849.158 49463.91 72491.46 85443.125 113607.77 129904.16 161399.25 165634.69 320976.06 396075.3 446748.47 520176.78 564749.56 554381.6 552344.1 568128.25  2225.99 7803.66 16698.209 26350.969 38301.637 45141.34 61303.09 70892.98 86110.63 90790.92 175113.86 219178.31 247012.77 287163 309157.75 303306.12 299997.22 309514.87  0.0108 0.0772 0.1795 0.3076 0.472 0.5847 0.8185 0.9267 1.1857 1.2733 2.6166 3.3071 3.805 4.4565 4.7986 4.7356 4.8764 4.9663  0.0586 0.2035 0.3818 0.5777 0.8411 1.0262 1.3013 1.4633 1.8614 1.9312 3.7723 4.7658 5.4984 6.4435 6.734 6.5875 6.4986 6.7406  STP     nificantly smaller than that of solving the original problem.
When density is high, the network cannot be decomposed, and F-W+AP and DPC+AP perform almost the same as F-W and DPC, respectively.
The problems generated by SPRAND cannot be decomposed because of the existence of a cycle that connects all nodes (i.e., structural constraint).
Indeed, Table 3 shows the same number of constraint checks for the algorithms with and without articulation points.
However, the required effort for finding these articulation points is negligible, as CPU times are the same within the resolution of the clock.
Improvements due to PPC: Given the constraint semantics, PPC is guaranteed to yield the same labels as F-W and F-W+AP on their common edges.
Since PPC operates on the triangulated graph, it performs significantly better for low density values than F-W, which operates on the complete graph, and even F-W+AP, which exploits the existence of articulation points.
When the constraint density increases, the number of triangles in the graph also increases and so does the cost of PPC.
However, the number of constraint checks and, to some extent, the CPU time for PPC remain less than those for F-W and F-W+AP, which quickly reach a stable  value,  fi .
For the larger problems generated by SPRAND and GenSTP-2), Figure 9 and 10 show the PPC outperforms DPC and DPC+AP, which in turn outperform F-W and F-W+AP.
Note, however, that DPC and DPC+AP do not yield the tightest network.
A comparison of Figure 9 and 10 shows that the performance of PPC is better on problems generated by GenSTP-2 than on those generated by SPRAND.
This is due to the existence of a cycle connecting all the nodes in problems generated by SPRAND, which prevents decompositions and causes the triangulation process to add relatively more edges.
Improvements due to STP.
As a refinement of PPC, STP exploits the benefits of triangulation to a greater degree than PPC does.
Experimental results show that STP has always better performance than PPC in all experiments we performed (Figure7 and Table 2 and 3).
For high density values, STP can show a worse performance than DPC (Figure 8).
However, this slight degradation is misleading since it does not account for the output of these two algorithms.
Indeed, STP guarantees the minimal network and DPC does not.
Hence, the performance of the former remains superior.
The           8  Table 3.
Experimental results for STP solvers on random STP generated by SPRAND.
Number of Edges 200 400 600 800 1000 1200 1400 1600 1800 2000  CC 125000 125000 125000 125000 125000 125000 125000 125000 125000 125000  F-W CPU (s) 0.8467 0.8492 0.8441 0.8467 0.8457 0.8521 0.8501 0.8513 0.8553 0.8621  400 600 800 1000 1200 1400 1600 2000 2400 2800  1000000 1000000 1000000 1000000 1000000 1000000 1000000 1000000 1000000 1000000  8.5076 8.5019 8.5177 8.5218 8.6507 8.6643 8.7028 8.7978 8.5296 8.8195  F-W+AP CC CPU (s) 125000 0.8255 125000 0.8301 125000 0.8244 125000 0.8274 125000 0.8281 125000 0.8242 125000 0.8243 125000 0.8331 125000 0.8343 125000 0.8363 1000000 1000000 1000000 1000000 1000000 1000000 1000000 1000000 1000000 1000000  Random STP generated by SPRAND with 50 nodes DPC DPC+AP CC CPU (s) CC CPU (s) 21824.031 0.2798 21824.031 0.2847 30981.5 0.3677 30981.5 0.3732 34524.73 0.4044 34524.73 0.4035 36254.89 0.4255 36254.89 0.4176 37302.24 0.4369 37302.24 0.4318 38020.63 0.4473 38020.63 0.4382 38502.508 0.4556 38502.508 0.4442 38902.95 0.4647 38902.95 0.4458 39166.152 0.4694 39166.152 0.4532 39381.36 0.4577 39381.36 0.4519  Random STP generated by SPRAND with 100 nodes 8.3707 167877.39 2.1703 167877.39 2.1947 8.3572 218599.22 2.5686 218599.22 2.5723 8.3523 245378.12 2.775 245378.12 2.7759 8.3476 263177.97 2.9213 263177.97 2.9205 8.3242 275036.7 3.0351 275036.7 3.0053 8.3216 283548.44 3.0986 283548.44 3.0367 8.3169 289520.4 3.1461 289520.4 3.082 8.3284 298104.53 3.2074 298104.53 3.148 8.3569 303608.8 3.2493 303608.8 3.2088 8.341 307894.12 3.2842 307894.12 3.2199      PPC CC CPU (s) 20247.77 0.236 42313.25 0.4893 56231.418 0.6606 64894.547 0.7594 69790.15 0.825 73899.914 0.8671 76743 0.9067 79116.336 0.927 80540.03 0.9526 81024.4 0.9536  CC 12111.471 25902.35 34142.043 39436.86 42623.07 44889.09 46354.59 47597.69 48321.05 48789.93  STP CPU (s) 0.1595 0.347 0.4656 0.5334 0.5697 0.5796 0.608 0.6287 0.6306 0.6291  144819.36 241016.73 318725.3 380805.94 434212.72 474789.12 512087.9 565111.94 599295 631238.44  85055.414 146966.83 198716.12 236103.58 268235.28 292905.87 313113.25 343748.66 365377.84 382691  1.4427 2.5927 3.441 4.1202 4.6083 4.886 5.4773 6.0293 5.9462 6.1807  1.7659 2.8585 3.7333 4.4388 5.0349 5.4202 6.1406 6.9515 6.7189 7.3478    GenSTP-1: 50 nodes  GenSTP-1: 50 nodes         &38WLPHV  &RQVWUDLQWFKHNV    DPC+AP      DPC       DPC+AP  'STP    'STP  DPC                                           'HQVLW\              'HQVLW\    Figure 8.
Constraint Checks (left) and CPU time (right) for DPC, DPC+AP, and  STP.
of the original TCSP and thus has a lower density than the TCSP.
This supports the importance of an efficient STP solver for low density networks.
We expect the combination of STP with a TCSP solver to improve dramatically the performance of current TCSP solvers.
experiments on large problems, shown in Figure 9 and 10, demonstrate that STP is the absolute winner over all algorithms.
A comparison of Figure 9 and 10 shows that STP, like PPC, is sensitive to the structure of the temporal graph (i.e., the existence of a cycle).
It is more effective on problems generated with GenSTP-2 than on those generated with SPRAND.
      6 Conclusion and future work  5.3 Significance of our results We introduced STP, a new efficient algorithm for solving the STP.
Our algorithm advantageously exploits previous results reported in the literature and binds them via a new strategy for constraint propagation based on triangles.
We demonstrated that this algorithm outperforms all previous ones in terms of pruning power and performance.
We are currently integrating our new STP solver with a TCSP solver to improve the performance of the latter.
More importantly, STP solver provides us   In practice, most real-world applications exhibit typically STPs with large size and low density [2].
The performance of an STP solver in these situations becomes extremely important.
STP is perfect for this kind of job.
Its outstanding performance under low density is particularly advantageous and makes it the best algorithm developed to date.
Further, when solving a TCSP with search, the STP examined at each node in the search tree is a subgraph     9     (       Constraint Checks    (   (    (    (  (  CPU time    (       (  (  GenSTP-2: 256 nodes 512 nodes    5DWLRFRPSDUHWR):  5DWLRFRPSDUHWR):  (    GenSTP-2: 256 nodes 512 nodes  (   '3&$3  ):  ( (  (    (  '3&  ( (    (  (  '3&   33&  9DULRXV673VROYHUV  '673  (  (    '3&$3  ):    ):$3  ( (  (  ):$3    33&  9DULRXV673VROYHUV  '673  Figure 9.
Constraint Checks (left) and CPU time (right) for STP solvers, problems generated by GenSTP-2.
 (  SPRAND: 257 nodes 513 nodes    5DWLRFRPSDUHWR):  5DWLRFRPSDUHWR):    (    Constraint Checks     ( (    ( (    ( (  ( (    '3&  '3&$3    (  (    (  (  (  '3&   33&  (  '3&$3  (   ):  ):$3  9DULRXV673VROYHUV  '673  (  (    (  9DULRXV673VROYHUV  (    (  ):$3  (  CPU time    ):  (  SPRAND: 257 nodes 513 nodes     33&  '673  Figure 10.
Constraint Checks (left) and CPU time (right) for STP solvers, problems generated by SPRAND.
  with a new perspective on temporal problems as composed by a set of triangles, where two triangles are connected if and only if they have one common edge.
Constraint propagation can be carried out according to this new graph of triangles.
We are exploiting this idea to improve the search performance of the TCSP solver.
Proc.
of the 16 IJCAI, pages 456a461, Stockholm, Sweden, 1999.
Acknowledgments: We are indebted to Mark Boddy, Paul Morris, Nicola Muscettola and Ioannis Tsamardinos for sharing data and information on the STP, and to Deb Derrick for editorial help.
This work is supported by a grant from NASA-Nebraska, the CAREER Award #0133568 from the National Science Foundation, and a gift from Honeywell Laboratories.
[ 2]  Mark Boddy.
Personal communication, 2002.
[ 3]  Amedeo Cesta, Angelo Oddi, and Stephen Smith.
A constraint-based method for project scheduling with time windows.
Journal of Heuristics, 8(1):109a136, April 2002.
[ 4]  Boris V. Cherkasskyn, Andrew V. Goldberg, and Tomasz Radzik.
Shortest Paths Algorithms: Theory and Experimental Evaluation.
Mathematical Programming, 73:129a174, 1996.
[ 5]  Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest.
Introduction to Algorithms.
McGraw-Hill Book Co & MIT Press, 2001.
[ 6]  Thomas Dean and Drew McDermott.
Temporal Data Base Management.
Artificial Intelligence, 32:1a55, 1987.
[ 7]  Rina Dechter.
Constraint Processing.
Manuscript, forthcoming, 2003.
References [ 1]  Christian Bliek and Djamilla Sam-Haroud.
Path Consistency for Triangulated Constraint Graphs.
In  10  [ 8]  Rina Dechter, Itay Meiri, and Judea Pearl.
Temporal Constraint Networks.
Artificial Intelligence, 49:61a 95, 1991.
[ 9]  Eugene C. Freuder.
A Sufficient Condition for Backtrack-Bounded Search.
JACM, 32 (4):755a 761, 1985.
[10] Alan K. Mackworth.
Consistency in Networks of Relations.
Artificial Intelligence, 8:99a118, 1977.
[11] Ugo Montanari.
Networks of Constraints: Fundamental Properties and Application to Picture Processing.
Information Sciences, 7:95a132, 1974.
[12] Nicolas Muscettola, Paul Morris, and Ioannis Tsamardinos.
Reformulating Temporal Plans for Efficient Execution.
In Sixth International Conference on Principles of Knowledge Representation and Reasoning (KRa98), pages 444a452, Trento Italy, 1998.
[13] U. KjASrulff.
Triagulation of Graphs - Algorithms Giving Small Total State Space.
Research Report R-90-09, Aalborg University, Denmark, 1990.
[14] Ioannis Tsamardinos.
Reformulating Temporal Plans for Efficient Excution.
Masteras thesis, Intelligent Systems Program, University of Pittsburgh, 1998.
11
Reasoning on Temporal Conceptual Schemas with Dynamic Constraints Alessandro Artale* Dept.
of Computer Science - Free Univeristy of Bozen-Bolzano artale@inf.unibz.it  Abstract This paper formally clarifies the relevant reasoning problems for temporal EER diagrams.
We distinguish between the following reasoning services: (a) Entity, relationship and schema satisfiability; (b) Liveness and global satisfiability for both entities and relationships; (c) Subsumption for either entities or relationships; (d) Logical implication between schemas.
We then show that reasoning on temporal models is an undecidable problem as soon as the schema language is able to distinguish between temporal and atemporal constructs, and it has the ability to represent dynamic constraints between entities.
1.
Introduction Temporally enhanced conceptual models have been developed to help designing temporal databases [12].
In this paper we deal with Extended Entity-Relationship (EER) diagrams1 used to model temporal databases.
The temporal conceptual model ERV T has been introduced both to formally clarify the meaning of the various temporal constructs appeared in the literature [2, 3], and to check the possibility to perform reasoning on top of temporal schemas [4].
ERV T is equipped with both a linear and a graphical syntax along with a model-theoretic semantics.
It supports valid time for entities, attributes, and relationships in the line of T IME ER [10] and ERT [15], while supporting dynamic constraints for entities as presented in MADS [14].
ERV T is able to distinguish between snapshot constructs--i.e.
each of their instances has a global lifespan--and temporary constructs-- i.e.
each of their instances have a limited lifespan.
Dynamic constructs capture the object migration from a source entity to a target entity.
The contribution of this paper is twofold.
Moving from the formal characterization of ERV T given in [3] we clarify the relevant reasoning problems for temporal EER diagrams.
In particular, we distinguish between six different reasoning services, introducing two new services for both entities and relationships: liveness satisfiability--i.e.
whether an entity or relationship admits a non-empty extension infinitely often in the future--and global satisfiability--i.e.
whether an entity or relationship admits a non-empty extension at all points in time.
After a systematic definition of the various reasoning problems we then show that all the satisfiability problems (i.e.
schema, * 1  The author has been partially supported by the EU projects Sewasie, KnowledgeWeb, and Interop.
EER is the standard entity-relationship data model, enriched with ISA links, generalized hierarchies with disjoint and covering constraints, and full cardinality constraints [8].
entity and relationship satisfiability problems) together with the subsumption problem (i.e.
checking whether two entities or relationships denote one a subset of the other so that there is an implicit ISA link between them) can be mutually reduced to each other.
On the other hand, checking whether a schema logically implies another schema is shown to be the more general reasoning service.
The second contribution is to prove that reasoning on temporal conceptual models is undecidable provided the diagrams are able to: (a) Distinguish between temporal and nontemporal constructs; (b) Represent dynamic constraints between entities, i.e.
entities whose instances migrate to other entities.
To the best of our knowledge, this is the first time such a result is proved.
Indeed, the result presented in [4] showed that ERV T diagrams can be embedded into the temporal description logic DLRU S --where U, S extend DLR with the until and since temporal modalities--and that reasoning in DLRU S was undecidable.
Instead, here we prove that even reasoning just on ERV T schemas is undecidable.
The undecidability result is proved via a reduction of the Halting Problem.
In particular, we proceed by first showing that the halting problem can be encoded as a Knowledge Base (KB) in ALC F --where F extends ALC with the future temporal modality--and then proving that such a KB in ALC F can be captured by an ERV T diagram.
Note that, in [9] the undecidability of ALC F is proved using: (a) complex axioms--i.e.
axioms can be combined using Boolean and modal operators--(b) both global and local axioms--i.e.
axioms can be either true at all time or true at some time, respectively.
Since ERV T is able to encode just simple global axioms, we modify the proof presented in [9] by showing that checking concept satisfiability w.r.t.
an ALC F KB made by just simple global axioms is an undecidable problem.
The paper in organized as follows.
The temporal description logic ALC F and the conceptual model ERV T are formally presented in Sections 2 and 3, respectively.
The various reasoning services for temporal conceptual modeling are defined in Section 4 and their equivalence is proved.
That reasoning in presence of dynamic constraints is undecidable is proved in Section 5.
Section 6 makes final conclusions and mention an interesting open problem.
2.
The Temporal Description Logic In this Section we introduce the ALC F description logic [16, 1, 9] as a the tense-logical extension of ALC.
Basic types of ALC F are concepts and roles.
A concept is a description gathering the common properties among a collection of individuals; from a logical point of view it is a unary predicate ranging over the domain of individuals.
Inter-relationships between these individuals are represented by means of roles, which are interpreted as binary  C, D  -  A| >| [?
]| !C | CuD | CtD | [?
]R.C | [?
]R.C | 3+ C | 2+ C |  (atomic concept) (top) (bottom) (complement) (conjunction) (disjunction) (exist.
quantifier) (univ.
quantifier) (Sometime in the Future) (Every time in the Future)  AI(t) >I(t) [?
]I(t) (!C)I(t) (C u D)I(t) (C t D)I(t) ([?
]R.C)I(t) ([?
]R.C)I(t) (3+ C)I(t) (2+ C)I(t)  [?]
= = = = = = = = =  [?
]I [?
]I [?]
[?
]I \ C I(t) C I(t) [?]
D I(t) C I(t) [?]
D I(t) {a [?]
[?
]I | [?
]b.RI(t) (a, b) = C I(t) (b)} {a [?]
[?
]I | [?
]b.RI(t) (a, b) [?]
C I(t) (b)} {a [?]
[?
]I | [?
]v > t.C I(v) (a)} {a [?]
[?
]I | [?
]v > t.C I(v) (a)}  Figure 1.
Syntax and Semantics for the ALC F Description Logic relations over the domain of individuals.
According to the syntax rules of Figure 1, ALC F concepts (denoted by the letters C and D) are built out of atomic concepts (denoted by the letter A) and atomic roles (denoted by the letter R).
Tense operators are added for concepts: 3+ (sometime in the future) and 2+ (always in the future).
Furthermore, while tense operators are allowed only at the level of concepts--i.e.
no temporal operators are allowed on roles--we will distinguish between so called local--RL--and global--RG--roles.
Let us now consider the formal semantics of ALC F .
A temporal structure T = (Tp , <) is assumed, where Tp is a set of time points and < is a strict linear order on Tp --T is assumed to be isomorphic to either (Z, <) or (N, <).
An ALC F temporal interpretation over T is a triple of the form .
I = hT , [?
]I , *I(t) i, where [?
]I is non-empty set of objects (the domain of I) and *I(t) an interpretation function such that, for every t [?]
T , every concept C, and every role R, we have C I(t) [?]
[?
]I and RI(t) [?]
[?
]I x [?
]I .
Furthermore, if R [?]
RG, then, [?
]t1 , t2 [?]
T .RI(t1 ) = RI(t2 ) .
The semantics of concepts is defined in Figure 1--note that the operator 2+ is the dual of 3+ , i.e.
2+ C [?]
!3+ !C.
A knowledge base (KB) in this context is a finite set S of terminological axioms of the form C v D. An interpretation I satisfies C v D if and only if the interpretation of C is included in the interpretation of D at all time, i.e.
C I(t) [?]
DI(t) , for all t [?]
T .
A knowledge base S is satisfiable if there is a temporal interpretation I which satisfies every axiom in S; in this case I is called a model of S. S logically implies an axiom C v D (written S |= C v D) if C v D is satisfied by every model of S. In this latter case, the concept C is said to be subsumed by the concept D in the knowledge base S. A concept C is satisfiable, given a knowledge base S, if there exists a model I of S such that C I(t) 6= [?]
for some t [?]
T , i.e.
S 6|= C v [?].
3.
Temporal Conceptual Modeling In this Section, the temporal EER model ERV T is briefly introduced.
ERV T supports valid time for entities, attributes, and relationships in the line of T IME ER [10] and ERT [15], while supporting dynamic constraints for entities as presented in MADS [14].
ERV T is able to distinguish between snapshot (see the consensus glossary [11] for the terminology used) constructs--i.e.
each of their instances has a global lifespan-- temporary constructs--i.e.
each of their instances have a limited lifespan--or implicitly temporal constructs--i.e.
their instances can have either a global or a temporary existence.
Two temporal marks, S (snapshot) and VT (valid time, i.e.
temporary), are introduced in ERV T to capture such temporal behavior.
Dynamic constructs capture the object migration from a source entity to a target entity.
If there is a dynamic extension between a source and a target entity (represented in ERV T by a dotted link labeled with DEX) models the case where instances of the source entity eventually become instances of the target entity.
On the other hand, a dynamic persistency (represented in ERV T by a dotted link labeled with PER) models the dual case of instances persistently migrating to a target entity (for a complete introduction on ERV T with a worked out example see [3]).
ERV T is equipped with both a linear and a graphical syntax along with a model-theoretic semantics as a temporal extension of the EER semantics [6].
Presenting the ERV T linear syntax, we adopt the following notation: given two sets X, Y , an X-labeled tuple over Y is a function from X to Y ; the labeled tuple T that maps the set {x1 , .
.
.
, xn } [?]
X to the set {y1 , .
.
.
, yn } [?]
Y is denoted by hx1 : y1 , .
.
.
, xn : yn i, and T [xi ] = yi .
In the following definition we refer to Figure 2 to show the visual syntax associated to the various ERV T constructs.
Definition 3.1 (ERV T Syntax).
An ERV T schema is a tuple: S = (L, REL , ATT, CARD , ISA, DISJ , COVER, S, T, KEY, DEX, PER), such that L is a finite alphabet partitioned into the sets: E (entity symbols), A (attribute symbols), R (relationship symbols), U (role symbols), and D (domain symbols).
We will call the tuple (E, A, R, U, D) the signature of the schema S. E is further partitioned into: a set E S of snapshot entities (the S-marked entities in Figure 2), a set E I of implicitly temporal entities (the unmarked entities in Figure 2), and a set E T of temporary entities (the VT-marked entities in Figure 2).
A similar partition applies to the set R. ATT is a function that maps an entity symbol in E to an Alabeled tuple over D, ATT(E) = hA1 : D1 , .
.
.
, Ah : Dh i. REL is a function that maps a relationship symbol in R to an U-labeled tuple over E, REL(R) = hU1 : E1 , .
.
.
, Uk : Ek i, and k is the arity of R. CARD is a function E x R x U 7- N x (N [?]
{[?]})
denoting cardinality constraints.
If REL(R) = hU1 : E1 , .
.
.
, Uk : Ek i, then CARD (E, R, U ) is defined only if U = Ui and E = Ei , for some i [?]
{1, .
.
.
, k}.
We denote with CMIN (E, R, U ) and CMAX (E, R, U ) the first and second component of CARD .
If not stated otherwise, CMIN is assumed to be zero, and CMAX is assumed to be [?].
In Figure 2, CARD (TopManager, Manages, man) = (1, 1).
ISA is a binary relationship ISA [?]
(E x E) [?]
(R x R).
ISA between relationships is restricted to relationships with the same arity.
ISA is visualized with a directed arrow, e.g.
Manager ISA Employee in Figure 2.
PaySlipNumber(Integer) Salary(Integer)  VT  emp  Name(String)  Employee S  Works-for VT  S  (1,n) act Manager  VT  Project  ProjectCode(String)  prj (1,n)  Resp-for S  (1,1) prj AreaManager  TopManager  man (1,1)  org  Manages  OrganizationalUnit S  DEX d  Department S  InterestGroup  Figure 2.
An ERV T diagram are binary relations over 2E x E, describing disjointness and covering partitions, respectively.
DISJ is visualized with a circled "d" and COVER with a double directed arrow, e.g.
Department, InterestGroup are both disjoint and they cover OrganizationalUnit.
S , T are binary relations over E x A containing, respectively, the snapshot and temporary attributes of an entity.
Furthermore, if hE, Ai [?]
S, T, then A is between the attributes in ATT(E) (see S , T marked attributes in Figure 2).
KEY is a function that maps entity symbols in E to their key attributes, KEY(E) = A.
Furthermore, if KEY(E) = A, then A is between the attributes in ATT(E).
Keys are visualized as underlined attributes.
Both DEX and PER are binary relations over E x E describing the dynamic evolution of entities2 .
DEX and PER are visualized with dotted directed lines labeled with DEX or PER, respectively (e.g.
AreaManager DEX TopManager).
DISJ , COVER  The model-theoretic semantics associated with the ERV T modeling language adopts the snapshot3 representation of abstract temporal databases and temporal conceptual models [7].
Following this paradigm, the flow of time T = hTp , <i, where Tp is a set of time points (or chronons) and < is a binary precedence relation on Tp , is assumed to be isomorphic to either hZ, <i or hN, <i.
Thus, a temporal database can be regarded as a mapping from time points in T to standard relational databases, with the same interpretation of constants and the same domain.
Definition 3.2 (ERVS T Semantics).
Let S be an ERV T schema, and BD = Di [?
]D BDi be a set of basic domains such that BDi [?]
BDj = [?]
for i 6= j.
A temporal database state B(t) for the schema S is a tuple B = (T , [?
]B [?]
[?
]B ), D, * B B such that: [?]
is a nonempty set disjoint from [?]
D; S B is the set of basic domain values used in = [?]
[?
]B Di D Di [?
]D B the ac[?]
BD --we call [?]
the schema S such that [?
]B i Di Di tive domain; *B(t) is a function such that for each t [?]
T , every domain symbol Di [?]
D, every entity E [?]
E, every relationship R [?]
R, and every attribute A [?]
A, we have: 2 3  For ISA relations, we use the notation E1 ISA E2 instead of hE1 , E2 i [?]
ISA.
Similarly for DISJ, COVER, DEX, PER.
The snapshot model represents the same class of temporal databases as the timestamp model [12, 13] defined by adding temporal attributes to a relation [7].
B(t)  B(t) Di = [?
]B [?]
[?
]B , RB(t) is a set of U-labeled tuDi , E B B(t) ples over [?]
, and A [?]
[?
]B x [?
]B D. B is a legal temporal database state if it satisfies all of the integrity constraints expressed in the schema: B(t)  * For each E1 , E2 [?]
E, if E1 ISA E2 , then, E1 * For each R1 , R2 [?]
R, if  B(t) R1 ISA R2 , then, R1  B(t)  [?]
E2 [?]
.
B(t) R2 .
* For each E [?]
E, if ATT(E) = hA1 : D1 , .
.
.
, Ah : Dh i, then, e [?]
E B(t) - ([?
]i [?]
{1, .
.
.
, h}, [?
]!ai .
he, ai i [?]
B(t) B(t) Ai [?]
[?
]ai .he, ai i [?]
Ai - ai [?]
[?
]B Di ).
* For each R [?]
R, if REL(R) = hU1 : E1 , .
.
.
, Uk : Ek i, then, r [?]
RB(t) - (r = hU1 : e1 , .
.
.
, Uk : ek i [?]
[?
]i [?]
B(t) {1, .
.
.
, k}.ei [?]
Ei ).
In the following, we adopt the convention: hU1 : e1 , .
.
.
, Uk : ek i [?]
he1 , .
.
.
, ek i, and r[Ui ] [?]
r[i] to denote the Ui /i-component of r. * For each cardinality constraint CARD (E, R, U ), then, e [?]
E B(t) - CMIN (E, R, U ) <= #{r [?]
RB(t) | r[U ] = e} <= CMAX(E, R, U ).
* For each snapshot entity E [?]
E S , then, 0 e [?]
E B(t) - [?
]t0 [?]
T .e [?]
E B(t ) .
* For each temporary entity E [?]
E T , then, 0 e [?]
E B(t) - [?
]t0 6= t.e 6[?]
E B(t ) .
* For each snapshot relationship R [?]
RS , then, 0 r [?]
RB(t) - [?
]t0 [?]
T .r [?]
RB(t ) .
* For each temporary relationship R [?]
RT , then, 0 r [?]
RB(t) - [?
]t0 6= t.r 6[?]
RB(t ) .
* For each entity E [?]
E with a snapshot attribute Ai , i.e.
hE, Ai i [?]
S, then, B(t) B(t0 ) (e [?]
E B(t) [?]
he, ai i [?]
Ai ) - [?
]t0 [?]
T .he, ai i [?]
Ai .
* For each entity E [?]
E with a temporary attribute Ai , i.e.
hE, Ai i [?]
T, then, B(t) B(t0 ) (e [?]
E B(t) [?
]he, ai i [?]
Ai ) - [?
]t0 6= t.he, ai i 6[?]
Ai .
* For E, E1 , .
.
.
, En [?]
E, - If {E1 , .
.
.
, En } DISJ E, then, [?
]i [?]
{1, .
.
.
, n}.Ei ISA E[?]
B(t) B(t) [?
]j [?]
{1, .
.
.
, n}, j 6= i.Ei [?]
Ej = [?].
- If {E1 , .
.
.
, En } COVER E, then, Sn B(t) [?
]i [?]
{1, .
.
.
, n}.Ei ISA E [?]
E B(t) = i=1 Ei .
2.
Schema satisfiability reduces to entity liveness satisfiability;  * For each E [?]
E, A [?]
A such that KEY(E) = A, then, hE, Ai i [?]
S--i.e.
a key is a snapshot attribute--and [?
]a [?]
B(t) [?
]B | he, ai [?]
AB(t) } <= 1.
D .#{e [?]
E * For each E1 , E2 [?]
E, B(t)  - [?
]t1 > t.e [?]
E2  B(t)  - [?
]t0 > t.e [?]
E2  - If E1 DEX E2 , then, e [?]
E1 - If E1 PER E2 , then, e [?]
E1  B(t1 )  ;  B(t0 )  .
4.
Reasoning on Temporal Models Reasoning tasks over a temporal conceptual model include verifying whether an entity, relationship, or schema are satisfiable, whether a subsumption relation exists between entities or relationships, or checking whether a new schema property is logically implied by a given schema.
The model-theoretic semantics associated with ERV T allows us to formally define these reasoning tasks.
We start with the formal definition of the relevant reasoning services in a temporal schema as presented in [3].
Based on this formal characterization we can prove the first results of this paper concerning reasoning in ERV T : a) Subsumption and satisfiability reasoning services relative to entities are mutually reducible to each other; b) Satisfiability problems relative to relationships are mutually reducible; c) Satisfiability of relationships reduces to satisfiability of entities and viceversa; d) Logical implication is the more general service.
Definition 4.1 (Reasoning in ERV T ).
Let S be an ERV T schema, E [?]
E an entity, and R [?]
R a relationship.
The following are the reasoning tasks over S: 1.
E (R) is satisfiable if there exists a legal temporal database state B for S such that E B(t) 6= [?]
(RB(t) 6= [?
]), for some t [?]
T ; 2.
E (R) is liveness satisfiable if there exists a legal temporal database state B for S such that [?
]t [?]
T .[?
]t0 > 0 0 t.E B(t ) 6= [?]
(RB(t ) 6= [?
]), i.e.
E (R) is satisfiable infinitely often; 3.
E (R) is globally satisfiable if there exists a legal temporal database state B for S such that E B(t) 6= [?]
(RB(t) 6= [?
]), for all t [?]
T ; 4.
S is satisfiable if there exists a legal temporal database state B for S that satisfies at least one entity in S (B is said a model for S); 5.
E1 (R1 ) is subsumed by E2 (R2 ) in S if every legal temporal database state for S is also a legal temporal database state for E1 ISA E2 (R1 ISA R2 ); 6.
A schema S0 is logically implied by a schema S over the same signature if every legal temporal database state for S is also a legal temporal database state for S0 .
We now prove that reasoning services (1-5) relative to entities and knowledge bases are mutually reducible to each other.
Proposition 4.2.
There is a mutual reducibility between the reasoning services (1-5) relative to entities in ERV T .
Proof Proving the mutual reducibility between satisfiability and subsumption in ERV T can be done similarly to [5].
Then, in the following we prove that given an ERV T schema S: 1.
Entity satisfiability reduces to schema satisfiability;  3.
Entity liveness satisfiability reduces to entity global satisfiability; 4.
Entity global satisfiability reduces to entity satisfiability.
(1) We prove that given an entity E0 [?]
E, then, E0 is satisfiable w.r.t.
S iff a new schema S0 is satisfiable.
S0 is obtained by adding to S the schema in figure 3(a), where >, E1 , E2 are new entities such that [?
]E [?]
E.E ISA >, and R is a new binary relationship.
"=" Let S0 be satisfiable, then, S0 has a model B (which is a model for S, too) such that [?
]t [?]
T .[?
]e [?]
[?
]B .e [?]
>B(t) (by definition of schema satisfiability and by construction of > as superclass of all entities in S0 ).
Because > is a snapshot entity, then, [?
]t [?]
T .e [?]
>B(t) .
Since E1 , E2 form a disjoint covering of >, and E1 , E2 B(t0 ) are both temporary, then, [?
]t0 [?]
T .e [?]
E1 .
Finally, because E1 totally participates in R, then, [?
]e0 [?]
0 B(t0 ) [?
]B .
(he, e0 i [?]
RB(t ) [?]
e0 [?]
E0 ).
Then, E0 is satisfiable w.r.t.
S. "=" Let E0 be satisfiable w.r.t.
S, then, S has a model B(t ) B such that [?
]t0 [?]
T .[?
]e0 [?]
[?
]B .e0 [?]
E0 0 .
We now 0 0 0 construct a model B for S .
Let B and B coincide on all constructs in S, and additionally, for all t [?]
T : S S 0 * >B (t) = v[?
]T E[?
]E E B(v) *  B0 (t) E1 B0 (t)  * E2  =    0  >B (t) [?]
0  if t = t0 otherwise B0 (t)  = >B (t) \ E1  B0 (t)  0  * RB (t) = {he, e0 i | e [?]
E1  }  0  It is easy to check that B is a model for S0 , then, S0 is satisfiable.
(2) We prove that a given schema S is satisfiable iff an entity is liveness satisfiable w.r.t.
a new schema S0 .
S0 is obtained by adding to S the schema in figure 3(b), where >1 , >2 , E1 , E2 are new entities and R is a new binary relationship.
Furthermore, {E | E [?]
E} COVER >2 .
In particular, we prove that S is satisfiable iff >1 is liveness satisfiable w.r.t.
S0 .
"=" Let >1 be liveness satisfiable w.r.t.
S0 .
Then, S0 has a model, B, such that [?
]t [?]
T .[?
]t0 > t.[?
]o [?]
B(t0 ) [?
]B .o [?]
>1 .
Since >1 is a snapshot entity, then, B(t) o [?]
>1 , for all t [?]
T .
Because E1 , E2 are a disjoint covering of >1 and they are both temporary, then, B(t) [?
]t [?]
T .o [?]
E1 .
Because E1 totally participates in R, B(t) then, [?
]e [?]
[?
]B .
(ho, ei [?]
RB(t) [?]
e [?]
>2 ).
Then, >2 is a satisfiable entity and, because of the covering constraint, S is satisfiable.
"=" Let S be a satisfiable schema and B a model for S. We now show how to build a model, B 0 , for S0 such that >1 is liveness satisfiable.
B 0 agrees with B on all constructs in S, and additionally, for all t [?]
T : S B0 (t) B(v) * >1 = v[?
]T >2 Note that, because by assumption S is satisfiable, then, >2 is satisfiable while >1 contains always at least one element (i.e., it is globally, and then liveness, satisfiable).
>  >1  S  d  E2  d  E1  VT  S  (1,n)  E0  R  VT  E2  (a)  VT  E1  (1,n) VT  R  >2  (b)  Figure 3.
Reductions: (a) From Entity Sat to Schema Sat; (b) From Schema Sat to Entity Liveness Sat.
* Let t0 [?]
T an arbitrary time such that [?
]e0 [?]
B(t0 ) [?
]B .e0 [?]
> then: 2 B0,(t) 0 B (t) >1 if t = t0 E1 = [?]
otherwise 0  B (t)  * E2  0  B (t)  = >1  0  B (t)  \ E1  B0 (t)  0  * RB (t) = {he, e0 i | e [?]
E1 0  }  0  Then, B is a model of S such that >1 is liveness satisfiable.
(3) We prove that given an entity E0 [?]
E, then, E0 is liveness satisfiable w.r.t.
S iff an entity is globally satisfiable w.r.t.
a new schema S0 .
S0 is obtained by adding to S the new entity E1 as showed in figure 4(a).
We prove that E0 is liveness satisfiable w.r.t.
S iff E1 is globally satisfiable w.r.t.
S0 .
"=" Let E1 be globally satisfiable w.r.t.
S0 .
Then, S0 B(t) has a model, B, such that [?
]t [?]
T .[?
]o [?]
[?
]B .o [?]
E1 .
Then, given the dynamic extension constraint between E1 and E0 , E0 is liveness satisfiable.
"=" Let E0 be liveness satisfiable w.r.t.
S. Then, S has a model, B, such that [?
]t [?]
T .[?
]t0 > t.[?
]e [?]
[?
]B .e [?]
B(t0 ) E0 .
We now extend B to E1 , such that for all t [?]
T : *  B(t) E1  B  0  = {e [?]
[?]
| [?
]t > t.e [?]
B(t0 ) E0 }  Then, B is a model of S0 such that E1 is globally satisfiable.
(4) We prove that given an entity E0 [?]
E, then, E0 is globally satisfiable w.r.t.
S iff an entity is satisfiable w.r.t.
a new schema S0 .
S0 is obtained by adding to S the schema in figure 4(b), where E1 is new snapshot entity and R is a new binary relationship.
"=" Let E1 be satisfiable w.r.t.
S0 , then, S0 has a B(t) model B such that [?
]o [?]
[?
]B .o [?]
E1 , for all t [?]
T (by construction of E1 as a snapshot entity).
Since E1 totally participates in R, then, [?
]e [?]
[?
]B .ho, ei [?]
RB(t) [?]
e [?]
B(t) E0 .
Since this must be true at all time, then, E0 is globally satisfiable w.r.t.
S. "=" Let E0 be globally satisfiable w.r.t.
S. Then, S B(t) has a model, B, such that [?
]t [?]
T .[?
]e [?]
[?
]B .e [?]
E0 .
0 0 We now construct a model, B , for S such that E1 is satisfiable.
B 0 agrees with B on all constructs in S, and additionally, for all t [?]
T : S B0 (t) B(v) * E1 = v[?
]T E0 * R  B0 (t) 0  = {ho, ei | o [?]
B0 (t) E1  [?]e[?]
B(t) E0 }  0  Then, B is a model of S such that E1 is satisfiable.
2  We are now able to prove that satisfiability problems for relationships are reducible to the same problems for entities and viceversa.
Proposition 4.3.
There is a mutual reducibility between the reasoning services (1-4) relative to both relationships and entities in ERV T .
Proof We only prove that satisfiability of relationships can be reduced to satisfiability of entities and viceversa.
The other mutual reductions easily follow from analogous results proved in Proposition 4.2.
"R SAT reduces to E SAT."
We can verify whether a relationship R is satisfiable in S by adding a new entity, say AR such that: (a) AR ISA E, with E an arbitrary entity participating in the relationship, and (b) AR totally participates in the relationship.
Then, R is satisfiable if and only if AR is satisfiable.
"E SAT reduces to R SAT."
We can verify whether an entity E is satisfiable in S by adding a new relationship, say RE such that: (a) RE is a binary relationship with both arguments restricted to E; (b) E totally participates in RE .
Easily follows that E is satisfiable if and only if RE is satisfiable.
2 Finally, we show that all the reasoning problems can be reduced to a logical implication problem.
Logical implication accounts for checking properties of a schema whenever they can be expressed in the ERV T schema language.
In particular, checking whether an entity E is satisfiable can be reduced to logical implication by choosing S0 = {E ISA A, E ISA B, {A, B} DISJ C}, with A, B, C arbitrary entities.
Then, E is satisfiable iff S 6|= S0 .
Given the result of Proposition 4.2, then the reasoning services (1-5) for entities are reducible to logical implication.
Furthermore, given two relationships R1 , R2 , checking for sub-relationship can be reduced to logical implication by choosing S0 = {R1 ISA R2 }.
This shows that logical implication is the most general reasoning service.
5.
Reasoning on ERV T is Undecidable We now show that reasoning on full ERV T is undecidable.
The proof is based on a reduction from the undecidable halting problem for a Turing machine to the entity satisfiability problem w.r.t.
an ERV T schema S. We apply ideas similar to [9] (Sect.
7.5) to show undecidability of certain product of modal logics.
The proof can be divided in the following steps: 1.
Definition of the halting problem; 2.
Reduction of the halting problem to concept satisfiability problem w.r.t.
an ALC F KB; 3.
Reduction of concept satisfiability w.r.t.
an ALC F KB to entity satisfiability w.r.t.
an ERV T schema.
E1  DEX  E0  (a)  E1  (1,n) S  R  E0  (b)  Figure 4.
Reductions: (a) From Entity Liveness Sat to Entity Global Sat; (b) From Entity Global Sat to Entity Sat.
The second step has been chosen as an intermediate step to better understand the halting problem reduction by using the concise ALC F linear syntax.
Then, the final step will show how ERV T is able to capture the ALC F axioms used in the reduction.
Halting problem We show here a formal representation of the halting problem for Turing machines as presented in [9].
A single-tape right-infinite deterministic Turing machine M is a triple hA, S, ri, where: A is the tape alphabet (b [?]
A stands for blank); S is a finite set of states with the initial state, s0 , and the final state, s1 ; r is the transition function, r : (S - {s1 }) x A - S x (A [?]
{L, R}).
A Configuration of M is an infinite sequence: hPS, a1 , .
.
.
, ai-1 , hsi , ai i, .
.
.
, an , b, .
.
.i, where, PS 6[?]
A is a symbol marking the left end of the tape, ai [?]
A, and si [?]
S is the current state.
The cell hsi , ai i is the active cell.
All the cells to the right of an are blank.
Since a transition function can only modify the active cell and its neighbors we introduce the instruction function, d, defined on triples in (A[?
]{PS})x((S -{s1})xA)xA, such that:  hai , hs0 , a0j i, ak i if r(s, aj ) = hs0 , a0j i    0 0    hhs , ai i, aj , ak i if r(s, aj ) = hs , Li and ai 6= PS d(ai , hs, aj i, ak ) = hPS, hs0 , aj i, ak i if r(s, aj ) = hs0 , Li     and ai = PS   hai , aj , hs0 , ak ii if r(s, aj ) = hs0 , Ri  A sequence hc0 , c1 , .
.
.
, ck , ck+1 , .
.
.i of configurations of M is said a computation of M if the state of c0 is s0 (the initial state), and, for all k, ck+1 is obtained from ck by replacing the triple centered around the active cell of ck by its d-image and living the rest unaltered.
We say that M halts, starting with the empty tape--i.e.
with starting configuration: hPShs0 , bi, b, .
.
.
, b, .
.
.i--if there is a finite computation, hc0 , c1 , .
.
.
, ck i, such that the state of ck is s1 (the final state).
Reasoning on ALC F is undecidable Using a reduction from the halting problem we now prove that reasoning involving an ALC F knowledge base is undecidable.
In [9] the undecidability of ALC F is proved using: (a) complex axioms--i.e.
axioms can be combined using Boolean and modal operators--(b) both global and local axioms--i.e.
axioms can be either true at all time or true at some time, respectively.
Since ERV T is able to encode just simple global axioms, we modify the proof presented in [9].
The following theorem proves that checking concept satisfiability w.r.t.
an ALC F KB made by just simple global axioms is an undecidable problem.
Proposition 5.1.
Concept satisfiability w.r.t.
an ALC F knowledge base is undecidable.
Proof Given a Turing machine, M = hA, S, ri, we construct an ALC F KB, say KBM , with a concept that is satisfiable w.r.t.
KBM iff the machine M does not halt.
We start by introducing some shortcuts.
The implication, C - D, is equivalent to the concept expression !C t D. Given two concepts C, D we define next(C, D) as the following axiom: C v 3+ D u !3+ 3+ D. This axiom says that whenever o [?]
C I(t0 ) , then, o [?]
DI(t0 +1) [?]
[?
]t 6= t0 .o 6[?]
C I(t) .
Let C, D1 , .
.
.
, Dn concepts, discover(C, {D1 , .
.
.
, Dn }) is defined as the conjunction of the following axioms: C v D1 t .
.
.
t Dn D1 v C u !D2 u .
.
.
u !Dn ... Dn-1 v C u !Dn i.e., there is a disjoint covering between C and D1 .
.
.
Dn .
Let A0 = A [?]
{PS} [?]
(S x A).
With each x [?]
A0 we introduce a concept Cx .
We also use concepts Cs , Cl , Cr to denote the active cell, its left and right cells, respectively.
The concept S1 denotes the final state.
The halting problem reduces to satisfiability of C0 .
Extra concepts C, D1 , D2 , D3 , will be also used.
R is a global role.
KBM contains the following axioms: C0 v CPS u 3+ Chs0 ,bi discover(C, {Cx | x [?]
A0 }) > v [?
]R.> next(CPS , D1 ) next(D1 , D2 ) Chs0 ,bi v D1  (1) (2) (3) (4) (5) (6)  Chs0 ,bi v 2+ Cb discover(Cs , {Chs,ai | hs, ai [?]
S x A}) next(Cl , Cs ) next(Cs , Cr ) next(Cr , D3 ) CPS v Cl t 3+ Cl Cl v Ca - [?
]R.Ca0 Cs v Cb - [?
]R.Cb 0 Cr v Cg - [?
]R.Cg 0 Ca v (!Cl u !Cs u !Cr ) - [?
]R.Ca , [?
]a [?]
A [?]
{PS} discover(S1, {Chs1 ,ai | a [?]
A [?]
{PS}}) Cs v !S1  (7) (8) (9) (10) (11) (12) (13) (14) (15) (16) (17) (18)  with axioms (13-15) for each instruction d(a, b, g) = ha0 , b 0 , g 0 i.
We now prove that C0 is satisfiable w.r.t.
KBM iff M has an infinite computation starting from the empty tape.
"=" Let C0 be satisfiable, then, [?
]hx0 , t0 i [?]
[?
]I x T .x0 [?]
I(t) I(t0 ) I(t ) C0 .
Then, by axiom (1), x0 [?]
CPS 0 , and [?
]t > t0 .Chs0 ,bi .
We now show that t = t0 + 1.
Indeed, if Chs0 ,bi is true, then, I(t)  by axiom (6), D1 must also be true, i.e.
x0 [?]
D1 .
On the other hand, by axiom (4), CPS is true at just one point in time and D1 is true next time and only there (by axiom (5)), I(t ) I(t +1) i.e.
x0 [?]
D1 0 .
Thus, t = t0 + 1, x0 [?]
CPS 0 , x0 [?]
I(t0 +1) I(t) Chs0 ,bi , and, by axiom (7), [?
]t > t0 + 1.x0 [?]
Cb .
FurtherI(t +1)  more, by axiom (8), x0 [?]
Cs 0 , while, by axioms (9-12), I(t ) I(t +2) x0 [?]
Cl 0 , x0 [?]
Cr 0 .
Because by axiom (2), for all  Top  D  S  d  C  D  C  (a)  D1  D2  Dn (b)  Figure 5.
Encoding axioms: (a) C v !D; (b) C v D1 t .
.
.
t Dn .
I(t)  t [?]
T there is at most one x [?]
A0 such that x0 [?]
Cx , then, the sequence hhx0 , t0 i, hx0 , t0 + 1i, .
.
.i represents the starting configuration of M. Now, by axiom (3) and the assumption that R is global, [?
]x1 [?]
[?
]I .[?
]t [?]
T .hx0 , x1 i [?]
RI(t) (we call x1 R-successor of x0 ).
Let hx0 , x1 , x2 , .
.
.i be a chain of I(t ) R-successors satisfying axiom (2).
Since x0 [?]
CPS 0 , then, by axioms (13) and (16), and the definition of the instruction I(t ) function, d, xi [?]
CPS 0 , for all i.
Then, given the axioms (12- 16), the chain of R-successor, hx0 , x1 , x2 , .
.
.i, represents a computation of M. Finally, axioms (17-18) guarantee that M never halts.
"=" Conversely, suppose that M is a Turing machine and hc0 , .
.
.
, ck , .
.
.i its infinite computation starting with the .
empty tape.
We construct a model I = hT , [?
]I , *I(t) i of KBM such that C0 is satisfiable.
In particular, we fix T = hN, <i4 , [?
]I = N, RI = sucN (the successor function over N), I(0) I(j) C0 = {0}, and C0 = [?
], for all j > 0.
Furthermore, [?
]j [?]
N:  1.
Axioms involving discover are mapped using disjoint and covering hierarchies in ERV T .
2.
Axioms of the form C v D, with C, D atomic concepts are encoded as C ISA D. 3.
For each axiom of the form C v !D we construct the hierarchy in Figure 5(a).
4.
For each axiom of the form C v D1 t .
.
.
t Dn we introduce a new entity, D, and then we construct the hierarchy in Figure 5(b).
5.
Axioms of the form C v [?
]R.D are mapped together with the axiom > v [?
]R.> by introducing a new subrelationship, RC , and considering R as a functional role5 .
Figure 6(a) shows the mapping where R is a snapshot relationship to capture the fact that R is a global role in KBM .
6.
For each axiom of the form C v 2+ D (C v 3+ D) we use a persistency (dynamic extension) constraint: C PER D (C DEX D).
I(j)  * Cx = {i [?]
N | the jth cell of ci contains x}, for all x [?]
A0 I(j)  * Cs *  I(j) Cl I(j)  * Cr  * C I(j) I(j)  * D1  I(j)  * D2  I(j)  * D3  7.
Axioms of the form next(C, D) are mapped by using the dynamic extension constraint to capture that C v 3+ D. To capture that C v !3+ 3+ D we rewrite it as C v 2+ 2+ !D, which, in turn, is encoded by the following set of axioms: C v 2+ C1 C1 v 2+ C2 C2 v !D Figure 6(b) shows the portion of the ERV T diagram that maps next axioms.
= {i [?]
N | the jth cell of ci is the active one} I(j+1)  = Cs  I(j-1)  = Cs S I(j) = x[?
]A0 Cx I(j-1)  = CPS  I(j-1)  = D1  I(j-1)  = Cr S I(j) * S1I(j) = a[?
]A Chs1 ,ai .
It is easy to verify that I is a model of KBM where C0 is satisfiable.
2  Reducing ALC F concept sat to ERV T entity sat We now show how to capture the knowledge base KBM with an ERV T schema, SM .
The mapping is based on a similar reduction presented in [5] for capturing ALC axioms.
For each atomic concept and role in KBM we introduce an entity and a relationship, respectively.
To simulate the universal concept, >, we introduce a snapshot entity, Top, that generalizes all the entities in SM .
Additionally, the various axioms in KBM are encoded in ERV T as follows:  The above reductions are enough to capture all axioms in KBM .
Indeed, axioms (13-15) have the form: C v !C1 t [?
]R.C2 .
They can be split by introducing new concepts C 1 , C 2 as follows: C v C1 t C2 C 1 v [?
]R.C1 C 2 v !C2 We proceed in a similar way to encode axioms (16) which have the form: Ca v Cl t Cs t Cr t [?
]R.Ca , and the axiom (12).
We are now able to prove the main result of this paper.
Theorem 5.2.
Reasoning in ERV T using persistency and dynamic constructs is undecidable.
Proof Proving that the above reduction from KBM to SM is true can be easily done by checking the semantic equivalence between each ALC F axiom and its encoding (for a similar proof see [5]).
Then, the concept C0 is satisfiable w.r.t.
KBM iff the entity C0 is satisfiable w.r.t.
SM .
Thus, because 5  4  A similar proof holds if T = hZ, <i.
Considering R as a functional role does not change the ALC F undecidability proof.
Top  S  R  S  (1,1)  Top  S  d  C  (1,1)  RC  D  C  PER  C1  PER  C2  D  DEX  (a)  (b)  Figure 6.
Encoding axioms: (a) C v [?
]R.D and > v [?
]R.>; (b) next(C, D).
of Proposition 5.1, the halting problem can be reduced to reasoning in ERV T .
2 [4]  6.
Conclusions We formally discussed the relevant reasoning problems for temporal conceptual models.
We distinguished between six different reasoning services: (a) Entity, relationship and schema satisfiability; (b) Liveness and global satisfiability for both entities and relationships; (c) Subsumption for either entities or relationships; (d) Logical implication between schemas.
While the problems (a-c) have been shown to be reducible to each other, checking whether a schema logically implies another schema has been shown to be the more general reasoning service.
We then investigated the complexity of reasoning on temporal models and we found that such problem is undecidable as soon as the schema language is able to distinguish between temporal and atemporal constructs (in particular, whether the language captures temporal relationships) and has the ability to represent dynamic constraints between entities.
We finally mention an interesting open problem which will be matter of a future work.
Does reasoning on ERV T become decidable if we drop dynamic constraints?
Without dynamic constraints it is possible to encode ERV T using a combination between the description logic ALCQI and the epistemic modal logic S5.
Decidability results have been proved for the logic ALC S5 [9].
But, it is still an open problem whether this result holds for the more complex logic ALCQI S5 .
[5] [6]  [7]  [8] [9]  [10]  [11]  [12]  Acknowledgments I would like to thank Diego CAlvanese, Enrico Franconi, Sergio Tessaris and Frank Wolter together with the anonymous referees for enlightening comments on earlier drafts of the paper.
[14]  References  [15]  [1] A. Artale and E. Franconi.
A survey of temporal extensions of description logics.
Annals of Mathematics and Artificial Intelligence, 30("1-4"), 2001.
[2] A. Artale and E. Franconi.
Temporal ER modeling with description logics.
In Proc.
of the International Conference on Conceptual Modeling (ER'99).
Springer-Verlag, Novembre 1999.
[3] A. Artale, E. Franconi, and F. Mandreoli.
Description logics for modelling dynamic information.
In J. Chomicki, R. van der  [13]  [16]  Meyden, and G. Saake, editors, Logics for Emerging Applications of Databases.
Lecture Notes in Computer Science, Springer-Verlag, 2003.
A. Artale, E. Franconi, F. Wolter, and M. Zakharyaschev.
A temporal description logic for reasoning about conceptual schemas and queries.
In S. Flesca, S. Greco, N. Leone, and G. Ianni, editors, Proceedings of the 8th Joint European Conference on Logics in Artificial Intelligence (JELIA-02), volume 2424 of LNAI, pages 98-110.
Springer, 2002.
D. Berardi, A. Cali, D. Calvanese, and G. De Giacomo.
Reasoning on UML class diagrams.
Technical Report 11-03, 2003.
D. Calvanese, M. Lenzerini, and D. Nardi.
Unifying class-based representation formalisms.
J. of Artificial Intelligence Research, 11:199-240, 1999.
J. Chomicki and D. Toman.
Temporal logic in information systems.
In J. Chomicki and G. Saake, editors, Logics for Databases and Information Systems, chapter 1.
Kluwer, 1998.
R. Elmasri and S. B. Navathe.
Fundamentals of Database Systems.
Benjamin/Cummings, 2nd edition, 1994.
D. Gabbay, A.Kurucz, F. Wolter, and M. Zakharyaschev.
Manydimensional modal logics: theory and applications.
Studies in Logic.
Elsevier, 2003.
H. Gregersen and J. Jensen.
Conceptual modeling of timevarying information.
Technical Report TimeCenter TR-35, Aalborg University, Denmark, 1998.
C. S. Jensen, J. Clifford, S. K. Gadia, P. Hayes, and S. J. et al.
The Consensus Glossary of Temporal Database Concepts.
In O. Etzion, S. Jajodia, and S. Sripada, editors, Temporal Databases - Research and Practice, pages 367-405.
SpringerVerlag, 1998.
C. S. Jensen and R. T. Snodgrass.
Temporal data management.
IEEE Transactions on Knowledge and Data Engineering, 111(1):36-44, 1999.
C. S. Jensen, M. Soo, and R. T. Snodgrass.
Unifying temporal data models via a conceptual model.
Information Systems, 9(7):513-547, 1994.
S. Spaccapietra, C. Parent, and E. Zimanyi.
Modeling time from a conceptual perspective.
In Int.
Conf.
on Information and Knowledge Management (CIKM98), 1998.
C. Theodoulidis, P. Loucopoulos, and B. Wangler.
A conceptual modelling formalism for temporal database applications.
Information Systems, 16(3):401-416, 1991.
F. Wolter and M. Zakharyaschev.
Satisfiability problem in description logics with modal operators.
In Proc.
of the 6 th International Conference on Principles of Knowledge Representation and Reasoning (KR'98), pages 512-523, Trento, Italy, June 1998.
Counterexample-Guided Abstraction RefinementPS Edmund Clarke School of Computer Science Carnegie Mellon University Pittsburgh, USA edmund.clarke@cs.cmu.edu  Abstract The main practical problem in model checking is the combinatorial explosion of system states commonly known as the state explosion problem.
Abstraction methods attempt to reduce the size of the state space by employing knowledge about the system and the specification in order to model only relevant features in the Kripke structure.
Counterexample-guided abstraction refinement is an automatic abstraction method where, starting with a relatively small skeletal representation of the system to be verified, increasingly precise abstract representations of the system are computed.
The key step is to extract information from false negatives ("spurious counterexamples") due to overapproximation.
The methods for alleviating the state explosion problem in model checking can be classified coarsely into symbolic methods and abstraction methods [6].
By symbolic methods we understand the use of succinct data structures and symbolic algorithms which help keep state explosion under control by compressing information, using, e.g., binary decision diagrams or efficient SAT procedures.
Abstraction methods in contrast attempt to reduce the size of the state space by employing knowledge about the system and the specification in order to model only relevant features in the Kripke structure.
An abstraction function associates a Kripke structure  with an abstract Kripke structure  such that two properties hold:  This research was sponsored by the Semiconductor Research Corporation (SRC) under contract no.
99-TJ-684, the National Science Foundation (NSF) under grant no.
CCR-9803774, the Office of Naval Research (ONR), the Naval Research Laboratory (NRL) under contract no.
N0001401-1-0796, and by the Defense Advanced Research Projects Agency, and the Army Research Office (ARO) under contract no.
DAAD19-01-1-0485, and the General Motors Collaborative Research Lab at CMU.
The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of SRC, NSF, ONR, NRL, DOD, ARO, or the U.S. government.
- Feasibility.
 is significantly smaller than  .
- Preservation.
 preserves all behaviors of  .
Preservation ensures that every universal specification which is true in  is also true in  .
The converse implication, however, will not hold in general: a universal property which is false in  may still be true in  .
In this case, the counterexample obtained over  cannot be reconstructed for the concrete Kripke structure  , and is called a spurious counterexample [10], or a false negative.
An important example of abstraction is existential abstraction [11] where the abstract states are essentially taken to be equivalence classes of concrete states; a transition between two abstract states holds if there was a transition between any two concrete member states in the corresponding equivalence classes.
In certain cases, the user knowledge about the system will be sufficient to allow manual determination of a good abstraction function.
In general, however, finding abstraction functions gives rise to the following dichotomy:  - If  is too small, then spurious counterexamples are likely to occur.
- If  is too large, then verification remains infeasible.
Counterexample-Guided Abstraction Refinement (CEGAR) is a natural approach to resolve this situation by using an adaptive algorithm which gradually improves an abstraction function by analysing spurious counterexamples.
(i) Initialization.
Generate an initial abstraction function.
(ii) Model Checking.
Verify the abstract model.
If verification is successful, the specification is correct, and the algorithm terminates successfully.
Otherwise, generate a counterexample  on the abstract model.
(iii) Sanity Check.
Determine, if the abstract counterexample  is spurious.
If a concrete counterexample  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  I can be generated, the algorithm outputs this counterexample and terminates.
[7]  (iv) Refinement.
Refine the abstraction function in such a way that the spurious counterexample I is avoided, and return to step (ii).
[8]  Using counterexamples to refine abstract models has been investigated by several researchers beginning with the localization reduction of Kurshan [19] where the model is abstracted/refined by removing/adding variables from the system description.
A similar approach has been described by Balarin and Sangiovanni-Vincentelli in [1].
A systematic account of counterexample guided abstraction refinement for CTL model checking was given in [10, 8].
Here, the initial abstraction is obtained using predicate abstraction [17] in combination with a simple static analysis of the system description; all other steps use BDD-based techniques.
The use of tree-like counterexamples guarantees that the method is complete for ACTL.
During the last few years, the CEGAR paradigm has been adapted to different projects and verification frameworks, both for hardware and software verification [20, 16, 14, 13, 3, 4, 2, 9, 7, 18, 5].
The major improvements to the method include, most notably, the integration of SAT solvers for both verification and refinement, and the use of multiple spurious counterexamples.
It is well-known that most abstraction methodologies can be paraphrased in the framework of abstract interpretation by Cousot and Cousot [12].
Giacobazzi and Quintarelli [15] have shown that, not surprisingly, this holds true for counterexample-guided abstraction refinement as well.
The practical and computational significance of such embeddings for verifying real-life systems however remains controversial.
[9]  [10]  [11]  [12]  [13] [14]  [15]  [16]  [17] [18]  References [1] F. Balarin and A. L. Sangiovanni-Vincentelli.
An iterative approach to language containment.
In Computer-Aided Verification, 1993.
[2] T. Ball and S. K. Rajamani.
Getting abstract explanations of spurious counterexamples in C programs, 2002.
Microsoft Technical Report MSR-TR-2002-09.
[3] S. Barner, D. Geist, , and A. Gringauze.
Symbolic localization reducation with reconstruction layering and backtracking.
In CAV 2002, volume 2404 of LNCS, pages 65-77, 2002.
[4] S. Chaki, J. Ouaknine, K. Yorav, and E. M. Clarke.
Multilevel abstraction refinement for concurrent C programs.
2002.
Submitted for Publication.
[5] E. Clarke, S. Chaki, S. Jha, and H. Veith.
Strategy guided abstraction refinement, 2003.
Manuscript.
[6] E. Clarke, O. Grumberg, S. Jha, Y. Lu, and H. Veith.
Progress on the state explosion problem in model checking.
[19] [20]  In Informatics, 10 Years Back, 10 Years Ahead, volume 2000 of LNCS, pages 176-194, 2001.
E. Clarke, A. Gupta, J. Kukula, and O. Strichman.
SAT based abstraction - refinement using ILP and machine learning techniques.
volume 2404 of LNCS, pages 265-279, Copenhagen, Denmark, July 2002.
E. Clarke, S. Jha, Y. Lu, and H. Veith.
Tree-like counterexamples in model checking.
In Proc.
Logic in Computer Science (LICS), 2002.
E. M. Clarke, A. Fehnker, Z. Han, B. H. Krogh, O. Stursberg, and M. Theobald.
Verification of hybrid systems based on counterexample-guided abstraction refinement.
In TACAS'03, pages 192-207, 2003.
E. M. Clarke, O. Grumberg, S. Jha, Y. Lu, and H. Veith.
Counterexample-guided abstraction refinement.
In Computer Aided Verification, pages 154-169, 2000.
E. M. Clarke, O. Grumberg, and D. E. Long.
Model checking and abstraction.
ACM Transactions on Programming Languages and Systems, 16(5):1512-1542, September 1994.
P. Cousot and R. Cousot.
Abstract interpretation : A unified lattice model for static analysis of programs by construction or approximation of fixpoints.
ACM Symposium of Programming Language, pages 238-252, 1977.
S. Das and D. Dill.
Successive approximation of abstract transition relations.
In LICS, pages 51-60, 2001.
S. Das and D. Dill.
Counter-example based predicate discovery in predicate abstraction.
In Formal Methods in Computer-Aided Design, pages 19-32, 2002.
R. Giacobazzi and E. Quintarelli.
Incompleteness, counterexamples and refinements in abstract model checking.
In SAS'01, pages 356-373, 2001.
M. Glusman, G. Kamhi, S. Mador-Haim, R. Fraer, and M. Vardi.
Multiple-counterexample guided iterative abstraction refinement: An industrial evaluation.
In TACAS'03, pages 176-191, 2003.
S. Graf and H. Saidi.
Construction of abstract state graphs with PVS.
In Computer-Aided Verification, June 1997.
T. A. Henzinger, R. Jhala, and R. Majumdar.
Counterexample guided control.
In ICALP, 2003.
To appear.
R. P. Kurshan.
Computer-Aided Verification of Coordinating Processes.
Princeton University Press, 1994.
Y. Lakhnech, S. Bensalem, S. Berezin, and S. Owre.
Incremental verification by abstraction.
pages 98-112, 2001.
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE
A Novel Approach to Model NOW in Temporal Databases Bela Stantic, John Thornton, Abdul Sattar School of Information Technology Griffith University Gold Coast, Australia b.stantic, j.thornton, a.sattar  @griffith.edu.au Abstract In bitemporal databases, current facts and transaction states are modelled using a special value to represent the current time (such as a minimum or maximum timestamp or NULL).
Previous studies indicate that the choice of value for now (i.e.
the current time) significantly influences the efficiency of accessing bitemporal data.
This paper introduces a new approach to represent now, in which current tuples and facts are represented as points on the transaction time and valid time line respectively.
This allows us to exploit the computational advantages of point-based query languages.
Via an empirical study, we demonstrate that our new approach to representing now offers considerable performance benefits over existing techniques for accessing bitemporal data.
1 Introduction Relational data models and their implementations usually only capture a snapshot or current state of the real world.
A transaction then changes the database from one state to another by replacing the old values with new ones.
However, there are many application domains where it is necessary to keep the old database states or even store future states.
In addition, most production databases contain some amount of time dependent data and most database technology applications are temporal in nature (e.g.
scheduling, financial and scientific applications).
In fact, it is difficult to identify any database application that does not require some form of time-varying data.
Conversely, the built-in temporal support offered by commercial database products and the Relational Query language fi 			 [8] is limited to predefined, time-related data types.
Some commercial databases include temporal extensions, e.g.
the Oracle TimeSeries cartridge, Oracle 9i aFlash-Backa, and the Informix TimeSeries Data-Blade, but these extensions still do not fully support the successful management of timevarying data.
Research has demonstrated that applications  can significantly benefit from using a temporal RDBMS, and also that temporal support is needed that goes beyond simple data types [7].
Associating data with time values and keeping a history of fact validity is not technically difficult even using nontemporal RDBMS technology [13] [4].
However, it is a difficult task to efficiently query such time-varying data and to identify integrity constraints that hold over several database states.
A database is considered temporal if it is able to manage time-varying data and it supports some time domain distinct from user-defined time.
In temporal databases time can be captured along two distinct time lines: transaction time and valid time.
A bitemporal database is a combination of valid time and transaction time databases and records the database states with respect to both valid time and transaction time.
The valid time line represents when a fact is valid and the transaction time line represents when a transaction was performed.
Recording bitemporal data generally requires that updates are appended to the database (rather than overwriting existing values).
This can easily lead to the storage of large volumes of data, and consequently makes the selection of efficient access methods very important.
Storing bitemporal data also requires the selection of appropriate time units (granularities).
Without careful management, the informational benefits of bitemporal data can be easily outweighed by the costs of poor access times and difficulties in formulating queries [5].
In our current work we use the TQuel four-timestamp format to represent bitemporal data [11], where, in addition to non-temporal attributes, each tuple has four temporal attributes:  and  representing the starting and ending time points of fact validity in the modelled world, and  and   representing the time when a tuple is inserted in the database and the time it is logically deleted.
Sample bitemporal data is shown in Table 1.
A tuple is considered current if it is part of the current database state, i.e.
it has not been logically deleted by assigning a timestamp to   different from the value of now.
In the literature such a tuple, where the validity of a fact in the modelled world is valid up to the  current time, is called now-relative.
Despite two decades of research in temporal databases, relatively few papers have addressed the issue of indexing temporal data.
Even less have addressed the issues of how to index now-relative data, or temporal data that are current or valid now.
Existing research shows that regular indices, - trees, are unsuited for temporal data [10], and such as recently several other indices have been proposed.
Only a few index structures address the need to store the current time, a need which is accommodated by almost all temporal data models and is natural and meaningful for many kinds of applications.
As valid time and transaction time are considered to be orthogonal [12], bitemporal data can be represented in two dimensional space, enabling us to apply spatial indexes.
For bitemporal indices based on R-trees, the maximum-timestamp approach is a straightforward solution to the indexing of now-relative data.
But it is obvious that in this approach, facts with now-relative valid-time intervals are represented using very large rectangles, and the resulting search performance is poor due to excessive dead space in the index nodes and overlap between nodes.
We are aware of only a few structures that address the issues related to storing now-relative data in temporal databases [1], [2], [9].
Some of the proposals rely on special variables until changed and now that should be part of a not yet existing temporal relational model.
Research also suggests that the widespread acceptance of such a model is unlikely, due to the large commercial investment in the existing relational model, both in terms of developed code and expertise [7].
At the same time, due to the significant drop in the price of disk storage, more and more database applications are using added temporal dimensions and, as a consequence, are facing increasingly poor response times.
Bitemporal databases store past, present and even future facts in either logically deleted or current tuples.
To represent that a fact is current now, or that a tuple has not been logically deleted, requires the storing of a value representing the current time.
In the literature, several concepts to represent current time have been proposed by including special variables, such as : anowa , auntil-changeda, aforevera, a a, a@a, and a-a.
However, the same basic issue applies to any approach, i.e.
how physically to store that concept in the database [3].
As now is not part of the domain of SQL1999 values [8], it is necessary to represent the current time by some other value, in such a way that the chosen value is not overloaded (i.e.
does not have more than one meaning).
It has been shown that the choice of the physical value for now significantly influences the efficiency of accessing bitemporal data [14].
Currently, the literature has concentrated on three basic approaches: firstly using NULL, secondly using the smallest timestamp and thirdly using the largest timestamp supported by the particular RDBMS.
    A disadvantage of using NULL is that columns that permit NULL values prevent the RDBMS from using indexes.
Conversely, using a non-NULL value can also affect indexing badly.
For example, when an index is used to retrieve tuples with a time period that overlaps current time, and now is represented with smallest or largest timestamp value, tuples with the    (Valid time end) or   (Transaction time end) attribute set to now will not be in the range retrieved.
We have seen that current facts are represented by assigning the value now to    , and that by assigning now to  we can represent the belief that a tuple is current (or not logically deleted).
This shows importance of now in bitemporal databases.
Further, the importance of now increases when we consider that current tuples and current facts are likely to be accessed more frequently.
While issues related to storing now are discussed in the literature in the context of temporal databases, this equally applies to conventional relational DBMS technology.
In this paper we propose a new approach to modelling current time in temporal databases that overcomes the limitation of an attribute set to now not being in the range retrieved.
In addition, our approach has significant computational advantages over the previously proposed methods and, to the best of our knowledge, it is the only approach to representing now that ensures the value now is not overloaded.
In the remainder of the paper, we first look more closely at previous approaches to modelling current time and highlight their limitations and disadvantages.
Then, in section 3, we present the acorea of the new proposed method for representing current time, and empirically evaluate our approach in comparison to two of the standard existing approaches.
Finally, in section 4, we present our conclusions and discuss possible extensions and future work.
2 Traditional representations of current time As time seems to be continuous, and current time is everincreasing, a significant question in computer science, particularly with respect to databases, is how to store the value of current time (or now).
In line with the existing research, we have accepted a discrete model of time.
Since digital computers only support a limited granularity for real numbers, most proposals for adding time to the relational model are based on a discrete, totally ordered set of time instants to represent both the transaction and valid time dimensions.
This ordering is defined as follows:  fi 	  	   !
 " " "$# fi% &'	(*),+ "$- " "$-.
0/21 + " "3- "$-.4/516+ "  " "$-(0/  For valid time :  fi3 	 !  "
  " " # fi& 	  ),+  " -  "  " -0 /51 + " "$- "$-4 /51 + "  " "$-0 /  For Transaction time :  value (MIN) or NULL have disadvantages related to indexing.
This has been highlighted in the literature and is particularly relevant to accessing bitemporal data, where the choice of the value of now has been shown to significantly affect access efficiency [14].
In a discrete totally ordered model, a time interval, denoted as [    ,   ), represents a set of a countably infinite equidistant time instants [4], where   is the starting time instant and   is the ending time instant representing the starting and ending boundaries respectively.
These time instants are the smallest and largest values on the time line in the set of continuous time instants making up a given interval.
Also, a time interval [    ,  ) is closed on the lefthand side and open on the right.
This means that the start point of the interval =   and the end point   , i.e.
the interval includes the point   and excludes the point   .
In such a model, now-relative data contained in tuples that are currently valid, can be represented as [    , now ).
Here  represents the time point when the fact started to be true and now represents that the fact is still current and that the time validity of fact is continuously expanding (i.e.
its end is unknown).
Assuming we are interested in using existing technology and given that the domain of SQL1999 values does not contain a special value for now, the task becomes one of selecting an appropriate value for now from an existing domain.
This value should firstly satisfy the requirement that it cannot be used with some other meaning, otherwise the meaning becomes ambiguous and the value is overloaded.
As mentioned before, previous work has concentrated on three physical values to represent now: the NULL value , the smallest timestamp (MIN) and largest timestamp (MAX) supported by a particular RDBMS.
It is clear that whichever of these values is chosen, the domain of the data type becomes limited and a potential for overloading is created.
This is especially the case for the NULL value, as it is already overloaded in its normal usage.
However, the NULL value does have the advantage that it takes up less space than a regular timestamp value and can be processed faster.
Despite this, the crucial disadvantage of NULL is that columns that permit NULL values cannot be indexed by a conventional RDBMS, leading to potentially unacceptable access times.
It is important to mention that using a non-NULL value for now also can affect indexing.
If, for example, a B-tree index on    or   is used to retrieve tuples with a time period that overlaps now, and now is represented with the smallest or largest timestamp value, tuples with the   or   attribute set to now will not be in the range retrieved (i.e.
they will be at the extreme left or extreme right of the B-tree).
We term this as the range indexing problem.
Hence all previous approaches to model current time using the largest timestamp value (MAX), the smallest timestamp  -  3 A new approach to model now Each of the previously discussed approaches to representing now has severe limitations in terms of indexing or overloading.
Of these approaches, the literature generally agrees that MAX is the best overall compromise [14], as it allows indexing and generally has better performance than MIN.
However, MAX and MIN have further performance problems navigating and updating time value indexes, due to the redundancy of the special timestamp value used to represent the current time.
This means that all   and   values will be indexed to the same special value (i.e.
MAX or MIN) causing the index to search sequentially through these records.
We term this the index redundancy problem.
A major aim of our research is to overcome the limitations of previous approaches to representing now.
These limitations have been identified as overloading, the range index problem and the index redundancy problem.
From a consideration of the redundancy problem, it became clear that our solution should produce a value for now that is related to some distinct property of the tuple to which it refers.
In this way the level of redundancy can be reduced.
Also, to avoid the range index problem, a value is needed that is contained in the interval between the start time and the actual current time.
We therefore concluded that the best solution would be to make the end point of any current interval equal to the start point (i.e.
  =   and   =   ).
This representation therefore defines the actual interval between the start and end points of a current interval to be zero (i.e.
it becomes a point on the time line rather than an interval).
A first objection to this approach could be to say that the value is overloaded, e.g.
it becomes impossible to distinguish between an interval that actually started on the 12.12.01 and finished on the same day, from one that started on 12.12.01 and is still current (given that the granularity of our time value or chronon is one day).
However, using the definitions of interval start and end points we can see that this objection is not valid: i.e.
the interval [12.12.01, 12.12.01) is closed on the left (and so starts on the 12.12.01) and open on the right (and so finishes before 12.12.01).
Therefore this interval has no meaningful duration.
If it is required to distinguish the start and end times more finely, then the granularity of the time value must be changed (e.g.
from days to hours).
From this discussion, we can see that our new approach to representing current time also solves the overloading problem as any tuple where   =  or   !
( 8 A  )I)=) !
*JK+ ( +JKL ; -  fi	 " #fi ./10324fi " fi6*> ./7B )I)I) )I)I)    %$'& fi52476 &@?
:	 CEDFD4G5H6 )=)I) )=)I)    ( *! )
+*,) +*+ (8 ) +9:) +*+   !
+) +*-) + ( 8 +) +!
*) +!
22.03.99 21.02.01  22.03.99 21.02.01  )I)I) )I)I)  )I)=) )I)=)         26.09.00  +!
*) +9:) +*+  26.09.00  10.04.99  10.04.99  !
8 ) + ( ) +!
)I)I) )I)I)  (<; )=!+) +*+  (8 ) + ( ) +!
)I)=) )I)=)  Table 1.
Sample Bitemporal data with POINT representation of now    =   is unambiguously current.
To illustrate our new point-based approach to now (which we term POINT), consider the relation in Table 1.
Here, in tuple ID = 3, we can conclude that Mark has the current position of aAdmina, firstly because   equals  (meaning the fact is currently valid) and secondly because  equals   (meaning the tuple has not been deleted).
Tuple ID = 1 represents that Megan had the position of aDBAa from a21.08.00a to a10.05.02a and because the tuple is current (as   equals   ) this represents our current belief about Meganas past position.
Tuple ID = 2 is logically deleted (as   differs from   ), which means that we believed from a01.07.00a to a26.10.00a that Stephan was employed as a aTeachera between a23.07.00a and a30.01.01a.
Tuple ID = 4 is logically deleted (as   differs from  ), which means that we believed from a13.02.01a to a23.02.01a that Steven has a current employment as a aOfficera from a21.02.01a.
Hence, when the timestamp for  is the same as  it means that a fact is valid now.
Similarly when  is the same as   it means that a tuple is current.
Note also that in Table 1 we are displaying the data as it would be stored in the database, in actual practice we would expect the end time value when   =    or   =  to be displayed to the user as some special symbol (such as now, until changed or NULL).
3.1 Experiments In order to evaluate our POINT approach to representing now we decided to empirically compare POINT to both the MAX timestamp approach and to using NULL.
In doing this we followed previous research in the area, but in contrast to previous work, we decided not to test the minimum timestamp value (MIN) as this has already been shown to be consistently worse than MAX [14].
For each of our methods we generated three relations, each differing only in the physical representation of the current time value.
Then on each relation we performed three different representative time slice queries shown in Figure 1.
We chose time slice queries because of their recognised importance in temporal databases [15].
Query One retrieves the current state in both transaction and valid time.
It selects tuples with transaction and valid  Figure 1.
Types of Time slice Queries  time intervals that both overlap with the current time, as well as retrieving tuples where valid time and transaction time ends at now.
Basically, Query One retrieves what we currently believe about the current state of the world.
Query Two time slices the relation as of now in transaction time and as of a past time in valid time.
In other words it retrieves our current belief about a past state of the world.
Query Three time slices the relation as of a past time in both transaction time and valid time, which means it retrieves a past belief about a past state of reality.
Queries One and Two favour the current state, because this state is assumed to be accessed much more frequently than old states.
Results for 10% current data Disk CPU Duration Exp.
Type (; 9*- ;A 9<-<, POINT10-1 ( L !
A8<8*8 !7- ; MAX10-1 ( - ;<;*; 8*( !*!
8 , NULL10-1 (<(A ,8<; !
,*-*POINT10-2 8 !
, ( 9 ( , (*( -:!
MAX10-2 A ,9+ 8 (<; +9 9 A NULL10-2 A L-<9!
8 , !
; +*, POINT10-3 ( !
-<+ - ( 9 A L -<9 MAX10-3 ;*( -L 8 , ( !
,<+!
NULL10-3  time slice query type (i.e.
either Query One, Two or Three).
Results for 20% current data Disk CPU Duration Exp.Type 8 9 !
(<;A !
!
(A L POINT20-1 ( 9<, A , (; 9!
A 9 MAX20-1 A +*,!
, A L !
;*8 , NULL20-1 8 - ( 9*( -*9*- ; POINT20-2 ; - A<A , A*8 L9 L!
MAX20-2 A !7- A , ( + A !
L*, NULL20-2 A:( 9 ;( ( 9<L ( ; POINT20-3 8 !
; , , ;( ,*+ ,!
MAX20-3 ( L*L*, L*,*+ 8*8 L*L NULL20-3  3.2 Analysis  Our tests were performed on a four 450MHZ CPU SUN UltraSparc II processor machine, running Oracle 9.2.0 RDBMS, with a database block size of 8K.
During the tests the server had no other significant load.
We performed experiments using four different sizes of SGA (System Global Area): 30MB, 50MB, 100MB and 200MB in order to investigate the effects of aging buffers.
We created -tree composite indexes on    and    and a -tree index on   for all tables.
We also performed additional tests using function based indexes.
  Results for 40% current data Disk CPU Duration  (*(;<A( A ,!
,<, - A ,*+*9 ;( , A , !
(A - ; !
!
A ,*-<L A 9 ; !
L!
!*!
, 8*8A !
+*L 8*8A  !
,9 ( A 9!+ (A:(; A:( - ; 9 8 - ; ; - A( A 9<L<+ ; !*!
A8*8<;  A , 9*9<+ ,*, !
A !
(<( !*!<!
!79<!7- A    Exp.Type POINT40-1 MAX40-1 NULL40-1 POINT40-2 MAX40-2 NULL40-2 POINT40-3 MAX40-3 NULL40-3  Our queries were executed on nine different bitemporal tables, three for each representation of now, with each table having a random distribution of one million temporal tuples and a granularity of one day.
Within this data set, each representation of now was tested on three separate relations: In the first relation, 10% of the tuples overlapped with the current time in both transaction and valid time.
In the second and third relations this percentage was 20 and 40, respectively.
The tables present our experimental results for 100MB SGA, where CPU usage is measured in CPU units, duration is measured in seconds and Experiment Type uses the notation METHOD - , where METHOD represents the method used to model now, i.e.
either our new POINT approach (where   =   and   =    ) or MAX (using the Oracle max timestamp a31-DEC-9999a) or NULL; represents the percentage of the tuples in the experiment overlapped with the current time in both transaction and valid time (i.e.
either 10, 20 or 40%); and represents the         The results show that the new POINT representation for now clearly outperforms both MAX and NULL in terms of disk reads, CPU usage and query duration across the full range of our problem set.
Looking more specifically, Query One most accurately measures the effect of varying the percentage of tuples overlapped with current time, as it retrieves all such tuples.
On this measure we can see all techniques start slowing down as the percentage of overlap increases (i.e.
as we move from 10-1 to 20-1 and 40-1) but also that the relative advantage of POINT over the other techniques grows as the overlap increases.
For instance, at 10-1 the duration times for POINT MAX and NULL are 26, 29 and 32 respectively, whereas at 40-1 this has changed to 48, 75 and 70 (i.e.
POINT has gone from being roughly comparable to NULL and MAX to performing almost as twice as fast).
This demonstrates the distinct advantage POINT has in retrieving current records, due to POINT representing current records as single points on the valid and transaction time lines rather than as the intervals defined by differing start and end points used by MAX.
The performances on Queries Two and Three again show POINT to be consistently superior, although here the relative performance of the three techniques remains relatively stable as the overlap percentage increases.
This is because Queries Two and Three are more concerned with past states of the database and so are not so affected by the proportion of tuples that overlap the current time.
Also, the disk access results show POINT to be consistently better than MAX or NULL especially on the 10-1 problems for Query One where the duration measures are fairly similar.
According to the Theory of Indexability [6], the I/O complexity cost measured by the number of disk accesses for updating and answering queries is one of the most important factors for measuring performance.
This is because, as technology advances, CPU speeds tend to increase relatively faster than disk I/O speeds.
In Figures 2, 3 and 4 we graphically represent our results for the case when = 10% (i.e.
10% of tuples overlap with current time in both valid and transaction time with SGA again set at 100MB).
This gives a clearer representation of the relative performance of each technique and highlights the dominance of POINT especially in terms of disk reads on Query One.
We also investigated, through further experimental study, several possible factors that could interfere with our results, but due the limitations of space we will only briefly mention them.
One of the factors that can affect the number of physical disk reads is the size of SGA.
As previously    Figure 2.
Number of Physical disk reads  Figure 4.
Duration of queries in seconds  4 Conclusion and Future work This study makes the following contributions to the field: by investigating different representations of now in bitemporal databases, we presented a better understanding of the significance of modelling current time, particularly in the context of efficiently accessing bitemporal data; we identified limitations of previous approaches to representing now, namely overloading, the range index problem and the index redundancy problem; we proposed a new approach, called POINT, to represent current time in bitemporal databases; and  Figure 3.
CPU Usage  mentioned, we looked at the effects of aging buffers by varying the size of SGA through 4 values: 30Mb , 50Mb, 100Mb and 200Mb.
Our results showed small differences between approaches and favoured larger values of SGA.
The main difference noted was that a smaller SGA size tends to favour the NULL representation of now, resulting in relatively fewer disk reads.
This is because a NULL value uses less space than a timestamp and so the chance of the buffer aging is reduced.
Apart from this, we did not notice any significant effects on the results when comparing the different representations of now on different sizes of SGA, i.e.
changes in the number of physical disk reads, CPU usage and query duration are virtually linear for all the approaches considered.
in the experimental study we have demonstrated that the proposed POINT based approach not only overcomes the limitations of previous approaches, but also leads to a significant improvement in the efficiency of querying bitemporal databases in comparison to existing methods.
Currently, we are investigating the effect of the proposed POINT approach on the performance of multidimensional indexes, such as the spatial R-tree index.
Finally, the POINT-based method to represent now in temporal databases introduced in this paper, could significantly effect performance of previously proposed index structures for temporal data.
It would be interesting to investigate the effect of POINT on various index structures for temporal data with respect to space usage and response time.
It would be also interesting to look at the time required for index restructuring in relation to insertion, deletion and updating.
This investigation should follow the directions used for the worst case scenario in [10].
References [1] R. Bliujute, C. S. Jensen, S. Saltenis, and G. Slivinskas.
R-tree based indexing of now-relative bitemporal data.
In VLDBa98, Proceedings of 24rd International Conference on Very Large Data Bases, New York City, New York, USA, pages 345a356, 1998.
[2] R. Bliujute, C. S. Jensen, S. Saltenis, and G. Slivinskas.
Light-weight indexing of general bitemporal data.
In Statistical and Scientific Database Management, pages 125a138, 2000.
[3] J. Clifford, C. Dyreson, T. Isakowitz, C. S. Jensen, and R. T. Snodgrass.
On the semantics of aNowa in databases.
ACM Transactions on Database Systems (TODS), 22(2):171a214, 1997.
[4] C. Date, H. Darwen, and N. Lorentzos.
Temporal Data and the Relational Model.
Morgan Kaufmann, 2002.
[5] C. E. Dyreson, R. T. Snodgrass, and M. Freiman.
Efficiently supporting temporal granularities in a DBMS.
Technical Report TR 95/07, 1995.
[6] J. Hellerstein, E. Koutsupias, and C. Papadimitriou.
On the analysis of indexing schemes.
16th ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, 1997.
[7] C. S. Jensen.
Introduction to temporal databases, research.
http://www.cs.auc.dk/ csj/Thesis/pdf/ chapter1.pdf, 2000.
[8] J. Melton and A. R. Simon.
SQL:1999 - Understanding Relational Language Components.
Morgan Kaufman, 2002.
[9] M. A. Nascimento and M. H. Dunham.
Indexing valid time databases via -tree.
IEEE Transactions on Knowledge and Data Engineering, 11(6):929a947, 1999.
  [10] B. Salzberg and V. J. Tsotras.
Comparison of access methods for time evolving data.
ACM Computiong Surveys, 31(1), 1999.
[11] R. Snodgras and et al.
The temporal query language TQEL.
ACM TODS, 12(2):247a298, 1987.
[12] R. Snodgrass and I. Ahn.
Temporal databases.
IEEE Computer, 19(9):35a42, 1986.
[13] R. T. Snodgrass.
Developing Time-Oriented Database Applications in SQL.
Morgan Kaufmann, 2000.
[14] K. Torp, C. S. Jensen, and M. Bohlen.
Layered implementation of temporal DBMS concepts and techniques.
A TimeCenter Technical Report TR-2, 1999.
[15] V. J. Tsotras, C. S. Jensen, and R. T. Snodgrass.
An extensible notation for spatiotemporal index queries.
ACM SIGMOD Record, 27(1):47a53, 1998.
On Incompleteness of Multi-dimensional First-order Temporal Logics David Toman Department of Computer Science, University of Waterloo Waterloo, Ontario, Canada N2L 3G1 E-mail: david@uwaterloo.ca Abstract In this paper we show that dZrst-order temporal logics form a proper expressiveness hierarchy with respect to dimensionality and quantidZer depth of temporal connectives.
This result resolves (negatively) the open question concerning the existence of an expressively complete dZrst-order temporal logic, even when allowing multi-dimensional temporal connectives.
1 Introduction We study the expressive power of multi-dimensional dZrst-order temporal logics and their relationship to twosorted dZrst-order logic.
In particular, we are interested in the following result: Two-sorted dZrst-order logic is strictly more expressive than any dZxed-dimensional dZrst-order temporal logic with a dZnite set of temporal connectives.
The paper proves this claim even when only dZnite temporal structures are considered.
To obtain the result we combine results of Bidoit et al.
[3, 4] on order independent properties dedZnable using standard temporal logic with results of Toman and Niwinski [16] on multi-dimensional temporal logics over dense linear order of time.
Interest in expressively complete temporal logics was, in the temporal database community, originally motivated by (unsuccessful) attempts to dedZne expressively complete temporal relational algebras closed over the timestamp or bi-temporal data models [8, 5] in order to implement expressively complete temporal query languages, such as SQL/Temporal [13] or SQL/TP [14, 15].
The result presented in this paper, however, is a general result on temporal logics and equally applies to problems in the area of knowledge representation.
The paper is organized as follows: Section 2 provides the necessary background and dedZnitions.
Section 3 introduces (appropriate extensions of) results in [3, 4, 16] needed to prove the claims in this paper.
Section 4 gives the main result and Section 5 concludes and discusses directions for future research.
2 DedZnitions In this section we give basic dedZnitions of temporal structures and temporal query languages.
The notation and dedZnitions are based on the development in the chapter Temporal Logic in Information Systems [7].
Temporal Structures.
Temporal structures (databases) are built from the following three basic building blocks:   to serve as the temporal domain, 1. a structure where  usually stands for a binary predicate denoting linear order () on .
We also consider (cf.
Section 4) temporal domains equipped with equality () only.
2. a structure  structure.
  to serve as the data domain of the  3. a set of single-sorted predicate symbols  A" fi    fi  ; the arity of the symbol   is  .
This choice dedZnes the database schema for our temporal structure.
In the rest of this section we use  for the signature  A" fi    fi  .
DedZnition 2.1 (Temporal Structure) Let be a temporal domain,  a data domain, and    A" fi    fi   a database schema.
We dedZne   to be a two-sorted predicate symbol of the sort    for each  in  .
We call  the temporal extension of   .
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTLa03) 1530-1311/03 $17.00 AS 2003 IEEE  A temporal extension of    dedZned as    fiA"      fi    is a two-sorted signature composed of the signature of the temporal domain, , the signature of the data domain, , and the temporal extensions fi  of the predicate symbols   in .
We dedZne a temporal structure  to be a two-sorted  structure     	      fiA"fi      fifi   The instances fifi of fi in  dedZne the interpretation of the symbols  in the database schema for every element of the time domain, formally:    A"         holds at time  iff   fi   A"           for   ,    , and    	 fi   fi  .
This observation links the above dedZnition with a Kripke-style dedZnition of temporal structures commonly used to dedZne semantics for temporal logics; [7] shows that these two approaches are equivalent.
Note that the interpretation of the predicate symbols connected solely with the temporal () and data domains ( ) is dZxed, while the interpretation of the symbols fi depends on the database instance.
In practice we often require the instances of the relational symbols   to be dZnite or dZnite at every time instant.
The later is equivalent to requiring the sets A"           fi  A"        are dZnite for every    and fi   fi .
Temporal Queries.
First-order properties of temporal structures can be captured by sentences (closed formulas) in an appropriate temporal query language.
We introduce two principal ways of temporalizing a dZrst-order query language (dZrst order logic) over -structures.
The dZrst approach (often referred to as the timestamp language) introduces explicit temporal variables, relationships between these variables (e.g., order), and quantidZers to the language.
The result is a two-sorted variant of dZrst-order logic over  (the temporal extension of ).
DedZnition 2.2 (2-FOL) Let  be the set of all formulas dedZned by the following BNF rule:              fi    A"             	    	   We call  a temporal variable and   a data variable.
A 2-FOL query is a formula in the language dedZned by the productions for  above.
A temporal 2-FOL property is a closed 2-FOL query.
The semantics of formulas in this language is the standard dZrst-order (Tarskian) semantics with respect to  -structures.
Note that the database schema is monadic with respect to the sort  , i.e., the predicate symbols in the database schema have always exactly one distinguished argument of sort  .
In the technical development we use the following syntactic property of 2-FOL formulas.
DedZnition 2.3 (QuantidZer Depth) We dedZne function  2-FOL        1.
If  is atomic then  2.
3.
4.
fi.
If  is  then  .
If  is A"  Az then   A"  Az .
 	  .
If  is 	  or 	  then   The second approach to developing a temporal query language on top of dZrst-order logic uses implicit temporal connectives to link truth of formulas with an evaluation point.
In the case of multi-dimensional dZrst-order temporal logics () the generalized evaluation point is a vector in the multiple time dimensions.
Temporal connectives can be dedZned by formulas in the language of the temporal domain extended with additional place-holders standing for   formulas (to be aconnecteda by the connective).
DedZnition 2.4 (Temporal Connective) Let  be a relation symbol in the signature of the temporal domain.
We dedZne a set of formulas                  	   A  -dimensional -ary temporal connective   A"        dedZned with respect to the signature of the temporal sort   is a formula dedZned by the productions for  with free variables Az       A" and place-holders  A"       .
The restriction placed on free variables of a  -dimensional temporal connective yields the intuitive behavior: the free variables stand for the generalized evaluation point of the whole connective; the formula that dedZnes the connective aexpectsa the (sub-)formulas substituted for the placeholders  to have the same set of (implicit) free variables.
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTLa03) 1530-1311/03 $17.00 AS 2003 IEEE  Note that there is no restriction on the number of the placeholders .
Example 2.5 We can express the standard linear-time temporal connectives [11] in the temporal signature of linear order as follows: A"  fi  Az Az  AzAz  Az   AzAz Az  Az  A" Az  A"  Az   Az Az A"  A"  A"Az  A"   AzAz A"  A" A" Az  A"   Az Az A"  A"   Az A" A"  Similarly, we can express the past temporal connectives , Az, and A".
Note that, in order to simplify further notation, we have added subformulas that yield a proper renaming of variables,  to the otherwise standard dedZnitions.
Thus, the fi formulas rooted by such connectives are expected to have a single (implicit) temporal variable named Az free.
The renaming subformulas guarantee that names of temporal variables match correctly when fi formulas are embedded into 2-FOL (cf.
DedZnition 2.8).
  Example 2.6 In Section 4 we use the following connectives to show separation between layers of multi-dimensional temporal logics.
Az fi   fi   Az Az A"   fi fi fi    Az A"      .. .
Az Az   fi           	A" fi   fi 	    	 	 	 	   A"fi   fi                  for    .
A  fi  query is a formula in the language dedZned by the productions for .
A fi   property is a closed fi  query.
           	A" fi    fi 	fi  	 	  A"  Az  	   	    A" fi   fi     Azfi 	A" fi   fi 	  	 	 A"   Az  	   	     A" fi    fi   fi        is the (C-)formula denoted by   in      .
Note that embeddings of closed  fi   formulas yield 2-FOL formulas with free variables Az A" .
Thus, the meaning of such formulas in a - structure is dedZned with respect to an evaluation point 	 	 	.
However, since there is no restriction on the content of the set   , we can simulate various alternative approaches, e.g., requiring the formula to be true with respect to an arbitrary evaluation point can be achieved using the Az  operator introduced in Example 2.6 and writing the formula Az  .
Similarly, requiring the formula to be true with respect to all evaluation points is equivalent to writing the formula 	Az  	 .
Thus the choice of an dZxed evaluation point does not affect temporal properties expressible in fi  .
This observation also reconciles the difference between closed formulas in fi and 2-FOL: sentences in 2-FOL can also be evaluated with respect to the 	 evaluation point, since an assignment to the Az A" variables cannot change the truth value of a closed formula.
The quantidZer depth of  fiformulas is dedZned as the quantidZer depth of their embeddings to 2-FOL.
Similarly, the quantidZer depth of temporal connectives is dedZned as the quantidZer depth of their embeddings into 2-FOL, assuming the place-holders stand for atomic formulas.
Given a dZnite set of temporal connectives  we dedZne        .
It is also easy to see that all formulas in  fi   can be considered to be formulas of  fiAz  for any   .
   fi   fi  fifi           fi   fi   DedZnition 2.7 (Temporal Logic) Let   be a dZnite set of -dimensional connectives dedZned over the signature of the temporal sort  .
We dedZne a set of formulas    DedZnition 2.8 (Embedding of fi in 2-FOL) Let  be a mapping of formulas in the language fi   to the language 2-FOL  dedZned as follows:  where  Az A" Az  fi  Az  It is easy to see that all the fi formulas can be naturally embedded into the language of 2-FOL formulas.
This embedding also dedZnes the semantics of the fi-formulas relatively to the semantics of 2-FOL:            3 Background Results The results in this paper depend crucially on the following two results.
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTLa03) 1530-1311/03 $17.00 AS 2003 IEEE  3.1 Games for fi   Example 3.2 Consider the  The dZrst result relates to a modidZed version of Ehrenfeucht-FraAaEsseE Games [10] that precisely captures properties dedZnable by First-order Temporal Logic fi  with an arbitrary set of temporal connectives  such that fi  .
The game is a natural extension of games for single-dimensional temporal logic used by Toman and Niwinski [16] to show separation of fiA"  from 2-FOLfi  in the case of dense ordering of the temporal domain.
  DedZnition 3.1 (Game States) Let structures.
A state size :    and    be two  -  fi of size  is a triple consisting of three vectors of   1. a vector  able names,  fi  			  of temporal and data vari-      			   3. a vector   fi 			  from domains of  .
We require that the elements   and  are elements drawn from the temporal domain of the appropriate structure whenever  is a name of a temporal variable and data elements whenever  is a name of a data variable.
Contiguous sub-sequences of temporal variable names in fi (or more precisely, in the dZrst component of fi ) are grouped 2. a vector  fi   of values from the domains of the structure , and  into disjoint blocks of size up to      .
Variable names  and  are compatible in the following condition holds.
fi if either of   both  and  are both (names of) data variables;   is a data variable,  is a temporal variable (named)  fi  formula  Az fi	fi fi fi	fi fi fi	fi   fi where Az , fi , and fi are temporal connectives introduced in Example 2.6.
The embedding of this formula into 2-FOL is as follows:     	fi	 fi   	    	fi   	    	fi   fi   fi fi    It is easy to see that the variable introduced by the Az  connective is only visible outside the scopes of the fi  and fi connectives.
Thus only data variables quantidZed outside these connectives can appear together with this instance of .
The rethe variable in an atomic predicate, e.g., fi maining data variables, namely, and are not compatible with this instance of , since they appear within scope of another temporal connective that dedZnes another instance of .
Similar observation can be made for the variable and the instance of introduced by fi  .
Moreover, for the same reason, within the (embedding of the) fi  connective, the original variable is no longer visible since another was introduced by Az  introduced by fi  .
Thus, the variable is not compatible with any of the temporal variables introduced by, e.g., fi  .
                    Since moves in a game correspond to nesting of quantidZers in a formula, the above observations tell us that we only have to consider compatible moves in the game when dedZning a winning condition for duplicator DedZnition 3.3 (Winning Game State) A state ning state for duplicator if,  fi is a win-  fifi    			      fi  			  ; (2)        ; and (3)          ;   , and  that occurs after  in fi ;   is a data variable,  is a temporal variable  , such that  is the last occurrence of  before  in fi ;  both  and  are temporal variables in the same block of temporal variables in fi ; or  both  and  are temporal variables, and   is the last occurrence of one of  			 before  in fi .
whenever the variable names corresponding to the values used as arguments of predicate symbols in (1)-(3) are pairwise compatible in , and assuming the values used as arguments of predicates belong to the appropriate sorts.
The intuition behind the dedZnition of moves compatible in a state derives from the syntactical structure of the 2-FOL images of  formulas.
DedZnition 3.4 (Game) A move is an extension of a given state of size as follows:  (1)  Az  A"  Az  A"  fi  fi    Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTLa03) 1530-1311/03 $17.00 AS 2003 IEEE  1. player I (spoiler) chooses a variable name AVA" and an element of the appropriate sort from the domains of  or  .
1 2. player II (duplicator) then chooses an element of the same sort from the domains of the other structure.
The values AVA" ,  AVA" , and fi AVA" are used to extend the three components of  yielding a new state of size   .
A game of  moves starting from a given state  consists of extending  by  moves.
Duplicator wins a game of  moves starting from state  if, after  moves the game ends in a winning state.
Spoiler wins otherwise.
Duplicator has a winning strategy for games of  moves starting from a state  if he wins every game of  moves that starts in  .
Note that whenever the duplicator wins a game of  moves, starting from a state  , he also wins all games of length up to .
In other words, when following a winning strategy for the game, the duplicator always moves from a winning state to another winning state.
Lemma 3.5 A winning strategy for duplicator for games of length  starting from a state   fi 	 	 fi and with blocks of temporal variables bounded by   dedZnes an equivalence  relation 	  fi 	 fi over the class of  -structures..
This equivalence relation captures classes of  -structures indistinguishable by formulas in fi	  of limited quantidZer depth.
Proposition 3.6 Let  and  be two  -structures, 	  a dZnite set of  -dimensional temporal connectives such that fi	   ,  a vector of variable names fi Az 	    	  A" , and    fi 	    	   .
Then 	         fi 	       	      for all temporal properties  fi  .
  fi  fi	    	       such that  P r o o f. (sketch) The proof is similar to the proof in the dZrst-order case: games won by the spoiler correspond to formulas separating  from  .
The weakening of the winning condition for duplicator is equivalent to observing which variable names can appear together in atomic (sub-)formulas of embeddings of fi	  formulas into 1 Spoiler  can also decide whether a block of temporal variables should end at this point.
However, there is no advantage to the spoiler to choose blocks of size less than .
Thus we can assume that the blocks only end after they reach the maximal size allowed in the game.
  2-FOL.
Thus, formulas constructed from games won by the spoiler yield fi	  formulas (or, more precisely, their embeddings to 2-FOL) such that fi	    , that distinguish  from  .
On the other hand, given a formula    fi	 that separates  from  , we can construct a win for the spoiler of length at most  that obeys the variAz able compatibility conditions.
Corollary 3.7 A temporal property  cannot be expressed by fi	 , fi	   , if and only if for all    we can dZnd  and  such that 1.
 	     fi  AV  	  ,  2.
 	    , and  	    .
where    fiAz 	    	   A".
3.2 Order-independent Temporal Properties The game-based techniques are very general.
However, their application to discrete linearly ordered structures, such as the dZnite temporal structures considered in this paper, is rather difdZcult.
The main difdZculty is in dZnding a winning strategy for duplicator in the presence of discrete order.
Indeed, the separation result [16] crucially depends on the use of dense order of the time domain (and the use of indZnite temporal structures).
Abiteboul et al.
[1] avoided the difdZculty by using a different technique based on communication protocols.
However, their technique doesnat seem to generalize to multi-dimensional temporal logics.
Recently, however, Bidoit et al.
[3, 4] made a crucial observation that sidesteps the difdZculties associated with discrete order of the temporal domain.
DedZnition 3.8 (Order-independent Property) Let  be a closed formula in 2-FOL.
We say that  dedZnes an orderindependent property if for all temporal structures  and  that differ only in the linear ordering of time instants we have    fi   .
Then, using a variant of Craigas Interpolation Theorem [6, 9], the expressive power of order independent temporal properties is related to Ehrenfeucht-FraAaEsseE Games that consider only equality on the temporal structure.
We extend the technique to  -dimensional temporal logics.
First an extension of the interpolation theorem: Proposition 3.9 Let 	  be a dZnite set of temporal connectives over a linear order    and    fi	  a formula expressing an order-independent property.
Then there is a set of connectives dedZned only using equality    on  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTLa03) 1530-1311/03 $17.00 AS 2003 IEEE  the temporal structure, such that  .
and a formula  fi   P r o o f. (sketch) Let  be a sentence expressing that a binary relation  is linear order and   the formula  in which all occurrences of the symbol  have been replaced by the binary symbol .
The rest follows from a direct extension of Craigas Interpolation Theorem [9], since, for order-independent properties, we have      fi fi    fi  for  fi two binary symbols not in .
The rest is similar to the proof used by Chang and Keisler [6] pages 87a89; we only have to observe that the interpolant is indeed an fi  formula.
Az  4 Inexpressibility Results We use the following property to separate from 2-FOL.
 fi    DedZnition 4.1 (Cover Property) We dedZne  fi                    we call fi the cover property of level 	 .
The fi property asks if there are 	 	   time instants such that the data values related by the relation  to these time instants (snapshots) acovera the data domain of the underlying temporal structure.
Note that the cover properties can also be expressed by the following range-restricted [2] formula:                       First we show that the cover property fi can also be dedZned in 	 	  -dimensional temporal logic.
Lemma 4.2 There is a temporal property fi fi  such that  fi      fi fi 	    fi fi for any  -structure  .
Moreover, the quantidZer depth of the connectives  fi   	 	  .
In the rest of this section we show that fi  fi  fi   for any set of temporal connectives  such that     .
We use the following pairs of temporal structures containing a single unary relational symbol  to show our separation results.
           	 	                 	 	      where     is the linear order on integers and where the instances of the relation  are dedZned by    	     fifi             for fi      fi the enumeration of all  element subsets of the data domain of the underlying structure.
Note that in both    and   the instances of the predicate symbol  are dZnite.
Thus, the temporal domain of the structures can be restricted to a dZnite sets of time instants corresponding to non-empty snapshots 2 of  (and the state ), yielding dZnite  -structures.
Such a restriction does not affect the following results.
be a set of temporal connectives such Lemma 4.3 Let that that    .
Then fi .
fi  fi P r o o f. It is easy to see that for any   	 we have     fi    fi fi fi  fi    fi fifi  We now show that temporal properties in fi  with quantidZer depth at most  cannot separate the above temporal structures  fi  and fi  .
We show this by dedZning a winning strategy for duplicator in the   fi     fi       game and then appealing to Corollary 3.7.
The game starts in an initial state     .
The following strategy shows that duplicator can play  moves always ending in a winning state (clearly, the initial state is a winning state for the duplicator).
During the course of the game the duplicator maintains an invariant guaranteeing a winning strategy.
Let fiA"      fi be all temporal variables in fi compatible with the next move3 .
For each of these variables we dedZne a snapshot in the corresponding structure as follows:  P r o o f. We dedZne  fi      fi fi   fi     fi fi    fi  Az          where  , .
.
.
,  , and Az are 	 	 -dimensional connectives Az introduced in Example 2.6.
            where          snapshot of at time  is the set   .
3 Note that we can consider pairs of variables only, since all relations in the signatures of the fi -structures used in this proof are binary.
2A  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTLa03) 1530-1311/03 $17.00 AS 2003 IEEE  Note that the elements and  associated with  in  are from the appropriate temporal sort of fi and  , respectively.
Also, the cardinality of each of the   and   is .
The invariant for the game is dedZned by the following two rules:  2.
  variable in  and      for all  such that  is a data A" 	       	 fi fi;       for all      1.
        we consider the case in which the spoiler chooses  .
The duplicator replies as follows:      A"	       	 fi fi.
1.
A temporal move  : There can be at most  fi    temporal variables compatible with the current move and at most    data variables in the current state.
Assume, without loss of generality, that the spoiler has   .
The duplicator then replies chosen a value as follows:      If the value is equal to a value associated with any of the temporal variables compatible with this move, the duplicator chooses the corresponding value in the other structure.
Otherwise, the new move dedZnes a distinct snapshot   .
The duplicator must choose    such that the snapshot  preserves the above dedZned invariant.
This, however, is always possible since all -element subsets of the respective data domains are present as snapshots in the structures and there can be at most fi  compatible temporal moves (thus the data domains must contain at least  elements outside of the union of all the snapshots).
The case in which the spoiler chooses  metric to the above case.
  is sym-  2.
A data move  : There are  temporal variables compatible with this move (dedZning  snapshots) and at most    data variables in the current state.
Again,  Otherwise, if the value belongs to       	  	 	   A"	       	  fi fi    the duplicator chooses an arbitrary value that belongs to  In other words, we require that, in both the structures, all values associated with the data variables in the current state belong to the same snapshots associated with the compatible temporal variables and that the cardinalities of intersections of the snapshots associated with an arbitrary subset of the compatible temporal variables are the same.
This invariant is used to guarantee that all states of the game are winning states for duplicator.
Since the initial state 	 	  is a winning state for the duplicator that satisdZes the above conditions (trivially), the following strategy preserves the invariant through  game moves.
There are two cases to consider.
If the value is associated with any of the data variables in the current state, duplicator picks the corresponding value from the current state.
      	    A" 	       	  fi fi    this is always possible since the intersections have the same cardinality in both structures.
  Otherwise the value does not belong to any of the snapshots   and the duplicator thus picks a value not belonging to any of the snapshots   in the other structure.
Again, this is always possible, since the set of such data values must contain at least  values in both of the structures.
The case in which the duplicator chooses  again symmetric.
 is  In all the cases, the resulting state is a winning state for the duplicator for games of up to  moves starting from the state 	 	  .
Az Now we are ready to apply the results of Bidoit et al.
[3, 4] to extend this result to ordered temporal domains.
Lemma 4.4 fi is order-independent.
P r o o f. Immediate from dedZnition of order-independence (Def.
3.8).
Az Combining the results of Corollary 3.7, Proposition 3.9, and Lemmas 4.3 and 4.4 we obtain the desired result.
be a dZnite set of temporal connectives Theorem 4.5 Let   dedZned over a discrete linear order 	.
Then there is a dZnite ,  set    , such that   is strictly Az Az  for     		  fi fi .
weaker than   Az This result holds for dZnite and discrete (integer-like) dZows of time.
Dense dZows of time (with time instants modeled by  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTLa03) 1530-1311/03 $17.00 AS 2003 IEEE  rational numbers) were considered by Toman and Niwinski [16].
However, the construction of the structures  and   and the associated game for  gives an alternative proof for dense linear order as well.
Corollary 4.6 fi 2-FOLfi  for any   connectives  .
 is strictly weaker than  and any dZnite set of temporal  5 Conclusion We have shown that there cannot be a dZxed-dimensional and expressively complete temporal logic.
This fact also precludes the use of dZxed-dimensional temporal relational algebras [8], e.g., algebras based on the bi-temporal data model [12], to implement expressively complete temporal query languages based on 2-FOL, e.g., SQL/TP [14, 15].
5.1 Future Work While the results in this paper achieve our main goala showing that dZxed-dimensional temporal logics are strictly weaker than two-sorted dZrst order logic, no matter what dZnite set of  -dimensional connectives is usedathe results are not quite satisfactory (tight enough): using the approach in this paper, the standard one-dimensional  is separated from 2-FOL by the fi Az property.
The results in [1, 3, 4, 16] can be easily modidZed to show the separation using fiA" .
We conjecture the following: Conjecture 5.1 fi is not expressible in fi independently of the quantidZer depth of connectives in .
  Another direction of research considers more complex structures of time (the language and theory of the temporal domain), e.g., temporal domains equipped with distance measure, periodic sets, etc.
For the separation results to hold, we have to extend Theorem 3.9 to reduce temporal connectives in the extended language of the temporal domain to connectives dedZned only using equality for temporal properties preserved under permutations of the temporal domain.
The separation results, however, can only apply to temporal domains whose theories cannot encode pairs of time instants as a single instant (i.e., theories that cannot dedZne pairs and projections).
Acknowledgments The author gratefully acknowledge the Natural Sciences and Engineering Research Council of Canada, the Canadian Foundation for Innovation, the Communications and Information Technology of Ontario, and Nortel Networks Ltd. for their support of this research.
References [1] S. Abiteboul, L. Herr, and J.
Van den Bussche.
Temporal Versus First-Order Logic to Query Temporal Databases.
In ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, pages 49a57, 1996.
[2] S. Abiteboul, R. Hull, and V. Vianu.
Foundations of Databases.
Addison-Wesley, 1995.
[3] N. Bidoit, S. de Amo, and L. SegoudZn.
ProprieEteEs Temporelles IndpeEndantes de laordre.
In 17eEmes JourneEes de Bases de DonneEes AvanceEes, pages 219a225, 2001.
[4] N. Bidoit, S. de Amo, and L. SegoudZn.
Order Independent Temporal Properties.
Technical report, http://www.deamo.prof.ufu.br/arquivos/ journalLC.ps, 2002.
(to appear in Journal of Logic and Computation).
[5] M. H. BoEhlen, C. S. Jensen, and R. T. Snodgrass.
Temporal Statement ModidZers.
ACM Transactions on Database Systems, 25(4):407a456, 2000.
[6] C. C. Chang and H. J. Keisler.
Model Theory, 3rd ed.
Studies in Logic and Foundations of Mathematics, vol.
73.
Elsevier Science Publishers, 1985.
[7] J. Chomicki and D. Toman.
Temporal Logic in Information Systems.
In J. Chomicki and G. Saake, editors, Logics for Databases and Information Systems, pages 31a70.
Kluwer, 1998.
[8] J. Clifford, A. Croker, and A. Tuzhilin.
On Completeness of Historical Relational Query Languages.
ACM Transactions on Database Systems, 19(1):64a116, 1994.
[9] W. Craig.
Three uses of the Herbrand-Genzen theorem in relating model theory and proof theory.
Journal of Symbolic Logic, 22:269a285, 1957.
[10] A. Ehrenfeucht.
An application of games to the completeness problem for formalized theories.
Fundamenta Mathematicae, 49:129a141, 1961.
[11] D. M. Gabbay, I. Hodkinson, and M. Reynolds.
Temporal Logic: Mathematical Foundations and Computational Aspects.
Oxford University Press, 1994.
[12] C. S. Jensen, R. T. Snodgrass, and M. D. Soo.
The TSQL2 Data Model.
In The TSQL2 Temporal Query Language, pages 153a238.
Kluwer Academic Publishers, 1995.
[13] R. T. Snodgrass, M. H. BoEhlen, C. S. Jensen, and A. Steiner.
Adding valid time to sql/temporal.
ISO/IEC JTC1/SC21/WG3 DBL MAD-146r2 21/11/96, (change proposal), International Organization for Standardization, 1996.
[14] D. Toman.
Point-based Temporal Extensions of SQL.
In International Conference on Deductive and Object-Oriented Databases, pages 103a121, 1997.
[15] D. Toman.
SQL/TP: A Temporal Extension of SQL.
In Kuper, Libkin, and Paredaens, editors, Constraint Databases, chapter 19, pages 391a399.
Springer Verlag, 2000.
[16] D. Toman and D. Niwinski.
First-Order Queries over Temporal Databases Inexpressible in Temporal Logic.
In Advances in Database Technology, EDBTa96, volume 1057, pages 307a324, 1996.
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTLa03) 1530-1311/03 $17.00 AS 2003 IEEE
Combining Simultaneous Values and Temporal Data Dependencies Avigdor Gal & Dov Dori Information Systems Engineering Department Faculty of Industrial Engineering and Management Technion - Israel Institute of Technology Haifa, 32000, Israel  Abstract  In temporal databases there are situations where multiple values of the same data item have overlapping validity times.
In addition to the common case of multi-valued properties, there are several possible semantics to multiple values with overlapping validity times of the same data item.
We refer to such data items as having simultaneous values.
This paper presents a polynomial algorithm for ecient handling of simultaneous values in a database with temporal data dependencies|integrity rules that dene relationships among values of dierent data items in a temporal database.
The algorithm is demonstrated using a case study from the game theory area.
An implementation of the algorithm is integrated in a prototype of a temporal active database.
keywords: temporal databases, simultaneous values, uncertainty, temporal data dependencies, action reasoning  1 Introduction and Motivation  A temporal database is a database that supports some aspects of time [5].
One of the basic temporal aspects supported by many temporal databases is the valid time, representing the time a data-item is considered to be true in the modeled reality [5].
There are situations where multiple values of the same data-item have overlapping valid times.
The multi-valued property is the most common case, where several values are grouped into a single property [4].
For example, a property that contains the languages that a person speaks, can have a set of values grouped into a single property.
There are situations, however, where multiple values with overlapping valid times of the same data-item exist in the database, but with dierent semantics than the multi-valued case.
We refer to these data-items as having simultaneous values.
While in the multi-valued case all values whose valid time include t are deemed to be valid in the modeled reality, it is possible that only part of the candidate values, i.e.
the values that were assigned to a data-item at time t, represent the data-item's value in the real world.
For example, a data-item that contains a spouse name is limited by law to be single The work was conducted while the author was in the Technion.
He is currently at the Department of Computer Science, University of Toronto, Toronto, Ontario, M5S 3H5 CANADA.
valued.
When there are several alternatives for the spouse name due to uncertain information, then only one value of the set is the data-item's value.
Each value of the set is possibly the data-item's value.
In temporal databases, change of decisions about the value and valid time of a data-item may cause a situation where two values of the same data-item have overlapping valid times.
For example, a value val1 valid during [Jan 1994, Mar 1994) of a data-item  is augmented at time point Aug 1993 by a value val2 valid during [Feb 1994, Apr 1994).
 has more than one value in the interval [Feb 1994, Mar 1994).
In some cases, the value that was inserted later corrects an erroneous value that was inserted earlier.
In other cases, both values are possibly correct, each with respect to a dierent time point.
Simultaneous values enable dierent semantics in mapping the stored values to the modeled reality values.
It is particularly useful in applications where a data-item may have multiple values representing the existence of dierent alternatives.
For example, if knowledge arrives from various sources, then no apriori selection of a single value should be enforced.
Instead, for each database retrieval operation, the user can choose the appropriate value, values or any aggregation of those values.
Handling simultaneous values in a temporal database requires the use of optimized update and retrieval mechanisms.
The maintenance problem of simultaneous values becomes more arduous in temporal active databases [2], where temporal data dependencies are enforced.
A temporal data dependency is a tool that supports rules for manipulating data-items which may have a variety of temporal characteristics.
Temporal data dependencies can be viewed as a type of integrity rules of the temporal active database.
Violating them, activates database operations that react to restore the database integrity.
As an example for the use of temporal data dependencies, we can consider decision support systems [3]|systems that model decisions about actions that should be performed in a target system.
Such systems consist of decision models that are rooted in the operations research or articial intelligence disciplines, and of a database, that stores the necessary data to support the decision models.
Decision support systems can benet signicantly from the temporal active paradigm.
As a concrete motivating case study, we present the following application of a decision support system, based on the Cournot game [7]; [1].
Three instant coffee manufacturers|Bilbo, Frodo and Gandalf, decide each month about the quantity of coee to be produced in the next month.
Each manufacturer bases the decision about its manufactured quantity upon estimation of the quantities manufactured by the other two manufacturers, its own strategy (maximum revenue, a certain market share, etc.
), and general knowledge about the market behavior.
Each manufacturer has its own deadline for making the production quantity decision.
We assume a single market price for the manufactured type of instant coee, which is determined periodically as a function of the total quantity produced in that period.
The relationships between the market price and the total quantity is modeled by the constraint Total ?
Quantity  Market ?
Price = Market ?
Constant (1) Each manufacturer attempts to estimate the best decision to be taken, based on its own competition strategy and the two competitors' decisions and competition strategies.
For example, Bilbo may assume that Frodo and Gandalf have an objective of maximum prot. Consequently, each manufacturer would like to produce as much coee as possible without lowering the price to a level that decreases its total prot. By assuming the competition strategy of both Frodo and Gandalf, Bilbo can estimate their production decisions and determine the optimal production level, based on the following temporal data dependency: Production-Decision q Market-Constant :=  Competitors-Total-Estimation ?
Unit-Cost Competitors-Total-Estimation  This temporal data dependency is a periodical result of maximizing the prot function of a single manufacturer.
The derivation of the temporal data dependency is given in Appendix A.
Other strategies would yield dierent temporal data dependencies.
Competitors-Total-Estimation is the sum of the production estimations of the other two manufacturers.
Since this information is often misleading, each manufacturer should collect as much estimations as possible on each one of the competitors.
The temporal dependency graph can be evaluated each time there is a change in one of the data-items MarketConstant, Competitors-Total-Estimation or the manufacturer's Unit-Cost.
Alternatively, it can be evaluated once each period, just before the manufacturer has to decide about the quantity to be produced for the following period.
Figure 1 presents the data over time of the dataitems Market-Constant, Competitors-Total-Estimation, Unit-Cost, and the resulting Production-Decision of Bilbo.
As the gure shows, the temporal validity of each value is bounded.
For example, Market-Constant has the value 10000 during the interval [Feb 94, June 94).1 The resulting values of Production-Decision are 1  We use a single month granularity, hence this interval is  Market Constant  10000 9500 9000 Feb 94  June 94  Sep Oct Dec Feb Apr 94 94 94 95 95  Aug 95  t  Aug 95  t  Aug 95  t  Aug 95  t  630  Competitors Total Estimations  600 570 540 Feb 94  June 94  Sep Oct Dec Feb Apr 94 94 94 95 95  Unit Cost  7 6 Feb 94  June 94  Sep Oct Dec Feb Apr 94 94 94 95 95  405 400  Production Decision  385 380 375 316 293  Feb 94  June 94  270  Sep Oct Dec Feb Apr 94 94 94 95 95  Figure 1: Exemplary data of Bilbo in the coee manufacturers case study valid during the time intervals as computed using all data-items that determine Production-Decision and shown in the bottom graph of Figure 1.
For example, a Market-Constant of 10000, a CompetitorsTotal-Estimations of 600, and a Unit-Cost of 6, yield a Production-Decision of 400.
Since during the interval [Jan 94, June 94), the value of Market-Constant is 10000, Competitors-Total-Estimations is either 600 or 570, and Unit-Cost is 6, Production-Decision in that interval is either 400 or 405.
As Figure 1 demonstrates, the number of values and their associated temporal intervals resulting from the computation of a single temporal data dependency may be very large.
These values are a subset of the Cartesian product of all the possible values of each data-item.
A naive approach would consider all the possible combinations in the Cartesian set (342=24 combinations in the example of Figure 1), yielding an algorithm with high time complexity.
However, due to the bound temporal validity of values, usually only a small subset of the combinations in the Cartesian set should be considered.
For example, in Figure 1 only interpreted as all the days from February 1, 1994 to May 31, 1994 (June 1, 1994 is not included).
A time interval is dened in [5] as \the time between two insatnces" and can be represented as either close or semi-open intervals.
8 combinations out of the 24 possible ones should be considered.
This work presents an algorithm that eciently computes temporal data dependencies.
Our approach for ecient evaluation of temporal data dependencies is based in part on previous works on computing temporal aggregates, including [8] and [6].
An aggregate function, such as selecting the minimumvalue of a set, is applied to a set of values (e.g.
relations in the relational database model) and yields a scalar value.
In temporal databases, the aggregate function is, in general, time dependent, i.e.
the result of the aggregate function is applied to a set of values, each possibly having a dierent temporal validity.
The calculation of temporal data dependencies is an extension of aggregate computing with temporal grouping, where the resulting values are grouped by time.
To carry out such calculation, it is necessary to know which values have overlapping validity intervals, and to consider each value in its own validity interval.
The approach proposed in [8] rst determines constant intervals as intervals within which there is no change in the data-item value.
It then selects tuples that overlap each of these constant intervals and calculates the result.
The work in [6] is based on a tree data structure for the time axis partition.
Extending these approaches to solve the problem of evaluating temporal data dependencies, we present a polynomialalgorithm for ecient evaluation of temporal data dependencies with simultaneous values.
The computation is not necessarily an aggregate operation that involves a single type of data-item with several values.
Rather, it is a formula that may involve several types of data-items, each of which may consist of simultaneous values.
The algorithm constructs a list sorted according to the time validity of data-items values, and then calculates the result as a function of the values in the list elements.
Section 2 presents the algorithm for calculating temporal data dependencies with simultaneous values, while the properties of the algorithm are discussed in Section 3.
2 Evaluation of temporal data dependencies with simultaneous values  In this section we provide an outline of the algorithm for evaluating temporal data dependencies with data-items that consist of simultaneous values.
The algorithm consists of two phases, namely generating a constant interval list and computing the value for each combination of each constant interval element, as follows.
The constant interval list is sorted by the starting time points of the constant intervals.
The algorithm generates a partition of the valid time interval within which the temporal data dependency is to be determined.
Each element of the constant interval list has a valid time , and it consists of all the values whose validity covers .
Initially, a constant interval element is generated s , te ), where ts and with a valid time interval of [t te are the start and end time points of the interval within which the temporal data dependency is to be  evaluated, respectively.
Each value is processed with respect to an interval that consists of its starting time point, as follows.
Let [ts , te ) be a valid time interval of a value val, and [tis , tie ) a constant time interval associated with a constant interval element cii .
If [ts, te )\[tis, tie )6= ;, then there are six possible relationships between [ts , te ) and [tis, tie ), which are listed below along with the corresponding actions taken by the algorithm.
1. ts = tis and te < tie : replace cii with two constant interval elements, ci with [ts , te ) and ci with [te, tie ).
ci and ci receive the values of cii , and val is added to ci .
2. ts = tis and te = tie: add val to cii .
3. ts = tis and te > tie: add val to cii , and process val again with a valid time of [tie , te ).
4. ts > tis and te < tie : replace cii with three constant interval elements, ci with [tis, ts), ci with [ts , te ) and ci with [te, tie ).
ci , ci and ci receive the values of cii , and val is added to ci .
5. ts > tis and te = tie : replace cii with two constant interval elements, ci with [tis, ts ) and ci with [ts, te ).
ci and ci receive the values of cii , and val is added to ci .
6. ts > tis and te > tie : replace cii with two constant interval elements, ci with [tis, ts ) and ci with [ts, te ).
ci and ci receive the values of cii , and val is added to ci .
In addition, process val again with a valid time of [tie, te ).
As an example of the activation of the rst part of the algorithm, consider the data set of Figure 1, and assume that the interval within which the temporal data dependency is to be evaluated is [Feb 94, Aug 95).
The initial element of the list would be h[Feb 94, Aug 95), Market-Constant=, Competitors-TotalEstimations=, Unit-Cost=i.
The market-Constant values were processed rst, then the CompetitorsTotal-Estimations values, and nally the Unit-Cost values.
The full constant interval list is presented in Figure 2.
To enhance comprehension, the gure presents all the elements that were generated throughout the process, in a form of a tree.
Each node in the tree (except the root node) is an element of the list that was generated as a result of processing a value.
A value at the bottom of a node represents the value whose processing resulted in splitting the node.
The nal Constant Interval List (CIL) is the set of all leaf nodes of the tree, represented in Figure 2 by bold rectangles.
All other nodes were deleted during the process.
The Production-Decision values, shown in Figure 1, are shown within the constant interval rectangles in Figure 2.
0  0  00  00  0  0  00  0  00  00  0  0  00  0  00  00  00  0  00  00  Constant interval: Market Constant: Competitors Total Estimation: Unit Cost: 10000  Constant interval: Market Constant:  Constant interval: 10000  Market Constant:  Competitors Total Estimation:  600, 570  Competitors Total Estimation:  Unit Cost:  6  Unit Cost:  9500  Constant interval:  Constant interval: Market Constant:  Market Constant: 9000  9500  Competitors Total Estimation:  Competitors Total Estimation:  Unit Cost:  Unit Cost:  600  Constant interval:  540  Constant interval:  Constant interval:  Constant interval:  Market Constant:  9500  Market Constant:  Market Constant:  9000  Market Constant:  9000  Competitors Total Estimation:  600, 570, 540  Competitors Total Estimation:  Competitors Total Estimation:  540, 630  Competitors Total Estimation:  630  Unit Cost:  Unit Cost:  Unit Cost:  6  9500  7  Unit Cost:  7  570  Constant interval: Market Constant:  3 Algorithm properties  Constant interval:  9500  This section discusses the algorithm complexity (Section 3.1) and the correctness of the algorithm (Section 3.2).
Market Constant: 9500  Competitors Total Estimation:  570, 540  Competitors Total Estimation:  Unit Cost:  6  Unit Cost:  540  6  Constant interval:  94).
The next constant interval element is scanned, and the same combination is found.
Therefore, the valid time of the combination is set to be [June 94, Oct 94).
The same combination is found again in the subsequent constant interval element, and the valid time of the combination is set to be [June 94, Dec 94).
At this point, the process is terminated since the following constant interval element does not consist of this combination.
The result of the second phase of the algorithm, applied on the data set of Figure 1 is the set of values of Production-Decision, which are also shown in Figure 1.
After deciding on the appropriate interval, the values are used for calculating the derived value for that interval.
For example, in the previous example, the derived value is calculated to be 385, valid during [June 94, Dec 94).
Constant interval:  Market Constant:  9500  Market Constant:  9500  Competitors Total Estimation:  540  Competitors Total Estimation:  540  Unit Cost:  6  Unit Cost:  7  Figure 2: The constant interval list (CIL) generation process A single constant interval element may consist of more than a single value for a data-item.
For example, the constant interval element with the constant interval [Feb 94, June 94) in Figure 2 has two sets of values: h10000, 600, 6i, and h10000, 570, 6i.
Each such set of values is a dierent combination for the calculation of the temporal data dependency, giving rise to a dierent value for the same interval.
The constant interval list may also be over split with respect to a combination, i.e.
it may have consecutive constant interval elements with identical sets of values.
For example, all the constant interval elements with the constant intervals [June 94, Sep 94), [Sep 94, Oct 94), and [Oct 94, Dec 94) consist of the combination h9500, 540, 6i.
This is a result of several overlapping values of the same data-item.
In this case, [June 94, Sep 94) and [Sep 94, Oct 94) are separate constant intervals, since the value 600 of CompetitorsTotal-Estimation is valid only during [June 94, Sep 94).
The second phase of the algorithm uses each of the possible value combinations in the constant interval elements to compute the new values.
The combinations are scanned for each constant interval element, starting from the constant interval element with the minimal valid time.
Subsequent constant interval elements are scanned to nd identical combinations.
If an identical combination is found, the valid time of the constant interval element is added to the valid time of the combination.
This process is repeated until no more identical combinations can be found.
For example, consider the example given in Figure 2.
The combination h9500, 540, 6i is selected from the constant interval element with the valid time of [June 94, Sep  3.1 Complexity  Let n be the number of processed values.
A value with a valid time interval of [ts, te ) can add two constant interval elements at the most, if for a constant interval element cii , ts > tis and te < tie , or if there are two constant interval elements cii and cij such that ts > tis and te < tje .
Consequently, if the number of processed values is n, then the upper bound on the number of constant interval elements is 2n.
For each value, the location of the rst constant interval element to be processed is searched.
This search is bounded by log2(2n).
In the worst case, a valid time of a value covers the valid times of all of the constant interval elements, resulting in 2n comparisons.
Hence, the time complexity of the CIL generation phase is bounded by O(n(log2 (2n) + 2n)) =O(n2 ).
The time complexity of the second phase of the algorithm is bound by O(m3 ), where: m is the number of all the valid combinations of a single value from all the data-items, i.e.
all the combinations for which values have valid time overlapping.
For example, in Figure 1, m=8 and the eight combinations are: h10000, 600, 6i, h10000, 570, 6i, h9500, 600, 6i, h9500, 570, 6i, h9500, 540, 6i, h9500, 540, 7i, h9000, 540, 7i, h9000, 630, 7i.
2n is the maximal number of constant interval elements.
for example, 18 is the maximal number of constant interval elements in Figure 1 since the number of state-elements is 9.
However, The actual number of constant interval elements is 7, as shown in Figure 2.
2mn is the maximal number of possible combinations in the list.
The algorithm generates all of the list combinations (2mn).
At each iteration of the algorithm, a single combination is processed.
The worst case is when at each iteration all the remaining combinations are scanned.
Thus, the worst case complexity is O(m2 n).
Since at each  constant interval element there is at least one combination, m  n. Therefore, the worst case complexity is bounded by O(m3 ).
From the discussion above we can conclude that the worst case complexity of the two phases of the algorithm is bounded by O(m3 ).
3.2 Correctness  Proposition 1 The partition of the time interval:  The entire set of constant interval elements constitute a partition of the evaluated interval.
The proof of Proposition 1 is done using induction on the number of constant interval elements.2 At each iteration we verify that each of the six possible relationships between a valid time of a value and a constant interval element results in a new constant interval list that maintains the partition assertion.
This proposition ensures the correctness of the list construction phase, where each value should allocate a single constant interval element.
Proposition 2 Algorithm correctness: The algorithm generates a correct result.
Given a set of values and a temporal data dependency, a correct result ensures that for each combination of values with overlapping valid times there is a value which is the result of applying the temporal data dependency on this combination, and its valid time consists of the intersection of the overlapping valid times of the values in the combination.
A simple algorithm, in which each combination of the Cartesian product is evaluated, can achieve a correct result but at a high computational cost.
The proof of Proposition 2 shows that if a combination is not processed by the algorithm, then it should not be in the resulting set.
4 Conclusion and future research  We have proposed an algorithm for ecient evaluation of temporal data dependencies in temporal databases with simultaneous values.
The algorithm consists of two phases, the rst generates a constant interval list and the second computes the value for each combination of each constant interval element.
A single constant interval element may consist of more than a single value for a data-item.
The constant interval list may also be over split with respect to a combination, i.e.
it may have consecutive constant interval elements with identical sets of values.
The time complexity of the calculation algorithm is bound by O(m3 ), where m is the number of all the valid combinations of a single value from all the data-items.
This result is less expensive, computation wise, than the 2 Full denitions and proofs of propositions in this paper can be obtained via anonymous ftp to ftp.technion.ac.il under directory/usr/local/servers/ftp/pub/supported/ie.
The le is called proofs.tex.
It is produced using LaTEX.
The proofs can also be obtained through the author's WWW home page, http://www.cs.toronto.edu/avigal.
result of scanning the Cartesian product (O(ni=1 mi ), where mi is the number of values of the i-th dataitem and n is the number of data-items involve in the calculation.
A prototype of a system that implements the algorithm exists on the basis of MAGIC 5.6 for DOS, under DOS 6.2.
Further research is aimed at a more general form of temporal data dependencies, where the valid time is dened indirectly through constraints, or relative to other time points.
References  [1] A. Cournot.
Researches into the Mathematical Principles of the Theory of Wealth.
Macmillan, New York, N.Y., 1897.
[2] O. Etzion, A. Gal, and A. Segev.
Temporal active databases.
In Proceedings of the International Workshop on an Infrastructure for Temporal Database, June 1993.
[3] K.M.
Van Hee, L.J.
Somers, and M. Voorhoeve.
A modeling environment for decision support systems.
Decision Support Systems, 7:241{251, 1991.
[4] R. Hull and R. King.
Semantic database modeling: Survey, application and research issues.
ACM Computing Surveys, 19(3):201{260, Sep 1987.
[5] C.S.
Jensen, J. Cliord, S.K.
Gadia, A. Segev, and R.T. Snodgrass.
A glossary of temporal database concepts.
ACM SIGMOD Record, 21(3):35{43, 1992.
[6] N. Kline and R.T. Snodgrass.
Computing temporal aggregates.
In Proceedings of the International Conference on Data Engineering, pages 223{231, Mar 1995.
[7] J. Tirole.
The Theory of Industrial Organization.
the MIT press, 1989.
[8] P.A.
Tuma.
Implementing historical aggregates in TempIS.
Master thesis.
Wayne State University, Nov. 1992.
Appendix A: The production decision temporal data dependency In this section we present the derivation of the temporal dependency graph given in Section 1.
The following notation is used: A  Prot B  Revenue C  Cost D  Market-Price E  Fixed-Cost F  Unit-Cost G  Market-Constant H  Total-Quantity I  Competitors-Total-Estimation X  Production-Decision We assume that Bilbo's strategy is to produce the amount that would maximize A, as follows.
A = B?C= X  D ?
(E + X  F) = X G H ?
(E + X  F) = X  X G+ I ?
(E + X  F) max (A) =) A0 = 0 =) G  (X(X+ +I) I)?2 X  G ?
F = 0 2 =) G  (X + I) ?
(XX + GI)?2 F  (X + I) = 0 =) F  X2 + 2  F  I  X + F  I2 ?
G  I = 0 p ?
2  F  I  4  F2  I2 ?
4  F  (F  I2 - G  I) =) X = = 2F qG  I ?I F  X is non-negative.
q  =) X = max( GF I ?
I, 0)
Proceedings of TIME-96  1  Gaining Efficiency and Flexibility in the Simple Temporal Problem Amedeo Cesta  Angelo Oddi  IP-CNR National Research Council of Italy Viale Marx 15, I-00137 Rome, Italy amedeo@pscs2.irmkant.rm.cnr.it  Dipartimento di Informatica e Sistemistica Universita di Roma "La Sapienza" Via Salaria 113, I-00198 Rome, Italy oddi@assi.dis.uniroma1.it  Abstract The paper deals with the problem of managing quantitative temporal networks without disjunctive constraints.
The problem is known as Simple Temporal Problem.
Dynamic management algorithms are considered to be coupled with incremental constraint posting approaches for planning and scheduling.
A basic algorithm for incremental propagation of a new time constraint is presented that is a modification of the Bellman-Ford algorithm for Single Source Shortest Path Problem.
For this algorithm a sufficient condition for inconsistency is given based on cycle detection in the shortest paths graph.
Moreover, the problem of constraint retraction from a consistent situation is considered and properties for repropagating the network locally are exploited.
Some experiments are also presented that show the usefulness of the properties.
1  Introduction  Knowledge-based architectures for planning and scheduling based on constraint propagation, e.g.
[5, 3, 8, 2], perform incremental constraint posting and retraction on a current partial solution.
A complete plan is created by efficiently searching in partial plans space, and, in other cases, it is adapted to new situations by partially removing parts of the solution.
A module for temporal constraint management that supports plan space search and current solution maintenance should be extremely efficient because is called into play at any modification (monotonic or not) of the current plan.
Such an efficiency is usually guaranteed by restricting the expressive power of the temporal representation.
Usually the so called Simple Temporal Problem (STP) [7] is used that allows the representation of binary quantitative constraints without disjunction.
In spite of the restriction of expressivity, also for STP it results useful to consider how the efficiency of manipulation primitives may be improved.
In our research, we have been investigating possible  algorithms for managing temporal information that: (a) allow dynamic changes of the constraint set for both incremental constraint posting and retraction; (b) exploit the localization of effects of any change in a subnetwork of the whole constraint graph; (c) do not compute the minimal network as done in [7] but just check for consistency.
A previous paper [1], in the same line of [6], has concerned the specialization of arc-consistency algorithm to the STP.
The choice of arc-consistency to propagate temporal constraints was motivated by the good trade-off wrt space and time complexity.
In the same paper some properties were given that were shown experimentally to improve the performance of the algorithm in the average case.
The present paper contains a further step in the direction of gaining efficiency in the solution of the STP.
After presenting the essentials of STP (Section 2), it presents dynamic algorithms based on the well known Bellman-Ford algorithm for computing Single Source Shortest Paths (Section 3).
It also introduces (Section 4) the concept of dependency that computes a particular spanning tree on the constraint graphs that allows the definitions of a sufficient condition for inconsistency detection (Section 5) and an algorithm for local constraint retraction (Section 6).
Some experiments (Section 7) show the usefulness of the properties.
2  The Temporal Problem  A Simple Temporal Problem is defined in [7] and involves a set of temporal variables {X1 , .
.
.
, Xn }, having continuous domains [lbi , ubi ] and a set of constraints {aij <= Xj - Xi <= bij }, where aij >= 0, bij >= 0 and aij <= bij .
A special variable X0 is added to represent the origin of the time (the beginning of the considered temporal horizon) and its domain is fixed to [0, 0].
A solution of the STP is a tuple (xi .
.
.
xn ) such that xi [?]
[lbi , ubi ] and every constraint aij <= Xj - Xi <= bij is satisfied.
An STP is inconsis-  Proceedings of TIME-96 tent if no solution exists.
In order to find the set of possible values [lbi , ubi ] for every variable Xi , a direct constraint graph Gd (Vd , Ed ) is associated to the STP, where the set of nodes Vd represents the set of variables {X1 , .
.
.
, Xn } and the set of edges Ed represents the set of constraints {aij <= Xj - Xi <= bij }.
Given a constraint aij <= Xj -Xi <= bij , we can rewrite it as a pair of inequalities: * Xj - Xi <= bij * Xi - Xj <= -aij For every linear inequality Xj - Xi <= wij (with wij equal to bij or -aij ) we have an edge (i, j) in Gd (Vd , Ed ) labeled with the weight wij .
Each path in Gd from the node i to the node j, i = i0 , i1 .
.
.
im = j induces between the variables Xj and Xi the constraint Xj - Xi <= lij , where lij is the sum of weights along the path, that is lij = w01 +w12 +.
.
.+w(m-1)m .
Considering the set of all paths between the nodes i and j, these paths induce a constraint Xj - Xi <= dij , where dij is the length of a shortest path between the nodes i and j.
Finally a cycle on the graph Gd is closed path i = i0 , i1 .
.
.
im = i and a negative cycle is a cycle with associated a negative length (lii < 0).
In [7] some useful properties of an STP are given and reported in the following theorems.
Theorem 1 [7] A Simple Temporal Problem is consistent iff Gd does not have negative cycles.
Defining d0i as the length of a shortest path on the graph Gd from the origin 0 and the node i and di0 as the length of a shortest path from the node i to the origin 0 we can also have the other following theorem.
Theorem 2 [7] Given a consistent Simple Temporal Problem, the set [lbi , ubi ] of feasible values for the variable Xi is the interval [-di0 , d0i ].
Theorem 2 shows that the Simple Temporal Problem is a Shortest Paths Problem and precisely we have to calculate two sets of shortest paths length: (a) the set of shortest paths from the node 0 (that represent the variable X0 ) to the nodes 1 .
.
.
n; (b) and the set of shortest paths from the nodes 1 .
.
.
n to node 0.
3  An Algorithm for the STP  To solve the basic STP we use the Bellman-Ford algorithm for the Single Source Shortest Paths Problem [4] giving an incremental version of the algorithm named Propagation, which accepts as an input the graph Gd and a new constraint Cij (where Cij = aij <= Xj - Xi <= bij ) and produces in output a new set of feasible values [-di0 , d0i ] for every variable Xi or a value fail in the case the new constraint induces a inconsistent situation.
To understand the algorithm, shown in Figure 1, some  2 simple definitions are useful: given a node i of the graph Gd we define EdgesOut(i) as the set of edges which leave from the node i and EdgesIn(i) as the set of edges which arrive to the node i. T and F are the boolean constants T rue and F alse.
The algorithm has two differences wrt the standard implementation on Bellman-Ford with a queue.
First, it calculates at the same time two sets of shortest distances.
Second, the algorithm has an internal test which detects negative cycles on the graph Gd which contain the reference node X0 .
In addition, every node u [?]
Vd has two boolean marks: LB(u) and U B(u).
This marks are useful in order to distinguish the two types of propagation in the graph Gd , that is, respectively U B(u) = T and LB(u) = T when a node is modified by the propagation process for the distance d0i and the distances di0 .
The Propagation calculates the set of distances {d0i } between Steps 6 and 14 and the set of distances {di0 } between Steps 16 and 24.
This last section of the algorithm, in order to calculates the set of distances di0 , (that is, the length of the shortest paths on the graph Gd between the nodes 1 .
.
.
n and the node 0) considers the set of direct edges in Gd as oriented in the opposite direction.
In this way when a shortest path between the nodes 0 and i is found, it is actually a shortest path in the opposite direction.
Finally, the tests at Steps 10 and 20 check for negative cycles in the graph Gd when they contain the node 0.
The algorithm calculates also two shortest path trees.
In fact Steps 11 and 21 respectively update the predecessor function pu, which represents the shortest path tree of the distances {d0i } and the predecessor function pl, which represents the shortest path tree of the distances {di0 }.
The complexity of the algorithm, as well known, is O(EN ).
Where N and E are respectively the number of nodes and the number of edges in Gd .
negative cycles  4  Focusing on Dependency  The temporal meaning of shortest path trees on the Gd graph is simple.
Every bound {d0i } (or {di0 }) is induced by the set of temporal constraints in the shortest paths between the origin 0 and the node i (or between the node i the origin 0).
The following definitions are useful: Definition 1 Let Gd a consistent distance graph.
The tree DTub of the shortest paths from the origin 0 to the nodes 1 .
.
.
n is called Upper Bounds' Dependency Tree.
Definition 2 Let Gd a consistent distance graph.
The tree DTlb of the shortest paths from to the nodes  Proceedings of TIME-96 Propagation (Gd , Cij ) 1. begin 2.
Q - {i, j} 2a.
LB(i) ::= T ; U B(i) ::= T 2b.
LB(j) ::= T ; U B(j) ::= T 3.
While Q 6= [?]
do begin 4. u - P op(Q) 5. if U B(u) then 6.
Foreach (u, v) [?]
EdgesOut(u) do 7. if d0u + wuv < d0v 8. then begin 9. d0v ::= d0u + wuv 10. if d0v + dv0 < 0 then exit(fail) 11. pu(v) ::= u 12.
U B(v) ::= T 13. if v 6[?]
Q then Q - Q [?]
{v} 14. end 15. if LB(u) then 16.
Foreach (u, v) [?]
EdgesIn(u) do 17. if du0 + wvu < dv0 18. then begin 19. dv0 ::= du0 + wvu 20. if d0v + dv0 < 0 then exit(fail) 21. pl(v) ::= u 22.
LB(v) ::= T 23. if v 6[?]
Q then Q - Q [?]
{v} 24. end 25.
LB(u) ::= F 26.
U B(u) ::= F 27. end 28. end Figure 1: Propagation algorithm  1 .
.
.
n to origin 0 is called Lower Bounds' Dependency Tree.
If a given graph Gd is consistent then the trees DTub and DTlb are always defined.
In fact, without negative cycles, the distances {d0i } and {di0 } are always defined.
In general, the trees DTub and DTlb may not be single.
In fact, the graph Gd may contain several paths with the same length.
A relevant situation is verified when the graph Gd contains at least a negative cycle.
In this case, the following Theorem holds.
Theorem 3 Give a distance graph Gd .
If during the update process of the Propagation algorithm the predecessor function pu (pl) represents a graph containing at least a cycle then the graph Gd is inconsistent.
3 Proof.
We give the proof for the distances {d0i }, but an analogous proof can be given for the distances {di0 }.
Suppose by hypothesis that during the update process of the algorithm, a dependency path exists between the nodes i and j named p1 : i = i0 , i1 .
.
.
ir = j, that is, a path such that pu(ik ) = ik-1 , with k = 1 .
.
.
r. If we sum the weights along this path, we have the following relation: d0j - d0i = w01 + w12 + .
.
.
+ w(r-1)r .
(1)  If successively the Propagation algorithm builds a dependency path p2 : j = j0 , j1 .
.
.
js = j, we can write the following relation: dnew - d0j = w01 + w12 + .
.
.
+ w(s-1)s .
0i  (2)  Where dnew is the new value of the distances d0i 0i updated along the path p2 .
If we sum the relations 1 and 2 we obtain the length of the cycle lii : lii = dnew - d0i .
0i  (3)  Observing that the link of two paths p1 and p2 is a cycle and dnew < d0i , then the length lii is negative 0i and this proves the inconsistency of the graph Gd .
2  5  Cycle Detection  In order to use the property expressed by Theorem 3 few changes are introduced in the Propagation algorithm.
Each edge (i, j) in the graph Gd have three new boolean marks: N EW ((i, j)), LBP ((i, j)) and U BP ((i, j)).
The mark N EW is useful in order to distinguish the new edges introduced in Gd , by the new temporal constraint Cij .
In fact, if in the graph there is at least a negative cycle, then it must contain at least one of the new edges introduced.
Instead, the two marks LBP ((i, j)) and U BP ((i, j)) are used to check when a bound changes two times as explained in the next Theorem 4: Theorem 4 Let Gd a consistent distance graph and Cij = aij <= Xj - Xi <= bij the new constraint added.
If during the propagation process the distance d0j (di0 ) changes two times, then the constraint Cij is inconsistent with the other constraints represented in Gd .
Proof.
We give the proof for the distances {d0j }, but an analogous proof can be given for the distances {dj0 }.
If the constraint represented by the edge (i, j) changes the distance d0j a first time, this means every new shortest paths built by the Propagation algorithm will contain the node j.
If the distances is changed a second time, then the algorithm has built a closed dependency path and for the Theorem 3 the graph Gd is inconsistent.
2  Proceedings of TIME-96 Figure 2 shows the modified version of the algorithm to check for cycle detection.
It is interesting to notice the complexity of the algorithm with cycles detection is the same of the Propagation algorithm.
In fact, the only difference with the previous algorithm is the check of the boolean marks N EW ((i, j)) LBP ((i, j)) and U BP ((i, j)).
Propagation-cd (Gd , Cij ) 1.
- 9. as in the Propagation algorithm 10a.
if d0v + dv0 < 0 10b.
then exit(fail) 10c.
else if N EW ((u, v)) 10d.
then if U BP ((u, v)) 10e.
then exit(fail) 10f.
else U BP ((u, v)) ::= T 11.
- 19. as in the Propagation algorithm 20a.
if d0v + dv0 < 0 20b.
then exit(fail) 20c.
else if N EW ((u, v)) 20d.
then if LBP ((u, v)) 20e.
then exit(fail) 20f.
else LBP ((u, v)) ::= T 24.
- 28. as in the Propagation algorithm  Figure 2: Differences introduced by cycle detection the average time  6  Retraction of Temporal Constraints from a Consistent Context  This paragraph deals with the problem of removing temporal constraints from a consistent graph Gd (a graph without negative cycles).
A basic way to do this consists of: physically removing the constraint from the graph Gd ; setting every distance {d0i } and {di0 } to the value +[?
]; finally, running the Propagation algorithm on the whole graph.
As a matter of fact, this method is not very efficient.
In fact, when retracting a constraint from the time map a lot of distances are likely not to be affected by the removal.
The dependency information may be used to focalize the part of the network actually affected by the removal and to run the Propagation algorithm on that part of the graph.
To state same properties some definitions are useful.
Given an upper bounds' dependency tree DTub (VDTub , EDTub ), each sub-tree STub [i](VSTub , ESTub ) of root i [?]
VDTub is called an Upper Bounds' Dependency Sub-tree.
Given a lower bounds' dependency tree DTlb (VDTlb , EDTlb )  4 every sub-tree STlb [i](VSTlb , ESTlb ) of root i [?]
VDTlb is called a Lower Bounds' Dependency Sub-tree.
Given a a distance graph Gd (VGd , EGd ) and a node i [?]
VGd , IN (i) is the set of start nodes of the edges which enter in the node i (in the edge (j, i), j is the start node and i is the end node).
The next Proposition explains the real effects of a removal constraints from a graph Gd and it is a starting point to write a new algorithm to remove temporal constraints from Gd .
Proposition 1 Let Gd be a consistent graph and DTub (VDTub , EDTub ) its upper bounds' dependency tree (DTlb (VDTlb , EDTlb ) its lower bounds' dependency tree).
The retraction of an edge (i, j) [?]
EDTub ( (i, j) [?]
EDTlb ) modifies at most the distances of the nodes k [?]
VSTub [j] ( k [?]
VSTlb [j] ).
No distances are modified when (i, j) 6[?]
EDTub ( (i, j) 6[?]
EDTlb ).
Proof.
We give the proof for the distances {d0i }, but an analogous proof can be given for the distances {di0 }.
The removal of an edge (i, j) [?]
EDTub can't modify a node's distance {d0k } in the case k 6[?]
VSTub [k].
In fact the removal of (i, j) does not change the shortest path between the origin 0 and the node k. If (i, j) 6[?]
EDTub then no distance is changed because no shortest path is changed.
2 The basic idea to write an efficient removal algorithm is run the Propagation algorithm on the only part of the Gd graph affected by the removal of the constraint.
The next Theorem formalize this concept and explains how to initialize the Propagation algorithm.
Theorem 5 Let Gd be a consistent distance graph.
To remove the effects of the constraint represented by the edge (i, j) [?]
EDTub ( (i, j) [?]
EDTlb ) the queue Q of the Propagation algorithm and the set of distances {d0i } ({di0 }) in the graph Gd need of the following initialize operations.
S S 1.
Q - k[?
]VST [j] IN (k) (Q - k[?
]VST [j] IN (k)) ub lb 2. d0u ::= +[?
], u [?]
VSTub [j] (du0 ::= +[?
], u [?]
VSTlb [j]) Proof.
We give the proof for the distances {d0i }, but an analogous proof can be given for the distances {di0 }.
By Proposition 1, for every node k [?]
VSTub [j], the distance {d0k } can change after the removal.
The Propagation algorithm have to rebuild the new shortest paths for every node k [?]
VSTub [j].
In order to update these distances to the new values, it is necessary to initialize them to the maximum possible value +[?].
In fact, it is not known what the new values will be and the Propagation algorithm can only reduce the bounds.
In addition, we have to put in the queue Q all  Proceedings of TIME-96 the nodes of the constraints (i, j) which enter in the set S of updated nodes.
That is, the nodes in the set k[?
]VSTub [i] IN (k).
In fact, these are the only nodes of the graph from which can start the new shortest paths of the nodes k [?]
VSTub [j].
2 The Remove algorithm is shown in Figure 3.
It accepts as an input a graph Gd and a constraint Cij which have to be removed from Gd and return the graph Gd updated.
At the step 13 is used the RePropagation algorithm that is similar to the Propagation algorithm but accepts as an input a list of nodes Q instead of an edge Cij .
The parameter Q is used as an initialization for the internal queue.
Moreover RePropagation does not check for the consistency of a modification because the removal of one or more constraints, relax the STP holding the consistency property.
Remove (Gd , Cij ) 1. begin 2.
Vm - [?]
3.
Q-[?]
4. if (i, j) [?]
EDTub 5. then Vm - Vm [?]
VSTub [j] 6. else if (j, i) [?]
EDTub 7. then Vm - Vm [?]
VSTub [i] 8. if (i, j) [?]
EDTlb 9. then Vm - Vm [?]
VSTlb [i] 7. else if (j, i) [?]
EDTlb 8. then Vm - Vm [?]
VSTlb [j] 9.
Foreach u [?]
Vm do begin 10.
Q - Q [?]
IN (u) 11. end 12.
EGd - EGd - {(i, j), (j, i)} 13.
RePropagation(Gd , Q) 14. end Figure 3: Remove algorithm  7  Performance Evaluation  In order to get some realistic evaluations of the algorithms, we have used a scheduling system described in [3] and the time network generated by the scheduler.
This scheduler solves instances of the Deadline Job Shop Scheduling Problem (DJSSP) by incremental precedence constraint posting between the activities until any conflict in the use of resources is resolved.
In the DJSSP, each activity in a job can request only one resource and a resource is requested only once in a job.
The sequence of resources requested by the activities in a job is random.
Every job has a  5 fixed release date and a due date.
More details on the random problem generator are described in [3].
All the evaluations are given as number of time points explored by the algorithms.
This choice is motivated from the fact that such number is both proportional to the time of computation and machine independent.
We have built two different types of time networks from the resolution of two different DJSSPs: the 8x8x8 (named P 8) and the 10x10x10 (named P 10), where the first number indicate the number of jobs, the second one the number of activities in a job and the third one the number of resources.
The data are obtained running ten instances of each type of problem.
Table 1 shows the number of time points N , the maximum number of distance constraints Emax and maximum connectivity Cmax for each problem.
The connectivity is defined as the ratio between the number of distance constraints E and the number of time points N .
The value N is two times the number of activities plus two (the origin point and horizon point).
The value Emax represents the maximum number of distance constraints which can be contained in a time network associated to the solution of the instance of the DJSSP.
Emax is obtained by the sum of the maximum values of the number of precedence constraints for each resource and the number of constraints before the scheduling algorithm starts to find a solution.
Table 2 and Table 3 present the perfor-  Table 1: Number of time points and maximum connectivity for the experimental time networks Problem P8 P 10  N 130 202  Emax 333 661  Cmax = Emax /N 2.56 3.27  mance of the Propagation algorithm when a modification is either consistent or inconsistent respectively.
This values are shown as a function of the average connectivity Av-conn, that is, every row of the table represents the average value obtained in the interval Av-conn +-0.25.
In order to get several values of the connectivity we have built a solution of an instance of a DJSSP and progressively reduced the number of edges and selected a time constraint Cij in random way.
In order to get the results showed in Table 2, we have modified the distance constraint selected Cij = aij <= Xj - Xi <= bij , in the constraint  Proceedings of TIME-96  6  Cij = aij + (dij - aij )U [0.05, 01] <= Xj - Xi <= bij .
Where U [x, y] represents a random value r with uniform distribution such that x <= r <= y and dij is minimal temporal distance between the nodes i and j on the Gd graph.
In this case, it is possible to make a comparison between the number of nodes scanned by the Propagation algorithm (Loc-prop values) and the number of nodes scanned by an algorithm which works from scratch (Scratch values).
In order to get the results showed in Table 3, we have induced an inconsistent situation by modifying the constraint Cij in the constraint dij (1 + U [0.05, 01]) <= Xj - Xi <= bij In this other case, it is possible make a comparison between the number of nodes visited by the Propagation algorithm which uses the property expresses by Theorem 4 (Cycle-det values) and the number without the previous property (No-cycle-det values).
Table 2: Incremental vs scratch propagation Problem P8  P 10  Av-conn 1.25 1.75 2.25 2.75 1.25 1.75 2.25 2.75 3.25  Loc-prop 38.76 52.33 34.08 39.51 51.42 67.20 64.34 57.00 63.92  Scratch 652.67 1111.47 1641.19 2048.28 1108.38 1928.54 2876.22 3817.79 4388.71  a solution; then we have reduced progressively the number of time constraints by using the Remove algorithm.
In this case, is possible to make a comparison between the average number of nodes scanned by the Remove algorithm (Loc-rem values) and the number of nodes scanned in the same case by a scratch algorithm (Scratch-rem values).
The scratch algorithm eliminates first the constraint from the time map; then puts all the bounds of the time points to the value +[?
]; finally updates all the network.
Table 4: Incremental vs scratch remove Problem P8  P 10  Av-conn 1.25 1.75 2.25 2.75 1.25 1.75 2.25 2.75 3.25  Loc-rem 2.37 35.34 56.02 96.35 2.69 33.12 55.06 70.58 156.97  Scratch-rem 652.67 1111.47 1641.19 2048.28 1108.38 1928.54 2876.22 3817.79 4388.71  Acknowledgments This research is partially supported by: ASI - Italian Space Agency, CNR Special Project on Planning, CNR Committee 04 on Biology and Medicine.
References [1] Cervoni, R., Cesta, A., Oddi, A., Managing Dynamic Temporal Constraint Networks, Proceedings of the Second International Conference on AI Planning Systems (AIPS94), AAAI Press, 1994.
Table 3: Propagation with and without cycle detection Problem P8  P 10  Av-conn 1.25 1.75 2.25 2.75 1.25 1.75 2.25 2.75 3.25  Cycle-det 3.02 2.92 2.47 1.92 3.21 2.78 2.68 2.55 2.63  No-cycle-det 114.42 77.30 43.87 9.75 199.45 133.81 86.85 27.15 14.58  Finally, Table 4 presents the performance of the Remove algorithm.
These results are obtained in the same way as the previous ones.
First we have built  [2] Cesta, A., Oddi, A., DDL.1: A Formal Description of a Constraint Representation Language for Physical Domains, Proceedings of the 3rd European Workshop on Planning (EWSP95), IOS Press, 1996.
[3] Cheng, C. Smith, S.,F., Generating Feasible Schedules under Complex Metric Constraints, Proceedings of the 12th National Conference on Artificial Intelligence (AAAI-94), AAAI Press, 1994.
[4] Cormen, T.H., Leierson, C.E., Rivest, R.L., Introduction to Algorithms, MIT Press, 1990.
[5] Currie, K., Tate, A., O-Plan: the open planning architecture, Artificial Intelligence, 52, 1991, 49-86.
[6] Davis, E., Constraint Propagation with Interval Labels, Artificial Intelligence, 32, 1987, 281-331.
[7] Dechter, R., Meiri, I., Pearl, J., Temporal constraint networks.
Artificial Intelligence, 49, 1991, 61-95.
[8] Ghallab, M, Laruelle, H., Representation on Control in IxTeT, a Temporal Planner, Proceedings of the Second International Conference on AI Planning Systems (AIPS94), AAAI Press, 1994.
Representing temporal interval relationships in a rst order logic for time Andre Trudel Jodrey School of Computer Science Acadia University Wolfville, NS, B0P 1X0 Canada andre.trudel@acadiau.ca January 16, 2001  Abstract  We present a simple classication of temporal information based on truth value at the point level.
Axioms are then derived for capturing temporal relationships and, strong and weak negation.
The main advantage of our logic independent approach is that it becomes simpler for a user to dene a rst order temporal logic.
Track 1: Temporal representation and reasoning in AI Topic: Temporal logics and ontologies 1 Introduction In addition to representing truth at a point and interval, a rst order logic for reasoning about time must also represent the relationships between an interval and its internal points and sub-intervals.
For example, \the house was red during 1999" is a statement about the time interval 1999.
It is also obviously true that the house was red during each season, month, week, day, etc.
in 1999.
There are two standard approaches in AI for representing the relationships between temporal intervals and their internal points and sub-intervals.
The rst is to have pre-specied axioms in the logic.
For example, Allen  1] denes three relationships: properties, events, and processes.
An assertion satises the property relationship, and is called a property, if is true over an interval I if and only if is true over every subinterval of I.
Thus, `block A is on the table' is a property.
Events and processes are similarly dened.
An assertion is an event if whenever is true over an interval I then is false over every subinterval of I.
For example, `ran a mile without stopping' is an event.
An assertion is a process if is true over an interval I if and only if is true over some subinterval of I.
`I am walking' is an example of a process.
A general problem with the approach of axiomatizing a small number of relationships is how do we choose which relationships 1  to axiomatize?
Since there are a large number of relationships, how do we guarantee that all the important ones have been chosen?
For example, Shoham  3] states that the assertion `I ran more than two miles' does not t into Allen's categorization scheme.
The other extreme to axiomatizing a small number of relationships in the logic is to not axiomatize any at all.
For example, in  3] Shoham allows the user to capture relationships with formulas.
For each item of temporal information, the user must specify the relationships it satises between an interval and its sub-intervals and internal points.
Because of the large number of possible relationships, this is a tedious, time consuming, and error prone task for the user.
We take the middle ground.
We categorize temporal information based on the truth value at the point level.
This categorization permits us to write general axioms.
We then allow the user to ne tune the relationships with formulas.
Our axiomatization also captures strong and weak negation which is one particular type of temporal relationship.
In the next section, we present our point based classication.
We then describe the major temporal relationships between an interval and its internal points and sub-intervals.
An axiomatization which uses our classication is given for the temporal relationships.
We also use our classication to axiomatize strong and weak negation.
2 Point based classication of temporal information At the point level, there are only three possibilities for an item of temporal information x: 1. x is always true at every point (written AT (x)).
For example, \ran less than 2 miles" is always true at every point (i.e., AT(ran-less-than-2-miles)).
2. x is always false at every point (written AF (x)).
For example, \ran a mile" is AF (i.e., AF(ran-a-mile)).
3. x can be either true or false at a point (written TF (x)).
For example, \alive" can be true at one point and false at a later point in time (i.e., TF(alive)).
Please note the following about the AT{AF{TF classication: Every item of temporal information falls into exactly one of the categories: 8x:  AT (x)  AF (x)  TF (x):  For example, \ran a mile" which is AF is never AT or TF .
The classication is temporally independent.
AT{AF{TF have no temporal arguments.
For example, we write TF(alive) regardless of the truth value of \alive" at particular points or intervals.
Even though an interpretation may assign true to \alive" at every point in an interval, \alive" is still TF.
It is important to note that even though tautologies (e.g., \it is either raining or not") are of type AT , we are excluding them from our categorization.
Tautologies are trivial to represent and not very interesting.
Similarly, we exclude statements that are always false.
2  3 Subinterval relationships We use the AT{AF{TF classication from the previous section to represent temporal relationships.
For the sake of uniformity, we adopt Shoham's  3] notation and terminology for specifying the relationships between an interval and its internal points and sub-intervals.
He denes four basic relationships: Denition 1 An item of temporal information x is point downward hereditary (written # x) if, whenever it holds over an interval, it holds at all of its internal points: 8t1  t2 t3:  t1 < t2 < t3 ^ true(t1  t3  x)] !
true(t2  x): p  x) if, whenever it holds at all internal points of some nonpoint interval, it holds also over the nonpoint interval itself: Denition 2 An item of temporal information x is point upward hereditary (written 8t1  t3:   (8t2 : t1 < t2 < t3 !
true(t2  x))  ^  t1 < t3]  "p  true(t1 t3 x):  !
Denition 3 An item of temporal information x is interval downward hereditary (written #i x) if,  whenever it holds over an interval, it holds over all of its nonpoint subintervals: 8t1  t2 t3 t4 :   t1  t2 < t3  t4  ^  true(t1 t4 x)]  !
true(t2  t3 x):  x) if, whenever it holds over all nonpoint subintervals of some nonpoint interval, it holds also over the nonpoint interval itself: Denition 4 An item of temporal information x is interval upward hereditary (written 8t1  t4   8t2  t3: (t1  t2 < t3  t4 !
true(t1  t4 x):  ^  (t1 6= t2 _ t3 6= t4 ))  !
"i  true(t2 t3 x)]  In the above denitions, true(t1  t2  x) species that x is true over the interval (t1  t2) and, true(t x) and true(t t x) both specify that x is true at point t. Assume that the variable t with or without a subscript represents a point from a dense and totally ordered set (e.g., the reals).
Shoham  3] describes other relationships based on denitions 1{4: Relationship Written as Denition downward-hereditary #x # x $  # x ^ # x] upward-hereditary "x " x $  " x ^ " x] liquid lx l x $  " x ^ # x] point-point-liquid l x l x $  " x ^ # x] interval-interval-liquid l x l x $  " x ^ # x] point-interval-liquid l x l x $  " x ^ # x] interval-point-liquid l x l x $  " x ^ # x] p p  p p i i i p p  p p i i i p p  i  i  3  i i  p  i  i  p  p  i  p  i  3.1 Axiomatization  Assume x is always true at every point (i.e., AT (x)).
Regardless of x's truth value over an interval, it will be true at the point level (i.e., # x).
It is not the case that " x.
For example, if \ran 5 miles" is true over (t1  t2 ), then \ran less than 2 miles" is true at every point between t1 and t2 , and false over (t1  t2).
We have the following axiom for AT : AT (x) !
: " x ^ # x: (1) Note that tautologies, which have been disallowed, make the above axiom false.
Tautologies are " .
Assume x is always false at every point (i.e., AF (x)).
Since x is never true at the point level, we automatically have " x and cannot have # x: AF (x) ! "
x ^ : # x: (2) Note that the above axiom does not hold for information that is universally false (which has been disallowed).
This type of information is # .
Temporal information of type TF can vary its truth value at every point.
It will be true at the interval level if and only if it is true at every interior point of the interval: TF (x) ! "
x ^ # x: (3) As pointed out by Shoham  3, p. 49] it is easy to verify the validity of the following from the denitions:  " x ^ # x] !  "
x ^ # x]: (4) Combining formulas (3) and (4) we have: TF (x) !  "
x ^ # x ^ " x ^ # x]: (5) p  p  p  p  p  p  p  p  p  p  p  p  p  i  p  p  i  i  p  i  3.2 All possible relationships  The 16 possible combinations of " , # , " , and # are shown in table 1, one per line.
For each possible combination of " , # , " , and # , we specify whether information satisfying these relationships is of type \AT", \AF", or \TF" in the last 3 columns of the table.
For example, in row 6 if x is " x, : # x, " x, : # x, then :AT (X ), AF (x), and :TF (x) (i.e., x is AF ).
In the remainder of this subsection we justify the entries in the last three columns.
For the column labelled \AT" in table 1, we inserted an \N" whenever axiom (1) is violated.
The only possibilities for \AT" are in rows 9 through 12.
These entries are indeed possible: Row 9: Let x be \ran 2 miles or less".
x is always true at the point level (i.e., AT (x)).
From axiom (1) we have : " x and # x.
If x holds over every subinterval then it holds over the interval itself (i.e., " x).
Similary, if x is true over an interval, it is also true over every subinterval (i.e., # x).
p  p  p  i  i  p  i  i  p  p  i  i  p  p  i  i  4  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  "p  #p  Y Y Y Y Y Y Y Y N N N N N N N N  Y Y Y Y N N N N Y Y Y Y N N N N  "i  Y Y N N Y Y N N Y Y N N Y Y N N  #i  Y N Y N Y N Y N Y N Y N Y N Y N  AT AF TF N N Y N N N N N N N N N N Y N N Y N N Y N N Y N Y N N Y N N Y N N Y N N N N N N N N N N N N N N  Table 1: Truth table Row 10: Let x be \travelled an even number of miles".
Since displacement is always zero at the  point level, x is always true at the point level (i.e., AT (x)).
From axiom (1) we have : " x and # x.
The only case where we have \x" true over every subinterval of an interval is when the number of miles travelled is zero.
In this case, " x.
Now assume the person travelled continuously for 10 miles during interval I .
x is true over I .
There is a subinterval of I where travel equals 5 miles and x is false over this subinterval.
Therefore, : # x.
Row 11: Let x be \program did not start, run and then terminate".
x is always true at the point level (i.e., AT (x)).
From axiom (1) we have : " x and # x.
Assume the program started at t1 , ran over (t1  t2) and terminated at t2 .
x is true over every proper subinterval of (t1 t2 ) and false1 over (t1  t2) (i.e., : " x).
If x is true over an interval, it will be true over every subinterval (i.e., # x).
Row 12: Let x be \program did not start at the start of the interval and did not terminate at the end of the interval".
x is always true at the point level (i.e., AT (x)).
From axiom (1) we have : " x and # x.
Assume the program started at t1 , ran over (t1 t2 ) and terminated at t2.
x is true over every subinterval of (t1 t2) and false over the interval (i.e., : " x).
Further assume x is true over (t0  t3 ) where t0 < t1 < t2 < t3 .
x is true over an interval and false over one of its subintervals (i.e., : # x).
For the column labelled \AF", we inserted an \N" whenever axiom (2) is violated.
The only p  p  i  i  p  p  i  i  p  p  i  i  1  x  is also false over every interval containing (t1  t2 ).
5  possibilities for \AF" are in rows 5 through 8.
These entries are indeed possible: Row 5: Let x be \the robot travelled a non-zero distance without stopping".
Since displacement is always zero at the point level, x is always false at the point level (i.e., AF (x)).
From axiom (2) we have " x and : # x.
It is also the case that x is true over an interval if and only if it is true over every subinterval (i.e., " x and # x).
Row 6: Let x be \ran a mile".
As in the previous row, displacement is zero at the point level and we have " x and : # x.
It is impossible for x to be true over every subinterval of an interval.
We therefore have " x.
Also, if x is true over an interval, there always exists at least one subinterval over which it is false (i.e., : # x).
Row 7: Let x be \ran continuously for a non-zero distance which is less than 5 miles".
For the same reasons as in rows 5 and 6, we have " x and : # x.
Assume the person ran exactly 5 miles over the interval (0,5) without stopping.
x is true over every subinterval of (0,5) and false over (0,5) (i.e., : " x).
It is the case that if x is true over an interval, then it is true over each subinterval (i.e., # x).
Row 8: Let x be \ran for a non-zero amount of time which does not equal 1 hour".
As before, we have " x and : # x.
Assume the person did not stop running over an interval which is one hour long.
x is true over each subinterval and false over the one hour interval (i.e., : " x).
Now assume the person did not stop running over an interval which is two hours long.
x is true over the interval, and false over all one hour subintervals (i.e., : # x).
For the column labelled \TF", we inserted an \N" whenever axiom (5) is violated.
The only possibilities for \TF" is row 1 (e.g., alive).
Based on the truth table shown in table 1, we can strenghten axioms (1, 2, 5): AT (x) $ : " x ^ # x: (6) p  p  i  p  i  p  i  i  p  p  i  i  p  p  i  i  p  AF (x)  $ "p  TF (x)  $  x  p  ^ : #p   " x ^ p  #p  x:  x^  (7) "i  x^  #i  x]:  (8)  Axioms (6,7,8) can be further reduced to: AT (x) $ : " x:  (9)  p  AF (x)  $ : #p  TF (x)  $ "p  x:  x^  (10) #p  x:  (11)  It is interesting to note that there are no \N" entries in the last three columns of rows 13{16 in table 1.
There exists no temporal information that is : " and : # .
All temporal information satises either or both of " and # .
Furthermore, both " and # are necessary for distinguishing between the types of temporal information (i.e., points are needed).
This may explain why Allen  1] ran into representational problems in his non-point interval logic.
p  p  p  p  6  p  p  Shoham  3, p. 49] observes that point-point-liquid information coincides with Allen's  1] properties.
Since the RHS of axiom (11) is the denition for point-point-liquid, Allen's properties also coincide with TF information.
Shoham also goes on to mention that liquid information coincides with philosopher's homogeneous propositions.
Since the RHS of axiom (8) is the denition liquid information, homogeneous propositions also coincide with TF information.
We therefore have the new result that Allen's properties, homogeneous information, and TF information are all dierent names for the same thing.
4 Negation Another family of relationships deals with strong and weak negation which is dened as: The strong negation of a formula would be true with respect to an interval if the formula itself were false through all subintervals of the interval, while the weak negation would be true merely if it was not the case that the formula negated was true with respect to the interval in question (even if it was true for some subintervals).
2, p. 172] \Ran a mile" is an example of an assertion which requires weak negation.
\Not ran a mile" is true over an interval if and only if \ran a mile" is false over the interval.
An example of an assertion which needs strong negation is \alive".
\Not alive" is true over an interval if and only if \not alive" is true over every subinterval of the interval.
4.1 Axiomatization  Dene the relation Weak(x) to be true if the item of temporal information x satises weak negation.
Similarly for Strong (x).
Note that every item of temporal information requires either weak or strong negation, but not both.
We use a notation similar to Shoham's  3, p. 50] for negation.
If x is false over (t1  t2) we write true(t1  t2 NOT (x)): Note that true(t1 t2 NOT (x)) and :true(t1  t2 x) are dierent.
The latter says that it is not the case that x is true over (t1  t2).
It may not be the case that x is false over (t1  t2 ).
For example, if alive is true over (0,1) and false (i.e., dead) over (1,2) then :true(0 2 alive) and :true(0 2 NOT (alive)).
x satises weak negation over an interval if and only if it is not true over the interval: Weak(x) !
8t1 t2: true(t1 t2 NOT (x)) $ :true(t1  t2 x)]: (12) For strong negation, we adopt Shoham's  3, p. 50] denition.
x is false over an interval if it does not hold over the interval and also does not hold over every subinterval (including internal points), except perhaps at the endpoints: Strong(x) !
8t1 t2: true(t1 t2 NOT (x)) $   t1 = t2 !
:true(t1  x)] ^  8t3  t4 : (t1  t3  t4  t2 ^ :(t1 = t4 _ t3 = t2 )) !
:true(t3  t4 x)]]]: (13)  7  Regardless of the type of negation, one desirable property is that double negation cancels2 itself out.
For strong negation, Shoham  3, p. 50, Corollary 2.4] derives the following result: (" x^ # x) $  8t1  t2 : true(t1  t2 x) $ true(t1 t2 NOT (NOT (x)))]: (14) Combining formulas (11) and (14) we get: TF (x) $ (8t1 t2: true(t1 t2 x) $ true(t1  t2 NOT (NOT (x)))): (15) The above states that strong negation cancels out if and only if we have information of type TF .
Therefore, only information of type TF should be used with strong negation: TF (x) $ Strong(x): (16) It then follows that AT and AF information require weak negation: (AT (x) _ AF (x)) $ Weak(x): (17) It is straightforward to prove that TF information is false over an interval if and only if it is false at every interior point.
From axiom (16), we conclude that strong negation holds over an interval if and only if it is false at every interior point.
We use this to simplify formula (13): Strong(x) !
8t1 t2: true(t1 t2 NOT (x)) $   t1 = t2 !
:true(t1  x)] ^ (18)  8t3 : t1 < t3 < t2 !
:true(t3  x)]]]: p  p  5 Conclusion Traditionally, temporal logics have either captured predetermined relationships in the logic, or left it up to the user to axiomatize the relationships.
In this paper, we took the middle ground.
Temporal information was classied based on its truth value at the point level.
The classication is simple and must be done by the user for each item of temporal information.
Once classied, axioms were derived (summarized in gure 1) to capture some of the temporal relationships.
The user can then write additional specic axioms as needed.
Our axiomatization also deals with strong and weak negation.
The onus is no longer on the user to decide for each item of temporal information whether or not it satises strong or weak negation.
Furthermore, our axiom for strong negation is computationally more ecient since we only need to verify falsity at the point level.
Although we used Shoham's notation in the paper, the results presented are logic independent.
Our classication and axioms can be adapted to most rst order logics for reasoning about time.
6 Acknowledgements Research supported by an NSERC grant.
2  It is simple to prove using axiom (12) that double weak negation cancels out.
8  AT (x)  $ : "p  x:  AF (x)  $ : #p  x:  TF (x)  $ "p  TF (x)  $  x^  Weak(x)  x:  Strong(x):  (AT (x) _ AF (x))  Strong(x)  #p  !
!
$  Weak(x):   8t1  t2 : true(t1  t2 NOT (x)) $   t1 = t2 !
:true(t1  x)] ^  8t3 : t1 < t3 < t2 !
:true(t3  x)]]]:  8t1  t2: true(t1  t2 NOT (x)) $ :true(t1  t2 x)]: Figure 1: Axiom summary  References  1] J.F.
Allen, Towards a General Theory of Action and Time, Articial Intelligence 23 (2), (1984), p. 123{154.
2] I.L.
Humberstone, Interval Semantics for Tense Logic: Some Remarks, Journal of Philosophical Logic 8, (1979), p. 171{196.
3] Y. Shoham, Reasoning about Change.
The MIT Press, Massachusetts, (1988).
9
Specification Patterns for Time-Related Properties Volker Gruhn, Ralf Laue University of Leipzig, Germany, Chair of Applied Telematics / e-Business* {gruhn,laue}@ebus.informatik.uni-leipzig.de Abstract We present a pattern system for property specification.
It extends the existing patterns identified in [4] which allow to reason about occurrence and order of events, but not about time conditions.
Introducing time-related patterns allows the specification of real-time requirements.
The paper is limited to 3 pages.
Therefore it contains only basic ideas.
The details can be found in [9].
Keywords: patterns, formal specification, timed model checking, verification  1 Introduction Often the persons who have to specify time-related requirements (for example business analysts who have to specify deadlines and other time-related conditions in business process models) are not familiar with existing formalisms and regard it as too difficult to use timed temporal logics.
On the other hand, model checking tools that can be used to verify the correctness of a system often require specifications given as temporal logics formulas.
Property specification patterns were successfully used to bridge this gap between practitioners and model checking tools.
However, the existing pattern system does not yet consider information about time.
We present a catalog of patterns for time-related requirements.
The pattern system does, however, not include timed properties like "Event A must be followed by event B within k time units".
The pattern system is explained more deeply in [4] and on patterns.projects.cis.ksu.edu.
A property specification consists of a pattern (which describes what must occur) and a scope, which describes when the pattern must hold.
The patterns are as follows (In an event-based formalism, capital letters stand for events): Absence: P never occurs.
Universality: P occurs throughout a scope.
Existence: P must occur sometime.
Bounded Existence: P must occur at least / exactly or at most k times.
Precedence: P must always be preceded by Q.
Response: P must always be followed by Q.
Chain Precedence / Chain Response: A sequence P1 , .
.
.
Pn must always be preceded / followed by a sequence Q1 , .
.
.
, Qm .
Scopes define, when the above patterns must hold: global: The pattern must hold during the complete system execution.
before: The pattern must hold up to an event X. after: The pattern must hold after the occurrence of an event X. between: The pattern must hold from the occurrence of X to the occurrence of Y. until: The same as "between", but the pattern must hold even if Y never occurs.
2 Untimed Specification Patterns 2.1 Adding Time Information Dwyer and his colleagues collected 555 specifications and found that 92% of them matched one of the patterns from their pattern system for property specification[4].
This pattern system enables people who are not experts in temporal logic to read and write formal specifications in a variety of formalisms.
With the help of this system, properties like "Event A must be followed by event B" can be expressed.
* The Chair of Applied Telematics / e-Business is endowed by Deutsche Telekom AG  We express properties using an event-based formalism.
This allows us to write something like "the point of time, when event P occurs", abbreviated by t(P).
We use terms like t(P ) +- k for "k time units after/before the occurrence of P".
We have added information about time to the following elements of the specification patterns: 1.
The events (see section 5): Instead of just specify-  ing that "X occurs", we consider events like "X occurs twice in n time units".
2.
The properties (see section 4): We want to be able to specify properties like time-bounded Response ("P must always be followed by Q within k time units").
3.
The scopes (see section 6): We want to delimit the period of validity for a pattern by scopes like "after t(P)+k".
3 Timed Observer Automata We use the concept of timed observer automata (observers)[2] to describe the desired system behavior.
Intuitively, observer automata run in parallel with the model under verification.
They reach a certain state if and only if some property can be violated in the model.
Synchronization labels are used for synchronizing the model under verification with observer automata: If "something interesting" occurs in the model, the observer automata can react immediately.
The majority of our patterns deal with safety properties.
In order to prove that such a property is true, it is sufficient to check that the observer cannot reach some location(s), which we call error locations.
For liveness properties (like "every occurrence of P is followed by an occurrence of Q"), reasoning about infinite runs is necessary.
As usual, we use acceptance conditions for this purpose: Some locations in the observer TA are marked as accepting locations.
A counterexample is detected, if there is a non-Zeno run entering an accepting location infinitely often.
4 The Patterns In [9], we have published the time-related patterns with their observers.
The timed patterns include: * time-bounded existence (something must occur within k time units) * time-bounded response (P must be followed by Q, and t(Q)  t(P ) + k or t(Q)  t(T ) +- k(where T is some external event and [?]
{<=, >=}).
For t(Q) >= t(P ) + k, we distinguish two subcases: Either events Q that occur "too early" (i.e.
before t(P)+k) are ignored or such events are regarded as a violation of the specification.
* precedence pattern "Q enables P after a delay" * precedence pattern "Q enables P for k time units"  5 (Combined) Events Using synchronization labels, a TA can "observe" the occurrence of an event in another TA and react in some way if the event occurs and synchronization can take place.
However, often we want to react to "observations" which are related to more than one event or to the time when it occurs.
To handle such "combined events" like "P occurs n times" or "both P and Q occur within a time span of no more than k time units" , we construct a reporting TA which can take certain transitions if and only if the combined event happens.
These transitions can be labeled with a new synchronization label M!.
In this way the reporting TA can signalize the combined event in the same way as "simple" events are signalized.
[9] lists reporting TA for these combined events: chains (sequences) of events time-bounded chains (sequences) of events an event occurs n times an event occurs n times within k time units collections of events (for example events A, B and C occur, regardless of the order between these events) time-bounded collections of events non-occurrence of some event in a given time span  6 Scopes Let A be the observer TA for some property.
It can observe whether the property holds globally, i.e.
during the entire execution of the model.
A can be modified in order to check the property over a given scope.
With the modified TA A', we can check the validity of a property before, after and until t(D), and we can also deal with scopes before, after and until t(D) +- k, where k is an integer.
This allows us to write specifications like "Something must happen within at least 10 time units after the system has been started".
7 Implementation The general procedure for implementing observers to check a property in a model-checking tool is as follows: The reporting TA and the observers must be scheduled together with the model under verification in a round-robin manner such that each step of the model is followed by a step of each reporting TA and a step of each observer without letting time pass.
Hereby, it is necessary to pay attention to the hierarchy of synchronization labels: The TA whose run depends on events reported by other TA must be scheduled after the TA that can report something to them.
Usually, this means that after each step of the model under verification, each reporting TA has the chance to evolve and finally the observers are scheduled.
8 Related Work The original pattern catalog was introduced in [4], a survey of property specifications was published in [5].
[6] specifies time conditions using sentences in structured English, which is a pattern-like system.
Later, the authors found that this approach was not suitable for their research project, so they started to use RT-OCL as an alternative approach[7].
The use of TA for specifying temporal properties is quite common[11][3].
[3] states that "automata based notations turned out to be simpler than most logics for describing sequences of events".
[8], [7], [10] and others1 use timed UML models to specify a system and its properties.
UML sequence diagrams, OCL constraints or UML state machines (acting as observers) serve as property specification language.
In the papers mentioned so far, the observer TA have to be constructed by hand, even if the translation of the model itself into the input language of a model checker can be done automatically.
[1] introduces a visual language to specify real-time requirements and a tool that translates these requirements into the input language of the model checker Kronos.
The user has still to learn a new notation, but the visual language is much easier to understand than other formalisms.
However, we see a major drawback in the way how properties have to be specified: The user has to graphically describe the scenarios which violate the requirements.
9 Conclusions and Directions for Future Research We believe that the proposed pattern system helps to specify time-related properties for model checking.
Because the system is most useful if the observers are generated automatically in the input language of existing model checking tools, we will develop tool-support for this task.
We believe that the vast majority of real-world specifications are instances of patterns in our system.
We should, however, evaluate the completeness of our pattern system by surveying an appropriate number of real-world specifications.
If necessary, the pattern system will be updated as a consequence of this study.
References [1] A. Alfonso, V. A. Braberman, N. Kicillof, and A. Olivero.
Visual timed event scenarios.
In 26th International Conference on Software Engineering (ICSE 2004), pages 168-177.
IEEE Computer Society, 2004.
1 [8]  mentions a number of other papers in the bibliography.
[2] R. Alur and D. L. Dill.
Automata for modeling real-time systems.
In Proceedings of the 17th International Colloquium on Automata, Languages and Programming, pages 322-335.
Springer-Verlag, 1990.
[3] V. A. Braberman and M. Felder.
Verification of real-time designs: Combining scheduling theory with automatic formal verification.
In ESEC / SIGSOFT FSE, pages 494-510, 1999.
[4] M. B. Dwyer, G. S. Avrunin, and J. C. Corbett.
Property specification patterns for finite-state verification.
In FMSP '98: Proceedings of the second workshop on Formal methods in software practice, pages 7-15.
ACM Press, 1998.
[5] M. B. Dwyer, G. S. Avrunin, and J. C. Corbett.
Patterns in property specifications for finite-state verification.
In Proc.
of the 21st international conference on Software engineering, pages 411-420.
IEEE Computer Society Press, 1999.
[6] S. Flake, W. Muller, and J. Ruf.
Structured english for model checking specification.
In GI-Workshop Methoden und Beschreibungssprachen zur Modellierung und Verifikation von Schaltungen und Systemen in Frankfurt, Berlin, 2000.
VDE Verlag.
[7] S. Flake, U. Pape, J. Ruf, and W. Muller.
Specification and formal verification of temporal properties of production automation systems.
In Integration of Software Specification Techniques for Applications in Engineering, volume 3147 of LNCS.
Springer Verlag, 2004.
[8] S. Graf, I. Ober, and I. Ober.
Model checking of UML models via a mapping to communicating extended timed automata.
In S. Graf and L. Mounier, editors, Proceedings of SPIN'04 Workshop, Barcelona, Spain, volume 2989 of LNCS.
Springer, April 2004.
[9] V. Gruhn and R. Laue.
Patterns for timed property specification.
In 3rd Int.
Workshop on Quantitative Aspects of Programming Languages (QAPL 05), Edinburgh, Scotland, April 2005, to appear, 2005.
[10] A. Knapp, S. Merz, and C. Rauh.
Model checking - timed UML state machines and collaborations.
In Proceedings of the 7th International Symposium on Formal Techniques in Real-Time and Fault-Tolerant Systems, pages 395-416.
Springer-Verlag, 2002.
[11] I. Ober and A. Kerbrat.
Verification of quantitative temporal properties of SDL specifications.
In SDL '01: Proceedings of the 10th International SDL Forum Copenhagen on Meeting UML, pages 182-202.
Springer-Verlag, 2001.
Engineering Time in Medical Knowledge-Based Systems through Time-Axes and Time-Objects E.T.
Keravnou Department of Computer Science, University of Cyprus Kallipoleos 75, P.O.Box 537, CY-1678 Nicosia, Cyprus email: elpida@turing.cs.ucy .ac.cy  Abstract  'natural ' granularities are lunar months, calendar months, years, etc, respectively.
Starting from the premise that time representation and temporal reasoning must constitute integral aspects of a competent, knowledge-based, medical system, the paper presents the relevant requirements and discusses their realisation in terms of a generic temporal kernel to be embedded in such a system.
The kernel has a layered architecture where the bottom laver gives the ontological primitives and their associated axioms, and rhe higher layers implement the required temporal reasoning.
The principal primitives of the ontology are the time-axis and the time-object.
1.2 Model of occurrences An appropriate model for occurrences must support the following: a Absolute and relative occurrences.
"No ossification of knee epiphyses at birth" expresses an occurrence in absolute terms, ie with respect to some fixed point, while "narrow thorax until kyphoscoliosis" expresses an occurrence, "narrow thorax", relative to another occurrence, "kyphoscoliosis".
e Absolute and relative vagueness.
"Wide triradiate cartilage upto about the age of 11 years" exprcsses absolute vagueness while "nausea precedes or coincides with the headache" expresses relative vagueness.
a Absolute and relative duration.
"Two days of headache" expresses the duration of headache in absolute terms while "headache during the nausea" does so in a relative way.
a Incompleteness.
Patient information is often expressed in a temporally discrete and thus temporally incomplete fashion, eg the record of some patient could include statements like "mild scoliosis at the age of 2 years" and "severe scoliosis at the age of 7 years" without any mention of the status of scoliosis (absent or present and of what severity) at any other points in time.
a Point and interval occurrences.
The same occurrence can be expressed with respect to different temporal granularities, thus giving it point or interval status (in a conceptual rather than real-life sense) depending on the temporal context of reference.
For example "I had flu for most of January 1995" expresses an interval occurrence at the granularity of days but a point occurrence at the granularity of months.
With respect to interval occurrences the issue of convexity, or nonconvexity [ 131, arises (convexity implies that the  1 Time representation requirements for medical problem solving Time is intrinsically relevant to medical problem solving.
Time representation requirements for medical tasks such as diagnosis, prognosis, monitoring, therapy planning, ctc.
dcmand more variety in expression and higher levels of abstraction than appears to be supported by well known, general theories of time proposed in the AI literature [1],[3],[ 121.
There are two basic issues here: how to model time per se and how to model time-varying situations or occurrences.
1.1 Model of time Real time is infinite and dense.
An abstraction of reality which models time as a single time-line (either in dense or discrete terms), a model often adopted in temporal databases and other applications, does not provide the appropriate abstraction for medical applications where a richer model providing a multidimensional structure to time, through a number of interrelated, conceptual temporal contexts, and niultQle granularities, is often required.
Examples of conceptual temporal contexts are the various developmental periods, eg fetal-period, infancy, early-childhood, etc whose  160 0-8186-7528196$5.00 0 1996 IEEE  ~  unfolding of the Occurrence entails some sort of activity throughout the particular period of time that defines its lifetime).
Compound occurrences.
Disease processes and therapeutic interventions define compound occurrences.
Compound occurrences are categorised into: periodic occurrences (eg "headache every morning for about two hours over a period of one week which worsens each day", "administration of drug x every four hours until the pain stops but not for more than two days", etc); temporal trends that describe changes and their direction and rate of change, eg "low pressure increasing slowly", which in turn indicate whether a situation is normal or whether it is converging towards, or diverging away from, normality and at what rate; and general temporal patterns, eg a sequence of meeting trends, a set of relative occurrences, a set of causally related occurrences, etc.
The modelling of compound occurrences requires mechanisms for abstraction and refinement.
Cuusulity.
Changes are explained through causal relations and hence time is directly relevant to causality.
The temporal principle underlying causality is that an effect cannot precede its cause.
unintentionally cultivate the treatment of time as an accessory.
To achieve the above, an ontology must force time to be an integral aspect of the domain entities that constitute the processing elements of the problem solver.
This the proposed ontology aims to achieve through its time-object primitive, which in addition results in the simple integration and uniform representation of structural, causal, and temporal knowledge.
Furthermore, the ontology aims to provide the necessary temporal abstraction through multiple temporal granularities.
The principal ontological classes are the time-axis and the time-object that respectively provide a model of time and a model of occurrences.
Time-axes provide conceptual contexts for the definition of time-objects and hence they result in organising time-objects in meaningful (temporal) clusters.
A time-axis is expressed at the granularity relevant to its semantics and multiple granularities are therefore appropriately modelled by confining their usage to relevant, conceptual, temporal contexts.
The notion of an (anonymous) time-axis is not original per se.
However, the notion of multiple, conceptual (ie named), interrelated, time-axes is.
Multiplicity of time-axes is necessary for the natural representation of some problem.
For example the ossification process for the first cervical vertebra of the spine begins at the second lunar month and terminates at the 25th year of age.
The initiation of this process is expressed at the granularity of lunar months and its terrnination at the granularity of years.
The time-values concerned refer to conceptual periods of time, namely fetal-period and maturity respectively.
The lifetime of the particular ossification process extends over fetalperiiod, infancy, childhood, puberty and part of maturity.
This process is temporally decomposed into a number of subprocesses.
A process and its subprocesses are modelled as time-objects.
Time-objects are dynamic entities.
The existence of some time-object could be expressed with respect to multiple temporal contexts (ie time-axes) and hence granularities, either in absolute or relative terms.
In addition, the existence of a time-object, in some context, can be expressed with a degree of vagueness.
Three important types of relations are defined for time-objects: structural, causal, and temporal relations.
The ontological classes are: Tunits,the set of time-units.
Axes, the set of discrete time-axes.
Times(a), the sequence of literal time-vulues on time-axis a. Tobjects, the set of time-objects.
Tobjects(a) (cTobjects), the set of time-objects that have a valid existence on time-axis a. Pobjects(a) (G Tobjects(a)), the set of time-objects that have a point existence on time-axis a. Zobjects(a) (E Tobjects(a)), the set of time-objects that have an interval existence on time-axis a.
2 Ontological primitives The relative simplicity and crimness of the Drimitives and reasoning of most widely adopted general t'heories of time which in fact justify the wide interest in them, unfortunately do not render their expressive power sufficient enough for a number of real-life medical problems where happenings are not as simple and as orderly as someone walking on a street or getting a promotion.
Such theories do not provide an adequate level of abstraction for knowledge engineering purposes in that their primitives, often the time-point, the timeinterval, or the event, are too 'primitive' and not at a knowledge level from the perspective of modelling complicated, dynamic, domain concepts such as disease processes or therapeutic interventions.
Another indirect limitation is based on the fact that still in many knowledge-based problem solvers temporal reasoning is seen as an accessory rather than as an integral aspect of the problem solver's reasoning.
For example the temporal reasoner could be a background process demoted to the role of preprocessing data (from some temporal database) in order to select or derive the information to be fed to the problem solver; the problem solver, therefore, is not reasoning with time or change in any conscious way.
An ontology of time that in a sense 'forces' time to be treated as an integral aspect of the particular problem solving and hence for temporal reasoning to constitute a very conscious activity on the part of the problem solver has significant advantages over an ontology that does not and which may even  161  0  Props, the set ofproperties.
Props(ji) (cProps), the set of properties of relevance to time-unit p,  certainty.
Thus the duration of a non-closed existence of some time-object can only be shortened, If z does not have a valid existence in the context of time-axis a,then &,(a)= 1.
Hence a time-object can exist as a point-object on some time-axis but as an interval-object on another timeaxis.
In the former case the extent of the time-object is less that the time-unit of the particular time-axis.
A special moving time-object is now which exists as a point-object on any relevant time-axis and functions to partition time-objects into past, hture, or ongoing.
A time-object whose existence is expressed with respect to an abstractkoncrete time-axis is an abstract (generic)/concretetime-object.
Regarding temporal relations between time-objects, Allen's set [l] has been adapted and extended to fit the discrete, multidimensional, and multigranular, model of time.
More specifically a time-axis constitutes an argument of these relations, instances of which may be derived from absolute existence expressions; furthermore new relations based on temporal distance are added and usehl disjunctions are directly defined.
The structural relations between time-objects are isa-component - of, and its inverse contains, and variant-component, and its inverse v a r i a n t - contains; the latter two express conditional containment.
2.1 Time-units and time-axes Time-units define the possible granularities: scale(p,p 'A) gives the scale relation between granularities p and p'.
A time-axis, a, is expressed discretely at a specific granularity, p, in terms of a sequence of timevalues, Times&) = {t,,t, ,..,ti,..,f } , given with respect to the origin of the time-axis.
The origin of an abstract time-axis denotes a generic time-point; when an abstract time-axis is instantiated its origin gets bound to a real time-point.
Hence a concrete time-axis is 'linked' to the universal, real-time, axis.
The basic relation between time-axes is t-link(a,t,a ',t 7 that links a lime-value on one time-axis with a time-value on another time-axis.
Other relations, eg concurrent, includes, intersects-with, etc can be expressed through t-link.
Some of the time-axes are defined as spanning a chaining sequence of other timeaxes.
A spanning time-axis has a hybrid granularity inherited from its components.
For example lifetime could be a spanning time-axis encompassing fetal-period, infancy, etc.
2.2 Time-objects  contains(z ,T ) 1 J C= v a r i a n t - contains(z.,z.,Cs) A  A time-object, z, is a dynamic entity for which time constitutes an integral aspect.
It is viewed as a tight coupling between a propert?, and an existence, where its existence can be expressed with respect to different time-axes.
Its existence with respect to the most appropriate time-axis for it, is called the time-object's main existence.
Thus z = <xT,sT> where T C ~E Props is the property of z (n is a selector function, and the notation TC, stands for function TC applied to argument z), and hnction E~ : Axes -+ Eexps is the existence function of z.
The domain of E, is the set of time-axes and its range is the set of absolute existence expressions.
An absolute existence expression gives the (earliest) initiation and (latest) termination of some existence.
If time-object, 5 , has a valid existence on some time-axis, a, then E,(OI) = < t , , t p ; t,, tf E Times(a); t, I tf; and the status E {closed, open, o p e n - f r o m - l e f t , open-from-right, moving}.
If there is openness in some valid existence of a time-object then its actual initiation and/or termination is not known but approximated through an admissibility margin.
The initiation and termination admissibility  conds-hold(cs) I J A variant component can only be assumed, in some situation (eg a specific patient), if the specified conditions are satisfied.
A compound time-object has a valid existence under any context in which at least one of its components has a valid existence, and a time-object exists within the one that contains it.
Temporal views of a compound time-object, from the perspective of specific temporal contexts, can thus be defined.
Finally, relation causes between a pair of (abstract) time-objects zi and z. specifies various constraints (temporal and other) that need to be satisfied in order for a causality-l i n k to be established to hold between a pair of concrete instances of zi and zj.
A generic constraint, that always needs to be satisfied, is that a potential effect cannot precede its potential cause.
c a u s a l i t y - link(Z.,t.,Cf) I J c causes(zi,zj,Cs,Cf) A conds-hold(cs) A 7s t a r t s -before(?.
Z.)
J' 1 The fourth argument of relation causes denotes a  margins are given by <t ,t > and < tf ,tf > respectively where t,.
= le-fr (a)and %"=ri-fr,(a) (functions le-fr and  certainty factor, ie even if all specified conditions are  satisfied, in some situation, still it may not be definite that the particular c a u s a l i t y - l i n k actually exists in that situation.
The uncertainty is due to knowledge incompleteness.
Trends and periodic occurrences are modelled as compound time-objects, subsuming a number of other time-objects at a lower data and/or temporal level.
A generic periodic time-object is defined through a  ri-fr respectively stand for left-freedom and rightfreedom).
The existence of an open time-object, on a given time-axis, is therefore defined through an intial period of uncertainty, an in between period of certainty and a final period of uncertainty.
If the earliest termination of an open time-object precedes or coincides with its latest initiation then there is no period of  162  repetition element, a repetition pattern, and a progression pattern.
Some of the axioms are treated as deductive rules (eg the causality axioms) and others as integrity constraints (eg the containment axioms), a distinction adopted from [ 151.
A dletailed comparison between the proposed ontology and three widely adopted time ontologies, namely Allen's interval-based temporal logic [I], Kowalski and Sergot's event calculus [12], and Dean and McDermott's time token maps [3] is given in [7].
2.3 Properties Properties (ontology class Props), that constitute the other half of time-objects, are atomic or compound (negations, disjunctions, or conjunctions), passive or active, and some are time-invariant.
Examples of properties are "sex male", "sore throat", "severe coughing", etc.
A property is associated with relevant granularities, eg "headache present" is associated with hours and days, but not months or years.
This way the time-axes meaningful to a property or the subset of properties that can be instantiated in the context of some time-axis, a,AxisP(a), can be defined.
A property, p, either has an infinite persistence, ir&er(p), or a finite persistence, jnper(p).
In the latter case the following are additionally specified: whether the property can reoccur (multiple instantiations of the given property in the same context are possible); maximum and minimum durations (max-dur, min-dur) under any of the relevant granularities, where the default is any duration; and a default admissibility margin for the initiation of any instantiation of the property, under a specific relevant temporal context (earliest-init, latest-init), which if not specified is assumed to be the entire extent of the particular time-axis.
In addition, the proposed ontology adopts the semantnc attributes of properties specified by Shoham, eg downward hereditary, upward hereditary, solid, gestalt, etc [19].
Causality, with explicit temporal constraints, is specified at the level of properties as well, through relation cause - spec which is a 6-place relation where the first two arguments are properties, the third is a granularity, the fourth and fifth are sets of relative and absolute temporal constraints respectively, and the last argument is a certainty factor.
This relation also enables the derivation of the existence of a causality-link between a pair o f (concrete) time-objects through the following axiom.
causality-link(Z.,Z.,Cf) ' J C= cause-spec(pi,pj,p,TRel,Css,cf)A '7t(ti) = p. A '7t(Z.)
= p. A satisfied(Ti,z.,piTRel3 A satisfied(zi,zj,p,Css)A  3 Medical problem solver with integral temporal reasoning 'flie ontological primitives and their associated axioms form the ground layer of the embedded temporal kernel.
This is the layer that provides the interface with the (medical) system's knowledge-base and (patient) datalbase.
The other layers of the temporal kernel provide various temporal reasoning fimctionalities.
3.1 Temporal reasoning requirements 'The required temporal reasoning functionalities are classified, in ascending order of level, into mapping and clipping, derivation, and consistency and querying, functionalities.
These are listed below: Mapping and clipping: -- Determining bounds for absolute existences of occurrences, ie determining admissibility margins for initiations and terminations of occurrences.
-- Mapping occurrences across temporal contexts.
-- Detecting direct conJicts and clipping persistences.
Derivation: There are two types of derivations; deriving new occurrences, or deriving new relations between occurrences.
-- Deriving new occurrences through merging (see section 3.3).
-- Deriving new occurrences through decomposition.
The potential components of compound occurrences are inferred.
-- Deriving new occurrences through temporal data abstraction: a. Persistence derivation (see section 3.3).
b. Deriving temporal trends.
c. Deriving periodic occurrences.
d. Deriving potential clusters of related occurrences.
This is the opposite of the decomposition derivation.
-- Deriving causal antecedentskonsequents of some occurrence.
-- Deriving a relation between a pair of occurrences.
The types of relations are temporal, structural, and causality links.
Consistency and querying:  -starts -betJore(zj,zi)  Other property relations include exclusion, necessitation, etc.
In summary the ontological primitives provide the necessary conceptual abstraction for the adequate modelling of medical concepts (disorders, patient data, therapeutic-actions) [6].
Medical knowledge-bases and patient databases are viewed as collections of abstract and concrete time-objects respectively (and their relevant time-axes) and hence time becomes an integral aspect of the particular applications.
All the ontological classes and their associated axioms are discussed in detail in [7].
--  Estahlishing the overall consistency of some world  of occurrences.
163  occurrences.
A (hypothesised) occurrence, of any degree of complexity is queried against some world.
- Deriving the state of some world at a particular rime.
In an explorative mode, the problem solver may need to be informed about what is believed to be true, at some specific time, in some specific context.
The query may be expressed relative to another point in time which defaults to now, eg at time point t, what was/is/will be believed to be true during some specified period p?
Below we give algorithm for some of the mapping and clipping, and derivation functionalities.
- Querying  a refers to the main existence o f t , and returns a list of direct conflicts.
detect-conflicts obsaxs let conflicts be initialised to the empty list for each (qa)E obsaxs do for each (z ',a')that follows (t,a)in obsaxs do if ITT excludes E , , then let a' be the finest granularity time-axis between U and a' if &$a*) # IA &,.
(a* )#I then If 1 dis jo i n t(a*,T,T ') then enter ( t , z ' , a ' ) into conflicts else let a*be the finest granularity time-axis amongst the remaining relevant time-axes on which both z and z ' have a valid existence if there is such a time-axis and  3.2 Mapping and clipping functionalities First we discuss the mapping of time-objects across time-axes.
In order to map a time-object, z, from timeaxis, a, to time-axis, a',the following conditions must be satisfied: e The property of t, IT,: can be instantiated in the context of time-axis a', le K , E AxisP(a').
e The scale relation between p and p', the respective time-units for a and a', is known.
A linkage between the two time-axes is specified.
This means that an instance of relation t-link involving the relevant pair of time-axes is designated as defining the (basic) linkage between the two.
If any of these conditions is not satisfied then E,(u')= 1.A basic function of the mapping operation, map-Val, is to map a time-value, t E Times(@, from a to a' [7].
If a particular time-value cannot be mapped then map-val returns 1.
The mapping operation essentially involves mapping the base (t,) and limit (tf>time-values of t onto a'.
Further, if the existence of t on U entails uncertainty, then time-values le-frT(a) and ri-frT(a), if applicable, are also mapped.
Overall, there are four cases of mapping: mapping an interval-object to a coarser/finer granularity time-axis; and mapping a point-object to a coarsedfiner granularity time-axis.
Information is lost when the existence of a time-object is mapped onto a coarser granularity time-axis.
It is therefore quite possible that map(map(&,(a),a'),a) # &,(U).
Since every time-object has a main existence, to avoid the above problem, where possible, mappings should be from the main existence.
Next we discuss direct conflict detection and clipping of persistence.
Two time-objects are implicated in a direct conflict, if their existences are not disjoint and their properties are mutually exclusive; they are implicated in an indirect conflict if there is a direct conflict between one of these and a derivative of the other time-object.
A conflict is denoted by a triple ( t , z ' , a ) that gives the pair of time-objects andVhe time-axis in whose context the conflict is established.
Once a conflict is established, the next step is to try and resolve it.
Algorithm .fir detecting direct conflicts: The algorithm where accepts a list of time-object, time-axis pairs, (t,a),  disjoint(d,T,t')  then enter (z,t ',a*) into conflicts end for end for return conflicts end detect-conflicts Algorithm .for resolving Conflicts by clipping persistences: The algorithm uses auxiliary functions apparent-overlap, terminating-relation, and clip-persistence.
Predicate function apparent-overlap accepts a conflict and returns true if the relevant overlap could be apparent.
It is based on the heuristic that two time-objects which are disjoint under some granularity may appear as coinciding (eg if both of them are mapped onto the same point existence) or chaining, under a coarser granularity.
Function terminating-relation accepts a conflict, the relevant domain of time-objects, and an operation mode, reactive (meaning that (therapeutic) actions are only instigated in order to combat some established abnormality, and hence such actions could not have been started prior to what they aimed to terminate), or proactive (where therapeutic actions could also be instigated in order to prevent an anticipated, future, undesired happening; such actions would normally commence prior to what they aim to prevent).
The function returns a triple of values, where either all three values are equal to nil or they denote three time-objects (t. z.,~'), meaning that z' causes z. in order to terminate ti J (t:;nk-object Z' therefore denotes a terminating action for zi); time-objects zI and z. are the two specified in the parameter ofthe function,Jbut not necessarily in the given order.
terminating-relation (z,z',a)tobs mode if the mode is proactive or the initiations of z and t' on a coincide then the following reasoning is repeated for either combination of the two time-objects, ie ( t , , .
~is~set ) both to (z,z') and to (z',t) elsif t begins before z ' on U then zI is set to t and zj is set to z'  164  else ti is set to t' and t. is set to t J end if let relact E tobs such that the property for each of the selected time-objects denotes a potential terminating action for the property of zi repeat for each t' E relact if a c a u s a l i t y - l i n k can be established from t' to t. J and 2' does not begin before tion U (if the reasoning is done in a reactive mode), then 't is the rellevant action until a relevant action is found if a relevant action, t', is found then retum (ti,tj,t*)else return (nilpilpil) end terminating-relation Finally function clip-persistence accepts a pair of time-objects (t,t') and modifies the main existence of z to the minimum necessary so that this is disjoint from the existence of t'.
If in order to do so the whole existence of t is revoked or its duration drops below a specified minimum for the relevant granularity then the hnction returns false else it returns true.
The main algorithm accepts a list of conflicts, the relevant domain of time-objects, and an operation mode (reactive or proactive).
It returns a triple (appoverlaps, unresolvedc, revokedobs) where appoverlaps gives the conflicts that it has not investigated because the relevant overlap may well be apparent, unresolvedc gives the conflicts that it has not managed to resolve, and revokedobs gives the time-objects whose existence is revoked in order to resolve the conflict.
and cl is the cluster of existing time-objects subsumed by it end merging-deriv The relevant subset of time-objects is defined as: relobs(p,u) = { z I t E Tobjects(a); x a p}.
Thus the set of relevant periods of time on a, f(p,a), is given by: I(p,u) = {&,(U)1 t E relobs(p,a)}.
Let int c I(p,u); int conistitutes a conjoined chain iff its elements collectively cover all time-values included in the minimal period of time that spans all elements of int: con-ioined-chain(int) e t, t E extent(int) (3 t p E int such that t is included in tp) where extent(int) is the sequence of time-values  'p'  tb = min,< t, I <tet,,tf > ,< > E int } ,t = max, { tf I < t,,tf ,< > E int } p is the granularity of a Let int 5: Int 5: I(p,u); int constitutes a maximal conjoined chain in Int, iff it is a coinjoined chain and no other subset of Int constitutes a conjoined chain that strictly includes int's chain.
mar;-conj-chain(int,Int) e (conjoined-chain(int) A 1(3 int' Int such that (conjoined-chain(int') A extent(int) c extent(int')))) Let int c I(p,u), where conjoined-chain(int).
The minimal existence that spans all elements of int is given by function chain: cham(int) = <ts,,tf ,p where t,.
= min, {t I <ts-> E int} tf = maxp (if I <-,t+> E int} c1= (<t,.,closed> E int) v (<t,.-,open- from-right> E int) c2 = (<-,tf .,closed> E int) v (<-,t, ,,open-from-left> E int) q = closed, if c1 A c2 ,C = open- f rom-right, if C1 A l C2 <=open-from-left,if-c, A C ~ 5 = o p e n , i f l ~A ~ ~ C ~ Let int c I(p,a).
The minimum set of maximal existences that collectively cover all elements of int is given by function merge: merge(int) = {chain(i) I i E power(int); max-conjchain(i,int)} The refined algorithm is now given.
Algorithmfor deriving time-objects lhrough merging merging-deriv (p,u) let result be initialised to the empty list if p is a concatenable property then do: let relobs = (z I t E Tobjects(u); x, a p} int = (eT(u)I t E relobs} maxexist = merge(int) repeat for each maxe E maxexist let ts = { t I z E relobs ; I  3.3 Derivation functionalities Derivation operations aim to derive, from the specified world of occurrences, new occurrences or new relations between occurrences.
A derivation is nondirected, if only the type is specified, or directed if a derivation pattern or template is specified which is matched against the particular world.
First we discuss the case of a semidirected merging derivation where the property concerned, p, and the time-axis, U,are specified.
The property p is assumed to be concatenable [19], and it may well be a compound property such as a disjunction of related properties, eg p mild-coughinq V moderate-coughing coughing.
The algorithm is outlined below:  V  I  severe-  merging-deriv ( p a ) do the following: 1. select the relevant subset of time-objects from the particular world, relobs(p,a) 2. partition relobs into clusters that constitute maximal, conjoined, chains 3. for each cluster, derive the time-object that subsumes it end return the list of associations (z,cl) where z is a newly derived time-object (xT= p)  165  extent {.$a)) c extent {maxe}} if 3 (maxe',zs) E result then replace this entry of result with the new entry (chain {maxe, maxe'}, zs) else enter the association (maxe,zs) into result end repeat return result end merging-deriv As a side point we define strong and weak chains of time-objects.
Let zs G relobs(p,a), int = {&,(a)1 z E zs) E I(p,a), such that conjoined-chain(int).
A conjoined chain formed by the existences of a set of time-objects is considered strong if the chain is not likely to break due to potential uncertainties regarding its elements: strong-chain(ts) e conjoined-chain(int ') where int' = {< le-frT(a)ji-frT(a),ciosed> I z E zs; le-fr,(a) 5 ri-fr,(a)) Thus, weak-chain(zs) = 7 strong-chain(zs) Next we discuss one form of temporal data abstraction, persistence derivation.
The problem of persistence derivation is explained through an example.
Suppose there is some patient whose record includes two radiographs of the spine, one taken at the age of 2 years and the other at the age of 4 years.
Both radiographs show the presence of abnormality "kyphoscoliosis".
These observations correspond to two point-objects at the granularity of years.
The problem is how to see beyond these two discrete sightings, both forwards and backwards in time, eg what can be inferred about the existence of this condition with regard to the particular patient, before the age of 2 years, between the ages of 2 and 4 years, and after the age of 4 years?
Disorder expectations usually refer to interval occurrences and hence persistence derivation is a necessary fbnctionality for the proper matching of a disorder profile against a patient profile.
The algorithm given below operates in a nondirected fashion; it accepts a time-axis, a, and a collection of time-objects, which have the same property, and exist as point-objects under 01, and returns the interval-objects that represent distinct, maximal occurrences of the particular property, under the given temporal context.
Algorithmfor persistence derivation derive-persistence a tobs let p be the time-unit for cx p be the property shared by the elements of tobs z, ,z2,...,z,, ..., t, be the elements of tobs in ascending temporal order t,,t, ,..., t,, .,..,t, E Times(a) be the respective positions of t,.t2...,zi ,..,z, on cc iobs be initialised to the empty set if p is an infinitely persistent property ;;case 1 then enter into iobs a sin$le interval-object with tf = upper-time(a) ;; termiriation t, = earliest-init(p,a) ;;earliest initiation ts.
= minp{latest-init(p,a),t,)};;lolest initiation  if t, > ts.
then a conflict is noted elsif p IS a finitely persistent, but not a reoccurring, property ;;case 2 then enter into iobs a single interval-object with t, = maxiearliest-init(p,a),(t, -I.I max-dur(p,a))} t,.
= minit,, t, -I.I mindur(p,a),latest-init(p,a)} tf = max{(t, +p min-dur(p,a)), b} t,, = (ts.
+p max-dur(p,a)) ;;earliest termination if ts > t,.
or tf , > tf then a conflict is noted else ;; p is a finitely persisting, reoccurring, ;;property (case 3 ) let uncovered-obs be initialised to the temporal sequence z, ,z2,...,zi,..,z, repeat 1. select the longest initial segment of uncovered-obs that constitutes a nonconflicting instance of case 2 2. enter the interval-object that corresponds to the selected segment into iobs 3. remove selected segment from uncovered-obs until uncovered-obs is the empty sequence return iobs end derive-persistence The clustering of the point-objects performed under case 3 determines significant temporal distances (on the basis of the property concerned) between neighbouring objects which sign@ separate incidents.
The derivation of temporal trends is based on a modification of the above algorithm [7].
4 Validation The original motivation for the ontology came mainly through the SDD system, a diagnostic expert system for the domain of skeletal dysplasias [l 11, although many of the temporal requirements identified for medical problem solving were in parallel detected in the context of other research projects whose scope was the modelling of biochemical and other industrial processes [4],[6].
In all these applications the relevance of temporal notions such as periodicity, trends, delays, prematurity, etc, was quite evident and hence of the notions of temporal distance and absoluteness, as well as the need for different granularities and conceptual temporal contexts.
In addition, through such practical work, it was appreciated that the ontological primitives had to be at a high level of abstraction (knowledge level) to adequately support the knowledge engineering of complex dynamic processes.
The original ideas, which were demonstrated through SDD's temporal reasoner [lo], have been considerably enhanced and consolidated in the context of the CEC AIM Project GAMES I1 whose objective was to develop a theoretical framework and practical tools for the development of medical knowledge-based systems.
The proposed ontology and temporal reasoning mechanisms constitute part of the overall GAMES I1 product [20].
166  Furthermore, the ontology has been applied in building a simple prototype system modelling the normal ossification processes for parts of the human skeleton, and has been utilised in formulating a temporal model for medical diagnostic reasoning [9].
The plan for the immediate future is to filly implement the ontology and associated temporal reasoning algorithms in the form of an efficient and reusable temporal kernel for knowledgebased problem solvers, primarily for medical applications, but for other applications as well.
Time", Artjlkial Intelligence, Vo1.23, pp.123-154, 1984.
L. Chittaro, M. Del Rosso, and M. Dojat M., "Modeling Medical Reasoning with the Event Calculus: an application to the management of mechanical ventilation", Proc AIME 95, Lecture Notes in Artificial Intelligence, Vol.
935, Springer, pp.79-90, 1995.
T. Dean and D. McDermott, "Temporal Data Base Management", Artificial Intelligence, Vo1.32, pp.
1-55, 1987.
G.A.
Dervakos, J.M.
Woodley, E.T.
Keravnou, J. Washbrook and M.D.
Lilly, "Development of a KBS for Biotransformation Process Design, Proc.
I.CHEME.E.
Symposium Series N0.114, pp.283-291, 1989.
LJ.
l-iaimowitz and LS.
Kohane, "Managing Temporal Worlds for Medical Trend Diagnosis", to appear in [8].
E.T.
Keravnou, "Modelling Medical Concepts as TimeObjects", Proc AIME 95, Lecture Notes in Artificial Intelligence, Vol.
935, Springer, pp.67-78, 1995.
E.T.
Keravnou, "An Ontology of Time Using Time-Axes and Time-Objects as Primitives", Technical Report, Department of Computer Science, University of Cyprus, 1995.
E.T.
Keravnou (ed.
), Temporal Reasoning in Medicine, special issue of ArtiJicial Intelligence in Medicine, Vo1.8, No.3, June 1996.
E.T.
Keravnou E.T., "Temporal Diagnostic Reasoning Based on Time-Objects", to appear in [8] E.T.
Keravnou, J. Washbrook, "A Temporal Reasoning Framework Used in the Diagnosis of Skeletal Dysplasias", Artificial Intelligence in Medicine, V01.2, NOS, pp.239-265, 1990.
E.T.
Keravnou, F. Dams, J. Washbrook, C.M.
Hall, R.M.
Dawood and D. Shaw, "Modelling Diagnostic Skills in the Domain of Skeletal Dysplasias", Computer Methods and Programs in Biomedicine, Vo1.45, pp.239-260, 1994.
R. Kowalski and M. Sergot, "A Logic-Based Calculus of Events", New Generaiion Comput, V01.4, pp.67-95, 1986.
P. Ladkin, "Primitives and Units for Time Specification", Proc AAAI-86, pp.354-359, 1986.
W. Long, "Temporal Reasoning for Diagnosis in a Causal Probabilistic Knowledge Base", to appear in [8].
J. Mylopoulos, A. Borgida, M. Jarke and M. Koubarakis, "Telos: A Language for Representing Knowledge About Information Systems (Revised)", Technical Report KRRTR-89-1, Department of Computer Science, University of Toronto, 1990.
A. Riva and R. Bellazzi R., "Learning Temporal Probabilistic Causal Models from Longitudinal Data", to appear in [SI.
B. Rosser, J. Washbrook, J. Campbell, E.T.
Keravnou and D. Long, "A Framework for Time Dependent Reasoning Systems", Product PI 1 1-1, Esprit Project P2409 (EQUATOR), 1989.
Y. Shahar and M.A.
Musen, "Knowledge-Based Temporal Abstraction in Clinical Domains", to appear in  5 Conclusions Time is essential in problem solving.
With regard to medical problem solving time is intrinsically relevant to all medical concepts [6] (disorders, patient data, therapeutic interventions) and hence temporal reasoning ought to constitute an integral aspect of the overall problem solving.
Many of the general theories of time proposed in the AI literature [1],[3],[12] focus on natural language processing, or a specific class of problem solving tasks, usually planning or monitoring, or the management of temporal databases.
Some of these general theories have been effectively adapted in the context of specific medical applications [2].
By and large the work done in temporal reasoning for medical applications focuses on temporal data abstraction and temporal causality [5],[ 14],[ 16],[ 181.
The overall aim of the research presented in this paper is to develop an ontology of time, and associated temporal reasoning, based on a global analysis of time representation requirements for different medical tasks (diagnosis, monitoring, therapy planning) [8] and to effectively implement these through a generic temporal kernel.
The ontology must provide the required level of abstraction for knowledge modelling purposes and its usage must result in treating time as an integral aspect of the application.
The chosen ontological primitives, the timeaxis and the time-object, satisfy these design objectives and in addition the notion of a time-object integrates in a uniform and natural way temporal, structural, and causal knowledge.
However, the high power of expression provided by the proposed ontology is associated with relevant computational overheads.
Acknowledgements The research reported here has been partly supported by the University of Cyprus, the Leverhulme Trust (UK) (under projects SDD and MSD), and the CEC (under AIM Project A2034 GAMES 11).
The development of the ontology has benefit from discussions with other participants on these projects, in particular John Washbrook, Hauke Kindler and Mark Leaning.
In addition Kazem Sadegh-Zadeh, John Mylopoulos, and the three unknown referees have offered useful suggestions.
[W.  Y. Shoham, "Temporal Logics in A I Semantical and Ontological Considerations", Artificial Intelligence, V01.33, pp.89-104, 1987.
G. van Heijst, M. Ramoni, G. Schreiber and M. Stefanelli (eds.
), CEC AIM Project A2034 GAMES-II Final Report, 1995.
References [l]  J.F.
Allen, "Towards a General Theory of Action and  167
Parallel Temporal Resolution Clare Dixon Department of Computer Science University of Manchester Manchester M13 9PL, UK.
Michael Fisher and Rob Johnson Department of Computing Manchester Metropolitan University Manchester M1 5GD, UK.
dixonc@cs.man.ac.uk  fM.Fisher,R.Johnsong@doc.mmu.ac.uk  Abstract Temporal reasoning is complex.
Typically, the proof methods used for temporal logics are both slow and, due to the quantity of information required, consume a large amount of space.
The introduction of parallelism provides the potential for significant speedups together with the increased memory size required to handle larger proofs.
Thus, we see the effective utilisation of parallelism as being crucial in making temporal theorem-proving practical.
In this paper we investigate and analyse opportunities for parallelism within a clausal resolution method for temporal logics.
We show how parallelism might be introduced into the method in a variety of ways, providing a range of options for the parallel implementation of proof procedures for this important class of logics.
1 Introduction Temporal logics are non-classical logics that have been specifically developed for reasoning about properties that vary over time [10].
Varieties of temporal logic have been used in both computer science and AI to represent and reason about dynamic systems and systems dealing directly with temporal information.
In many of these areas, some form of proof or validation is required.
However, as complex systems are often represented by correspondingly large temporal formulae, proofs about such systems are usually long and computationally intensive.
A further problem is that, in general, deciding whether a temporal formula is valid or not is difficult.
Although the worstcase complexity for temporal proof methods may not arise frequently, a large amount of temporal information needs to be handled even in the average-case.
We see the practical viability of large-scale temporal theorem proving as being dependent upon the effective utilisation of parallel architectures.
The successful implementation of parallel proof methods appears to be the only means whereby these large amounts of temporal information can be handled tractably.
Although parallel techniques cannot always be applied productively, certain proof methods that have been developed for temporal logics, particularly clausal resolution and semantic tableau,  show potential for parallelisation.
The exploitation of parallelism is, unfortunately, far less straightforward for temporal logics than in the case of classical logics since the additional complication of partitioning graphs, together with other graph manipulation operations, must be introduced.
It is a variety of approaches to tackling this problem that we seek to address in this paper.
1.1  Parallel Theorem-Proving  The parallel implementation of a theorem-prover, whilst not affecting the worst-case complexity of the underlying proof procedure, often enables us to handle much larger formulae, and to process them much faster.
In classical logics, parallel implementations of various proof procedures have been developed, including resolution [7], semantic tableau [13], and model elimination [21].
The parallel implementation of these theorem-provers is closely related to the development of parallel implementations of logic programming systems, where potential for parallelism occurs at several different levels of granularity within logical formulae (e.g., formula, clause, or literal) and where several alternative evaluation strategies (e.g., AND/OR parallelism) can be utilised [15].
1.2  Temporal Theorem-Proving  What makes temporal theorem-proving more difficult than the classical case, and why can't we directly use the systems already developed?
The answer to both questions lies in the fact that temporal logics, particularly the logic we study in this paper, provide more expressive power than their classical counterparts.
As discrete temporal logics essentially encode a simple form of induction, then decision procedures for such logics usually involve two forms of search -- tree search within non-temporal formulae and graph search within temporal formulae [11].
Thus, the parallelisation of such decision procedures will involve the integration and parallel implementation of two, distinct, types of search.
While the first of these is equivalent to the problems encountered in mechanising classical logic, the necessity of implementing additional graph search mechanisms means that temporal theoremproving is fundamentally different to classical theoremproving.
1.3 Structure of the Paper We begin, in SS2, by outlining the temporal logic that we wish to mechanise.
In SS3, the clausal resolution method that we use for proofs in this logic is described [11].
This description includes not only an outline of the proof method, but also an overview of the areas within this algorithm where parallel implementation is either possible or particularly beneficial.
The subsequent sections describe the potential for parallelism within these areas in more detail, examining translation to the clausal form (SS4), non-temporal resolution (SS5), simplification (SS6), temporal resolution (SS7), and the possibility of executing several of these elements in parallel (SS8).
Finally, in SS9, we present a summary and outline our future work in this area.
2 A Propositional Temporal Logic In recent years, temporal logics have been used for representing time-dependent information [3], for the specification and verification of reactive systems [17], and for direct execution [5].
Within the broad category of temporal logics, a range of different models of time have been utilised, including discrete, dense, linear, branching and interval-based models [10].
In this paper, we will consider a discrete, linear model of time, and will outline a propositional temporal logic based on this model.
This logic is simply called PTL.
Rather than give the syntax and semantics of the full logic, we will present a brief description of the elements of the language, together with their semantics, required for representing the clausal resolution proof method [11].
2.1 Syntax of PTL The set of well-formed formulae of PTL (WFFp ) is defined as follows.
* Any proposition symbol is in WFF p .
* If A and B are in WFFp , then so are !A A  }  A[?
]B A  A[?
]B wA  A=B gA f e d c  AW B  Here, the temporal operators used consist of the futuretime temporal operators, ' ' and ' ', and the past-time g'.
f e temporal operators, ' w' and ' cd  }  2.2 Semantics of PTL Intuitively, the models for PTL formulae are based on discrete, linear structures having a finite past and infinite future, i.e.
sequences such as s0 , s1 , s2 , s3 , ..., where each si , called a state, provides a propositional valuation.
We can visualise this as representing a sequence of 'moments' in time.
Thus, the semantics of a proposition is defined by the valuation given to it at a particular state.
While the semantics of the standard propositional connectives are as in classical logic, the semantics of the future time temporal operators are as follows: A is satisfied at a particular state if A is satisfied at some state in the future; A is  }  satisfied at a particular state if A is satisfied at all states in the future; A W B is satisfied at a particular state if A is satisfied unless a state where B is satisfied occurs.
We are also able to refer to properties in the past.
As temporal formulae are interpreted at a particular stateindex, i, then indices less than i represent states that are 'in the past' with respect to state si .
The two past-time operators have similar semantics but, as there is a unique start state, termed the beginning of time, they have slightly different behaviour when interpreted at this state.
Thus, gA are satisfied at a particular state if A is f e both wA and cd satisfied at the previous state, but wA is satisfied, while gA is not, when interpreted at the beginning of time.
In f e cd particular, wfalse is only satisfied when interpreted at the beginning of time.
Note that the following equivalence relates these two last-time operators.
g!A f e cd  = !
wA  Although this implies that only one of these operators need be defined, we will see later that it is useful, particularly when defining the normal form, to utilise both operators.
Note that a variety of other temporal operators are available but, as these will be represented within the normal form discussed in the next section simply in terms of the above operators, we will omit the definitions of these derived operators.
2.3  Proof Methods for PTL  Discrete temporal logic essentially characterises the combination of a classical logic system with a minimal form of induction.
Decision procedures for such temporal logics are correspondingly more complex, being PSPACE complete in the worst-case.
Several proof methods for propositional discrete temporal logics, such as PTL, have been developed, the most widely used being semantic tableau or automata-based methods [23, 12].
More recently, proof methods have been developed based on a translation to classical logics [18] and on resolution, both clausal [11] and non-clausal [1].
Several of these proof techniques, particularly clausal resolution and semantic tableau, show potential for parallelisation.
The parallelisation of the temporal tableau method is discussed in [14], while the parallelisation of the resolution method is the subject of this paper.
3  Clausal Temporal Resolution  The temporal resolution system that we investigate here is presented in [11].
The basic idea behind it is to translate an arbitrary PTL formula into a set of formulae in a normal form.
This normal form, which corresponds to clausal form in classical resolution, is called Separated Normal Form (SNF).
Formulae in SNF are of the form  ^(P n  i  i=1  = Fi ) .
Here, each Pi is a strict past-time temporal formula and each Fi is a non-strict1 future-time formula.
Each of the 'Pi = Fi ' (called rules) is further restricted to the following:  _q m  wfalse  ^p  =  l  g f e cd  b  (an initial  b  (a global  _q m  a  =  a=1  -rule)  b=1  wfalse  =  }s  (an initial  ^p  =  }s  (a global  l  g f e cd  -rule)  b=1  a  }-rule)  }-rule)  a=1  where each pa , qb or s is a literal.
Once the translation to SNF has been carried out, then all temporal statements within PTL are represented as sets of such rules.
Thus, if we are to derive a contradiction, there are effectively only two ways to achieve this, given a set of SNF rules.
The first is to apply resolution within a state (i.e., between -rules), the second is to apply resolution to -rules.
The former is equivalent to classical resolution within a state, the latter involves a search for sets of rules which, together, represent a -formula that complements the selected -formula.  }
}  3.1 Sequential Resolution Method Given an arbitrary temporal formula, ph , that we wish to show is unsatisfiable, the clausal temporal resolution method [11] proceeds through the following steps: 1. rewrite ph into Separated Normal Form (SNF), giving a set of rules phs (note that this transformation preserves satisfiability); 2. perform non-temporal (effectively classical) resolution on pairs of rules within ph s -- if false is derived, terminate, noting that ph is unsatisfiable, otherwise continue; 3. perform simplification and subsumption; 4. choose an eventuality that appears in the set of SNF rules, e.g., !p, and look for sets of rules forming "loops in p" (i.e., sets of rules representing ' p'), add resolvents (rewritten into SNF) for all the loops found;  }  5. if any new formulae are generated, rewrite them into SNF, add them to phs and go to 2; 6. terminate, noting that ph is satisfiable.
Before examining the potential for parallelism in each step of the proof process, we will consider which steps involve the greatest complexity.
1  Here, 'non-strict' means "including the present".
3.2 Complexity of the Procedure The complexity of the current (sequential) resolution procedure [9] is: * translation to SNF -- although used many times throughout the proof process, this transformation is not expensive, usually involving only a linear increase in the length of formula and a linear increase in the number of symbols; * non-temporal resolution -- as in classical case, i.e.
NP-complete; * temporal resolution -- exponential for each formula.
In practice, the translation to SNF is relatively quick, the non-temporal resolution step can be slow, while the temporal resolution step is usually slow.
As this suggests, the main effort has involved investigating the parallelisation of the temporal resolution step.  }
3.3 Opportunities for Parallelism There are several places in the sequential algorithm where parallel techniques can (possibly) be exploited: 1. in the translation from an arbitrary formula to SNF; 2. in the non-temporal resolution step; 3. during simplification and subsumption; 4. in the application of temporal resolution to distinct eventualities; 5. when searching for loops.
We will consider the potential for parallelising these parts of the process in the subsequent sections2 .
In SS8, we consider the coarser-level parallelism available by executing some of these components in parallel.
4  Conversion to SNF  Although parallelisation is possible in the conversion of an arbitrary formula into SNF, we note that 1. this translation is relatively efficient anyway, and, 2. even if parallel techniques are applied, the bottleneck of ensuring that instances of the same formula are identically renamed, is a (potentially severe) restriction upon parallelism.
Thus, we will not consider the parallelisation of this step (although, in the journal paper, we will outline approaches to this problem).
5  Non-Temporal Resolution  Non-temporal resolution is a variation on classical resolution, being given as the rule: gA f e cd gB f e cd  g(A [?]
B) f e cd  = F[?]
l = G [?]
!l = F[?
]G  2 Apart from (4), which will be considered in more detail in the journal version of this paper.
The exploitation of parallelism is possible here and, indeed, many systems have been developed with this in mind.
This enables us to utilise the parallel resolution schemas that have been examined by others for classical logics.
For example, the following elements have already been developed within parallel resolution systems.
7  Temporal Resolution  The general temporal resolution rule, written as an inference rule, can be described as gA f e cd  wtrue  * The generation of new literals (ROO [16]).
* OR-parallelism with spanning sets (DelPhi [8]).
* OR+AND parallelism (the Andorra model [22]).
* OR-independent AND parallelism (PEPSys [4]).
* Pipelined parallelism (POPE [6]).
Rather than describe any of these options in more detail, we simply note that our non-temporal resolution step is able to utilise such advances in parallel resolution for classical logic.
We will give an agent-oriented (see SS8) description of such non-temporal resolution in the journal paper.
gP = falseg - f e Simp3 : f cd    = !A [?]
!Q  where 'L is a last-time operator.
Here, the first resolvent shows that both A and Q cannot be satisfied together, while the second that once Q has occurred then !p must occur (i.e.
the eventuality must be satisfied) before A can be satisfied.
The full temporal resolution rule is given by expanding gA = f e the ' cd p' rule into its constituent parts composed of global -rules, as follows.
gA f e cd  0 = F0 ... ... gA f e cd n = Fn LQ = !p  }  wtrue  6.1 Simplification The simplification rewrite rules used in the resolution method for PTL are: Simp2 : fP = trueg - fg;  p  }!p  LQ = (!A) W (!p)  6 Simplification and Subsumption  gfalse = Ag - fg; f e Simp1 : f cd  =  LQ =  = !Q [?]
LQ = (  ^ !A n  i  ^ !A ) W !p n  i=0 i  i=0  wfalse gtrue f e d c  fi = !P = !P  The first two remove valid formulae, while the third is the basic mechanism for transferring constraints between states.
Since the pattern of the left-hand sides of each of these three simplification rewrite rules is different we may concurrently apply each to the ruleset (R).
If we attribute a heterogeneous simple process to the application of each rule Simpi they may all operate in parallel on R. Furthermore homogeneous copies of each simplification process may execute concurrently.
Therefore we may (theoretically) utilise a potential 3 x |R| processors in the simplification process at this level of granularity.
6.2 Subsumption As in the classical case, a particular bottleneck for parallel resolution is in the subsumption checking for newly generated resolvents.
The formulation of processes for this portion of the algorithm can be tailored according to the required granularity.
We may combine all simplification and subsumption elements together.
Alternatively we may consider two processes, one dealing with simplification, and the other with subsumption.
In the journal paper, we will expand this section, outlining the finer structure of processes for simplification and subsumption (and actually defining them in terms of 'agents').
with side conditions that for all i such that 0 <= i <= n, 1.
` Fi = p, and, 2.
` Fi =  _A .
n  j  j=0  Thus, the side conditions ensure that each -rule makes p true and the right hand side of each -rule ensures that the left hand side of one of the -rules will be satisfied.
So if any of the Ai are satisfied then p will be always be satisfied, i.e.,  _A = n  g f e cd  k  p.  k=0  Such a set of rules are known as a loop in p. Thus, the temporal resolution step essentially consists of a search for a set of rules which together represent a -formula, complementary to the -formula to which the resolution is applied.
It is this search that is usually the most costly element of the whole resolution process.
Fortunately, it is also an element with a significant potential for parallelisation.
There are (currently) two general ways to implement the temporal resolution search, given a set of rules and a -formula.
1.
Merge all possible SNF rules to give SNFm rules3 , then search for strongly connected components (SCCs) within the graph defined by the SNF m rules [11].  }
}  3  SNFm is effectively a form of SNF where the right-hand side of -rules are in Disjunctive Normal Form (DNF), rather than just being a disjunction of literals.
2.
Attempt to lazily construct portions of the above SNFm structure directly from the SNF rules, searching for SCCs during this construction [9].
In the worst case, the second approach has the complexity of the first.
The complexity of the first approach is an exponential step (SNF - SNFm ) followed by a linear one (SCC detection using Tarjan's algorithm [2]).
We will term the first approach the naive search, and the second approach the lazy search.
7.1 Parallelising Naive Search The basic bottleneck in the naive search is the SNF to SNFm translation.
This involves conjoining all possible subsets of the set of SNF rules and rewriting the new rules' right-hand sides in to DNF.
Potentially many parallel processes can be employed in order to achieve this.
The main problem that occurs is that of merging all the generated sets of rules back in to one set (and removing subsumed rules).
One approach that may alleviate this is by using an intelligent merge, where the sets of rules generated are first sorted before merging.
We will not consider the parallelisation of the naive search in any more detail here, as it is the lazy search that has been the focus of development for the sequential proof method.
Its advantage in the sequential case, namely the minimal space consumed, ensures that this approach is also (initially) more desirable in the parallel case.
Thus, it is this lazy search that we concentrate upon.
7.2 Parallelising Lazy Search Within the lazy search approach, there are two basic algorithms: 1. depth-first search through the rule-set looking for a 'loop'; 2. breadth-first search through the rule-set looking for a 'loop'.
As in the sequential case, as the breadth-first algorithm detects all loops for a particular eventuality whereas the depth-first search described in [9] finds one at a time, we will predominantly consider the parallelisation of the former.
However, we should mention that, as the depthfirst search is an example of independent search with backtracking, we can utilise many of the techniques developed for standard OR-parallel search.
In particular multiple depth-first searches could be invoked in parallel, with the first successful process being the loop considered.
An important element of this would be the sharing of results between independent branches, which would avoid several searches 'discovering' the same loops.
Returning to the breadth-first algorithm, we first give a brief outline of its operation, assuming we wish to detect a loop in the literal p. The breadth-first algorithm builds a graph structure consisting of nodes labelled by formulae in disjunctive normal form (DNF).
When building each node we attempt to use all possible combinations of SNF rules that satisfy the expansion conditions rather than picking one and backtracking as in the depth-first algorithm.
The  top node, N0 , is the disjunction (and simplification), of the conjunction of literals appearing on the left-hand side of rules that ensure p in the next moment in time.
A new node Ni+1 is constructed from node Ni by using all possible SNFm rules where, for each rule, the conjunction of literals on the left-hand side implies the top node and whose right-hand side implies the previous node N i .
The former makes sure that the rule is guaranteed to give p in the next moment in time, and the latter is for constructing the looping we require.
Termination occurs either when the node we have just constructed is equivalent to the previous node (or equivalent to true), or when we have been unable to construct a new node.
If the former, then we have detected a loop, if the latter then we have not found a loop to resolve with the eventuality that we are considering.
More formally, we can describe the breadth-first loopsearch algorithm as follows.
For each rule of the form LQ = !p (where L is either of the last-time operators) do the following.
gT = p, (called f e 1.
Search for all the rules of the form cd i start rules), disjoin the left hand sides, simplify and make the top node N0 equivalent to this i.e.  }
N0 =  _T .
i  i  If ` N0 = true we terminate having found a loop.
2.
Given node Ni = Dk  _ k  where Dk is a conjunction of literals, to create node Ni+1 for i = 0, 1, ... look for rules or combinations of gA = B where ` B = N and f e rules of the form cd j j j i ` Aj = N0.
Disjoin the left hand sides so that Ni+1 =  _A  j  j  and simplify as previously.
3.
Repeat 2. until (a) ` Ni = true.
We terminate having found a breadth-first loop and return true.
(b) ` Ni = Ni+1 .
We terminate having found a breadth-first loop and return the DNF formula Ni .
(c) The new node is empty.
We terminate without having found a loop.
We next consider a range of strategies for parallelising this algorithm.
Although this list is not meant to be exhaustive, it does provide a basis for comparison between approaches.
Indeed the prototype implementations of some of these strategies have shown them to be relatively successful.
7.2.1 Strategy 1: Top-level partition of disjuncts This strategy essentially consists of examining the first node, partitioning the disjuncts within this node, and performing breadth-first search separately on each of these new nodes as normal.
Advantages If we pick a good subset of disjuncts we can produce a breadth-first graph for this subset that not only finds the loop, but has fewer nodes and contains fewer literals (just by totaling the number of occurrences of each literal in the graph) than the full graph.
Thus, if we can apply some heuristic to detect a good subset then we can detect a solution more quickly than with full breadth-first search.
Disadvantages If we pick a bad subset of disjuncts we may produce a breadth-first graph that finds the loop, contains the same number of nodes, but contains more literals (since we have had to carry out more combinations of SNF rules) than the full breadth-first graph.
If a loop is detected each solution (i.e., a DNF formula) will imply the solution from the full breadth-first graph.
However, even though this holds, we may obtain a less general solution than with the full breadth first search.
This means that if the less general solution does not give specific enough resolvents to produce a contradiction we may have to do another round of temporal resolution to obtain a more general solution.
Perhaps most importantly, we do not yet have a suitable heuristic for identifying good partitions, although work on developing heuristics to discard rules that will never form part of the loop (in the sequential algorithm) may be applicable here.
Also, since a smaller set of disjuncts is not guaranteed to lead to a solution, then, to retain completeness, we must always ensure that the full breadth-first search runs in parallel.
7.2.2 Strategy 2: Parallel node construction Here, we carry out a normal breadth-first search but, when constructing the next node, apply the expansion of each disjunct in parallel.
Advantages Recall that we are searching for combinations of rules where the right hand side implies the previous node, and the conjunction of literals on the left hand side implies the top node.
Thus, for each disjunct di,j in the previous node we look for sets of SNF rules to combine where each right hand side contains a literal in di,j .
We combine them so that every literal in d i,j is covered, making sure that the right hand side of the combined rule we have built does actually imply the previous node.
By expanding each disjunct separately we will obtain a DNF formula representing the disjunction of the left-hand side of the combined rules used to expand each di,j as long as the right-hand side of the combined rule has no disjunctions in it.
By carrying this out in parallel we will construct the (unsimplified) new node more quickly than in the sequential version.
Further, as the same disjunct may appear in several nodes, and therefore need expansion several times, by storing the DNF formula obtained by expanding a particular disjunct we can avoid repeating calculations to expand the same disjunct several times.
Disadvantages By expanding each disjunct in parallel we may merge the same combinations of rules more than once to cover two or more disjuncts when the combined rule we are applying has disjuncts on the right-hand side (i.e.
we may repeat work by looking at each disjunct separately).
Having expanded each disjunct in node Ni we may end up with several (large) DNF formulae which we must now disjoin and simplify to build node N i+1 .
In the sequential version simplification is carried out as we use a new rule to expand a node so that the DNF formula does not become excessively large.
7.2.3  Strategy 3: Search re-use  Here, the basic approach is to take some subset of the set of disjuncts from the top node (as strategy 1), try to build a breadth-first graph and, if we fail, add new disjuncts to the subset from the original top node and extend the graph, saving what we had before.
Advantages Appears to allow the incremental construction of the breadth-first graph by reusing parts of the graph that have previously been computed.
Disadvantages Unfortunately, in most practical cases we have to amend what has been saved so extensively it is just as costly as building the full breadth-first structure in the first place.
Specifically, when trying to reuse parts of the graph saved from a previously failed search we must check that 1. we have not overcombined the required rules, and, 2. we have combined the rules enough times.
This occurs simply due to the fact that we have added disjuncts to the top node and now have extra conjunctions of literals that will generate the required literal.
However, there seems to be no way to easily test whether, for the disjuncts already constructed, it was necessary to add in more disjuncts, "uncombine" them, or just leave them as they were.
A further disadvantage is that, while the basic breadthfirst search algorithm only requires that the top and previous nodes be kept, this strategy requires that all the constructed nodes be kept for re-use.
Thus, the use of this approach in practice would have major storage implications.
In summary, strategy 2 seems to have the most potential, strategy 1 also seems worth pursuing, while strategy 3 appears to have too many drawbacks.
In fact, the most productive approach may be to use a combination of the first two strategies.
In the journal paper we will include a more detailed comparison of these approaches, as well as relative timings for specific examples.
8 Combining Agents The temporal resolution algorithm contains a number of opportunities for parallelisation.
These opportunities vary in their granularity and impact.
We can examine the potential parallelism in the algorithm by deconstructing it in order to extract the fundamental operations involved, and to identify any necessary inter-dependencies between components.
Following deconstruction the fundamental operations may be composed in order to carry out the same function as the original algorithm.
However, the aim of the deconstruction is to liberate parallelism wherever possible by identifying the parts of the algorithm that may execute in parallel without affecting the overall soundness or completeness of the algorithm.
In the journal paper we derive the collection of finegrained subtasks (fundamental operations) that compose the steps of the algorithm.
Each subtask has a processing object associated with it.
We shall refer to these objects as simple agents.
We subsequently reformulate the steps of the algorithm by providing cohesive arrangements of simple agents that reflect the coarser-grained processes evident in the algorithm.
We refer to these coarser-grained objects as process agents.
Given differing arrangements of simple agents and process agents we may reflect the potential parallelisations available to us with varying task grain sizes.
We will describe the constraints and necessary synchronisation points (dependencies) in the algorithm that inhibit parallelism.
These dependencies will generally affect the soundness or the completeness of the proof method and hence must not be disregarded during the reformulation process.
We will deconstruct the steps of the algorithm with a view to identifying subtasks that may be carried out by simple agents executing in parallel.
When considering forms of parallelism it is useful to consider the activities of homogeneous and heterogeneous agents.
Heterogeneous agents carry out different tasks in a domain in parallel, whereas homogeneous agents carry out the same task but on different data.
These agent schemas may be viewed (broadly speaking) as mechanisms for describing control and data parallel activities respectively.
Finally, we note that pipelined parallelism may be employed between several of the steps, for instance new constraints produced by temporal resolution may be rewritten as soon as they are derived.
Similarly there is no reason to wait for the completion of temporal resolution before commencing non-temporal resolution operations with the rewritten rules produced previously.
9 Conclusions and Future Work Parallelism seems to provide the potential for efficiently implementing temporal reasoning methods.
While we have outlined some of the opportunities for parallelism within one particular proof method, it is important to note that, as there has been very little work in the specific area of parallel temporal theorem proving, it is not yet clear what type of parallel architecture is best suited for this  application.
It appears that some implementation of the system on different machines will be necessary to determine the "best fit".
In the longer paper we present a decomposition of the approach we describe here such that component parts of the proof process are identified to enable the reconstruction of the algorithm as an agent-based design.
9.1 Future Work Obvious future work includes the further investigation of alternative strategies for parallelising both the temporal resolution method as a whole, and the temporal resolution step within this.
The approaches outlined within this paper are currently being developed and implemented -- work will continue on these.
In the journal paper, a deeper comparison of the approaches outlined here will be presented.
This will incorporate both correctness and efficiency issues.
As several of the graph search algorithms described in SS7 have already been implemented, comparative timings will be provided.
We also hope to utilise advances in state-space search [20], in order to improve the tree-based search procedures, and parallel graph algorithms, such as [19], in order to improve the graph-based search procedures.
Finally, we note that, developing efficient parallel algorithms is a difficult task.
For example, it is sometimes the case that algorithms considered naive in the sequential case turn out to be optimal given the correct parallelisation.
Thus, it is possible that the algorithms considered unsuitable for temporal theorem-proving may be extended and, possibly, generalised, in order to develop appropriate parallel versions.
Acknowledgements This work was partially supported by a SERC Research Studentship and under SERC Research Grant GR/J48979.
References [1] M. Abadi and Z.
Manna.
Nonclausal Deduction in First-Order Temporal Logic.
ACM Journal, 37(2):279-317, April 1990.
[2] A. Aho, J. Hopcroft, and J. Ullman.
The Design and Analysis of Computer Algorithms.
Addison-Wesley, 1974.
[3] J. Allen and P. Hayes.
A Common Sense Theory of Time.
In Proceedings of the International Joint Conference on Artificial Intelligence, pages 528-531, Los Angeles, California, August 1985.
[4] U. Baron, J. de Kergommeaux, M. Hailpern, M. Ratcliffe, M. Roberts, J-Cl.
Syre, H. Westphal.
The parallel ECRC Prolog System PEPSys: An Overview and Evaluation Results.
In Future Generation Computing Systems, 1988.
[5] H. Barringer, M. Fisher, D. Gabbay, G. Gough, and R. Owens.
METATEM: A Framework for Programming in Temporal Logic.
In Proceedings of REX Workshop on Stepwise Refinement of Distributed Systems: Models, Formalisms, Correctness, Mook, Netherlands, June 1989.
(Published in Lecture Notes in Computer Science, volume 430, Springer Verlag).
[6] J.
Beer and W. Giloi.
POPE - A Parallel Operating Prolog Engine.
In Future Comp.
Systems, 3:83-92, 1987.
[7] R. Butler, I.
Foster, A. Jindal, and R. Overbeek.
A High-Performance Parallel Theorem Prover.
Lecture Notes in Computer Science, 449:649-650, 1990.
[8] W. Clocksin.
Principles of the DelPhi Parallel Inference Machine.
In Computer Journal vol.31, no.3, 1987.
[9] C. Dixon, M. Fisher, and H. Barringer.
A graphbased approach to temporal resolution.
In First International Conference on Temporal Logic (ICTL), July 1994.
[10] E. Emerson.
Temporal and Modal Logic.
In J. van Leeuwen, editor, Handbook of Theoretical Computer Science, pages 996-1072.
Elsevier, 1990.
[11] M. Fisher.
A Resolution Method for Temporal Logic.
In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence (IJCAI), Sydney, Australia, August 1991.
Morgan Kaufman.
[12] G. Gough.
Decision Procedures for Temporal Logic.
M.Sc.
Thesis, Department of Computer Science, University of Manchester, U.K., October 1984.
[13] R. Johnson.
Concurrent Theorem Proving with Tableaux and the Connection Method using Strand over a Distributed Network.
technical report 538, Department of Computer Science, Queen Mary and Westfield College, University of London, July 1991.
[14] R. Johnson.
A Blackboard Approach To Parallel Temporal Tableaux.
In Proceedings Artificial Intelligence, Methodologies, Systems, and Applications (AIMSA), World Scientific, 1994.
[15] F. Kurfess.
Parallelism in Logic.
Vieweg, 1991.
[16] E. Lusk and W. McCune.
Experiments with ROO: A Parallel Automated Deduction System In Proceedings of Parallelization in Inference Systems, International Workshop.
Springer-Verlag.
1990.
[17] Z.
Manna and A. Pnueli.
The Temporal Logic of Reactive and Concurrent Systems: Specification.
Springer-Verlag, New York, 1992.
[18] D. Plaisted and S-J.
Lee.
Inference by clause matching.
In Z. Ras and M. Zemankova, editors, Intelligent Systems, chapter 8, pages 200-235.
Ellis Horwood, Chichester, England, 1990.
[19] V. Ramachandran and J. Reif.
An Optimal Parallel Algorithm for Graph Planarity.
In Proceedings of the 30th Annual IEEE Symposium on Foundations of Computer Science, pages 282-287, 1989.
[20] V. Saletore and L. Kale.
Consistent Linear Speedups to a First Solution in Parallel State-Space Search.
In Proceedings of the Eighth National Conference on Artificial Intelligence (AAAI), pages 227-233, Boston, Massachusetts, 1990.
MIT Press.
[21] J. Schumann and R. Letz.
PARTHEO: A HighPerformance Parallel Theorem Prover.
Lecture Notes in Computer Science, 449:40-56, 1990.
[22] D. Warren.
The SRI model for OR-parallel execution of Prolog-abstract design and implementation issues.
In International Symposium on Logic Programming, pp92-102, 1987.
[23] P. Wolper.
The Tableau Method for Temporal Logic: An overview.
Logique et Analyse, 110-111:119- 136, June-Sept 1985.
Modeling Temporal Aspects of Visual and Textual Objects in Multimedia Databases Carlo Combi Laboratory of Artificial Intelligence Department of Mathematics and Computer Science University of Udine combi@dimi.uniud.it  Talk Overview  Introduction;  Motivation;  The Multimedia Temporal Data Model a basic concepts; a composing visual data; a the temporal dimension of visual data; a integrating visual and textual data; a temporal aspects of observations.
Final Outlines.
1  Introduction The integrated management of video, audio, and textual data is a need for several application domains:  geographical information systems;  medical information systems;  video/movie archive systems.
2  Introduction Modeling multimedia information at conceptual and logical levels:  Composition of visual data;  Temporal dimension of visual data;  Multimedia data as integration of visual and textual data;  Temporal aspects of textual observations related to visual data.
3  A Motivating Application Domain  Cardiac angiography is a technique adopted to study the situation of coronary vessels (coronary angiography) and of heart functionalities (left ventriculography).
The result of a cardiac angiography consists of an X-ray movie.
Diagnoses based on the content of the movie consist of identifying stenoses (i.e., reductions of vessel lumen) and problems in the movement (contraction/relaxation) of the heart.
4  A Motivating Application Domain The stored movies can be used in many different ways:  the physicians could be interested in composing a video, where different movies collected on the same patient can be viewed in sequence, to control the patientas state evolution;  it could be useful to compose other videos, on the basis of movies from several patients, showing, for didactic/research reasons, different approaches/results in a given patients population.
5  The Temporal Object-Oriented Data Model GCH-OODM (Granular Clinical History - Object- Oriented Data Model) is an object-oriented data model extended to consider and manage the valid time of information.
Basic concepts  class (type) and object: a database schema consists of a set of classes; objects are created as instances of a class.
a state (attributes) a interface (methods)  object identity; abstract data types; single inheritance; polymorphism;management of complex objects; persistence 6  GCH-OODM classes the usual types (   fi  , 	  fi     the usual type constructors (  );  ,   ,    ,        ,   ,  the class hierarchy composed by classes      !
#"$ 	  ,   , and  a  a      );           (elementary time) allows us to model a chronon;          allows us to represent a time point,  identified by the granule, i.e.
a set of contiguous chronons, containing it;    !%    a  allows us to model a generic duration, specified at arbitrary granularity; a     #"$ models a generic interval, i.e.
a  set of contiguous time points.
Notation: &('*),+ - .0/21354/6387 9 ) - .0/:1;354/6387 &*)<' - =>@?435.BA</C7 .
7  ,  The Temporal Object-Oriented Data Model  GCH-OODM relies on a three-valued logic mod!!<E eled by the class D , allowing the management of the uncertainty coming from comparisons between temporal dimensions expressed with different granularities.
Classes modeling objects having a temporal di9 	Ffi! ),D$G  	 : mension inherit from the class "$    "$$H8I   the method defined for this   "$ class, returns an object of the class  , thus allowing one to consider the valid time for that object.
8  Composing Visual Data In our multimedia data model, we define three abstraction layers for video data:  the physical layer, where we model the data sequence (stream) coming from an acquisition device;  the logical layer, where we are able to identify meaningful frame sequences into the raw stream;  the compositional layer, where we can associate frame sequences from different streams, to compose videos.
9  Composing Visual Data    ( !
(K ( the class J  allows one to store video  <M allows the storage data, while the class L of static images;    !#N* ;F   ( !
 and J  allow the user to J  & identify suitable subparts into a video stream and to refer to it;    !
the class J  allows one to create different videos by composing image sequences from different video streams.
10  Composing Visual Data The Booch Class Diagram for Visual Classes  11  Composing Visual Data  #   !#N F P OJ    !
(K  R J   T %M  & '   "$!  FTR,UH8IRS  V S   F    Ffi!  D  9 )D$G  Q    !
(K (6H8IRS  J     	T H8IRS  &   VideoClip C1  C2  C3  C4  VideoStream  12  Composing Visual Data  #   !
P F    9 Ffi!  OJ  D  ),DMG      !#N F 7 W "   (!
!FX!
 - J     R!% !
 T H8IRS   &  YZYY YZYY "$!  FT#UH8IRS  V S       Q  <!
6H8IRS  13  The Temporal Dimension of Visual Data  intrinsic time: the time we can use to identify some frames inside the frame sequence, on the basis of their distance from the first frame of the sequence;  extrinsic time: the usual valid time, possibly given at different granularities.
14  The Temporal Dimension of Visual Data  [ \ ]_^a`cbedededbf^hgji is an object of the class k,lcm;npo ; ^ ` bedededbX^hg are objects of the class kl5m;nporq;s;lut ; v \ wyxf`cbedededbxYgpz is the set of valid times of objects ^{`yb5dededb^hg The valid time of | is  | 3[ }  H 3 [    H8I,~ 3 [    H8I,~ 3 [  H8II |    |  | < , where:  [AAA{dAAyA	AAArAAhAad luAA BA AA\ A Ax AAAxAA5dAAyAAAArAABAAd luA;A hA AeAab$xAAAA v [A A dAAyA	AAArAAhAadAA5A t BA AA\ A Ax AAAx A dAAyAAAArAABAAdAA5A t hA AeAab$x A A v [pA A d nyAm ABAAd luAA AhAA\ A A AAAx A d njAm AhAad luAA ABAeAAbMx A A v [A A d njAm ABAAdAA5A t AhAA\ A A AAAx A d nyAm ABAAdAA5A t AhAeAab$x A A v [pA A d m AA<ABAAd luAA AhAA\ A AAAAA AAAAx A d m AA<ABAad lAAA ABAeAAb AA[pA A d njAm AhAad luAA ABA@A [pA A dAAyA	AaArAABAAdAA5A t AhAAAcAab$x A A v [pA A d m AA<ABAAdAA5A t AhAA\ A AAAAA AAAAx A d m AA<ABAadAAeA t ABAeAAb AA[pA A d njAm AhAadAA5A t ABA@A [pA A dAAyA	AaArAABAAd luAA AhAAAcAab$x A A v 15  .
The Temporal Dimension of Visual Data Example  frame rate = 30 fps  ^8A .A ns AjA nAmrkl5m;npoAAA  ApA n AeAVABA returns the videostream [	ALA 8^ A .
AyA	AAAAA njAm A$?
AAeAV n A	AhA returns ]AAL$A,b{A8AS(ASSi 	[ ALA .A" A slcm luA A n A A" A s ABA returns A$?AAASAS{ATAAZrAZAZ A sAAZpAdegAa5A,AlA$?AS{ASCA'8Al A8A ^eAs .A ns AjA nAmrkl5m;npoAAA  ApA n AeAVABA returns the videostream [	ALjAs e^ As .
AyA	AAAAA njAm A$?
AAeAV n A	AhA returns ]ZAbfASTAri 	[ ALjAs .A" A slcm luA A n A A" A s ABA returns A$?ASjASAVAacA, A8AAAs ASAsAZAAAZAZ A sAAZAdegAa5A,OAt#AacA, 16  The Temporal Dimension of Visual Data Example  ^8A .A" A s l5m luA A A$?AAASAS{AT AzAlAZAAAZAZ A$?ASjASAAz d A'AAA, A8A  n A A" A s AhA returns A sAAZAdegAa5A,OAt	Al8AlAVAtAl dAzA"8A"	b AZrAZAZ A sAAZpAdegAauAz At	Al8AlAVAtAl Azd A" 	A, A  ^eAs .A" A sl5m luA A n A A" A s AhA returns A$?ASjASAVAa d A, A"AA8A As ASAAz:AZAAAZ;AZ A sAAZAdeg	Aa5A,AVAtAauAz pAt Az A"	d A,8A b AZrAZAZ A sAAZpAdegAa5A,AAtAacA,AVAtAz 	A" d A, A" A [	AL .A" A slcm lAA A n A A" A s ABA returns A$?AAASAS{ATAAzAlAZAAAZAZ A sAAZAdegAa5A,OAtAl8AlAAtAl dAzA" A, b AZAAAZAZ A sAAZAdegAa5A,AVAtAacA,AVAtpAz8Az d AZ8AZA As ASAAz:AZAAAZ;AZ A sAAZAdeg	Aa5A,AVAtAauAz AtpAz A"	d A,8A b AZrAZAZ A sAAZpAdegAauAz AtAl8AlAVAtAz d AZAAAZ	A A$?ASjASAAzAAz d A'rA, AAAA;b AaTA8AAAl AV luAAAz d A'8A' A8A A 17  Integrating Visual and Textual Data: Observations An observation is any kind of textual information related to a visual object.
# #"M  !
P F   9 Ffi!     )D  (D  ),D$G   Q fi    fi ;F  !
AH8IRS        R   H8IRS 7   ),DMG   ),D$G T !R !#fiH  IRS ' ),D$G  (  ~ !R 7 !
!R H8IRS ),D$G  ' D),DMG'  ( -   !M~  - T M 7 7 A T   ( !fiH8IRS J    & ' 	J   - <M 7 R  T<* H8IRS   L  L   -   #"$ 7 fi!%MN!
"M<A  #"$*H8IRS  L  V S  #   !
P F    9 Ffi!  OJ  D  ),DMG      !#N F 7 W "   (!
!FX!
 J     R!% !
 T H8IRS   &  YZYY YZYY "$!  FT#UH8IRS  9  !
D 9 fiH8IRS V S      Q  <!
6H8IRS  18  Integrating Visual and Textual Data: Observations Example  13yA	/AA#1.A1 -A ?A1A/AA;A .
A T  J     !XH8I returns V I H ~ ~ ~ ~ |6A QfiAAAUA AAAAWA AAAAAA AU A AWA and V H ~ ~ I |*A QfiAAAUA AAAAWA  19  Temporal Aspects of Observations Dynamic vs. static properties of a video subsequence.
20  Temporal Aspects of Observations Types of observation in a multimedia database  concatenable observations; if a concatenable observation is valid on the consecutive frame intervals interval  ]AzxAbAAri .
]AxAb0Api  and  ]AAA AbAAri , it is valid on the frame  Example: aperfusion of the contrast agent through the coronary vesselsa.
point-upward observations; if a point-upward observation is valid on the consecutive frame intervals  ]AzxAbAxYi  and  ]AxA A,bAxA Ari , deded , ]AzxA AbAxA A(i  A consecutive frames), interval ]AzxAbAx(A A(i .
(i.e., on  it is valid on the frame  Example:  21  Temporal Aspects of Observations Types of observation in a multimedia database  weakly-upward-hereditary observations; given a set of  ]AzxAAA bAxAAAAi  A  (possibly intersecting) frame intervals  over which a weakly-upward-hereditary observa-  tion holds, the observation holds also on the intervals obtained as union of the  A  frame intervals  ]AxAA0bAxAAAAVi .
Example: aperfusion of the contrast agent through the coronary vesselsa is weakly-upward too.
downward-hereditary observations; a downward hereditary observation holding on a frame  ]AzxAAA bAxAAAAi holds on any frame interval ]ALA5A0bAAeAAAi , where x AAA$?
A A(AL A AAAA$?
x AA .
interval  Example: athe contrast agent highlights less than half of the left coronary treea.
22  Temporal Aspects of Observations Types of observation in a multimedia database  liquid observations; those observations which are both downward and pointupward hereditary, are termed as liquid.
Example: apresence of a stenosisa.
solid observations;  ]AxAABbAxAAAAi cannot hold on any frame interval ]ALA A bAA AA i , for which AAx A A$?
A5A AL A5A A$?
xAAAAATAS AAxAA A$?
AeAA AL AeAA A$?
xAAAAA .
a solid observation holding on a frame interval  Example: aexactly a cardiac cycle, from the systole (empting phase) to the diastole (filling phase)a.
23  Temporal Aspects of Observations Types of observation in a multimedia database  gestalt observations;  ]Ax A bAx AA i cannot hold on any frame interval ]ALA5A0bAAeAAAi , for which AAxAA A$?
A A AL A AA A$?
x AA AWAS AAA A A$?
x A AL x AA A$?
A AA A .
a gestalt observation holding on a frame interval  Example: atwo cardiac cyclesa.
disjointed observations; if a disjointed observation is associated to a frame inter-  ]AzxAABbAxAAAAVi , it cannot be associated to any interval ]ALA5A0bAAeAAAi such that: A A A$?
x A A$?
A AA A$?
x AA AS x A A$?
A A A$?
x AA A$?
A AA .
val  Example:  24  Temporal Aspects of Observations  # #"M  !
P F   9 Ffi!    )D  (D  ),D$G  Q fi   fi ;F  !
AH8IRS      S YZYYYYZ YZYYYYZ S !
(!E    6H   !M~ 	T MIRS D  J  & ' !
(!E   "$  H   (!M~  *IRS DV   J  & ' S  ALuAAASS5AA"8ALAxASAL -AS AT ASSjALyASS5A^eASS8d l A Al A ([A , [124,128]) returns AspAaA n ALuAAASS5AA"8ALAxASAL -ASAT jASS ALyASS5A^eASS8d l A A" A s lcm ([	AL , [124,128]) returns AspAAA n AASAZ2A" -A^ A AAT A xAA^ -A^ AZ^eAZAdegASS{ALrd l AASAZ2A" -^AA ATAA Ax A^ -A^ AZ^eAZAdegASS{ALrd l AASAZ2A" -^AA ATAA Ax A^ -A^ AZ^eAZAdegASS{ALrd l  A Al A ([ A , [100,110]) returns AspAAA n A A" A scl m ([(A , [100,110]) returns A$?
A s A n A A" A slcm (([ A , [100,154]) returns AspAAA n 25  Temporal Aspects of Observations  A"  is an object of the class  v \ wcx ` bAxAA,AAbedededbAxYg8z  AScAa A n A A" {A A al oyA  ;  the set of time intervals related to A" by the  associations between A" and several frame intervals of different videos.
A is H 3 A"    H8I%~ 3 A"   H8I,~ 3 A"  H8I	I |    |  |  , where:  The valid time of  | 3A" }  [pAA'AludAAyAAAArAABAAd luA;A AhAA\ A Ax AAAxAA5dAAyA	AAArAAhAad luAA ABAeAAbMxAAAA v [A Al dAAcA	AAArAABAadAAeA t ABAA\ A Ax AAAxAA5dAAyA	AaArAABAAdAA5A t AhAeAab$xAAAA v [pA Al d njA;m AhAad luAA ABAA\ A A AAAx A d njAm ABAad lAAA ABAeAAbMx A A v [A Al d nyAm ABAAdAA5A t AhAA\ A A AAAxAAcd njA;m AhAadAA5A t ABAeAab$xAAAA v [pA Al d m A;A<AhAad luAA ABAA\ AA[pA Al d njAm ABAad lAAA [pA Al d m A;A<AhAadAA5A t ABAA\ AA[pA Al d njAm ABAadAAeA t  A AAAAA AAAAx A d m AA<ABAAd luA;A AhAeAab ABA@A [pA Al dAAyA	AAArAAhAadAA5A t ABAAAcAAbXxAAAA v A AAAAA AAAAx A d m AA<ABAAdAA5A t AhAeAab ABA@A [pA Al dAAyA	AAArAAhAad luAA ABAAAcAAbXxAAAA v 26  Temporal Aspects of Observations Example  13yA	/AA#1	.A1 -A ?A1%A	/AA;A : A$?AAASAS{ATAAzAlAZAAAZAZ A sAAZAdegAa5A,OAtAl8AlAAtAl d AZ8AZ b AZAAAZAZ A sAAZAdegAauAz AtAl8AlAVAtAl d AZAAAZ	A A$?ASjASAA, d AZ AAAA (from clip ^8A in videos [A and [	AL ); A$?AAASAS{ATAAzAlAZAAAZAAl A;As AacAlpAdegAa5A,OAtA,8AlAVAt#Aa dAzA" A, b AZAAAZAAl AAs AacAlpAdeg	AacA,OAtA,AaAAtAa dAzA" A'A [(A ); A$?ASjASAAl d AlAAA, A;A8As A (from clip ^5AL ind video A$?AAASAS{ATAAzAlAZAAAZAAl AacAlpAdegAa5A,OAtA,8AlAVAt	A' AzAZ b AZAAAZAAl AAs AacAlpAdeg	AacA,OAtA,AaAAtA' d Az;AZ	A A$?ASjASAAl d AZ A"AA8A (from clip ^5AL in video [(A ).
Intervals related to  ALuAAASS5AA"8ALAxASAL -AS AT ASSjALyASS5A^eASS .A" A sl5m luA A n A A" A s ABA returns A$?AAASAS{ATAAzAlAZAAAZAZ A sAAZAdegAa5A,OAtAl8AlAAtAl d AZ8AZ b AZAAAZAZ A sAAZAdegAauAz AtAl8AlAVAtAl d AZAAAZ	A As ASAAz:AZAAAZAAl A;As AacAl8AdegAacA,AVAtA,pAlAAt;A, d AzA, b AZrAZAAl A;As Aa5AlpAdegAacA,OAtA,,AaAAtA, d Az8AzA 27  Temporal Aspects of Observations  .
~.
~  ~.
V  Q A AL  A of intervals reGiven the set A, As ~rAz A of a video, given the lated to the frame interval AAtAs ~ 3 V 3 ~ 3 ~ set Q| A | AL  | A of valid times of A" temporal objects involved in the considered observation, we can associate the observation to the frame interval ~AAz AAzAs A of the video, only if Az  3|  H  As A ~  ~ A" I Az .
AZ .
AZ  	 'L  H | 3 I returns 9    Hfi As  A  ~  ~ / I  Example  13yA	/AA#1.A1 -A ?A1A/AA;A  R    ),DMG  T 8H I returns Q 13yA	/ A V 13yA	/AA#1.A1 -A ?
A1A/AA;A  "$       #"$$H8I  H 1;3cA	/  "$     #"$$H8II 	 'L  A   9    must return  .
28  Final Outlines  Composition of temporal visual data.
a three-layer approach to compose videos; a valid time of visual objects at different granularities and/or with indeterminacy.
Integration of temporal visual and textual data.
a taxonomy for observations based on their temporal features; a valid time of observations and constraints with valid times of other involved database objects.
29    REFERENCES    C. Combi, G. Cucchi, and F. Pinciroli, aApplying ObjectOriented Technologies in Modeling and Querying TemporallyOriented Clinical Databases Dealing with Temporal Granularity and Indeterminacya, IEEE Transactions on Information Technology in Biomedicine, 1997, 1(2), pp.
100a 127.
  J.D.N.
Dionisio and A.F.
Cardenas, aA Unified Data Model for Representing Multimedia, Timeline, and Simulation Dataa, IEEE Transactions on Knowledge and Data Engineering, 1998, 5, pp.
746a767.
  H. Jiang and H.K.
Elmagarmid, aSpatial and Temporal Content-Based Access to Hypervideo Databasesa, The VLDB Journal, 1998, 7, pp.
226a238.
J.Z.
Li, I.A.
Goralwalla, M.T.
OEzsu, and D. Szafron, aModeling Video Temporal Relationship in an Object Database Management Systema, in IS&T/SPIE International Symposium on Electronic Imaging: Multimedia Computing and Networking, San Jose, CA, February 1997, pp.
80a91.
30
Reasoning about generalized intervals: Horn representability and tractability Philippe Balbiani, Jean-FrancESSois Condottay, GeErard Ligozatz  Abstract This paper organizes the topologic forms of the possible relations between generalized intervals.
Working out generalized interval algebra on the pattern of point algebra and interval algebra, it introduces the concept of Horn representability just as the one of convexity.
It gives the class of Horn representable relations a simple characterization based on the concept of strong preconvexity.
Adapting the propagation techniques designed to solve the networks of constraints between points or between intervals, it shows that the issue of consistency of a Horn representable generalized interval network can be solved in polynomial time by means of the weak path-consistency algorithm, a new incomplete algorithm for computing a minimal set of temporal constraints.
1 INTRODUCTION It is hard to overemphasize the importance for computer science and artificial intelligence of the development of reasoning systems that are concerned with temporal information.
The thing is that the nature of time raises grave difficulties for those who take on the matter of its representation.
Without doubt, the model of the points designed by Vilain and Kautz [13] and the model of the intervals elaborated by Allen [1] are the better known models for reasoning about time.
In these models, temporal information is represented by a network of constraints between a finite number of variables.
An important matter is deciding consistency of a network.
Concerning points, Ladkin and Maddux [6] prove that the issue of consistency of a point network can be solved in polynomial time by means of the path-consistency algorithm.
Relating to intervals, Vilain and Kautz [13] demonstrate that deciding consistency of an interval network becomes NP-complete.
Laboratoire dainformatique de Paris-Nord, 99 avenue Jean-Baptiste CleEment, F-93430 Villetaneuse y Institut de recherche en informatique de Toulouse, 118 route de Narbonne, F-31062 Toulouse Cedex 4 z Laboratoire dainformatique pour la meEcanique et les sciences de laingeEnieur, BP 133, F-91403 Orsay  Therefore, the question of characterizing tractable subclasses of interval algebra has been considered.
Nebel and BuErckert [10] give a definitive answer to the question of which subclasses among those which contain base relations are tractable.
To be more precise, the subclass of Horn representable relations is the unique maximal tractable subclass having this property.
Moreover, deciding consistency can be accomplished by using the path-consistency algorithm.
Horn representability is a syntactic concept, in view of the fact that Horn representable relations can be described by Horn clauses in a suitable language.
Ligozat [8] produces a simple characterization of the same class in terms of preconvex relations.
Preconvexity is a geometric concept, for the simple reason that preconvex relations can be roughly described as convex relations with some lower dimensional base relations taken out.
Working out generalized interval algebra on the pattern of point algebra and interval algebra, Ligozat [7] organizes the topologic forms of the possible relations between generalized intervals.
A problem is that the coincidence between the syntactic concept of Horn representability and the geometric concept of preconvexity does not hold any longer, as Ligozat [8] notices in the context of generalized interval algebra, because the subclass of Horn representable relations is a proper subset of the set of all preconvex relations.
A further complication is that, as Balbiani, Condotta and FarinEas del Cerro [3] remark in the context of rectangle algebra, the set of all preconvex relations is not a subclass in the usual sense, given that it is not closed for intersection.
This leads them to define a restricted geometric notion, the concept of strong preconvexity, which has this closure property.
In the context of generalized interval algebra as well, it is interesting to consider the tractability issues both from the syntactic point of view and the geometric one.
A primary goal of this paper is to give the class of Horn representable relations between generalized intervals a simple characterization in terms of strongly preconvex relations.
An outcome of this characterization is that it allows to demonstrate that the issue of consistency of a Horn representable generalized interval network can be solved in polynomial time by means of the weak path-consistency algorithm, a new incomplete algorithm for computing a minimal set of temporal constraints.
Successive sections are arranged along the following lines.
Section 2 introduces the relational algebra of generalized intervals.
We devote the whole section 3 to the syntactic concept of Horn representability.
In sections 4, 5 and 6, we identify the geometric concepts of convexity, weak preconvexity and strong preconvexity.
Section 7 focuses on the issue of consistency of a generalized interval network.
2 GENERALIZED ALGEBRA Given a model of time consisting of the totally ordered set of all rational numbers, a generalized interval is a list of p rational numbers, with the first number less than the second, the second less than the third, etc.
We will use x, y, z , etc, for these, assuming for any list x of p rational numbers, the first number is denoted by x 1 , the second by x2, etc.
Such lists of p rational numbers are also sometimes called p-intervals.
We want  to formalize the notion of binary relation between a p-interval and a q-interval for any p q 1.
To keep things concrete, we will confine ourselves to results about the notion of binary relation between two p-intervals for some p 1.
Extending these to the remaining cases is a simple matter.
In order to formalize the position of x with respect to y we have to decompose the set of all rational numbers into subsets.
If we define y0 as ;1 and yp+1 as +1 then the numbers y1 , : : :, yp clearly define a partition of this set into 2 fi p + 1 zones numbered from 0 to 2 fi p such that: - For all i 2 f0 : : :  pg, zone 2 fi i is ]yi yi+1 ; - For all i 2 f1 : : :  pg, zone 2 fi i ; 1 is fyi g.  x1  x2  x3  Figure 1: A 3-interval x = (x 1  x2 x3).
Obviously, each rational number belongs to exactly one zone.
This shows that the position of x with respect to y is a sequence of p zones which specifies for each i 2 f1 : : :  pg which zone the number xi belongs to.
Let Q be the set of all sequences of p zones.
We will use a, b, c, etc, for these, assuming for any sequence a of p zones, the first zone is denoted by a1 , the second by a2 , etc.
If we define a0 as 0 and ap+1 as 2 fi p then: -  is a position between generalized intervals iff, for all i 2 f1 : : :  pg, ai 2 a a b of integers, a b]] is the largest integer interval with even endpoints and contained in a b].
a   i;1 i+1]], assuming for any pair a  y1 zone 0  y2 zone 2  zone 1  zone 3  y3 zone 4  zone 6 zone 5  Figure 2: The 7 zones defined by a 3-interval y  =(  y1  y2  y3  ).
Let P be the set of all positions.
Positions are also called basic relations.
They constitute the exhaustive list of the possible relations between generalized intervals.
For example, if x1 2]y0  y1, x2 2]y0  y1 and x3 2]y2 y3  then the position of x with respect to y is the sequence (0 0 4) and if x1 2]y0 y1 , x2 = y1 and x3 2]y2 y3  then the position of x with respect to y is the sequence (0 1 4).
In order to represent indefinite information, we allow the binary relation between two generalized intervals to be any subset of the set of all basic relations.
We will use fi,  ,  , etc, for these.
In the relational approach to temporal reasoning, the operations of inverse and composition play an important role.
The inverse of a, denoted by a;1, is the position b such that, for all i 2 f1 : : :  pg:  - For all j - For all j  2 f0 2 f1   : : : p  : : : p  g, if 2 fi ; 1 2  j j +1]] then i = 2 fi g, if 2 fi ; 1 = j then i = 2 fi ; 1. i  a a  i  a  b  b  j  ;  j  The composition of a and b, denoted by a b, is the set of all positions c such that, for all i 2 f1 : : :  pg: - For all j - For all j  2 f0 2 f1   : : : p  : : : p  g, if i = 2 fi then i 2  j g, if i = 2 fi ; 1 then i = a  j  a  j  c  j +1]];  b b  c  j.  b  For instance, the inverses of (0 0 4) and (0 1 4) are the positions (4 4 6) and (3 4 6) whereas the composition of (0 0 4) and (0 1 4) is the set f(0 0 2) (0 0 3) (0 0 4)g of positions.
Since binary relations between generalized intervals are sets of basic relations, the operations of inverse and composition are extended as follows.
The inverse of fi, denoted by fi;1 , is fa;1 : a 2 fig.
The composition of fi and  , denoted by fi  , is fa b : a 2 fi&b 2  g. This brings us to the question of whether these definitions capture the intended meaning of the operations involved.
Let xfiy mean that the position of x with respect to y belongs to fi.
Ligozat [7] shows that the operations of inverse and composition have the following important properties:  S  ;1y iff yfix;  -  xfi  -  xfi y    iff there is a generalized interval z such that xfiz and zy.
This proves a simple but fundamental result: - The algebra (2P   \ ;  P ;1    f(1 : : :  2 fi p ; 1)g) is a relational algebra.
3 HORN REPRESENTABILITY Horn representable relations correspond to particular sets of clauses.
Clauses are built up from p variables u1, : : :, up and p variables v1, : : :, vp using the arithmetical symbols <, =, >, 	, 6= and .
A literal is any expression of the form ui<vj , where i j 2 f1 : : :  pg and < is an arithmetical symbol.
A set of literals is a Horn clause iff it contains zero or exactly one positive literal, assuming for any i j 2 f1 : : :  pg, the literals ui < vj , ui = vj , ui > vj , ui 	 vj and ui vj are positive and the literal ui 6= vj is negative.
For example, the clauses fu2 = v1  u3 6= v2g and fu2 = v1  u3 6= v3 g are Horn clauses whereas the clauses fu2 < v1  u3 6= v2  u3 = v3 g and fu2 > v1  u3 = v2  u3 6= v3 g are not Horn clauses.
A definite clause contains exactly one positive literal and zero or more negative literals.
A positive unit clause is a definite clause containing no negative literal.
Nebel and BuErckert [10] consider only Horn clauses using the arithmetical symbols =, 	, 6= and .
It is straightforward to prove that every Horn clause using the arithmetical symbols <, =, >, 	, 6= and is equivalent to a couple of Horn clauses using the arithmetical symbols =, 	, 6= and .
To define Horn representable relations we need to evaluate variables occurring in the  given literals of a clause.
We shall say that a validates the literal ui<vj iff ai <(2 fi j ; 1).
For instance, (0 1 3) and (0 1 5) validate the literal u2 = v1 , seeing that 1 = (2 fi 1 ; 1).
In this spirit, a basic relation validates a clause iff it validates at least one literal of the clause.
For example, (0 1 3) and (0 1 5) validate the clauses fu2 = v1  u3 6= v2 g and fu2 = v1  u3 6= v3 g. Then we can define the Horn representable relations as follows: -  is Horn representable iff there is a set S of Horn clauses such that exactly the basic relations of fi validate every clause of S .
fi  One says that S is a Horn representation of fi.
To illustrate the truth of this, the binary relations f(0 0 4) (0 1 3) (0 1 4) (0 1 5)g and f(0 1 3) (0 1 4) (0 1 5) (0 2 4)g are Horn representable or to be more precise, the Horn clauses fu1 < v1g, fu2 	 v1 g, fu3 v2 g, fu3 	 v3 g, fu2 = v1  u3 6= v2 g and fu2 = v1  u3 6= v3 g constitute a Horn representation of the former binary relation whereas the Horn clauses fu1 < v1 g, fu2 v1 g, fu2 < v2 g, fu3 v2 g, fu3 	 v3 g, fu2 = v1  u3 6= v2 g, fu2 = v1  u3 6= v3 g constitute a Horn representation of the latter binary relation.
It is not necessarily the case that the line of reasoning suggested by Nebel and BuErckert [10] within the context of Horn representable relations between intervals applies to Horn representable relations between generalized intervals when p 3.
In particular, although the reader may easily verify that the set of all Horn representable relations between generalized intervals is closed for intersection and inverse, there is no evidence that the set of all Horn representable relations between generalized intervals is closed for composition when p 3.
Therefore, we are not in a position to give any sort of proof that the set of all Horn representable relations between generalized intervals constitutes a subclass of the generalized interval algebra when p 3.
4 CONVEXITY To define the set of all convex relations, it is helpful to first arrange in ascending order the sequences of p zones.
Let a  b mean that ai 	 bi for all i 2 f1 : : :  pg.
For instance, (0 0 3)  (0 1 5) and (0 1 3)  (0 2 5).
As a product of chains, it is easily shown that (Q ) is a distributive lattice.
(P  ) is also a distributive lattice, because (P  ) is a sublattice of (Q ).
The interval bounded by a and b, denoted by a b], is the binary relation fc: a  c and c  bg.
This leads us to make the following definition: -  fi  is convex iff there are positions a b such that fi = a b].
To illustrate the truth of this, the binary relation f(0 1 3) (0 1 4) (0 1 5)g is convex.
Clearly, convex relations are Horn representable or to be more precise, if fi is convex then there is a set S of positive unit clauses such that S is a Horn representation of fi.
For instance, the positive unit clauses fu1 < v1 g, fu2 	 v1 g, fu3 v2 g, fu3 	 v3 g constitute a Horn representation of the interval bounded by (0 0 3) and (0 1 5) whereas the positive unit clauses fu1 < v1g, fu2 v1g, fu2 < v2 g,  (0,6,6)  (6,6,6)  (4,4,4) (0,0,6)  (2,2,2)  (0,0,0)  Figure 3: The lattice (P  ) for p = 3.  f  u3  v2  g, f 3 	 3 g constitute a Horn representation of the interval bounded by u  v  (0 1 3) and (0 2 5).
In general, the converse is false, Horn representable relations need not be convex.
For a counterexample, take the case of the Horn representable relations f(0 0 4) (0 1 3) (0 1 4) (0 1 5)g and f(0 1 3) (0 1 4) (0 1 5) (0 2 4)g. Evidently, the set of all convex relations is closed for intersection.
In particular, the set of all convex relations containing fi contains a least element, denoted by I (fi), the convex closure of fi.
Ligozat [7] demonstrates that the convex closure has the following important properties:         ( ;1 ) = ( );1 ; (  ) = ( ) ( ).
-  I fi  I fi  -  I fi   I fi  I   The conclusion can be summarized as follows: the set of all convex relations is closed for inverse and composition.
Therefore, the set of all convex relations constitutes a subclass of the generalized interval algebra: the convex class.
5 WEAK PRECONVEXITY To define the set of all weakly preconvex relations, we have to bring in the operations of topologic closure and dimension as follows.
The topologic closure of a, denoted by C (a), is the set of all positions b such that, for all i 2 f1 : : :  pg, either b i = ai  or j bi ; ai j = 1 and bi is odd.
For instance, the topologic closure of (0 1 4) is the binary relation f(0 1 3) (0 1 4) (0 1 5)g. The dimension of a, denoted by dim(a), is p ; fai mod 2: i 2 f1 : : :  pgg.
For example, the dimension of (0 1 4) is 2.
Seeing that binary relations between generalized intervals are sets of basic relations, we extend the operations of topologic closure and dimension as follows.
The topologic closure of fi, denoted by C (fi), is fC (a): a 2 fig.
The dimension of fi, denoted by dim(fi), is supfdim(a): a 2 fig.
Ligozat [9] proves that the following conditions are equivalent:  S  ( ) is convex; ( )  ( ); dim( ( ) n ) dim( ).
-  C fi  -  I fi  -  C fi  I fi  fi  <  fi  This justifies the role played by topologic closure in the following definition: -  fi  is weakly preconvex iff C (fi) is convex iff I (fi)  C (fi) iff dim(I (fi) n fi) <  dim( ).
fi  For instance, the binary relations f(0 0 4) (0 1 3) (0 1 5)g and f(0 1 3) (0 1 5) (0 2 4)g are weakly preconvex or more exactly, the topologic closure of the former binary relation is the interval bounded by (0 0 4) and (0 1 5) whereas the topologic closure of the latter binary relation is the interval bounded by (0 1 3) and (1 3 5).
It                                                 is clear that convex relations are weakly preconvex.
This brings us to the question of whether Horn representable relations are weakly preconvex.
Suppose fi is Horn representable, we show it is weakly preconvex.
By our definition of Horn representability, we know that there is a set S of Horn clauses such that S is a Horn representation of fi.
Let S  consist of all positive unit clauses of S .
With no loss of generality, suppose, for all i j 2 f1 : : :  pg, if ui 6= vj appears in some clause of S n S then ui = vj is not a consequence of S  .
Let  be the binary relation which elements are exactly the basic relations validating every clause of S  .
Since S   S , then fi   .
Since S  is a set of positive unit clauses, then  is convex.
It follows that I (fi)   .
Let us show that   C (fi).
Consider the basic relation a of  .
It follows that a validates every clause of S  .
Then we can define a basic relation b of fi such that a 2 C (b) as follows.
Let i 2 f1 : : :  pg.
If ai is even then there is j 2 f0 : : :  pg such that ai = 2 fi j and let bi = 2 fi j .
Otherwise ai is odd and there is j 2 f1 : : :  pg such that ai = 2 fi j ; 1.
If ui = vj is a consequence of S then let bi = 2 fi j ; 1.
Otherwise ui 	 vj is not a consequence of S or ui vj is not a consequence of S  .
In the former case let bi = 2 fi j whereas in the latter case let bi = 2 fi (j ; 1).
The reader may easily verify that b is a basic relation of fi such that a 2 C (b).
This gives us the following result:   C (fi).
It follows that I (fi)  C (fi), hence fi is weakly preconvex.
From all this it follows that: Theorem 1 If fi is Horn representable then fi is weakly preconvex.
Although Ligozat [8] shows that weakly preconvex relations between intervals are Horn representable, weakly preconvex relations between generalized intervals need not be Horn representable when p 3, unfortunately.
Take, for example, the case of the weakly preconvex relations f(0 0 4) (0 1 3) (0 1 5)g and f(0 1 3) (0 1 5) (0 2 4)g. Ligozat [9] demonstrates that the topologic closure has the following important properties:  ( ;1)  ( );1; (  )  ( ) ( ).
-  C fi  C fi  -  C fi   C fi  C   The interesting result is: the set of all weakly preconvex relations is closed for inverse and composition.
Although Ligozat [8] proves that the set of all weakly preconvex relations between intervals is closed for intersection, the reader may easily verify that the set of all weakly preconvex relations between generalized intervals is not closed for intersection when p 3.
Consider, for instance, the weakly preconvex relations f(0 0 4) (0 1 3) (0 1 5)g and f(0 1 3) (0 1 5) (0 2 4)g. Therefore, the set of all weakly preconvex relations between generalized intervals does not constitutes a subclass of the generalized interval algebra when p 3.
6 STRONG PRECONVEXITY Assume p 3.
The trouble with the set of all weakly preconvex relations is that it is not closed for intersection with convex relations.
One has only to consider the weakly preconvex relations f(0 0 4) (0 1 3) (0 1 5)g and f(0 1 3) (0 1 5) (0 2 4)g and the convex relation f(0 1 3) (0 1 4) (0 1 5)g. This justifies the role played by intersection with convex relations in the following definition: -  is strongly preconvex iff, for all binary relation  , if  is convex then fi \  is weakly preconvex.
fi  To illustrate the truth of this, the binary relations f(0 0 4) (0 1 3) (0 1 4) (0 1 5)g and f(0 1 3) (0 1 4) (0 1 5) (0 2 4)g are strongly preconvex.
Clearly, strongly preconvex relations are weakly preconvex.
Let us see if there is any connection between Horn representability and strong preconvexity.
Suppose fi is Horn representable, we demonstrate it is strongly preconvex.
By our definition of Horn representability, we know that there is a set S (fi) of Horn clauses such that S (fi) is a Horn representation of fi.
If fi is not strongly preconvex then there is a binary relation  such that  is convex and fi \  is not weakly preconvex.
Since  is convex, then it is Horn representable.
In particular, there is a set S ( ) of positive unit clauses such that S ( ) is a Horn representation of  .
Furthermore, the reader may easily verify that S (fi)  S ( ) is a Horn representation of fi \  .
By theorem 1, fi \  is weakly preconvex, and this is impossible.
These considerations prove: Theorem 2 If fi is Horn representable then fi is strongly preconvex.
Now suppose fi is strongly preconvex, we prove it is Horn representable.
Since I (fi) is convex, then it is Horn representable.
In particular, there is a set S (I (fi)) of positive unit clauses such that S (I (fi)) is a Horn representation of I (fi).
Consider the basic relation a of I (fi).
Suppose a 62 fi.
Let fia = fb: for all i 2 f1 : : :  pg, if a i is odd then bi = ai g. Obviously, a 2 fia .
Furthermore, the reader may easily verify that fia is convex.
It follows that fi a is Horn representable.
What is more, there is a set S (fia ) = ffui1 = vj1 g : : :  fuiM = vjM gg of positive unit clauses using the arithmetical symbol = such that S (fia ) is a Horn representation of fia .
Let a = fi \ fia .
Since fi is strongly preconvex, then a is weakly preconvex.
It follows that I (a )  C (a ).
Let us demonstrate that a 62 I (a ).
Suppose a 2 I (a ), we derive a contradiction.
Since I (a )  C (a ), then a 2 C (a ) and there is a basic relation b of a such that a 2 C (b).
Since b 2 a , then b 2 fi and b 2 fia .
It follows that b 6= a and, for all i 2 f1 : : :  pg, if ai is odd then bi = ai .
Since a 2 C (b), then, for all i 2 f1 : : :  pg, either ai = bi or j ai ; bi j = 1 and ai is odd, hence if ai is even then bi = ai .
It follows that b = a, a contradiction.
This gives us the following result: a 62 I ( a ).
Since I (a ) is convex, then I (a ) is Horn representable.
To be more precise, there is a nonempty set S (I (a )) = ffuk1 <l1 vm1 g : : :  fukN <lN vmN gg of positive unit clauses such that S (I (a )) is a Horn representation of I (a ).
Let a = fia n I (a ) and 	a = I (fi) n a .
It follows that 	a is Horn representable.
Ultimately, then, the set S (	a ) = S (I (fi))  ffui1 6=  j  iM  v 1 : : : u  6= jM v   u1  <l1  m1 g : : :  fui1  v  6=  j  iM  v 1 : : : u  6= jM SkN <lN mN gg v  u  v  of definite clauses is a Horn representation of 	a .
All in all, let S = fS (	a ): a 2 I (fi) n fig.
The reader may easily verify that exactly the basic relations of fi validate every clause of S .
It follows that fi is Horn representable.
Hence we have:  Theorem 3 If fi is strongly preconvex then fi is Horn representable.
7 GENERALIZED NETWORKS Assume p 3.
A generalized interval network is a structure of the form (n M ) where n 1 and M is a square n fi n matrix with entries in 2P .
Hence M isa function assigning, for all i j 2 f1 : : :  ng, a subset M (i j ) of P , i.e.
a binary relation.
A tuple (x(1) : : :  x(n)) of generalized intervals is called a (maximal) solution of (n M ) iff, for all i j 2 f1 : : :  ng, there is a basic relation a of (maximal dimension in) M (i j ) such that x(i) a x(j ).
(n M ) is (maximally) consistent iff it possesses a (maximal) solution.
In what follows we assume that all our generalized interval networks satisfy the following conditions: - For all i 2 f1 : : :  ng, M (i i) = f(1 : : :  2 fi p ; 1)g;  - For all i j 2 f1 : : :  ng, M (i j ) = M (j i);1.
It is a well-known fact that by applying the following algorithm a the path-consistency algorithm a we obtain in polynomial time an equivalent generalized interval network:  - Successively replace, for all pairwise distinct i j k 2 f1 : : :  ng, the constraints M (i k ) and M (k i) by the constraints M (i k ) \ (M (i j ) M (j k )) and M (k i) \ (M (k j ) M (j i)).
We make use of this in the following definition: -  (  ) is path-consistent iff, for all pairwise distinct )  ( ) ( ) and ( )  ( ) ( ).
n M  k  i j k  M i j  M j k  M k i  M k j  2 f1   : : : n  g, (  M i  M j i  An important matter is deciding consistency of a generalized interval network.
It would be naive to suppose that constraints between generalized intervals can be expressed in terms of constraints between intervals, for the simple reason that although Ligozat [9] shows that if a weakly preconvex generalized interval network is path-consistent then either it contains the empty constraint or it is maximally consistent, the issue of consistency of a weakly preconvex generalized interval network is NP-complete.
The fact of the matter is that the issue of consistency of an interval network is polynomial-time reducible to the issue of consistency of a weakly preconvex generalized interval network.
Consider the interval network (n M ).
Then we can define the weakly preconvex generalized interval network (n0  M 0) as follows.
Let n0 = n fi (n + 1).
For all i j 2 f1 : : :  ng, let: -  M  -  M  0 (i j ) = f(a1  a2 5 : : :  2 fi p ; 1): (a1 a2) 2 I (M (i j ))g;  0 (i n + i + (j ; 1) fi n) = f(a1  a2 5 : : :  2 fi p ; 1): (a1  a2) 2 M (i j )g   f( 1  2 fi ): 0 ( + + ( ; 1) fi  b  b2   -  M  n  6   : : :  i  j  p  2 f0 2 4g and 1 	 2g; ) = f(1 3 5 2 fi ; 1)g.  b1  b2      n j      The reader may easily verify that this goes to show that:  (0  n M  b   : : :  b  p  0) is consistent iff (n M ) is consistent.
All  Theorem 4 The issue of consistency of a weakly preconvex generalized interval network is NP-complete.
This polynomial-time reducibility of the issue of consistency of an interval network to the issue of consistency of a weakly preconvex generalized interval network serves to illustrate the role played by intersection with convex relations in the following definition: -  (  ) is weakly path-consistent iff, for all pairwise distinct 2 f1 ( )  ( ( ) ( )) and ( )  ( ( ) ( )).
n M  M i k  i j k  I M i j  M j k  M k i  I M k j   : : : n  g,  M j i  From all the evidence it is clear that by applying the following algorithm a the weak path-consistency algorithm a we obtain in polynomial time an equivalent weakly pathconsistent network: - Successively replace, for all pairwise distinct i j k 2 f1 : : :  ng, the constraints M (i k ) and M (k i) by the constraints M (i k ) \ I (M (i j ) M (j k )) and M (k i) \ I (M (k j ) M (j i)).
We first observe that path-consistent generalized interval networks are weakly pathconsistent.
In general, the converse is false, weakly path-consistent generalized interval networks need not be path-consistent.
Let us demonstrate that the problem of deciding consistency of a generalized interval network can be solved in polynomial time by means of the weak path-consistency algorithm if only Horn representable relations are used.
Consider the Horn representable generalized interval network (n M ).
Seeing that Horn representable relations are strongly preconvex, (n M ) is a strongly preconvex generalized interval network.
In view of the fact that the set of all strongly preconvex relations is closed for intersection with convex relations, it is beyond question that by applying the weak path-consistency algorithm, we obtain in polynomial time an equivalent weakly path-consistent strongly preconvex generalized interval network (n M 0).
Then we can define the convex generalized interval network (n M 00) as follows.
For all i j 2 f1 : : :  ng, let M 00(i j ) = I (M 0 (i j )).
Given that the set of all convex relations is closed for composition, (n M 00) is a path-consistent convex generalized interval network.
Seeing that convex relations are weakly preconvex, (n M 00) is a path-consistent weakly preconvex generalized interval network.
In this respect, either it contains the empty constraint or it is maximally consistent.
The former case implies that (n M 0) contains the empty constraint, hence (n M ) is not consistent.
The latter case implies that (n M 0) is maximally consistent, hence (n M ) is consistent.
Now tractability of the issue of consistency of a Horn representable generalized interval network follows easily: Theorem 5 The issue of consistency of a Horn representable generalized interval network can be solved in polynomial time by means of the weak path-consistency algorithm.
8 CONCLUSION We would like to emphasize that so far our main concern has been the connection between the syntactic concept of Horn representability and the geometric concepts of convexity, weak preconvexity and strong preconvexity.
More precisely, we have given the set of all Horn representable relations between generalized intervals a simple characterization based on the concept of strong preconvexity.
An advantage of this characterization is that it has allowed to present a simple proof that the issue of consistency of a Horn representable generalized interval network can be solved in polynomial time by means of the weak path-consistency algorithm.
Much remains to be done.
We wish to investigate the question whether the class of binary relations between two generalized intervals generated by the set of all Horn representable relations between generalized intervals is the unique maximal tractable subclass among those which contain base relations.
Future work also includes permitting the processing of metric constraints between generalized intervals, an important matter in the development of reasoning systems that are concerned with temporal information.
References [1] J. Allen, Maintaining knowledge about temporal intervals, Communications of the ACM, Vol.
26, pp.
832a843, 1983.
[2] P. Balbiani, J.-F. Condotta, L. FarinEas del Cerro, A model for reasoning about bidimensional temporal relations, KRa98, 1998, 124a130.
[3] P. Balbiani, J.-F. Condotta, L. FarinEas del Cerro, A new tractable subclass of the rectangle algebra, IJCAI-99, 1999, 442a447.
[4] P. van Beek, Reasoning about qualitative temporal information, Artificial Intelligence, Vol.
58, pp.
297a321, 1992.
[5] J.-F. Condotta, ProbleEmes de satisfaction de contraintes spatiales: algorithmes et complexiteE, Technical report, Institut de recherche en informatique de Toulouse, Toulouse, 2000.
[6] P. Ladkin, R. Maddux, The algebra of constraint satisfaction problems and temporal reasoning, Technical report, Kestrel institute, Palo Alto, 1988.
[7] G. Ligozat, On generalized interval calculi, AAAI-91, pp.
234a240, 1991.
[8] G. Ligozat, A new proof of tractability for 395a401, 1996.
ORD  -Horn relations, AAAI-96, pp.
[9] G. Ligozat, Generalized intervals: a guided tour, Technical report, Laboratoire dainformatique pour la meEcanique et les sciences de laingeEnieur, Orsay, 1998.
[10] B. Nebel, H.-J.
BuErckert, Reasoning about temporal relations: a maximal tractable subset of Allenas interval algebra, Journal of the ACM, Vol.
42, pp.
43a 66, 1995.
[11] J. Renz, Maximal tractable fragments of the region connection calculus: a complete analysis, IJCAI-99, pp.
448a454, 1999.
[12] J. Renz, B. Nebel, On the complexity of qualitative spatial reasoning: a maximal tractable fragment of the region connection calculus, Artificial Intelligence, Vol.
108, pp.
69a123, 1999.
[13] M. Vilain, H. Kautz, Constraint propagation algorithms for temporal reasoning, AAAI-86, pp.
377a382, 1986.
The Role of Labelled Partitionings for Modelling Periodic Temporal Notions Hans Jurgen Ohlbach Institut fur Informatik, Universitat Munchen email: ohlbach@lmu.de Abstract The key notion for modelling calendar systems as well as many periodic events, for example the seasons, is the notion of a partitioning of the real numbers.
A partitioning of R splits the time axis into a finite or infinite sequence of intervals.
Basic time units like seconds, minutes, hours, days, weeks, months, years etc.
can all be represented by finite partitionings of R. There are a lot of other temporal notions which can be modelled as partitions either: the seasons, the ecclesiastical calendars, financial years, semesters at universities, the sequence of sunrises and sunsets, the sequence of the tides, the sequence of school holidays etc.
In this paper a formalization of periodic temporal notions by means of partitionings of R is presented.
The paper is limited to 4 pages.
Therefore it contains only basic ideas and informal descriptions.
The technical details can be found in [9].
1.
Motivation and Introduction The basic time units of calendar systems, years, months, weeks, days etc.
are the prototypes of periodic temporal notions.
They can be modelled with algorithms mapping the begin and end times of a given year, month etc.
to dates on a reference time axis [3].
This is sufficient for translating dates between different calendar systems.
Another approach is to introduce an intermediate level between the reference time axis and the time units.
Different formalizations of this intermediate level have been proposed.
In this paper the intermediate level consists of the mathematical concept of partitionings of the time axis.
A partitioning splits the time axis into disjoint parts, and these parts can represent years, months etc.
They can also represent other periodic temporal notions, for example seasons, financial years, ecclesiastical calendars, semesters, Olympic years etc.
By introducing partitionings as an explicit datatype one can then write algorithms which work for basic time units in the same way as for other periodic temporal notions.
The algorithm for computing the n th week of a month can also com-  pute for example the n th week in the summer.
Or the query 'when is the next Wednesday?'
is answered in the same way as 'when is the next summer holiday?'.
Many periodic temporal notions not only correspond to sequences of partitions, but the partitions are named.
For example, the days are named 'Monday', 'Tuesday', etc.
The seasons are named 'winter', 'spring' etc.
There is usually a finite sequence of names, and the sequence of partitions is named by starting at a particular partition, and repeating the sequence of names as often as necessary.
To account for this, the concept of labelled partitionings is introduced.
With labelled partitionings temporal notions like 'next Wednesday', 'last summer' etc.
can be modelled in a uniform way.
A built-in label is 'gap'.
It can be used to denote a partition which logically does not belong to a given partitioning.
The work on partitionings for modelling periodic temporal notions is part of the W EB C AL-project.
W EB C AL [2] is a system for understanding, representing and manipulating complex temporal notions from everyday life.
It can represent and manipulate fuzzy time intervals [8] to deal with fuzzy notions like 'early today' or 'in the late 20s'.
It can deal with different calendar systems, even with historical sequences of calendar systems.
Other components are a specification language for specifying application specific temporal notions like 'my weekend' [6, 7].
A prototype of the W EB C AL system is currently being tested.
2.
Relation to Other Work Periodic temporal notions have been modelled in various ways.
For example, in their book, 'Time Granularities', Bettini, Jajodia and Wang [1] introduce 'time granularities' as a generalization of partitionings.
The 'granules' in time granularities are like the partitions in partitionings, but there can be gaps between two subsequent granules, and the granules can be non-convex intervals.
There is an algebra of time granularities, which allows one to define new time granularities from existing ones in various ways.
Other approaches are the formalisms of 'collections' [4] and 'slices' [5].
A collection is a structured set of intervals where the order of the collection gives a measure of the structure depth.
The  Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIME'04) 1530-1311/04 $20.00 (c) 2004 IEEE  slice formalism was introduced in [5] as an alternative to the collection formalism in order to have an underlying evaluation procedure for the symbolic formalism.
There are certain fundamental differences between these approaches and the approach in this paper.
The basic mathematical structure used in this paper are just ordinary partitionings, which makes things simpler and easier to understand than granules, collections and slices.
We get, however, additional expressivity by labelling the partitions with symbolic labels.
One of the labels is 'gap', which corresponds to the gaps between granules.
The labels have certain advantages.
First of all, they are finite data structures for infinite partitions, which is exploited by certain algorithms.
Labellings can be put into a hierarchy of labellings, which permits different views on the same partitioning.
The algebraic operations for constructing new granularities or collections from given ones only approximate reality in most cases.
For example, the construction of minutes from seconds usually ignores the leap seconds introduced almost every year.
A precise definition of minutes,however, must take into account information from a database of leap seconds.
A similar problem occurs when days are constructed from hours.
The day in spring when daylight savings time is enabled has only 23 hours, and the day in autumn when it is disabled has 25 hours.
This information must also be taken from a database.
More extreme is the definition of the ecclesiastical calendar, which is anchored at the date for easter.
This date depends on the moon cycle, and must therefore be computed algorithmically.
Therefore, the proposal in this paper is to define partitionings not algebraically, but algorithmically.
This allows one to take all the irregularities of the real calendar systems into account.
Many periodic temporal notions, for example, school holidays depend on concrete dates in concrete calendar systems.
Therefore a two-level approach is necessary.
At the first level, the basic temporal notions are defined, which are needed for modelling dates in concrete calendar systems, and then new partitionings can be defined which use the dates for their specification.
There is a further difference to the time granularities of Bettini et al.
Granules may have holes, and this can be used, for example to define the notion of 'working days within a month'.
This is not directly possible in the partitioning approach.
Instead, one can define such a notion as a function which maps intervals (one month, for example) to intervals (the working days in the month).
These functions may use the partitionings, but their specification and use is a completely different issue and not discussed here [6, 10].
3.
Partitionings A partitioning of the real numbers R may be for example (..., [-100, 0[, [0, 100[, [100, 101[, [101, 500[, ...).
The in-  tervals in the partitionings considered in this paper need not be of the same length (because time units like years are not of the same length either).
The intervals can, however, be enumerated by natural numbers (their coordinates).
For example, we could have the following enumeration ... [-100 0[ [0 100[ [100 101[ [101 500[ ... ... -1 0 1 2 ...
The formal definition for partitionings of R which is used in this paper is: Definition 3.1 (Labelled Partitionings) A partitioning P of the real numbers R is a sequence .
.
.
[t-1 , t0 [, [t0 , t1 [, [t1 , t2 [, .
.
.
of half open intervals in R with integer boundaries, such that either the sequence is infinite at one or both ends, or it is preceded by an infinite interval ] - [?
], t[ (the start partition) or/and it is ended by an infinite interval [t, +[?
][ (the end partition).
A coordinate mapping of a partitioning P is a bijective mapping between the intervals in P and a subset of the integers.
Since we always use one single coordinate mapping for a partitioning P , we can just use P itself to indicate the mapping.
Therefore let pP be the coordinate of the partition p in P .
For a coordinate i let i P be the partition which corresponds to i.
A Labelling L is a finite sequence of strings l 0 , .
.
.
, ln-1 .
A labelling L = l0 , .
.
.
, ln-1 is turned into a labelling function L P (p) for a partitioning P as follows:  if p is finite  lpP mod n 'startPartition' if p = [-[?
], t[ = LP (p) def  'endPartition' if p = [t, +[?
][ where p is a partition in P .
In contrast to coordinate mappings, where only one coordinate mapping per partitioning is considered, it makes sense to allow many different labellings for the same partitioning.
In most cases the labels are not arbitrary strings, but words of a particular language, and these words have a certain meaning.
Therefore one can allow different labellings where the labels are words in different languages.
For example the days can be labelled in English: 'Thursday', 'Friday' etc.
and in German: 'Donnerstag', 'Freitag' etc.
Labellings can be ordered hierarchically.
For example there may be labellings 'BavarianHolidays' and 'BerlinHolidays', which correspond to the different school holiday periods.
Both labellings can be a 'sub-labelling' of a labelling 'schoolHolidays'.
The difference between 'BavarianHolidays' and 'schoolHolidays' is that the labels in 'BavarianHolidays', which are not the label 'gap', are all labelled 'holiday' in the labelling 'schoolHolidays'.
This offers for example the possibility to intersect a partitioning  Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIME'04) 1530-1311/04 $20.00 (c) 2004 IEEE  labelled 'BavarianHolidays' with another partitioning labelled 'BerlinHolidays'.
To do this, the labellings 'BavarianHolidays' and 'BerlinHolidays' are replaced by 'schoolHolidays' in both cases, and the intersection of the partitions with label 'holiday' is computed.
This is a non-trivial operation which is beyond the scope of this paper.
The details of the algebra of labelled partitionings have not yet been investigated.
4.
Specifying Partitionings Five different ways to specify partitionings are presented.
Each way influences the details of the functions which map coordinates to partitions and vice versa.
Standard Partitionings are specified by (i) an average length of a partition, (ii) an offset for the partition with coordinate 0, and (iii) a correction function.
The correction function computes for a partition with coordinate n the difference between the reference time of the beginning of the partition with coordinate n according to the average length, and the real beginning of the partition with coordinate n. All basic time units, seconds, minutes etc.
are modelled as standard partitionings.
Other periodic temporal notions which can be modelled by standard partitionings are for example sunrises and sunsets, moon phases, the church year which starts with Easter, etc.
Regular Partitionings are characterized by an anchor date and a sequence of shifts in a given time unit.
A typical example is the notion of a semester at a university.
In the Munich case, the dates could be: anchor date: 2000/10 (October 2000).
The shifts are: 6 months (with label 'winter semester') and 6 months (with label 'summer semester').
This defines a partitioning which extends into the future and the past.
The partition with coordinate 0 is the winter semester 2000/2001.
Further examples for periodic temporal notions which can be encoded as regular partitionings are decades, centuries, the British financial year which starts April 1 st , the dates of a particular lecture which is every week at the same time.
Duration Partitionings: Regular partitionings are a special case where the shifts are specified in terms of the same partitioning.
A natural generalization of this would be to specify the shifts by 'durations'.
A duration is a list of pairs (number, partitioning).
For example (3 month), (1 week) is a duration.
A duration partitioning is specified by an achor date and a sequence of durations.
For example, we could start with the anchor date 2000 and then define partitions by the three durations (3 month, 1 week), (5 weeks, 10 days), (1 year).
The above notion of a duration is a mathematical concept worthwhile to be investigated separately.
It can also play a role in temporal constraint networks [11] if the constraints for temporal distances is not specified in constant time units like seconds, but in time units like months, whose length depends on the position at the time axis.
Calendrical Partitionings are specified by an anchor date and then a finite sequence of partial dates.
For example, the seasons can be defined with the anchor date 2000/3/21 (spring begin).
The partial dates are 6/21 (summer) 9/23 (autumn) 12/21 (winter) +1/3/21 (spring again).
This also defines an infinite partitioning.
Another example could be a whole timetable for a semester in the university, where it is sufficient to specify the dates for one week, and then it is repeated every week.
Date Partitionings: In this version we provide the boundaries of the partitions by concrete dates.
Therefore the partitioning can only cover a finite part of the time line.
An example could be the dates of the Time conferences: 1994/5/4 Time94 1994/5/4 gap 1995/4/26 Time95 1995/4/26 .
.
.
2004/7/1 Time04 2004/7/3.
5.
Operations with Partitionings shift: Notions like 'in two weeks time' or 'three years from now' etc.
denote time shifts.
They can be realized by a function which maps a time point t to a time point t  such that t - t is just the required distance of 'two weeks' or 'three years' etc.
A function shift(t, m, P ) is defined where t is a reference time point, n is a real number, and P is a partitioning.
The function shifts the time point t by m partitions of the partitioning P .
The definition of shift depends on the type of partitioning.
The idea for standard partitionings is as follows: if for example the partitioning represents months, and t is the reference time of the current moment in time, then shift(t, 1, month) should calculate the reference time of 1 month in the future.
The first problem is that months have different lengths.
For example, if t is in February, does 'in one months time' mean in 28 days or in 31 days?
This problem can be overcome and a very intuitive solution is obtained if the calculations are not done on the level of reference time points, but on the level of dates.
If, for example, t represents 2004/1/15, then 'in one month time' usually means 2004/2/15.
That means the reference time must be turned into a date, the date must be manipulated, and then the manipulated date is turned back into a reference time.
This is quite straight forward if the partitioning represents a basic time unit of a calendar system (year, month, week, day etc.
), and this calendar system has a date format where the time unit occurs.
Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIME'04) 1530-1311/04 $20.00 (c) 2004 IEEE  The next problem is to deal with fractional shifts.
How can one implement, say, 'in 3.5 months' time'?
The idea is as follows: suppose the date format is year/month/day/hour/minute/second, and the reference time corresponds to, say, 2004/1/20/10/5/1.
First we make a shift by three months and we end up at 2004/4/20/10/5/1.
This is a day in May.
From the date format we take the information that the next finer grained time unit is 'day'.
May has 31 days.
0.5 * 31 = 15.5.
Therefore we need to shift the date first by 15 days, and we end up at 2004/6/4/10/5/1.
There is still a remaining shift of half a day.
The next finer grained time unit is hour.
One day has 24 hours.
0.5 * 24 = 12.
Thus, the last date is shifted by 12 hours, and the final date is now 2004/6/4/22/5/1.
This is turned back into a reference time.
advanceCoordinate(i, n, P, skipGap) advances the coordinate i of a partitioning P by n partitions.
If skipGap = f alse or there is no labelling defined for the partition, then the result is just i + n. If skipGap = true then all partitions labelled 'gap' are ignored.
The result is i + n where n - n is the number of ignored gap partitions.
The advanceCoordinate function realizes notions like 'tomorrow' or 'in two weeks time' or 'last semester' etc.
nextCoordinateWithLabel: this function computes for a given coordinate the coordinate of the next/previous mth partition with the given label.
The label of the partition with the given coordinate is ignored.
The function can be used to realize notions like 'next Wednesday' or 'Wednesday in three weeks' or 'last summer' or 'the summer 10 years ago' etc.
If the partitionings become part of a calendar system, and concrete time intervals are introduced independently of partitionings, then functions are possible which let partitionings operate on intervals.
For example 'new year' can be defined as a function mapping an arbitrary interval I to the first day partition of the year partitions contained in I (see [6, 7]).
6.
Summary  tioning and gave examples of the kind of periodic temporal notions which can be encoded this way.
References [1] Claudio Bettini, Sushil Jajodia, and Sean X. Wang.
Time Granularities in Databases, Data Mining and Temporal Reasoning.
Springer Verlag, 2000.
[2] Francois Bry, Bernhard Lorenz, Hans Jurgen Ohlbach, and Stephanie Spranger.
On reasoning on time and location on the web.
In N. Henze F. Bry and J. Malusynski, editors, Principles and Practice of Semantic Web Reasoning, volume 2901 of LNCS, pages 69-83.
Springer Verlag, 2003.
[3] Nachum Dershowitz and Edward M. Reingold.
Calendrical Calculations.
Cambridge University Press, 1997.
[4] B. Leban, D. Mcdonald, and D.Foster.
A representation for collections of temporal intervals.
In Proc.
of the American National Conference on Artificial Intelligence (AAAI), pages 367-371.
Morgan Kaufmann, Los Altos, CA, 1986.
[5] M. Niezette and J. Stevenne.
An efficient symbolic representation of periodic time.
In Proc.
of the first International Conference on Information and Knowledge Management, volume 752 of Lecture Notes in Computer Science, pages 161-169.
Springer Verlag, 1993.
[6] Hans Jurgen Ohlbach.
About real time, calendar systems and temporal notions.
In H. Barringer and D. Gabbay, editors, Advances in Temporal Logic, pages 319-338.
Kluwer Academic Publishers, 2000.
[7] Hans Jurgen Ohlbach.
Calendar logic.
In I. Hodkinson D.M.
Gabbay and M. Reynolds, editors, Temporal Logic: Mathematical Foundations and Computational Aspec ts, pages 489-586.
Oxford University Press, 2000.
[8] Hans Jurgen Ohlbach.
Fuzzy time intervals and relations - the FuTIRe library.
Technical report, Inst.
f. Informatik, LMU Munchen, 2004.
See http://www.pms.informatik.unimuenchen.de/mitarbeiter/ohlbach/systems/FuTIRe.
[9] Hans Jurgen Ohlbach.
The role of labelled partitionings for modelling periodic temporal notions.
httpd://www.informatik.unimuenchen.de/mitarbeiter/ohlbach/homepage/publications/PRP/abstracts.shtml, 2004. to be published.
[10] Hans Jurgen Ohlbach and Dov Gabbay.
Calendar logic.
Journal of Applied Non-Classical Logics, 8(4), 1998.
[11] L. Vila and L. Godo.
On fuzzy temporal constraint networks.
Mathware and Soft Computing, 1994.
The mathematical tool of partitionings, augmented with labels, together with suitable operations, is presented as a useful tool for unifying the treatment of many different periodic temporal notions.
This ranges from the basic time units in calendar systems until concrete timetables which are defined for a certain period, usually a week, and then get repeated every week.
The characteristics of the concrete partitioning is encoded in the two functions which map coordinates to partitions and vice vers.
All other operations are generic.
I proposed five different ways to specify a parti-  Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIME'04) 1530-1311/04 $20.00 (c) 2004 IEEE
Propagating Possibilistic Temporal Constraints Rasiah Loganantharaj  1 Introduction  Automated Reasoning laboratory The Center for Advanced Computer Studies University of SouthWestern Louisiana Lafayette, LA - 70504  The notion of time plays an important role in any intelligent activities.
Time is represented either implicitly or explicitly.
We are interested in explicit representation of time.
The popular approaches for such representation are based on points or intervals or a hybrid of both.
Propositional temporal assertions are represented as relations among the points, or among the intervals.
The indenite information among either the points or the intervals are represented as disjunctions.
In real world, information is often incomplete, imprecise, uncertain and approximate.
Temporal knowledge is not an exception to this reality.
For example, consider the following information: John often drinks coee during his breakfast.
Sometimes, he drinks his coee before the breakfast and drinks orange juice during his breakfast.
There were few occasions he drank water during his breakfast and drank coee after the breakfast.
Mike talked to John over the phone while John was having coee.
Suppose, we are interested in nding out how Mik's telephone conversation was related to John's breakfast.
From the given information, we have denite relation between the telephone call and John's coee, but there is no information about the relationship between the coee and the breakfast on that particular day.
In the absence of such information, we can use John's habitual pattern to infer plausible relations.
Suppose, Ic and Ib respectively represent the interval over which John was having coee, and John was having breakfast.
Let It represents the interval over which Mike was having telephone conversation with John.
In interval logic, the information is represented as It is during Ic , and Ic is before or during or after Ib .
In such representation, the disjunctive relations do not provide any clue about which relation is highly probable than the others.
Instead, the representation may let us believe that all the relations of a disjunction have equal probability, for example, having coee before, during or after breakfast has equal probability.
This is not what the original information tells us.
Based on John's habit, having coee during breakfast is much more probable than having coee either before or after the breakfast.
This issues have not been studied in temporal reasoning.
In  this paper we will provide a representation to specify uncertain information and to propagate them over temporal constraint network.
This paper is organized as the following.
We introduce interval-based logic in Section 2.
In Section 3, we describe the representation of uncertain temporal knowledge and its propagation.
This paper is concluded by a summary and discussion in section 4.
2 Background on Interval Based System Allen 1] has proposed an interval logic that uses time intervals as primitives.
In this logic, the following seven relations and their inverses are dened to express the temporal relations between two intervals: before(after), meets(metby), overlaps(overlapped-by), starts(started-by), during(contains), ends(ended-by), and equals.
Here, the inverse relations are indicated within parentheses.
Since the inverse of equal is same as itself, there are, in fact, only thirteen relations.
Temporal inferencing is performed by manipulating the network corresponding to the intervals.
Each interval maps onto a node of a network called temporal constraint network (TCN).
A temporal relation, say R, from an interval, say Ii , to another interval, say Ij , is indicated by the label Rij on the directed arc from Ii to Ij .
Obviously, the label Rji of the directed arc from Ij to Ii is the inverse relation of Rij .
If we have denite information about the relation Ii to Ij then Rij will be a primitive interval relation, otherwise it will be disjunctions of two or more primitive interval relations.
Suppose the relation Ii to Ij (Rij ) and the relation Ij to Ik (Rjk ) are given.
The relation between Ii and Ik , constrained by Rij and Rjk , is given by composing Rij and Rjk .
In general, Rij and Rjk can be disjunctions of primitive relations, they are represented as: 1  r2  : : : rn g Rij = frij1  rij2  : : : rijm g and Rjk = frjk jk jk where rij is one of the primitive relations dened in the system.
The interval Ii is related to the interval Ik by the temporal relation given by the following expression:  Rij  Rjk =  (  p) (rijl  rjk  )  l=1:::m p=1:::n p l where (rij  rjk) is a composition (transitive relap , and is obtained from the entry tion) of rijl and rjk j of the transitivity table 1] at the riji row and the rjk  column.
Alternatively this could be written as Rij  Rjk = fT(rij  rjk)jrij 2 Rij ^ rjk 2 Rjkg where T(rij  rjk) is the value of Allen's look up composition table of row rij and column rjk .
Temporal constraints are propagated to the rest of the network to obtain the minimal temporal network in which each label between a pair of intervals is minimal with respect to the given constraints.
Vilain et al.
7] have shown that the problem of obtaining minimal labels for an interval-based temporal constraint network is NP-complete.
Approximation algorithms, however, are available for temporal constraint propagation.
Allen proposed an approximate algorithm that has an asymptotic time complexity of O(N 3 ) where N is the number of intervals.
His algorithm is an approximate one in the sense that it is not guaranteed to obtain the minimum relations, but it always nd the superset of the minimal label.
Since any set is a super set of a null set it is not very comforting because it is possible that global inconsistency may be hiding under 3-consistency.
3 Representation of Uncertainty  The problem of uncertainty is not new to AI problem solving.
In many expert system applications, uncertainty have been studied under approximate reasoning.
Mycin system 6, 2] has used certainty factor whose value varies from -1 to 1 through 0 to represent the condence on an evidence or on a rule.
The value 1 indicates the assertion is true while the value -1 indicates the evidence is false.
0 indicates no opinion on the evidence.
The other values correspond to some mapping of the belief on the evidence onto the scale of -1 to 1.
Prospector model 4, 5] uses probabilistic theory with Bayes' theorem and other approximation techniques to propagate evidences over causal network.
Fuzzy logic 8] has also been used in expert systems to capture knowledge with fuzzy quantiers such as `very much', `somewhat' etc.
Other techniques have also been used to handle uncertainty in expert systems.
Let us look back at the same example presented in the introduction of this paper.
John often drinks coee during his breakfast.
Sometimes he drinks his coee before the breakfast and drinks orange juice during his breakfast.
There were few occasions he drank water during his breakfast and drank coee after the breakfast.
The statement has fuzzy quanties indicating that the frequency of John having coee during his breakfast is much higher than he is having coee either before or after his breakfast.
We should capture the fuzzy quantiers into probabilistic measures in our temporal constraint representation such that the summation of the probabilities of the relations between a pair of intervals is equal to 1.
Let us represent this idea more formally.
The relation Rij , the relation Ii to Ij , is represented as frij1 (w1) rij2 (w2) ::: rijm(wm )g where rijl (wl ) is a primitive relation with its probability or the relative weight of wl .
Since each primitive relation between a pair of intervals is associated with a weight to represent the probability or the relative strength, the summation of the weights must be equal to 1 which we P call probabilistic condition for the weights.
That is, mi=1 wi = 1.
The boundary value of the weight 1 indicates that the relation is true while the value 0 indicates that the relation is false.
Further, the inverse relation of Rij will be the inverse of all the primitive relations of Rij with the same weights.
In the presence of new evidences, the probability values of the relations are modied to take account of the new evidences.
That is, when the relations between a pair of intervals are modied or rened because of other constraining relations, the probability or the weights of the relations are adjusted to satisfy the probabilistic condition.
This process is called normalization.
Let us explain this with an example: Suppose (1) R12 = fb(0:6) O(0:3) d(0:1)g, (2) R13  R32 = fb(0:4) o(0:5) m(0:1)g. The relation R12 is rened to fb(w1) o(w2)g. The weights w1 and w2 are computed using the intersection operation and then the weights are normalized such that w1 + w2 = 1.
When propagating constraints with probabilistic weights, we may need to dene such as union, intersection, composition and normalization operations which are used in propagation.
0  0  Union operation:  Rij p Rij = fr(w)jr(wij ) 2 Ri ^ r(wij ) 2 Rj ^ w = max(wij  wij )g 0  0  0  Intersection Operation:  Rij \p Rij = fr(w)jr(wij ) 2 Ri ^ r(wij ) 2 Rj ^ w = min(wij  wij )g 0  0  0  Composition Operation: Rij  Rjk =  l (wl ) 2 Rij ^ rm (wm ) 2 Rjk fp r(w)jrij jk l  rm ) ^ w = min(wl  wm )g ^ r = T(rij jk  Normalization operation: Suppose the label Rij takes the following form after renement 1 (w ) r2 (w ) ::: rm(w )g. Let w = Pm w .
frij 1 ij 2 ij m i=1 i After normalization operation the label becomes frij1 (w1) rij2 (w2) :::Prijm(wm )g where wi = wi=w which ensures that mi=1 wi = 1 In this approach we use possibilistic ways of combin0  0  0  0  0  ing the constraints as has been used in many expert systems under uncertainty.
3.1 Temporal Propagation  A temporal constraint network (TCN) is constructed from the given temporal assertion such that each node of the constraint network represents each interval of the temporal assertions.
The labels on each arc of a TCN corresponds to the relations between the corresponding intervals.
Further the summation of the weights of each component in each arc should be equal to 1.
If no constraint is specied between a pair of intervals it will take a universal constraint1 as label in the TCN and it will not be used for the purpose of propagation.
When propagating a constraint, other label of the network may get updated to a subset of its label.
The process of updating of a label as a result of propagating a constraint is called label renement.
The label renement takes the following forms: (1) The weights of the labels of an arc get changed, or (2) the primitive relations of a label get reduced.
In either case normalization operation is performed to ensure the summation of the weights adds to 1.
Suppose we are propagating the label Rij of the arc < I J > to the arc < I K > of the triangle IJK.
Let the labels of the arcs < J K > and < I K > are respectively Rjk and Rik.
The new label of the arc < I K > is computed as Rik \ fRij  Rjkg.
When the new relation is not null the weights on the label are normalized.
3.2 Temporal Constraints Propagation Algorithm for uncertain constraints  This is an extension of Allen's propagation algorithm.
The algorithm uses a rst in rst out (FIFO) queue to maintain the constraints that need to be propagated.
Initially all the pairs of constrained intervals are placed into the queue.
The propagation of constraints is initiated by removing an arc, say < Ii  Ij >, from the queue and checking whether the relation between Ii to Ij can constrain the relations on all the arcs incident to either Ii or Ij except the arc < Ii Ij >.
When a new relation is constrained, that is, the old label is modied, the arc (the pair of intervals related by this relation) is placed in the queue.
The main propagation algorithm is described in Figure 1.
In this algorithm, we use the notation Rij to denote the label of the arc i to j.
When we omit the weights on the labels and use the union, the intersection operations of set, and the composition operation of temporal logic, the algorithm becomes identical to the one of Allen's 1].
One may expect the asymptotic complexity to be O(N 3 ) where N is the number of nodes of the TCN.
Intuitively one may make a conclusion that this algorithm 1 a universal constraint is the weakest constraint and thus it is a disjunctions of all the primitive relations of the time model  will also converge in O(N 3) time.
This may be misleading because we have not yet considered the instability eect of the algorithm due to it normalization operation.
An arc is placed back in the queue when either the number of primitive relations of the label is reduced or the weight of the primitive relation of the label is modied even when the label remains unchanged.
An arc may be placed in the queue at most 12 times as a result of the disjunctive relations being rened one at a time till it becomes a singleton relation.
On the other hand, the number of times an arc is placed in the queue due to the change of weight depends on the following parameters: (1) the resolution of the weights (the number of decimal places that counts) and (2) the error bound we are prepared to tolerate.
A complete study on these issues can be found in one of our report 3].
Let us consider an example.
The probability of `having coee' before breakfast is 0.15, during breakfast is 0.8 and after breakfast is 0.05.
The probability of `having a coee' overlapping `reading morning newspaper' is 0.8 and `having coee' meets `reading the newspaper' is 0.2.
Suppose we want to nd the relation between having breakfast and reading newspaper.
Let us propagate the constraints and nd out how the labels gets rened.
Suppose Ib  Ic  Ir respectively represent the intervals of having breakfast, coffee and reading newspaper.
Using the notations dened in this paper we can dene the labels of the initial TCN as following.
Initial TCN Rcb = fb(0:15) d(0:8) a(0:05)g Rcr = fo(0:8) m(0:2)g Rbr = to be computed TCN after propagating Rbc to < Ib  Ir > Rcb = fb(0:15) d(0:8) a(0:05)g Rcr = fo(0:8) m(0:2)g Rbr = fo(:8) oi(:15) d(:15)di(:8)f(:15) fi(:8) b(:05) a(:15)g After normalizing Rbr Rbr = fo(:26) oi(:049) d(:049) di(:26)f(:049) fi(:26) b(:024) a(:049)g This is also the 3-consistent TCN labels.
4 Summary and Discussion  In many real world applications we are faced with the information that is incomplete, indenite, imprecise and uncertain.
When explicit time was used for temporal reasoning, indenite information is accommodated as disjunctions.
For example, drinking coffee is either before, during or after the breakfast.
The disjunctive information implicitly assume equal probability of occurrence even though exactly one can be true between a pair of intervals (points).
In such representation we fail to distinguish or dierentiate the highly probable one from the remotely possible one.
According to our example, having coee during breakfast is highly probable than having coee before  procedure propagate1() 1 While queue is not empty Do 2  f  3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  g  get next < i j > from the queue /* propagation begins here */ For ( ( k 2 set of intervals ) ^k 6= i ^ k 6= j )Do f temp 	 Rik \p (Rij  Rjk) If temp is null Then signal contradiction and Exit Normalize temp If Rik 6= temp Then f place < i k > on queue Rik 	 temp g temp 	 Rkj \p (Rki  Rij ) If temp is null Then signal contradiction and Exit Normalize temp If Rkj 6= temp Then f place < k j > on queue Rkj 	 temp g  g  Figure 1: propagation algorithm or after a breakfast.
In this paper we have proposed a formalism to represent temporal constraints with the associated probabilistic weights and use them to propagate to the rest of the network to obtain 3-consistency.
We have extended Allen's algorithm to handle probabilistic relations among intervals.
The operations we have dened for the labels with weights are applicable to both the points and the intervals.
Therefore, our formalism will be applicable to both the points and the intervals.
Acknowledgement  This research was partially supported by a grant from Louisiana Education Quality Support Fund (LEQSF).
References  1] J. F. Allen.
Maintaining knowledge about temporal intervals.
Communications of ACM, 26(11):832{843, 1983.
2] B. G. Buchanan and E. H. Shortlie Eds.
RuleBased Expert Systems.
Addision-Wesley, 1984.
3] R. Loganantharaj.
Complexity issues of possibilistic temporal reasoning.
preperation, 1994.
4] J. Gaschnig R. O. Duda and P. E. Hart.
Model design in the prospector consultant system for mineral exploration.
In D. Michie, editor, Expert Systems in the Microelectronics Age.
Edinburgh University Press, 1979.
5] P. E. Hart R. O. Duda and N. J. Nilsson.
Subjective bayesian methods for rule-based inference  systems.
In Proceedings of the AFIPS National Computer Conference, volume 45, 1976.
6] E. H. Shortlie.
Computer Based Medical Consultations:MYCIN.
Elsvier, 1976.
7] M. Vilain and H. Kautz.
Constraint propagation algorithm for temporal reasoning.
In Proceedings of AAAI-86, pages 377{382, 1986.
8] L. A. Zadeh.
Making computers think like people.
IEEE Spectrum, pages 26{32, August 1984.
Temporal Resolution: A Breadth-First Search Approach Clare Dixon Department of Computing Manchester Metropolitan University Manchester M1 5GD United Kingdom C.Dixon@doc.mmu.ac.uk  Abstract  An approach to applying clausal resolution, a proof method for classical logics suited to mechanisation, to temporal logics has been developed by Fisher.
The method involves translation to a normal form, classical style resolution within states and temporal resolution between states.
The method consists of only one temporal resolution rule and is therefore particularly suitable as the basis of an automated temporal resolution theorem prover.
As the application of this temporal resolution rule is the most costly part of the method, involving search amongst graphs, it is on this area we focus.
A breadth-rst search approach to the application of this rule is presented and shown to be correct.
Analysis of its operation is carried out and test results for its comparison to a previously developed depth-rst style algorithm given.
1 Introduction  Temporal logics have been used extensively for the specication and verication of properties of concurrent systems, see for example 18, 19, 15, 11, 3, 21, 2, 14].
Important computational properties such as liveness, deadlock and mutual exclusion can be expressed easily and simply in temporal logic making it useful for specication.
Verifying that a temporal logic specication satises a temporal property usually requires some form of theorem proving.
Model checking approaches have been the most popular, particularly based on tableau 25] or automata 23].
However, standard model checking approaches are limited as only nite state problems can be handled and, even then the number of states required soon becomes large due to the combinatorial explosion.
Alternatively one can adopt a resolution based approach 22] which is is not limited to nite state problems and has a signicant body of work on heuristics to control search (see This work was supported partially by an EPSRC PhD Studentship and partially by EPSRC Research Grant GR/K57282  standard texts such as 6] for example).
Decision procedures based on resolution have been developed for temporal logics in 5, 24, 1], however in many cases they are unsuitable for implementation either because they only deal with a small number of the temporal operators or because of problems with proof direction due to the large numbers of resolution rules that may be applied.
In this paper we present a breadth-rst search style algorithm which enables practical implementation of the resolution method for temporal logics developed by Fisher 9].
The resolution procedure is characterised by translation to a normal form, the application of a classical style resolution rule to derive contradictions that occur at the same points in time (termed step resolution), together with a new resolution rule, which derives contradictions over time (termed temporal resolution).
This paper is structured as follows.
In x2, a description of the propositional temporal logic used and the normal form required for the temporal resolution method is given.
An outline of this temporal resolution method is given in x3, while the BreadthFirst Search algorithm to implement the temporal resolution step is described in x4.
The output of the Breadth-First Search algorithm is examined in x5, results comparing it with a previously described algorithm and conclusions are drawn in x6.
2 A linear temporal logic  Here we summarise the syntax and semantics of the logic used and describe the normal form required for the resolution method.
2.1 Syntax and semantics  The logic used in this report is Propositional Temporal Logic (PTL), in which we use a linear, discrete model of time with nite past and innite future.
PTL may be viewed as a classical propositional logic augmented with both future-time and past-time tempo-  ral operators.
Future-time temporal operators include `}' (sometime in the future ), ` ' (always in the future ), ` g' (in the next moment in time ), ` U ' (until ), ` W ' (unless or weak until ), each with a corresponding past-time operator.
Since our temporal models assume a nite past, for convenience, two last-time operators are used namely ` v' (weak last) and ` bcdef' (strong last).
When evaluated at any point other than the beginning of time, both bcdefA and vA are true if and only if A was true at the previous moment.
However, for any formula A, bcdefA is false, when interpreted at the beginning of time, while vA is true at that point.
Particularly, vfalse is only true when interpreted at the beginning of time.
The weak lasttime operator may be dened in terms of the strong last operator as follows  A:  v  :A:  f e d bc  Models for PTL consist of a sequence of states, representing moments in time, i.e.,   = s0 fi s1 fi s2 fi s3 fi : : : Here, each state, si , contains those propositions satised in the ith moment in time.
As formulae in PTL  are interpreted at a particular moment, the satisfaction of a formula f is denoted by (fi i) j= f where  is the model and i is the state index at which the temporal statement is to be interpreted.
For any well-formed formula f , model  and state index i, then either (fi i) j= f or (fi i) 6j= f .
For example, a proposition symbol, `p', is satised in model  and at state index i if, and only if, p is one of the propositions in state si , i.e., (fi i) j= p i p 2 si : The semantics of the temporal connectives used in the normal form or the resolution rule are dened as follows (fi i) j= vA i i = 0 or (fi i ; 1) j= A (fi i) j= bcdefA i i > 0 and (fi i ; 1) j= A (fi i) j= }A i there exists a j > i s.t.
(fi j ) j= A (fi i) j= A i for all j > i then (fi j ) j= A (fi i) j= A U B i there exists a k > i s.t.
(fi k) j= B and for all i 6 j < k then (fi j ) j= A (fi i) j= A W B i (fi i) j= A U B or (fi i) j= A: The full syntax and semantics of PTL will not be presented here, but can be found in 9].
2.2 A normal form for PTL  Formulae in PTL can be transformed to a normal form, Separated Normal Form (SNF), which is the basis of the resolution method used in this paper.
SNF was introduced rst in 9] and has been extended to rst-order temporal logic in 10].
While the translation from an arbitrary temporal formula to SNF will not be described here, we note that such a transformation preserves satisability and so any contradiction generated from the formula in SNF implies a contradiction in the original formula.
Formulae in SNF are of the general form  ^ Ri i  where each Ri is known as a rule and must be one of the following forms.
false  v  f e d bc  ^g ka  a=1  false  v  f e d bc  ^g ka  a=1  ) ) ) )  _r lb  (an initial  _ lb  (a global  b=1 r  b=1  }l }l  {rule) {rule)  (an initial }{rule) (a global }{rule)  Here ka , lb , and l are literals.
The outer ` ' operator, that surrounds the conjunction of rules is usually omitted.
Similarly, for convenience the conjunction is dropped and we consider just the set of rules Ri .
We note a variant on SNF called merged-SNF (SNFm ) 9] used for combining rules by applying the following transformation.
A B f(A ^ B ) e d bc f e d bc  f e d bc  ) ) )  F G F ^G  The right hand side of the rule generated may have to be further translated into Disjunctive Normal Form (DNF), if either F or G are disjunctive, to maintain the general SNF rule structure.
3 The resolution procedure  Here we present a review of the temporal resolution method 9].
The clausal temporal resolution method consists of repeated applications of both `step' and `temporal' resolution on sets of formulae in SNF, together with various simplication steps.
3.1 Step resolution  `Step' resolution consists of the application of standard classical resolution rule to formulae representing constraints at a particular moment in time, together with simplication rules for transferring contradictions within states to constraints on previous states.
Simplication and subsumption rules are also applied.
Pairs of initial {rules, or global {rules, may be resolved using the following (step resolution) rule where L1 and L2 are both last-time formulae.
L1 ) L2 ) (L1 ^ L2 ) )  A_r B _ :r A_B  Once a contradiction within a state is found using step resolution, the following rule can be used to generate extra global constraints.
P  f e d bc  true  v  false  ) ) :P  This rule states that if, by satisfying P in the last moment in time a contradiction is produced, then P must never be satised in any moment in time.
The new constraint therefore represents :P (though it must rst be translated into SNF before being added to the rule-set).
The step resolution process terminates when either no new resolvents are derived, or false is derived in the form of one of the following rules.
false true  v  f e d bc  ) )  false false  3.2 Temporal resolution During temporal resolution the aim is to resolve a }{rule, LQ ) }l, where L may be either of the last-  time operators, with a set of rules that together imply :l, for example a set of rules that together have the eect of bcdefA ) :l. However the interaction between the ` g' and ` ' operators in PTL makes the denition of such a rule non-trivial and further the translation from PTL to SNF will have removed all but the outer level of {operators.
So, resolution will be between a }{rule and a set of rules that together imply an {formula which will contradict the }{ rule.
Thus, given a set of rules in SNF, then for every rule of the form LQ ) }l temporal resolution may be applied between this }{rule and a set of global {rules, which taken together force :l always to be satised.
The temporal resolution rule is given by the following  A0 ::: fA e d bc n LQ f e d bc  true  v  ) ) )  )  F0 ::: Fn }l n ^ :Ai :Q _ n i ^ ( :Ai ) W l =0  LQ )  with side conditions for all 0   i   n and  8> < >:  i=0  ` `  Fi ) :l _n Fi ) Aj j =0  9> = >	  where the side conditions ensure that the set of rules fA ) F together imply e d bc :l. In particular the i i rst side condition ensures that each rule, bcdefAi ) Fi , makes :l true now if bcdefAi is satised.
The second side condition ensures that the right hand side of each rule, fA ) F , means that the left hand side of one of e d bc i i the rules in the set will be satised.
So once the left hand side of one of these rules is satised, i.e.
if Ai is satised for some i in the last moment in time, then :l will hold now and the left hand side of another rule will also be satised.
Thus at the next moment in time again :l holds and the left hand side of another rule is satised and so on.
So if any of the Ai are satised then :l will be always be satised, i.e., f e d bc  _n Ak )  k=0  :l:  Such a set of rules are known as a loop in :l.  3.3 The temporal resolution algorithm  Given any temporal formula  to be shown unsatisable the following steps are performed.
1.
Translate  into a set of SNF rules s .
2.
Perform step resolution (including simplication and subsumption) until either (a) false is derived - terminate noting  unsatisable or (b) no new resolvents are generated - continue at step 3.
3.
Select an eventuality from the right hand side of a }{rule within s , for example }l. Search for loops in :l and generate the resolvents.
4.
If any new formulae have been generated, translate the resolvents into SNF add them to the ruleset and go to step 2, otherwise continue to step 5.
5.
Terminate declaring  satisable.
Completeness of the resolution procedure has been shown in 16].
3.4 Loop search  As it is the application of the temporal resolution rule, i.e.
the search for a set of rules that together imply :l, assuming we are resolving with }l, that is the most dicult part of the problem it is on this we concentrate in the rest of the paper.
Dierent approaches to detecting such loops have been described in 7] and in particular a depth-rst search style algorithm was outlined in 8].
With this algorithm, rules are used as edges in a graph and nodes represent the left hand sides of rules.
The Depth-First Search algorithm uses SNFm rules to try build a path of nodes, where every path leads back into the set of nodes already explored.
The SNFm rules are applied one at a time in a depth-rst manner, as several SNFm rules may be used to expand from a particular node, backtracking when a dead end is reached.
The rules governing expansion from a node ensure that the desired looping occurs and, assuming we are resolving with }l, that the required literal :l, is obtained.
In the next section we give an alternative, BreadthFirst Search algorithm, for detecting loops together with an example of its use.
4 Breadth-First Search  Using the Breadth-First Search Algorithm, rules are only selected for use if they will generate the required literal at the next moment in time and their right hand side implies the previous node.
The algorithm operates on SNF rules, only combining them into SNFm when required.
With Breadth-First Search all possible rules (but avoiding the duplication of information) are used to expand the graph, rather than just selecting one rule.
The graph constructed using this approach is a sequence of nodes that are labelled with formulae in Disjunctive Normal Form.
This represents the left hand sides of rules used to expand the previous node which have been disjoined and simplied.
If we build a new node that is equivalent to the previous one, using this approach, then we have detected a loop.
However if we cannot create a new node then we terminate without having found a loop.
4.1 Breadth-First Search Algorithm For each rule of the form LQ ) }l carry out the  following.
1.
Search for all the rules of the form bcdefXk ) :l, for k = 0 to b (called start rules), disjoin the left hand sides and make the top node H0 equivalent to this, i.e.
H0 ,  _b Xk :  k=0  Simplify H0 .
If ` H0 , true we terminate having found a loop.
2.
Given node Hi , build node Hi+1 for i = 0fi 1fi : : : by looking for rules or combinations of rules of the form bcdefAj ) Bj , for j = 0 to m where ` Bj ) Hi and ` Aj ) H0 .
Disjoin the left hand sides so that  Hi+1 ,  _m Aj  j =0  and simplify as previously.
3.
Repeat (2) until (a) ` Hi , true.
We terminate having found a Breadth-First loop and return true.
(b) ` Hi , Hi+1 .
We terminate having found a Breadth-First loop and return the DNF formula Hi .
(c) The new node is empty.
We terminate without having found a loop.
Algorithms to limit the number of rule combinations required have also been developed and are given in 7].
Input to the Breadth-First Search will be a set of SNF rules and these algorithms show how to test whether rules can be used as they are for node expansion, require combination only with start rules, or must be combined with other rules.
Similarly, when rules are combined together an algorithm is given to make the number of combinations required as few as possible.
Nodes (in DNF) are kept in their simplest form by carrying out simplication and subsumption.
4.2 Example  Breadth-First Search is used to detect the loop in the set of rules given below.
Assume we are trying to resolve with the rule LQ ) }l where L is either of the last-time operators, and the set of global -rules is fe e d bc 1: bcdefa ) :l 5: ) e f e d c b f e d c b 2: b ) :l 6: (a ^ e) ) a fb e d bc 3: bcdefc ) :l 7: ) b f e d bc f e d bc 4: d ) :l 8: c ) b  To create a new node Hi , we examine each rule in turn, disjoining the conjunction of literals on the left hand side of the new node if the rule satises the criteria given.
The new node is then simplied where necessary.
1.
The rules 1{4 have :l on their right hand side.
We disjoin their left hand sides and simplify (although in this case no simplication is necessary) to give the top node  H0 = a _ b _ c _ d: 2.
To build the next node, H1 , we see that rules 6, 7 and 8 satisfy the expansion criteria in step (2) of the Breadth-First Search Algorithm (i.e.
the right hand side and the literals on the left hand side of each rule implies H0 ) but rule 5 does not.
Note if we combine rule 5 with any of the other rules to produce an SNFm rule that satises the expansion criteria, its left hand side will be removed through simplication.
So we disjoin the literals on their left hand sides of rules 6, 7 and 8 to obtain node  H1 = (a ^ e) _ b _ c: 3.
Rules 7 and 8 satisfy the expansion criteria and so do rules 5 and 6 when combined together to give the rule bcdef(a ^ e) ) a ^ e: Thus node H2 becomes H2 = (a ^ e) _ b _ c: As H2 , H1 we terminate having detected a loop.
The Breadth-First loop we have found is (a ^ e) _ b _ c. The graph constructed using Breadth-First Search for this set of rules is shown in Figure 1.
4.3 Resolvents  W  The Breadth-First Search algorithm returns a DNF formula i Di that means (assuming we are resolving with LQ ) }l) if this formula is satised in the previous moment in time then :l always holds, i.e.
f e d bc  _ Di ) i  :l:  Rather than try reconstruct a set of rules with which to apply the temporal resolution rule we use the output from Breadth-First Search directly (as if each disjunct were the left hand side of a rule).
In the previous example the loop output was (a ^ e) _ b _ c and the resolvents obtained are  true  v  ) :Q _ :((a ^ e) _ b _ c) LQ ) :((a ^ e) _ b _ c) W l  and must be further translated into SNF.
4.4 The main processing cycle  The Breadth-First search procedure nds all loops (see x5) for a particular eventuality.
Each time round the main processing loop an eventuality is taken with which to resolve.
Termination, declaring the formula satisable is only allowed if no more new loops may be found (and step resolution produces no new rules).
Given Li , for i = 1 to n, loops detected previously we can ensure that a new loop L provides no new information by checking whether n _ L ) Li : i=1  This is because any new resolvents produced from L will be subsumed by those already generated from loops Li .
4.5 Correctness issues H0 H1  a_b_c_d (  a ^ e) _ b _ c  Figure 1 Breadth-First Search Example  Soundness, completeness and termination of the Breadth-First Search algorithm are proved in 7].
5 Breadth-First Search loops  The two main characteristics of loops output by the Breadth-First Search are that all loops are detected for an eventuality and that paths into the loop are also detected.
These are described below and compared with the output of the Depth-First Search algorithm.
Finally we consider the memory implications for Breadth-First Search.
5.1 Detection of all the loops The Breadth-First Algorithm nds all the loops for a particular }{rule.
Considering the example given in x4.2 it is clear that the three SNFm rules,  M1 M2 M3  (a ^ e)  f e d bc  b fc e d bc  f e d bc  ) ) )  (a ^ e ^ :l) (b ^ :l) (b ^ :l)  satisfy the criteria for a loop together (and in any combination of subsets of these three rules where if rule M 3 occurs then rule M 2 also occurs).
We could combine these SNFm rules further to give other SNFm rules for example combining rule M 1 and M 2 we obtain  M4  (a ^ b ^ e)  f e d bc  )  (a ^ b ^ e ^ :l):  Performing Breadth-First Search we return the formula (a ^ e) _ b _ c as shown previously (because of simplication we will never return (a ^ e) _ b _ c _ (a ^ b ^ e) for example).
However performing a Depth-First Search we detect loops one at a time i.e.
we would only ever return a loop relating to either rule M 1 or rule M 2 or rule M 4.
5.2 The lead into the loop  Breadth-First Search nds disjuncts representing rules that are never detected in Depth-First Search.
An example of this is the disjunct c representing the use of the SNFm rule bcdefc ) (b ^ :l): This rule represents the path into the loop (there are no rules that make this rule `re').
Both systems are still complete but having found the Depth-First loop bcdefb ) (b ^:l)fi for example, we will have to carry out further applications of the step resolution rule to generate rules equivalent to the complete set of resolvents from BreadthFirst Search.
5.3 Memory considerations  Usually a disadvantage of breadth-rst search algorithms is the amount of memory required to construct the search space.
The Breadth-First Search Algorithm described here does attempt to use all the rules or combinations of rules to construct the next node so, in the worst case, memory requirements may be a problem.
However, such problems are avoided in many cases for the following reasons.
Firstly, the BreadthFirst Search Algorithm only requires the storage of the top node, H0 , and the last node constructed, Hi , to enable the construction of node Hi+1 and to test for termination.
The full search space constructed between these two points is not required and therefore after the construction of node Hi+1 , and failure of the termination test, the memory required for the  storage of node Hi may be released.
Secondly each node constructed is a DNF formula kept in its simplest form so any redundant information is removed from the node.
Finally, although the number of rules in which we search for loops may be large, containing many propositions, typically, the loops will relate to only a few of these rules containing a subset of the total number of propositions.
6 Results and conclusions  Results comparing the Breadth-First Search Algorithm with a previous loop search algorithm are given and conclusions are then drawn.
6.1 Results  A prototype implementation performing the temporal resolution method has been built.
Two dierent loop search programs have been provided|one for Breadth-First Search, the other for the DepthFirst Search algorithm.
The programs are written in SICStus Prolog 4] running under UNIX and timings have been carried out on a SPARCstation 1 using compiled Prolog code.
The test data is a set of valid temporal formulae taken from 13] chosen as it is a reasonably sized collection of small problems to be proved valid.
An example of the type of formula being shown valid (we actually shown the negation is unsatisable) is  }w1 ^ }w2 ) }(w1 ^ }w2 ) _ }(w2 ^ }w1 ): We note that, although not presented in the results given here, larger examples have also been tackled, for example Peterson's Algorithm 17, 20].
The full set of results, including timings for each eventuality and example, are given in 7] however, due to space restrictions, we only present a summary of the data here.
Table 1 shows the number of times the total loop search for Depth-First Search (DFS) was less than or equal to, or greater than that for Breadth-First Search (BFS), for each eventuality and example.
The gures in brackets are the values as percentages.
Values are given for the full data set and for those examples where at least one of the times is greater than 60 and then 100 milliseconds.
By considering these categories we hope to eliminate inaccuracies from very low timings.
The gures in Table 1 indicate that BreadthFirst Search performs better in signicantly more examples than Depth-First Search.
The increase in the percentage of examples where Breadth-First Search is quicker than Depth-First Search as we move from the examples with (at least one) time over 60 milliseconds to (at least one) time over 100 milliseconds suggests  Subset of Data DFS 6 BFS BFS < DFS (i) Full data set 20 (40 %) 30 (60 %) (ii) Time > 60 10 (37 %) 17 (63 %) (iii) Time > 100 6 (27 %) 16 (73 %)  Table 1 Summary of Comparative Timings that Breadth-First Search performs better on larger examples.
Further, the raw data shows that timings for Depth-First Search have a larger range of values than Breadth-First Search.
This is what we would expect from depth-rst search style algorithms because if the correct search path is chosen rst the solution can be detected more quickly than breadth-rst search type algorithms.
However, if a large amount of time is spent exploring fruitless paths then the overall time may be far greater than that for Breadth-First Search.
Table 2 shows the same data displayed in columns representing the number of calls made by each example to the Depth-First Search algorithm, i.e.
how many times we have had to search for a loop using DepthFirst Search for each example.
Recall that BreadthFirst Search nds all loops for an eventuality where as Depth-First Search nd them one at a time.
The rst row in the table gives the total number of examples for the respective number of calls.
The second and third rows give the number and percentage, respectively, of these examples where the time for detecting the loop with Breadth-First Search was less than the total time spent in loop search by Depth-First Search.
The column headed 3 has been omitted as there were no examples that required 3 calls to the Depth-First Search Algorithm.
1 2 4 5 Total 25 22 2 1 BFS < DFS (No.)
13 14 2 1 BFS < DFS (%) 52% 64% 100% 100%  Table 2 Summary by Number of Calls The table indicates that for this set of examples the more calls to the loop nding section that Depth-First Search makes the more likely it is that it is quicker to do a Breadth-First Search than a Depth-First Search.
If we take the greater number of calls to the DepthFirst loop nding program as an indication of the size of the example, then this matches the observation that  was made previously that Breadth-First Search performs better on larger examples.
6.2 Conclusions  A Breadth-First Search algorithm for implementing Fisher's temporal resolution method has been described and given.
Output from this algorithm is analysed and compared with a previously described depthrst approach.
An prototype implementation has been produced and run on a variety of valid temporal formulae.
Test results suggest that breadth-rst search does perform better than this alternative on larger examples.
Although we have only considered the propositional version of the logic it may be possible to extend the resolution method to rst-order temporal logic (and even to other temporal logics).
Indeed a rst-order version of the normal form exists 10].
However, as full rst-order temporal logic is undecidable 12] we must rst consider subsets to which the resolution method can successfully be applied.
The suitability of Fisher's temporal resolution method to mechanisation, the algorithms developed for the temporal resolution step and prototype implementation together means that temporal resolution provides a viable option for automated temporal theorem proving.
6.3 Acknowledgements  Thanks to my supervisors Professor Howard Barringer and Dr. Michael Fisher for their guidance and encouragement during this work.
Thanks also to Graham Gough for many helpful comments and advice.
References  1] M. Abadi and Z.
Manna.
Nonclausal Deduction in First-Order Temporal Logic.
ACM Journal, 37(2):279{317, April 1990.
2] H. Barringer.
Using Temporal Logic in the Compositional Specication of Concurrent Systems.
In A. P. Galton, editor, Temporal Logics and their Applications, chapter 2, pages 53{90.
Academic Press Inc. Limited, London, December 1987.
3] H. Barringer, R. Kuiper, and A. Pnueli.
Now You May Compose Temporal Logic Specications.
In Proceedings of the Sixteenth ACM Symposium on the Theory of Computing, 1984.
4] M. Carlsson and J.
Widen.
SICStus Prolog User's Manual.
Swedish Institute of Computer Science, Kista, Sweden, September 1991.
5] A. Cavalli and L. Fari~nas del Cerro.
A Decision Method for Linear Temporal Logic.
In R. E.  Shostak, editor, Proceedings of the 7th International Conference on Automated Deduction, volume 170 of Lecture Notes in Computer Science, pages 113{127.
Springer-Verlag, 1984.
6] C. L. Chang and R. C. T. Lee.
Symbolic Logic and Mechanical Theorem Proving.
Academic Press, 1973.
7] C. Dixon.
Strategies for Temporal Resolution.
PhD thesis, Department of Computer Science, University of Manchester, 1995.
8] C. Dixon, M. Fisher, and H. Barringer.
A GraphBased Approach to Resolution in Temporal Logic.
In D. M. Gabbay and H. J. Ohlbach, editors, Temporal Logic, First International Conference, ICTL '94, Proceedings, volume 827 of Lecture Notes in Articial Intelligence, Bonn, Germany, July 1994.
Springer-Verlag.
9] M. Fisher.
A Resolution Method for Temporal Logic.
In Proceedings of the Twelfth International Joint Conference on Articial Intelligence (IJCAI), Sydney, Australia, August 1991.
Morgan Kaufman.
10] M. Fisher.
A Normal Form for First-Order Temporal Formulae.
In Proceedings of Eleventh International Conference on Automated Deduction (CADE), volume 607 of Lecture Notes in Computer Science, Saratoga Springs, New York, June 1992.
Springer-Verlag.
11] B. T. Hailpern.
Verifying Concurrent Processes Using Temporal Logic, volume 129 of Lecture Notes in Computer Science.
Springer-Verlag, 1982.
12] W. Hussak.
Decidability in Temporal Presburger Arithmetic.
Master's thesis, Department of Computer Science, University of Manchester, February 1987.
13] Z.
Manna and A. Pnueli.
Verication of Concurrent Programs: The Temporal Framework.
In Robert S. Boyer and J. Strother Moore, editors, The Correctness Problem in Computer Science, pages 215{273.
Academic Press, London, 1981.
14] Z.
Manna and A. Pnueli.
The Temporal Logic of Reactive and Concurrent Systems: Specication.
Springer-Verlag, New York, 1992.
15] S. Owicki and L. Lamport.
Proving Liveness Properties of Concurrent Programs.
ACM Transactions on Programming Languages and Systems, 4(3):455{495, July 1982.
16] M. Peim.
Propositional Temporal Resolution Over Labelled Transition Systems.
Unpublished Technical Note, Department of Computer Science, University of Manchester, 1994.
17] G. L. Peterson.
Myths about the Mutual Exclusion Problem.
Information Processing Letters, 12(3):115{116, 1981.
18] A. Pnueli.
The Temporal Logic of Programs.
In Proceedings of the Eighteenth Symposium on the Foundations of Computer Science, Providence, November 1977.
19] A. Pnueli.
The Temporal Semantics of Concurrent Programs.
Theoretical Computer Science, 13:45{60, 1981.
20] A. Pnueli.
In Transition From Global to Modular Temporal Reasoning about Programs.
In Krysztof Apt, editor, Logics and Models of Concurrent Systems, pages 123{144, La Colle-surLoup, France, October 1984.
NATO, SpringerVerlag.
21] A. Pnueli.
Applications of Temporal Logic to the Specication and Verication of Reactive Systems: A Survey of Current Trends.
In J.W.
de Bakker, W. P. de Roever, and G. Rozenberg, editors, Current Trends in Concurrency, volume 224 of Lecture Notes in Computer Science.
Springer-Verlag, August 1986.
22] J.
A. Robinson.
A Machine{Oriented Logic Based on the Resolution Principle.
ACM Journal, 12(1):23{41, January 1965.
23] M. Y. Vardi and P. Wolper.
An AutomataTheoretic Approach to Automatic Program Verication.
In Proceedings IEEE Symposium on Logic in Computer Science, pages 332{344, Cambridge, 1986.
24] G. Venkatesh.
A Decision Method for Temporal Logic based on Resolution.
Lecture Notes in Computer Science, 206:272{289, 1986.
25] P. Wolper.
The Tableau Method for Temporal Logic: An overview.
Logique et Analyse, 110{ 111:119{136, June-Sept 1985.
Ecient Handling of Context-Dependency in the Cached Event Calculus Luca Chittaro and Angelo Montanari  Abstract  Dipartimento di Matematica e Informatica, Universita di Udine, Via Zanon, 6, 33100 Udine, Italy e-mail: fchittaro j montanarig@uduniv.cineca.it  This paper deals with the problem of providing temporal deductive databases with an ecient implementation in a logic programming framework.
We restrict our attention to historical databases based on Kowalski and Sergot's Event Calculus extended with context-dependency.
The paper aims at being benecial to both the theoretically-minded and the implementation-oriented research communities.
It provides a mathematical analysis of the computational complexity of query and update processing in the Event Calculus, and proposes a cached version of the calculus that moves computational complexity from query to update processing, and features an absolute improvement of performance when contextdependency is added.
1 Introduction  The paper deals with the problem of providing temporal deductive databases (TDDs) with an ecient implementation in a logic programming framework.
TDDs provide the possibility of managing temporal information not only to retrieve information as it was stored in the database, but also to automatically derive further data  1, 2, 14, 20].
In the case of incomplete temporal data, information that is neither explicitly asserted nor monotonically implied by the available knowledge can be inferred by drawing conclusions according to suitable assumptions such as closed world or default persistence.
Conclusions derived in this way are obviously defeasible and TDDs must withdraw them if the addition of further information makes them inconsistent.
Dierent TDDs have been proposed in the literature  9, 11, 12, 16].
They dier from each other either in the underlying model of change (state-based in  15, 16], event-based in  11, 12]), or in the programming language paradigm they adopt (functional programming in  9, 10], logic programming in  11, 12]), or in both.
We restrict our attention to historical databases based on Kowalski and Sergot's Event Calculus model of change enriched with context-dependency  5, 11].
From a description of events that occur in the real world, the Event Calculus (EC) allows one to derive various properties and the time periods for which they  hold.
The addition of context-dependency makes it possible to constrain the initiation and termination of properties to the validity of some given conditions at the time at which events occur.
EC database and rules are formulated in a logic programming framework as a logic program.
EC presents several advantages over relational temporal databases as well as over most TDDs  12].
First of all, while relational temporal databases only record the starting and ending of properties, thus losing the semantical structure of causing events, EC supports an explicit representation of events.
In this way, updates are performed by entering events and deriving the starting and ending of properties as a logical consequence from event descriptions by means of general rules which express the semantics of events.
Moreover, the domain modeler is fully in charge of ensuring the integrity of data, and thus expensive integrity checking procedures are not needed.
Secondly, insertion of events in the database is not required to follow the chronological order of their occurrences.
Thirdly, default persistence can be dened both in the future and the past.
The cost of update processing in EC is the cost of entering a new event, and thus it is constant  11], while query processing is expensive.
The addition of context-dependency, which is crucial to deal with real-world problems  3, 5, 8], heavily deteriorates performance of query processing.
The paper extends EC with a lemma storage mechanism (Cached Event Calculus) that records maximal validity intervals (MVIs) of properties for later use in query processing and updates them as soon as a new event is entered in the database.
CEC moves computational complexity from query to update processing, and features an absolute improvement of performance when contextdependency is added, because CEC update processing costs less than EC query processing.
Furthermore, CEC preserves the basic requirement of EC of making no assumptions about the temporal order of input events.
As an example, CEC accepts an event happened before some of the already acquired events and revises all and only the aected MVIs.
Obviously, acquiring a complete sequence of events according to their chronological order avoids the revision of past data, thus strongly increasing the performance of CEC.
This is the case of several application domains,  such as patient monitoring  8].
The paper is organized as follows.
Section 2 introduces EC extended with context-dependency.
Section 3 presents in detail the basic features of CEC.
It illustrates how entering an event in the database may cause the assertion of new MVIs for the properties it aects or the shortening of cached ones, and how assertions and retractions are possibly propagated to other properties.
A sample execution of CEC concludes the section.
Section 4 analyzes and compares the complexity of query and update processing in EC and CEC.
Conclusions provide an assessment of the work.
2 The logical calculus of events  In this section, we present the main features of the logical calculus of events.
It extends EC with the notions of type and context-dependency.
EC proposes a general approach to represent and reason about events and their eects in a logic programming framework.
It takes the notions of event, property, timepoint and time-interval as primitives and denes a model of change in which events happen at timepoints and initiate and/or terminate time-intervals over which some property holds.
Time-points are unique points in time at which events take place instantaneously.
Time-intervals are represented as pairs of time-points.
EC also embodies a notion of default persistence according to which properties are assumed to persist until an event occurs which terminates them.
A specic domain evolution is called an history and is modeled by a set of event occurrences (event instances).
The calculus allows us to infer a set of time intervals over which the properties initiated and/or terminated by event occurrences maximally hold (property instances).
Instances of events and properties are obtained by attaching a time-point and a time-interval to event and property types and are denoted by the pairs (event,time-point) and (property, time-interval), respectively1 .
Formally, we represent event occurrences by means of the happens at predicate: happens_at(event,timePoint).
Furthermore, we represent (part of the) domain knowledge by means of initiates at and terminates at predicates that express the eects of events on properties: initiates_at(event1,prop1,T):happens_at(event1,T).
terminates_at(event2,prop2,T):happens_at(event2,T)  Initiates at (terminates at) predicates state that each instance of event1 (event2) initiates (terminates) a period of time during which prop1 (prop2) holds, The pair (event, time-point) uniquely identies an event occurrence provided that two events of the same type can not simultaneously happen.
In situations where this assumption is not acceptable, explicit identiers for event occurrences have to be added.
1  respectively2 .
A particular initiates at clause can be used to deal with initial conditions.
Initial conditions describe a possibly partial initial state of the world and are specied by means of a number of events of type initially(prop).
Their validity from the beginning of time can then be derived by means of the clause: initiates_at(initially(Prop),Prop,0):happens_at(initially(Prop),0).
Such a clause is parametric with respect to the property argument and takes 0 as the initial instant of the time axis.
This allows us to distinguish explicitly stated initial conditions (starting at 0) from initial conditions derived by persistence in the past, and to assign a greater degree of condence to the former.
2.1 The basic axioms of EC  The basic Event Calculus model of time and change is dened by means of a set of axioms.
The rst axiom we introduce is the mholds for.
It allows us to state that the property P holds maximally (i.e.
there is no larger time-interval for which it also holds) over  Start End] if an event E1 which initiates P occurs at the time Start, and an event E2 which terminates P occurs at time End, provided there is no known interruption in between: mholds_for(P,Start,End]):initiates_at(E1,P,Start), terminates_at(E2,P,End), End gt Start, \+ broken_during(P,Start,End]).
mholds_for(P,Start,infPlus]):initiates_at(E1,P,Start), \+ broken_during(P,Start,infPlus]).
mholds_for(P,infMin,End]):terminates_at(E2,P,End), \+ broken_during(P,infMin,End]).
where the predicate gt extends the ordinary ordering relationship > to include the cases involving innite arguments, sintactically denoted by infMin and infPlus.
Analogously, we dened the predicates ge, lt and le which extend  < and , respectively.
The negation involving the broken during predicate is interpreted using negation-as-failure.
This means that properties are assumed to hold uninterrupted over an interval of time on the basis of failure to determine an interrupting event.
Should we later record an initiating or terminating event within this interval, we can no longer conclude that the property holds over the interval.
This gives us the non-monotonic character of the calculus which deals with default persistence.
The predicate broken during is dened as follows: 2 Dierently from the original denition of the Event Calculus 11], devoid of any notion of event type and then only supporting an extensional denition of initiates and terminates predicates at the instance level, domain relations are intensionally dened in terms of event and property types.
broken_during(P, Start,End]):(terminates_at(E,P,T)initiates_at(E,P,T)), Start lt T, End gt T.  This axiom provides a so-called strong interpretation of initiates at and terminates at predicates: a given property P ceases to hold at some point T during the time-interval  Start End] if there is an event E which initiates or terminates P occurring at a time T belonging to  Start End].
Notice that if we record three events e1, e2 and e3 such that e1 precedes e2, e2 precedes e3, both e1 and e2 initiates a property p and e3 terminates p, we can only conclude that p holds between the occurrence times of e2 and e3.
This behaviour can be explained as follows: the strong interpretation assumes that an event terminating p actually occurred between e1 and e2, but it is not known when it occurred, and thus it is not possible to derive any validity interval for p between e1 and e2.
According to this interpretation, pending events like e1 characterize situations of incomplete information about event occurrences.
An alternative interpretation of initiates at and terminates at predicates, called weak interpretation, is also possible.
According to such an interpretation an event initiates a property unless it has been already initiated and not yet terminated.
To support this weak interpretation the denition of broken during must be revised so that only terminating events can break the validity of property P  8].
Finally, we add the holds at axiom relating a property to a time-point rather than to a time-interval: holds_at(P,T):mholds_for(P,Start,End]), T gt Start, T le End.
The holds-at predicate conventionally assumes that a property is not valid at the starting point of the MVI, while it is valid at the ending point.
These axioms constitute the kernel of basic (typed) EC.
They provide a simple and eective tool to reason about events and their eects, but have a limited expressive power.
In particular, they provide no primitives for modeling relevant features such as contextdependency, discrete and continuous processes, time granularity.
Several extensions  5, 7, 12, 13, 19, 18, 20] have been proposed to overcome these limitations.
In this paper, we deal with the addition of contextdependency.
In basic EC, both initiates at and terminates at are context-independent predicates: the occurrence of an event of the given type initiates, or terminates, the validity of the relevant property whatever is the context in which it occurs.
On the contrary, making initiates at and terminates at predicates contextdependent allows us to state that the occurrence of an event of a given type at a certain time point initiates or terminates the validity of the associated property provided that some given conditions hold at such an instant  3].
Formally, initiates at and terminates at predicates are generalized as follows3 : initiates_at(event,prop1,...,propN],prop,T):happens_at(event,T), holds_at(prop1,T),...,holds_at(propN,T).
terminates_at(event,prop1,...,propN],prop,T):happens_at(event,T), holds_at(prop1,T),...,holds_at(propN,T).
where N is greater than 0 when initiates at and terminates at are context-dependent and equal to 0 when they are context-independent.
In this last case, we simply dene initiates at and terminates at as: initiates_at(E,],P,T):initiates_at(E,P,T).
terminates_at(E,],P,T):terminates_at(E,P,T).
As shown elsewhere, the inclusion of holds at atoms in the body of terminates at and initiates at makes EC no more stratied  17, 4].
As a consequence, termination of the computation of MVIs is not guaranteed anymore.
2.2 Comparing EC with and without context-dependency  Consider a simple lighting system that is operated by its user using just one switch, whose functioning can be described as follows.
The user can set the switch in one of two positions: on or o.
If there is electrical power available, the eect of setting the switch in the on or o position is to switch the light on or o.
If there is no electrical power available, the eect of setting the switch in the on position is delayed until electrical power is provided.
A failure in providing power can anticipate the eect of setting the switch in the o position.
We identify four types of events and three types of properties.
Event types are: turnOn (the user sets the switch in the On position), turnO (the user sets the switch in the O position), pwrFail (a failure in the electrical power distribution network), pwrRstr (the failure is xed, and power is restored).
Property types are: switchOn (the position of the switch is on), pwrAvail (electrical power is available), lightsOn (the lights are lit).
Using EC extended with context-dependency, the knowledge about eects of events on properties can be formalized as follows: initiates_at(turnOn,],switchOn,T):happens_at(turnOn,T).
terminates_at(turnOff,],switchOn,T):happens_at(turnOff,T).
initiates_at(pwrRstr,],pwrAvail,T):happens_at(pwrRstr,T).
3 The 2nd argument in the 4-argument version of initiates at and terminates at allows to statically inspect domain axioms in order to detect dependencies among properties as shown in Section 3.2.  update  terminates(pwrFail,],pwrAvail,T):happens_at(pwrFail,T).
initiates_at(turnOn,pwrAvail],lightsOn,T):happens_at(turnOn,T), holds_at(pwrAvail,T).
terminates_at(turnOff,pwrAvail],lightsOn,T):happens_at(turnOff,T), holds_at(pwrAvail,T).
update Init  breaking I  extending I  update Termin  breaking T  extending T  initiates_at(pwrRstr,switchOn],lightsOn,T):happens_at(pwrRstr,T), holds_at(switchOn,T).
terminates_at(pwrFail,switchOn],lightsOn,T):happens_at(pwrFail,T), holds_at(switchOn,T).
On the contrary, in the case of basic EC the modeler would be forced to use context-independent initiates at and terminates at predicates also to represent context-dependent knowledge.
This can be done only by introducing additional types of events and naming them properly to highlight that they assume a given context.
Considering for example the eect of the turnOn event on the property lightsOn, we can reformulate it with the EC axiom: initiates_at(turnOnwithPwrAvail,lightsOn,T):happens_at(turnOnwithPwrAvail,T).
This not only introduces a more complex event, but forces us to introduce another type of event (turnOnwithNoPwrAvail) in order to rewrite the description of the eects of turnOn on the property switchOn.
The drawbacks of this solution aect both the modeler and the nal user of the database.
On one hand, the modeler is indeed forced to (i) devise and introduce a number of new events that increases greatly with the complexity of the considered domain and the possible combinations of preconditions, (ii) highlight the dierences among these events using long, awkward names.
On the other hand, the nal user is burdened with the responsability of precisely evaluating contexts in the real world in order to choose the proper event that has to be entered in the database (for example, in the simple lighting system example, the user must know if power was supplied when he/she pushed the button).
Moreover, it becomes impossible in this way to enter an event when there's only partial knowledge about the context it assumes.
These limitations become unbearable in complex domains and unnecessarily narrow the deductive power of the database.
If EC is indeed extended with context-dependency, it becomes able to automatically identify contexts and therefore the user has only to enter basic types of events which he/she easily recognizes, leaving to the database the task of evaluating and updating the context and the eects.
Furthermore, the database becomes able to reason with incomplete knowledge about contexts.
propagate Retract  propagate Assert  Figure 1: Architecture of CEC.
3 The Cached Event Calculus (CEC)  In this section we describe the main features of the Cached Event Calculus (CEC).
It extends EC with a caching mechanism that receives instances of events as input and updates accordingly the cached set of MVIs (mholds for assertions).
Temporal reasoning in EC is performed at query time: the system logs any input event without processing it, and accesses the log when a query is posed.
Thanks to the negation as failure rule, conclusions no longer supported are not derived anymore.
The fact that the results of computations are never cached for later use makes query processing in EC very inecient.
A great deal of unnecessary computation is indeed performed whenever the same mholds for query about a property p is processed several times between two consecutive updates aecting MVIs of p, because EC repeats each time the whole computation from the beginning instead of saving previous conclusions.
Even worse, when context-dependency is added multiple computations of identical mholds for (sub)queries can occur during the processing of a single mholds for query, due to contexts evaluation.
To eliminate these useless computations, EC should be provided with a mechanism for caching the currently believed set of MVIs, so that accesses to cached data replace blind recomputations in answering mholds for queries.
However, conceiving a suitable caching mechanism is not a trivial task, because it should be able to incrementally add new results to the old ones and to update or delete only the old results which need it.
Moreover, dierently from general caching mechanisms, we do not only need to add and/or remove assertions, but also to clip and/or extend existing MVIs according to domain and temporal knowledge.
The general architecture of CEC is depicted in Figure 1, where all its conceptual modules and their hierarchical relations are highlighted.
We briey summarize here the purpose of the modules their func-  tioning will be described in detail in the next sections.
Each new event is entered into the database by update updateInit and updateTermin are then called in order to manage properties which are initiated and terminated by the event, respectively.
In both cases, the event can lead either to clip an existing MVI (this case is handled by breakingI and breakingT) or to possibly create a new MVI (this case is handled by extendingI and extendingT).
When a MVI (or a part of it) is retracted (asserted), propagateRetract (propagateAssert) takes care of propagating that change to properties which depend on the changed one.
Propagation of assertion and retractions can recursively activate the process of breaking or extending MVIs.
In the next two sections, we rst describe the process of breaking and extending MVIs and then the process of propagating retractions and assertions.
3.1 Breaking and adding maximal validity intervals  CEC allows to enter events in the database by means of the update predicate.
This predicate rst explicitly records the instance of the event and then the properties initiated and terminated by the event are considered separately, by means of the updateInit and updateTermin predicates: update(E,T):assert(happens_at(E,T)), bagof(P,(updateInit(E,T,P) updateTermin(E,T,P)),_).
update(_,_).
If E initiates a property Prop at time T , CEC tests if there exists a MVI  T 1 T 2] for Prop such that T 1  T < T 2: updateInit(E,T,Prop):initiates_at(E,_,Prop,T), (insideLeftClosedInt(Prop,T,T1,T2]) -> breakingI(Prop,T,T1,T2]) extendingI(Prop,T)).
insideLeftClosedInt(Prop,T,T1,T2]):mholds_for(Prop,T1,T2]), T1 le T, T lt T2.
If such an interval exists, we face two possibilities, which are handled by the breakingI predicate: (i) if T 1 = T , there is already an event occuring at T , that initiates Prop and then no changes are needed to interval  T 1 T 2] at the moment (ii) otherwise, interval  T 1 T 2] is shortened in such a way that the new starting point becomes T , Prop does not hold anymore in the clipped part  T 1 T ], and the retraction of  T 1 T ] has to be propagated.
breakingI(_,T1,T1,_]):-!.
breakingI(Prop,T,T1,T2]):retract(mholds_for(Prop,T1,T2])), assert(mholds_for(Prop,T,T2])), propagateRetract(T1,T],Prop).
If instead there are no MVIs for Prop satisfying the test T 1  T < T 2, then T is the starting point of a new MVI for Prop.
The new interval of validity  starting at T is added by the extendingI predicate, and the new assertion has to be propagated.
extendingI(Prop,T):new_termination(Prop,T,NewEnd]), assert(mholds_for(Prop,T,NewEnd])), propagateAssert(T,NewEnd],Prop).
The new termination predicate nds the ending point of the new interval, distinguishing the following two cases: (i) there is a `pending' terminating event for property Prop occurring at time NewEnd after T and thus the new MVI becomes  T NewEnd], (ii) there are no initiating or terminating events for property Prop occurring after T and thus the new MVI becomes  T infPlus]: new_termination(Prop,T,NewEnd]):terminates_at(_,_,Prop,NewEnd), T lt NewEnd, \+broken_during(Prop,T,NewEnd]), !.
new_termination(Prop,T,infPlus]):\+broken_during(Prop,T,infPlus]).
The case in which the entered event E terminates instead of initiating a property is handled by a set of predicates (updateTermin, insideRightClosedInt, breakingT, extendingT, new initiation), that is symmetrical to the previously discussed one  6].
3.2 Propagation of retractions and assertions  Each time a MVI  T 1 T 2] for a property is retracted (asserted), the update has to be propagated to properties whose validity may rely on such an interval.
The retraction (assertion) of  T 1 T 2] indeed modies the context of events occurring at time points belonging to it and, then, it can possibly invalidate (activate) their eects.
More precisely, only those contextdependent initiates at or terminates at clauses having the retracted (asserted) property as a condition have to be reconsidered over the interval (we implement this selection by inspecting the domain axioms using the standard clause predicate).
In the case of propagation of retractions, we distinguish two relevant cases: (i) possibly invalidated initiations the retracted property RetractedProp is a condition for the initiation of property P at the occurrence of event E and the occurrence time T 3 of E belongs to  T 1 T 2].
In this case, the right part of the retracted interval  T 1 T 2] overlaps the possibly aected MVI  T 3 T 4] for P : propagateRetract(T1,T2],RetractedProp):clause(initiates_at(E,PropList,P,_),_), memberchk(RetractedProp,PropList), happens_at(E,T3), rightOverlap(T1,T2],P,T3,T4]), retractForRightOverlap(P,T2,T3,T4]), fail.
rightOverlap(T1,T2],P,T3,T4]):T1 lt T3, T3 le T2,  mholds_for(P,T3,T4]).
(ii) possibly invalidated terminations RetractedProp is a condition for the termination of property P at the occurrence of event E and the occurrence time T 4 of E belongs to  T 1 T 2].
In this case, the left part of the retracted interval  T 1 T 2] overlaps the possibly aected MVI  T 3 T 4] for P : propagateRetract(T1,T2],RetractedProp):clause(terminates_at(E,PropList,P,_),_), memberchk(RetractedProp,PropList), happens_at(E,T4), leftOverlap(T1,T2],P,T3,T4]), retractForLeftOverlap(P,T1,T3,T4]), fail.
leftOverlap(T1,T2],P,T3,T4]):T1 lt T4, T4 le T2, mholds_for(P,T3,T4]).
The clauses for propagateRetract end with a fail predicate in order to cause the examination of all aected properties.
When no more initiations and terminations are left to examine, a third clause guarantees success: propagateRetract(_,_).
When an initiation is invalidated, there are four possible situations: 1. independency P still initiates at T 3, because there exists a successful initiates at clause, which does not include RetractedProp as condition: retractForRightOverlap(P,_,T3,_]):initiates_at(_,_,P,T3),!.
The MVI for P is thus unchanged.
2. revised initiation with nite termination property P terminates at a time instant T 4 (it does not hold forever) and either there exists an initiating event preceding T 4 or no initiating or terminating events occur before T 4 and P is assumed to hold from infMin by default.
The new initiation predicate is used to identify the proper case, determining the new starting point NewStart of the interval.
Finally, if T 3 < NewStart then the retraction of validity over  T 3 NewStart] is propagated, otherwise the extension of validity over  NewStart T 3] is propagated.
retractForRightOverlap(P,_,T3,T4]):T4 \== infPlus, new_initiation(P,NewStart,T4]), !, retract(mholds_for(P,T3,T4])), assert(mholds_for(P,NewStart,T4])), ((T3 lt NewStart) -> propagateRetract(T3,NewStart],P) propagateAssert(NewStart,T3],P)).
3. revised initiation with innite termination property P holds until infPlus and there exists an event occurring at NewStart that initiates P with P holding uninterrupted after  NewStart, that is, there are no events initiating or terminating P occurring after NewStart.
In such a case NewStart becomes the new starting point and this modication is propagated: in case T 3 < NewStart, the retraction of validity over  T 3 NewStart] is propagated otherwise, the extension of validity over  NewStart T 3] is propagated.
retractForRightOverlap(P,_,T3,infPlus]):initiates_at(_,_,P,NewStart), \+broken_during(P,NewStart,infPlus]),!, retract(mholds_for(P,T3,infPlus])), assert(mholds_for(P,NewStart,infPlus])), ((T3 lt NewStart) -> propagateRetract(T3,NewStart],P) propagateAssert(NewStart,T3],P)).
4. vanishing if none of the above described situations applies, the MVI for P is fully retracted and this retraction is propagated: retractForRightOverlap(P,_,T3,T4]):retract(mholds_for(P,T3,T4])), propagateRetract(T3,T4],P).
In the case of invalidated terminations, there are four possible situations, which are handled by retractForLeftOverlap that is symmetrical to the previously described predicate  6].
In the case of propagation of assertions, we distinguish two relevant cases: (i) possibly new initiations the asserted property AssertedProp is a condition for the initiation of property P at the occurrence of event E , the occurrence time T of E belongs to  T 1 T 2] and there is not already a MVI for P with T as its starting point.
In this case, the already described updateInit predicate is used to check if P is now initiated at T and possibly revising the database accordingly.
propagateAssert(T1,T2],AssertedProp):clause(initiates_at(E,PropList,P,_),_), memberchk(AssertedProp,PropList), happens_at(E,T), T1 lt T, T le T2, \+mholds_for(P,T,_]), updateInit(E,T,P), fail.
(ii) possibly new terminations the asserted property AssertedProp is a condition for the termination of property P at the occurrence of event E , the occurrence time T of E belongs to  T 1 T 2] and there is not already a MVI for P with T as its ending point.
In this case, the already described updateTermin predicate is used to check if P is now terminated at T and possibly revising the database accordingly.
propagateAssert(T1,T2],AssertedProp):clause(terminates_at(E,PropList,P,_),_), memberchk(AssertedProp,PropList), happens_at(E,T),  T1 lt T, T le T2, \+mholds_for(P,_,T]), updateTermin(E,T,P), fail.
As in the case of propagateRetract, the fail predicate has been used to force backtracking in order to examine all aected properties.
Therefore, a third clause guarantees success as before: propagateAssert(_,_).
3.3 Running the lighting system example in CEC  We will now show how CEC builds and maintains the set of cached MVIs, by examining one execution of the lighting system example.
Suppose to perform the following updates in the shown order, that is not chronological: update(initially(pwrAvail),0), update(turnOff,8), update(turnOn,4), update(turnOn,10), update(pwrFail,6), update(pwrRstr,12).
Figure 2 illustrates pictorially the eects of each of the six updates.
The eect of the rst update is to initiate a pwrAvail property that holds between 0 and infPlus (extendingI with initial conditions).
Propagation of the assertion of this property has no eect because there are no inuenced events in the database.
The eects of the second update are to terminate: (i) a lightsOn interval that holds between infMin and 8 and (ii) a switchOn interval that also holds between infMin and 8 (extendingT in both cases).
Propagation of assertion for both properties has no eect because there are no other events in the interval of time considered by propagation.
The eects of the third update are: (i) the MVI of property lightsOn is broken at 4, and validity between infMin and 4 is thus retracted and (ii) the MVI of property switchOn is also broken at 4, and validity between infMin and 4 is thus retracted (brokenI in both cases).
Propagation of retraction for both properties has no eect because there are no other events in the interval of time considered by propagation.
The eects of the fourth update are to initiate: (i) a lightsOn interval that holds between 10 and infPlus and (ii) a switchOn interval that also holds between 10 and infPlus (extendingI in both cases).
Propagation of assertion for both properties has no eect because there are no other events in the interval of time considered by propagation.
The eects of the fth update are: (i) the MVI of property pwrAvail is broken at 6 and validity between 6 and infPlus is thus retracted (brokenT), (ii) retraction is propagated and leads to reconsider the eects of event turnOn at 10, leading to the retraction of property LightsOn holding between 10 and infPlus (vanishing), (iii) the eects of event turnO at 8 are then reconsidered and property LightsOn is retracted between 6 and 8 (revised termination with nite initiation).
Propagation of the retractions of lightsOn  have no eect because there are no inuenced events.
The eects of the sixth update are to initiate: (i) a pwrAvail interval that holds between 12 and infPlus and (ii) a lightsOn interval that also holds between 12 and infPlus (extendingI in both cases).
Propagation of assertion for both properties has no eect because there are no other events in the interval of time considered by propagation.
As already pointed out, changing the order of execution of the six updates has no inuence on the nal contents of the database, but could aect eciency.
In particular, if the complete sequence of events was entered chronologically, there would be no need for a substantial revision of the database as that caused by the fth update.
4 Complexity Analysis  In this section we analyze the complexity of executing EC and CEC with an ordinary Prolog intepreter.
We focus on the execution of mholds for queries returning the full set of MVIs for a given property prop.
Such queries take the following form: ?- bagof(MVI,mholds_for(prop,MVI),MVIs).
We also assume that the database contains a set of n initiating events and n terminating events for any property.
First of all, we determine the complexity of query processing in EC devoid of contextdependency, and show how EC performance heavily decreases when context-dependency is added.
Then, we prove how the addition of a caching mechanism strongly reduces the cost of query processing.
We show that the cost of query processing in CEC, with and without context-dependency, is linear, and that the complexity of CEC update processing is less than the complexity of EC query processing except in the case of context-independency where it is equal.
In all proofs we assume the strong interpretation of initiates at and terminates at predicates.
It is possible to show that in the case of weak interpretation the worst-case analysis leads to the same results.
We only sketch out the structure of proofs the details can be found in  6].
4.1 The complexity of query processing in EC  We initially consider the case of a database with only context-independent denitions of initiates at and terminates at.
It is possible to prove the following theorem.
Theorem 4.1  The complexity of EC query processing, measured in terms of accesses to happens at facts, is O(n3 ), where n is the number of initiating (terminating) events in the database for the considered property.
We performed the proof in two steps.
We rst determined a cubic upper bound (2n3 +n2 +n) for the total number of accesses to happens at facts in answering  update(initially(pwrAvail),0).
update(turnOn,10).turnOn initially(pwrAvail)  initially(pwrAvail)  pwrAvail  0  ++++++++++++++++++++++++ pwrAvail  8  4  0  lightsOn turnOn  10 ++++++++  turnOff  ++++++++ switchOn  update(turnOff,8).
update(pwrFail,6).
turnOff initially(pwrAvail) 0  pwrFail initially(pwrAvail)  pwrAvail  8  ++++++++++ lightsOn  ---------------------  pwrAvail  8  4  0  ---  lightsOn turnOn  ++++++++++ switchOn  turnOff  switchOn  update(turnOn,4).
pwrRstr  turnOn pwrAvail 4 -----lightsOn ------  turnOn  update(pwrRstr,12).
initially(pwrAvail)  initially(pwrAvail) 0  10 -----------  ++++  pwrAvail 8 turnOff  4  0  6  = valid initiation/termination of properties = invalidated initiation/termination = new initiation/termination  10  lightsOn turnOn switchOn  switchOn  8  turnOff  12 ++++  turnOn  = maximal validity interval (MVI) - - - - - - = retraction of validity + + + + + = assertion of validity  Figure 2: A sample execution of CEC.
mholds for queries with only the property argument instantiated.
Then we showed that the cubic limit is actually reached.
In order to determine the complexity of query processing in EC with context-dependency some preliminary notions are necessary.
We must take into account that a condition of a context-dependent initiates at or terminates at may itself be contextdependent.
Therefore, we must consider in general an arbitrary nesting level of context-dependent properties.
Let Lbk be the maximum level of nesting from a single property.
To formally dene Lbk , we introduce the notion of property dependency graph associated with a set of initiates at and terminates at clauses.
A property dependency graph is a directed acyclic graph where4 : (i) each vertex denotes a property p The requirement that the graph is acyclic is needed to make it possible the comparison between EC and CEC complexities.
EC indeed loops whenever there is a cycle among context-dependent properties.
On the contrary, there exist sucient conditions for the termination of CEC also in case of property cycles (the main condition is that at each time instant at most one of the properties involved in the property cycle may hold).
Examples of non-critical property cycles in a medical application domain are given in 8].
The general problem of looping in EC and CEC is currently under investigation.
4  (ii) there exists an edge (pj  pi ) if and only if there exists an initiates at clause for pi having pj as one of its conditions or a terminates at clause for pi having pj as one of its conditions.
Lbk is dened as the length of the longest path in the graph.
It is possible to prove the following theorem by induction on the nesting level of context-dependent properties Lbk .
Theorem 4.2  The complexity Comp(query(Lbk )) of EC query processing, measured in terms of accesses to happens at facts, is O(n(Lbk +1) 3 ), where n is the number of initiating (terminating) events in the database for the considered property.
If we indeed consider Lbk = 0, we fall in the already discussed case of the basic calculus, whose complexity has been shown to be O(n3 ).
What happens when Lbk is 1 or more is that the evaluation of each condition in the initiates at or terminates at predicates for p will result in the evaluation of a mholds for predicate for this condition with the temporal argument unbound and a Lbk ; 1 nesting level.
We showed that the relationship between the complexity upper bounds for query(Lbk ) and query(Lbk ; 1) is expressed by the recurrent expression: Comp(query(Lbk )) = n2 2n (1+Cbk Comp(query(Lbk 1))) ;  were Cbk is the maximum number of conditions found in the context-dependent initiates at or terminates at clauses.
We then proved by induction that the order of complexity of query(Lbk ) is at most O(n(Lbk +1) 3 ).
As in the basic calculus, it is straightforward to show that this limit is actually reached5 .
4.2 The complexity of update processing in CEC  In the case of CEC, the complexity has to be measured in terms of accesses to both happens at and mholds for facts.
Dierently from EC, where each evaluation of a mholds for query results into a number of accesses to happens at facts, in CEC mholds for predicates are explicitly recorded in the database as facts.
Under the given assumption that there is a set of n initiating events and n terminating events for every property in the database, there are at most n disjoint MVIs for each property (n mholds for facts recorded in the database).
Thus all EC accesses to happens at facts for a mholds for query for a given property collapse into at most n CEC accesses to mholds for facts about such a property.
Therefore, the cost of a mholds for query in CEC is linear in the number of cached MVIs for the considered property, whatever is the value of Lbk .
Dierently from EC, the complexity of update processing in CEC is not constant at all.
To precisely determine such a complexity some preliminary notions are needed.
First of all, let P be the maximum number of properties initiated or terminated by a single event.
Furthermore, we must take into account that the assertion of a new mholds for fact (the retraction of an existing mholds for fact) may cause (suppress) the initiation or the termination of a property depending on it and then the assertion of an additional mholds for fact (the retraction of an existing mholds for fact) concerning such a property.
Therefore, we must consider in general an arbitrary level of propagation of assertions (retractions).
Let Lfw be the maximum level of propagation from a single property.
Lfw can be formally dened as the length of the longest path in the property dependency graph, and then it it is equal to Lbk 6 .
It is possible to prove the following lemma by induction on the propagation level Lfw .
Lemma 4.3  The complexity of propagating assertions, measured in terms of accesses to happens at and mholds for facts, is O(nLfw +3 ), where n is the number of initiating (terminating) events in the database for the It is worth noting that this is a worst case analysis.
In fact, Cbk and Lbk can be redened for each single property to evaluate the complexity of specic queries or classes of queries.
6 Note that if Lbk and Lf w are redened to account for nesting and propagation levels of specic properties, they are not necessarily equal anymore.
5  considered property, and it is equal to the complexity of propagating retractions.
The proof is accomplished in three steps.
We rst determine the recurrent expression of the costs of propagateRetract and propagateAssert with respect to a level of propagation Lfw in terms of their costs with respect to the level Lfw ; 1 then we prove that the cost of propagateRetract is always less than the cost of propagateAssert nally we provide the general cost expressions of propagateRetract and propagateAssert.
On the basis of Lemma 4.3, it is straightforward to prove the following theorem about the complexity of update processing in CEC.
Theorem 4.4  The complexity Comp(update(Lfw )) of CEC update processing, measured in terms of accesses to happens at and mholds for facts, is O(nLfw +3 ), where n is the number of initiating (terminating) events in the database for the considered property.
It is worth noting that in the context-independent case (Lfw = 0), the complexity of update processing is O(n3 ).
In such a case there are neither propagation of assertions nor propagation of retractions, and the worst-case complexity of update processing is just the complexity of adding at most P new mholds for facts.
The orders of complexity of query and update processing in EC and CEC are summarized in Table 1.
5 Conclusions  This paper has proposed a caching mechanism for an ecient implementation of Kowalski and Sergot's Event Calculus (EC).
In the context-independent case (Lbk = Lfw = 0), the cached version of EC we developed (CEC) makes the complexity of query processing linear, shifting temporal reasoning from query to update processing.
The complexity of update processing in CEC is indeed equal to the complexity of query processing in EC.
In the more significant context-dependent case (Lbk = Lfw 1), CEC allows us to obtain an absolute improvement in performance because the order of complexity of update processing in CEC (Lfw + 3) is strictly lower than the order of complexity of query processing in EC ((Lbk + 1)  3).
CEC has been fully implemented on a Sun Sparc2 in Quintus Prolog and extensively tested in order to ensure that it yields the same outputs as EC (but much more eciently).
Acknowledgements  We would like to thank one of the anonymous referees for the useful and perceptive comments.
This work has been partially supported by the Italian National Research Council (Special Project on Management of Temporal Information in Data and Knowledge Bases), and the P.A.O.L.A.
Consortium, whose members are ASEM Resolution, INSIEL, and Universita di Udine.
EC update  EC query  CEC update CEC query  Lbk = Lfw = 0  const  O (n 3 )  O(n3 )  O(n)  Lbk = Lfw = 1  const  O (n 6 )  O(n4 )  O(n)  Lbk = Lfw = 2  const  O (n 9 )  O(n5 )  O(n)  :::  :::  :::  :::  :::  Lbk = Lfw = k  const  O(n(k+1) 3 )  O(nk+3 )  O(n)  Table 1: Comparing complexities of EC and CEC.
References  1] J. Allen, Planning as Temporal Reasoning in Proc.
of KR-91, Cambridge, MA, pp.3{14.
2] M. Baudinet, J. Chomicki, P. Wolper, Temporal Deductive Databases.
Chapter 13 in 21], pp.
294{320.
3] M. Boddy, Temporal Reasoning for Planning and Scheduling, SIGART Bulletin, Vol.4, No.3, 1993, pp.1720.
4] I. Cervesato, A. Montanari, A. Provetti, On the Nonmonotonic Behavior of Event Calculus for Deriving Maximal Time-Intervals to appear in The International Journal of Interval Computations, 1994.
5] L. Chittaro, A. Montanari, Experimenting a Temporal Logic for Executable Specications in an Engineering Domain, in G. Rzevski, J. Pastor, R.A. Adey (eds), Applications of Articial Intelligence in Engineering VIII, Computational Mechanics Publications & Elsevier Applied Science, Boston and London, 1993, pp.
185{202.
6] L. Chittaro, A. Montanari, Facing E	ciency and Looping Problems of the Event Calculus through Caching.
Part I: E	ciency Research Report RR18/93, Dipartimento di Matematica e Informatica, Universita di Udine, November 1993.
7] L. Chittaro, A. Montanari, A. Provetti, Skeptical and Credulous Event Calculi for Supporting Modal Queries to appear in Proc.
of ECAI'94, Amsterdam, The Netherlands, Wiley & Sons Publishers, August 1994.
8] L. Chittaro, A. Montanari, M. Dojat, C. Gasparini, The Event Calculus at work: a Case Study in the Medical Domain (submitted), 1994.
9] T. Dean, D. McDermott, Temporal Data Base Management Articial Intelligence, Vol.
32, 1987, pp.
1{55.
10] T. Dean, Using Temporal Hierarchies to E	ciently Mantain Large Temporal Databases Journal of the ACM, Vol.
36, No.
4, October 1989, pp.
687{718.
11] R. Kowalski, M. Sergot, A Logic-based Calculus of Events New Generation Computing, 4, 1986, pp.
67{ 95.
12] R. Kowalski, Database Updates in the Event Calculus Journal of Logic Programming, Vol.
12, June 1992, pp.
121{146.
13] A. Montanari, E. Maim, E. Ciapessoni, E. Ratto, Dealing with Time Granularity in the Event Calculus Proceedings FGCS-92, Fifth Generation Computer Systems, Tokyo, Japan, IOS Press, 1992, pp.
702{712.
14] A. Montanari, B. Pernici, Temporal Reasoning.
Chapter 21 in 21], pp.
534{562.
15] J. Pinto, R. Reiter, Temporal Reasoning in Logic Programming: A Case for the Situation Calculus, Proc.
ICLP'93, Budapest, Hungary, 1993, pp.
203-221.
16] R. Reiter, Proving Properties of States in the Situation Calculus Articial Intelligence, Vol.
64, no.2, 1993, pp.
337-351.
17] M. Shanahan, Prediction is Deduction, but Explanation is Abduction Proc.
of IJCAI'89, Detroit, The MIT Press, 1989, pp.
1055{1060.
18] M. Shanahan, Representing Continuous Change in the Event Calculus Proc.
of ECAI'90, Stockholm, Sweden, 1990, pp.
598{603.
19] M. Sergot, (Some Topics in) Logic Programming in AI Advanced School on Foundations of Logic Programming, Alghero, Italy, 1990.
20] S. Sripada, Temporal Reasoning in Deductive Databases PhD thesis in Computing, Imperial College, London, 1990.
21] A. Tansell, J. Cliord, S. Gadia, S. Jajodia, A. Segev, and R. Snodgrass (eds.
), Temporal Databases: Theory, Design and Implementation, The Benjamin/Cummings Series on Database Systems and Applications, Benjamin/Cummings Publishers, 1993, pp.
534{562.

Pre-Processing Time Constraints for Efficiently Mining Generalized Sequential Patterns Florent Masseglia INRIA Sophia Antipolis AxIS Research Team 2004 rte des lucioles 06902 Sophia Antipolis, France Florent.Masseglia@inria.fr  Pascal Poncelet Laboratoire LGI2P Ecole des Mines d'Ales Parc Sc.
Georges Besse 30035 Nimes, France Pascal.Poncelet@ema.fr  Abstract In this paper we consider the problem of discovering sequential patterns by handling time constraints.
While sequential patterns could be seen as temporal relationships between facts embedded in the database, generalized sequential patterns aim at providing the end user with a more flexible handling of the transactions embedded in the database.
We propose a new efficient algorithm, called GTC (Graph for Time Constraints) for mining such patterns in very large databases.
It is based on the idea that handling time constraints in the earlier stage of the algorithm can be highly beneficial since it minimizes computational costs by preprocessing data sequences.
Our test shows that the proposed algorithm performs significantly faster than a stateof-the-art sequence mining algorithm.
Maguelonne Teisseire LIRMM UMR CNRS 5506 161 Rue Ada 34392 Montpellier, France Teisseire@lirmm.fr  algorithm, called GSP was proposed.
In this paper, we propose a new efficient algorithm, called GTC (Graph for Time Constraints) for mining generalized sequential patterns in large databases.
GTC minimizes computational costs by using a data-sequence preprocessing operation that takes time constraints into account.
The main new feature of GTC is that time constraints are handled prior to and separately from the counting step of a data sequence.
The rest of this paper is organized as follows.
In Section 2, the problem of mining generalized sequential patterns is stated and illustrated.
A review of related work is presented in Section 3.
The reasons for our choices are discussed in Section 4.
The GTC algorithm for efficiently discovering all generalized sequential patterns is given in Section 5.
Section 6 presents the detailed experiments using both synthetic and real datasets, and performance results obtained are interpreted.
Section 7 concludes the paper.
1.
Introduction Although sequential patterns are of great practical importance (e.g.
alarms in telecommunications networks, identifying plan failures, analysis of Web access databases, etc.
), in the literature, they have received relatively little attention [1, 6, 4, 5, 2].
They could be seen as temporal relationships between facts embedded in the database.
A sequential pattern could be "5% of customers bought 'Foundation', then 'Foundation and Empire', and then 'Second Foundation"'.
The problem of discovering sequential patterns in databases, introduced in [1], is to find all patterns verifying user-specified minimum support, where the support of a sequential pattern is the percentage of data-sequences that contain the pattern.
Such patterns are called frequent patterns.
In [6], the definition of the problem is extended by handling time constraints and taxonomies (is-a hierarchies) and a new  2.
From Sequential Patterns to Generalized Sequential Patterns First of all, we assume that we are given a database DB of customers' transactions, each of which has the following characteristics: sequence-id or customer-id, transactiontime and the items involved in the transaction.
Such a database is called a base of data sequences (Cf.
Figure 4).
Definition 1 Let I = {i1 , i2 , ..., im } be a set of literals called items.
An itemset is a non-empty set of items.
A sequence s is a set of itemsets ordered according to their timestamp.
It is denoted by < s1 s2 ...sn > where sj is an itemset.
A k-sequence is a sequence of k-items (or of length k).
A sequence < s1 s2 ...sn > is a sub-sequence of another sequence < s01 s02 ...s0m > if there exist integers i1 < i2 < ... < in such that s1 [?]
s0i1 , s2 [?]
s0i2 , ...sn [?]
s0in .
Example 1 Let us consider that a given customer purchased items 10, 20, 30, 40, 50, according to the following sequence: s =< (10) (20 30) (40) (50)>.
This means that, apart from 20 and 30 which were purchased together, i.e.
during a common transaction, the items in the sequence were bought separately.
The sequence s0 = < (20) (50) > is a sub-sequence of s because (20) [?]
(20 30) and (50)[?]
(50).
However < (20) (30) > is not a sub-sequence of s since items were not bought during the same transaction 2 In order to efficiently aid decision making, the aim is to discard non-typical behaviors according to the user's viewpoint.
Performing such a task requires providing any data sub-sequence s in the DB with a support value (supp(s)) giving its number of actual occurrences in the DB1 .
This means comparing any sub-sequence with the whole DB.
In order to decide whether a sequence is frequent or not, a minimum support value (minSupp) is specified by the user, and the sequence s is said to be frequent if the condition supp(s) >= minSupp holds.
This definition of sequence is not appropriate for many applications, since time constraints are not handled.
In order to improve the definition, generalized sequential patterns are introduced [6].
When verifying whether a sequence is included in another one, transaction cutting enforces a strong constraint since only pairs of itemsets are compared.
The notion of the sized sliding window enables that constraint to be relaxed.
More precisely, the user can decide that it does not matter if items were purchased separately as long as their occurrences enfold within a given time window.
Thus, when browsing the DB in order to compare a sequence s, assumed to be a pattern, with all data sequences d in DB, itemsets in d could be grouped together with respect to the sliding window.
Thus transaction cutting in d could be resized when verifying if d matches with s. Moreover when exhibiting from the data sequence d, subsequences possibly matching with the assumed pattern, non-adjacent itemsets in d could be picked up successively.
Minimum and maximum time gaps are introduced to constrain such a construction.
In fact, to be successively picked up, two itemsets must occur neither too close nor too far apart in time.
More precisely, the difference between their time-stamps must fit in the range [min-gap, max-gap].
Window size and time constraints as well as the minimum support condition are parameterized by the user.
Mining se1  A sequence in a data-sequence is taken into account once and once only for computing the support of a frequent sequence even if several occurrences are discovered.
quences with time constraints allows a more flexible handling of the transactions.
We now define frequent sequences when handling time constraints: Definition 2 Given a user-specified minimum time gap (minGap), maximum time gap (maxGap) and a time window size (windowSize), a data-sequence d =< d1 ...dm > is said to support a sequence s =< s1 ...sn > if there exist integers l1 <= u1 < l2 <= u2 < ... < ln <= un such i that: (i) si is contained in [?
]uk=l dk , 1 <= i <= n; (ii) i transaction-time (dui ) - transaction-time (dli ) <= ws, 1 <= i <= n; (iii) transaction-time (dli ) - transaction-time (dui-1 ) > minGap, 2 <= i <= n; (iv) transaction-time (dui ) - transaction-time (dli-1 ) <= maxGap, 2 <= i <= n. The support of s, supp(s), is the fraction of all sub-sequences in DB supporting s. When supp(s) >= minSupp holds, being given a minimum support value minSupp, the sequence s is called frequent.
Example 2 As an illustration for the time constraints, let us consider the following data-sequence describing the purchased items for a customer: Date 01/01/2000 02/02/2000 03/02/2000 04/02/2000 05/02/2000  Items 10 20, 30 40 50, 60 70  In other words, the data-sequence d could be considered as follows: d =< (10)1 (20 30)2 (40)3 (50 60)4 (70)5 > where each itemset is stamped by its access day.
For instance, (50 60)4 means that the items 50 and 60 were purchased together at time 4.
Let us now consider a candidate sequence c=< (10 20 30 40) (50 60 70) > and time constraints specified such as windowSize=3, minGap=0 and maxGap=5.
The candidate sequence c is considered as included in the datasequence d for the following two reasons: 1. the windowSize parameter makes it possible to gather together on the one hand the itemsets (10) (20 30) and (40), and on the other hand the itemsets (50 60) and (70) in order to obtain the itemsets (10 20 30 40) and (50 60 70), 2. the minGap constraint between the itemsets (40) and (50 60) holds.
Considering the integers li and ui in the Definition 2, we have l1 = 1, u1 = 3, l2 = 4, u2 = 5 and the data sequence d is handled as illustrated in Figure 1.  maxGap  <  l1 1  [ (10) 2 (20 30) 3 (40) u1 ] l2 [ 4 (50 60) 5 (70) u2 ] > minGap  Figure 1.
Illustration of the time constraints  maxGap  maxGap  < (10)1 (20 30)2  (40)3  (50 60)4 (70)5 >  windowSize minGap  windowSize minGap  Figure 2.
Illustration of the time constraints In a similar way, the candidate sequence c=< (10 20 30) (40) (50 60 70) > with windowSize=1, minGap=0 and maxGap=2, i.e.
l1 = 1, u1 = 2, l2 = 3, u2 = 3, l3 = 4 and u3 = 5 (C.f.
Figure 2) is included in the data-sequence d. The two following sequences c1 =< (10 20 30 40) (70) > and c2 = < (10 20 30) (60 70) >, with windowSize=1, minGap=3 and maxGap=4 are not included in the datasequence d. Concerning the former, the windowSize is not large enough to gather together the itemsets (10) (20 30) and (40).
For the latter, the only possibility for yielding both (10 20 30) and (60 70) is to take into account ws for achieving the following grouped itemsets [(10) (20 30)] and [(50 60) (70)].
maxGap is respected since [(10) (20 30)] and [(50 60) (70)] are spaced 4 days apart(u2 = 5, l1 = 1).
Nevertheless, in such a case minGap constraint is no longer respected between the two itemsets because they are only 2 days apart (l2 = 4 and u1 = 2) whereas minGap was set to 3 days 2 Given a database of data sequences, user-specified minGap and maxGap constraints and a user-specified sliding windowSize, the generalized sequential problem is to find all the sequences whose support is greater than the userspecified minSupp.
3.
Related Work In the following section, we review the most important work carried out within a sequential pattern framework.
Since they consider the generalized sequence problem and as they are the basic of our approach, particular emphasis is placed on the GSP [6] and PSP [3] algorithms.
The concept of sequential pattern is introduced to capture typical behaviors over time, i.e.
behaviors repeated sufficiently often by individuals to be relevant for the decision maker.
The GSP algorithm, proposed in [6], is intended for mining Generalized Sequential Patterns.
It extends previous proposal by handling time constraints and taxonomies (is-a hierarchies).
For building up candidate and frequent sequences, the GSP algorithm performs several iterative steps such as the k th step handles sets of k-sequences which could be candidate (the set is noted Ck ) or frequent (in Lk ).
The latter set, called the seed set, is used by the following step which, in turn, results in a new seed set encompassing longer sequences, and so on.
The first step aims at computing the support of each item in the database.
When completed, frequent items (i.e.
satisfying the minimum support) are discovered.
They are considered as frequent 1-sequences (sequences having a single itemset, itself being a singleton).
This initial seed set is the starting point of the second step.
The set of candidate 2-sequences is built according to the following assumption: candidate 2-sequences could be any pair of frequent items, embedded in the same transaction or not.
From this point, any step k is given a seed set of frequent (k-1)-sequences and it operates by performing the two following sub-steps: * The first sub-step (join phase) addresses candidate generation.
The main idea is to retrieve, among sequences in Lk-1 , pairs of sequences (s, s0 ) such that discarding the first element of the former and the last element of the latter results in two fully matching sequences.
When such a condition holds for a pair (s, s0 ), a new candidate sequence is built by appending the last item of s0 to s. * The second sub-step is called the prune phase.
Its objective is yielding the set of frequent k-sequences Lk .
Lk is achieved by discarding from Ck , sequences not satisfying the minimum support.
To yield such a result, it is necessary to count the number of actual occurrences matching with any possible candidate sequence.
Candidate sequences are organized within a hash-tree data-structure which can be accessed efficiently.
These sequences are stored in the leaves of the tree while intermediary nodes contain hashtables.
Each data-sequence d is hashed to find the candidates contained in d. When browsing a data sequence, time constraints must be managed.
This is performed by navigating downward or upward through the tree, resulting in a set of possible candidates.
For each candidate, GSP checks whether it is contained in the data-sequence.
Because of the sliding window, and minimum and maximum time gaps, it is necessary to switch during examination between forward and  backward phases.
Forward phases are performed for progressively dealing with items.
Accordingly, in earlier work [3], we proposed a new approach called PSP, Prefix-Tree for Sequential Patterns, which fully resumes the fundamental principles of GSP.
Its originality is to use a different hierarchical structure than in GSP for organizing candidate sequences, in order to improve retrieval efficiency.
In the hash-tree structure managed by GSP, the transaction cutting is not captured.
The main drawback of this approach is that when a leaf of the tree is obtained, an additional phase is necessary in order to check time constraints for all sequences embedded in the leaf.
The tree structure, managed by PSP, is a prefix-tree.
At the k th step, the tree has a depth of k. It captures all the candidate k-sequences in the following way.
Any branch, from the root to a leaf stands for a candidate sequence, and considering a single branch, each node at depth l (k >= l) captures the lth item of the sequence.
Furthermore, along with an item, a terminal node provides the support of the sequence from the root to the considered leaf (included).
Transaction cutting is captured by using labeled edges.
Example 3 Let us assume that we are given the following set of frequent 2-sequences: L2 = {< (10) (30) >, < (10) (40) >, < (30) (20) >, < (30 40) >, < (40 10) >}.
It is organized according to our tree structure as depicted in Figure 3.
Each terminal node contains an item and a counting value.
If we consider the node having the item 40, its associated value 2 means that two occurrences of the sequence {< (10) (40) >} have been detected so far 2  root a   @aaa    @ a 20 10 30 40 A   A  301 402 202 402 103 Figure 3.
The PSP Tree data structure  the base given in Figure 4.
Client C1 C1 C1 C1 C1  Date 01/04/2000 03/04/2000 04/04/2000 07/04/2000 17/04/2000  Item 1 2 3 4 5  Figure 4.
A database example  Let us assume that windowSize = 1 and minGap=1.
Upon reading the customer transaction, PSP has to determine all combinations of the data sequence in accordance with the time constraints in order to increment the support of a candidate sequence.
Then the set of sequences verifying time constraints is the following: without time constraints <(1)> ... <(5)> ... < ( 1 ) ( 2 )> ... < ( 3 ) ( 4 )> ... <(1)(2)(3)(4)(5)>  with windowSize & minGap <(2)(5)>* ... <(1)(3)(5)>* ... <(1)(23)(5)>* ... < ( 1 ) ( 2 ) ( 4 ) ( 5 )> * < ( 1 ) ( 3 ) ( 4 ) (5)> * < ( 1 ) ( 2 3 ) ( 4 ) ( 5 )> *  We notice that the sequences marked by a * are included in the sequence marked by a *.
That is to say that if a candidate sequence is supported by < ( 1 ) ( 2 ) ( 4 ) ( 5 ) > or < ( 1 ) ( 3 ) ( 4 ) ( 5 )> then such a sequence is also supported by < ( 1 ) ( 2 3 ) ( 4 ) ( 5 ) >.
The test of the two first sequences is of course useless because they are included in a larger sequence.
Client C1 C1 C1 C1 C1 C1  Date 01/04/2000 07/04/2000 13/04/2000 17/04/2000 18/04/2000 24/04/2000  Item 1 2 3 4 5 6  Figure 5.
A database example  4.
Motivations The PSP approach outperforms GSP by using a more efficient data structure.
If such a structure seems appropriate to the problem of mining generalized sequential patterns, it seems, on the other hand, that the algorithm can be improved by paying particular attention to time constraint handling.
To illustrate, let us consider the following customer data sequence < ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) > of  Let us now have a closer look at the problem of the windowSize constraint.
In fact, the number of included sequences is much greater when considering such a constraint.
For instance, let us consider the database given in Figure 5.
When windowSize=5 and minGap=1, PSP has to test the following sequences into the candidate tree structure (we only report sequences when windowSize is applied):  <(1)(2)(3)(5)(6)>* <(1)(2)(3)(4)(6)>* <(1)(2)(3)(45)(6)>* <(1)(2)(34)(6)>* <(1)(2)(345)(6)>  We notice that the sequences marked by a * (resp.
*) are included in the sequence marked by a * (resp.
).
That is to say that we have only to consider the two following sequences < ( 1 ) ( 2 ) ( 3 ) ( 4 5 ) ( 6 ) > and < ( 1 ) ( 2 ) ( 3 4 5 ) ( 6 ) > when verifying a data sequence in the candidate tree structure.
In fact, we need to solve the following problem: how to reduce the time required for comparing a data sequence with the candidate sequences.
Our proposition, described in the following section, is to precalculate, by means of the GTC (Graph for Time Constraints) algorithm, a relevant set of sequences to be tested for a data sequence.
By precalculating this set, we can reduce the time spent analysizing a data sequence when verifying candidate sequences stored in the tree, in the following two ways: (i) The navigation through the candidate sequence tree does not depend on the time constraints defined by the user.
This implies navigation without backtracking and better analysis of possible combinations of windowSize which are for PSP, as well as for GSP, computed on the fly.
(ii) This navigation is only performed on the longest sequences, that is to say on sequences not included in other sequences.
5.
GTC: Graph for Time Constraints Our approach takes up all the fundamental principles of GSP.
It contains a number of iterations.
The iterations start at the size-one sequences and, at each iteration, all the frequent sequences of the same size are found.
Moreover, at each iteration, the candidate sets are generated from the frequent sequences found at the previous iteration.
The main new feature of our approach which distinguish it from GSP and PSP is that handling of time constraint is done prior to and separate from the counting step of a data sequence.
Upon reading a customer transaction d in the counting phase of pass k, GTC has to determine all the maximal combinations of d respecting time constraints.
For instance, in the previous example, only < ( 1 ) ( 2 ) ( 3 ) ( 4 5 ) ( 6 ) > and < ( 1 ) ( 2 ) ( 3 4 5 ) ( 6 ) > are exhibited by GTC.
Then the Main algorithm has to determine all the k-candidates supported by the maximal sequences issued from GTC iterations on d and increment the support counters associated with these candidates without considering time constraints any more.
In the following sections, we decompose the problem of discovering non-included sequences respecting time constraints into the following subproblems.
First we consider the problem of the minGap constraint without taking into account maxGap or windowSize and we propose an algorithm called GTCminGap for handling efficiently such a constraint.
Second, we extend the previous algorithm in order to handle the minGap and windowSize constraints.
This algorithm is called GTCws .
Finally, we propose an extension of GTCws , called GTC, for discovering the set of all non included sequences when all the time constraints are applied.
5.1.
GTCminGap Algorithm: solution for minGap In this section, we explain how the GTCminGap algorithm provides an optimal solution to the problem of handling the minGap constraint.
1 r  2 - r/  R3 r  / / / / / / / /  4 - r  5 - r   Figure 6.
A data sequence representation  To illustrate, let us consider Figure 6, which represents the data sequence of the base given in Figure 4.
We note, / / / / / / / / , the minGap constraint between two itemsets a and b.
Let us consider that minGap is set to 1.
As items 2 and 3 are too closed according to minGap, they cannot occur together in a candidate sequence.
So, from this graph, only two sequences (< (1) (2) (4) (5) > and < (1) (3) (4) (5) >) are useful in order to verify candidate sequences.
We can observe that these two sequences match the two paths of the graph beginning from vertex 1 (standing for source vertex) and ending in vertex 5 (standing for sink vertex).
From each sequence d, a sequence graph can be built.
A sequence graph for d is a directed acyclic graph Gd (V, E) where a vertex v, v [?]
V , is an itemset embedded in d and an edge e, e [?]
E, from two vertices u and v, denotes that u occurred before v with at least a gap greater than the minGap constraint.
A sequence path is a path from two vertices u and v such as u is a source and v is a sink.
Let us note SPd the set of all sequence paths.
In addition, Gd has to satisfy the following two conditions: 1.
No sequence path from Gd is included in any other sequence path from Gd .
2.
If a candidate sequence is supported by d, then such a candidate is included in a sequence path from Gd .
an itemset occurring before v. The algorithm operates by performing, for each itemset, the following two sub-steps:  Given a data sequence d and a minGap constraint, the sequential path problem is to find all the longest paths, i.e.
those not included in other ones, by verifying the following conditions:  1.
Propagation phase: the main idea is to retrieve the first itemset u by verifying (u.date() - v.date() > minGap)2 , i.e.
the first itemset for which the minGap constraint holds, in order to build the edge (u, v).
Then for each itemset z such as (z.date()-y.date() > minGap), the algorithm updates z.isP rec[x] indicating that v will reach z traversing the itemset u.
2.
"gap-jumping" phase: its objective is to yield the set of edges not provided by the previous phase.
Such edges (v, t) are defined as follows (t.date() - x.date() > minGap) and t.isP rec[x] 6= 1.
1.
[?
]s1, s2 [?]
SPd /s1 6[?]
s2.
2.
[?
]c [?]
As /d supports c, [?
]p [?]
SPd /p supports c where As stands for the set of candidate sequences.
3.
[?
]p [?]
SPd , [?
]c [?]
As /p supports c, then d supports c. The set of all sequences SPd is thus used to verify the actual support of candidate sequences.
Example 4 To illustrate, let us consider the graph in Figure 8 representing the application of GTCminGap to the database depicted in Figure 7.
Let us assume that the minGap value was set to 2.
According to the time constraint, the set of all sequence paths, SPd , is the following: SPd ={< (1) (2) (6) >, < (1) (3 4) (6) >, < (1) (5) >}.
From this set, the next step of the Main algorithm is to verify these sequences into the candidate tree structure without handling time constraints anymore 2  Client C1 C1 C1 C1 C1  Date 01/04/2000 05/04/2000 06/04/2000 07/04/2000 09/04/2000  2 - r/  R34 r/ /  / / / / / / / /  Item 1 2 34 5 6  R5 r/  / / / / / / /  The following theorem guarantees that, when applying GTCminGap , we are provided with a set of data sequences where the minGap constraint holds and where each yielded data sequence cannot be a sub-sequence of another one.
Theorem 1 The GTCminGap algorithm provides all the longest-paths verifying minGap.
Figure 7.
An example database  1 r  Once the GTCminGap has been applied to a data sequence d, the set of all sequences, SPd , for counting the support for candidate sequences is provided by navigating through the graph of all sequence paths.
R6 r  / / / / / / / /  / / / / / / / / / / / / / / / / / / / / / / / / /   Figure 8.
The example sequence graph with minGap =2  We now describe how the sequence graph is built by the GTCminGap algorithm.
Auxiliary data structure can be used to accomplish this task.
With each itemset v, the itemsets occurring before v are stored in a sorted array, v.isP rec of size |E|.
The array is a vector of boolean where 1 stands for  First, we prove that for each p, p0 [?]
SPd , p 6[?]
p0 .
Next we show that for each candidate sequence c supported by d, a sequence path in G supporting c is found.
Let us assume two sequence paths, s1, s2 [?]
SPd such as s1 [?]
s2.
That is to say that the subgraph depicted in Figure 9 is included in G. In other words, there is a path (a, .
.
.
, c) of length >= 2 and an edge (a, c).
If such a path (a, c) exists, we have c.isP rec[a] = 1.
Indeed we can have a path of length >= 1 from a to b either by an edge (a, b) or by a path (a, .
.
.
, b).
In the former case, c.isP rec[a] is updated by the statement c.isP rec[a] - 1, otherwise there is a vertex a0 in (a, .
.
.
, b) such as (a, a0 ) is included in the path.
In such a case c.isP rec[a] - 1 has already occurred when building the edge (a, a0 ).
Then, after building the path (a, .
.
.
, b, .
.
.
, c) we have c.isP rec[a] = 1 and the edge (a, c) is not built.
Clearly the sub-graph depicted in Figure 9 cannot be obtained after GTCminGap .
Finally we demonstrate that if a candidate sequence c is supported by d, there is a sequence path in SPd supporting c. In other words, we want to demonstrate that GTCminGap provides all the longest paths satisfying the minGap constraint.
The data sequence d is progressively browsed starting with its first item.
Then if an itemset x is embedded in a path satisfying the minGap constraint it is included in SPd .
We 2  where x.date() stands for the transaction time of the itemset x.  have previously noticed that all vertices are included into a path and for each p, p0 [?]
SPd , p 6[?]
p0 .
Furthermore if two paths (x, .
.
.
, y)(y 0 , .
.
.
, z) can be merged, the edge (y, y 0 ) is built when browsing the itemset y.  a r  b - r  Rc - r  Definition 3 An itemset i is included in another itemset j if and only if the following two conditions are satisfied:  Figure 9.
Minimal inclusion schema  * i.begin() >= j.begin(), * i.end() <= j.end().
5.2.
GTCws Algorithm: solution for minGap and windowSize In this section, we explain how the algorithm GTCws provides an optimal solution to the problem of handling minGap and windowSize.
As we have already noticed in Section 4, the problem of handling windowSize is much more complicated than handling minGap since the number of included sequences is much greater when considering such a constraint.
(1) r  (2) - r   (3) - r  (4 5) - r  then x and y can be "merged" into the same transaction.
The structure described above is thus extended to handle such an operation.
Each itemset, in the new structure, is provided by both the begin transaction date and the end transaction date.
These dates are obtained by using the v.begin() and v.end() functions.
(6) - r  - r (3 4 5)  Once the graph satisfying minGap is obtained, the algorithm detects inclusions in the following way: for each node x, the set of all its successors x.next must be exhibited.
For each node y in x.next, if y [?]
z, z [?]
x.next and y.next [?]
z.next then the node y can be pruned out from the graph.
The following theorem guarantees that, when applying GTCws , we are provided with a set of data sequences where the minGap and windowSize constraints hold and that each yielded data sequence cannot be a sub-sequence of another one.
Theorem 2 The GTCws algorithm provides all the longest paths verifying minGap and windowSize.
6   Figure 10.
A sequence graph obtained when considering windowSize  To take into account the windowSize constraint we extend the GTCminGap algorithm by generating coherent combinations of windowSize at the beginning of algorithm and, once the graph respecting minGap is obtained, inclusions are detected.
The result of this handling is illustrated by Figure 10, which represents the sequence graph of the database given in Figure 5 when windowSize=5 and minGap=1.
Due to lack of space, we no not provide the algorithm but we give an overview of the method.
To yield the set of all windowSize combinations, each vertex x of the graph is progressively browsed and the algorithm determines which vertex can possibly be merged with x.
In other words, when navigating through the graph, if a vertex y is such that y.date() - x.date() < windowSize,   b - r r  a  b b' Rcr - r  Figure 11.
An included path example  Theorem 1 shows that we do not have included data sequences when considering minGap.
Let us now examine the windowSize constraint in detail.
Let us consider two sequence paths s1 and s2 in Gd such that s1 [?]
s2 .
Figure 11 illustrates such an inclusion.
In the last phase of the GTCws algorithm, we examine for each vertex x of the graph, the set of its successors by using the x.next function.
So, for each vertex y in x.next, if y [?]
z, z [?]
x.next and y.next [?]
z.next, the vertex y is pruned out from the graph.
So, by construction, s1 cannot be in the graph.
5.3.
Solution for all time constraints In order to handle the maxGap constraint in the GTC algorithm, we have to consider the itemset time-stamps into  the graph previously obtained by GTCws .
Let us remember that, according to maxGap, a candidate sequence c is not included in a data sequence s if there exist two consecutive itemsets in c such that the gap between the transaction time of the first itemset (called li-1 in Definition 2) and the transaction time of the second itemset (called ui in Definition 2) in s is greater than maxGap.
According to this definition, when comparing candidates with a data sequence, we must find in a graph itemset, the time-stamp for each item since, due to windowSize, items can be gathered together.
In order to verify maxGap, the transaction time of the sub-itemset corresponding to the included itemset into the graph, must verify the maxGap delay from the preceding itemset as well as for the following itemset.
To illustrate, let us consider the database depicted in Figure 12.
Let us now consider, in Figure 13, the sequence graph obtained from the GTCws algorithm when windowSize was set to 1 and minGap was set to 0.
In order to determine if the candidate data sequence < ( 2 ) ( 4 5 ) ( 6 ) > is included into the graph, we have to examine the gap between item 2 and item 5 as well as between item 4 and item 6.
Nevertheless, the main problem is that, according to windowSize, itemset (3) and itemset (4 5) were gathered together into (3 4 5).We are led to determine the transaction time of each component in the resulting itemset.
Customer C1 C1 C1 C1  Date 01/01/2000 03/01/2000 04/01/2000 06/01/2000  Items 2 3 4 5 6  Figure 12.
A database example  ( 2) r  ( 3 4 5) - r  ( 6) - r  Figure 13.
Sequence graph obtained by GTCws  Before presenting how maxGap is taken into account in GTC, let us assume that we are provided with a sequence graph containing information about itemsets satisfying the maxGap constraint.
By using such an information the candidate verification can thus be improved as illustrated in the following example.
Example 5 Let us consider the sequence graph depicted in Figure 13.
Let us assume that we are provided with information about reachable vertices into the graph according to maxGap and that maxGap is set to 4 days.
Let us now consider how the detection of the inclusion of a candidate sequence within the sequence graph is processed.
Candidate itemset (2) and sequence graph itemset (2) are first compared by the algorithm.
As the maxGap constraint holds and (2) [?]
(2), the first itemset of the candidate sequence is included in the sequence graph and the process continues.
In order to verify the other components of the candidate sequence, we must know what is the next itemset ended by 5 in the sequence graph and verifying the maxGap delay.
In fact, when considering the last item of the following itemset, if we want to know if the maxGap constraint holds between the current itemset (2) and the following itemset in the candidate sequence, we have to consider the delay between the current itemset in the graph and the next itemset ended by 5 in this graph.
We considered that we are provided with such an information in the graph.
This information can thus be taken into account by the algorithm in order to directly reach the following itemset in the sequence graph (3 4 5) and compare it with the next itemset in the candidate sequence (4 5).
Until now, the candidate sequence is included into the sequence graph.
Nevertheless, for completeness, we have to find in the graph the next itemset ended by 6 and verifying that the delay between the transaction times of items 4 and 6 is lower than 4 days.
This condition occurs with the last itemset in the sequence graph.
At the end of the process, we can conclude that c is included in the sequence graph of d or more precisely that c is included in d. Let us now consider the same example but with a maxGap constraint set to 2.
Let us have a closer look at the second iteration.
As we considered that we are provided with information about maxGap into the graph, we know that there is no itemset such that it ends in 5 and it satisfies the maxGap constraint with item 2.
The process ends by concluding that the candidate sequence is not included into the data sequence and without navigating further through the candidate structure 2 Let us now describe how information about itemsets verifying maxGap is taken into account in GTC.
Each item in the graph is provided with an array indicating reachable vertices, according to maxGap.
Each array value is associated with a list of pointed nodes, which guarantees that the pointed node corresponds to an itemset ending by this value and that the delay between these two items is lower or equal to maxGap.
Candidate verification algorithms can thus find candidates included in the graph by using such information embedded in the array.
By means of pointed nodes, the maxGap constraint is considered during evaluation of candidate itemset.
6.
Experiments In this section, we present the performance results of our GTC algorithm.
The structure used for organizing candidate sequences is a prefix tree structure as in PSP.
All experiments were performed on a PC Station with a CPU clock rate at 1.8 GHz, 512M Bytes of main memory, Linux System and a 60G Bytes disk drive.
In order to assess the relative performance of the GTC algorithm and study its scale-up properties, we used different kinds of datasets.
Due to lack of space we only report some results obtained with one access log file.
It contains about 800K entries corresponding to the requests made during March of 2002 and its size is about 600 M Bytes.
There were 1500 distinct URLs referenced in the transactions and 15000 clients.
Figure 14.
Execution times and recursive calls (minGap=1)  Figure 14 shows experiments conducted on the different datasets using different windowSize ranges to get meaningful response times.
minGap was set to 1 and maxGap was set to [?].
Note the minimum support thresholds are adjusted to be as low as possible while retaining reasonable execution times.
Figure 14 clearly indicates that the performance gap between the two algorithms increases with increasing windowSize value.The reason is that during the candidate verification, PSP has to determine all combination of the data sequence according to minGap and windowSize constraints.
In fact, the more the value of windowSize increases, the more PSP carries out recursive calls in order to apply time constraints to the candidate structure.
According to these calls, the PSP algorithm operates a costly backtracking for examining the prefix tree structure.In order to illustrate correlation between the number of recursive calls and the execution times we compared the number of recursive calls needed by PSP and our algorithm and as expected, the number of recursive calls increases with the size of the windowSize parameter.Additional experiments  were performed in order to study performance in scale-up databases.
they showed that GTC scaled up linearly as the number of transactions is increased.
7.
Conclusion We considered the problem of discovering sequential patterns by handling time constraints.
We proposed a new algorithm called GTC based on the fundamental principles of PSP and GSP but in which time constraints are handled in the earlier stage of the algorithm in order to provide significant benefits.
Using synthetic datasets as well as real life databases, the performance of GTC was compared against that of PSP.
Our experiments showed that GTC performs significantly better than the state-ofthe-art approaches since the improvements achieved by GTC over the counting strategy employed by other approaches are two-fold: first, only maximal sequences are generated, and there is no need of an additional phase in order to count candidates included in a data sequence.
In order to take advantage of the behavior of the algorithm in the first scans on the database, we are designing a new algorithm called PG-Hybrid using the PSP approach for the two first passes on the database and GTC for the following scans.
First experiments are encouraging, even for short frequent sequences.
References [1] R. Agrawal and R. Srikant.
Mining Sequential Patterns.
In Proceedings of the 11th International Conference on Data Engineering (ICDE'95), Tapei, Taiwan (1995).
[2] J. Ayres, J. Gehrke, T. Yiu, and J. Flannick.
Sequential Pattern Mining Using Bitmap Representation.
In Proocedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Edmonton, Alberta, Canada (2002).
[3] F. Masseglia, F. Cathala, and P. Poncelet.
The PSP Approach for Mining Sequential Patterns.
In Proceedings of the 2nd European Symposium on Principles of Data Mining and Knowledge Discovery (PKDD'98), LNAI, Vol.
1510, pp.
176-184, Nantes, France (1998).
[4] F. Masseglia, P. Poncelet, and M. Teisseire.
Incremental Mining of Sequential Patterns in Large Databases.
Data and Knowledge Engineering, 46(1):97-121 (2003).
[5] J. Pei, J. Han, B. Mortazavi-Asl, H. Pinto, Q. Chen, U. Dayal, and MC.
Hsu.
PrefixSpan: Mining Sequential Patterns Efficiently by Prefix-Projected Pattern Growth.
In Proceedings of 17th International Conference on Data Engineering (ICDE'01) (2001).
[6] R. Srikant and R. Agrawal.
Mining Sequential Patterns: Generalizations and Performance Improvements.
In Proceedings of the 5th International Conference on Extending Database Technology (EDBT'96), pp.
3-17, Avignon, France (1996).
Data & Knowledge Engineering 44 (2003) 219a238 www.elsevier.com/locate/datak  Deciding LTL over Mazurkiewicz traces Benedikt Bollig a, Martin Leucker  b,*,1  a  b  Lehrstuhl faZur Informatik II, RWTH Aachen, 52074 Aachen, Germany Department of Computer and Information Science, University of Pennsylvania, Philadelphia PA 19104, USA Received 10 December 2001; accepted 3 July 2002  Abstract Linear temporal logic (LTL) has become a well established tool for specifying the dynamic behaviour of reactive systems with an interleaving semantics, and the automataatheoretic approach has proven to be a very useful mechanism for performing automatic veridZcation in this setting.
Especially alternating automata turned out to be a powerful tool in constructing edZcient yet simple to understand decision procedures and directly yield further on-the-dZy model checking procedures.
In this paper, we exhibit a decision procedure for LTL over Mazurkiewicz traces that generalises the classical automataatheoretic approach to a LTL interpreted no longer over sequences but certain partial orders.
SpecidZcally, we construct a (linear) alternating BaZ uchi automaton (ABA) accepting the set of linearisations of traces satisfying the formula at hand.
The salient point of our technique is to apply a notion of independence-rewriting to formulas of the logic.
Furthermore, we show that the class of linear and trace-consistent ABA corresponds exactly to LTL formulas over Mazurkiewicz traces, lifting a similar result from LaZ oding and Thomas formulated in the framework of LTL over words.
 2002 Elsevier Science B.V. All rights reserved.
Keywords: LTL; Model checking; Mazurkiewicz traces; Alternating automata  *  Corresponding author.
E-mail addresses: bollig@informatik.rwth-aachen.de (B. Bollig), leucker@cis.upenn.edu (M. Leucker).
1 Part of this work was done during the second authoras stay at BRICS and RWTH Aachen.
He is grateful for the hospitality and the overall support.
0169-023X/03/$ - see front matter  2002 Elsevier Science B.V. All rights reserved.
PII: S 0 1 6 9 - 0 2 3 X ( 0 2 ) 0 0 1 3 6 - 2  220  B. Bollig, M. Leucker / Data & Knowledge Engineering 44 (2003) 219a238  1.
Introduction Linear time temporal logic (LTL) as proposed by Pnueli [17] has become a well established tool for specifying the dynamic behaviour of distributed systems.
The traditional approach towards automatic program veridZcation is model checking specidZcations in LTL.
A basic feature of LTL has been that its formulas are interpreted over sequences.
Typically, such a sequence will model a computation of a system: a sequence of states visited by the system or a sequence of actions executed by the system during the course of the computation.
The automataatheoretic approach of Vardi and Wolper [22] for satisdZability checking has proven to be very useful and edZcient for performing the automatic program veridZcation.
In its purest form, this amounts to the construction of a BaZ uchi automaton accepting precisely the set of sequences that satisdZes the specidZcation expressed as an assertion of LTL.
In the last years, a shift towards the employment of alternating automata for dedZning decision procedures took place.
Alternating automata provide simple, edZcient, and easy to understand decision procedures.
They have proven to be useful for dedZning satisdZability algorithms for LTL over words [23], branching time logics [2,12] over dZnite transition systems, and the l-calculus over (indZnite) predZx-recognisable graphs [11].
The idea is that the states of the automaton are constructed essentially from the subformula closure of the specidZcation formula, and the automaton operates in a tableau-like fashion.
The satisdZability problem is then solved by checking whether the constructed automaton accepts any strings.
This approach forms the conceptual basis of many veridZcation algorithms.
Several tools (e.g., SPIN [8]) being employed in industry are built upon this translation from formulas to automata.
To improve performance, however, a number of substantial optimisations must be incorporated.
One observation is that the state space of the product automaton needs seldomly to be fully constructed.
Often the answer to the veridZcation problem can be established by investigating only a subset of states, and this subset might be considerably smaller than the entire state space.
This is the main idea underlying the so-called on-the-dZy veridZcation techniques.
To support on-the-dZy checking, an automaton corresponding to a formula should be dedZned in a topadown manner.
This means that, given a formula u and one of its subformulas w, a part of the automaton Au should be constructible without constructing Aw , where Ag denotes the automaton accepting the models of g. In this way, the automaton for a given formula and an underlying transition system may only be constructed partly viz if the model-checking or satisdZability question can already be answered by considering this part.
In many applications, the computations of a distributed system will constitute interleavings of the occurrences of causally independent actions.
Consequently, the computations can be naturally grouped together into equivalence classes where two computations are equated in case they are two didZerent interleavings of the same partially ordered stretch of behaviour.
It turns out that many of the properties expressed as LTL-formulas happen to have the so called aaall-or-noneaa property.
Either all members of an equivalence class of computations will have the desired property or none will do (aaleads to deadlockaa is one such property).
For verifying such properties, one has to check them for just one member of each equivalence class.
This is the insight underlying many of the partial-order based veridZcation methods (e.g., [16,21]).
As may be guessed, the importance of these methods lies in the fact that via these methods  B. Bollig, M. Leucker / Data & Knowledge Engineering 44 (2003) 219a238  221  the computational resources required for the veridZcation task can often be dramatically reduced.
Often, the equivalence classes of computations generated by a distributed system constitute objects called Mazurkiewicz traces [4,15].
They can be canonically represented as restricted labelled partial orders.
This opens up an alternative way of exploiting the non-sequential nature of the computations of a distributed system and the attendant partial-order based methods.
It consists of developing LTLs that can be interpreted directly over Mazurkiewicz traces.
In these logics, every specidZcation is guaranteed to have the aaall-or-noneaa property and hence can be subjected to the partial-order based reduction methods during the veridZcation process.
A number of LTLs to be interpreted over Mazurkiewicz traces directly (e.g., [1,19,20]) has been proposed starting with TrPTL [19].
There are several possible routes towards extending LTLs to traces.
TrPTL is based on locations, where one reasons explicitly about a distribution of computing agents cooperating through some communication structure given as an alphabet distribution.
Another option [1] is to view events as the partial-order computation points in time, and base the specidZcations upon the relationship between individual events.
Together, these paradigms constitute the local trace logics.
In contrast, in the global view of computations, condZgurations are seen as instantaneous snapshots of the system at hand.
In this sense, a condZguration is a global view capturing a collection of simultaneous local views.
The aarightaa temporal logic for traces should be equal in expressive power to dZrst-order logic for traces (FO).
It follows from [5] that such a logic would capture exactly those properties of LTL that have the aaall-or-noneaa property and hence are amenable to partial-order veridZcation.
However, none of the local logics are known to be expressively equivalent to FO.
This led Thiagarajan and Walukiewicz to dedZne the condZguration based LTrL [20], which they indeed prove equivalent to FO.
LTrL was later redZned [3] to a straightforward formulation of LTL for traces essentially extending Kampas Theorem [10] to the setting of traces.
While both the event based and location based logics have elegant (exponential-time) decision procedures smoothly extending the classical automataatheoretic approach to the setting of traces, no such smooth extension exists for global logics such as LTL.
The essence of this anomaly is the complications that arise as a consequence of the fact that the satisdZability problem for LTL has a non-elementary lower bound [24].
However, experience [9] has shown that decision procedures can still be useful in practice despite discouraging lower bounds.
Gastin et al.
[6,7] do give a direct decision procedure for LTL based on automata.
However, the construction of the automaton corresponding to a given LTL-specidZcation u proceeds by induction on u, thus in a bottomaup manner.
Hence it is not an extension of the classical automataa theoretic approach, and more important, it requires the construction of the full automaton, so optimisations such as on-the-dZy checking cannot be applied.
A further drawback is its high complexity.
While an exponential blow-up is unavoidable for nested until-formulas, the procedure has also an exponential blow-up for every negation.
Since nested until-formulas are rare in specidZcations but negations are typical for specifying unwanted behaviour, this limits the practical applicability of this procedure.
In this paper, we propose a decision procedure for LTL for traces directly extending the classical approach [23].
Our procedure is based upon an extended subformula closure and independence  222  B. Bollig, M. Leucker / Data & Knowledge Engineering 44 (2003) 219a238  rewriting of formulas of LTL.
We employ this to construct a tableau-style alternating BaZ uchi automaton (ABA) accepting the set of linearisations of traces satisfying the specidZcation at hand.
In this sense, our procedure dZlls the missing gap for global trace logics by extending the classical approach to this last remaining case.
Our procedure corresponds exactly to the version given in [23] when restricted to an empty independence relation.
Furthermore, our automata can be constructed on-the-dZy, which is crucial.
Last but not least, for the fragment of LTL without untilformulas, our procedure is exponential.
In [14], it was shown that word languages dedZnable by LTL-formulas over words correspond to the languages of linear ABA.
We prove that our construction yields a linear BaZ uchi automaton as well.
Furthermore, we show that our linear BaZ uchi automata accept trace-consistent languages.
Conversely, we show that the class of trace-consistent languages dedZnable by linear ABA coincides with the class of languages that are dedZnable by LTL-formulas over Mazurkiewicz traces for a given dependency relation.
In other words, LTL-dedZnable trace languages correspond to languages dedZnable by trace-consistent linear ABA.
The results of this paper will also appear in an extended version in [13].
In the next section, we recall Mazurkiewicz traces and some related notions that will play a crucial r^ ole for our present purposes.
In Section 3, we introduce the basic object of our study, LTL, and interpret it directly over the domain of Mazurkiewicz traces.
Following this, we give in Section 4 a brief account of ABA underlying our decision procedure to be presented in Section 5.
There we supply a proof of correctness of our construction before giving a few concluding remarks in Section 6.
2.
Mazurkiewicz traces A (Mazurkiewicz) trace alphabet is a pair AdegR; IA, where R, the alphabet, is a dZnite set and I  R fi R is an irredZexive and symmetric independence relation.
Usually, R consists of the actions performed by a distributed system while I captures a static notion of causal independence between actions.
We dedZne D Az AdegR fi RA  I to be the dependency relation, which is then redZexive and symmetric.
For the rest of the section, we dZx a trace alphabet AdegR; IA.
We will use aIb to denote that the actions a and b are independent, i.e., that Adega; bA 2 I, and use similar notation for Adega; bA 2 D. We extend the notion to sets of actions X , Y  R, and let XIY denote the fact that each pair of actions a 2 X and b 2 Y is independent.
Moreover, XDY will denote that X is dependent on Y, i.e., that there exists a pair of actions a 2 X and b 2 Y with a and b dependent.
For convenience, we will write fagIY as aIY etc.
For the purpose of interpreting LTL over traces, we will adopt the viewpoint that traces are restricted labelled partial orders of events and hence have an explicit representation of causality and concurrency.
Let T Az AdegE; 6 ; kA be a R-labelled poset.
In other words, AdegE; 6 A is a poset and k : E !
R is a labelling function.
k can be extended to subsets of E in the straightforward manner.
For e 2 E, we dedZne # e Az fx 2 Ejx 6 eg and " e Az fx 2 Eje 6 xg.
We let U be the covering relation given by xUy idZ x < y and for all z 2 E, x 6 z 6 y implies x Az z or z Az y.
B. Bollig, M. Leucker / Data & Knowledge Engineering 44 (2003) 219a238  223  A (Mazurkiewicz) trace over AdegR; IA is a R-labelled poset T Az AdegE; 6 ; kA satisfying: aV # e is a dZnite set for each e 2 E. aV For every e, e0 2 E, eUe0 implies kAdegeADkAdege0 A. aV For every e, e0 2 E, kAdegeADkAdege0 A implies e 6 e0 or e0 6 e. We shall let TRAdegR; IA denote the class of traces over AdegR; IA.
As usual, a trace language L is a subset of traces, i.e., L  TRAdegR; IA.
Throughout the paper we will not distinguish between isomorphic elements in TRAdegR; IA.
We will refer to members of E as events.
Let T Az AdegE; 6 ; kA be a trace over AdegR; IA.
The dZnite predZxes of T, to be called condZgurations, will play a crucial r^ ole in S what follows.
A condZguration of T is a dZnite subset of events c  E with # c Az c where # c Az e2c # e. The set of condZgurations of T will be denoted CT .
Trivially, ; 2 CT for any trace T. CT can be equipped with a natural transition relation !T  CT fi R fi CT given a by: c !T c0 idZ there exists an e 2 E such that kAdegeA Az a, e 62 c and c0 Az c [ feg.
CondZgurations of CT are the trace-theoretic analogues of dZnite predZxes of strings.
As will become apparent in Section 3, the formulas of LTL are to be interpreted at condZgurations of traces.
In its original formulation [15], Mazurkiewicz introduced traces as certain equivalence classes of strings, and this correspondence turns out to be essential to our developments here.
To bring this out, let R be the set of dZnite strings over R and Rx be the set of (countably) indZnite strings generated by R with x Az f0; 1; 2; .
.
.g.
We set R1 Az R [ Rx and denote the empty word by e. We let w, w0 range over Rx and u, v with or without primes range over R .
Finally, we take prf(w) to be the set of dZnite predZxes of w and let alphAdegwA denote the set of actions occurring in w. Next, let T Az AdegE; 6 ; kA 2 TRAdegR; IA.
Then w 2 R1 is a linearisation of T idZ there exists a map q: prfAdegwA !
CT , such that the following conditions are met: aV qAdegeA Az ;.
a aV qAdegvA !T qAdegvaA for each va 2 prfAdegwA.
aV For every e 2 E, there exists some u 2 prfAdegwA such that e 2 qAdeguA.
The function q will be called a run map of the linearisation w. Note that the run map of a linearisation is unique.
In what follows, we shall take linAdegT A to be the set of linearisations of the trace T. A set p  R is called a D-clique idZ p fi p  D. The equivalence relation  R1 fi R1 induced by I is given by: w  w0 idZ wp Az w0 p for every D-clique p. Here and elsewhere, if X  R, wX is the sequence obtained by erasing from w all occurrences of letters in R  X .
We take A"w to denote the -equivalence class of w 2 R1 .
It is not hard to show that elements of TRAdegR; IA and -equivalence classes are two representations of the same object: A labelled partial-order T 2 TRAdegR; IA is represented by linAdegT A and vice versa (see [4] for a proof of this fact and a more thorough account of traces).
We exploit this duality of representation and let Tw denote the trace corresponding to A"w .
Moreover, for each v 2 prfAdegwA we will use cv to denote the condZguration of CTw given by qAdegvA.
To illustrate these concepts, consider the trace alphabet AdegR; IA with R Az fa; b; dg and I Az fAdega; bA; Adegb; aAg.
An example trace T over AdegR; IA is depicted in Fig.
1 with smaller elements (with respect to 6 ) appearing below larger elements.
Furthermore, it can easily be veridZed that  224  B. Bollig, M. Leucker / Data & Knowledge Engineering 44 (2003) 219a238  Fig.
1.
A trace over AdegR; IA.
abdbabd 2 linAdegT A so T Az Tabdbabd , but adabbbd 62 linAdegT A.
The condZguration c 2 CT consists of the dZrst two aas, dZrst d, dZrst two bas and is also denoted by cabdab , which is identical to cbadab as abdab  badab.
We transfer considering traces as equivalence classes to the level of languages and call a word language L  Rx trace-consistent if for all words w, w0 2 Rx with w  w0 , it holds w 2 L idZ w0 2 L. 3.
LTL for Mazurkiewicz traces In this section, we bring out the syntax and semantics of the linear time temporal logic LTL, which will be our basic object of study.
It was originally introduced for strings by Pnueli [17].
It was later equipped with a trace semantics [20] and proved expressively equivalent to dZrst-order logic for traces by Diekert and Gastin [3], and this is the version we will consider here.
The formulas of LTL are parameterised by a trace alphabet AdegR; IA and are dedZned inductively as follows: LTLAdegR; IA ::Az ttj:uju _ wjhaiujuUw;  a 2 R:  Formulas of LTLAdegR; IA are interpreted over condZgurations of traces over AdegR; IA.
More precisely, given a trace T 2 TRAdegR; IA, a condZguration c 2 CT , and a formula u 2 LTLAdegR; IA, the notion of T , c  u is dedZned inductively via: aV aV aV aV aV  T ; c  tt.
T ; c  :u idZ T ; c 2 u. T ; c  u _ w idZ T ; c  u or T ; c  w. a T ; c  haiu idZ there exists a c0 2 CT such that c !T c0 and T ; c0  u. T ; c  uUw idZ there exists a c0 2 CT with c  c0 such that T ; c0  w and all c00 2 CT with c  c00  c0 satisfy u.
We will freely use the standard abbreviations such as e.g., ff Az :tt, u ^ w Az :Adeg:u _ :wA.
Furthermore, we sometimes abbreviate T ,;  u by T  u.
All models of a formula u 2 LTLAdegR; IA constitute a subset of TRAdegR; IA, thus a language.
It is denoted by LAdeguA and is called the language  B. Bollig, M. Leucker / Data & Knowledge Engineering 44 (2003) 219a238  225  dedZned by u.
Furthermore, every formula dedZnes an x-language viz the set fw 2 linAdegT AjT  ug, which is also indicated by LAdeguA.
A simple example of a formula of LTL is u Az haihbiw.
Note that for the trace of Fig.
1 it holds that T  u if and only if T ,cab  w. Moreover, u is equivalent to u0 Az hbihaiw over this particular trace alphabet because aIb, i.e., the models of u and u0 and coincide.
Such considerations will play a prominent r^ ole when we dedZne the decision procedure in Section 5.
For bringing out the decision procedure itself, it will be convenient to assume that the syntax of LTL is augmented with an indexed until operator UUZ w where Z  R and U Az fuY11 ; .
.
.
; uYnn g is a dZnite set of indexed formulas with Yi  R. Formally, it will have the following semantics: aV T ; c  UUZ w idZ there exists a c0 2 CT with c  c0 such that T ; c0  w and kAdegc0  cAIZ, and, for each 1 6 i 6 n and every c00 with c  c00  c0 and kAdegc00  cAIYi , it holds c00  ui .
Hence, a trace satisdZes the formula UUZ w in the condZguration c idZ there is a future condZguration c0 satisfying w and all the actions from c to c0 are independent from the actions in Z.
Furthermore, the condZgurations between c and c0 which can be reached from c by performing actions independent of Yi all satisfy ui .
Note that uUw can be identidZed with fu; gU; w and we will not always make this distinction explicit.
It is not hard to see that UUZ w is expressible within FO, so this indexed modality is derivable within LTL itself.
We remark that, in case of the empty independence relation, LTLAdegR; IA and LTL interpreted over words (denoted by LTLAdegRA) coincide in the expected manner.
Thus, we identify LTL over words with LTLAdegRA, especially in the proof of Theorem 13, and save the work for introducing LTL over words formally.
4.
Alternating BaZ uchi automata Alternating automata extend non-deterministic automata by universal choices.
The transition function denotes no longer a set of possible next states but a (positive) Boolean combination.
In this section, we recall the notion of alternating automata along the lines of [23] where ABA are used for model checking LTL over strings.
However, we modidZed the dedZnition of a run to redZect the ideas presented in [14].
For a dZnite set X of variables, let BAz AdegX A be the set of positive Boolean formulas over X, i.e., the smallest set such that aV X  BAz AdegX A aV tt, ff 2 BAz AdegX A aV u, w 2 BAz AdegX A ) u ^ w 2 BAz AdegX A, u _ w 2 BAz AdegX A.
In the following, we assume for every positive Boolean formula that it is in disjunctive normal form and that it is reduced with respect to idempotence and commutation.
Hence, for a set X with jX j jX j elements, the size of BAz AdegX A is bounded by 22 .
This can easily be seen by considering the formulas as sets of sets.
226  B. Bollig, M. Leucker / Data & Knowledge Engineering 44 (2003) 219a238  We say that a set Y  X satisdZes (or is a model of) a formula u 2 BAz AdegX A idZ u evaluates to tt when the variables in Y are assigned to tt and the members of X n Y are assigned to ff.
A model is called minimal if none of its proper subsets is a model.
For example, fq1 ; q3 g as well as fq2 ; q3 g are minimal models of the formula Adegq1 _ q2 A ^ q3 .
Later in our construction, logical formulas will take over the r^ ole of states.
Therefore, we should formally distinguish between disjunctions of formulas and disjunctions of states.
However, to simplify our presentation, we identify these disjunctions when the context makes clear which one is meant.
In particular, given a formula u in disjunctive normal form, u Az _ ^ uij where no uij is a (top level) disjunction or conjunction, we identify u with the positive Boolean combination of states uij .
To avoid confusion, we sometimes write stAdeguA to denote fuij j _ ^uij g. An ABA over an alphabet R is a tuple A Az AdegQ; R; d; q0 ; F A such that Q is a dZnite non-empty set of states, q0 2 Q is an initial state, F  Q is a set of accepting states and d : Q fi R !
BAz AdegQA is a transition function.
Because of universal quantidZcation, a run over an indZnite string is no longer a sequence but a labelled directed acyclic graph.
A nodeas label redZects one of the current states of the automaton, and the edges redZect transitions of the automaton with respect to the input string.
Hence, this graph should have a unique aarootaa labelled with q0 .
Furthermore, it has to be divisible into aalevelsaa i 2 N corresponding to the ith input letter.
Every node except the root must have a aapredecessoraa.
For a node v, the labels of nodes of level i Az 1 connected with v should further be a model for the transition in state lAdegvA reading the ith letter.
More precisely: A run over an indZnite string w Az a0 a1 .
.
.
2 Rx is a Q-labelled directed acyclic graph AdegV ; EA such that there exist labellings l : V !
Q and h : V !
N which satisfy the following properties.
aV aV aV aV aV  h1 Adeg0A S Az fvg with lAdegvA Az q0 .
E  i2N Adegh1 AdegiA fi h1 Adegi Az 1AA.
For every v0 2 V with hAdegv0 A P 1, fv 2 V jAdegv; v0 A 2 Eg 6Az ;.
For every v; v0 2 V , v 6Az v0 , lAdegvA Az lAdegv0 A implies hAdegvA 6Az hAdegv0 A.
For every v 2 V , flAdegv0 AjAdegv; v0 A 2 Eg is a minimal model of dAdeglAdegvA; ahAdegvA A.
A run AdegV ; EA is accepting if every maximal dZnite path ends in a node v 2 V with dAdeglAdegvA; ahAdegvA A Az tt and every maximal indZnite path, wrt the labelling l, visits at least one dZnal state indZnitely often.
The language LAdegAA of an automaton A is determined by all strings for which an accepting run of A exists.
Let us dedZne a subclass of alternating automata corresponding to LTL formulas over words, as shown in [14].
The transition graph of an ABA A Az AdegQ; R; d; q0 ; F A is the graph AdegQ; EA such that Adegq; q0 A 2 E idZ there is an action a such that for dAdegq; aA Az _ ^ qij , the state q0 is one of the qij .
2 The automaton is called linear idZ its transition graph has only trivial cycles.
Finally, we call an ABA A trace-consistent if its language LAdegAA is trace-consistent.
We will show that linear trace-consistent automata correspond to LTL formulas over Mazurkiewicz traces.
Obviously, every BaZ uchi automaton can be turned into an equivalent (wrt the accepted language) ABA.
Vice versa, for every ABA, an equivalent BaZ uchi automaton can be constructed with  2  Silently considering tt and ff as states here.
B. Bollig, M. Leucker / Data & Knowledge Engineering 44 (2003) 219a238  227  an exponential blow-up.
The construction is described for example in [23].
Hence, it is easy to see that the emptiness problem for ABAs is exponential in its number of states.
5.
A decision procedure for LTL We have now set the scene to bring out our decision procedure for LTL.
Our procedure generalises the classical approach by constructing an ABA Au accepting the set of linearisations of traces satisfying a given formula u.
The states of this automaton are derived from an extended subformula closure, which we dZrst dedZne.
Following this, we dedZne a notion of independencerewriting of such formulas, and this will eventually become the transition relation of Au .
We pin down the details of our construction and give a proof of correctness.
Then we consider the complexity of our decision procedure and point out the correspondence between linear traceconsistent alternating automata and LTL over Mazurkiewicz traces.
5.1.
The construction In essence, we will construct an automaton Au that accepts a string w 2 Rx whenever the corresponding trace Tw satisdZes u.
To appreciate the developments to come, we commence with a small example.
Consider the example formula u Az haihbiw of Section 3.
Suppose that w is of the form abv for some v 2 Rx .
It is then not hard to see that Tw , ;  u if and only if Tw ; ca  hbiw.
Consider now instead w0 Az bav.
Since the underlying domain is traces, Tw0 might still satisfy u even though the dZrst action is a b and not an a, because aIb.
In fact, Tw0 , ;  u exactly when Tw0 ; cb  haiw.
In this sense, the proof obligation at the empty condZguration, aahaihbiwaa, has been transformed by b to the proof obligation aahaiwaa at cb ; the a-action still has to be witnessed, but the present b has been matched.
(Note by the way that either both or none of w and w0 should be accepted, because w  w0 and hence Tw Az Tw0 .)
In edZect, our automaton proceeds in this way by aaindependence-rewritingaa the proof obligations by the actions read.
The state space thus consists of all subformulas together with formulas obtained by transformations as described above.
We will call this set the extended closure of u. DedZnition 1.
Let g be a formula of LTL.
We take ecl(g) to be the least set that satisdZes the following: aV aV aV aV  g itself is contained in its closure.
For u _ w 2 eclAdeggA, it also contains the closure of u and of w. For haiu 2 eclAdeggA, it also contains the closure of u as well as haiu0 for every u0 2 eclAdeguA.
For u 2 eclAdeggA, it also contains :u 2 eclAdeggA.
We identify ::u with u.
Hence, eclAdeggA is closed under negation.
aV For uUw 2 eclAdeggA, the closure contains eclAdeguA as well as eclAdegwA.
Furthermore, for all Z  R, all w0 2 eclAdegwA, and all U  fu0Y ju0 2 eclAdeguA; Y  Rg the closure contains UU Z w0 .
aV The closure is closed under positive Boolean combinations, i.e., BAz AdegeclAdeggAA  eclAdeggA.
Intuitively, the extended closure of a formula u contains all formulas which may be obtained by substituting a subformula w of u by w0 where w0 is a positive Boolean combination of formulas  228  B. Bollig, M. Leucker / Data & Knowledge Engineering 44 (2003) 219a238  derived from w by applying this rule.
This is because our automaton will be dedZned in the way that before it is considering a formula u it may consider a subformula w of u, transforming this into a positive Boolean combination of new formulas w0 .
This result is processed in the way that w is substituted by w0 within u.
We assume that all positive Boolean formulas are in disjunctive normal form and moreover that they are reduced wrt idempotence and commutation.
With these assumptions we can prove the following crucial result.
Proposition 2. eclAdeggA is a finite set for each formula g of LTL.
Proof.
The proof proceeds by a standard induction.
The claim is obvious for atomic formulas.
For g Az haiu, the extended closure of g contains the extended closure of u and for every element u0 of eclAdeguA also haiu0 .
Thus, we get jeclAdeguAj !
2 elements.
Since the extended closure contains for every element also a negated one, we get another factor 2.
Now, positive Boolean combination yields jeclAdeguAj!2!2 .
For g Az fuY11 ; .
.
.
; uYnn gUZ w, a double exponential blow-up.
Altogether, we have jeclAdeggAj 6 22 it can be veridZed that jeclAdeggAj is bounded by  P fi n  22  2  2  iAz1  AdegjeclAdegui Aj!2jRj A  jeclAdegwAj !2jRj !22  !2  :  The three factors are upper bounds for the derivatives of f!
!
!g, UZ , and w, resp., and the powers bound their positive Boolean combination.
 We will refer to formulas of this set as extended formulas.
Furthermore, we will say that a formula is a diamond-formula in case it is of the form haiu for some extended formula u and some a 2 R. In a similar vein, we let the until-formulas consist of those of the form UUZ w with U being a dZnite set of extended formulas, w a single extended formula and Z  R. For extended formulas, we will make use of the important notion of its dual, which is obtained as usual by applying de Morganas laws to push negations inwards as far as possible.
DedZnition 3.
The dual of an (extended) formula is given inductively as follows: aV aV aV aV aV aV  tt Az ff, ff Az tt.
:u Az u. u _ w Az u ^ w, u ^ w Az u _ w. haiu Az :haiu.
uUw Az :AdeguUwA.
UUZ w Az :AdegUUZ wA.
We are now set to introduce the operator k  k , which will constitute the transition relation of the alternating automaton.
Essentially, kuka is to be thought of as the independence-rewriting of u by the action a.
It follows from the intuition conveyed earlier that it should be the case that khaiuka is u.
Then, for the case where aIb, khbiuka Az hbiu0 where u0 Az kuka .
Of course, whenever aDb and the actions  B. Bollig, M. Leucker / Data & Knowledge Engineering 44 (2003) 219a238  229  are not identical, then khbiuka must be ff because b cannot be a next action.
DedZnition 4 formally captures this intuition.
DedZnition 4.
For each extended formula u and each action a, the operator kuka yields a formula of BAz AdegeclAdeguAA and is dedZned inductively via: kttka Az tt: ku _ wka Az kuka _ kwka : k:uka Az kuka : 8 <u khbiuka Az hbikuka : ff  if a Az b if aIb if aDb; a 6Az b:  Note that since eclAdeggA is closed under positive Boolean combination, we have eclAdeggA Az BAz AdegeclAdeggAA.
We now only need to specify the case of kUUZ wka .
This turns out to be inherently more complex, and before providing the precise dedZnition, we carefully analyse the semantics of the indexed until modality in Fig.
2.
For this purpose, consider some trace T be given and suppose c; c0 2 CT such that c  c0 .
Furthermore, let c00 be a condZguration between c and c0 (Fig.
2(i)).
Suppose, we can augment c by an a-labelled event e to obtain a successor condZguration c000 of c, a i.e., c!c000 .
Then c000  c00  c0 or c000 6 c00 but c000  c0 or c000 6 c0 , as shown in Fig.
2(a)a(c) resp.
In case (b), it is obvious that kAdegc00  cAIa and for case (c), we have kAdegc0  cAIa (as well as kAdegc00  cAIa).
The situation shown in Fig.
2 can, in other words, be described in the following manner: The action a is (a) neither in the future of c00 nor of c0 (case (a)), (b) in the future of c00 (case (b)), or (c) in the future of c00 as well as of c0 (case (c)).
Consider a formula uUw, which is to be checked in the condZguration c. In case (c), we have to employ a for verifying w as well as u.
Note that for case (c), we get two subcases depending upon whether c0 Az c or c0 # c. While w is not relevant to the dZrst case, u is required to hold in the  Fig.
2.
CondZguration and actions.
230  B. Bollig, M. Leucker / Data & Knowledge Engineering 44 (2003) 219a238  condZgurations between c and c0 .
Note that these condZgurations are reached by actions independent of a.
For case (a), we have to employ a for verifying u in condZguration c but not for c00 .
In case (b), we have to prove u considering a in the condZguration c00 , which might be equal to c as well as didZerent from c. Note that in the latter case, every event of c00  c is independent of a. Consequently, we dedZne the rewriting operator for a formula UUZ w as follows.
DedZnition 5 (extends DedZnition 4).
Let n o W1 Az kwka W2 Az kukYa [fag juY 2 U UZ[fag kwka : Moreover, we set W0 Az W1 _ W2 .
Let n o kUka Az kukaY [fag juY 2 U [ uY juY 2 U; aIY and U1 Az ^uY 2U kuka  U2 Az kUka UZ w  and U0 Az U1 ^ U2 .
Then we dedZne   0 W if aDZ; Z kUU wka Az W0 _ U0 else aIZ: Note that W0 captures case (c) in which an action a is employed for verifying w under the assumption that c0 Az c AdegW1 A or not (W2 ).
U0 covers the idea that a is not in the future of c0 but is employed for verifying the obligations in U.
It is not hard to verify that k  k is well-dedZned.
However, to show the correctness of our construction, the following proposition is essential.
Suppose that we have given one linearisation of a trace and one formula and we want to check the formula wrt the trace obtained from the linearisation.
According to the following proposition it is possible to consider the word action by action and to modify the formula according to the rewriting operator.
Proposition 6.
Let g be any formula of LTLAdegR; IA.
Then for all w 2 Rx and for all v 2 R , a 2 R, w0 2 Rx with vaw0  w Tw ; cv  g if and only if Tw ; cva  kgka : Proof.
The proof proceeds by induction on the formula g. We only show the most important cases as the other cases follow in a similar manner.
The cases where g Az tt or g Az ff are trivial.
Suppose g Az u _ w. Then Tw ; cv  u _ w means by dedZnition that Tw ; cv  u or Tw ; cv  w. By induction, this is equivalent to Tw ; cva  kuka or Tw ; cva  kwka , which is equivalent to Tw ; cva  ku _ wka by dedZnition of the rewrite operator.
Suppose g Az :u.
By dedZnition, Tw ; cv  :u idZ not Tw ; cv  u.
Induction yields Tw ; cva 2 kuka , which means Tw ; cva  :kuka .
The dual of a formula is obviously logically equivalent to the negation of the formula so that the previous statement is equivalent to Tw ; cva  kuka .
B. Bollig, M. Leucker / Data & Knowledge Engineering 44 (2003) 219a238  231  Suppose g Az hbiu.
By dedZnition, Tw ; cv  hbiu if and only if there is a condZguration c0 such that b cv !
c0 Az cvb and Tw ; c0  u.
We consider three didZerent cases: aV b Az a: then c0 Az cva .
Hence Tw ; cva  u. b a aV bAz 6 a, bDa: then cv !
c0 and cv !
cva .
However, then w is not a linearisation of the trace, which is a contradiction.
aV bIa: Tw ; cvb  u is by induction equivalent to Tw ; cvba  kuka .
Since aIb, this means Tw ; cvab  kuka , which is equivalent to Tw ; cva  hbikuka .
Putting together all the cases we get that Tw ; cv  hbiu if and only if Tw ; cva  khbiuka .
The most involved case is g Az UUZ w. Let U Az fuY11 ; .
.
.
; uYNN g. Recall that Tw ; cv  fuY11 ; .
.
.
; uYNN gUZ w if and only if 9x 2 R ; y 2 Rx ; aw0  xy; xIZ; such that Tw ; cvx  w; and 8i 2 f1; .
.
.
; N g; 8x1 ; x2 2 R with x1 x2  x; x1 IYi ; x2 6Az e; it holds Tw ; cvx1  ui : We consider here only the case where aIAdegYi [ ZA.
The other cases follow similarly.
Let us dZrst discuss the implication from left to right: we consider the following cases for x: aV x Az e: then Tw ; cvx  w means Tw ; cv  w, which implies by induction Tw ; cva  kwka .
This shows AdegW1 A. aV x 6Az e, a 62 alphAdegxA: We consider the cases for w and ui simultaneously. )
I:H :  ) ) )  aIx Tw ; cvx  w Tw ; cvxa  kwka Tw ; cvax  kwka 9x 2 R ; xIAdegZ [ fagA Tw ; cvax  kwka  ) I:H :  ) ) )  aIx1 Tw ; cvx1  ui Tw ; cvx1 a  kui ka Tw ; cvax1  kui ka 8x1 ; x2 2 R ; x1 x2  x; x1 IAdegYi [ fagA x2 6Az e; Tw ; cvax1  kui ka  Hence, Tw ; cva  fkukaY [fag juY 2 UgUZ[fag kwka , which shows (W2 ).
aV x 6Az e, a 2 alphAdegxA: We easily see that x  ax0 and a, x0 IZ and Tw ; cvax0  w. We will show AdegU1 A and AdegU2 A.
Let us consider x1 .
If x1 Az e then Tw ; cv  ui implies by induction Tw ; cva  kui ka .
If x1 6Az e and a 62 alphAdegx1 A, we see that aIx1 and x1 IYi .
Hence, x1 IAdegYi [ fagA.
Now, Tw ; cvx1  ui yields by induction Tw ; cvx1 a  kui ka proving Tw ; cvax1  kui ka since aIx1 .
For the case x1 6Az e but a 2 alphAdegx1 A, we see that x1  ax01 and Tw ; cvax01  ui .
Summing up the cases for x1 , we get Tw ; cva  kUka UZ w, which shows AdegU2 A, and Tw ; cva  kuka for all uY 2 U, which shows AdegU1 A.
Altogether, we showed that W0 or U0 hold in the until case proving the aaifaa-part.
Now, let us consider the implication from right to left: suppose Tw ; cva  kUUZ wka , i.e., Tw ; cva  W1 _ W2 _ U0 :  232  B. Bollig, M. Leucker / Data & Knowledge Engineering 44 (2003) 219a238  We discuss the disjunction by drawing the conclusions of each formula aV Tw ; cva  kwka : this implies by induction that Tw ; cv  w. Hence, Tw ; cv  UUZ w. Y [fag juY 2 UgUZ[fag kwka : Then there exist x; y, xIAdegZ [ fagA, w  vaxy such that aV Tw ; cva  fkuka Tw ; cvax  kwka .
Since xIa also Tw ; cvxa  kwka , which yields by induction Tw ; cvx  w. We further know that for every proper predZx (modulo ) x1 of x with x1 IAdegY [ fagA, we have Tw ; cvax1  kuka .
Then Tw ; cvx1 a  kuka and, by induction, Tw ; cvx1  u.
Hence, Tw ; cv  UUZ w. aV Tw ; cva  ^uY 2U kuka ^ kUka UZ w: We dZrst obtain by induction that Tw ; cv  u for every uY 2 U.
Let us consider Tw ; cva  AdegfkukYa [fag juY 2 Ug [ fuY juY 2 U; aIY gAUZ w: It implies that there is an x0 , independent of Z, such that Tw ; cvax0  w. Since we are in the case of aIZ, we conclude that there is an x, xIZ(x  ax0 ) such that Tw ; cvx  w. Now, consider x1 x2  x, x2 6Az e. For every x1 IAdegY [ fagA, x1 a predZx of x0 , we know Tw ; cvax1  kuka and, by induction, Tw ; cvx1  u.
For x0 Az e, we already know Tw ; cv  u.
For x1 IY and x1 Da, we obtain x1  ax01 , x01 IY , since cva is a valid condZguration.
By Tw ; cvax01  u we deduce Tw ; cvx1  u.
Altogether, this shows Tw ; cv  UUZ w. This concludes the proof.
 We can now dZnally bring the dedZnition of the ABA Au corresponding to a formula u 2 LTLAdegR; IA as follows.
DedZnition 7.
Given a formula u 2 LTLAdegR; IA, the ABA Au is the tuple AdegQ; R; d; q0 ; F A where aV aV aV aV  Q Az eclAdeguA is the set of states.
dAdegq; aA Az kqka is the transition function.
q0 Az u is the initial state.
F Az f:wj:w 2 eclAdeguAg is the set of accepting states.
Note that we dedZned the set of dZnal states to be all negated formulas.
The intuitive idea is that failing to prove a proposition indZnitely often sudZces to assume that its negation is true.
In the work of Vardi [23], one could likewise take all negated formulas as dZnal states.
Since in the case of LTL over words, only until-formulas may occur indZnitely often, the set of dZnal states is there restricted to negated until-formulas.
The correctness of the construction is summarised in the following theorem, which is the main contribution of the paper.
Theorem 8.
Let u be a formula of LTLAdegR; IA and let its ABA be given as Au Az AdegQ; R; d; q0 ; F A.
Then w 2 LAdegAu A if and only if Tw ; ;  u for every w 2 Rx .
In other words, LAdegAu A Az LAdeguA.
B. Bollig, M. Leucker / Data & Knowledge Engineering 44 (2003) 219a238  233  Proof.
For w 2 Rx , we have to show that Au has an accepting run on w idZ Tw  u.
Note that every run has (at most) three types of paths: aV dZnite paths ending in tt, aV indZnite paths on which from some point on every node is labelled by an until- or diamondformula, or aV indZnite paths on which from some point on every node is labelled by a negated until- or diamond- formula.
Let us give a sketch of the proof.
For w 2 eclAdeguA and w Az aw0 2 Rx , let dAdegw; wA be the extension of d dedZned by dAdegw; aw0 A Az dAdegdAdegw; aA; w0 A.
By Proposition 6, dAdegu; wA Az dAdegdAdegu; aA; w0 A Az dAdegkuka ; w0 A and Tw ; cfi  u idZ Tw ; ca  kuka .
Now, consider an accepting run of Au .
Its dZnite paths end in tt, thus all proof obligations are proved.
Conversely, a run should be accepted only if the dZnite paths end in tt, i.e., that all proof obligations are proved.
Now, let us consider the indZnite paths of a run.
These can only occur by unwinding a (negated) until-formula indZnitely often or by reading actions independent of the one given within a diamond-formula.
This can be accepted idZ the underlying until-formula or diamond-formula is preceded by a negation.
This is captured by the acceptance condition for indZnite paths given by the dZnal states of the automaton.
 5.2.
State space To simplify the presentation, we have dedZned the state space of our automaton in a straightforward manner and presented a simple argument to show that it is dZnite.
Thus, we indeed obtained a decision procedure.
Now, let us take a closer look to the states that are really needed in our construction.
In other words, let us consider the states that are reachable from the initial state.
Given a formula u 2 LTLAdegR; IA, a state w of Au , and a set Y  R, let reachY AdegwA denote the set of states reachable from w in Au by words whose actions are independent of Y.
More precisely, reachY AdegwA Az fw0 j9w 2 R ; wIY : w0 2 stAdegdAdegw; wAAg; where d is the extension of d dedZned in the obvious manner.
Proposition 9.
Given u 2 LTLAdegR; IA, we get upper bounds for the number of states reachable from a state of Au wrt Y inductively as follows: aV aV aV aV aV  jreachY AdegttAj Az 1 jreachY AdegffAj Az 1 jreachY Adeg:wAj Az jreachY AdegwAj jreachY Adegw1 _ w2 Aj 6 jreachY Adegw1 Aj Az jreachY Adegw2 Aj jreachY Adegw1 ^ w2 Aj 6 jreachY Adegw1 Aj Az jreachY Adegw2 Aj   jreachY AdegwAj Az jreachY [fag AdegwAj Az 1 aV jreachY AdeghaiwAj 6 jreachY [fag AdegwAj Az 1  if aIY if aDY  234  aV  B. Bollig, M. Leucker / Data & Knowledge Engineering 44 (2003) 219a238  jreachY AdegfuY11 ; .
.
.
; uYnn gUZ wAj 6 22  Pn  iAz1  AdegjreachY Adegui Aj!2jRj A  !
2jRj !
22  jreachY AdegwAj  :  Proof.
The obvious cases are if the state formula is tt or dZ.
Since negation is shifted inwards by the dual operator , the states reachable from :w are the same states as reachable from w, except that every state is preceded by :.
Thus, the cardinality is the same.
Given haiw, assume a to be independent of Y.
Reading an action dependent on but didZerent from a (and independent of Y) yields the state dZ and in our formula the 1.
Reading a yields the state w, thus, the states reachable from w are obviously reachable from haiw AdegjreachY AdegwAjA.
The last possibility is reading an action b independent of a and Y.
This yields formulas of the form haiw0 where w0 is obtained by rewriting w by actions independent of Y and a.
Since haiw0 distributes over disjunctions and conjunctions, we get the same number of states as obtained by considering the states reachable from w by words independent of Y [ fag (jreachY [fag AdegwAj).
The bound for until-formulas follows by a simple combinatorial argument.
Before and after the U within an until-formula, only positive Boolean combinations of derivations of respectively U and w may occur, and the U is indexed with subsets of R.  Let us call the fragment of LTL dedZned without until-formulas HennessyaMilner fragment.
Proposition 10.
Given a formula w from the HennessyaMilner fragment of LTLAdegR; IA, we obtain reachY AdegwA 6 jwjjRY j .
Proof.
The proof follows a simple induction of which we pick out two cases: aV Applying Proposition 9, the induction hypothesis, and the binomial formula, jRY j  jreachY Adegw1 _ w2 Aj 6 jreachY Adegw1 Aj Az jreachY Adegw2 Aj 6 jw1 j  jRY j  Az jw2 j  3  6 Adegjw1 j Az jw2 j Az 1A  jRY j  :  aV Assuming aIY , jreachY AdeghaiwAj 6 jreachY AdegwAj Az jreachY [fag AdegwAj Az 1 6 jwjjRY j Az jwjjRY j1 Az 1 6 Adegjwj Az 1AjRY j :    Due to the exponential blow-up the construction of an equivalent BaZ uchi automaton for Au causes, we conclude.
Theorem 11.
Checking satisfiability of a formula from the HennessyaMilner fragment of LTLAdegR; IA can be done in exponential time.
5.3.
LTL and linear automata Now we characterise LTLAdegR; IA as equivalent to that subclass of ABA that we called traceconsistent linear ABA, and we start observing the linearity of the above construction.
3  Adega Az bAn Az   fi n ni i a b. iAz0 i  Pn  B. Bollig, M. Leucker / Data & Knowledge Engineering 44 (2003) 219a238  235  Proposition 12.
Given u 2 LTLAdegR; IA, Au is linear.
Proof.
We have to show that the transition function only admits trivial cycles.
Therefore we dedZne a well-founded strict ordering relation 4 & on the states of our automaton and show that kwka yields a Boolean combination of strictly smaller states or w. For a formula g 2 LTLAdegR; IA, & eclAdeggA fi eclAdeggA is inductively dedZned by aV aV aV aV aV  u & haiu, haiu & haiw if u & w, u & :w if u & w, wY11 & wY22 if w1 ' w2 and Y1 ( Y2 and one of the orderings is strict, i.e., w1 & w2 or Y1 )Y2 , _ ^ uij & _ ^ wij if fuij g ) fwij g where ) is the (strict) (multi-)set ordering induced by &, i.e., M1 ) M2 idZ there exist a set X and an element m 2 M2 with m0 & m for all m0 2 X such that M1 Az AdegM2  fmgA [ X .
In other words, a set M1 is smaller than M2 if an element of M2 is replaced by a set of smaller elements resulting in M1 .
aV w0 & UUZ w if w0 & w, aV _ ^ uij & UUZ w if fuRij g ) U, aV U1 UZ1 w1 & U2 UZ2 w2 if U1 )U2 and Z1 ( Z2 and w1 ' w2 and one of the orderings is strict, i.e., U1 ) U2 or Z1 )Z2 or w1 & w2 , where ) is the redZexive closure of ),  and contains its transitive closure.
Here, ' is the redZexive closure of &.
We easily verify that, given formulas u, w 2 eclAdeggA, an action a 2 R, and a minimal model W of kwka with u 2 W, it holds u ' w and furthermore that for arbitrary u; w 2 eclAdeggA, u &Az w implies u 6Az w. We conclude the linearity of our construction.
 Theorem 13.
Let A Az AdegQ; R; d; q0 ; F A be a trace-consistent linear ABA.
There is a formula u 2 LTLAdegR; IA such that Tw ; ;  u if and only if w 2 LAdegAA for every w 2 Rx .
In other words, LAdeguA Az LAdegAA.
Before we are going to prove the previous theorem, let us mention two facts: Proposition 14.
Let A Az AdegQ; R; d; q0 ; F A be a linear ABA.
There is a formula u 2 LTLAdegRA such that LAdeguA Az LAdegAA.
Proposition 15.
Let L  Rx and I  R fi R be an independence relation.
Then the following statements are equivalent.
1.
L is trace-consistent wrt I and LTLAdegRA-definable.
2. fTw jw 2 Lg is FOAdegR; IA-definable.
3. fTw jw 2 Lg is LTLAdegR; IA-definable.
4  That is, a transitive and acyclic relation.
236  B. Bollig, M. Leucker / Data & Knowledge Engineering 44 (2003) 219a238  Proposition 14 was independently shown by [18] and [14].
As aforementioned in the introduction, the equivalence of (1) and (2) traces back to [5], that one between (2) and (3) back to [3].
Now, we are ready to prove Theorem 13.
Proof.
In accordance with Proposition 14, given a trace-consistent linear ABA A, there is a formula w 2 LTLAdegRA satisfying LAdegwA Az LAdegAA where LAdegwA is likewise trace consistent.
Employing the equivalences from Proposition 15, it immediately follows the existence of a formula u 2 LTLAdegR; IA with Tw ; ;  u if and only if w 2 LAdegAA for every w 2 Rx .
 Let us bring out two important consequences of the last theorem: 1.
Given an LTL formula u over Mazurkiewicz traces, it is simple to construct a trace-consistent LTL formula w over words dedZning the same set of x-words.
Just construct Au , and for Au , a corresponding formula w according to the proof given in [14].
2.
Partial-order reduction techniques work for LTL over Mazurkiewicz traces.
Given an LTL formula u over Mazurkiewicz traces, consider its automaton Au .
It is a linear trace-consistent automaton over words.
For this kind automata, several powerful partial-order reduction techniques have been developed, which will have the same success here [16].
Hence, specifying with LTL over Mazurkiewicz traces promisesaadespite the bad worst-case runtime of its decision procedureaaedZcient veridZcation tasks in practice.
Note that the dZrst item even implies that the languages dedZnable by LTL-formulas over Mazurkiewicz traces are FO-dedZnable over Mazurkiewicz traces.
Thus, we obtained one direction of the expressive completeness proof given in [3].
6.
Conclusion We have exhibited a decision procedure demonstrating that the classical automataatheoretic approach can be generalised to condZguration based temporal logics for traces such as LTL.
In particular, Theorem 8 asserts that it is possible to directly construct an ABA accepting the set of linearisations of traces satisfying the formula at hand.
The main idea underlying this construction is to use a notion of independence-rewriting to an extended subformula closure.
It easily follows from [24] that this closure must be of nonelementary size and, moreover, that this is unavoidable for any decision procedure directly generalising the classical automataatheoretic approach.
Our approach clearly yields an optimal (non-elementary) decision procedure and shares this similarity with [7].
We are sure that an actual implementation of our approach would compare favourably due to the fact that the automata need not necessarily be constructed in full and especially because it avoids an exponential blow-up for negation.
We showed that trace-consistent linear automata correspond to LTL over Mazurkiewicz traces wrt language dedZnability, transferring a similar result shown in [14] from the setting of words to the setting of traces.
As a consequence, it is quite natural to construct trace-consistent linear  B. Bollig, M. Leucker / Data & Knowledge Engineering 44 (2003) 219a238  237  automata as a tool to answer the satisdZability problem, and our approach exactly follows this idea.
It is easy to adapt our decision procedure for the unary fragment of LTL, where the untilmodality is replaced by an eventually-modality with the obvious semantics.
Unfortunately, it remains an open question whether the decision procedure obtained in this way is optimal.
Acknowledgement We would like to thank Jesper G. Henriksen for fruitful discussions and valuable comments on this work.
References [1] R. Alur, D. Peled, W. Penczek, Model checking of causality properties, in: Proceedings of the 10th Annual IEEE Symposium on Logic in Computer Science (LICSa95), IEEE Computer Society Press, San Diego, California, 1995, pp.
90a100.
[2] O. Bernholtz, M.Y.
Vardi, P. Wolper, An automataatheoretic approach to branching-time model checking, in: D.L.
dill (Ed.
), Proceedings of the 6th International Conference on Computer-Aided VeridZcation (CAVa94), vol.
818 of Lecture Notes in Computer Science, Springer, 1994, pp.
142a155.
[3] V. Diekert, P. Gastin, LTL is expressively complete for Mazurkiewicz traces, in: Proceedings of International Colloquim on Automata, Languages and Programming (ICALPa2000), vol.
1853 Lecture Notes in Computer Science, Springer, 2000, pp.
211a222.
[4] V. Diekert, G. Rozenberg (Eds.
), The Book of Traces, World ScientidZc, Singapore, 1995.
[5] W. Ebinger, A. Muscholl, Logical dedZnability on indZnite traces, Theor.
Comput.
Sci.
154 (1) (1996) 67a84.
[6] P. Gastin, R. Meyer, A. Petit, A (non-elementary) modular decision procedure for LTrL, in: MFCS: Symposium on Mathematical Foundations of Computer Science, Lecture Notes in Computer Science, vol.
1450, 1998.
[7] P. Gastin, R. Meyer, A. Petit.
A (non-elementary) modular decision procedure for LTrL.
Technical report, LSV, ENS de Cachan, 1998, extended version of MFCSa98.
[8] J.-C. Gregoire, G.J.
Holzmann, D.A.
Peled, (Eds.
), The Spin VeridZcation System, vol.
32 of DIMACS series, American Mathematical Society, 1997, ISBN 0-8218-0680-7, p. 203.
[9] J.G.
Henriksen, J.L.
Jensen, M.E.
JA,rgensen, N. Klarlund, R. Paige, T. Rauhe, A. Sandholm, Mona: Monadic second-order logic in practice, in: E. Brinksma, R. Cleaveland, K.G.
Larsen, T. Margaria, B. StedZan (Eds.
), Tools and Algorithms for the Construction and Analysis of Systems, vol.
1019 of Lecture Notes in Computer Science, Springer, 1995, pp.
89a110.
[10] H.W.
Kamp.
Tense Logic and the Theory of Linear Order, Ph.D. thesis, University of California, Los Angeles, 1968.
[11] O. Kupferman, M.Y.
Vardi, An automataatheoretic approach to reasonings about indZnite-state systems, in: E.A.
Emerson, A.P.
Sistla (Eds.
), Proceedings of the 12th International Conference on Computer-Aided VeridZcation (CAVa00), vol.
1855 of Lecture Notes in Computer Science, Springer, 2000.
[12] O. Kupferman, M.Y.
Vardi, P. Wolfer, An automataatheoretic approach to branching-time model checking, J. ACM 47 (2) (2000) 312a360.
[13] M. Leucker, Logics for Mazurkiewicz traces.
Ph.D. thesis, Lehrstuhl faZ ur Informatik II, RWTH Aachen, 2002.
Also appeared as Technical Report 2002-10, RWTH Aachen.
[14] C. LaZ oding, W. Thomas, Altering automata and logics over indZnte words, in: Proceedings of the IFIP International Conference on Theoretical Computer Science, IFIP TCS2000, vol.
1872 of Lecture Notes in Computer Science, Springer, 2000, pp.
521a535.
[15] A. Mazurkiewicz, Concurrent program schemes and their interpretations, DAIMI Rep. PB 78, Aarhus, 1977.
238  B. Bollig, M. Leucker / Data & Knowledge Engineering 44 (2003) 219a238  [16] D. Peled, Ten years of partial order reduction, in: Proceedings of 10th International Conference on ComputerAideds VeridZcation (CAVa98), vol.
1427 of Lecture Notes in Computer Science, Springer, Vancouver, BC, Canada, 1998, pp.
17a28.
[17] A. Pneuli, The temporal logic of programs, in: Proceedings of the 18th IEEE Symposium on the Foundations of Computer Science (FOCS-77), Providence, Rhode Island, October 31aNovember 2, 1977, IEEE Computer Society Press.
[18] S. Rohde, Alternating automata and the temporal logic of ordinals, Ph.D. thesis, University of Illinois at UrbanaChampaign, 1997.
[19] P.S.
Thiagarajan, A trace based extension of linear time temporal logic, in: Proceedings of the Ninth Annual IEEE Symposium on Logic in Computer Science, Paris, France, 4a7 July, IEEE Computer Society Press, 1994, pp.
438a 447.
[20] P.S.
Thiagarajan, I. Walukiewicz, An expressively complete linear time temporal logic for Mazurkiewicz traces.
in: Proceedings, Twelth Annual IEEE Symposium on Logic in Computer Science, Warsaw, Poland, 29 Junea2 July, IEEE Computer Society Press, 1997, pp.
183a194.
[21] A. Valmari, A stubborn attack on state explosion, in: E.M. Clarke, R.P.
Kurshan (Eds.
), Proceedings of Computer-Aided VeridZcation (CAVa90), vol.
531 of Lecture Notes in Computer Science, Springer, Berlin, Germany, 1991, pp.
156a165.
[22] M.Y.
Vardi, P. Wolper, An automataatheoretic approach to automatic program veridZcation, in: Symposium on Logic in Computer Science (LICSa86), IEEE Computer Society Press, Washington, DC , USA, 1986, pp.
332a345.
[23] M.Y.
Vardi, in: An AutomataaTheoretic Approach to Linear Temporal Logic, vol.
1043 of Lecture Notes in Computer Science, Springer, New York, NY, USA, 1996, pp.
238a266.
[24] I. Walukiewicz, DidZcult condZgurationsaaon the complexity of LTrL, in: K.G.
Larsen, S. Skyum, G. Winskel (Eds.
), Proceedings of 25th International Colloquium on Automata, Languages and Programming (ICALPa98), vol.
1443 of Lecture Notes in Computer Science, 1998, pp.
140a151.
Benedikt Bollig received his M.Sc.
degree (Dipl.-Inform.)
in Computer Science in 2000 from the University of Technology Aachen (RWTH Aachen).
He is currently doing his Ph.D. degree in Computer Science at Lehrstuhl faZ ur Informatik II, RWTH Aachen.
His research interests include model checking, Mazurkiewicz traces, and message sequence charts.
Martin Leucker received his M.Sc.
degree (Dipl.-Math.)
in Mathematics in 1996 from the University of Technology Aachen (RWTH Aachen).
He got his Ph.D. degree (Dr. rer.
nat.)
at Lehrstuhl faZ ur Informatik II, RWTH Aachen.
In his thesis, he studied several logics for Mazurkiewicz traces.
Currently, he is a postdoctoral researcher at the University of Pennsylvania, USA.
Characterizing Temporal Repetition Diana Cukierman and James Delgrande Simon Fraser University, Burnaby, B.C., V5AlS6, Canada {diana,jim}@cs.sfu.ca Abstract  may be dependant on the occurrence of other events, and so on.
In this paper we analyze the essence of repetition, propose some basic terminology to refer to repetitive temporal objects and present a classification of types of repetition according to parameters determining these various types of repetition.
Several notions are considered, some are extremely general, some are very specific.
The granularity or precision with which the events in a repetition are expressed is also taken into consideration.
This study focuses on repetition occurring in time.
Nonetheless, what we investigate is analogous to or can be extended to other forms of repetition.
A straightforward parallel is that of spatial repetition, for example, verbal descriptions (a particular phrase for example) can be repeated in a novel to attain a certain structure in a text, etc.
Two dimension or three dimension spatial repetition can also be conceived, for example a figure or pattern can be repeated or combined in many ways to obtain a whole.
In this paper we concentrate on one dimensional repetition.
This paper is organized as follows.
Next section presents related work.
Section 3 discusses the level of ontological similarity between the different instances or repeats in a repetition.
Section 4 defines the terminology which is used to describe the repetition patterns.
Section 5 introduces the parameters according to which repetition is classified, and Section 6 combines these parameters to form some of the classes that belong to a general taxonomy.
Section 7 compares proposals that have appeared in the literature under one of the parameters in our classification.
We propose that more comparisons of the sort can be done.
Finally Section 8 presents a summary of this work and discusses future research venues.
This paper is a preliminary investigation of temporal repetition.
W e review work in Artificial Intelligence of both formal and practical systems that deal with repetitive temporal objects (a. e. repetitive points and/or intervals).
W e analyze the essence of repetition, and present a n extensive classification of types of repetition.
1  Introduction  This work presents results of our research on representing and reasoning about schedulable, repeated activities, specified using calendars.
Examples of such activities include going to a specific class every Tuesday and Thursday during a semester, attending a seminar every first day of a month, and going to fitness classes every other day.
This research provides for a valuable framework for scheduling systems, financial systems, process control systems and, in general, date-based systems, and any application where repetition is an important component.
Being able to clearly specify the type of repetition encountered in the application can provide for valuable restrictions which can contribute towards a better description and understanding of the domain and more efficient algorithms.
Very recently work has been done related to reasoning about repetition, [3, 4, 5, 9, 13, 14, 16, 17, 18, 19, 201.
However, to our knowledge no extensive taxonomy of repetition has been proposed in the literature.
We believe that reasoning about repeated activities calls for a study and precise definition of the topological characteristics of the time during which the instances of the activities occur, and the repetition patterns they present.
We argue that this kind of study helps comparing existing research in the area and provide insight for possible options not considered before in related research.
Repetition and cycles appear in nature, in everyday life.
A myriad of examples can be mentioned: seasons keep recurring, it rains every now and then, the sun rises every day, one year follows another.
Many human activities involve some form of repetition in time; waking up every day, eating, going to class every week, the beating of the heart, walking, etc.
In all these examples different repetition patterns can be abstracted; some are based on temporal measures, for example a repetition occurring every minute; some repetition patterns are predictable, some not; some repetition  2  80 0-8186-7528/96 $5.00 0 1996 IEEE  Related work  Non-convex intervals (intervals with "gaps") are employed in [12, 131 when referring to recurring periods.
This work is an extension of [l],where (convex) time intervals are considered as the basic temporal objects, and 13 basic binary relations are defined, as for example "before", "overlaps", "during", etc.
between them.
[12] presents a taxonomy of binary relations between non-convex intervals.
One type of these relations is generated by functors like "mostly", "always", "partially", etc.
applied to the basic binar relations between convex intervals.
For example, $121,  page 362) "I mostly meets J" indicates that for every component of the non-convex interval J there is a component of the non-convex interval I that meets it.
Other binary relations between non-convex intervals include "dis'oint from", "strictly intersects" and "bars" (or union3 [121.
[18] also elaborate on the notions of non-convex interval relations defined in [12, 131.
They deal with qualitative relations between periodic events, considering for example a quantifier "always before".
These relations are defined so that correlated subintervals relate with a basic interval binary relation.
They define an algebra of relations between what they call n-intervals, a subclass of Ladkin's non-convex intervals.
New qualitative relations between repetitive events can be derived.
I151 proposes a generalization of non-convex intervals as an ordered, finite sequence of oints in a linear order.
Poessio and Brachman [lgyare mainly concerned with the implementation of algorithms to detect overlapping repeated activities.
This work relies on temporal constraint satisfaction results and algorithms [GI.
Repeated activities have non-convex intervals as their temporal counterpart.
Leban et al deal with repetition and time units [14.
This work relies on sequences of consecutive interva s combined into "collections".
The collection representation makes use of "primitive collections" (essentially circular lists of integers), and two basic operators, slicing and dicing, which subdivide an interval and select a subinterval from another collection respectively.
They propose to build new collections with these operators, and so represent calendar based repetitive intervals.
[lo presents a first-order axiomatization of recurrence, ased on Allen's interval calculus which allows atemporal entities to be associated "incidentally or repeatedly'' with temporal intervals.
To refer to repetition associated to calendar based dates, and also allowing for repetition to include gaps, this proposal relies on Leban et al's collections.
[9] proposes a notation based on first order logic to represent recurrence of points and intervals.
[20] presents a temporal formalism combining research done in several streams of temporal reasoning involving calendar based repetition.
[2, 7, 81 include the formalization of time granularity.
A brief classification of repeated events appears in 1171.
"Repetitive events" are partitioned into 'periodic" and "aperiodic" and aperiodic is partitioned as "random" and "stochastic".
[E]also introduces the term of "near-periodic event".
In [3, 4, 51 we define an abstract hierarchical unit structure (a calendar structure) that expresses specific relations and properties among the units that compose it, for example year, week.
firthermore, we define a decomposition relation between time units and categorize it with two properties, constancy and alignment.
Decomposition thus takes into account repetitive containment between time units.
Based in this formalism, we represent specific intervals or time moments in the time line with Calendar expressions.
Examples include: ( (year,1995), (month,M)1 where ( M = 1 or M = 4) C E , i.e.
January or April 1995; g 3 0 [(year,1995), (week,W ) ,(dag,D ) ] C E , i.e.
three days per week in 1995.
These expressions subsume the language of [14].
Their slicing can be paralleled with the use of composed time units and the intersection operator in our calendar expressions, and their dicing is similar to assigning values to the different time units in our calendar expressions.
Uncertainty is covered by both formalisms (e.g.
three days per week).
Our calendar expressions additionally allow universal quantification, the "union" and "difference" operations, and displacement of the whole repetitive series by a duration.
3  Ontological similarity among the occurrences in a series  Each occurrence in a series of events or activities should have certain characteristics in common to be considered as a repeated series.
In this respect, some philosophers distin uish 'naked" repetition from "clothed" repetition [ll The first kind involves the "unvarying repetition o the same".
This case of repetition can be found for example in a cyclic numerical series, where identical numbers are repeated, as in 123123.. .. Naked repetition can also appear in a literary work, for example an identical phrase being repeated to stress a certain aspect of the novel.
On the other hand, clothed repetition introduces difference at each of the occurrences.
Arguably, repetition of events or activities in time correspond to 'clothed" repetition, given that no event or activity ever gets repeated exactly; it is a matter highly dependant on the level of abstraction.
At some level of abstraction repetition is naked.
Having said this, for the present study, we ignore these differences.
Thus, for example, having lunch every day is considered a repeated activity even though quite likely it could include different meals, etc.
Furthermore, we are concerned about repetition in terms of temporal objects, and thus in the previous example we look at the intervals during which the lunches occur and not at the quality of the lunches per se.
Hence, when giving examples, we assume instances of repetition as "similar to other instances" when they have "certain" essence in common, without entering into the philosophy of the similarity aspect.
Another aspect that we abstract in this classification is the nature of the objects that are repeated.
We won't distinguish a repeated series of, for example, "activities" from a repeated series of "properties", i.e.
we don't distinguish the different ontological nature of the occurrences or repeats.
Hence "going to a meeting every Friday" and 'we had sunny weather every day last week" are both valid examples.
Other kind of examples simply consist of a graphical picture.
8:  1  b  4  Basic terminology and definitions  In this section we formally define terminology which is used throughout this paper.
As we discussed in the previous section, the objects we consider to repeat are "temporal objects":  f  Definition 1 (Temporal objects) Temporal objects include time intervals and time points during  81  which the repeated event, activity or property of interest OCCUTS.
Gaps are also conceived as temporal objects, representing the (temporal) separation between successive repeats an a series.
of combined or superimposed repetitive series (one for each singer) with the same repeat duration but "out of phase".
1=====1=====1=====1=====1 1=====1=====1=====1===== I- - -i- - -I- - -  Definition 2 (Temporal Series) W e define temporal series occurring within a reference frame as a sequence of temporal objects as follows: A temporal series 7 of n E N repeats i s a sequence of 2n elements: 7 =< Tl,g1,1"2,g2,...,rn,Sn >, where T ; and g; are temporal objects, the i t h repeat and the i t h gap respectively; i E N,1 5 i 5 n. The reference frame i s the interval that starts when the first repeat starts and finishes when the nth gap finzshesl .
Other variations of a choir singing the same melody in some counterpoint style can be conceived to illustrate other relations between repeats which intersect.
Parallel processing and pipelining architectures provide for yet other (computer related) examples.
In the series "reading and understanding a scientific paper" there can be several papers being "processed" at the same time, thus repeats may relate with any of the relations that the definition above includes.
Even though we stick to temporal series, these definitions are directly applicable to one dimension spatial series, and in fact, we omit the "temporal" adjective whenever this does not lead to ambiguities.
Definition 4 (Point series) A point series is a temporal series where the repeats are points.
We consider point series to stress the fact that each repeat corresponds to a (durationless) time point, or to an event in time where the duration concept does not apply.
An example of a point series is that of the "daily stock market values".
Also, a point series could be representing an interval series, via the extreme points of each subinterval.
Such is the case of "P-(generalized) intervals", where "P7' is the set of even integers [15].
Furthermore, a point series can be extracted or derived from an interval series.
For example we may refer to the series of "beginning times of an (interval) series of lecture classes".
Definition 3 (Interval series) An interval series i s a temporal series whose repeats are intervals.
T h e beginning point of a subsequent repeat has to be equal OT after the beginning point of the previous one.
Therefore two contiguous intervals relate with the relations an the set {before, meets, overlaps, finished-by, conlains, starts, started by, equals} OT a disjunction of the previous.
These relations are part of the basic 13 interval relations in the Interval Algebra of [l].
To illustrate interval series graphically, a repeat will be indicated with equal signs, === and a gap with hyphens ---.
Lengths will be proportional to the duration, and a vertical bar I will separate each couple repeat-gap.
So, for example, in the series "last week it rained every day", contiguous (one day) repeats meet.
I == I == I == ... .
In the series "they go to swim Mondays and Wednesdays", contiguous repeats I ==-are one before the next one.
I ==-- I ==-------.. .
If a series contains intervals during which a device is manufactured, and if this device is manufactured in an assembly line, then these intervals overlap.
Definition 5 (Duration of a temporal object) A point has duration 0.
A convex interval duration is the daflerence between the beginning and ending poznt of the interval.
If the ending point of a n interval zs conventionally defined to be before the beginning point, the duration as conventionally defined negative.
Durations are expressed with time units.
The precision of a duration is the finest time unit the duration i s expressed with.
.
1-------------  I-----------------  We denote the beginning and ending points of an interval i as beg(i) and end(i) res ectively.
We measure durations with time units.
See 73, 41 for a formal definition of time units.
Examples of time units include year, week, etc.
A duration is for example years, 10 days and 10 hours".
In this case the precision is hours.
But the same duration could be expressed in a coarser time unit.
We consider durations rounded up to integers, for example "2 years, 10 days and 10 hours" becomes "2 years and 11 days".
More precisely, we define a duration round up function as follows:  I  I I=====-------- I  I  ----- ----- ----- I- ------I ...  ...  A series can be conceived of singers singing the same melody (over and over again) in a choir; hence (the time during which) each person sings one instance of the melody can be considered a repeat.
If they all sing together, then all repeats are equal, ie., all singers start and finish at the same time each instance of the melody.
Now consider a choir singing a canon; a canon is a contrapuntal musical composition in which various instruments or voices take up the same melod successively, before the previous one has finished [21f Canon singing is thus an example where contiguous repeats overlap.
As well, this can be seen as a number  Definition 6 (Duration function) The function 2), : Temporal-Object -+ associates a temporal object with a n integer, which denotes its duration in the time unit OT precision T. This function D r maps a n interval to a number whose absolute value as the lowest integer greater OT equal than the "exact" (possibly fractional) duration absolute value.
Duration values are negative when they correspond to temporal intervals where conventionally end(i) is before beg(i).
z  l T h e term "reference frame" appears in [NI, is referred t o as ''R-interval" in [17], "frame-time" in [20] and'keference-date" in ~91.
82  We refer to this "round up to the next integer duration" as duration, unless otherwise specified.
For example: Dday( "Meeting held on Tuesday from 1O:OO to 12:30") = 1 day.
(Notice that the "exact" duration in days would be 2.5124 2 0.1 days).
The duration of this same example, with a different precision is Dhour( "Meeting held on Tuesday from 1O:OO to 12:30") = 3 hours.
The finest precision in the preceding example is minutes, or hay hours, if such time unit existed in the time unit hierarchy; time units can be added to a time unit hierarchy to contemplate the required precision for the application [3].
Duration values can be negative; for example, Dday("a gap whose exact duration is -2.5124 -0.1 days") = -1 day.
This is further addressed in the following lemma:  Definition 8 (Distances between two intervals) We define 4 diflerent distances between two intervals il and i a : 1.
Strict-distance or gap, beg(i2) - end(i1).
2.
B-distance, i.e.
beg(i2) beg(i1) 3.
Edistance, end(i2) - end(i1) 4 .
Convex distance, the period minimally covered by the two intervals, i.e.
end(&) - beg(&).
-  Lemma 1 (Repeats duration and gaps duration) Let T =< r l , g l , r 2 , g ~ , .
.
.
, r ~ , g>, n n,i E N , 1 5 i 5 n, be a temporal series.
If 7 is a point series, then, for any precision T, 1 5 i 5 n, D*(ri) = 0 and D,(gi) > 0.
Zf 7 is an interval series, then, for any precision n, 1 5 i 5 n, Dr(ri) > 0 and  >0  'Dw(gj)  {  For example consider the series "Monday of every week".
The b-distance and e-distance between each successive subinterval are 1 week (or 7 days).
The strict distance or gap is 6 days.
The convex distance is 8 days.
For a graphical picture of the different distances between two disjoint intervals see Figure 1.
Both, the b-distance and e-distance include the duration of one of the two intervals involved.
Furthermore, the strict distance is negative when the two intervals intersect.
Our convex distance definition is inspired in the convexification operation defined in [13].
i  I  zfrj before) ri+l, = 0 if ri meets) r;+l, < 0 if ri (overlaps or finished-by or contains or equals or starts or started-by)) ri+l.
I2  I1  : L  bdistance e-distance  j  strictdistance  '  convex distance  Figure 1: Graphical representation of four different distances between two disjoint intervals  Specifically, Dn(gj) = -lDr(rj+l)l if ri (equals or starts or started-by) ri+l,  Definition 7 (Repeats-precision, gaps-precision) The repeats-precision is the precision used to measure all the repeats in a series.
The gaps-precision (possibly diflerent than the repeats-precision) is used to measure all gaps.
For example, the series "someone's birthdays" can be measured with day as the repeats-precision and year as the gaps-precision.
The intuitive idea of having a precision, is that the finest possible is the one that the natural language expression of the series provides information about.
Hence, there is no information to express durations in seconds in the previous example.
But this is not imposed as a restriction, and therefore the series "someone's birthdays" could be expressed in seconds, if the application so requires.
Similarly, a precision of millennium wouldn't be very natural in the previous example, but similarly it could be so required.
Some repeats may have an undefined duration, start and/or end (and therefore an undefined precision), as in "they go rafting every Spring".
In this example a point series would be an adequate representation, where gaps would be of one year of duration.
Since points are conventionally durationless, the distance between two successive points in a point series is equal to the duration of the gap between them, i.e.
the difference between the two points.
For example, "the daily stock market value" series has a distance of one day between each pair of successive points.
Definition 9 (Convex closure) The convez closure of successive repeats in an interval series is the minimal convex interval covering all those repeats.
This definition generalizes the convex distance between two intervals just defined.
A new series can be obtained by convexifying subseries in it.
For example 'classes are held on Mondays, Wednesdays and Sundays every week" can be convexified t o "Classes are held every week"; each subseries composed of three (day) intervals and the gaps between them was convexified into one interval.
5  Parameters of a temporal series: Towards a classification.
In this section we analyze parameters or classification axes that characterize temporal series.
Series are organized in a lattice according Lo possible combinations of "values" each parameter takes.
We distinguish five parameters to classify the repetition pattern: 1.
Interval series qualitative structure.
2.
Duration of repeats pattern.
3.
Distance between repeats pattern.
4.
Frequency of repeats per period of time.
Additionally, to present a broader taxonomy, we analyze repetition situations which stem from the application domain and not the temporal domain: 5 .
Temporally vs. event driven repetition.
Finally, we analyze how the reference frame (time during which the series occurs) can be specified: 6.
Reference frame specification.
83  Arguably, these parameters analyze the patterns of the essential elements in a repetitive series.
Hence, we propose that cyclic, probabilistically cyclic, random, and any other type of series results from the combination of possibilities of these parameters.
Further sections define the possible values for the parameters, along with numerous examples.
Parameter  -  a.
Fully specified i.
With a function 1. constant durations 2. time unit based constant durations 3. cyclical equal durations 4. cyclical functionally related durations ii.
By extension 5. known a-priori durations b.
Partially specified i.
Approximate information 6 .
bounded durations 7. probabilistic durations ii.
Incomplete information 8. incomplete duration information c. Not specified 9. not known duration repetition pattern  Interval series qualitative struc-  ture: Let 7 =< rl,gl,r2,g2 ,...,rn,gn >, n,i E N,1 i n, be a temporal series.
There are  < <  2* - 1 = 255 possible values for this parameter, which reflects how ri relates to ri+l for all i : by definition, the possible relations are 8 basic meets, before, overlaps, finished-by, contains, equals, starts, started-by and any disjunction of them.
One possible value (of the 255) is all-before, as in the series "artificial intelligence seminar meetings take place on Fridays".
Another possible structure is all-(meet or before), as in "sunny days in Vancouver", where the repeats-precision is expressed in days.
Another possible structure is all-overlaps as in the canon singing example above.
Notice that a new series can be obtained by convexifying subseries in it, and the qualitative structure (as well as other parameters) of the convexified series can vary from the original one.
For ., with an example, I ===e=I=---- I===== I =---- I all-(before or meets) structure can be convexified to I===========I===========I , with an all-meets structure.
Similarly, series with intersecting consecutive repeats can be convexified to non-intersecting repeats.
Clearly, the structure should reflect the detail according to the precision level desired for the application.
We believe that it useful and arguably needed to specify precisely this parameter when defining a repetitive structure.
This parameter qualitatively characterizes a repetitive series and thus provides information which importantly can restrict the usage and algorithms needed to deal with it.
Table 1: Possible values for the duration repetition pattern parameters.
3.
Cyclical equal durations: There exist IC > 0 different values v ., and these values repeat cyclically, i.e: For ah j E N, 1 5 j 5 IC, for all t E N , ( j tIC) 5 n, Dr(rj+tk) = D, 1.'j.
IC is the repeats duration cycle.
Examp e: series of experiments such that the even numbered experiments take 10 min., and the odd numbered experiments take 15 min."
(IC = 2).
pLL=  +  ..  ...  4.
Cyclical functionally related durations: There exist IC > 0 different values vj, and these  values repeat cyclically modulo a function (not the identity), i.e: For all j E N,1 5 j 5 IC, for all t E N , ( j ( t 1)IC) 5 n, D r ( r j ) = v j , Dr(rj+(t+l)k) = f j ( D r ( r j + t k ) ) - Example: consider the series of intervals during which a pendulum moves between each (almost instantaneous) stop when it reaches one of the two highest positions.
Since there is friction, the intervals during which the pendulum moves decrease monotonouslv (according to some function f), until the pendilum stops.
So, D,(ri+l) =  + +  -  Parameter Repeats duration repetition pattern: We define 9 possible values for this parameter.
Table 1 summarizes these values and how they are structured.
Given this structure of possible values, it appears that they cover all possibilities within the parameter; the value is either completely, partially or not specified, etc.
The definition of these 9 values with examples follow.
Let 7 =< rl,gl,rz,g2!...,r,,g, >, n,i E N, 1 5 i 5 n, be a temporal series.
f (vr(ri))-  5.
Known a-priori durations: Durations are known a priori but with no specific repetition pattern, Le, there exist n > 0 different values vi,.and Dor(ri)= U;, 1 i 5 n. Example: "in a seminar, attending a talk of 1 hour, then another during half hour and then another for 1 hour.
<  1.
Constant durations: For all i, j , 1 5 i , j 5 n, Dor(ri)= D r ( r j ) , for any precision T. Example: "series of experiments taking 10 min each".
<  6 .
Bounded durations: For all i,1 i 5 n, lowDr(ri) highest-value.
Example: est-value "series of songs which take from 2 min.
to 5 min.".
<  2.
Time unit based constant durations: For all i,j 1 5 i , j 5 n, DT(1.i) = Dor(rj),but this constancy is apparent because of the time unit.
Example: "series of observations of 1 month each".
If the precision is month, then all the durations are the same, however, because months have different lengths in days, durations clearly are not 'really" constant.
<  7.
Probabilistic durations: There exist w > 1 different values vj, and the probability of a repeat having one of these values as a duration is known: For all i, 1 5 i 5 n, P(D,(ri) = vj) = p j , C,",,pj = 1.
Example: "series of experiments  84  -  which take 10 min 40% of the repetitions and 15 min 60% of the repetitions".
Parameter Temporally or event driven repetition: We analyze here event-driven repetition as opposed to temporally driven repetition.
This parameter is orthogonal to the previous.
Consider the example "change the oil in the car every 6000 km".
It can be noticed that there is no temporal information that can predict when the change of oil will take place.
However, there is a certain repetitive pattern, the event that the car covers 6000 km.
In fact in this case we are comparing two series.
The series which has an unknown temporal repetition pattern: "the car covers 6000 km" and the series "the oil in the car is changed", which is related to and inherits the randomness from the former.
The two series are related in this example with the operator always after [12].
8.
Incomplete duration information: The duration information is not known for all n repeats, but there exists a proper subseries about which information regarding repeats duration is known with some of the previous criteria.
9.
Not known duration repetition pattern: Each repeat has no known a priori duration, i.e., there is no specific pattern.
Example: "reading and understanding scientific papers" (in principle, it is not possible to bound the time it takes to read different papers and understand them).
Parameter - Distance between repeats pattern: The formulation of the possibilities for a parameter analyzing the strict distance between repeats is analogous to the one for durations of repeats.
In both cases we analyze durations of intervals, in one case we analyze repeat durations, in the other, distances between repeats durations.
Therefore there are 9 possible values just as in the previous case, see Table 5.
Definitions are analogous; for example, constant repeats duration was defined as ri = r j , for all i , j , 1 5 i,j 5 n, whereas a constant strict distances (or constant gaps) occurs when g; = g i , for all i , j , 1 5 i , j 5 n. Furthermore, in Section 4 we defined four kinds of distances between intervals.
Possible values of a parameter analyzing these distances are also the same for the four different distances.
Thus the same 9 possible values are considered for each distance.
Obviously the resulting series may present very different patterns for the same kind of value.
For example the following series has constant b-distance but not constant strict distance: ==I-- I==--- I=---- I==--- .. .
.
NOtice that for those distances which include at least one of the repeats, there is an interconnection between the parameter analyzing the durations of the repeats and the parameter about distances between them.
For example, if the durations are constant and the strict distances are cyclic, the b-distances are cyclic: ===--- I I ===--- I ....  ---_--  -  Parameter Reference frame: The reference frame of a series is the interval minimally covering it, Definition 2.
One way of looking at the reference frame is thus as one repeat series, the result of convexifying any whole series.
Therefore the values considered for the previous parameters also suggest possibilities for classifying the reference frame.
Since it particularly constitutes the interval during which there is a series of repeats, other values can specify it, such as total number of repeats.
Table 2 summarizes the structure used to classify this parameter and following we provide the definition and examples.
Temporally determined reference frame 1.
Fully specified 2.
Partially specified i.
Approximate information ii.
Incomplete information 3.
Not specified Application determined reference frame 4.
Repeat since 5.
Repeat while 6.
Repeat until Table 2: Possible values for the reference frame parameter.
--___-  -  Parameter Frequency: The frequency of repetition over a certain period is related to the previous parameters in many cases and is independent in others.
For example "Mondays and Tuesdays every week" has a frequency of two repeats per week.
This corresponds to a cyclical equal gaps series, with cycle length equal to 2.
The series "going to fitness classes three times a week" has uncertainty with respect to the duration of the repeats and the distance between them, thus the previous two parameters could be assigned the gthvalue, i.e.
not known duration repetition pattern.
However, clearly this series has a known frequency repetition pattern.
An example of a bounded frequency series is "my heart beats between 150 and 174 times per minute after a series of cardio exercises", as is "at least three times a week".
This parameter then can have the 9 possible values like in the previous parameters, adopted t o frequencies.
1.
Fully specified reference frame.
Reference frames are convex intervals, and as such can be fully specified by two values: the two extreme points or an extreme point and a duration.
Since they also constitute the interval during which there is a series of repeats, they can also be defined as an extreme point and total number of repeats.
Examples include a series takin place "from January lst1996 until December 313 1996", or a series of "10 classes, with a certain repetition pattern, starting March llth1996".
Infinite intervals also are considered fully specified intervals, for example AD.
2.
Partially specified reference frames include bounded or probabilistically known values.
For example the starting point could be defined by  85  "one day in March lSt- March 3'd or "March lSt 1996 with 10% probability, March 2nd with 70% probability" and March 3'd with 20% probability.
A doubly cyclic series includes the class whose durations are cyclic and distances are cyclic as well.
The general cycle is the minimum common multiple of the two cycles, the repeats cycle and the gaps cycle.
For example D,(r,) = 2, Dr(rz) = 3, and then it repeats (i.e repeat cycle is 2).
Likewise, D o , ( g l ) = 3, D,(g2) = 1, D,(g3) = 2 and then it repeats (gaps cycle is 3).
The general cycle is 6.
Graphically:  3.
Incomplete information includes cases where only one value specifying the reference frame is known.
For example only the duration is known or only the number of repeats, but no extreme point.
==---1===-111--1===---1==-1===--1==---  ....  A cyclic series (not "doubly") could be cyclic with respect to distances between repeats, but have some other pattern of repetition with respect to repeats duration.
For example "One hour classes taking place Monday and Tuesday every week".
A triple stochastic series is the class with probabilistic repeats duration, gaps duration and frequency.
4-6.
Application determined.
Finally, the reference frame of a series, just as the repetition pattern, can be determined by the application domain.
Hence the series can be defined since, while or until a condition holds.
The options of specifying the reference frame by the number of repeats, while or until a condition holds correspond to the basic iterative constructs in an imperative programming language.
7  Classification of different proposals under the present parameters  It is possible to classify and compare the different approaches that have appeared in the literature according to the parameters suggested here, and therefore be able to compare them.
This kind of classification should also help as a guide to define or review languages and algorithms dealing with repetition.
We next analyze several approaches that have recently been reported according to the qualitative structure parameter.
A complete comparison considering all parameters is out of the scope of this paper.
In the literature surveyed, when the subsequent intervals have a gap between them, the series is considered as one "non-convex interval" [12,13].
Thus it corresponds to interval series where subsequent repeats are one before the next one.
A series with either disjoint or meeting contiguous subintervals is referred to as a "general non-convex interval" in [12], and can be paralleled with an "order 1 collection of intervals" [14] and with the temporal counterpart of "repeated activities" [19].
Collections of greater order also constitute interval series of either disjoint or meeting repeats.
A "partition of intervals" [18] is a series of meeting subintervals.
Similarly, meeting or disjoint repeats are contemplated in "sequences of occurrences within an R-interval" [17].
Expressions representing "periodic events" [20] have a calendar-based repetition subexpression which rely on Leban et al's collections.
Repetition as axiomatized by Koomen [lo] allows for meeting contiguous intervals only, and to express calendar based repetition and allow gaps between repeats this axiomatization relies on Leban's collections.
Gabbay's language to represent temporal repetition includes interval series and point series, where the distance between subsequent beginning points of the repeats is always constant [9].
Our calendar expressions represent repetitive series where successive repeats are disjoint or meet [4].Additionally, these expressions can be combined with "set operations": union, intersection and difference or displaced by a duration.
It is therefore possible to express, for example, the union of series which would result in overlapping repeats.
We are not aware of repetitive temporal series being defined with overlapping nor any kind of intersecting repeats in the AI and temporal reasoning literature.
6 Combining the values of patterns to form a taxonomy The combination of all the possible values of the parameters defined is currently under study.
Maybe not all the parameters generate a realistic repetition pattern, and several combinations may result in the same class.
We preview a machine case analysis as well, to obtain a complete taxonomy and further investigate repetition characteristics.
However, we believe the characterization we present in this work already provides for a simple and complete classification of repetitive series, with respect to the suggested parameters.
We present here only some classes that should belong to such taxonomy to exemplify the coverage of our classification.
An important class is that of periodic series, which we define as the series having constant b-distances (constant distances from be inning point to beginning point of subsequent repeat$.
However, given the parameters and values proposed we distinguish several variations of periodic series, a matter which is not distinguished in the literature surveyed.
In a simple periodic series, repeats durations are constant, gaps are constant (and, in case of an interval series it follows that the other distances are constant as well).
Frequency is constant too.
Furthermore, the structure of the series can be all-meet or all-before.
For example "a TV show that takes place every day, only in one TV channel, at the same time, with a one hour duration" constitutes such series.
An intersecting periodic series would be a variation of the previous where repeats may intersect.
The same example applies, but the same (pre-recorded) show would be forecasted in other TV channels as well, at intersecting time intervals.
A time unit based periodic series is "apparently" periodic, because of the time unit chosen t o measure distances between repeats, such as in a repetition once a month.
A periodic beginnings only series is such that the duration of the repeat and the strict distances are not constant, though the b-distances are, for example: -----1=----1=====1==---I------- I ...  86  8  Summary  TIME'95, pages 147-154, Melbourne, FL, USA, 1995.
In this paper we have analyzed the essence of temporal repetition, and presented a classification according to parameters determining the various types of repetition.
We believe that reasoning about repeated activities calls for such a study.
Several notions have been considered, some are extremely general, some are very specific.
The analysis of all possible combinations of the parameter values is under study.
We are also investigating how operations on repetitive series interact with the values the parameters take.
We believe that the set of parameters proposed involve the essential components in a repetitive series: the repeats, the distances between them and the reference frame.
Different applications could lead to the consideration of additional parameters, or a more detailed partitioning in the values the ones proposed here take, however we believe this characterization provides for a simple and complete classification or scheme of classification of repetitive series, with respect to the proposed parameters.
Finally, we find that this kind of classification can help comparing different proposals addressing problems where the domain involves repetitive structures, and guide new developments of both declarative languages and implementations.
[9] D. M. Gabbay.
A temporal logic programming machine.
In T. Dodd, Owens R, and S. Torrance, editors, Logic programming: expanding the horaxons, pages 82-123.
Intellect, Oxford, England, 1991.
[lo] J.
A. G. M. Koomen.
Reasoning about recurrence.
International Journal of Intellzgent systems, 6:461-496, 1991.
[ll] U. Kumar.
The Joycean labyrinth: repetition, time, and tradition an Ulysse8.
Clarendon Press, Oxford [England], 1991.
[12] P. Ladkin.
Time representation: A taxonomy of interval relations.
In PTOC.of the AAAI-86, pages 360-366, 1986.
[13] P. B. Ladkin.
Primitives and units for time specification.
In PTOC.of the AAAI-86, pages 354-359, 1986.
[14] B. Leban, D. D. McDonald, and D. R. Forster.
A representation for collections of temporal intervals.
In PTOC.of the AAAZ-86, pages 367-371, 1986.
References [l]J. F. Allen.
Maintaining knowledge about temporal intervals.
Communications of the ACM, 26( 11):832-843,1983.
[15] G .
Ligozat.
On generalized interval calculi.
In PTOC.of the AAAI-91, pages 234-240, 1991.
[2] E. Ciapessoni, E. Corsetti, A. Montanari, and P. San Pietro.
Embedding time granularity in a logical specification language for synchronous real-time systems.
Science of Computer Programming, 20:141-171, 1993.
[16] R. Loganantharaj.
Representation of, and reasoning with, near-periodic recurrent events.
In Workshop of Temporal and Spatial reasoning, zn conjunction with IJCAI-95, pages 179-184, 1995.
[3] D. Cukierman.
Formalizing the temporal domain with hierarchical structures of time units.
M.Sc.
Thesis.
Simon F'raser University, Vancouver, Canada, 1994.
[17] R. Loganantharaj and S. Giambrone.
Probabilistic approach for representing and reasoning with repetitive events.
In PTOC.of the the Eaghth Florida Artificial Intelligence Research symposium, FLAIRS-95, pages 26-30, 1995.
[4] D. Cukierman and J. Delgrande.
Expressing time intervals and repetition within a formalization of calendars.
1995.
Accepted in the Computational Intelligence journal subject to revisions.
[18] R. A. Morris, W. D. S h o d , and L. Khatib.
Path consistency in a network of non-convex intervals.
In PTOC.of the IJCAI-93, pages 655-660, 1993.
[19] M. Poesio and R. J. Brachman.
Metric constraints for maintaining appointments: Dates and repeated activities.
In PTOC.of the AAAI-91, pages 253-259, 1991.
(51 D. Cukierman and J. Delgrande.
A language to express time intervals and repetition.
In International Workshop on Temporal Representation and Reasoning, TIME'95, pages 41-48, Melbourne, FL, USA, 1995.
[20] P. Terenziani.
Reasoning about periodic events.
In International Workshop on Temporal Representation and Reasoning, TZME '95, pages 137144, Melbourne, FL, USA, 1995.
[6] R. Dechter, I. Meiri, and J. Pearl.
Temporal constra.int networks.
Artificial Intelligence, 49:61-95, 1991.
[21] Webster.
Webster 's Ninth New Collegiate Dictionary.
[7] C. E. Dyreson, R. T. Snodgrass, and M. F'reiman.
Efficiently supporting temporal granularities in a dbms.
1995.
Personal communication, submitted to VLDB'95.
[8] J. Euzenat.
An algebraic approach to granularity in time representation.
In International Workshop on Temporal Representation and Reasoning,  a7
Extending the RETE Algorithm for Event Management Bruno Berstel ILOG 9, rue de Verdun 94253 Gentilly Cedex, France berstel@ilog.fr  Abstract A growing number of industrial applications use rule-based programming.
Frequently, the implementation of the inference engine embedded in these applications is based on the RETE algorithm.
Some applications supervise a flow of events in which time, through the occurrence dates of the events, plays an important role.
These applications need to be able to recognize patterns involving events.
However the RETE algorithm does not provide support for the expression of time-sensitive patterns.
This paper proposes an extension of RETE through the concepts of time-stamped events and temporal constraints between events.
This allows applications to write rules that process both facts and events.
Category, Track, and Topics Regular paper.
Track 1: Temporal Representation and Reasoning in AI.
Topics: temporal languages and architectures, reasoning about actions and change, time and nonmonotonism.
1 Introduction Incremental pattern matching algorithms, and in particular the RETE algorithm, are extensively used in industrial applications implementing pattern matching problems.
However, time is not specifically considered in these algorithms.
As a consequence, applications which monitor a flow of time-stamped events cannot use the pattern matching algorithm for the recognition of time-sensitive patterns.
This paper presents an extension of the RETE algorithm that supports temporal constraints between time-stamped events, and incrementally matches the occurrences of patterns including events as well as regular facts.
In the next section we review some related works, both about incremental pattern matching algorithms and about recognition of temporal patterns.
In Section 3 we introduce an example of a rule involving facts and events.
Section 4 presents our extension of the RETE algorithm, and illustrates it using the example rule.
Finally, Section 5 discusses the benefits of integrating event management in the rule engine, and Section 6 concludes and proposes future work directions.
2 Related Works Incremental pattern matching algorithms have been studied for some time now.
The two most widely known are RETE [7] and TREAT [18]; the Gator algorithm [12] is derived from them.
As mentioned before, these algorithms do not specifically consider time and thus offer no time-related constructs.
Several industrial  products implement variations of the RETE algorithm: ILOG JRules [13], Rete++ [11], OPSJ [20], or JESS [21] to mention a few of them.
On-line recognition of temporal patterns has been formalized under the term of chronicle recognition by Dousson [4, 9, 3].
Several chronicle recognition algorithms have been published [16, 4, 6].
Likewise, several chronicle recognition systems have been developed, such as IxTeT [10], two systems named CRS [8, 19], or FONSYNT [14].
Unlike the work described in this paper, these algorithms and their implementations do not aim at providing advanced pattern matching features.
They concentrate on partial order handling, and aim primarily at identifying the sets of events which satisfy the temporal constraints.
Only then do they address the pattern matching criteria, as far as such criteria can be formulated in these systems.
All these works either focus on the pattern matching problem without encompassing time, or they only reason on the event occurrence dates without providing more than very basic pattern matching features.
Applications which want to both apply elaborate pattern matching operations, and detect the occurrence of temporal patterns, on complex objects and events, would demand the integration of event management into a commonly implemented pattern matching algorithm such as RETE.
A couple of start-up companies (Apama [1], SpiritSoft [22]) have recently announced inference engines with advanced features in temporal patterns recognition.
Unfortunately the access to the algorithms used by the products of these companies is restricted by their commercial nature.
3 Example We start with an example of a rule matching both facts and events.
The example models the following situation.
A supervisor receives events from equipments that may be off-line, on-line, or active.
The events are of three types: alarms, related to an equipment; confirmations of alarms, which mean that the reason that triggered the alarm still holds; and cancellations of alarms.
The supervisor maintains internal representations of the monitored equipments and of the events, as objects (e.g.
instances of Java classes).
These are processed according to rules.
One of them is given below (another will be given later).
It is expressed in the ILOG JRules language, a Java-like variant of the OPS5 language.
The rule monitors a sequence of alarms occurring on an active equipment, made of an initial alarm followed within 5 clock ticks with a confirmation signal.
rule AlarmConfirmation { when { ?e: Equipment(state == ACTIVE); ?a: event Alarm(eqpt == ?e); ?c: event Confirmation(alarm == ?a; ?this after[1,5] ?a); } then { assert new ConfirmedAlarm(?a,?c); } };  This rule reads as follows: For all facts of the Equipment class with a state field having the ACTIVE value; For all events of the Alarm class with an eqpt field matching an Equipment satisfying the previous condition; For all events of the Confirmation class, related to an Alarm matched by the second condition, and occurring between 1 and 5 clock ticks after this alarm; Create an instance of the ConfirmedAlarm class and assert it as a new fact.
2  This example introduces some new concepts, compared to constructs of non-temporal rule systems: event as opposed to fact, event condition versus fact condition, temporal constraint.
It also presents a rule matching together time-stamped events, and facts which have no temporal reference.
All this will be detailed in the next section.
Compared to chronicle recognition systems, the rule language illustrated above is less concise than Carleas CRS, which uses expressions close to regular expressions to describe sequences of events.
Also, in contrast with Doussonas CRS, the event time-stamps are implicit1 : temporal constraints are formulated as if the events themselves were compared.
These choices aim at keeping in line with the style of the rest of the pattern matching language.
Temporal constraints between the events are written as standard tests applied to the objects matched by the rule.
Yet this language gives the rule developer an appreciable power of expression.
Partial order between two events, for instance, can be expressed simply by extending the range of a temporal condition, as in ?this after[-10,10] ?a.
Expressing partial order in regular rule systems is usually painful.
4 Incremental Algorithm for Temporal Pattern Matching 4.1 Summary of RETE A rule engine implementing the RETE algorithm applies a set of rules to a set of facts.
A rule is made of a collection of conditions (the example rule above has three conditions) associated with a sequence of actions to be applied to each collection of facts matching the rule conditions.
The rules are compiled into a graph, commonly known as the RETE network.
The nodes of this network represent the tests expressed in the rule conditions.
There are three types of nodes, corresponding to three types of tests.
Class nodes filter facts according to their classes; discrimination nodes retain facts according to the values of their attributes; join nodes combine facts satisfying a given relation into tuples.
In figure 1 on page 5 class nodes are not represented; the discrimination nodes representing the three conditions of the rule are respectively left1, right2, and right3; the join2 and join3 nodes are the join nodes of the last two conditions.
The rule engine only considers the objects stored in the working memory when looking for facts to match the rule conditions.
Three operations are defined on the working memory: assert adds a fact to the working memory; retract removes a fact from the working memory; and update instructs the engine to reconsider the matching state of a fact.
When a fact is asserted into the engine working memory, it is submitted to the class nodes of the network.
If the fact satisfies the tests held by these nodes, it is stored there and passed to the next level of nodes.
This is repeated until either the fact fails on a test, or it waits for another object to match, or it exits the network in a tuple and a rule is fired.
At any time, the RETE network stores the facts and combinations of facts that match as much of the condition rules as possible.
Each operation on the working memory incrementally updates the network state.
Using the example rule above, let us assume that one active Equipment and two Alarm on this equipment have been asserted.
At this point, the RETE network is in the state described by figure 1, with the exception that node right3 is still empty.
An instance of the Confirmation class is created for the second alarm within 5 time units and asserted into the engine working memory: It is stored in the class node for the Confirmation class.
Then it is passed to the right3 discrimination node, which stores it.
1  Although they are accessible using the timeof primitive.
3  Then it is passed to the join3 join node, which evaluates its tests on the Confirmation instance and on the pairs stored in the left2 node.
The tests succeed with the second pair, so the node creates a triple from the matching pair and instance.
The actions of the rule are then executed on this triple.
4.2 Introducing Time and Events In order for the engine to support temporal reasoning, we equip it with a clock.
The operations required on the engine clock are: a function returning the current time, and a function incrementing the current time by one tick.2 When the clock time is actually incremented, is left to the application embedding the rule engine.
Observe that the clock needs not be connected to a real-time clock.
In the regular RETE algorithm the facts, that is, the objects stored in the engine working memory, bear no temporal information.
In particular, our extension, in the absence of events, is strictly equivalent to classical RETE.
Here we introduce the concept of event.
An event can be stored in the working memory just like a fact, and the engine maintains a time-stamp for each event.
In the simplest case, the time-stamp of an event is the value of the engine clock when the event is asserted.
Since there is no difference in structure between a fact and an event, what makes the engine distinguish between them is the way they are asserted.
The facts are the objects asserted with the assert primitive, whereas the events are the objects asserted with the assert-event primitive.
The conditions in rules explicitly indicate whether they match facts or events.
In order to specify temporal constraints between events, we use before and after predicates.
The  after[ fi ,  ]  predicate is true iff   	   fi  , where  is the time-stamp of the   event.
The bounds may be infinite.
Temporal constraints can be combined together and with non-temporal tests, using disjunction, conjunction, and negation operators.
Of course, temporal constraints can only be specified between events.
Note that there is always a temporal constraint between two events in a rule.
If none is explicitely specified, it means that the events can occur in any order, which corresponds to a constraint with two infinite bounds.
4.3 Focus on the Join Node The join nodes are the places where objects which individually satisfy the conditions of a rule are matched against each other.
When they match, they are assembled into tuples on which the actions of the rules are eventually executed.
In the RETE algorithm this assembly is incremental: objects matching the first two conditions of a rule are assembled into pairs, which in turn are assembled into triples with the objects matching the third condition, and so on.
Each join node has two inputs and one output.
The inputs are an  -tuple coming from the join nodeas parent left node, and an object coming from its parent right node; the output is an   -tuple and is sent to a child node, which in turn is the parent left node of another join node.
Figure 1 represents the left, right, and join nodes corresponding to the conditions of the rule introduced in the previous section.
Letas illustrate on this figure the behaviour of RETE when the second alarm and its confirmation are asserted.
When the second alarm (noted Alarm2 in the figure) is asserted, it is stored in the right2 node and submitted to the join2 node.
The join node matches the alarm against the sole element in its left nodeas storage, that is, [Equipment1].
The match is successful, so a pair [Equipment1,Alarm2] is formed and transmitted to the left2 node, which stores it and submits it to the join3 node.
2  We consider time as a discrete succession of ticks.
4  left1 [Equipment1]  right2 Alarm1 Alarm2  [?e]  ?a  join2 ?a.eqpt == ?e  right3 Confirmation2  left2 [Equipment1,Alarm1] [Equipment1,Alarm2] [?e,?a]  ?c  join3 ?c.alarm == ?a ?c after[1,5] ?a  [Equipment1,Alarm2,Confirmation2]  Figure 1: Focus on the join nodes of the RETE network implementing the example rule The join3 node receives the new pair and matches it against all the objects in its right nodeas storage, that is, none for the time being.
This stops the propagation of Alarm2.
When the confirmation for the second alarm (noted Confirmation2 in the figure) is asserted, it is stored in the right3 node and submitted to the join3 node.
The join node matches it against all the pairs in its left nodeas storage.
Only the match with the [Equipment1,Alarm2] pair is successful, so a triple is formed and sent down.
Note that parent nodes of a join node store tuples and objects, and that these are used by the join node each time a new element is submitted.
In other words, the parent nodes store all the candidates for future matches by the join node.
4.4 Extending RETE When time is not involved, each parent node stores all the tuples or objects it receives, so that the join node may match them with new objects or tuples coming from the other parent node.
Furthermore, the node must store them until they are explicitly retracted from the working memory, since a new object matching them may be asserted at any time.
On the other hand, when a condition includes temporal constraints, this gives a limit in time to its satisfiability.
This limit is used to bound the time during which objects and tuples are stored in parent nodes of a join node.
For instance, the third condition of the example rule includes the constraint ?c after[1,5] ?a, that is, athe confirmation must occur between 1 and 5 ticks after the alarma.
Assume that the first alarm 5  occurred at date 10: after date 15, the condition will never match this alarm with any confirmation, because the temporal constraint will never be satisfied.
This means that the left2 node needs only store the [Equipment1,Alarm1] pair until date 15, and can then release it.
In contrast, the temporal constraint in join3 indicates that confirmations only match with alarms that occurred beforehand.
This means that the right3 node never needs to store Confirmation instances.
Similarly, instances of the Alarm class need not be stored by right2.
In our extension of the RETE algorithm, we implement this by adding a dialog between the join node and its parent nodes: When an element (tuple or object) is submitted to a join node by a parent (left or right) node, the join node computes the elementas expiry date with respect to the temporal constraints it stores.
This date is computed from the time-stamps of the events borne by the element, and from the bounds in the temporal constraints held by the join node.
If the expiry date of the element has not yet been reached, the parent node keeps the element at least until this date, so that the join node may match it against new objects or tuples being that it may be submitted.
On the contrary, if the element has expired, there is no need for the parent node to store it.
In both cases, the join node informs its parent node of whether the element should be kept or not.
In the case where the expiry date of the element is in the future, the join node posts a request to be notified at that date.
When it is notified, the parent node will no longer need storing the element.
The join node then informs its parent node, which can thus remove the element from its storage.
The dialog takes place at two moments.
First when the parent node submits the element to the join node, as an answer from the join node to its parent, telling whether the parent node should store the element for future matches.
Second when the expiry date of the element for the join node is reached, as a message from the join node to its parent, telling that the parent node can now stop storing the element.
The join node must thus be notified of time changes, or at least of the relevant ones.
This can be implemented through a timer mechanism managed by the engine and based on its clock.
Note that the computation of the element expiry date, as well as the dialog between the join node and its parent nodes, always occur.
They do not depend on whether the element could be matched by the join node or not.
In our example, alarms and confirmations are never stored in the right nodes, yet they take part to tuples when they match other facts and events.
This is because the matching trials are performed on data previously asserted, whereas the expiry date computation addresses future assertions.
Observe also that, in the RETE network resulting from the compilation of the rule set, a left or right node can be the parent of several join nodes.
This happens when similar conditions appear in several rules.
As a result the parent node should store an element as long as at least one of the join nodes requests it.
4.5 Matching Facts With Events Let us consider an extension of our example with the following rule: rule AlarmCancelled { when { ?a: event Alarm(); ?c: event Cancellation(alarm == ?a; ?this after[1,4] ?a); } then { retract ?a; } };  The first condition of this rule concerns the Alarm class and contains no discrimination tests, just like the second condition of the AlarmConfirmation rule.
In the RETE network, these two rule conditions 6  will share the right2 node.
When asserted, alarms will be stored in right2, and then passed both to join2 and to a new join node (name it join4) representing the tests in the second condition of the AlarmCancellation rule.
Because this new join4 node contains the ?c after[1,4] ?a temporal constraint, the dialog between right2 and join4 will conclude that the alarms should be kept for 4 ticks by right2.
(Whereas join2 doe not require right2 to keep the alarms.)
Assume now that an Alarm3 is occurs (at date 30, say) on an equipment which state is aon-linea and not aactivea.
The right2 node will keep the alarm until date 34.
Suppose that the equipment becomes active at date 32, it will become eligible for a match with Alarm3 in the join2 node.
Would the AlarmCancellation rule not have been in the rule set, this match would not have occurred.
This is not normal.
What is not normal is that a condition may or not match, depending on the presence or absence of other rules.
The join2 node should behave identically regardless of the result of the dialog between right2 and join4.
Namely, it should behave as if Alarm3 had not been kept by right2, and ignore it when the equipment it submitted from left1.
We integrate this behaviour into our extension of RETE by formulating the following principle: In a rule matching facts and events, all fact conditions must be satisfied as soon as the first event condition has been satisfied, and until the last one has been satisfied.
This principle is sufficient to solve our problem as explained below.
Consider a rule with fact conditions and event conditions.
When an event satisfies the first event condition, it is submitted by the discrimination node of this condition to the child join node, to be matched against the tuples in the parent left node.
Since this is the first event condition, the tuples in the left node only contain facts.
These are the facts that satisfied (and still satisfy) the fact conditions preceding the event condition in the rule.
For successful matches, augmented tuples are passed down to the join node representing the next condition.
Event conditions will simply add matching events to the tuples.
When the next fact condition is reached, the tuple containing events is matched against the facts stored in the parent right node.
These facts satisfied (and still satisfy) the newly reached fact condition.
Because it is a fact condition, the dialog between the join node representing the condition and its parent left node will conclude that the tuples need not be kept by the left node.
The propagation continues until either all the conditions have been satisfied and the rule is triggered, or no match succeeds in a join node.
Along the propagation path, the tuples containing events are only stored in nodes which are parent nodes of join nodes representing event conditions.
Parent nodes of join nodes representing fact conditions do not store tuples containing events.
In the case where a node is, like right2, the parent of several join nodes, some of which represent event conditions and other represent fact conditions, it is the job of those join nodes representing fact conditions to ignore the objects stored in its parent node when a new fact satisfies the fact condition.
The principle exposed above states in which time space facts are avisiblea to events.
For each event it divides the world of facts into their states before the event was asserted, and their states after the event was asserted.
(The state of a fact can be defined as the collection of the values of its attributes.)
Only the facts in their states before an event was asserted are considered for matches with the event.
The second part of the principle (auntil the last one has been satisfieda) addressed symmetrical issues, for example the case where the equipment state would change from aactivea to aon-linea between the occurrences of an alarm and of its confirmation.
7  5 Benefits 5.1 Non-monotonism It is usually difficult to write a rule that recognizes a change in the value of an attribute of a fact.
Indeed such a rule would typically include two conditions describing the initial and final values of the attribute, but these conditions can not be matched simultaneously.
A common solution is to introduce a control object which states that the initial value has been reached, and to write a rule on this object and the final value.
Such a solution artificially adds intermediary objects and increases the number of rules, leading to a program which is harder to write, understand, and maintain.
Instead, we can leverage the event concept to express that a fact has reached a given state, or changed its state from some initial to some final value.
This can be done either by introducing state change event classes in the application model, or by augmenting the rule language with dedicated constructs which will synthesize the events.
Once using events, the rule programmer can even use temporal constraints to express additional conditions on the state changes.
As an example, consider the following rule.
It will be triggered when an alarm occurs on an equipment within two ticks after the equipment became active.
rule AlarmOnActivation { when { ?act: event change ?e: Equipment(state == ONLINE) to (state == ACTIVE); event Alarm(eqpt == ?e; ?this after[0,2] ?act); } then { modify ?e { state = OFFLINE; } } };  This rule uses the aevent change ... toa construct which recognizes an initial and a final state on a fact, and generates an event when the state change occurs.
Temporal constraints can then be expressed between this and other events.
This illustrates how integrating event management in the rule engine allows to directly address the expression of non-monotonism in the rules.
5.2 Garbage Collection of Events Another benefit of integrating event management in the rule engine is the management of event retraction.
In an application which monitors a flow of events, the facts in the working memory are commonly used to describe the current state of the monitoring system, and events are asserted as they occur to be processed according to the rules.
Facts remain in the working memory until the application decides to retract them, based on its model of the monitoring system.
Events need to remain in the working memory as long as they participate to the processing logic, but should be retracted as soon as possible once they no longer play a role, in order not to affect performance.
The handling of event retraction by the application requires the programmer to take into account all the rules and the temporal constraints they include.
It can be implemented using additional rules or in the application procedural code.
In all cases this part of the program is very fragile, and difficult to test, debug and maintain.
When event management in integrated in the rule engine, this task can be performed by the engine itself.
The expiry date of each event is computed at the join node level, as explained in section 4.4, each time a temporal constraint involves the event.
The engine can thus detect when an event has expired throughout the RETE network, and then retract it from the working memory.
8  This automatic retraction mechanism mimics the work of garbage collectors in regular programming languages.
Here it applies to events in the engine working memory, to be compared with objects in the program heap.
6 Conclusion In this paper we presented an extension of the RETE algorithm to integrate event management in a rule engine.
This extension uses the concepts of event, and of temporal constraint between events.
It allows rule-based programs to recognize patterns involving time-independent facts and time-stamped events.
The expression power available to rule programmers is augmented, thanks to a more integrated handling of nonmonotonism and to the automatic retraction of obsolete events.
More generally, the rule-based programs become easier to write, debug, understand, and maintain.
The work described in this paper has been implemented in the ILOG JRulesd product.
This product includes a rules engine whose implementation is based on the RETE algorithm, as well as advanced rule programming tools, such as business rule languages, a debugger, and an extensible rule management environment.
Future work directions could be twofold.
The algorithm presented here is in essence incremental.
Other work on chronicle recognition use techniques such as domain propagation to leverage properties at the rule or at the ruleset level, and it could be interesting to integrate these techniques crosswise to the incremental approach.
Also, the acquisition of expertise is a common problem in A.I.
systems, and important pieces of work [2, 15, 17, 5] exist on the subject of extracting and learning temporal patterns from sets of data such as alarm logs, which often exist in monitoring applications.
References [1] Apama.
http://www.apama.com [2] S. Bibas, M.-O.
Cordier, P. Dague, F. LASvy, L. RozAS.
Scenario generation for telecommunication network supervision.
Workshop on A.I.
in Distributed Information Networks, 1995.
[3] M.-O.
Cordier, C. Dousson.
Alarm driven monitoring based on chronicles.
SafeProcess (to appear), 2000.
[4] C. Dousson.
Suivi daASvolutions et reconnaissance de chroniques.
ThA"se de doctorat, UniversitAS Paul Sabatier, Toulouse, 1984.
[5] C. Dousson, T. Vu Duong.
Discovering chronicles with numerical time constraints from alarm logs for monitoring dynamic systems.
IJCAI, 1999.
[6] D. Fontaine, N. Ramaux.
An approach by graph for the recognition of temporal scenarios.
IEEE Transactions on System, Man and Cybernetics, 1997.
[7] C. Forgy.
RETE: A fast match algorithm for the many pattern/many object pattern match problem.
Artificial Intelligence, no 19, pp.
17a37, 1982.
[8] France TASlAScom R&D.
http://crs.elibel.tm.fr [9] M. Ghallab, On chronicles: Representation, on-line recognition and learning.
KR, 1996.
[10] M. Ghallab, A. M. Alaoui.
Extended planning through preprocessing of knowledge: the IxTeT approach.
AAAI Workshop on Automated Planning for Complex Domains, 1990.
[11] The Haley Enterprise.
http://www.haley.com [12] E. Hanson, M. Hasan.
Gator: An optimized discrimination network for active database rule condition testing.
Tech.
Report TR93-036, Univ.
of Florida, 1993.
[13] ILOG.
http://www.ilog.com [14] J.-P. Krivine, O. Jehl.
The AUSTRAL system for diagnosis and power restoration: an overview.
ISAP, 1996.
[15] P. Laborie, J.-P. Krivine.
Automatic generation of chronicles and its application to alarm processing in power distribution systems.
DX, 1997.
9  [16] F. LASvy.
Recognising scenarios: a study.
DX, 1994.
[17] E. Mayer.
Inductive learning of chronicles.
ECAI, 1998.
[18] D. Miranker.
TREAT: A better match algorithm for AI production systems.
National Conf.
on AI, 1987.
[19] ONERA.
http://www.onera.fr/dtim/intartdis/crs.html [20] Production Systems Technologies.
http://www.pst.com [21] Sandia National Laboratories.
http://herzberg.ca.sandia.gov/jess [22] SpiritSoft.
http://www.spirit-soft.com  10
Reasoning with Sequences of Point Events* (An Extended Abstract) R. Wetprasit and A. Sattar  L. Khatib  School of Comp.
and Infor.
Tech.
Griffith University Nathan, Brisbane, 41 11 AUSTRALIA  Computer Science Program Florida Institute of Technology 150 W. University Blvd., Melbourne, F1.
32901, USA 2  Abstract We propose to model recurring events as multipoint events by extending Vilain and Kautz's point algebra [7].
We then propose an exact algorithm based on van Beek's exact algorithm) forfinding feasi le relations for multi-point event networks.
The complexity of our method is compared with previously known results both for recurring and non-recurring events.
We identify the special cases for which our multi-point based algorithm can find exact solution.
Finally, we summarise our paper with brief discussion on ongoing and future research.
A multi-point event (MPE) is a collection of points, when each point represents the related subevents.
A MPE is in normal form if all points are totally ordered, or the relation between the ith point to the i l t h point of the MPE is '<'.
Given two MPEs: I of size n and J of size m, R f I , J ) is the multi-point relation (MPR) between MPE I and J .
An element R(Ij,J j ) is a disjunction of point relation defined in point algebra [7] representing the relationship between the ith point of I and the j t h point of J .
For computational purpose, R ( I ,J ) is represented by using a A,,, matrix relation, when the rows of matrix represent the points of I and the columns represent the points of J .
Solving reasoning tasks for multi-point events in our framework is based on constraint satisfaction techniques.
A binary constraint network of k MPEs, each contains n point subevents, consists of le nodes where each node represents an individual MPE.
The domain of each node is a set of real numbers each representing a point subevent: Di = {aI,a?, ..., a, : aj < aj+l}.
The labels on the arcs are matrix relations representing constraints between two nodes.
An instantiation of a node is a k-tuple ( ~ 1 ~ x..,, 2 xk) , when zj E Dj.
The minimal label or the matrix of feasible relations between two MPEs is the matrix in which each element is consisting of all and only the feasible relations.
A scenario is a set of atomic relations between pairs of MPEs.
Each atomic relation corresponds to a matrix label for each arc.
6  1  +  Introduction  In this paper, we consider the events that are sequences of time points as multa-point events by extending Vilain and Kautz's point algebra.
Each multi-point event could be a finite sequence of recurring instantaneous actions, or a finite set of the beginning or ending of interval events that occur repeatedly.
It is straightforward to transform a multi-point event network into corresponding point algebra network.
We can then apply the existing PA algorithms to the reasoning tasks in questions.
Intuitively, we believe that reasoning with multi-point event networks should have better performance than the traditional PA network because computation on implicit relations of the same multipoint event could be omitted.
Even though, the non-convex interval model [3] can represent and reason about the recurring interval events, obtaining an exact solution for interval-based problem is a hard problem which requires exponential time algorithms [l, 31.
However, if the numbers of subintervals are known and we restrict the relations between subintervals to pointizable relations (SIA) [6], by translating to multi-point event networks, our algorithm computes the exact solutions for the original non-convex interval networks in polynomial time.
3  Reasoning with MPE Networks  The path-consistency checking for MPE network is done in two levels: between two MPEs and three MPEs.
The canonical form is defined to ensure pathconsistency between three points being in two different MPEs.
The path-consistency between three MPEs obtains from the compositzon operations over two matrix relations (defined in Section 3.2).
*A full version of this paper will appear in Proceedings of Canadian AI Conference 1996 (CSCSI-96).
36 0-8186-7528/96 $5.00 0 1996 IEEE  Multi-Point Events and Their Relations  3.1  Canonical Form  of four vertices called a forbadden subgraph.
We define the forbidden subgraph for MPE network in terms of points in MPEs as follow:  A matrix relation A,,, is said to be in the canonical form if the path-consistency conditions 51 among their neighbours as following are satisfied more detail of the algorithm for converting a matrix into the canonical form is in [3]):  t  1.
Ai,J-i  E A,,jo > ( V l 5 i 5 n, 1 < j 5 m )  2.
C > OAi,, ( V I 5 i < n , 1 5 j 5 m )  Definition 3 (A forbidden subgraph) Gwen a MPE subgraph of any four nodes: V , W , S , and T .
Let V be (Vi < ... < V, , W be < .. < Wn), S be (SI< ... < So), and d b e (Ti < ... < Tp),where m, n, o and p are the numbers of points in V,W ,5' and T respectauely.
The subgraph as called a forbidden MPE subgraph, af there exast v , w,s, and t , whach are any valid indices of V , W , S , and T , such that the followang condataons are satasjied:  3.
A;,j+l E A;,jo < (Vl 5 i 5 n, 1 5 j < m ) 4.
A ; - I , ~5 < oA;,j (V1 < i 5 n , 1 5 j 5 m )  R(V,,WW) R(V,,S,) R(W,,S,) R(V,,Tt) R(WW,Tt) R(S,,Tt)  These conditions could be used to approximate the path-consistency of the MPE network.
In our algorithm for finding feasible relations, we first transform the given matrices into canonical form.
This is to reduce the elements in the matrices and also maintain the path-consistency between two MPEs.
Lemma 1 Given a MPE network with IC MPEs (k 2 l), each contains n points (n 2 2).
The MPE network has a corresponding PA network.
3.2 Matrix Relations Operations The operations on two matrix relations, which are necessary in solving reasoning tasks, are defined by adopting the operations on constraint matrices from 41.
Table 1 defines the result of matrix operations $, @,*, 0,and') on two MPRs in terms of basic opX , -, 0 , erations in point algebra.
The symbols and correspond to union, intersection, complement, composition and inverse operators defined in PA, respectively.
+,  Complement Inverse ComDosition  = = = = =  '#;  '2 '2'  '<'  '7' -  '5'  The infeasible relation in the forbidden MPE subgraph is R(S,,Tt), which is '5'.This relation causes inconsistency among those four nodes.
However, if we don't allow '=' between S, and T,, the subgraph becomes 4-consistent [a].
If S is identical to T , the relation between S, and must be '<' because all MPEs are in normal form (either '<' or '>' allowed between points in MPE).
Thus, we don't need to take this case into consideration.
Similarly, we can show that other pairs of MPE nodes in the forbidden subgraph cannot be identical.
Therefore, the set of four MPE nodes considered as a forbidden subgraph are mutually different.
The following algorithm using the canonical form matrices and the matrix operations to solve the minimal label problem in polynomial time.
We found that we do not need to check the pathconsistency again after performing the procedure FIND-SUBGRAPHS-MPE since the network is still path-consistent.
Theorem 2 Suppose a MPE network has two nodes I and J.
Let A be the matrix relation between I and J. I f A is in canonical form, then the corresponding PA network is path-consistent.
i  =  Algorithm FEASIBLE-MPE Input: A MPE network represented as a matrix C where entry CIJ is the label on the arc from nodes I to J.
Each C I J is a matrix relation R [ I ,J],where an entry of R [ I ,J ] is an internal relation between points in MPEs I and J Output: The set of feasible relations for C I J , I , J = 1 , 2 , .., k begin PATH-CONSISTENCY-MPE FIND-SUBGRAPHS-MPE end  C = A" iff Ci,j = -Ai,i C = A iffci i = -Aji C = A 0B iff  Table 1: The Multi-Point Event Operations  Procedure PATH-CONSISTENCYXPE begin For each matrix relation R [ I ,J ] do Canonical-Conv (R[I, J1) Q := { ( I ,I(, J ) I 1 1 I-< k, 1 2 IS 5 k, K # I , J} While Q is not emDtv do selek and de1eie"a path ( I ,I(, J ) from Q  3.3 The Algorithm The algorithm to solve the minimal labels problem for MPE network is based on van Beek's exact algorithm for PA network [6].
He has shown that the pathconsistency algorithm alone is not sufficient for PA network by pointing out a counter-example consisting  j's  37  4  Temp := R[I, J ] 63 (R[I, I<]0 R[K, 51) If (Temp # R[I, J ] ) then Canonical-Conv(Temp) R[I, J ] := Temp R[J, I] := TLmp (inverse of Temp) Q := Q U RELATEDPATHS(I, J )  Conclusion  The main contribution of this framework is an extension of point-based representation to reason with the recurring events that are considered as collections of point subevents.
The algorithm we proposed correctly finds all feasible relations in the MPE network.
The complexity of this algorithm is O ( m a z ( n 5 k 3 , m n 2 k 2 ) where ), k is the number of MPEs with maximum n points and m is the number of 'f' internal relations in the network.
The complexity for finding the same solutions for non-recurring PA network with the same data (nk point events) is O(,max(n3k3,mn'k')), where m is also the number of # relations in the network [6].
For non-convex interval network, the minimal labels of the network can be approximately achieved in O(n5k3)complexity algorithm, or 0 n3k3) when considering only the affected relations [3 .
However, if we restrict the internal relations between subintervals to be pointizable interval relations (SIA) [6] and transform into MPE network, our algorithm yields the exact solutions.
The complexity of our algorithm is proportional to the number of '#' edges, the worst case would seldom occur in the real world domain.
Currently, we are working on improving Complexity of the algorithm we presented in this paper by avoiding the redundant computation of the unconstrained relations, and on constructing the algorithm for finding a consistent scenario for the MPE network.
We are implementing our algorithms and comparing the performance to other non-multi-point event approaches.
end  Procedure RELATED-PATHS/I.
J ) Return {(I, J , IC), ( K ,I , J ) ( 1 ' d K 6 k , I< # I, I< # J } Procedure FIND-SUB GRAPHS-MPE begin For each matrix relation such that R[V,W ] , = ' # I (1 5 V < W 5 k ) and (1 5 U ,w 5 n ) do Initialize P1, P 2 , P I Q1, Q2, Q to empty set For each MPE K (1 5 K 5 k , IC # V,W ) do P1 = P1 U adj-MPEs(>, Vu,IC) P 2 = P2 U a d j - M P E s ( 2 , W,, K ) Q1 = Q1 U adj-MPEs <, Vu,IC) Q2 = Q2 U adj-MPEs <, W,, K ) P =PlnP2, Q = QnnQ2 For each S, E P do For each Tt E Q do If S # T then R [ S , T ] , , ,:= ' < I R[T,S]t,,:= ( > I end  \  i  Our algorithm for computing feasible relations consists of two main tasks: checking path-consistency between three MPEs throughout the network (procedure PATH-CONSISTENCY-MPE), and eliminating the infeasible relations from the forbidden subgraphs (procedure FIND-SUBGRAPHS-MPE).
We maintain the consistency between two MPEs by calling procedure Canonical-Conv (detailed in [3]).
This procedure transforms the matrix relations into canonical form before inserting to the database.
Function a d j - M P E s ( L , V,, IC) in procedure FIND-SUBGRAPHS-MPE returns the set of elements I < k l in which R(Vu,I < k ) = '>' by checking the relations between V, and I - ,, (1 5 i 5 n ) .
Here are some technical results:  References J. Allen, "Maintaining Knowledge about Temporal Intervals," Communication of the A CM, Vol.
26:11, pp.
832-843, 1983.
E.C.
Freuder, "A Sufficient Condition for Backtrack-Free Search."
Journal of ACM.
Vol.
29, pp.
24-32, 1982.
L. al-Khatib, Reasonzng wath Non- Convex Tame Intervals.
PhD dissertation, Florida Institute of Technology, Melbourne, Florida.
1994.
P. Ladkin and R.D.
Maddux, "On Binary Constraint Problems."
Journal of ACM.
Vol.
41:3, pp.
435-409, 1994.
Lemma 4 Changzng the label R(Ss, Tt) of the forbadden subgraph defined an Definztaon 3 wall not lead to path znconszstency  A.K.
Mackworth, "Consistency in Networks of Relations."
Artzjkzal Intellagence.
Vol.
8 , pp.
99-118, 1 n-7  131 1 .
Theorem 5 The closure of a MPE network, calculated by the algorzthm FEASIBLE-MPE, corresponds to a path conszstent PA network  P. van Beek, Exact and Approximate Reasoning about Qualatataue Temporal Relations.
Technical Report TR-90-29, University of Alberta, Edmonton, Alberta, Canada.
1990.
Theorem 6 The algorathm FEASIBLE-MPE correctly finds the manzmal labels for all znternal relatzons an the MPE network.
M. Vilain and H. Kautz, "Constraint Propagation Algorithms for Temporal Reasoning."
Proceedings of AAAI-86, San Mateo; Morgan Kaufman.
1986.
Theorem 7 The algorathm FEASIBLE-MPE, for k MPE nodes an the constraant network and each node contaans at most n poants, has a tame complexaty of O ( m a x ( n 5 k 3m, n 2 k 2 ) ) where , there are m f-relataons.
38
Automata Representations for Concurrent METATEM Adam Kellett and Michael Fisher Department of Computing Manchester Metropolitan University Manchester MI 5GD United Kingdom EMAIL:{A.Kellett,M.Fisher}@doc .mmu.ac.uk  Abstract  of this automaton is exponential in the length of the original temporal specification, it has been considered that the automata representation is likely to be too large for practical use, even when improved encoding techniques are employed [4].
Consequently, only the direct interpretation of the temporal specification has been so far developed as an implementation technique [ 101.
While the interpreted Concurrent METATEMsystem is appropriate for small and medium-size examples, it often becomes unacceptably inefficient for large-scale applications, such as those involving multi-agent systems.
Thus, we here investigate improved automata representations with a view to utilising these in a compiler for Concurrent METATEM.
The structure of this paper is as follows.
In 52, we provide a brief review of temporal logic and its relationship to finite automata, followed, in 53, by an introduction to the Concurrent METATEMlanguage.
In 54, the basic approach of constructing an automaton that represents a specific temporal formula is considered, together with a number of improvements designed to reduce both space and execution time.
In $5,the relative merits of all these representations are considered.
Finally, in 56, we summarize the contributions of the paper and outline future work.
Concurrent METATEMis a language based on the execution of temporal logic formulae.
The current implementations are based upon direct interpretation, yet are too slow for large applications.
We present an approach to implementing the language by representing temporal formulae as jinite-state automata.
To combat the problems associated with constructing a single automaton containing all possible models of a formula, we partition the representation into three closely coupled automata.
This provides structures small enough to be effectively usedfor larger applications, while still allowing increased pe~ormanceover the direct interpretation of temporal formulae.
CLASSIFICATION: executable temporal logics, compilation mechanisms, automata-based representation.
1 Introduction Concurrent METATEMis a programming language for reactive systems [14] that has been shown to be particularly useful in representing and developing multi-agent systems [8].
It is based on the combination of two complementary elements: the direct execution of temporal logic specifications providing the behaviour of an individual object [9]; and a concurrent operational model in which such objects execute asynchronously, communicate via broadcast message-passing, and are organised using a grouping mechanism [7].
In this paper we consider the implementation of Concurrent METATEMobjects situated in an unconstrained environment.
In particular, we investigate altemative representations for the behaviour of an individual object based upon finite-state automata.
The temporal specification of an object's behaviour directly corresponds to a formula in a suitable temporal logic and, since there is a close correspondence between temporal formulae and automata [18], it is not surprising that an automaton representation for an object's behaviour can be generated.
However, as the size  0-8186-7937-9197 $10.00 0 1997 IEEE  2 Temporal Logic Temporal logic can be seen as classical logic extended with various modalities representing temporal aspects of logical formulae [5].
The propositional temporal logic we use (called FTL) is based on a linear, discrete model of time.
Thus, time is modelled as an infinite sequence of discrete states, with an identified starting point, called 'the beginning of time'.
Classical formulae are used to represent constraints within states, while temporal formulae represent constraints between states.
As formulae are interpreted at particular states in a sequence, operators which refer to both the past and future are required.
Examples of  12  ocp  such operators are as follows': is satisfied now if cp is satisfied sometime in the future (often termed eventualc p is satisfied now if cp is satisfied always in the ities); n future; cpU+ is satisfied now if cp is satisfied from now until a future moment when .J, is satisfied; Ocp is satisfied now if cp is satisfied at the next moment in time; cp S is satisfied now if $ was satisfied in the past and cp was satisfied from that moment until (but not including) the present moment; 0 cp is satisfied if there was a previous moment in time and, at the moment, cp was satisfied; start is only satisfied at the beginning of time.
Finally, '@ ' is the pasttime analogue of while 'M',is the past-time analogue of 'U'.
Here, the alphabet, E, is just the set of propositions, L,, used in a formula.
Each state, s E S , represents the valuation for a temporal state, i.e.
an assignment of truth values to each p E C. The initial state, SO, is distinguished as the beginning of time, specifying the first state of any model.
The set of transitions, A, is a subset of S x S , linking states to produce full sequences.
The set of final states, F , represents states which must be visited infinitely often, effectively those in which eventualities can be satisfied.
The finite model property of PTL allows an automaton representing a PTL formula to model every potential temporal sequence.
PTL formulae can therefore be fully specified by an automaton.
Unfortunately, not only is the construction of such an automaton time consuming, but it is generally much larger than the original formula.
+  'o',  2.1  Separated Normal Form As an object's behaviour is represented by a temporal formula, we can transform this formulainto Separated Normal Form (SNF) [13].
This not only removes the majority of the temporal operators, but also translates the formula into a set of rules suitable for direct execution.
Each of these rules is of one of the following forms.
3 Concurrent METATEM The motivation for the development of Concurrent METATEM [ 141 has been provided from many areas.
Being based upon executable logic, it can be utilised as part of the formal specification and prototyping of reactive systems.
In addition, as it uses temporal, rather than classical, logic the language provides a high-level programming notation in which the dynamic attributes of individual components can be concisely represented [l].
This, together with its use of a novel model of concurrent computation, ensures that it has a range of applications in distributed and concurrent systems [7].
Concurrent METATEMis an object-based programming language comprising two distinct aspects:  r  start  + V mj  (an initial 0-rule)  j=1 9  0  I\  ki  +  i=l  start f4  v r  mj  (aglobal 0-rule)  j=1  + 01  (an initial 0-rule)  01  (a global 0-rule)  OAki +  1 .
the fundamental behaviour of a single object is represented as a temporal formula and animation of this behaviour is achieved through the direct execution of the formula [9];  i=l  where each k i , mj or 1 is a literal.
Note that the left-hand side of each initial rule is a constraint only on thefirst state, while the left-hand side of each global rule represents a constraint upon the previous state.
The right-hand side of each U-rule is simply a disjunction of literals referring to the current state, while the right-hand side of each 0-rule is a single eventuality (i.e., '0' applied to a literal).
2. objects are placed within an operational framework providing both asynchronous concurrency and broadcast message-passing.
While these aspects are, to a large extent, independent, the use of broadcast communication provides a natural link between them as it represents both a flexible communication model for concurrent objects [3] and a natural interpretation of distributed deduction [ 151.
Thus, these features together provide an coherent and consistent programming model within which a variety of reactive systems can be represented and implemented.
2.2 Automata and Temporal Logic Automata are structures based on states and transitions.
Finite-state automata have been shown to have a close correspondence to formulae of temporal logic [ 181.
In particular, it has been shown that any FTL formula can be modeled by a w-automaton [16].
Each state of such an automaton represents a possible interpretation for a temporal state, while transitions between states determine valid successors.
An w-automaton consists of  3.1 Objects The basic elements of Concurrent METATEMare objects.
These are considered to be encapsulated entities, executing independently, and having complete control over their own intemal behaviour.
There are two elements to  'Due to lack of space, the full syntax and semantics of this temporal logic is omitted - see, for example, 151 for details.
13  controlled by its environment.
To fit in with this logical view of communication, whilst also providing a flexible and powerful message-passing mechanism, broadcast message-passing is used to pass information between objects.
Here, when an object sends a message it does not send it to a specified destination, it merely sends it to its environment where it can be received by all other objects.
Although broadcast is the basic mechanism, both multicast and point-to-point message-passing can be defined on top of this [7].
Finally, the default behaviour for a message is that if it is broadcast, then it will eventually be received at all possible receivers.
Also note that, by default, the order of messages is not preserved, though such a constraint can be added, if required.
each object: its integace definition and its internal dejnition.
The definition of which messages an object recognises, together with a definition of the messages that an object may itself produce, is provided by the interface definition for that particular object.
The internal definition of each object is provided by a temporal specification.
In order to animate the behaviour of an object, we choose to execute its temporal specification directly [9].
Execution of a temporal formula corresponds to the construction of a model for that formula and, in order to execute a set of SNF rules representing the behaviour of a Concurrent METATEMobject, we utilise the imperative future [2] approach.
This evaluates the SNF rules at every moment in time, using information about the history of the object in order to constrain future execution.
Thus, a forward-chaining process is employed to produce a model for a formula; the underlying (sequential) M ETATEMlanguage [l] exactly follows this approach.
The operator used to represent the basic temporal indeterminacy within the SNF rules is the sometime operator, When is executed, the system must try to ensure that cp eventually becomes true.
As such eventualities might not be able to be satisfied immediately, we must keep a record of the unsatisfied eventualities, retrying them as execution proceeds.
It should be noted that the use of temporal logic as the basis for the computation rules gives an extra level of expressive power over the corresponding classical logics.
In particular, operators such as give us the opportunity to specify future-time (temporal) indeterminacy.
Transformation to SNF allows us to capture these expressive capabilities concisely.
As an example of a simple set of rules which form a fragment of an object's description, consider the following.
3.3 Applications and Implementation The combination of executable temporal logic, asynchronous message-passing and broadcast communication provides a powerful and flexible basis for the development of reactive systems.
Concurrent METATEMis being utilised in the development of a range of applications in areas from distributed artificial intelligence [ 1 11, concurrent theorem-proving [15], agent societies [12], and transport systems [6].
A survey of some of the potential applications of the language is given in [7], The current implementation is based upon the direct interpretation of Concurrent METATEMrules.
It is written in C++ and incorporates many of the features of interpreters developed for sequential METATEM[lo].
'0'.
ocp  '0'  start Ogo 0 (moving A go)  + +  4  Compilation of Concurrent METATEM  The purpose of the compiler is to produce a representation for the compact and efficient execution of Concurrent METATEMprograms.
Having a close relationship to temporal logic, finite-state automata are a natural choice on which to base these structures [19, 181.
The product of a Concurrent METATEMexecution is a model of the temporal formula corresponding to the program.
By representing a program as an automaton all models, and therefore all possible execution sequences, are determined during compilation.
The effect is to reduce execution to a process of traversing the automata, as directed by environment interaction.
In this section we introduce the framework for the compiled Concurrent METATEMsystem.
The generation of automata representing both propositional and first-order programs is described, including a number of approaches designed to reduce the space requirements of the compiled representation.
In $5, we examine the effect of our model on both the compilation and execution behaviour.
Tmoving Omoving overheat V f u e l  Here, we see that moving is false at the start of execution and, whenever go is true in the last moment in time, a commitment to eventually make moving true is made.
Similarly, whenever both go and moving are true in the last moment in time, then either o v e r h e a t or f u e l must be made true.
3.2 Concurrency and Communication It is fundamental to our approach that all objects are (potentially) concurrently active.
In particular, they may be asynchronously executing.
Each object, in executing its temporal formula, independently constructs its own temporal sequence.
Within Concurrent METATEM,a mechanism is provided for communication between separate objects which simply consists of partitioning each object's propositions into those controlled by the object and those  4.1 Compilation Framework For our approach, the METATEM language consists of a compiler, translating from program formulae to an  14  automata-based representation, and an execution mechanism able to interpret these structures.
The compiler guarantees that for any model of the program formula, there exists a path in the automaton which represents this model.
The execution mechanism takes a compiled representation as input and produces a single infinite execution sequence, by traversing the automaton and producing a temporal state valuation for each moment in time.
Execution is directed by environmental interaction and heuristics for the satisfaction of eventualities [lo].
Although w-automata provide a mechanism for representing temporal formulae, the size of the structures produced can quickly become unmanageable.
As the formulae of METATEMprograms can be complex, the specification of many applications would be prohibited by storage requirements.
For this reason we have developed an approach to reduce the size of the representation produced.
This is based on the removal of some subset of the program formula from the interpretation of automaton states.
Based on the set of propositions L,, three subsets are defined, and each is treated separately by the compilation process:  where each p,, qj is a predicate symbol, and 2 , g are tuples of terms.
From an arbitrary FML program formula, the representation of all grounded atomic formulae derived by assigning elements of the domain to variable symbols is impractical.
An automaton must therefore provide a more general representation of first-order programs.
This requires a framework in which variable assignment can be performed at a later stage (i.e.
run-time).
First-order temporal models are generated by combining an automaton with a variable assignment produced during execution.
A finite number of automaton states can therefore represent a general interpretation of temporal states over an unconstrained domain.
We propose an automata representation for first-order logic in which an automaton state is a general representation of a set of temporal states.
An automaton state provides a binary value for each predicate symbol represented by the program.
For any element of the set of predicates, L,,, an assignmentp I-+ true indicates at least one ground predicate p ( 3 ) in a temporal state generated during execution.
The interpretation of an automaton state is therefore analogous to that in the PTL model, by treating predicate symbols as propositions.
To determine the set of reachable states, the program formula is evaluated for each automaton state.
As grounded instantiations of predicates are not used during compilation, we may only determine rules for which the past-time formulae are potentially satisfied during execution.
This is defined as any rule for which each positive literal used in the past-time formula has the same value in the current automaton state.
A state determines a set of successors, representing potential interpretations for the next moment in time.
As interpretations may only be valid under some constraint on the value of terms, a transition label specifies the conditions under which a transition may be used.
Le: The set of environment propositions representing messages received by an object.
Le,: The set of propositions specified as eventualities.
La: The set of propositions represented by L, - ( L e U Le,).
The constraints on each subset are represented by an automaton.
The intemal evaluation of a program is represented by a cyclic automaton generated for the set L,.
The automata for Le and Le, are used to integrate the satisfaction of any members of these sets with the internal automaton.
This approach reduces the maximum number of states used by the compiled representation from  4.3 Internal Evaluation of Program Formulae As a structure capable of representing both PTL and FML programs, we use the predicate automaton describe above.
Three automata, representing subsets of the program formula, are used to limit the size of the compiled representation.
In effect, the internal automaton models the execution sequences of programs, while the environment and eventuality automata provide direction for the execution mechanism.
We define transitions in the intemal automaton to reflect any value derivable from these external structures.
From the definition of the set of predicates La given above, a set of program rules R, is defined.
This consists of every rule for which any predicates of the past-time formula, where p I+ true, belong to L,.
No rule therefore requires the receipt of environment predicates or the satis-  Thus, much of the work described in this paper ensures that sufficient structure is present within the separate automata to ensure that execution is not prohibitively expensive.
4.2 Automata and METATEM Concurrent METATEMactually executes formulae of both propositional and first-order temporal logic.
A set of SNF program rules for the first-order temporal logic FML are of the form2 n  0  A Pi(3) * i= 1  v m  Qj(!d  j=1  2Again, the full syntax and semantics of the first-order temporal logic FML are omitted (they are standard -see, for example [2]).
15  transition represents the result of internal rule evaluation together with the environment predicates received.
As, in general, behaviour involving interaction with the environment requires a significant number of states to represent, we define a separate structure, performing this function.
Our approach uses the set of environment predicates, L,, in order to create a separate environment automaton, not dependent on program rules.
The environment automaton consists of an initial state, linked by transitions to states representing every subset of the environment set:  faction of an eventuality to be used in a temporal state.
Concurrent METATEMprohibits the use of any member of the set of environment predicates in a future-time formula [l].
As we wish to treat eventualities separately, we define a subset, R,,,, specifying rules for which the future-time formula is of the form o q ( Z ) .
The set of automaton states created represents interpretations for the remaining rule set, i.e.
R, - Rae",and therefore contains only the predicates La.
To determine the potential initiation of an eventuality, a state represents any rule belonging to R,,, for which the past-time formula is satisfied by the automaton state valuation.
An eventuality is initiated in a state specifying T C Rae"if the past-time formula of T can be satisfied during execution.
A transition relation models the conditions required for a destination state to be a valid successor state.
This is determined during execution from the input of the environment and eventuality automata, and from the evaluation of the set of rules, R,, associated with a state.
Ve C_ Le.
V ( s e )= e A T ( s g , s e )  At each moment in time, the environment automaton performs a transition from the initial (empty) state, to some final state recording the combination of environment predicates received in this cycle.
The timing of this transition is synchronized with that of the internal automaton.
Instead of a single automaton state representing internal and environment predicates, a full state formula consists of the combination of environment and internal automata states.
From the previous example in Figure 1, the result of internal evaluation, $, is the held by the internal automaton, while the potential environment predicates received ( { p } ,{}), are represented as final states in the environment automaton (see Figure 2).
The final state of the environment automaton acts as a link to the internal automaton and is useful when choosing a subsequent transition.
4.4 Environment Interaction At any moment in time, messages may be received by an object and must be incorporated in future evaluation.
For any environment predicate, p , corresponding to an external message received at time i, the temporal state formula for i 1 must consist of $ A p , where q5 is the result of the internal evaluation of i.
An I/O automaton [17], capable of interaction with the environment from any state, is used to provide communication between Concurrent METATEM objects.
This approach divides the actions performed by an automaton into three sets: input, internal and output.
Each state may receive input from some external source, and produce output to be distributed by the communication mechanism.
+  Figure 2 -Environment and Internal Automata  Figure 1 -Integrated U 0 Automaton  4.5 Eventualities Figure 1 shows the structure for a state directly combining internal and environment predicates.
Transitions link successor states representing the combination of local and external actions.
Labels associated with transitions specify the environment predicates required for a transition to be selected during execution.
The destination state of any  The use of Separated Normal Form (SNF) removes all future-time temporal operators except 0 from program rules.
Whenever an eventuality is initiated, there must exist a future state at which this can be satisfied.
Satisfaction is possible in any temporal state in which the negation of the specified predicate is not forced.
The most beneficial exe-  16  cution sequences provide the earliest possible satisfaction of eventualities.
While using an w-automaton to model METATEMprograms can potentially describe the full program formula, it introduces a number of problems when used as a compiled representation.
As the eventualities outstanding cannot be fully determined during compilation, every state must effectively provide successor states in which the satisfaction or non-satisfaction of any eventuality is represented.
As with environment predicates, this means that successor states must exist for every state, to represent the satisfaction of any subset of the set of eventualities.
For our approach, only the initiation of eventualities is specified by the intemal automaton.
We define a separate automaton to represent the effect of the satisfying any ew E Le,.
This is in a similar form to the environment automaton, consisting of an initial state, linked by transitions to states representing every subset of Le,: Vew  Lev.
250 Temporal  I  States per second  100  50  Full Automaton  lntemall  I"lerprele4  Environment Automala  Figure 3 -Execution time for a Dining Philosopher in the period of execution.
The reduction in the execution time of our model compared with the full automaton, indicates the effect of combining the separate automata.
Because of the significantly smaller size of our approach, shown in Figure 4,we consider to be an adequate tradeoff.
V ( s e W=) ew A T(so,S e w )  At each moment in time, a transition from the initial to some final state represents the set of eventualities satisfied in the current temporal state.
This consists of some subset of the outstanding eventualities initiated by previous intemal automaton states.
Eventualities are satisfied in any temporal state which does not contain formulae prohibiting their satisfaction.
5  2w  Comparison of Results  This approach to the compilation of Concurrent METATEMhas been applied to a number of existing programs.
In this section we examine the effect on execution time of the compiled model, and the effectiveness of the space reduction techniques applied.
We demonstrate both the evaluation of an object from the well-known concurrency problem, the dining philosophers [7],and illustrate the more general case with some baseline examples.
In the tests performed, we assume an equivalent subset of the set of environment propositions, received at each moment in time with each approach.
We compare the interpreted language, performing direct execution of formulae, with our automata model.
A single automaton implementation of the dining philosopher problem is used to evaluate of the effect of our divided model.
The dining philosophers is a concurrent problem implemented using propositional Concurrent METATEM.While the SNF representation of this is too complex to be included here, the program contains 16 propositions, with 3 eventualities and an environment set of size 4.
A comparison of the execution time for this application using both automata models and the existing interpreter is given in Figure 3.
The states per second ratio is unaffected by changes  Full  Divided  Automaton  Automala  Figure 4 - No.
States for a Dining Philosopher For both automata-based approaches, the reduction in execution time over the interpreted language is large.
In general, the increase in performance becomes greater as the complexity of the program increases.
To illustrate this, the execution of a set of baseline examples, producing a fixed number of successor state interpretations at each moment in time, is shown in Figure 4.
As the number of interpretations increases, there is a rapid increase in the execution time for the interpreted language.
This reflects the fact that all possible interpretations must be generated for each state.
As a compiled model determines these choices in advance, this cost is removed from execution.
The formulae producing the most interpretations for a successor state are those containing many eventualities or disjuncts of literals in future-time formula.
The benefits of the compiled language over direct execution is reflected most significantly in programs of this type.
17  [6] M. Finger, M. Fisher, and R. Owens.
METATEM at Work: Modelling Reactive Systems Using Executable Temporal Logic.
In Sixth International Conference on Industrial and Engineering Applications of Art8cial Intelligence and Expert Systems (IEALAIE), Edinburgh, U.K., June 1993.
Gordon and Breach Publishers.
301  Seconds  20  Zoo0 states  /  1  101  [7] M. Fisher.
A Survey of Concurrent METATEMThe Language and its Applications.
In First International Conference on Temporal Logic (ICTL),Bonn, Germany, July 1994.
Automata  V  0  2  4  8  16  No.
Interpretations per state  [SI M. Fisher.
Representing and Executing Agent-Based Systems.
In M. Wooldridge and N. R. Jennings, editors, Intelligent Agents - Proceedings of the 1994 Workshop on Agent Theories, Architectures, and Languages.
Springer-Verlag, 1995.
Figure 5 -Increasing Interpretations per State  6 Conclusions and Future Work We have presented an automata-based approach to the compilation of Concurrent METATEM.
Rather than utilising a standard u-automaton representing the full program, we utilise a combination of internal, environment and eventuality automata.
While this split approach produces marginally slower (but acceptably so) execution, it dramatically improves the space requirements for the compiled representation.
Future work comprises further enhancement of the automata representations, and their incorporation in a full compiler for Concurrent METATEM.
[9] M. Fisher.
An Introduction to Executable Temporal Logics.
Knowledge Engineering Review, 11(1):4356, March 1996.
[lo] M. Fisher and R. Owens.
From the Past to the Future: Executing Temporal Logic Programs.
In Proceedings of Logic Programming and Automated Reasoning (LPAR),St. Petersberg, Russia, 1992.
[ l l ] M. Fisher and M. Wooldridge.
Executable Temporal Logic for Distributed A.I.
In Twelfth International Workshop on Distributed A I .
, Hidden Valley Resort, Pennsylvania, May 1993.
References [ l ] H. Barringer, M. Fisher, D. Gabbay, G. Gough, and R. Owens.
METATEM:An Introduction.
Formal Aspects of Computing, 7(5):533-549, 1995.
[12] M. Fisher and M. Wooldridge.
A Logical Approach to the Representation of Societies of Agents.
In N. Gilbert and R. Conte, editors, ArtiJicial Societies.
UCL Press, 1995.
[2] H. Barringer, M. Fisher, D. Gabbay, R. Owens, and M. Reynolds, editors.
The Imperative Future: Principles of Executable Temporal Logics.
Research Studies Press, Chichester, United Kingdom, 1996.
[ 131 M. Fisher.
A Normal Form for First-Order Tempo-  ral Formulae.
In Proceedings of Eleventh International Conference on Automated Deduction (CADE), Saratoga Springs, New York, June 1992.
[3] K. Birman.
The Process Group Approach to Reliable Distributed Computing.
Techanical Report TR911216, Department of Computer Science, Cornel1 University, July 1991.
[14] M. Fisher.
Concurrent METATEM - A Language for Modeling Reactive Systems.
In Parallel Architectures and Languages, Europe (PARLE), Munich, Germany, June 1993.
[4] R. E. Bryant.
Symbolic Boolean Manipulation with Ordered Binary Decision Diagrams.
Technical Report CMU-CS-92-160, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, July 1992.
[15] M. Fisher.
An Open Approach to Concurrent Theorem-Proving.
In Parallel Processing for Artificial Intelligence.
North-Holland, 1996.
(In press.).
[161 L. H. Landweber.
Decision problems for w-automata.
[5] E. A. Emerson.
Temporal and Modal Logic.
In J. van Leeuwen, editor, Handbook of Theoretical Computer Science, pages 996-1072.
Elsevier, 1990.
Mathematical Systems Theory, 3:37&384, December 1969.
18  [17] N. A. Lynch and M. R. Tuttle.
An Introduction to Input/Output Automata.
CWI Quarterly, 2(3):219246, September 1989.
Centre for Mathematics and Computer Science, Amsterdam.
[I81 A. P. Sistla, M. Vardi, and P. Wolper.
The complementation problem for biichi automata with applications to temporal logic.
Theoretical Computer Science, 49:217-237,1987.
[19] P. Wolper, M. Vardi, and A. P. Sistla.
Reasoning about infinite computation paths.
In Proceedings of the Twentyfourth Symposium on the Foundations of Computer Science.
IEEE, 1983.
19
Discovering Calendar-based Temporal Association Rules Yingjiu Li, Peng Ning, X. Sean Wang, Sushil Jajodia Center for Secure Information Systems, George Mason University, Fairfax, VA 22030 yli, pning, xywang, jajodia  @ise.gmu.edu  Abstract A temporal association rule is an association rule that holds during specific time intervals.
An example can be that eggs and coffee are frequently sold together in morning hours.
This paper studies temporal association rules during time intervals specified by user-given calendar schemas.
Generally, the use of calendar schemas makes the discovered temporal association rules easier to understand.
An example of calendar schema is (year, month, day), which yields a set of calendar-based patterns of the form example,    fi	   , where each    is either an integer or the symbol  .
For  is such a pattern, which corresponds to the time intervals consisting of all the 16th days of  all months in year 2000.
This paper defines two types of temporal association rules: precise-match association rules that require the association rule hold during every interval, and fuzzy-match ones that require the association rule hold during most of these intervals.
Compared to the non-temporal association rule discovery, temporal association rules are more difficult to find due to the usually large number of possible temporal patterns for a given calendar schema.
The paper extends the well-known Apriori algorithm, and also develops two optimization techniques to take advantage of the special properties of the calendar-based patterns.
The paper then studies the performance of the algorithms by using a real-world data set as well as synthetic data sets.
The performance data show that the algorithms and related optimization techniques are effective.
1 Introduction Among various types of data mining applications, the analysis of transactional data has been considered important.
It is assumed that the database keeps information about user transactions, where each transaction is a collection of data items.
The notion of association rule was proposed to capture the cooccurrence of items in transactions [AIS93].
For example, given a database of orders (transactions) placed in a restaurant, we may have an association rule of the form egg  which means that    coffee (support: 3%, confidence: 80%),  of all transactions contain the items egg and coffee, and  !#"$  of the transactions that have the  item egg also have the item coffee in them.
The two percentage parameters above are commonly referred to as support 1  and confidence respectively.
One interesting extension to association rules is to include a temporal dimension.
For example, eggs and coffee may be ordered together primarily between 7AM and 11AM.
Therefore, we may find that the above association rule has a support as high as 40% among the transactions that happened between 7AM and 11AM and has a support as low as 0.005% in other transactions.
As another example, if we look at a database of transactions in a supermarket, we may find that turkey and pumpkin pie are seldom sold together.
However, if we only look at the transactions in the week before Thanksgiving, we may discover that most transactions contain turkey and pumpkin pie.
That is, the association rule aturkey   pumpkin piea has a high support and a high confidence in the transactions that happened in the week  before Thanksgiving.
The above suggests that we may discover different association rules if different time intervals are considered.
Some association rules may hold during some time intervals but not during others.
Discovering temporal intervals as well as the association rules that hold during the time intervals may lead to useful information.
Informally, we refer to the association rules along with their temporal intervals as temporal association rules.
The discovery of temporal association rules has been discussed in the literature.
For example, in [ OERS98], discovery of cyclic association rules (i.e., the association rules that occur periodically over time) was studied.
However, periodicity has limited expressiveness in describing real-life concepts such as the first business day of every month since the distances between two consecutive such business days are not constant.
In general, the model does not deal with calendric concepts like year, month, day, etc.
In [RMS98], the work in [ OERS98] was extended to treat user-defined temporal patterns.
Although the work in [RMS98] is more flexible than that of [ OERS98], it only considers the association rules that hold during the time intervals described by a user-given calendar algebraic expression.
In other words, a single set of time intervals is given by the user and only the association rules on these intervals are considered.
This method hence requires useras prior knowledge about the temporal patterns.
(Other related work will be discussed later in the paper.)
In this paper, we propose an approach to discovering temporal association rules with relaxed requirement of prior knowledge.
Instead of using cyclic or user-defined calendar algebraic expressions, we use calendar schemas as frameworks for discovering temporal patterns.
Our approach is not a simple extension of the previous approaches, because we assume less prior knowledge and, therefore, we have more potential temporal patterns to explore.
It is necessary to explore optimization opportunities afforded by the relationships among the temporal patterns in order to achieve the performance and scalability for practical uses.
A calendar schema is determined by a hierarchy of calendar concepts.
For example, a calendar schema can be (year, month, day).
A calendar schema defines a set of simple calendar-based patterns (or calendar patterns for short).
For example, given the above calendar schema, we will have calendar patterns such as every day of January of 1999 and every 16th day of January of every year.
Basically, a calendar pattern is formed for a calendar schema by fixing some  2  of the calendar units to specific numbers while leaving other units afreea (so itas read as aeverya).
It is clear that each calendar pattern defines a set of time intervals.
Based on the work for generating user-defined calendars, e.g., [LMF86], cyclic patterns of [OERS98] and calendar algebra expressions of [RMS98] can be considered as special cases of calendar patterns.
We assume that the transactions are timestamped so we can decide if a transaction happens during a specific time interval.
Given a set of transactions and a calendar schema, our first interest is to discover all the association rule and calendar pattern pairs such that for each pair  , the association rule  fifi    satisfies the minimum support and  confidence constraint among all the transactions that happen during each time interval given by the calendar pattern   .
For example, we may have an association rule turkey  pumpkin pie along with the calendar pattern every day in   every November.
We call the resulting rules temporal association rules w.r.t.
precise match.
In some applications, the above temporal association rules may be too restrictive.
Instead, we may require that the association rule hold during aenougha number of intervals given by the corresponding calendar pattern.
For example, the association rule turkey   pumpkin pie may not hold on every day of every November, but holds on more than !"  of November days.
We call such rules temporal association rules w.r.t.
fuzzy match.
Our data mining problem is to discover from a set of timestamped transactions all temporal association rules w.r.t.
precise or fuzzy match for a given calendar schema.
We extend an existing algorithm, Apriori, to discover all such temporal association rules.
In addition, we observe that the calendar patterns formed from a calendar schema are not isolated but related to each other.
We use the relationship to develop two optimization techniques called temporal aprioriGen and horizontal pruning.
These optimization techniques can be applied to both classes of temporal association rules with some adaptation.
Our contribution in this paper is two-fold.
First, we develop a new representation mechanism for temporal association rules on the basis of calendars and identify two classes of interesting temporal association rules: temporal association rules w.r.t.
precise match and temporal association rules w.r.t.
fuzzy match.
Our representation requires less prior knowledge than the prior methods and the resulting time intervals are easier to understand.
Second, we extend the algorithm Apriori and develop two optimization techniques to discover both classes of temporal association rules.
Our experiments demonstrate that our optimization techniques are effective.
The rest of the paper is organized as follows.
In section 2, we define temporal association rules in terms of calendar patterns in the framework of calendar schemas.
In section 3, we extend Apriori to discover large itemsets for temporal association rules and present our optimization techniques to improve the performance and scalability.
In section 4, we present the experimental evaluation of our algorithms using both real and synthetic data sets.
Section 5 presents the related work, and section 6 concludes the paper with some discussions.
Appendix A gives the proof of the lemmas and theorems that appear in the paper.
3  2 Problem Formulation 2.1 Association Rule The concept of association rules, which was motivated by market basket analysis and originally presented in [AIS93], has been discussed in many application domains.
Let   fi      subset of .
An itemset is also defined to be a subset of .
Given a set     denote a set of data items.
A transaction is defined to be a  is a relationship between the two disjoint itemsets  and  of transactions, an association rule of the form .
An association rule satisfies some user-given   	            requirements.
The support of an itemset by the set of transactions is the fraction of transactions that contain the itemset.
     An itemset is said to be large if its support exceeds a user-given threshold over  is the fraction of transactions containing  that also contain  is large and its confidence exceeds a user-given threshold     fi  .
The confidence of  .
The association rule       holds in  if  .
(There are constraints other than    user-specified minimum support and minimum confidence that define interesting rules, e.g., minimum improvement constraint [BAG99].
However, they are out of the scope of this paper.)
2.2 Simple Calendar-based Pattern In the following, we present a class of calendar related temporal patterns called simple calendar-based patterns.
'2'3 ' $54 ' $,+-648771794 ' -  %3  !#" 	%$&'($ )*$,+.-/&'($,+- 01010 )-/&  A calendar schema is a relational schema (in the sense of relational databases)   together with a valid constraint (explained below).
Each attribute  week etc.
Eeach domain        is a calendar unit name like year, month, and  ' $:4;71717<4 ' >= ?
(&A@%BC%C%D 1BC%CE 7177 BCC%CGF <H.<I& KL= < M,N  is a finite subset of the positive integers.
The constraint valid is a Boolean function on  specifying which combinations of the values in  are avalida.
This constraint serves  two purposes.
The first is to exclude the combinations that do not correspond to any time intervals due to the inter-  @%B )J 7177 1BJ,F )%?=&6@B )J 7717 6B*F  action of the calendar units.
For example, we may have a calendar schema                  with the constraint valid that evaluates  fi              to True only if the combination    gives a valid date1 .
The second purpose of the valid constraint is to exclude the time intervals that we are not interested in.
For example, if we do not want to consider the weekend days and holidays, we can let valid evaluate to False for all such days.
'3 !O" P*$&'($ )*$,+.-/&'($,+- 01010 )-/&%'Q!
!
PK R$ M$,+.- 10010 ),-N S G3 'Q3 S K> $ M $,+.- 10010 ) - N VPW<XYXYZ[MWY[	\] VPW<XYXY^[_[P\WM]  For brevity, we may omit the domains arises.
and/or the constraint valid from the calendar schema when no confusion  Given a calendar schema    pattern for short) on the calendar schema    is a tuple on  the wild-card symbol .
Here, we choose to use The calendar pattern 1 For  example,        is a valid date while      3 '(3  , a simple calendar-based pattern (or calendar  of the form  for both an element of        where each  is in  4  ,T>- U  and the symbol to simplify our notation.
represents the set of time intervals that are intuitively described by athe is not.
or  -  of the  %T>U  0 010 %T>$ U  $ T>3 U K	S 1B B N ,  , of  .a In the above description, if  ,3  is the wild-card symbol a*a (instead of an integer), then  K<B YS 1B %N  the phrase athe  a is replaced by the phrase aeverya.
For example, given the calendar schema week, day, hour  , the  calendar pattern     "  means athe 10th hour on the first day (i.e., Monday) of every weeksa.
Similarly,  'Q$ 4 7177 4 '2-     "  represents the time intervals athe 10th hour of every day of week 1a.
Each calendar pattern in effect represents the time intervals given by a set of tuples in  that are valid.
For simplicity, we omit a more formal treatment  of the above concepts.
  )%?= )IA K<B YS 1B %N  We say a calendar pattern  covers another calendar pattern  in the same calendar schema if the set of time intervals   K9B 1B 1B N P$ ) $G+.- 7177 )B    of  is a subset of the set of intervals of  .
For example, given the calendar schema the 10th hour of every day of week 1) covers  K> $ ) $,+- 17 717 M - N  for a given calendar schema                     if and only if for each ,     "  , either       KP$ )%$G+.- 17 77 MG-N , 3 3 "  3  fi        ,     "  (i.e.,  (i.e., the 10th hour of day 1 of week 1).
It is easy to see that  , a calendar pattern          = a*a or  covers another calendar pattern      .
Simple calendar-based patterns give a simple and intuitive representation of sets of time intervals in terms of a calendar schema.
Note that time intervals or periodic cycles can be easily described by calendar patterns with appropriate  KS <9N  B   ! "
)%?= 3  calendar schemas having perhaps user-defined calendars.
For example, the periodic cycle aevery seventh daya can be expressed by a calendar pattern    , where    	   which day the cycle starts.
3 - B   L= ?
<H .<I  , under the calendar schema   $ ) $,+.- 10100  >.<I M?=  For simplicity, we require that in a calendar schema ( tained in a unit of  H.<I    , for    month.
However, the schema  .
For example,      fi       fi                  fi   ), each calendar unit of    depending on  is uniquely con-  is allowed since each day is covered by a unique  is not allowed because a      fi  may not be contained in a unique  .
It is often convenient and sometimes necessary for users to define calendar units and then use them in calendar  schemas.
For example, the 24 hours of a day may be partitioned into five parts, representing early morning, morning, work hour, night, and late night respectively, forming a new calendar unit.
The reader is referred to [LMF86] and [BJW00] for generation of user-defined calendars.
For brevity of presentation, we introduce some notations for calendar patterns.
We call a calendar pattern with exactly  fi  wild-card symbols a k-star calendar pattern (denoted  ) and a calendar pattern with at least one wild-card  symbol a star calendar pattern.
In addition, we call a calendar pattern with no wild-card symbol (i.e., a 0-star calendar  '$ 4 7717R4 '2-  pattern) a basic time interval under the calendar schema if the combination is valid with respect to the constraint given in the calendar schema.
In other words, a basic time interval corresponds to a tuple in  that is valid.
2.3 Temporal Association Rules We assume that each transaction is associated with a timestamp that gives the time of the transaction.
For example, a    transaction may be associated with a timestamp that represents November 1, 2000, which indicates that the transaction occurred on November 1, 2000.
Given a basic time interval (or a calendar pattern  ) under a given calendar schema,  5    we denote the set of transactions whose timestamps are covered by (or  ) as  !
!
Syntactically, a temporal association rule over a calendar schema rule and   is a calendar pattern on     fi (or  is a pair  fi fi    ).
, where  is an association   .
However, multiple meaningful semantics can be associated with temporal  association rules.
For example, given a set of transactions, one may be interested in the association rules that hold in the transactions on each Monday, or the rules that hold on more than 80% of all Mondays, or the rules that hold in all transactions on all Mondays (i.e., consider the transactions on all Mondays together).
In the following, we identify two classes of temporal association rules on which we will focus in this paper.
Other kinds of temporal association rules may be interesting, but we consider them as possible future work.
  Temporal Association Rules w.r.t.
Precise Match Given a calendar schema       of timestamped transactions, a temporal association rule  L= ?
<H.<I /I6 * %?=  association rule  holds in   fi        fifi  ! "
P$ )*$,+.- 7177  KS BB N   holds w.r.t.
precise match in  , we may have a temporal association rule (turkey  (i.e., the 4th Thursday in November of every year).
timestamped transactions, and a real number  rule  holds in     and a set    ("   ) that holds    pumpkin pie holds on all Thanksgiving days           , a set  , called match ratio), a temporal association rule    if and only if for at least "#"  may have a temporal association rule (turkey     ! "
P$ ) $,+- 71717 )-   B B *  >= ?
<H.<I M?%=  " R0 KS BB YS*N  of fi fi  of the basic time intervals covered by  , the association  .
For example, given the calendar schema  the association rule turkey     if and only if the  pumpkin pie,   Temporal Association Rules w.r.t.
Fuzzy Match Given a calendar schema      for each basic time interval covered by  .
For example, given the calendar schema  w.r.t.
precise match.
This rule means that the association rule turkey  holds w.r.t.
fuzzy match in  fi  pumpkin pie,      fi      and the match ratio  "  !
, we  ) that holds w.r.t.
fuzzy match.
This means that    pumpkin pie holds on at least 80% of the days in November.
Given a calendar schema, we want to discover all interesting association rules with all their calendar patterns in the given calendar schema.
Specifically, we attack the following two data mining problems:  !
1.
(Precise match) Given a calendar schema tion rules  fi fi  that hold w.r.t.
precise match in  2.
(Fuzzy match) Given a calendar schema temporal association rules  fi   !
  and a set    of timestamped transactions, find all temporal associa-  .
, a set      of timestamped transactions, and a match ratio  that hold w.r.t.
fuzzy match in    , find all  .
We further assume that we are not interested in the association rules that only hold during basic time intervals.
Indeed, such rules do not reveal much information in terms of time.
Therefore, we exclude the 0-star calendar patterns   	   from the output of our data mining problems.
       Now we introduce some additional notations for the sake of presentation.
For a basic time interval in a calendar schema, we say an itemset is large for if it is large in  w.r.t.
precise match if it is large for each basic time interval covered by  .
Further consider a fuzzy match ratio 6    .
For a calendar pattern  , we say an itemset is large for  , we    (1)  large 1-itemsets  ;    (2) for (k=2; fi (3) (4) (5) (6)  	    do begin   = aprioriGen  fi   ; // New candidates forall transactions  do subset(    ); //   !#"%$  if &  ' ' (  )!#"%$+*-,/."102!4353#6$ ;   is contained in   (7) end (8) Answer =  7    ;      B *  Figure 1: Algorithm 8    say an itemset is large for  w.r.t.
fuzzy match if it is large for at least    ""  .
of the basic time intervals covered by  .
3 Finding Large Itemsets Mining temporal association rules can be decomposed into two subproblems: (1) finding all large itemsets for all star calendar patterns on the given calendar schema, and (2) generating temporal association rules using the large itemsets and their calendar patterns.
Finding large itemsets along with their calendar patterns is the crux of the discovery of temporal association rules.
In the following, we will focus on this problem.
The generation of temporal association rules from large itemsets and their calendar patterns is straightforward and can be resolved using the method discussed in [AS94].
Our approaches to finding large itemsets for all calendar patterns are based on Apriori [AS94].
Before going into details of our approaches, we briefly go over Apriori.
3.1 Apriori Figure 1 shows the outline of Apriori.
The algorithm fi  8        algorithm tries to find large -itemsets (i.e., itemsets with generates 9     fi  fi  consists of a number of passes.
During pass , the  items that have at least the minimum support).
It first  fi  , the set of candidate large -itemsets, then counts the support of each candidate itemset by scanning all  the transactions in the data set, and finally finds  :   fi  , the set of large -itemsets, by inspecting the supports of all the  candidate itemsets.
The algorithm terminates when no large itemset is discovered after a pass.
Function  ?   <;      fi    +.-  B  fi  9  is a critical step of Apriori (step 3).
It constructs the set of candidate large -itemsets,  from the set of large ( -1)-itemsets, :   , ensuring that all  >= fi    -item subsets of each candidate in 9   are in :   +.  , .
It turns out that aprioriGen is very effective in reducing the size of the candidate set [AS94].
Scanning the transactions and updating the supports of candidate itemsets (step 4 and 5) are the most time7  (1) forall basic time intervals  do begin (2)        fi  large 1-itemsets in      forall star patterns that cover  do  (3) (4)  update      using     ;  (5) end (6) for (  (7)     ; 	 a star calendar pattern such that  fi   &	   ;'  -  ) do begin  forall basic time intervals  do begin // Phase I: generate candidates      ;  (8)  generate candidates  (9)  // Phase II: scan the transactions fi forall transactions      do subset (   (10)      ,  ); //  )!#"%$   if '+    &   2 (  )!#"%$+*, . "
0 5!
343#6$	 ;       (11)  is contained in   // Phase III: update for star calendar patterns forall star patterns that cover  do  (12) (13)  update  (14)  end  (15)  Output               using     ;  for all star calendar pattern .
(16) end  Figure 2: Outline of our algorithms for finding large k-itemsets        consuming steps, since they require access to both disk and a potentially large set of candidate itemsets.
Apriori uses a hash tree to store all candidate itemsets and their supports [AS94].
In Figure 1, function  hash tree according to the transaction      traverses the    and increments the supports of the candidate itemsets contained in .
3.2 Overview of Our Algorithms  !
  We extend Apriori to discover large itemsets w.r.t.
precise and fuzzy match.
When precise match is considered, the  	    input of our algorithms consists of a calendar schema , a set     of timestamped transactions, and a minimum support  .
When fuzzy match is considered, an additional input, a match ratio  !
, is given.
Depending on the data  mining tasks, our algorithms output the large itemsets for all possible star calendar patterns on  in terms of precise  match or fuzzy match.
Figure 2 shows the outline of our algorithms.
(This outline is generic for both precise and fuzzy match as well as with and without our optimization techniques discussed later.
For different algorithms, appropriate subroutines will be supplied.)
As Apriori, the algorithms work in passes.
In each pass, the basic time intervals in the calendar schema are  8  processed one by one.
During the processing of basic time interval  first computed , and then : 2     	   fi  	  fi  fi  in pass , the set of large -itemsets :   	   	   is   is used to update the large -itemsets for all the calendar patterns that cover  .
Note   that although our data mining tasks do not need association rules for basic time intervals, the large itemsets for basic time intervals  :   	     K N K N  K  N  are used in the algorithms for efficiency considerations.
Indeed, assume we have the calendar   K  N  schema year, month, day  .
In the algorithms, we may need to consider, e.g., the calendar patterns 1995, *, 1 as well  K  N  K  N  as *,1,* .
These two patterns have an overlapping basic time interval, namely 1995, 1, 1 .
In our algorithms, we use the large itemsets for 1995, 1, 1 (and for other basic time intervals) to derive the large itemsets for 1995, *, 1 and *,1,* to avoid duplicate tasks.
This strategy is reflected in lines (4) and (13).
 %%A   The first pass is specially handled.
In the first pass, we compute the large 1-itemsets for each basic time interval by counting the supports of individual items and comparing their supports with    .
In the subsequent passes,  we divide the processing of each basic time interval into three phases.
Phase I generates candidate large itemsets for the basic time interval from the previously generated large itemsets.
Phase II reads the transactions whose timestamps are covered by the basic time interval, updates the supports of the candidate large itemsets, and discovers large itemsets for this basic time interval.
Phase III uses the discovered large itemsets to update the large itemsets for each star calendar fi  pattern that covers the basic time interval.
At the end of each pass, it outputs the set of large -itemsets, :	  fi  , for all  star patterns  w.r.t.
precise or fuzzy match.
Similar to the discovery of non-temporal association rules, phase I is the critical step in mining temporal association rules.
Indeed, the fewer candidate large itemsets are generated, the less time phase II will take, and the better performance can be achieved.
Several observations can be used to reduce the number of candidate large itemsets.
We will discuss phase I in detail in the following subsections.
Phase II is performed in the same way as in Apriori by using the candidate large itemsets generated in phase I.
Now let us explain phase III.
After the basic time interval  calendar patterns that covers   :"  fi  	    fi  :   :      fi    	    fi  are updated as follows.
For precise match, this is done by intersecting the set :  of large -itemset for the basic time interval   :  fi  is processed in pass , the large -itemsets for all the  ).
(Certainly, when :  	    with the set    :   fi  fi  5"   	       of large -itemsets for the calendar pattern  (i.e.,  is updated for the first time, we let :   fi  fi  :   	       .)
It is easy to  see that after all the basic time intervals are processed, the set of large -itemsets for each calendar pattern consists of fi  the -itemsets that are large for all basic time intervals covered by the pattern.
Update for fuzzy match is a little more complex.
We associate a counter c update with each candidate large itemset for each star calendar pattern.
The counters are initially set to 1.
When the counters of the itemsets in : but not in :   fi  are added to :        fi  by  and this is the -th update to : 2 When  that are also in :  fi    	   fi  :    	     is used to update :     fi  in phase III,  are incremented by 1, and the itemsets that are in :  with the counter set to 1.
Suppose there are totally      9   	    A?    basic time intervals covered  .
It is easy to see that an itemset cannot be large for  if its counter  some of our optimization techniques are used, a subset of the large  -itemsets for fi  may be used as      fi 	   as explained later.
  MK	S JN :   %%? /"  A? /"  A?% 8"  A?% 8"    AB,  J    AC,  B    AD,  J    BC,    <K )J%N :  (Before update)       <KS JN  A? /"  %%? /" J  A? /"#B  A? 8"  A? /" B  :     AB  AB,  AC  AC,  B  (After update)          AD, BC  BC,  BD  BD,      (X)      (X)  Figure 3: Update candidate large 2-itemsets for fuzzy match (Example 1)  does not satisfy   %%?      7  =      .
Thus, in the algorithm outlined in Figure 2, steps 4 and 13 for fuzzy    match can be instantiated by the following procedure.
Procedure Update4FuzzyMatch ( :    Let if :  be the number of times that :    :" :  	 fi90  A?% 8"#B "  @ fi :      ,:  fi   	    )   has been updated (including this update);  fi  <0  A? 8"#B  has never been updated then     Let : else set  :             , and set fi  for each fi    fi    :  fi    for each fi    :  	  fi<0   	  =  :  A?        fi      :   <0  A? 	    7  F  , and fi  =  ;  fi    for each fi  :  	     :   fi  ;  ;    endif  " R0 fi" D  KS JN  Example 1 Suppose we are given a calendar schema ratio  "  !
.
Consider the calendar pattern  K	S )J%N    ! "
&@%B 7717 )DGF  week        day  &@B 17 717 F K	S )J%N K J N : <KS JN  <KS )J%N  "  left column, and the set of large 2-itemsets for third time that :  <KS )J%N      K )J%N  is updated (i.e.,           :       (i.e.,      ) is given in the  ) is in the middle column.
Suppose this is the  ).
Then the resulting  The itemsets marked with aXa do not satisfy the condition  :  <K )J%N : <KS JN  A?%   =    7   (i.e.,    and want to update the     .
In Figure 3, the set of candidate large 2-itemsets (i.e.,    and a fuzzy match     fi  .
There are totally 5 basic time intervals covered by  ).
Suppose we have computed the large 2-itemsets for the basic time interval  candidate large 2-itemsets for          can be computed as in the last column.
and thus are dropped from      .
fi  If the set of large -itemsets : fi    	   	  is correctly computed for each basic time interval  , then Update4FuzzyMatch  can correctly generate large -itemsets w.r.t.
fuzzy match for all star calendar patterns.
This is guaranteed by the following lemma.
Lemma 1 Consider the algorithm outlined in Figure 2.
If procedure Update4FuzzyMatch is used at steps 4 and 13 and  :    	   fi  fi  is the set of large -itemsets for each basic time interval  processed, for each calendar pattern  ,  :   fi   fi  	 , then after all the basic time intervals are  contains all and only the -itemsets that are large for at least 10  B * "#"  of the basic time intervals covered by  .
3.2.1 Calendar Tree In the algorithm, it is necessary to locate the large itemsets for a given calendar pattern quickly.
We use a data structure called a calendar tree to organize the large itemsets for all the calendar patterns.
! "
 $ &G' $ ) $,+.- &R' ,$ +.- 10100 ) - &R'  '3 =  B  %$ 2" K>G$ )%$G+.- 0100 MG-N  Given a calendar schema        , the calendar tree for    !
Itemsets are stored in the leaf nodes.
An interior node at height contains a look-up table of size one cell for each domain value in pointer to a node at height  ,$    , which has  plus a cell for the wild-card symbol a*a.
Each cell of the look-up table contains a  .
The root is at height  (corresponding to  that stores the set of large itemsets for a calendar pattern pointer corresponding to  ' 3  B    is a tree of height .
    ).
When we want to locate the leaf node      G$G+.-  , we start from the root, follow the  , and from this node follow the pointer corresponding to  , and so on, until we reach  the leaf node corresponding to  .
3.3 Generating Candidate Large Itemsets for Precise Match 3.3.1 Direct-Apriori for Precise Match A naive approach to generating candidate large itemsets is to treat each basic time interval individually and directly apply Apriorias method for candidate generation.
We call this approach Direct-Apriori for precise match, or just Direct-Apriori when it is clear from the context.
Phase I of Direct-Apriori is instantiated as follows.
9    	   " ?   <;  : .+            	      fi  Direct-Apriori for precise match can correctly generate the large -itemsets w.r.t.
precise match.
As we discussed    B  earlier (in subsection 3.2), pass 1 of the algorithm can correctly generate the large 1-itemsets for all calendar patterns.
Consider a basic time interval itemsets, 9     	  fi      	  in pass  fi  for fi  fi  fi  .
According to Apriori [AS94], the set of candidate large -  	  , is a super set of all the large -itemsets for  .
Thus, phase II of the algorithm will correctly generate  	  the set of large -itemsets for  .
By the argument in subsection 3.2, for each calendar star pattern  , : fi       will consist  of the -itemsets that are large for each basic time interval covered by  after all the basic time intervals are processed.
3.3.2 Temporal-Apriori for Precise Match Direct-Apriori cannot achieve the best performance; it not only ignores the assumption that we are not interested in temporal association rules for individual basic time intervals, but also the relationship among calendar patterns.
Here we present two optimization techniques, which we call temporal aprioriGen and horizontal pruning respectively, to  11  improve the candidate generation by considering these issues.
The resulting algorithm is called Temporal-Apriori for precise match, or Temporal-Apriori when it is clear from the context.
The first optimization technique temporal aprioriGen is partially based on the assumption mentioned above.
Since  	 , we do not need to count the supports for all the potentially large -itemsets generated by 9 	 " ?   ;  : +- 	 .
Indeed, we only need the supports of the itemsets that are potentially large for some star calendar patterns that covers 	 .
In other words, given a basic time interval 	 , if a candidate large -itemset cannot be large for any of the star calendar patterns that cover 	 , we can ignore it even if it could be large for 	 .
we are not interested in the large itemsets for basic time intervals, during the processing of each basic time in-  terval   fi                       fi        Temporal aprioriGen is also based on an observation about the relationships between the calendar patterns on the same calendar schema.
This observation is given in the following lemma.
Lemma 2 Given a star calendar pattern  , an itemset is large for  w.r.t.
precise match only if it is large w.r.t.
precise match for all 1-star calendar patterns covered by  .
Lemma 2 gives us an opportunity to improve the generation of candidate large itemsets.
Consider the set of candi-  	  fi  date large -itemset for  , i.e.,  9   	      .
We only need  9   	    to generate large itemsets for patterns  that cover    fi  (since our data mining problem excludes 0-star patterns in the output).
Now we need to have a -itemset in  	  9     	  	    only if it is large for all the 1-star patterns that cover  .
Indeed, an itemset is large for a given star calendar pattern   B  only if it is large for all 1-star calendar patterns covered by  by the above lemma.
Thus, using temporal aprioriGen, fi  we can generate the candidate large -itemsets (  fi  ) via the following procedure.
Procedure TemporalAprioriGen4PreciseMatch(  )  9    	     *-   = ;   forall 1-star patterns    	  9   =9  return 9   fi   	       	   that covers     	  +.- *-  do  :  aprioriGen      '  ! "
& @B 71717 D,F &@B 71717 F : <K )J%N " 5@ 8 %8 9 8/' %8fi /9  ' 9;' 9fi(F : <KS JN " @58 %8 9 ' 19fi(F : <K SN " 5@ 8 18 9 8 '   ' 9;'HF 9 <K	S )J%N " @58 9 8 'HF 9 <K YS*N " @58 ' )8 9;'HF  9 MK )JN " 9 MK	S JN 9 MK S*N " @58/9 8 ' 18 9;'HF : MK JN 9  <K )J%N # " @58 9 8 ' 8 9;' 8 9fi /9;'HF  	 : **	 : : 	 - : - : 	  the following large 2-itemsets:  	 9 	    ;    Example 2 Consider the calendar schema  8     , and          week                       day      the candidate large 3-itemsets, we will first generate Then the set of candidate large 3-itemsets is          3-itemsets as             .
Suppose we have computed ,    and             contrast, if we use Direct-Apriori, we will generate the candidates from      fi          .
If we use temporal aprioriGen to compute                          .
.
In  and have the set of candidate large  .
  Our second optimization technique, horizontal pruning, is also based on Lemma 2, but applied during a pass.
fi  Consider pass .
For each basic time interval  , we update (among others) the first time        is updated, for every    processed, we update 12              to be  for each        that covers  .
After       , i.e., drop the  itemsets in :  *   fi  that do not appear in :  -    all the large -itemsets for     	   .
Hence, after the first time :     *  is updated, :   *-  *   always contains   -  (plus other itemsets that will eventually be dropped).
In other words, at any time of the  processing (except before the first update), if a -itemset fi does not appear in : Now we can use the tentative :  as follows.
If an itemset fi in  9  	        	    -  fi      , then fi is not large for  .
fi  (i.e., updated at least once) to prune the candidate large -itemsets in 9	   does not appear in any of the tentative :      	      , where     -  at least one of the 1-star patterns that cover  	    	.
-  Let this particular 1-star pattern be   -  	   is a 1-star pattern that  covers  , then fi cannot be large for any star pattern  that covers  .
Indeed, any star pattern  covering      	  must cover  .
Since fi is not large for any  1-star pattern that covers  , fi is not large for  .
By Lemma 2, fi cannot be large for  .
Therefore, we may drop fi from  9    	       .
In summary, Horizontal pruning can be implemented by the following procedure.
Procedure HorizontalPrune4PreciseMatch( 9  -  that covers   if there exists a 1-star pattern   "  then return 9    ;    	     " : *-  forall 1-star patterns   9  return     	              	  	 )   *-      such that :     has not been updated even once   ;  	  that covers    	    do  ;  .
  K J N :  MK	S JN " @58 H ' F :  <K SN " @58 ' )8 9;'HF  9  MK )JN " @48/9 )8 ' 8 9;'HF 9  MK J N " 9  MK )JN  :  <K	S )J%N :  MK YS*N " @58 ' )8 9;'HF   Example 3 Let us continue example 2.
Suppose when the basic time interval have       and               .
Given the generated set of candidate large 3-itemsets    , we can further prune it by    is being processed, we already                         .
In summary, phase I of Temporal-Apriori for precise match can be instantiated as follows.
   9     	  9  	  	    = TemporalAprioriGen4PreciseMatch( );   = HorizontalPrune4PreciseMatch( 9    	 ,  	 );       We prove the correctness of Temporal-Apriori for precise match in the following way.
First, we show that the algorithm has the same output as Direct-Apriori if for each basic time interval  	  fi    	 , it uses a super set of the union of  large -itemsets for all 1-star calendar patterns that cover  .
Then we prove the equivalence of Temporal-Apriori and fi  Direct-Apriori by showing that the set of candidate large -itemsets used for each basic time interval in TemporalApriori is such a super set.
This result is summarized in Lemma 3 and Theorem 1.
Lemma 3 If Temporal-Apriori for precise match uses a super set of fi  	  7fi 	 :   *    as the set of candidate large  -itemsets for each basic time interval  , then it has the same output as Direct-Apriori for precise match.
Theorem 1 Temporal-Apriori for precise match is equivalent to Direct-Apriori for precise match.
13  week 1  day 1  day 2  day 3  day 4  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  week 2 week 3  X  week 4 week 5  X  day 5  day 6  day 7 X  Figure 4: Distribution of large itemset fi  3.4 Generating Candidate Large Itemsets for Fuzzy Match 3.4.1 Direct-Apriori for Fuzzy Match  B *  As we discussed earlier, given a fuzzy match ratio it is large for at least  "#"    , an itemset is large for a calendar pattern  w.r.t.
fuzzy match if  of the basic time intervals covered by  .
For brevity, we still refer to such itemsets as  large itemsets in the context of fuzzy match.
Similar to Direct-Apriori for precise match, Apriorias candidate generation method can be directly applied to each  	 ;"  individual basic time interval when fuzzy match is considered.
We call this approach Direct-Apriori for fuzzy match.
?   ;  : + -  9  Then phase I of Direct-Apriori for fuzzy match is instantiated in the same way as for precise match, i.e.,           	        .
   fi  Indeed, Direct-Apriori supplies phase III (i.e., procedure Update4FuzzyMatch) with the set of large -itemsets for each basic time interval.
By Lemma 1, it is easy to see that Direct-Apriori for fuzzy match can correctly generate large itemsets w.r.t.
fuzzy match for all calendar patterns.
3.4.2 Temporal-Apriori for Fuzzy Match Recall that our first optimization technique temporal aprioriGen is based on both Lemma 2 and the assumption that we are not interested in large itemsets for basic time intervals.
The assumption still applies when fuzzy match is considered.
However, Lemma 2 is not true in the context of fuzzy match.
This can be seen through a counter example.
& @%B 7717 D,F KS YS*N R0 7 D 7 " J 60 7 " DG0 E  Example 4 Consider a calendar schema  ! "
week        day  & @%B 7177 F   and fuzzy match ratio     fi   " 60 "  .
!
Suppose an itemset fi is large for the basic time intervals marked with aXa in Figure 4.
The figure shows that fi is large  <K B BN fi  " R0 fi K	S 1BN K<B YS*N 	K S 1BN  for    .
Moreover, is large for the calendar pattern  ratio   "  nor    !
requires is large for at least .
For    "  !
since it is large for 29 basic time intervals (the match    !
basic time intervals).
However, fi is not large in neither  60 7 D "  , fi is large for 3 basic time intervals, which is less than "  5 basic time intervals, which is less than "  !
.
14  !
 .
For K<B YS*N , fi   is large for    Example 4 shows that an itemset may be large for a star calendar pattern  even if it is not large for any 1-star pattern covered by  .
Therefore, we cannot directly use the same approach as we did for precise match.
Nevertheless, we can still apply temporal aprioriGen to fuzzy match with some modification: We just consider all star calendar patterns fi  B  instead of 1-star calendar patterns.
This is correct since if a -itemset is large for a calendar pattern  , then each of its fi  =    -item subset must be large for  .
The procedure is shown as follows.
Procedure TemporalAprioriGen4FuzzyMatch(  )  9     	    = ;     	  forall star patterns  that covers   9 return 9   	    =9   fi     	     	     +.-  do  aprioriGen  :    :   fi  +.-   	  fi   (*);     .
  ! "
& @B 71717 D,F &@B 17 717 F : <K )J%N " 4@ 8 %8 9 8 ' %8fi  ' 9;' 9fi(F : <KS )J%N " @48 #8 9 8 ' /9  ' 9fi(F : <K SN " @58 %8 9 8 '  ' 9;'HF : MK	S YS*N " @58 %8 '  ' 9;' %8 9 8fi(F : " : <KS JN : <K )J%N " @58 8 9 8 '  ' )9fi(F 9  MK	S )JN " : " @58 'HF 9 <K SN :" @58 ' 8 9;'HF 9 MK	S S*N 5" 4@ 8 ' 8 9fi(F   9  MK )JN " 9  <K	S )J%N 9  MK YS*N 9  <KS SN "#5@ 8 ' 8 9;' 8 9fi(F  = B : +.
: +.-  : +.- 	  Example 5 Consider the calendar schema the following large 2-itemsets:   ,            week                  day          , and           fi  .
Suppose we have computed  ,                  we use Temporal-Apriori for fuzzy match to compute the candidate large 3-itemsets, we first get        can get               and then generate    and                     aprioriGen      .
If        .
Similarly, we    .
Then the set of candidate large 3-itemsets is            .
  Note that in the procedure TemporalAprioriGen4FuzzyMatch, the statement marked with a*a requires access to  the large  fi    -itemsets for basic time intervals.
When the calendar schema includes a large number of basic time  intervals, this step will greatly increase the memory requirement, since the large itemsets for these basic time intervals must be kept.
An alternative is to use    fi  instead of    fi        when memory is the critical resource.
This alternative can reduce the memory requirement by not keeping all the large itemsets for basic time intervals; however, the downside is that it may generate some candidates that would not even be generated by Direct-Apriori.
In our experiments, we use the original proposal for pass 2 and the alternative way for the later passes.
Due to the difference between precise match and fuzzy match, our second optimization technique for precise match, horizontal pruning, cannot be directly applied to fuzzy match, either.
This is because fuzzy match allows a large itemset to be not large for some basic time intervals.
Nevertheless, a similar idea can be applied to fuzzy match.
The idea is based on the observation that an itemset is not large for a calendar pattern if it is not large for a certain number of basic time intervals covered by the pattern.
For example, an itemset fi can never be large for 80% of all Mondays if it is already known not to be large for 20% of the Mondays.
This observation leads to the following pruning procedure.
Note that we reuse the procedure Update4FuzzyMatch, which was developed to update the large itemsets w.r.t.
fuzzy match (see subsection 3.2).
The idea is to discard the  	  candidate large itemsets that cannot be large for calendar pattern  even if they are large for  .
15  Procedure HorizontalPrune4FuzzyMatch( 9  	  if there exists  that covers   "   then return 9   	      such that :  "  forall star patterns  that covers    fi  :  "     Update4FuzzyMatch  end return  9   9     	      	)  has not been updated     do begin  9  29    fi      	      ;  ;  fi  	   	  ;       ;  ;  9    .
  9   <K )J%N " @58 '  ;' 8 9fi(F :  <KS JN :  MK YS*N :  <KS YS*N 9  MK JN K JN :  <KS )J%N 9  MK JN 9  <K	S )J%N :" @48 ' 8 fi(F 5@ 8 ' )8 9;'HF 9  MK	S YS*N " @58 'HF 9  <K )J%N 9  <K )J%N " 9  <K JN  9 MK S*N 9  <KS SN "#@48 ' 8 9;'HF  Example 6 Let us continue example 5.
We have generated a set of candidate large 3-itemsets  8 9  .
Suppose all of    all itemsets in   with         ,          , and                .
If we also get 9    MK YS*N :"  9  <KS JN  can be pruned as                         .
    , we can use the procedure Update4FuzzyMatch to update a copy of  , then      have been updated at least once.
Assuming that    and get the result, for example,          were large for    and               Using the fuzzy match version of temporal aprioriGen and horizontal pruning, phase I of Temporal-Apriori can be instantiated as follows.
   9     	  9  	  	    = TemporalAprioriGen4FuzzyMatch( );   = HorizontalPrune4FuzzyMatch( 9    	 ,  	 );       The correctness of Temporal-Apriori for fuzzy match can be shown in the same way as for precise match.
First,  	  we show that the algorithm has the same output as Direct-Apriori for fuzzy match if for each basic time interval  , it  	  fi  uses a super set of the union of large -itemsets for all calendar patterns covering  .
Then we prove the equivalence fi  of Temporal-Apriori and Direct-Apriori by showing that the set of candidate large -itemsets used for each basic time interval in Temporal-Apriori is such a super set.
This result is summarized in Lemma 4 and Theorem 2.
Lemma 4 If Temporal-Apriori for fuzzy match uses a super set of fi  	  7   	 :    fi  as the set of candidate large  -itemsets for each basic time interval  , then it has the same output as Direct-Apriori for fuzzy match.
Theorem 2 Temporal-Apriori for fuzzy match is equivalent to Direct-Apriori for fuzzy match.
4 Experiments To evaluate the performance of our algorithms and optimization techniques, we performed a series of experiments on a DELL OptiPlex GX200 PC running Windows 2000 Professional.
The PC has a 667 MHz Pentium III CPU with 256 16  Precise match pass   # calendar    patterns  Fuzzy match (,  # large  # calendar  -itemsets  patterns         )       Fuzzy match (,  # large  # calendar  -itemsets  patterns  # large    -itemsets  2  130  3812  130  4003  130  4472  3  130  1770  130  1868  130  2179  4  92  341  103  366  122  445  5  28  29  29  30  32  33  Figure 5: Discovery in the KDD Cup 2000 data (  	   " 60 D   )  "    #  )  KB full cache and 256 MB main memory.
The data sets were stored on a 30 GB, 7200 RPM EIDE hard disk.
In the following, we first assess the performance of our algorithms using the transactional data published in KDD Cup 2000 [KB00].
Then we generate synthetic data sets to further evaluate the algorithms with data sets having various characteristics.
4.1 KDD Cup 2000 Data Set We choose the clicks data file in the KDD Cup 2000 data sets to perform our experiments.
The clicks data file consists of homepage request records, each of which contains attribute values describing the request and the person who sent the request.
Examples of the attributes include when the request was submitted, where the person lives, and how many children the person has, and so on.
We consider each request record as a transaction.
The requests recorded in the clicks data file are from January 30, 2000 to March 31, 2000, which cover 8 weeks (from the 6th to the 13th week in year 2000) plus 6 days (in the 14th week).
We use timeOfDay to represent  !fifi  "  &Q@E 7717 BfiRF  &  the calendar concept formed by partitioning each day into three parts: early morning (0am - 8am), daytime (8am  @%B )J 7177 ,F  & @B )J RF  - 4pm), and evening (4pm - 12pm).
We use the calendar schema           timeOfDay         7177  week           day  , where the domain values of week represent the number of week of year  2000, the domain values of day represent Sunday, Monday,  , Saturday, the domain values of timeOfDay represent  early morning, daytime, and evening.
The predicate valid evaluates to True for all basic time intervals between January 30, 2000 and March 31, 2000.
We preprocess the clicks data file to remove NULL and unknown values marked with a?a.
To simplify the problem, we focus on the categorical attributes and ignore all the attributes identified as aignorea, adatea, atimea, and acontinuousa.
The preprocessed data set consists of 777,480 transactions.
The largest transaction consists of 100 items, the  !
smallest transaction consists of 5 items, and the transactions contain 23.4 items on average.
Using the aforementioned calendar schema fifi    , the maximum and the minimum number of transactions per basic time interval are 27,807  and 12, respectively, and the average number of transactions per basic time interval is 4,180.
17  (b) Fuzzy match  (a) Precise match 1400  1400 TemporalaApriori DirectaApriori  1200 Execution time (sec)  Execution time (sec)  1200 1000 800 600 400  TemporalaApriori (m=0.9) DirectaApriori (m=0.9) TemporalaApriori (m=0.8) DirectaApriori (m=0.8)  1000  800  600  400  200 2  1.5  1 0.75 0.5 Minimum support (%)  0.33  200 2  0.25  1.5  1 0.75 0.5 Minimum support (%)  0.33  0.25  Figure 6: Execution time of our algorithms on the KDD Cup 2000 data   A 2" R0 *D  The experimental results are summarized in Figures 5 and 6.
Figure 5 shows the number of calendar patterns and large itemsets discovered from the data set with the minimum support    "    #  .
We discovered up  to large 5-itemsets along with their calendar patterns.
It is also interesting to note that we indeed discovered more patterns with fuzzy match than with precise match, and the smaller the match ratio we used for fuzzy match, the more patterns we discovered.
Figure 6 shows the execution time of Direct-Apriori and Temporal-Apriori w.r.t.
both precise match and fuzzy match with match ratios 0.9 and 0.8.
(Note that in Figure 6(b), Direct-Apriori for fuzzy match took almost the same time for match ratios 0.9 and 0.8.)
The result shows that our optimization techniques improve the performance by 2 to 3 times.
However, since the size of the data set is not very large (777,480 transactions with 23.4 items per transaction on average), the operating system can cache all the data in physical memory.
In the next subsection, we validate this performance gain for very large data sets.
4.2 Synthetic Data Sets In order to generate data sets with various characteristics, we extend the transaction data generator proposed in [AS94] to incorporate temporal features.
Specifically, for each basic time interval   	    in a given calendar schema, we first  generate a set of maximal potentially large itemsets called per-interval itemsets and then generate transactions    	  from per-interval itemsets following the exact method in [AS94].
(Due to space reason, we do not repeat this method in the paper; the reader is referred to [AS94] for the details.)
To model the phenomenon that some itemsets may have temporal patterns but others may not, we choose a subset of the per-interval itemsets from a common set of itemsets called pattern itemsets that are shared across basic time intervals but generate the others independently for each basic time interval.
We use a parameter pattern-ratio, denoted   , to decide the percentage of per-interval itemsets that should be chosen from the pattern itemsets.
18  Notation  ( ( ( ( ( ( ( (      fi   Meaning  Default value  Number of transactions per basic time interval  10,000  Avg.
size of the transactions  10  Avg.
size of the maximal potentially large itemsets  4  Avg.
number of the maximal potentially large itemsets per basic time interval  1,000  Number of items  1,000  Calendar Schema  (year: 1995-1999  ,month,day)  Pattern-ratio  0.4  Avg.
number of star calendar patterns per pattern itemset  40  Figure 7: Parameters for data generation To decide which pattern itemsets should be used for a basic time interval, we associate several star calendar patterns  4  with each pattern itemset.
For each basic time interval, we choose itemsets repeatedly and randomly from the pattern itemsets until we have enough number of pattern itemsets (i.e.,    the total number of per-interval itemsets).
Each  time when a pattern itemset is chosen, we use it as a per-interval itemset if it has an associated calendar pattern that covers the basic time interval; otherwise, the itemset is ignored.
Intuitively, the more calendar patterns are assigned to a pattern itemset, the more chances that the pattern itemset is used as per-interval itemsets.
We use a parameter 	 to adjust this feature such that the number of calendar patterns assigned to each pattern itemset conforms to a Poisson distribution with mean   .
The calendar patterns assigned to pattern itemsets are selected from the space of all star calendar patterns.
In order to model the phenomenon that the calendar patterns covering more basic time intervals are less possible than those  R0 D  covering fewer ones, we associate with each calendar pattern a weight, which corresponds to the probability that this calendar pattern is selected.
The weight of a calendar pattern is set to "    fi  , where is the number of wild-card symbols  in the calendar pattern.
The weight is then normalized so that the sum of the weights of all calendar patterns is 1.
The calendar pattern to be assigned to a pattern itemset is then chosen by tossing an      -sided weighted coin, where     is  the total number of calendar patterns.
Our data generation procedure takes eight parameters, which are shown in Figure 7.
The upper part of table shows the parameters required by the original data generator proposed in [AS94], while the lower part shows the parameters related to temporal features.
Figure 7 also shows the default values of the parameters.
To examine the performance of the algorithms with data sets having different characteristics, we generated a series of data sets, most of which were generated by varying one parameter while keeping others at their default values.
The size of the data sets ranges from 739 MB to 5.41 GB.
Our first set of experiments was to evaluate the optimization techniques with synthetic data sets.
We generated  19  x 10  (b) Fuzzy match (m=0.9)  4  3.5 TemporalaApriori DirectaApriori  x 10  3 Execution time (sec)  Execution time (sec)  2  1.5  1  2.5 2 1.5 1  0.5 0.5 0 2  1.5  1 0.75 0.5 Minimum support (%)  0.33  10  6  10  4  10  2  10  1 0.75 0.5 Minimum support (%)  0.33  10  0.25  2  3  (e) Precise match (minsup=0.75%)  10  4 Pass Number  5  6  (f) Precise match (minsup=0.75%)  12000  12000  10000  10000  8  10  6  10  4  10  Execution time (sec)  TemporalaApriori DirectaApriori Execution time (sec)  Number of candidate large itemsets  1.5  (d) Fuzzy match (m=0.9, minsup=0.75%)  10  TemporalaApriori DirectaApriori 8  0  0 2  0.25  (c) Precise match (minsup=0.75%)  10  10 TemporalaApriori DirectaApriori  Number of candidate large itemsets  (a) Precise match  4  2.5  8000 6000 TemporalaApriori DirectaApriori 4000  8000 6000 TemporalaApriori DirectaApriori 4000  2  10  2000  0  10  2  3  4 Pass Number  5  0 0  6  2000  0.2  0.4 0.6 Pattern ratio (Pr)  0.8  0  1  1  (h) Fuzzy match (m=0.9, minsup=0.75%)  (g) Precise match (minsup=0.75%) 12000  20 40 60 80 100 Number of calendar patterns per pattern itemset (N p)  (i) Fuzzy match (m=0.9, minsup=0.75%)  12000  12000  10000  10000  Execution time (sec)  Execution time (sec)  10000 8000 6000 4000  8000 6000 TemporalaApriori DirectaApriori 4000 2000  2000 0 3  Execution time (sec)  TemporalaApriori DirectaApriori  4 5 6 Avg.
size of potentially large itemsets (|I|)  0 0  7  (j) Fuzzy match (m=0.9, minsup=0.75%)  8000 6000 TemporalaApriori DirectaApriori 4000 2000  0.2  0.4 0.6 Pattern ratio (Pr)  0.8  0  1  12000  12000  10000  10000  TemporalaApriori DirectaApriori  8000 6000 4000 2000 0 3  TemporalaApriori DirectaApriori  Execution time (sec)  Execution time (sec)  Execution time (sec)  10000  8000 6000 4000  7  0 1  Precise match Fuzzy match, m=0.95 Fuzzy match, m=0.9 Fuzzy match, m=0.8  8000 6000 4000 2000  2000  4 5 6 Avg.
size of potentially large itemsets (|I|)  20 40 60 80 100 Number of calendar patterns per pattern itemset (N p)  (l) Scaleaup tests for TemporalaApriori (minsup=0.75%)  (k) Fuzzy match (minsup=0.75%)  12000  1  0.95  0.9  0.8  0.7  Match ratio (m)  Figure 8: Experimental result on synthetic data sets 20  0 10000 30000 50000 75000 Number of transactions per basic time interval (|D|)   " 60 C  a data set using the default parameters and performed experiments with various minimum supports for both precise match and fuzzy match (match ratio  "  ).
The execution time is shown in Figures 8(a) and 8(b).
The experimental  result shows that our optimization techniques are quite effective.
For precise match, Temporal-Apriori is 5 to 22 times faster than Direct-Apriori; for fuzzy match, Temporal-Apriori is 2.5 to 12 times faster than Direct-Apriori.
Moreover, all the algorithms are sensitive to the minimum support: the smaller the minimum support is, the longer the execution time is.
However, the execution time of Temporal-Apriori increases much slower than that of Direct-Apriori.
Figures  R0 *D  8(c) and 8(d) also give the total number of candidate large itemsets for the experiments with the minimum support   $  "  , showing that our two optimization techniques greatly reduced the number of candidates in each pass.
Our second set of experiments was intended to evaluate the performance of both Temporal-Apriori and DirectApriori with various kinds of data sets.
We generated three sets of data sets.
The first set of data sets uses different    values for  and default values for other parameters.
Similarly, the second and the third set of data sets uses different    and   , respectively, and uses default values for other parameters.
The experiments were performed  60 D    values for  using the minimum support "    #  .
Figures 8(e) through 8(j) show the execution time of both precise match and fuzzy  match for these data sets.
In all the experiments, Temporal-Apriori performs significantly better than Direct-Apriori.
Figures 8(e) and 8(h) indicate that Direct-Apriori is not very sensitive to pattern ratio.
However, the execution time of Temporal-Apriori increases by 100% for precise match and 200% for fuzzy match as the pattern ratio 0 to 1.
The reason is that when the pattern ratio      ranges from  increases, the number of large itemsets that have temporal patterns  increases as well and thus results in a larger number of candidate large itemsets.
In Figures 8(f) and 8(i), the execution time of Temporal-Apriori increases by 75% for both precise and fuzzy match when the parameter 	 (i.e., the number of calendar patterns per pattern itemset) ranges from 1 to 100.
This is because a larger 	   increases the supports of itemsets and results in a larger set of candidates.
In contrast, the execution time of Direct-Apriori decreases slightly as    increases.
Figures 8(g) and 8(j) show that the execution time of Temporal-Apriori increases slightly as the average  size of potentially large itemsets (   ) ranges from 3 to 7, while the execution time of Direct-Apriori decreases by   about 20%.
Our third set of experiments was intended to study the impact of match ratio to our fuzzy-match algorithms.
Fig-  60 D  ure 8(k) shows the execution time of both Temporal-Apriori and Direct-Apriori with various match ratios.
These experiments used the data set with all default parameters and the minimum support "    #  .
The result indicates that  Temporal-Apriori is very sensitive to match ratio: the larger the match ratio is, the longer time Temporal-Apriori takes.
The execution time of Direct-Apriori also increases slightly.
Nevertheless, in the worst case when match ratio is 0.7, Temporal-Apriori still performs significantly better than Direct-Apriori.
Finally, to examine the scalability of Temporal-Apriori, we generated a series of data sets with increasing number of transactions per basic time interval and performed a set of experiments for precise match and fuzzy match with different match ratios.
The sizes of the data sets range from 739MB to 5.41 GB.
As shown in Figure 8, TemporalApriori takes time linear to the number of transactions.
21  In summary, our experiments on synthetic data sets show that our optimization techniques are quite effective and the algorithms are stable for various kinds of data sets.
In addition, our optimized algorithm scales up very well w.r.t.
the number of transactions.
5 Related Work Since the concept of association rule was first introduced in [AIS93], discovery of association rules has been extensively studied [AS94, SON95, BMUT97, ZPOL97, AS96, HKK97, SK98].
The concept of association rule was also extended in several ways, including generalized rules and multi-level rules [SA95, HF95], multi-dimensional rules, quantitative rules [SA96, MY97], and constraint-based rules [BAG99, NLHM99].
Among these extensions is the discovery of temporal association rules.
There are several kinds of meaningful temporal association rules.
The problem of mining cyclic association rules (i.e., the association rules that occur periodically over time) has been studied in [ OERS98].
Several algorithms and optimization techniques were presented in [OERS98] and shown effective through a series of experiments.
However, this work is limited in that it cannot deal with multiple granularities and cannot describe real-life concepts such as the first business day of every month.
In [RMS98], the work in [OERS98] was further extended to approximately discover user-defined temporal patterns in association rules.
The work in [RMS98] is more flexible and practical than [ OERS98]; however, it requires user-defined calendar algebraic expressions in order to discover temporal patterns.
Indeed, this is to require useras prior knowledge about the temporal patterns to be discovered.
Although the calendar algebra adopted in [RMS98] is a powerful tool to define temporal patterns, users need to know exactly what temporal patterns they are interested in to give such expressions.
In some cases, users lack such prior knowledge.
Our work differs from [OERS98] and [RMS98] in that instead of using cyclic patterns or user-defined calendar algebraic expressions, we use calendar schema as a framework for temporal patterns.
As a result, our approach usually requires less priori knowledge than [OERS98] and [RMS98].
In addition, unlike [RMS98], which discover temporal association rules for one user-defined temporal pattern, our approach considers all possible temporal patterns in the calendar schema, thus we can potentially discover more temporal association rules.
Finally, based on the representation mechanisms proposed in [LMF86] or [BJW00], we can have calendar schemas for both cyclic and userdefined temporal patterns.
Thus, cyclic patterns and calendar algebra expressions can be considered as special cases of calendar patterns.
In [AR00], the discovery of association rules that hold in the transactions during the itemsa life time was discussed.
The algorithm Apriori was extended to discover such association rules.
Our problem differs in that we consider the association rules for calendar patterns instead of the life time of the items.
There are other related research activities.
In [LWJ00], discovery of calendar-based event patterns was discussed.
22  In [CP98], a generic definition of temporal patterns and a framework for discovering them were presented.
In [CP99], the discovery of the longest intervals and the longest periodicity of association rules was discussed.
In [RR99], it was proposed to add temporal features to association rules by associating a conjunction of binary temporal predicates that specify the relationships between the timestamps of transactions.
These works consider different aspects of temporal data mining; we consider them as complementary to ours.
Finally, a bibliography of temporal data mining can be found in [RS99].
6 Conclusion and Future Work In this paper, we studied the discovery of association rules along with their temporal patterns in terms of calendar schemas.
We identified two classes of temporal association rules, temporal association rules w.r.t.
precise match and temporal association rules w.r.t.
fuzzy match, to represent regular association rules along with their temporal patterns.
An important feature of our representation mechanism is that the corresponding data mining problem requires less prior knowledge than the prior methods and hence may discover more unexpected rules.
The discovered rules are easier to understand.
Moreover, we extended Apriori, an existing algorithm for mining association rules, to discover temporal association rules w.r.t.
both precise match and fuzzy match.
By studying the relationships among calendar patterns, we developed two optimization techniques to improve the performance of the data mining process.
Our experiments showed that our optimization techniques are quite effective.
The future work includes two directions.
First, we would like to explore other meaningful semantics of temporal association rules and extend our techniques to solve the corresponding data mining problems.
Second, we would like to consider temporal patterns in other data mining problems such as clustering.
References [AIS93]  R. Agrawal, T. Imielinski, and A. N. Swami.
Mining association rules between sets of items in large databases.
In Proc.
of the 1993 Intal Conf.
on Management of Data, pages 207a216, 1993.
[AR00]  J.M.
Ale and G.H.
Rossi.
An approach to discovering temporal association rules.
In Proc.
of the 2000 ACM Symposium on Applied Computing, pages 294a300, 2000.
[AS94]  R. Agrawal and R. Srikant.
Fast algorithms for mining association rules in large databases.
In Proc.
of the 1994 Intal Conf.
on Very Large Data Bases, pages 487a499, 1994.
[AS96]  R. Agrawal and J. C. Shafer.
Parallel mining of association rules.
IEEE Transactions on Knowledge and Data Engineering, 8(6):962a969, 1996.
23  [BAG99]  R.J. Bayardo Jr., R. Agrawal, and D. Gunopulos.
Constraint-based rule mining in large, dense databases.
In Proc.
of the 15th Intal Conf.
on Data Engineering, pages 188a197, 1999.
[BJW00]  C. Bettini, S. Jajodia, and X.S.
Wang.
Time granularities in databases, data mining, and temporal reasoning.
Springer-Verlag, 2000.
[BMUT97] S. Brin, R. Motwani, J. D. Ullman, and S. Tsur.
Dynamic itemset counting and implication rules for market basket data.
In Proc.
of ACM SIGMOD Intal Conf.
on Management of Data, pages 255a264, 1997.
[CP98]  X. Chen and I. Petrounias.
A framework for temporal data mining.
In Proc.
of the 9th Intal Conf.
on Database and Expert Systems Applications, pages 796a805, 1998.
[CP99]  X. Chen and I. Petrounias.
Mining temporal features in association rules.
In Proc.
of the 3rd European Conf.
on Principles and Practice on Knowledge Discovery in Databases, pages 295a300, 1999.
[HF95]  J. Han and Y. Fu.
Discovery of multiple-level association rules from large databases.
In Proc.
of 21th Intal Conf.
on Very Large Data Bases, pages 420a431, 1995.
[HKK97]  E. Han, G. Karypis, and V. Kumar.
Scalable parallel data mining for association rules.
In Proc.
of the 1997 ACM SIGMOD Intal Conf.
on Management of Data, pages 277a288, 1997.
[KB00]  R. Kohavi and C. Brodley.
2000 knowledge discovery and data mining cup.
Data for the Cup was provided by Blue Martini Software and Gazelle.com, 2000. http://www.ecn.purdue.edu/KDDCUP/.
[LMF86]  B. Leban, D. McDonald, and D. Foster.
A representation for collections of temporal intervals.
In Proc.
of AAAI-1986 5th Intal Conf.
on Artifical Intelligence, pages 367a371, 1986.
[LWJ00]  Y. Li, X.S.
Wang, and S. Jajodia.
Discovering temporal patterns in multiple granularities.
In Proc.
of Intal Workshop on Temporal, Spatial and Spatio-temporal Data Mining, 2000.
[MY97]  R. J. Miller and Y. Yang.
Association rules over interval data.
In Proc.
of the 1997 ACM SIGMOD Intal Conf.
on Management of Data, pages 452a461, 1997.
[NLHM99] R. T. Ng, L. V. S. Lakshmanan, J. Han, and T. Mah.
Exploratory mining via constrained frequent set queries.
In Proc.
of the 1999 ACM SIGMOD Intal Conf.
on Management of Data, pages 556a558, 1999.
[OERS98]  B. OEzden, S. Ramaswamy, and A. Silberschatz.
Cyclic association rules.
In Proc.
of the 14th Intal Conf.
on Data Engineering, pages 412a421, 1998.
[RMS98]  S. Ramaswamy, S. Mahajan, and A. Silberschatz.
On the discovery of interesting patterns in association rules.
In Proc.
of the 1998 Intal Conf.
on Very Large Data Bases, pages 368a379, 1998.
24  [RR99]  C.P.
Rainsford and J.F.
Roddick.
Adding temporal semantics to association rules.
In Proc.
of the 3rd European conf.
on principles and practice of knowledge discovery in databases, pages 504a509, 1999.
[RS99]  J.F.
Roddick and M. Spiliopoulou.
A bibliography of temporal, spatial and spatio-temporal data mining research.
SIGKDD Explorations, 1(1):34a38, June 1999.
[SA95]  R. Srikant and R. Agrawal.
Mining generalized association rules.
In Proc.
of the 21th Intal Conf.
on Very Large Data Bases, pages 407a419.
Morgan Kaufmann, 1995.
[SA96]  R. Srikant and R. Agrawal.
Mining quantitative association rules in large relational tables.
In Proc.
of the 1996 ACM SIGMOD Intal Conf.
on Management of Data, pages 1a12, 1996.
[SK98]  T. Shintani and M. Kitsuregawa.
Parallel mining algorithms for generalized association rules with classification hierarchy.
In Proc.
of ACM SIGMOD Intal Conf.
on Management of Data, pages 25a36, 1998.
[SON95]  A. Savasere, E. Omiecinski, and S. B. Navathe.
An efficient algorithm for mining association rules in large databases.
In Proc.
of the 1995 Intal Conf.
on Very Large Data Bases, pages 432a444, 1995.
[ZPOL97] M.J. Zaki, S. Parthasarathy, M. Ogihara, and W. Li.
New algorithms for fast discovery of association rules.
In Proc.
of the 3rd Intal Conf.
on Knowledge Discovery and Data Mining, pages 283a286, 1997.
A  Proof Sketch   %%? # 7   %%? # A?  Proof of Lemma 1 Consider any calendar pattern  .
Suppose    itemset in  B *  :   fi  , its counter  in the -th update of than     , :    fi    7 B * -  since it is not dropped in the last update.
For each itemset dropped    , its counter          contains all and only the itemsets that are large for at least  fi  Proof of Lemma 2 Suppose there exists a 1-star calendar pattern   *-  for   =      of the basic time intervals covered by  .
"#"  covered by  such that an itemset fi is not large  w.r.t.
precise match.
Then there exists at least one basic time interval   Since   is covered by  , the basic time interval    	  interval   , i.e., it cannot be large for more  of the basic time intervals covered by  .
Since all large itemsets are processed, for all calendar patterns  "#"  *-  :   basic time intervals are covered by  .
For each  	  	  *-  covered by   for which fi is not large.
is also covered by  .
Then fi is not large for at least one basic time    covered by  , which leads to contradiction.
fi  Proof of Lemma 3 It suffices to prove that for each pass , if Temporal-Apriori uses a super set of 7 fi  	  fifi	  :        as the set of candidate large -itemsets for  , it has the same output as Direct-Apriori for precise match.
fi  Consider the algorithm Direct-Apriori.
Denote the set of candidate large -itemsets generated in phase I as 9 fi  the set of large -itemsets generated in phase II as :    	     , and the output for each star calendar pattern  as : 25       	  :"    fi    ,   fi	 fi  :   	    in output.
  fi  *-  Consider the algorithm Temporal-Apriori.
Denote the set of candidate large -itemsets, which is a super set of    fi	   7  :          star calendar pattern  as :  :  Since   	      	    , as 9   ?
fifi  is the set of    Thus, we have :  :  fi	    "       fi	      :  large -itemsets in  	          "  fi      :  , and the output for each   for each star pattern  .
fi   	  	    :  by definition, it is easy to see     	    in phase II as :   .
We need to prove :     	    fi   	    , the large itemsets derived from 9      :     	    for all    	 .
  .
fi  	 , let :  	 " : 	 7   fi	   : *2"   	  : 	 .
Since 9 	 is a super set of 7fifi	    : - , 9 	 is also a super and : set of : 	 .
When : 	 is computed from 9 	 , all -itemsets in : 	 remain in : 	 since : 	 " : 	 7    	   : - .
Thus, we have : 	 	 : 	 and then : 	 : .
By definition, for each fi : , fi is in : 	 for all  	 covered by .
By lemma 2, fi is also in : *- for all *covered by .
It is easy to see that if 	 is covered by , then at least one 1-star calendar pattern %- that covers 	 is also covered by .
It follows that for all 	 covered by , fi is in :  	 , i.e., fi-: .
This shows :  	 : .
  Now letas prove        :  fi	            fi       :                .
Given a basic time interval          	  Proof of Theorem 1  *      :     -  :      fi                                       fi    , we have :  fi	  :           fi                fi                                    Consider the fact :           fi        :    fi                        7fifi	          of         fi        fi          .
This concludes the proof.
fi    -  First, the set of candidate large -itemsets generated by TemporalAprioriGen is a super set , since for each 1-star calendar pattern  .
Second, if the input of HorizontalPrune is a super set of  set, since the output is the intersection of the input and 7   	 , aprioriGen generates a super set of    	    : *- , then its output is also a super  that covers    7     	    :    *            .
By Lemma 3, we know Temporal-Apriori  has the same output as Direct-Apriori.
That is, Temporal-Apriori is equivalent to Direct-Apriori.
Proof of Lemma 4 and Theorem 2    Lemma 4 and Theorem 2 can be proved in the same way as Lemma 3 and  Theorem 2, respectively.
Proofs are omitted due to space reason.
26  
On Non-local Propositional and Local One-variable Quantified CTL Sebastian Bauer Institut fuEr Informatik, UniversitaEt Leipzig, Augustus-Platz 10a11, 04109 Leipzig, Germany bauer@informatik.uni-leipzig.de Frank Wolter Institut fuEr Informatik, UniversitaEt Leipzig, Augustus-Platz 10a11, 04109 Leipzig, Germany wolter@informatik.uni-leipzig.de  Abstract We prove decidability of anon-locala propositional CTL , where truth values of atoms may depend on the branch of evaluation.
This result is then used to show decidability of the aweaka one-variable fragment of first-order (local) CTL , in which all temporal operators and path quantifiers except atomorrowa are applicable only to sentences.
Various spatio-temporal logics based on combinations of CTL and RCC-8 can be embedded into this fragment, and so are decidable.
1 Introduction This paper continues the investigation of the computational behaviour of first-order branching time temporal logics started in [6].
A anegativea result obtained in [6] is the undecidability of the one-variable fragment of quantified computational tree logic CTL (both bundled [1, 10] and aunbundleda versions, and even with sole temporal operator asome time in the futurea).
On the other hand, it was shown that by restricting applications of first-order quantifiers to state (i.e., path-independent) formulas, and applications of temporal operators and path quantifiers to formulas with at most one free variable, decidable fragments can be obtained.
Here we prove decidability of another kind of fragment of first-order CTL , the so-called weak one-variable fragment, in which quantifiers are not restricted to state formu-  Ian Hodkinson Department of Computing, Imperial College, 180 Queenas Gate, London SW7 2BZ, U.K. imh@doc.ic.ac.uk Michael Zakharyaschev Department of Computer Science, Kingas College, Strand, London WC2R 2LS, U.K. mz@dcs.kcl.ac.uk  las, but only the next-time operator may be applied to open formulas, while all other temporal operators and path quantifiers are applicable only to sentences.
The main technical instrument is the method of quasimodels [4].
We first show decidability of the non-local version of propositional CTL , where truth values of atoms may depend on the branch of evaluation.1 We then show that this logic can express the existence of a certain quasimodel associated with a given formula of the weak one-variable fragment.
Since this existence is equivalent to the satisfiability of the formula, the decidability of the weak fragment follows.
This decidability result is not only of interest per se, but also because it can be used to obtain decidability results for certain spatiotemporal logics based on CTL and the region connection calculus RCC-8 (see the survey papers [5, 9]).
All omitted details of proofs can be found in the full draft version of the paper at http://www.dcs.kcl.ac.uk/staff/mz.
2 Decidability of non-local PCTL The propositional language PCTL [3, 7] extends propositional logic with temporal connectives U; S (auntil,a asincea) and a path quantifier E (athere exists a branch (or history)a).
The dual path quantifier A (afor all branches (or histories)a) is defined as an abbreviation: AD = E D. Other standard abbreviations we need are: 3F D = U D, 2F D = 3F D, 3PD = S D, D = U D, P D = S D (asome time in the future,a aalways in the future,a asome time  : :  >  ?
:: >  ?
1 This contrasts with the behaviour of process logic, the local version of which is decidable, while the non-local one is undecidable [2].
Proceedings of the Ninth International Symposium on Temporal Representation and Reasoning (TIMEa02) 1530-1311/02 $17.00 AS 2002 IEEE  in the past,a aat the next momenta and aat the previous momenta).
We write P for the underlying set of propositional atoms.
PCTL is interpreted in (bundled and unbundled) models based on D-trees.
A tree is a strict partial order T = hT ; <i such that for all w 2 T , the set fv 2 T : v < wg is linearly ordered by <.
When we write T for a tree, it will be implicit that T = hT ; <i.
For t 2 T , let ht (t ) = jfu 2 T : u < t gj.
A full branch of F is a maximal linearly-ordered subset of T .
In this paper we are only concerned with D-trees.
An D-tree is a rooted tree whose full branches, ordered by <, are all order-isomorphic to the natural numbers hS N ; <i .
A bundle on T is a set H of full branches of T with H = T .
In this paper we deal with the anon-locala variant of PCTL in which truth values of atoms can depend on the branch  of evaluation.
Thus, a bundled model has the form  M = T; H ; h , where T is an D-tree, H is a bundle on T, and h : P !
a(f(I,; t ) : t 2 I, 2 H g).
M is a full tree model if H is the set of all full branches of T. The semantics of PCTL is now defined as follows, where t 2 I, 2 H :     h0 ( p) = h( p) \ f(I,; t ) : t 2 I, 2 H0 g for any atom p. It is easy to translate PCTL -formulas to two-sorted first-order formulas with the same meaning.
It follows that for any I, 2 H0 , t 2 I,, and any formula D, we have (M; I,; t ) j= D iff (N; I,; t ) j= D. This completes the proof for the bundled case.
Suppose D now thatEH contains all full branches of T and let N = T0 ; H 0 ; h0 be the full tree model based on N, where H 0  H0 is the set of all full branches of T0 , and h0 ( p) = h( p) \ f(t ; I,) : t 2 I, 2 H 0 g, for an atom p. We claim that for all PCTL -formulas D, all full branches Il of T0 and all t 2 Il, we have (M; Il; t ) j= D   for an atom p, (M; I,; t ) j= p iff (I,; t ) 2 h( p);  the booleans are defined as usual;   (M; I,; t ) j=    (M; I,; t ) j= D S D    (M; I,; t ) j=  D U D iff there is u > t such that u 2 I,, D, and (M; I,; v) j= D for all v 2 (t ; u), where (t ; u) = fv 2 T : t < v < ug;  (M; I,; u) j=  iff there is u < t with (M; I,; u) j= D and (M; I,; v) j= D for all v 2 (u; t );  t 2 Il.
ED iff (M; Il; t ) j= D for some Il 2 H  with  R EMARK 1.
By requiring that (I,; t ) 2 h( p) if and only if (I,0 ; t ) 2 h( p) for all p, t, I,, I,0 with t 2 I, \ I,0 , we obtain the traditional local semantics in which the truth values of atoms do not depend on the branch of evaluation.
For an atom p, this independence is expressible (at the root of an D-tree) in the non-local semantics by It( p) = (E p !
A p) ^ A2F (E p !
A p): Thus, a formula D is satisfiable in the local semantics iff (D _  3F D) ^ :3P> ^  ^  (N; Il; t ) j= D:  Fix a PCTL -formula D. D EFINITION 3.
Let sub(D) denote the set of subformulas of D and their negations.
A type for D is a subset D of sub(D) such that D ^ D 2 D iff D 2 D and D 2 D, for every D ^ D in sub(D), and :D 2 D iff D 2 = D, for every :D 2 sub(D).
A set IL of types is said to be coherent if it is non-empty and for T S D 2 sub(D), the conditions ED 2 IL, ED 2 IL, and all ES D 2 IL are equivalent.
Fix an D-tree T = hT ; <i.
D EFINITION 4.
Given a non-empty set ILt of types for each t 2 T , and a full branch I, of T, a run in I, is a map  It( p)  L EMMA 2.
If a PCTL -formula D is satisfiable in a full (bundled) tree model, then D is satisfiable in a full (respectively, bundled) tree model based on a countable D-tree.
  iff  The proof is by induction on D. The atomic, boolean, and temporal cases are trivial.
Consider the case ED and inductively assume the result for D. If (M; Il; t ) j= ED, pick I, 2 H0 containing t. Clearly, (M; I,; t ) j= ED, so (N; I,; t ) j= ED.
Then there is I,0 2 H0 with (N; I,0 ; t ) j= D. Thus, (M; I,0 ; t ) j= D. Inductively, (N; I,0 ; t ) j= D. So (N; Il; t ) j= ED, as required.
The converse implication is easy.
r:I,!
p 2IS  is satisfiable in the non-local semantics, where IS denotes the set of atoms occurring in D. Hence, local satisfiability is reducible to non-local satisfiability.
Proof.
Let M = T; H ; h be a tree model.
We may view M as a two-sorted first-order structure, the two sorts being T and H .
Taking a countable elementary of    substructure this yields a bundled tree model N = T0 ; H0 ; h0 whose tree T0 and bundle H0 are countable.
Here,  [  ILt  t 2I,  such that   r(t ) 2 ILt for each t 2 I,,  for all D U D 2 sub(D) and t 2 I,, we have D U D 2 r(t ) iff there is u > t with u 2 I,, D 2 r(u), and D 2 r(v) for all v 2 (t ; u),  Proceedings of the Ninth International Symposium on Temporal Representation and Reasoning (TIMEa02) 1530-1311/02 $17.00 AS 2002 IEEE   for all D S D 2 sub(D) and t 2 I,, we have D S D 2 r(t ) iff there is u < t with D 2 r(u) and D 2 r(v) for all v 2 (u; t ).
D EFINITION 5.
A family (ILt : t 2 T ) of coherent sets of types is said to be an unbundled quasimodel for D over if  T  1.
D 2 D 2 ILt for some t 2 T and D 2 ILt ,  T  2. for all t 2 T , D 2 ILt , there is a full branch I, of containing t and there is a run r in I, such that r(t ) = D,  T  3. for each full branch I, of , there exists a run in I,.
: t 2 T ) is a bundled quasimodel for D over fies conditions 1 and 2.
(ILt  T if it satis-  L EMMA 6.
D is satisfied in a (bundled) model iff there is a (bundled) quasimodel for D over a countable D-tree.
M  T  M    = ; I,0 ; t0 ) j= D for ; H ; h be such that ( Proof.
Let some I,0 2 H and t0 2 I,0 .
By Lemma 2, we can assume that is countable.
For I, 2 H , t 2 I, and t 2 T , let  T  M  tp(t ; I,) = fD 2 sub(D) : ( ; I,; t ) j= Dg; ILt = ftp(t ; I,) : t 2 I, 2 H g: Clearly, tp(t ; I,) is a type for D and ILt is coherent.
For any I, 2 H , the map rI, : t 7!
tp(t ; I,) is then a run in I,.
We claim that (ILt : t 2 T ) is a quasimodel for D over (a bundled one if is bundled, and an unbundled one otherwise).
As ( ; I,0 ; t0 ) j= D, we have D 2 tp(t0 ; I,0 ) 2 ILt0 .
For each t 2 T and D 2 ILt , we have D = tp(t ; I,) for some I, 2 H containing t, so rI, (t ) = D and condition 2 of Definition 5 holds.
Finally, is a full tree model, for all I, 2 H , rI, is a run in I,, so if it is clear that condition 3 holds.
Conversely, let (ILt : t 2 T ) be a quasimodel for D over a countable D-tree .
Let <D 2 denote the set of finite sequences of 0s and 1s.
For IV 2 <D 2, jIVj denotes the length of IV.
By replacing with  T  M  M  M  T   T T  <  D  2 =def f(t ; IV) : t 2 T ; IV 2 <D 2; ht (t ) = jIVjg;  i.e., a countable D-tree when ordered by (t ; IV) < (u; Iz) iff t < u in T and IV is an initial segment (a prefix) of Iz, and by letting IL(t ;IV) = ILt for all t ; IV, we can assume that () for each t 2 T and D 2 ILt , there are 2D full branches I, of containing t such that there is a run r in I, with r(t ) = D. Each ILt is finite, so there are countably many pairs (t ; D) with t 2 T , D 2 ILt .
Enumerate them as (tn ; Dn ), n < D. Inductively, using (), choose a full branch I,n 3 tn for each n < D, such that (i) there is a run rI,n in I,n with rI,n (tn ) = Dn , and (ii) I,n = 6 I,m for all m < n. If we have a bundled quasimodel, let H = fI,n : n < Dg.
This is clearly a bundle on .
If we have an unbundled quasimodel, let H be the set of all full branches of , and further choose for each  T  T  T  I, 2 H n fI,n : n < Dg a run rI, in I,; this can be done by condition 3 of Definition 5.
So we have defined a run rI, in    I,, for each I, 2 H .
Now define a model = ; H ; h by taking h( p) = f(I,; t ) : t 2 I, 2 H ; p 2 rI, (t )g for p 2 P .
M T  C LAIM .
For all I, 2 H , all t 2 I,, and all D 2 sub(D), we have ( ; I,; t ) j= D iff D 2 rI, (t ).
P ROOF OF CLAIM .
The proof is by induction on D. For atomic D = p, we have ( ; I,; t ) j= p iff (I,; t ) 2 h( p), iff p 2 rI, (t ) as required.
The boolean cases are trivial.
For D U D 2 sub(D), we have ( ; I,; t ) j= D U D iff there is u 2 I, such that u > t, ( ; I,; u) j= D, and ( ; I,; v) j= D for all v 2 (t ; u).
Inductively, this holds iff there is u 2 I, with u > t, D 2 rI, (u), and D 2 rI, (v) for all v 2 (t ; u).
Since rI, is a run in I,, this is iff D U D 2 rI, (t ), as required.
The case of S is similar.
Finally, for ED 2 sub(D), we have ( ; I,; t ) j= ED iff ( ; Il; t ) j= D for some Il 2 H with t 2 Il.
Inductively, this is iff D 2 rIl (t ) for some Il 2 H with t 2 Il.
But evidently, S ILt = frIl (t ) : Il 2 H ; t 2 Ilg, so this is iff D 2 ILt .
Since ILt is coherent, this is iff ED 2 rI, (t ), as required.
The claim is proved.
Now let t 2 T be such that D 2 D for some D 2 ILt .
We may choose n < D with (t ; D) = (tn ; Dn ).
Then t 2 I,n 2 H and rI,n (t ) = D, so by the claim, ( ; I,n ; t ) j= D. Thus, D has a model.
M  M M  M  M  M  M  M  L EMMA 7.
Given a PCTL -formula D, it is decidable whether D has an unbundled quasimodel over a countable D-tree.
The same holds for bundled quasimodels.
Proof.
We will express the existence of a quasimodel in monadic second-order logic.
Given D, we can effectively construct the set C of all coherent sets of types.
A quasimodel over an D-tree has the form (ILt : t 2 T ) where ILt 2 C for each t; we will express this by unary relation variables PIL for each IL 2 C , the aim being that PIL is true at t iff ILt = IL.
We then express the stipulations of Definition 5 in terms of the PIL , as follows.
Let RD (D 2 sub(D)) be unary relation variables.
For a type D for D, let  T  DD (x)  ^  =  D2D  R D (x ) ^  ^  :RD (x):  D2sub(D)nD  The formula DD (x) says that the RD (x) define the type D at x.
For a unary relation variable B, let D be the conjunction of:    V  IL2C  8x B(x) ^ PIL (x) !
W    D2IL DD (x)  ,   8x RD1 UD2 (x) $9y (B(y) ^ x < y ^ RD2 (y) ^ 8z (x < z < y !
RD1 (z))) , for all D1 U D2 2 sub(D),  8x RD1 SD2 (x) $ 9y (y < x ^ RD2 (y) ^ 8z (y < z < x !
RD1 (z))) , for all D1 S D2 2 sub(D).
Proceedings of the Ninth International Symposium on Temporal Representation and Reasoning (TIMEa02) 1530-1311/02 $17.00 AS 2002 IEEE  So assuming that B defines a full branch, D says that the RD define a run in B.
Let I,(B) be a monadic second-order formula expressing that B is a full branch (a maximal linearlyordered set): It(X ) X Y I,(B)  8xy X x ^ X y !
x y _ x y _ y 8x X x !
Y x It B ^8X It X ^ B  X !
X  B  =    ( ( )  =  ( )  ( ( )  =  =  <  <  ( ( )  9  IL2C  8x _  h  PIL (x)  IL2C  i  IL0 (x)  IL0 2C IL= 6 IL0  h  ^ 8B I, B !
9 R ( )  D  ^ 9x _  D  i  D2sub(D)  IL2SC D2 IL  IL( )  IL2C D2IL  ( )  ( )  PIL (x)  ^  h  8x ^ P x !9B I, B ^ B x ^ 9 R D ^ D D(  !
i D (x))  :  9  f  g  Here, 9IL2C PIL denotes PIL1 : : : PILk , for C = IL1 ; : : : ; ILk , and similarly for the other 9s.
If we are dealing with bundled models, the conjunct B [I,(B) 9D2sub(D) RD D] on the second line should be omitted.
It should be clear that for any D-tree , we have = Al iff there is a quasimodel for D over (bundled or unbundled, as appropriate).
It follows from decidability of S2S [8] that it is decidable whether a given monadic second-order sentence is true in some countable D-tree.
The lemma now follows.
8  T  !
T  Tj  As a consequence of Lemmas 6 and 7 we finally obtain T HEOREM 8.
It is decidable whether a PCTL -formula has a full tree model in the non-local semantics.
The same holds for bundled models.
3 Decidability of the weak one-variable fragment of quantified PCTL Fix an individual variable x and denote by QPCTL1 the one-variable fragment of first-order PCTL , which can be defined as the closure of the sets P0 (x); P1 (x); : : : of unary predicates and p0 ; p1 ; : : : of propositional variables under is the operators x, , , E, , U and S. Note that now regarded as a primitive operator.
The weak one-variable fragment QPCTLw of QPCTL consists of all QPCTL1 formulas in which the temporal operators U and S and the path-quantifier E are applied to sentences only.
Thus, is the only temporal operator which can be applied to open formulas.
f g 9 ^:   f  g        ;:::;  2  I (w)  p0  T  E ;:::  M a f g!
in the signature of QPCTL1 athe state of at w. As before, is called a full model if H contains all full branches is a function : x D. Let of .
An assignment in w I, H and let D be a formula.
The truth-relation ( ; I,; w) =a D (or (I,; w) =a D if is understood, or ( ; I,; w) = D[d ], where (x) = d) is defined inductively by taking:  M T 2 2 M j M j    D2sub(D)  9  I (w)  I (w) = D; P0  ):  ^ ^ :P  M  D  x);  Thus, the following monadic second-order formula Al is effectively constructible from D: PIL  T h i  ( ));  ( )  M  T  = ; H ; D; I , A QPCTL1 -model is a quadruple where = T ; < is an D-tree, H is a bundle on , D is a non-empty set, the domain of , and I is a function associating with every time point w T a usual first-order structure  M  j  a  j Ia iff I w j a Ia, for atomic Ia; I, w j a 9x D iff I, w j b D for some assignment b;  (I,; w) =a (  M  ;  ( ) =  ) =  (  ;  ) =  for the temporal operators and path quantifiers, the definition is the same as in the propositional case.
Note that we have returned to the traditional alocala semantics in which truth values of atoms do not depend on the branch I, of evaluation.
The main result we prove in the remainder of this section is the following T HEOREM 9.
The satisfiability problem for QPCTLw formulas in both bundled and full models is decidable.
R EMARK 10.
(1) We remind the reader that the satisfiability problem for the full one variable fragment QPCTL1 in both bundled and full models is undecidable [6].
(2) Actually, using somewhat more sophisticated machinery (in particular, a mosaic technique) one can generalise Theorem 9 to the two variable, monadic, and guarded can be applied to formufragments of QPCTL in which las with at most one free variable and the other temporal operators and path quantifiers only to sentences.
(3) Theorem 9 and its generalisation above still hold if we extend QPCTLw with individual constants; however, functional symbols and equality may lead to undecidability, cf.
[4].
We will prove this result in two steps.
First, we show that a QPCTLw -formula is satisfiable iff it is satisfiable in certain quasimodels.
Then we will reduce satisfiability in quasimodels to non-local propositional satisfiability.
We begin the proof of Theorem 9 by recalling that the bundled case is reducible to the aunbundleda one [6].
So it is enough = ; D; I .
to consider satisfiability in full models Fix a QPCTLw -sentence D. For simplicity we may assume that any subsentence D of D is replaced by U D. Thus, is only applied to formulas with free variable x.
Proceedings of the Ninth International Symposium on Temporal Representation and Reasoning (TIMEa02) 1530-1311/02 $17.00 AS 2002 IEEE    M hT i ?
We define sub(D) and types for D as in Definition 3.
For every formula I,(x) = D(x) 2 sub(D) we reserve fresh unary predicates PI,i (x), and for every I, of the form ED, D1 U D2 , or D1 S D2 in sub(D) we reserve fresh propositional variables piI, , where i = 0; 1; : : : .
The PI,i (x) and piI, are called the i-surrogates of I,(x) and I,, respectively.
For D 2 sub(D), denote by Di the result of replacing in D all its subformulas of the form D, D U D, D S D, or ED that are not within the scope of another occurrence of a non-classical operator by their i-surrogates.
Thus, Di is a purely first-order (non-temporal) formula.
Let Ii = fDi : D 2 Ig for any set I  sub(D).
The idea behind these definitions is as follows.
The formulas Di abstract from the temporal component of D and can be evaluated in a first-order structure without taking into account its temporal evolution.
Of course, later we have to be able to reconstruct the truth value of D in temporal models from the truth value of the Di .
In contrast to the linear time case, we need a list of abstractions D0 ; D1 ; : : :, since the temporal evolution depends on branches.
So, intuitively, each i < D represents a branch.
(Actually, we will see that finitely many i < D are enough, since we have to represent branches only up to a certain equivalence relation).
sub(D) Let D(D) = 44 .
j  \\  Si  iff  ik  ED 2  [\  Si  iff  ik  D2  T Lest this be confusing, we note that S  [\  Si :  ik  i is the set of formulas occurring in every type in Si .
S (ii) T is a set of maps D : f1; : : : ; nI g !
ik Si , called traces, where nI D(D) is a natural number 	 depending on I and such that fD(i) : D 2 T g : i  nI = S .
T  The set of sentences in fD(i) : D 2 T g will be denoted by I(i).
For a trace D, we set D=  [  i (D(i)) ;  inI  T  =  fD : D 2 T g  :  D EFINITION 12.
Let I = hS ; T i be a state candidate for D and   D D  = D; P0 ; : : : ; p0 ; : : :  D  a first-order structure in the signature of QPCTL1 .
For every a 2 D we define the trace of a (with respect to I) as tr(a) = fD 2  We say that  L EMMA 13.
A state candidate I = hS ; T i for D is realisable iff the first-order sentence IaI =  ^  D2T  9x D ^ 8x  _  D  D2T  is satisfiable.
D EFINITION 14.
A connection is a quadruple (a; I; R; N ) consisting of two realisable state candidates a = (S ; T ) and I = (U ; V ), a relation R  T  V with domain T and range V , and a relation N  f1; : : : ; na gf1; : : : ; nI g with range f1; : : : ; nI g, such that for all (i; j) 2 N, all (D; D0 ) 2 R, and all D 2 sub(D), we have D 2 D(i) iff D 2 D0 ( j).
j  D EFINITION 11.
A state candidate for D is a pair of the form I = (S ; T ) in which: (i) S = fS1 ; : : : ; Sk g, where each Si is a set of types for D such that, for every sentence D, we have D 2 D iff D 2 D0 , for any D; D0 2 Si , and for every ED 2 sub(D),  ED 2  State candidates represent states w of temporal models.
The intuition behind this definition will be clear from the proof of the theorem below.
Here we only say that, roughly, the components Si of a state candidate I = hS ; T i represent the states of a moment w in different branches, and each trace D 2 T shows the types of one element of the domain of w in these states (i.e., its possible states in different histories).
It follows immediately from the definition that we have:  [  inI  i (sub(D))  :  D j= D[a]g  D realises I if T = ftr(a) : a 2 Dg.
:  A connection describes how (the abstract representation I of) a state w is related to (the abstract representation a of) its immediate predecessor.
To this end, the relation R between the traces in both representations is fixed.
For an D-tree = hT ; <i and w 2 T , denote by B(w) the set of full branches coming through w.  T  T  D EFINITION 15.
A quasimodel for D over is a map f associating with the root w0 of a pair f (w0 ) = (Iw0 ; gw0 ), where Iw0 is a realisable state candidate, and with every non-root point w 2 T a pair f (w) = (Cw ; gw ), where Cw = (aw ; Iw ; Rw ; Nw ) is a connection, and all gw , for w 2 T , are functions from B(w) onto f1; : : : ; nIw g such that the following hold:  T  T  1. if v is the immediate predecessor of w in , then Iv = aw and Nw = f(gv (I,); gw (I,)) : I, 2 B(w)g;  2. for all I, 2 B(w), D U D 2 Iw (gw (I,)) iff there exists u > w with u 2 I,, D 2 Iu (gu (I,)) and D 2 Iv (gv (I,)), for all v 2 (w; u) (I(i) was defined after Definition 11);  3. for all I, 2 B(w), D S D 2 Iw (gw (I,)) iff there exists u < w with D 2 Iu (gu (I,)) and D 2 Iv (gv (I,)) for all v 2 (u; w).
We say that f satisfies D if there exists w 2 T such that S Iw = (Sw ; Tw ) and D 2 S for some S 2 Sw .
While the connections take care of the truth values of alocala formulas of the form D, quasimodels take care of the remaining aglobala temporal operators.
Proceedings of the Ninth International Symposium on Temporal Representation and Reasoning (TIMEa02) 1530-1311/02 $17.00 AS 2002 IEEE  S(I,; w) = ftp(I,; w; a) : a 2 Dg;  An induction on d shows that the number ](d ) of dw -classes is at most Iz(D)d  2(d +1)jsub(D)j (for any w).
For d = 0, one may check that if (I,; w) j= D iff (I,0 ; w) j= D for each sentence D 2 sub(D), then I, 0w I,0 .
So ](0)  2jsub(D)j .
Assume the result for d. One may check that if I,; I,0 2 B(w) contain a common immediate successor v of w, (I,; w) j= D iff (I,0 ; w) j= D for each sentence D 2 sub(D), and I, dv I,0 , then I, wd +1 I,0 .
Both checks involve an induction on D in (2) above.
It follows that ](d + 1)  Iz(D)  2jsub(D)j  ](d ), and hence that ](d )  Iz(D)d  2(d +1)jsub(D)j for all d, as required.
jsub(D)j 0 I, then I, w I,0 , so that w Finally observe that if I, w has at most ](jsub(D)j)  D(D) classes.
MI,w j  Let I,w1 ; : : : I,wnw be some minimal list of full branches such that f[I,w1 ]w ; : : : ; [I,wnw ]w g is the set of all w -equivalence classes.
With each a 2 D we associate a trace  T HEOREM 16.
D is satisfiable in a QPCTL1 -model iff there exists a quasimodel satisfying D. Proof.
()) Suppose that D is satisfied in some model.
We may replace its tree by + =   <DD, as in the proof of Lemma 6; <D D denotes the set of finite sequences of natural numbers.
Every branch of is aduplicateda D times in + at each node, and D is still satisfied in the resulting model = h + ; D; I i.
Thus ( ; D; v) j=a D for some v 2 T + , D 2 B(v) (defined with respect to + ) and some assignment .
If w 2 T + and I, 2 B(w), let  T T T T M T  M T a where  tp(I,; w; a) = fD 2 sub(D) : (  T  ;  ;  g  ) = D[a]  :  T  Let Sw = fS(I,; w) : I, 2 B(w)g. We extract from + a subtree 0 = hT 0 ; <0 i in which every node has at most sub(D) immediate successors.
To this end, we inIz(D) = 22 ductively define Tn0  T + with this property.
Set T00 = fw0 g, where w0 is the root of + .
Given Tn0 , for each w 2 Tn0 with ht (w) = n, and each S 2 Sw , we pick I,S 2 B(w) such that S(I,S ; w) = S, and (we use the form of + =   <D D here) I,S \ Tn0 = I,S \ I,S = ft 2 T + : t  wg for S distinct S; S0 2 Sw .
Let Bw = fI,S : S 2 Sw g, and Tw = Bw .
We can assume thatSD 2 Bw0 .
Note that jBw j  Iz(D).
Now 0 set Tn0S fTw : w 2 Tn0 ; ht (w) = ng.
Finally define +1 = Tn [ 0 0 T = n<D Tn .
Note that D  T 0 and v 2 T 0 .
Let 0 = h 0 ; D; I 0 i and 0 be the restrictions of and + to T 0 .
One can easily show by induction on the construction of D 2 sub(D) that ( ; I,; w) j=a D iff ( 0 ; I,; w) j=a D, for all full branches I, in 0 and all w 2 I,.
(For example, suppose ( ; I,; w) j=a ED.
Then there is I,0 2 B(w) in + such that ( ; I,0 ; w) j=a D. Pick a full branch Il in 0 for which S(I,0 ; w) = S(Il; w).
Since D is a sentence, we have ( ; Il; w) j=a D. It follows by IH that ( 0 ; Il; w) j=a D and so ( 0 ; I,; w) j=a ED.)
Thus 0 satisfies D and we can work with this model instead of .
Define an equivalence relation w on B(w) (defined in 0 now), for w 2 T 0 , by taking I, w I,0 when ( 0 ; I,; w) j=a D iff ( 0 ; I,0 ; w) j=a D, for every D 2 sub(D) and every assignment .
The w -equivalence class generated by I, will be denoted by [I,]w .
Since only  is applied to open formulas, we can show that the number of w -equivalence classes is bounded by D(D).
To show this, for w 2 T 0 , full branches I, and I,0 in B(w), and d < D, we put I, dw I,0 if for all t 2 T 0 with t  w and ht (t )  ht (w) + d, we have  T  j  j  T  T T  0  M T  T  M M  M T  M M  M  M M T  1. t 2 I, iff t 2 I,0 ,  T  M  M  M T  T  M a  a  2. if t 2 I,, then for all assignments and all D 2 sub(D) with at most ht (w) + d ht (t ) occurrences of , we have (I,; t ) j=a D iff (I,0 ; t ) j=a D.  Dwa : f1; : : : ; nw g !
[  Sw  by taking Dwa (i) = tp(I,wi ; w; a).
Denote the resulting set of traces by Tw .
Let Iw = (Sw ; Tw ) for all w 2 T 0 .
We are now in a position to define a quasimodel f over 0 satisfying D. If w is not the root, then set f (w) = ((Iv ; Iw ; Rw ; Nw ); gw ), where v is the immediate predecessor of w, and for root w0 let f (w0 ) = (Iw0 ; gw0 ), where  T      gw (I,) = i iff I, 2 [I,wi ]w ,  Rw = f(Dva ; Dwa ) : a 2 Dg,  Nw = f(gv (I,); gw (I,)) : I, 2 B(w)g.  It is not hard to check that f is a quasimodel satisfying D.  (() Now suppose that f is a quasimodel for D over T = hT i with root w0 .
Let f (w0 ) = (Iw gw ) and f (w) = (Cw gw ) = ((aw Iw Rw Nw ) gw ) for non-root w 2 T .
Let ;<  0;  ;  ;  ;  ;  0  ;  Iw = (Sw ; Tw ) and nw = nIw .
A run r in f is a function associating with any w 2 T a trace r(w) 2 Tw such that (r(v); r(w)) 2 Rw for any non-root w with immediate predecessor v. Using the condition that the range and domain of Rw coincide with f1; : : : ; nw g and f1; : : : ; nvg, respectively, it is not difficult to see that, for any w and any D 2 Tw , there exists a run r with r(w) = D. Let R be the set of all runs.
For every w 2 T we find a first-order structure I (w) with domain D = R realising Iw = (Sw ; Tw ) and such that for all i 2 f1; : : : ; nw g, r 2 D, and D 2 sub(D),  M T  D 2 r(w)(i)  iff I (w) j= Di [r]:  a  Let = h ; D; I i and let be any assignment in D. One can show by induction that for all D 2 sub(D), all w 2 T , and all I, 2 B(w) with gw (I,) = i, say, we have I (w) j=a Di  iff  (  M I, w j aD ;  ;  ) =  :  Since D 2 r(w)(gw (D)) for some w 2 T , D 2 B(w) and r 2 R , we finally obtain ( ; D; w) j= D.  Proceedings of the Ninth International Symposium on Temporal Representation and Reasoning (TIMEa02) 1530-1311/02 $17.00 AS 2002 IEEE  M  Now we construct a reduction of QPCTLw to non-local PCTL by means of encoding quasimodels in non-local propositional tree models.
Suppose again that a QPCTLw sentence D is fixed.
With every realisable state candidate I = (S ; T ) for D, every connection C, every i  D(D), and every sentence D 2 sub(D), which either is a propositional variable or starts with an existential quantifier 9x, we associate propositional variables pI , pC , pi , and pD , respectively.
Let ] be a translation from the set of QPCTLw -sentences into the set of PCTL -formulas which distributes over the booleans, temporal operators and path quantifiers, and D] = pD , where D is a propositional variable or a sentence of the form 9x D. Clearly, D] is a PCTL -formula.
Then the following formula D?
is effectively constructable from D:  tended to mean gw (I,) = i.
This is ensured by the formulas  A  ( i  j );  !
:  (5)  1i< j D(D)  A2F  ^    ^  I2R (D)  A2F  ( i  !
:  j );  (6)  1i< j D(D)  pI !
i^    1inI  pC !
i^  1inI  C2C (D)  (7)  ;  i  1inI  (8)  :  i  1inI  Here and below we assume that C = (a; I; R; N ).
Now we write down a formula which says that N in C is determined by the functions gw :  A2F    D?
= D] ^ (D ^ :3P>) _ 3P(D ^ :3P>)  ^ p p ^ p p ^ Ep A _ p  ^ Ep A _ p   ^    pC !
C2C (D)  ^  E( p j ^ P pi )  (i; j )2N  ;  ^  A  _    ( p j ^ P pi )  (9) :  (i; j )2N  where D is the conjunction of the formulas (1)a(11) defined below.
_ _R  I2  A2F    A pI ^  ^Ap ^Ap  A pC ^  C2C (D)  !
:  pI ); 0  0  ( C  (1)   !
:  pC ) 0  :  (2)  C6=C0  Here R (D) and C (D) are the sets of realisable state candidates and connections for D, respectively.
The formulas in (1) say that the pI and pC are alocala (so we can write w j= pI and w j= pC ) and that precisely one pI holds at the root and precisely one pC holds at each non-root point.
Intuitively, w j= pC means that f (w) = (C; g), for some g. Say that a pair of connections (C1 ; C2 ) is suitable if the second state candidate of C1 coincides with the first state candidate of C2 .
The set of all suitable pairs of connections is denoted by Cs (D).
A pair (I; C) is suitable if the first state candidate of C coincides with I.
The set of all suitable pairs of this form is denoted by R s (D).
The following formulas say that the pair induced by a point and its immediate predecessor is suitable:  A  _ _R  (I;C)2  A2F  ( pI ^  pC );  (3)  s (D)  ( pC1  ^ ^ ^D ^  ^ p R ^ p A2  include:  ( I  I6=I  (D)  Finally, we have to ensure that the set of sentences true at (I,; w) corresponds to the set of sentences in Iw (gw (I,)).
Let I0w (i) be the set of sentences in sub(D) n Iw (i).
Then we  ^  pC2 ):  (4)  (C1 ;C2 )2Cs (D)  Intuitively, for i such that 1  i  D(D), (I,; w) j= pi is in-  A  ( I ^ pi ) !
( D I2 (D);1inI D2I(i) ( C ^ pi ) !
( C2S (D);1inI D2I(i) F  ]  ]    D] )  D2I0 (i)  ^  ;  (10)  :  (11)  :  ^    D] )  :  D2I0 (i)  T HEOREM 17.
A QPCTLw -sentence D is satisfiable in a full model iff the PCTL -formula D?
is satisfiable in a full non-local model.
Proof.
()) If D is satisfiable, then it is satisfied in a quasimodel f for D based on an D-tree = hT ; <i.
Let f (w) = (Cw ; gw ) = ((aw ; Iw ; Nw ; Rw ); gw ) if w is not the root and f (w0 ) = (aw0 ; gw0 ) for root w0 of .
Define a (propositional) valuation h in by taking, for all w 2 T and I, 2 B(w):  T  T    (I,; w)    (I,; w) 2 h( pC )    (I,; w) 2 h( pi )    (I,; w)  T  2 h( pI ) iff I = Iw , for every realisable state candidate I;  iff C = Cw , for every connection C;  iff gw (I,) = i, for all i < D(D);  2 h( pD ) iff D 2 Iw (gw (I,)), for all sentences D in sub(D).
It is not hard to prove that the full model required.
M T T  M Th =  h  ;  i  is as  Conversely, suppose = h ; hi satisfies D?
.
Then D is true at the root w0 of .
Define a quasimodel f by taking f (w) = (Cw ; gw ) = ((aw ; Iw ; Nw ; Rw ); gw ) if w 6= w0 , and f (w0 ) = (Iw0 ; gw0 ), where (()  Proceedings of the Ninth International Symposium on Temporal Representation and Reasoning (TIMEa02) 1530-1311/02 $17.00 AS 2002 IEEE   Iw0 is the unique I for which w0 j= pI (this is independent of the branch of evaluation);  for w 6= w0 , Cw is the unique C such that w j= pC ;  gw (I,) = i for the unique i for which (I,; w) j= pi .
The reader can check that f is a quasimodel satisfying D.  4 Conclusion The decidability of the weak one-variable fragment of first-order CTL can be used to obtain decidability results for certain spatio-temporal logics based on CTL and the region connection calculus RCC-8 (see the survey papers [5, 9]).
From this viewpoint it has sufficient expressive power to be useful.
However, there is still a gap between the undecidability of the one-variable fragment of first-order CTL and the decidability of its weak one-variable fragment.
In particular, the following problems are still open.
1.
What happens if the path-quantifier open formulas as well?
E  is applied to  2.
Or, what happens if all temporal operators are applied to open formulas, but E only to sentences?
3.
Another open problem is the computational complexity of the logics considered above.
The reduction proofs presented in this paper provide only non-elementary decision procedures (simply because the complexity of S2S is non-elementary).
We do not believe that this is optimal.
[2] A. Chandra, J. Halpern, A. Meyer, and R. Parikh.
Equations between regular terms and an application to process logic.
In STOC, pages 384a390, 1981.
[3] E. Emerson and J. Halpern.
asometimesa and anot nevera revisited: on branching versus linear time.
Journal of the ACM, 33:151a178, 1986.
[4] I. Hodkinson, F. Wolter, and M. Zakharyaschev.
Decidable fragments of first-order temporal logics.
Annals of Pure and Applied Logic, 106:85a134, 2000.
[5] I. Hodkinson, F. Wolter, and M. Zakharyaschev.
Monodic fragments of first-order temporal logics: 2000a2001 a.d.
In R. Nieuwenhuis and A. Voronkov, editors, Logic for Programming, Artificial Intelligence and Reasoning, volume 2250 of LNAI, pages 1a23.
Springer-Verlag, 2001.
[6] I. Hodkinson, F. Wolter, and M. Zakharyaschev.
Decidable and undecidable fragments of first-order branching temporal logics.
In Proc.
Logic in Computer Science (LICSa02).
IEEE Computer Science Press, 2002 (in print).
(Draft available at http://www.dcs.kcl.ac.uk/staff/mz.)
[7] F. Laroussinie and P. Schnoebelen.
A hierarchy of temporal logics with past.
Volume 775 of LNCS, pages 47a58.
Springer-Verlag, 1994.
[8] M. O. Rabin.
Decidability of second order theories and automata on infinite trees.
Transactions of the American Mathematical Society, 141:1a35, 1969.
[9] F. Wolter and M. Zakharyaschev.
Qualitative spatio-temporal representation and reasoning: a computational perspective.
In Exploring Artificial Intelligence in the New Millenium.
Morgan Kaufmann Publishers, 2002 (in print).
[10] A. Zanardo.
A finite axiomatization of the set of strongly valid ockamist formulas.
Journal of Philosophical Logic, 14:447a468, 1985.
Acknowledgements The work of the second and forth authors was partially supported by UK EPSRC grant GR/R45369/01 aAnalysis and mechanisation of decidable first-order temporal logics;a the fourth author was also partially supported by UK EPSRC grant GR/R42474/01 aComplexity analysis of reasoning in combined knowledge representation systems.a The work of the third author was partially supported by Deutsche Forschungsgemeinschaft (DFG) grant Wo583/3-1.
We are grateful to Szabolcs MikulaEs for helpful discussions and Colin Stirling who attracted our attention to the non-local version of CTL .
Thanks are also due to the anonymous referees for useful suggestions and comments on the first version of this paper.
References [1] J. Burgess.
Logic and time.
Journal of Symbolic Logic, 44:566a582, 1979.
Proceedings of the Ninth International Symposium on Temporal Representation and Reasoning (TIMEa02) 1530-1311/02 $17.00 AS 2002 IEEE
A graph-theoretic approach to efficiently reason about partially ordered events in the Event Calculus Massimo Franceschet Angelo Montanari Dipartimento di Matematica e Informatica, Universita di Udine Via delle Scienze, 206 - 33100 Udine, Italy {francesc |montana }@dimi.uniud.it  Abstract In this paper, we exploit graph-theoretic techniques to efficiently reason about partially ordered events in the Event Calculus.
We replace the traditional generate-and-test reasoning strategy by a more efficient generate-only one that operates on the underlying directed acyclic graph of events representing ordering information by pairing breadth-first and depth-first visits in a suitable way.
We prove the soundness and completeness of the proposed strategy, and thoroughly analyze its computational complexity.
Furthermore, we show how it can be generalized to deal with the Modal Event Calculus, that provides a uniform modal framework for the basic Event Calculus and its skeptical and credulous variants.
1.
Introduction In this paper, we propose a graph-theoretic approach to the problem of efficiently reasoning about partially ordered events in Kowalski and Sergot's Event Calculus (EC for short) [5, 11] and in its skeptical and credulous modal variants [4].
Reasoning about the evolution of the world as the result of the occurrence of a set of events is crucial in a variety of applications, including diagnosis, robotics, agent modelling, qualitative physics, monitoring, planning and plan validation, and natural language understanding.
In many of these applications, a reasoner is forced to deal with incomplete knowledge about the events it is concerned with and/or their temporal order [9].
We consider the problem of efficiently inferring what is true over certain event-bounded time intervals when only incomplete knowledge is available.
Even though we develop our solution in the (Modal) Event Calculus framework, we expect it to be applicable to any formalism for reasoning about partially ordered events.
Partial ordering information about event occurrences can be naturally represented by means of a directed acyclic  graph G = hE, oi, where the set of nodes E is the set of events and, for every ei , ej [?]
E, there exists (ei , ej ) [?]
o if and only if it is known that ei occurs before ej .
EC updates are of additive nature only and they just consist in the addition of new events (G nodes) and/or of further (consistent) ordering information about the given events (G edges).
Given a directed acyclic graph G = hE, oi, representing a set of partially ordered events, EC allows one to compute the set of event-bounded maximal validity intervals (MVIs for short) over which the properties initiated and terminated by such events hold uninterruptedly.
To compute the set of MVIs for any given property p, it exploits a simple generate-and-test strategy [5]: first, it blindly picks up all pairs (ei , et ) of initiating and terminating events for p; then, it checks whether or not they occur in the proper ordering, that is, if the initiating event ei precedes the terminating event et ; finally, it looks for possible interrupting events e occurring in between.
Checking whether ei precedes et or not reduces to establish if the edge (ei , et ) belong to the transitive closure o+ of o as well as checking if there exists an interrupting event e for p in (ei , et ) requires to verify if both (ei , e) and (e, et ) belong to o+ .
In [6], Chittaro et alii have shown that the complexity of query processing based on this simple generate-and-test strategy is O(n5 ), provided that suitable graph marking techniques are used.
In this paper, we propose a more efficient generate-only strategy which reduces the computation of the set of MVIs for any given property p to a non-standard visit of the graph G. The idea of exploiting graph-theoretic techniques to speed up temporal reasoning about partially ordered events in EC was originally proposed by Chittaro et alii in [6].
They provide a precise characterization of what EC actually does to compute MVIs and show that, whenever all recorded events are concerned with the same unique property p, shifting the perspective from the transitive closure o+ of the given partial order o to its transitive reduction o- allows one to do it more efficiently using a generateonly strategy.
Their solution can be easily generalized to the case of multiple incompatible properties, that is, prop-  erties whose validity intervals cannot overlap.
In this paper, we will first show that such a solution cannot be further extended to deal with the general multiple-property case, because it does not properly work whenever there exist two or more non-transitive paths of different length between an ordered pair of events that respectively initiate and terminate a given property.
Then, we will describe an alternative generate-only strategy for MVIs computation in the general multiple-property case, which pairs breadth-first and depthfirst visits of o in a suitable way, and thoroughly analyze its complexity.
As pointed out in [5], however, when only partial information about the occurred events and their exact order is available, the sets of MVIs derived by EC bear little relevance, since the acquisition of additional knowledge about the set of events and/or their occurrence times might both dismiss current MVIs and validate new MVIs.
To overcome these limitations, two variants of the basic EC, respectively called the Skeptical EC (SKEC) and the Credulous EC (CREC), have been proposed in [7].
For any given property p, SKEC computes the set of necessarily true MVIs, that is, the set of MVIs which are derivable in all refinements of the given partial order, while CREC computes the set of possibly true MVIs, that is, the set of MVIs which are derivable in at least one refinement of the given partial order.
In [2], Cervesato et alii defined a uniform modal interpretation for EC, SKEC, and CREC, called the Modal Event Calculus (MEC), and extended the generate-and-test strategy for MVIs computation in EC to MEC, without any rise in computational complexity [3].
In the last part of the paper, we will show that the proposed generate-only strategy for MVIs computation in EC can be easily tailored to MEC.
The paper is organized as follows.
In Section 2, we recall some background knowledge on ordering relations, transitive reduction, and transitive closure.
In Section 3, we present the basic features and properties of EC and MEC, and point out the limitations of the existing algorithms for MVIs computation when only partial ordering information is available.
In Section 4, we describe a new generate-only algorithm for MVIs computation and prove its soundness and completeness.
The increase in efficiency of the proposed solution is demonstrated by the complexity analysis reported in Section 5.
In Section 6, we show how to adapt the proposed algorithm to MEC.
In the conclusions, we briefly discuss the achieved results and outline possible directions for future research.
2.
On ordering relations, transitive reduction, and transitive closure Let us first remind some basic notions about ordering relations and ordered sets, transitive closure, and transitive reduction upon which we will rely in the following [12].
EC usually represents ordering information as a binary acyclic relation on the set of events, that is, as an ordering relation possibly missing some transitive links, but it uses ordering information as a (strict) partial order that can be recovered as the transitive closure of the given binary acyclic relation.
Definition 2.1 (DAGs, strictly ordered sets, non-strictly ordered sets, generated DAGs, induced DAGs) Let E be a set and o a binary relation on E. o is called a (strict) partial order if it is irreflexive and transitive (and, thus, asymmetric), while it is called a reflexive partial order if it is reflexive, antisymmetric, and transitive.
The pair (E, o) is called a directed acyclic graph (DAG) if o is a binary acyclic relation; a strictly ordered set if o is a partial order; a non-strictly ordered set if o is a reflexive partial order.
Moreover, given a DAG G = hE, oi and a node e [?]
E, the subgraph G(e) of G consisting of all and only the nodes which are accessible from e and of the edges that connect them is called the graph generated by e. Finally, given a DAG G = hE, oi and a set T [?]
E, the subgraph of G induced by T consists of the nodes in T and the subset of edges in o that connect them.
We will denote the sets of all binary acyclic relations and of all partial orders on E as OE and WE , respectively.
It is easy to show that, for any set E, WE [?]
OE .
Moreover, we will use the letters o and w, possibly subscripted, to denote binary acyclic relations and partial orders, respectively.
When one is mainly interested in representing the path information of a DAG, two extreme approaches can be followed: (i) transitive reduction (or minimum storage representation), and (ii) transitive closure (or minimum query time representation).
Transitive reduction and closure of a DAG can be formally defined as follows.
Definition 2.2 (Transitive reduction and closure of DAGs) Let G = hE, oi be a directed acyclic graph.
The transitive reduction of G is the (unique) graph G- = hE, o- i with the smallest number of edges, with the property that, for any pair of nodes i, j [?]
E, there is a directed path from i to j in G if and only if there is a directed path from i to j in G- .
The transitive closure of G is the (unique) graph G+ = hE, o+ i with the property that, for any pair of nodes i, j [?]
E there is a directed path i to j in G if and only if there is an edge (i, j) [?]
o+ in G+ .
In [1], Aho et alii show that every (directed) graph has a transitive reduction, which can be computed in polynomial time.
They also show that such a reduction is unique in the case of directed acyclic graphs.
Furthermore, they prove that the time needed to compute the transitive reduction of a  graph differs from the time needed to compute its transitive closure by at most a constant factor.
MVIs computation requires the derivation of the transitive closure of the given partial order.
Clearly, if (E, o) is a DAG, then (E, o+ ) is a strictly ordered set.
We say that two binary acyclic relations o1 , o2 [?]
OE are equally infor+ mative if o+ 1 = o2 .
This induces an equivalence relation ~ on OE .
It is easy to prove that, for any set E, the quotient set OE / ~ and WE are isomorphic.
In the following, we will often identify a binary acyclic relation o with the corresponding element o+ of WE .
3.
Basic and Modal Event Calculi In this section, we first recall the syntax and semantics of EC and MEC; then, we discuss the effects of the addition of new events and/or pieces of ordering information on the sets of MVIs computed by EC and MEC; finally, we briefly review the existing algorithms for MVIs computation.
3.1.
Syntax and semantics of EC and MEC Kowalski and Sergot's Event Calculus (EC) [11] aims at modeling situations that consist of a set of events, whose occurrences over time have the effect of initiating or terminating the validity of properties, some of which may be mutually exclusive.
We formalize the time-independent aspects of a situation by means of an EC-structure, which is defined as follows [4].
Definition 3.1 (EC-structure) A structure for the Event Calculus (abbreviated ECstructure) is a quintuple H = (E, P, [*i, h*], ]*,*[) such that: * E = {e1 , .
.
.
, en } and P = {p1 , .
.
.
, pm } are finite sets of events and properties, respectively; * [*i : P - 2E and h*] : P - 2E are respectively the initiating and terminating map of H. For every property p [?]
P , [pi and hp] represent the set of events that initiate and terminate p, respectively; * ]*,*[[?]
P x P is an irreflexive and symmetric relation, called the exclusivity relation, that models exclusivity among properties.
Unlike the original presentation of EC [11], we focus our attention on situations where the occurrence time of events is unknown.
Indeed, we assume that incomplete information about the relative order in which events occur is available.
We however require temporal data to be consistent so that an event cannot both precede and follow any other event.
We formalize the time-dependent aspects of an EC problem by specifying a partial order, called knowledge state, on the set of events E [4].
Given a structure H, we adopt as the query language of EC the set: L(EC) = {p(e1 , e2 ) : p [?]
P and e1 , e2 [?]
E} of all property-labeled intervals over H. Given a knowledge state w, a maximal validity interval (MVI) for a property p with respect to w is an interval of w over which the property p holds uninterruptedly.
We represent an MVI for p as p(ei , et ), where ei and et are the events that initiate and terminate the interval over which p maximally holds, respectively.
The task performed by EC reduces to deciding which of the elements of L(EC) are MVIs and which are not, with respect to the current partial order of events.
We interpret the elements of L(EC) relative to the set WE (denoted WH in this context) of partial orders among events in E. In order for p(e1 , e2 ) to be an MVI relative to the knowledge state w, (e1 , e2 ) must be an interval in w, i.e.
e1 <w e2 .
Moreover, e1 and e2 must witness the validity of the property p at the ends of this interval by initiating and terminating p, respectively.
These requirements are enforced by conditions i., ii.
and iii., respectively, in the definition of valuation given below.
The maximality requirement is caught by the negation of the meta-predicate br(p, e1 , e2 , w) in condition iv., which expresses the fact that the truth of an MVI must not be broken by any interrupting event.
Any event e which is known to have happened between e1 and e2 in w and that initiates or terminates a property that is either p itself or a property exclusive with p interrupts the truth of p(e1 , e2 ).
These observations are formalized as follows [4].
Definition 3.2 (Intended model of EC) Let H = (E, P, [*i, h*], ]*,*[) be a EC-structure.
The intended EC-model of H is the propositional valuation uH : WH - 2L(EC) , where uH is defined in such a way that p(e1 , e2 ) [?]
uH (w) if and only if i. e1 <w e2 ; ii.
e1 [?]
[pi; iii.
e2 [?]
hp]; iv.
br(p, e1 , e2 , w) does not hold, where br(p, e1 , e2 , w) abbreviates there exists an event e [?]
E such that e1 <w e, e <w e2 , and there exists a property q [?]
P such that e [?]
[qi or e [?]
hq], and either ]p, q[ or p = q.
As a general rule, an event e interrupts the validity of a property p if it initiates or terminates p itself or a property q which is incompatible with p. This rule adopts the  so-called strong interpretation of the initiate and terminate relations: given a pair of events ei and et , with ei occurring before et , that respectively initiate and terminate a property p, we conclude that p does not hold over (ei , et ) if an event e which initiates or terminates p, or a property incompatible with p, occurs during this interval, that is, (ei , et ) is a candidate MVI for p, but e forces us to reject it.
The strong interpretation is needed when dealing with incomplete sequences of events or incomplete information about their ordering.
An alternative interpretation of the initiate and terminate relations, called weak interpretation, is also possible.
According to such an interpretation, a property p is initiated by an initiating event unless it has been already initiated and not yet terminated (and dually for terminating events).
Further details about the strong/weak distinction can be found in [4].
In the case of partially ordered events, the set of MVIs derived by EC is not stable with respect to the acquisition of new ordering information.
Indeed, if we extend the current partial order with new pairs of events, current MVIs might become invalid and new MVIs can emerge.
The Modal Event Calculus (MEC) allows one to identify the set of MVIs that cannot be invalidated no matter how the ordering information is updated, as far as it remains consistent, and the set of event pairs that will possibly become MVIs depending on which ordering data are acquired.
These two sets are called necessary MVIs and possible MVIs, respectively, using 2-MVIs and 3-MVIs as abbreviations.
The query language L(MEC) of MEC consists of formulas of the form p(e1 , e2 ), 2p(e1 , e2 ) and 3p(e1 , e2 ), for every property p and events e1 and e2 defined in H. The intended model of MEC is given by shifting the focus from the current knowledge state to all knowledge states that are accessible from it.
Since [?]
is a reflexive partial order, (WH , [?])
can be naturally viewed as a finite, reflexive, transitive and antisymmetric modal frame.
This frame, together with the straightforward modal extension of the valuation uH to the transitive closure of an arbitrary knowledge state, provides a modal model for MEC.
Given an EC-structure H and a partial order w, the sets of MVIs that are necessarily and possibly true in w correspond respectively to the 2- and 3-moded atomic formulas which are valid in H with respect to w. We define the sets M V I(H, w), 2M V I(H, w) and 3M V I(H, w) of respectively MVIs, necessary MVIs and possible MVIs which a true in H with respect to w as follows: M V I(H, w) = {p(e1 , e2 ) : IH ; w |= p(e1 , e2 )}; 2M V I(H, w) = {p(e1 , e2 ) : IH ; w |= 2p(e1 , e2 )}; 3M V I(H, w) = {p(e1 , e2 ) : IH ; w |= 3p(e1 , e2 )}.
In [2], it has been shown that the sets of 2- and 3-MVIs can be computed by exploiting necessary and sufficient local conditions over w, thus avoiding a complete (and expensive) search of all the consistent refinements of w. More precisely, a property p necessarily holds between two events e1 and e2 if and only if the interval (e1 , e2 ) belongs to the current order, e1 initiates p, e2 terminates p, and no event that either initiates or terminates p (or a property incompatible with p) will ever be consistently located between e1 and e2 .
Similarly, a property p may possibly hold between e1 and e2 if and only if the interval (e1 , e2 ) is consistent with the current ordering, e1 initiates p, e2 terminates p, and there are no already known interrupting events between e1 and e2 .
This is precisely expressed by the following proposition.
Proposition 3.4 (Local conditions) Let H = (E, P, [*i, h*], ]*,*[) be a EC-structure.
For any atomic formula p(e1 , e2 ) on H and any w [?]
WH , * IH ; w |= 2p(e1 , e2 ) if and only if i. ii.
iii.
iv.
IH ; w |= p(e1 , e2 ) IH ; w |= 2p(e1 , e2 )  iff iff  IH ; w |= 3p(e1 , e2 )  iff  p(e1 , e2 ) [?]
uH (w); [?
]w0 .
w0 [?]
WH [?]
w [?]
w0 , = IH ; w0 |= p(e1 , e2 ); [?
]w0 .
w0 [?]
WH [?]
w [?]
w0 [?]
IH ; w0 |= p(e1 , e2 ).
hold,  where  there exists an event e [?]
E such that (e, e1 ) 6[?]
w, e 6= e1 , (e2 , e) 6[?]
w, e 6= e2 , and there exists a property q [?]
P such that e [?]
[qi or e [?]
hq], and either ]p, q[ or p = q.
Definition 3.3 (Intended model of MEC) Let H, WH , and uH be defined as in Definition 3.2.
The MEC-frame FH of H is the frame (WH , [?]).
The intended MEC-model of H is the modal model IH = (WH , [?
], uH ).
Given w [?]
WH and ph [?]
L(MEC), the truth of ph at w with respect to IH , denoted by IH ; w |= ph, is defined as follows:  (e1 , e2 ) [?]
w; e1 [?]
[pi; e2 [?]
hp]; sbr(p, e1 , e2 , w) does not sbr(p, e1 , e2 , w) abbreviates  * IH ; w |= 3p(e1 , e2 ) if and only if i. ii.
iii.
iv.
(e2 , e1 ) 6[?]
w; e1 [?]
[pi; e2 [?]
hp]; br(p, e1 , e2 , w) does not hold.
Proposition 3.4 allows us to give an alternative definition of the sets 2M V I(H, w) and 3M V I(H, w).
Given  w [?]
WH and p [?]
P , let S(H, w) be the set of atomic formulas p(e1 , e2 ) such that all other events in E that initiate or terminate p, or a property incompatible with p, are ordered with respect to e1 and e2 in w, and let C(H, w) be the set of atomic formulas p(e1 , e2 ) such that e1 initiates p, e2 terminates p, and e1 and e2 are unordered in w. The set 2M V I(H, w) (resp.
3M V I(H, w)) can be alternatively defined as the intersection (resp.
union) of the set M V I(H, w) with S(H, w) (resp.
C(H, w)), as stated by the following corollary.
Corollary 3.5 Let H = (E, P, [*i, h*], ]*,*[) be an ECstructure and w [?]
WH be a partial order.
It holds that: 2M V I(H, w) = M V I(H, w) [?]
S(H, w); 3M V I(H, w) = M V I(H, w) [?]
C(H, w).
In Section 6, we will exploit Corollary 3.5 to devise an algorithm for MVIs computation in MEC.
Furthermore, from Corollary 3.5 it is immediate to conclude that the sets of necessary MVIs, MVIs, and possible MVIs with respect to the current state of knowledge form an inclusion chain as formally stated by the following proposition.
Proposition 3.6 (Necessary MVIs and possible MVIs enclose MVIs) Let H = (E, P, [*i, h*], ]*,*[) be an EC-structure and w [?]
WH be a partial order.
It holds that 2M V I(H, w) [?]
M V I(H, w) [?]
3M V I(H, w).
Notice that if w is a total order, then S(H, w) = L(EC) and C(H, w) = [?
], and thus 2M V I(H, w) = 3M V I(H, w) = M V I(H, w).
to conclude that the function M V I(H, *) is nonmonotonic with respect to the evolution of the ordering information.
On the contrary, S(H, *) and C(H, *) possess a monotonic behavior: the set S(H, *) grows monotonically as the current ordering information is refined, while the set C(H, *) shrinks monotonically.
However, even though M V I(H, *) has a nonmonotonic behaviour, it is possible to show that its intersection (resp.
union) with S(H, *) (resp.
C(H, *)) does not shrink (resp.
grow) when the current partial order is updated with new consistent pairs of events.
We first prove that for any pair w, w0 [?]
W , with w [?]
w0 , M V I(H, w) [?]
S(H, w) [?]
M V I(H, w0 ) [?]
S(H, w0 ).
To this end, it suffices to prove that if p(e1 , e2 ) [?]
M V I(H, w) \ M V I(H, w0 ), then p(e1 , e2 ) 6[?]
S(H, w).
From p(e1 , e2 ) [?]
M V I(H, w) and p(e1 , e2 ) 6[?]
M V I(H, w0 ), it follows that moving from w to w0 transforms a previously innocuous event e into an interrupting event for p(e1 , e2 ).
This means that the event e affects either p or a property incompatible with p and e is located between e1 and e2 in w0 , while it is unordered with respect to e1 or e2 in w. By the definition of S(H, *), this allows us to conclude that p(e1 , e2 ) 6[?]
S(H, w).
In a similar way, we can prove that M V I(H, w0 ) [?]
C(H, w0 ) [?]
M V I(H, w) [?]
C(H, w).
To this end, it suffices to prove that if p(e1 , e2 ) [?]
M V I(H, w0 ) \ M V I(H, w), then p(e1 , e2 ) [?]
C(H, w).
From p(e1 , e2 ) [?]
M V I(H, w0 ) and p(e1 , e2 ) 6[?]
M V I(H, w), it follows that moving from w to w0 creates a new MVI p(e1 , e2 ) by connecting an event e1 , that initiates p, to an event e2 , that terminates p. This means that the events e1 and e2 , that respectively initiate and terminate p, are ordered in w0 and unordered in w, and thus, by the definition of C(H, *), p(e1 , e2 ) [?]
C(H, w).
Exploiting Corollary 3.5, this allows us to prove the following proposition.
3.2.
MVIs computation and updates  Proposition 3.7 (Monotonicity of necessary and possible MVIs w.r.t.
the addition of further ordering information)  In this section we discuss the problem of determining how the acquisition of further information about the set of event occurrences and/or their occurrence times may affect the behaviour of EC and MEC.
We first discuss updates of ordering information; then, we change the perspective and analyze the effects of acquiring new event occurrences.
Given an EC-structure H, we want to study the behaviour of the sets of true, necessarily true and possibly true MVIs with respect to the acquisition of new ordering information [4].
When the arrival of a new piece of ordering information causes a transition into a more refined state of knowledge, the current set of MVIs may vary in two different ways.
On the one hand, the update may create a new MVI by connecting an event e1 , initiating a property p, to an event e2 terminating p. On the other hand, a new link can transform a previously innocuous event e into an interrupting event for a current MVI p(e1 , e2 ).
This allows us  Let H = (E, P, [*i, h*], ]*,*[) be an EC-structure and w, w0 [?]
WH be two partial orders.
It holds that: a. if w [?]
w0 , then 2M V I(H, w) [?]
2M V I(H, w0 ); b. if w [?]
w0 , then 3M V I(H, w0 ) [?]
3M V I(H, w).
By combining Propositions 3.6 and 3.7, we have that 2M V I(H, *) and 3M V I(H, *) constrain the variability of the set of MVIs derivable using EC.
The state of minimum information corresponds to the absence of any ordering data: 2M V I(H, *) and M V I(H, *) derive no formula, while 3M V I(H, *) derives all consistent property-labeled intervals.
As new ordering information arrives, 2M V I(H, *) increases, 3M V I(H, *) decreases, but M V I(H, *) always sits somewhere between them.
When enough ordering information has been entered  (at the limit, when the set of events has been completely ordered) 2M V I(H, *) and 3M V I(H, *) meet at a common value, constraining M V I(H, *) to assume that same value.
We now consider the evolution of the sets of MVIs, necessarily true MVIs, and possibly true MVIs in the case in which the knowledge state w remains unchanged and the EC-structure H is refined thanks to the acquisition of new event occurrences.
Even though the addition of a new event occurrence always causes a transition into a richer EC-structure, the set of true MVIs remains stable, since no ordering information about the entered event occurrence is added.
On the contrary, the set S(*, w) can only shrink as new events arrive, while the set C(*, w) grows monotonically.
Taking advantage of Corollary 3.5, we can immediately prove the following proposition.
Proposition 3.8 (Monotonicity of 2- and 3-MVIs w.r.t.
the addition of new event occurrences) Let H = (E, P, [*i, h*], ]*,*[) and H0 = (E , P, [*i0 , h*]0 , ]*,*[) be two EC-structures, such that E [?]
E 0 and [*i0 and h*]0 respectively extend [*i and h*] to model the effects of the events in E 0 \ E on the properties in P , and let w be a partial order.
It holds that: 0  Update  e3  e2  EC  SKEC  a(e1 , e4 )  a(e1 , e4 )  a(e1 , e4 )  a(e1 , e4 )  O  a(e1 , e4 ), o(e3 , e4 )  O  a(e1 , e2 ), a(e1 , e4 ), o(e3 , e2 ), o(e3 , e4 )  a(e1 , e4 )  CREC  (e1 , e2 )  a(e1 , e4 )  O  a(e1 , e2 ), a(e1 , e4 ), o(e3 , e2 ), o(e3 , e4 )  (e2 , e4 )  a(e1 , e2 )  O  a(e1 , e2 ), o(e3 , e2 ) o(e3 , e4 )  (e2 , e3 )  a(e1 , e2 )  a(e1 , e2 )  a(e1 , e2 ), o(e3 , e4 )  (e3 , e4 )  a(e1 , e2 ), o(e3 , e4 )  a(e1 , e2 ), o(e3 , e4 )  a(e1 , e2 ), o(e3 , e4 )  Figure 3.1.
The Beverage Dispenser Example.
a.
2Ph(H0 , w) [?]
2Ph(H, w); b.
3Ph(H, w) [?]
3Ph(H0 , w).
It is worth noting that, whenever we allow the addition of both ordering information and new event occurrences, it is impossible to identify any general rule constraining the behaviour of M V I(*, *), 3M V I(*, *), and 2M V I(*, *).
Example 3.9 (Beverage dispenser) We illustrate the relationships between MVIs computation in EC and MEC and updates by means of a simple example.
We want to model the operations of a simple beverage dispenser [4].
It can output either apple juice or orange juice (but not both simultaneously).
The choice is made by means of a selector with three positions (apple, orange and stop): by setting the selector to the apple or to the orange position, apple juice or orange juice is obtained, respectively; choosing the stop position terminates the production of juice.
In our example, we distinguish three types of events corresponding to the various settings of the selector and two relevant properties, supplyApple and supplyOrange, indicating that apple juice or orange juice is being dispensed, respectively.
The event of setting the selector to the apple (orange ) position initiates the property supplyApple (supplyOrange ), while setting it to the stop position terminates both properties.
The properties supplyApple and supplyOrange are exclusive since apple juice and orange juice cannot be output simultaneously.
We consider a scenario consisting of an event e1 , that initiates the property supplyApple, and a stop event e4 , that terminates both supplyApple and supplyOrange.
Furthermore, we assume that e1 precedes e4 .
This scenario can be formalized as follows.
E = {e1 , e4 }; P = {supplyApple, supplyOrange}; [supplyApplei = {e1 }; hsupplyApple] = hsupplyOrange] = {e4 }; ]supplyApple, supplyOrange[.
We describe the evolution of the sets of true MVIs, necessarily true MVIs, and possibly true MVIs when the following sequence of database updates is performed: (i) an event e3 , that initiates the property supplyOrange, is added; (ii) an event e2 , that terminates both properties, is inserted; (iii) the following sequence of ordered pairs of events is entered: (e1 , e2 ), (e2 , e4 ), (e2 , e3 ), and (e3 , e4 ).
In Figure 3.1, we describe the effects of each update on these three sets.
The first row of the table reports their initial values; each subsequent row is associated with an update to the database and filled in with the corresponding values of the three sets.
The first column shows the performed update; the second column contains the MVIs derived by EC, where a(ei , et ) (resp.
o(ei , et )) is a shorthand for the statement that property supplyApple (resp.
supplyOrange) holds between ei and et ; the third and fourth columns contain the sets of nec-  essary and possible MVIs, respectively.
The fact that, when the pair (e2 , e4 ) is entered, the MVI a(e1 , e4 ) disappears, while a new MVI a(e1 , e2 ) is added, provides an example of the nonmonotonic behavior of M V I(*, *) with respect to the addition of ordering information.
As for the monotonic behavior of 2M V I(*, *) and 3M V I(*, *), we can observe that the set of necessary MVIs grows (resp.
shrinks) when new ordered pairs of events (resp.
event occurrences) are acquired, while the set of possibly MVIs shrinks (resp.
grows) as new ordering information (resp.
information about event occurrences) is added.
Finally, observe that the set of MVIs always lies somewhere between 2M V I(*, *) and 3M V I(*, *), and, when the ordering information is complete, the three sets meet at a common value.
3.3.
Existing algorithms for MVIs computation Given an EC-structure H and a knowledge state w, the set of MVIs for a given property p, with respect to H and w, can be computed according to two alternative temporal reasoning strategies: a generate-and-test strategy and a generate-only one.
The generate-and-test strategy first generates all ordered pairs of initiating and terminating events for p, and then, for every pair, it verifies whether there are known interrupting events in between or not.
On the contrary, the generate-only strategy identifies possible interrupting events during the search of candidate MVIs for p, i.e.
pairs (e1 , e2 ) such that e1 initiates p and e2 terminates p. Generate-only strategies generally leads to the development of algorithms for MVIs computation with a lower worst-case complexity.
Traditional algorithms for MVIs computations adopt the simpler generate-and-test strategy, which can be easily derived from the specification of EC semantics given in Definition 3.2 [5, 11] (this strategy can be easily adapted to MEC by exploiting the local conditions given in Proposition 3.4 [2, 11]).
In order to compute all MVIs p(e1 , e2 ), with respect to w, such algorithms first generate all consistent intervals (e1 , e2 ) such that e1 initiates p, e2 terminates p, and e1 <w e2 ; then, they check whether or not the validity of p is broken during the interval (e1 , e2 ).
Such algorithms can be easily proved to be sound and complete with respect to the semantics of EC, but they are quite expensive: they operate in O(n5 ) time, where n is the number of events [3, 6].
A generate-only algorithm for MVIs computation can be found in [6].
Such an algorithm operates on the transitive reduction of the given partial ordering, which needs to be updated (paying a non-constant cost) whenever further ordering information is entered in the database.
The behavior of this algorithm can be described as follows: for any given property p and any event e1 initiating p, the algorithm ex-  amines all events accessible from e1 , searching for events terminating p. The search starts from the successors of e1 , and proceeds breadth-first.
The nodes which are directly accessible from e1 (nodes that belong to the first layer) can be partitioned into two categories: interrupting events, that is, events that affect either p or a property incompatible with p, and independent events, that is, events that affect neither p nor properties incompatible with p. Events belonging to the first category, which terminate p, contribute to the set of MVIs for p initiated by e1 , and are returned to the user; moreover, nodes which are reachable from them are marked, since there is no need to keep them into consideration during further processing.
The remaining nodes belonging to the first category are marked, together with their direct and indirect successors, because they cannot belong to a successful path for the user query.
Nodes belonging to the second category are used to determine the next layer to explore, which consists of the collection of all their unmarked successors.
The procedure repeats recursively these steps until the last layer is reached.
It is possible to show that this strategy is sound and complete whenever every property is incompatible with all the other ones.
In particular, it is sound and complete whenever the set of properties P is a singleton set (the singleproperty case studied in [6]).
In such a restricted context, MVIs computation can actually be simplified.
Whenever all event occurrences affect the same property p, any interval (e1 , e2 ) is an MVI for p if and only if e1 initiates p, e2 terminates p, and e1 is an immediate predecessor of e2 , that is, there are no recorded events in between, with respect to the transitive reduction of the given partial ordering.
Unfortunately, in the general case, in spite of the conjecture formulated in [6], such an algorithm is complete, but not sound.
A simple counterexample will be provided in Section 4.
In the next section, we propose an efficient, sound and complete generate-only algorithm for the MVIs computation problem, which successfully pairs breadth-first and depth-first visits of the graph representing the given partial order of events.
4.
A sound and complete generate-only algorithm for MVIs computation In this section, we describe a new generate-only algorithm that computes the set of MVIs which are true with respect to a partial order w and an EC-structure H. We provide a high-level description of the algorithm and prove its soundness and completeness with respect to the semantics of EC.
Let H = (E, P, [*i, h*], ]*,*[) be an EC-structure and o [?]
OE (denoted by OH hereinafter) be an acyclic binary relation.
We define an algorithm for MVIs computation that combines a breadth-first and a depth-first visit of the graph  (E, o), which is directed and acyclic, but not necessarily connected (background knowledge on elementary graph algorithms can be found in [8]).
In the following, whenever it does not lead to ambiguities, we denote the graph (E, o) by G and the subgraph of (E, o) generated by e by G(e).
The algorithm behaves as follows: for every property p [?]
P and every event e1 [?]
E initiating p, it searches the graph G(e1 ) for all events e2 such that the interval (e1 , e2 ) is an MVI for p. Given a property p and an event e1 , the algorithm associates the following labels with the nodes of G(e1 ): * unmarked: it denotes nodes (events) to be visited; * visited: it denotes nodes (events) already visited; * marked: it denotes nodes (events) that initiate or terminate either p or a property incompatible with p; * cutoff: it denotes nodes (events) which are cut off from the search space, because they cannot terminate any MVI for p initiated by e1 .
The set of events e2 such that p(e1 , e2 ) is an MVI is computed as follows.
Initially, all nodes in G(e1 ) are labeled with unmarked; then, the graph G(e1 ) is visited breadthfirst.
The breadth-first visit of G(e1 ) starts from the successors of e1 (first layer) and proceeds, layer by layer, until the last layer is reached.
The last layer is a layer followed by an empty layer; since G(e1 ) is acyclic, such a layer always exists and it is unique.
At each layer, only unmarked events are processed.
Let e be an unmarked event belonging to the current layer.
The algorithm labels e as visited and checks whether or not it initiates or terminates either p or a property incompatible with p. If the outcome of the test is positive, then the following operations are executed before processing the next event of the layer: 1. e is labeled as marked; 2. the label cutoff is assigned to all nodes of G(e) different from e; 3. if e terminates p, then the node e is saved.
Once the whole graph G(e1 ) has been visited, all the saved nodes, which are still labeled as marked, are returned; they are all and only the events that terminate an MVI for p initiated by e1 .
A pseudo-code description of such an algorithm for MVIs computation can be given as follows.
MV I - [?]
for each p [?]
P do for each e1 [?]
[pi do S-[?]
for each e [?]
G(e1 ) do set(e, unmarked)  L - nextlayer({e1 }) while L 6= [?]
do for each e [?]
L do if is relevant to(e, p) then set(e, marked) cutoff(e) if e [?]
hp] then S - S [?]
{e} L - nextlayer(L) for each e2 [?]
S do if label(e2 , marked) then M V I - M V I [?]
{p(e1 , e2 )} return M V I The procedure set(e,l) assigns the label l to the event e, the boolean function label(e,l) checks whether the label l is associated with the event e or not, and the boolean function is relevant to(e,p) tests whether or not e initiates or terminates either p or a property incompatible with p. The procedure nextlayer(L) computes the next layer in the breadth-first visit of the graph G(e1 ): nextlayer(L) L0 - [?]
for each e [?]
L do if label(e, unmarked) or label(e, visited) then for each successor e0 of e do if label(e0 , unmarked) then set(e0 , visited) L0 - L0 [?]
{e0 } return L' Finally, the procedure cutoff(l) visits depth-first the subgraph generated by the event e and labels as cutoff all its nodes: cutoff(e) for each successor e0 of e do if not label(e0 , cutoff) then set(e0 , cutoff) cutoff(e0 ) Before proving that such an algorithm in sound and complete with respect to the semantics of EC, we illustrate its behaviour by means of two simple examples.
Let e1 , e2 , and e3 be three event occurrences, p and q be two incompatible properties, and o = {(e1 , e2 ), (e1 , e3 ), (e2 , e3 )} be the current knowledge state (cf.
Figure 4.2, left side).
Suppose that e1 initiates p, e2 initiates q, and e3 terminates p. The set of MVIs for p, which are initiated by e1 , is computed as follows.
The algorithm first labels as unmarked all nodes of G(e1 ), and then it visits breadth-first G(e1 ).
The first layer contains both e2 and e3 .
Suppose that the algorithm first processes e3 and then e2 .
The node e3 is labeled as marked and saved, because it terminates p. The  e2 e1  Theorem 4.1 The proposed generate-only algorithm is sound and complete.
e6  e3 e1 e2  e3  e4  e5  Figure 4.2.
Two graphs representing ordering relations  propagation of the label cutoff has no effect, since e3 has no successors.
Hence, the node e2 is processed and labeled as marked, because it initiates a property q which is incompatible with p. The effect of propagating the label cutoff is that of replacing the label marked of e3 by the label cutoff.
Then, the visit of G(e1 ) terminates (all nodes have already been visited) and the algorithm returns no MVIs for p initiated by e1 , because the label associated with e2 (the only saved event) is cutoff and not marked.
This example should clarify the role of the label cutoff: some events may be labelled as marked along a "short" path (e1 - e3 in the example) and saved as candidate ending points of an MVI for the considered property.
However, an interval is an MVI for a property if and only if all paths leading from the initiating event to the terminating one do not contain interrupting events, that is, events that initiate or terminate either the considered property or a property incompatible with it.
If there exists a longer path (e1 - e2 - e3 in the example) which contains an interrupting event, then the candidate node is cut off during the propagation of the label cutoff.
It is worth noticing that the event graph of the example contains a transitive edge (e1 - e3 ).
The next example shows that cutoff labels are needed also for reasoning about event graphs devoid of transitive edges (their use can be avoided only if we restricted ourselves to multiple incompatible properties).
Consider a scenario consisting of six event occurrences e1 , e2 , e3 , e4 , e5 , and e6 , two incompatible properties p and q, and the knowledge state o depicted in Figure 4.2, right side, which has no transitive edges.
Suppose that e1 initiates p, e5 initiates q, e6 terminates p, and e2 , e3 , and e4 affect neither p nor a property incompatible with p. The interval (e1 , e6 ) is not an MVI for p, because there exists an interrupting event, namely e5 , which occurs between e1 and e6 .
The algorithm removes the node e6 from the set of candidate terminating events associated with initiating event e1 when it propagates the label cutoff during the processing of e5 .
The following theorem proves that the proposed algorithm computes exactly the set of MVIs as defined in Definition 3.2.
Proof.
We first prove that the algorithm is sound, that is, if p(e1 , e2 ) is generated by the algorithm, then p(e1 , e2 ) is an MVI.
Given a property p and an event e1 , the algorithm searches the acyclic graph G(e1 ) for terminating events e2 .
Since the visit is breadth-first, each node is reached along the shortest path on G(e1 ) starting from e1 .
Given a node e, we denote by D(e) the length of the shortest path on G(e1 ) connecting e1 to e. We show that p(e1 , e2 ) is an MVI if and only if e1 initiates p, e2 terminates p, e2 belongs to G(e1 ), and every path e1 ; e2 from e1 to e2 in G(e1 ) does not contain interrupting events for p, that is, events that affect either p or a property incompatible with p and differ from both e1 and e2 .
We proceed by contradiction.
Suppose that p(e1 , e2 ) is returned by the algorithm, but it is not an MVI.
If e1 does not initiate p or e2 does not terminate p, then p(e1 , e2 ) cannot be retrieved.
Moreover, if e2 does not belong to G(e1 ), then the visit of G(e1 ) does not retrieve e2 , and hence p(e1 , e2 ) cannot be generated.
Finally, suppose that there exists at least one path e1 ; e2 in G(e1 ) that contains at least one node z which affects either p or a property incompatible with p and is different from e1 and e2 .
If D(z) < D(e2 ), then the node z is visited before e1 , it is labeled as marked, and the label cutoff is propagated to the nodes of G(z) different from z.
In particular, e2 is labeled as cutoff during such a propagation and thus it cannot be chosen as the terminating event of an MVI for p initiated by e1 .
Hence, p(e1 , e2 ) cannot be generated by the algorithm.
If D(z) > D(e2 ) (notice that D(z) 6= D(e2 ), since z 6= e2 and there are not simultaneous events), then the node e2 is visited before z, it is labeled as marked, and the label cutoff is propagated to the nodes of G(e2 ) different from e2 .
Since the graph G(e1 ) is acyclic and there exists a path from z to e2 , there are no paths from e2 to z; hence the propagation of the label cutoff does not reach the node z.
The node z is processed at some later stage, it is labeled as marked, and the label cutoff is propagated to the nodes of G(z) different from z.
In particular, the label of e2 is changed from marked to cutoff, and thus p(e1 , e2 ) cannot be generated by the algorithm.
We now prove that the algorithm is complete, that is, if p(e1 , e2 ) in an MVI, then p(e1 , e2 ) is generated by the algorithm.
Since (e1 , e2 ) is an interval, e2 is reachable from e1 in the graph G(e1 ).
Since p(e1 , e2 ) is an MVI, every path e1 ; e2 from e1 to e2 in G(e1 ) does not contain interrupting events for p different from e1 and e2 .
Hence, the node e2 is not cut off and, since it terminates p, it is labeled as marked and retrieved as the terminating event of an MVI for p initiated by e1 .
Thus, p(e1 , e2 ) is generated by the algorithm.
The proposed strategy is a forward strategy: given a property p and an initiating event e1 , it visits the graph G(e1 ), looking for a terminating event e2 such that p(e1 , e2 ) is an MVI.
Nothing prevents us to define an equivalent backward strategy as follows.
Given a directed graph G, let us denote by G the graph in which each edge (ei , ej ) has been replaced by the edge (ej , ei ).
Given a property p and a terminating event e2 , we visit the graph G(e2 ) as before, looking for initiating events e1 such that p(e1 , e2 ) is an MVI.
5 Complexity analysis In this section, we analyze the worst-case computational complexity of the proposed algorithm for MVIs derivation.
Given an EC-structure H and an acyclic binary relation o [?]
OH , we determine the complexity of computing the set of MVIs with respect to o and H, i.e.
the set of formulas p(e1 , e2 ) such that o+ |= p(e1 , e2 ), by means of the proposed generate-only algorithm.
We measure the complexity in terms of the size n of the structure H (where n is the number of recorded events) and the size m of the relation o.
Given an EC-structure H, the set E of events can be arbitrarily large, while the set P of properties is fixed once and for all, since it is an invariant characteristic of the considered domain.
Since the cardinality of P does not change from one problem instance to another one (unless we change the application domain), while the cardinality of E may grow arbitrarily, we choose the cardinality of E, that is, the number n of events, as the size of H and consider the number of properties as a constant.
Furthermore, we assume that verifying the truth of the propositions "e initiates p" and "e terminates p" costs O(1).
Since the number of properties is constant, the tests "e affects either p or a property incompatible with p" and "p is incompatible with q" cost O(1) too.
The parameters n and m are equal to the number of nodes and the number of edges of the graph (E, o) which is visited during the computation, respectively.
Moreover, we have that m = O(n2 ) when the event graph is dense, m = O(n) for sparse event graphs, and m = O(1) when only a constant number of events is ordered in o.
The following theorem proves that, under the above assumptions, the complexity of the proposed algorithm is quadratic for sparse event graphs and cubic for dense ones.
Theorem 5.1 The complexity of the generate-only algorithm is O(n * m).
Proof.
For every property p and every event e1 initiating p, the algorithm visits the graph G(e1 ) and retrieve all the events e2 such that p(e1 , e2 ) is an MVI.
Since the number of properties is constant, the complexity is O(n * f (n, m)), where  f (n, m) is the complexity of the procedure that visits the graph G(e1 ) and retrieves the nodes that terminate the MVI for p initiated by e1 .
It holds that f (n, m) is the sum of the costs of the visit of G(e1 ) and of the processing of the nodes of G(e1 ).
The graph G(e1 ) is visited breadth-first to construct the layers and to retrieve the terminating events, while it is visited depth-first to propagate the labels cutoff.
Each edge of the graph G(e1 ) is visited at least once (depth-first or breadth-first) and at most twice (first breadth-first, and then depth-first).
Indeed, if an edge (e1 , e2 ) is depth-first visited, then e1 is labeled as marked or cutoff.
Hence, neither a breadth-first visit nor a depth-first one will later reconsider it.
However, edges which have been already breadth-first visited can also be visited depth-first in order to propagate the label cutoff.
It follows that the cost of visiting G(e1 ) is O(m).
Similarly, each node of the graph G(e1 ) is processed at least once (depth-first or breadth-first) and at most twice (first breadth-first, and then depth-first).
Indeed, if the depth-first visit cuts off a node, then it will not be processed anymore.
However, nodes labeled as marked or visited, which have been already processed during the breadth-first visit, can also be processed during a depth-first visit and labeled as cutoff.
The processing of a node consists of the operations of labeling and testing for interrupting or terminating events.
Both these operations cost O(1).
Therefore, processing all nodes of G(e1 ) costs O(n).
Putting together the results of our analysis, we can conclude that f (n, m) = O(m) + O(n) = O(m + n).
If m = O(1), then only a constant number of nodes is processed, and hence f (n, m) = O(1); otherwise, n = O(m), and thus f (n, m) = O(m).
This allows us to conclude that the cost of the algorithm is O(n * f (n, m)) = O(n * m).
In particular, if the event graph is dense, that is, m = O(n2 ), then the complexity is O(n3 ), while if it is sparse, that is, m = O(n), then the cost is O(n2 ).
6.
The generalization to MEC Given an EC-structure H = (E, P, [*i, h*], ]*,*[) and a partial order w, two efficient algorithms, that respectively compute necessary and possible MVIs with respect to H and w, can be obtained from Corollary 3.5 taking advantage of the algorithm for MVIs computation in EC.
In order to compute the sets C(H, w) and S(H, w) (cf.
Section 3), we proceed as follows.
The elements of C(H, w) are obtained by selecting all property-labeled pairs of events p(e0 , e00 ) such that e0 initiates p, e00 terminates p, and e0 and e00 are unordered in w:  C-[?]
for each p [?]
P do for each (e1 , e2 ) [?]
E x E do if e1 [?]
[pi and e2 [?]
hp] and (e1 , e2 ) 6[?]
w and (e2 , e1 ) 6[?]
w then C - C [?]
{p(e1 , e2 )} return C The computation of S(H, w) is more involved.
First, we compute the set U (H, w), containing all pairs (e, p) [?]
E x P such that there exists another event e0 , which affects either p or a property incompatible with p and is unordered with respect to e in w. It is easy to see that if (e, p) [?]
U (H, w), then e neither initiates nor terminates a 2-MVI for p. The set S(H, w) is obtained by selecting those atomic formulas p(e1 , e2 ) such that e1 initiates p, e2 terminates p, and neither (e1 , p) nor (e2 , p) belong to U (H, w): // compute U (H, w) U -[?]
S-[?]
for each p [?]
P do for each e [?]
E do Found - False V -E while not Found and V 6= [?]
do let e0 [?]
V if (e, e0 ) 6[?]
w and (e0 , e) 6[?]
w and e0 is relevant to(e0 , p) then Found - True U - U [?]
{(e, p)} else V - V \ {e0 } // compute S(H, w) taking advantage of U (H, w) for each p [?]
P do for each (e1 , e2 ) [?]
E x E do if (e1 , p) 6[?]
U and (e2 , p) 6[?]
U then S - S [?]
{p(e1 , e2 )} return S In order to determine the set of necessarily true MVIs, it suffices to compute the sets M V I(H, w) (as proposed in Section 4) and S(H, w) (as explained above); the set 2MVI(w) can be obtained by intersecting them.
Similarly, possibly true MVIs are obtained taking the union of M V I(H, w) and C(H, w).
The proof of soundness and completeness easily follows from Corollary 3.5 and Theorem 4.1.
Theorem 6.1 The proposed algorithms for necessary and possible MVIs computation are sound and complete.
The following theorem states that the complexity of the algorithms for necessary and possible MVIs computation is (slightly) higher than that of the algorithm for basic MVIs only in the case of sparse event graphs.
Theorem 6.2 The complexity of the algorithms for necessary and possible MVIs computation is O(n*m+n2 *log n).
Proof.
Given a knowledge state w, the algorithm for the computation of M V I(H, w) has complexity O(n * m) (Theorem 5).
Moreover, it is immediate to see that determining the sets C(H, w) and S(H, w) costs O(n2 ).
Finally, taking the intersection (resp.
union) of two sets of cardinality r costs O(r * log r).
Since M V I(H, w), C(H, w), and S(H, w) have cardinality O(n2 ), the overall cost is O(n * m + n2 * log n).
7.
Conclusions and further developments In this paper, we outlined a graph-theoretic approach to the problem of efficiently reasoning about partially ordered events in Kowalski and Sergot's Event Calculus [11].
The proposed algorithm exploits a generate-only strategy based on a graph representation of ordering information that reduces the computation of the MVIs to a visit of the event graph that pairs traditional breadth-first and depthfirst searches.
Furthermore, we showed how the proposed strategy can be extended to deal with the Modal Event Calculus [4].
In [6], Chittaro et alii propose a generate-only algorithm for MVIs computation that operates on the transitive reduction of the given partial ordering.
Such an algorithm is sound and complete whenever every property is incompatible with all the other ones.
In particular, it is sound and complete whenever there is only one property (singleproperty case).
We are currently working at the development of a sound and complete algorithm that generalizes to the multi-property case the strategy discussed in [6].
The basic steps of this generalized strategy are the following ones: first, it computes (and maintains) the transitive closure G+ = hE, o+ i of the graph G representing the available ordering information; then, for every property p, it extracts from G+ the subgraph induced by the set of events that initiate or terminate p, or a property incompatible with p; finally, it derives the set of MVIs for any property p by applying the strategy for the single-property case to the transitive reduction of the subgraph for p. We expect to achieve complexity results comparable with the ones we reported in the present work.
Acknowledgements We would like to thank the anonymous reviewers for their useful comments.
Thanks also to Roberto Fracas whose Tesi di Laurea contributed to the achievement of the results reported in this paper [10].
References [1] A. V. Aho, M. R. Garey, and J. Ullman.
The transitive reduction of a directed graph.
SIAM Journal of Computing, 1(2):131-137, 1972.
[2] I. Cervesato, L. Chittaro, and A. Montanari.
A modal calculus of partially ordered events in a logic programming framework.
In L. Sterling, editor, Proceedings of the Twelfth International Conference on Logic Programming -- ICLP'95, pages 299-313, Kanagawa, Japan, 13-16 June 1995.
MIT Press.
[3] I. Cervesato, M. Franceschet, and A. Montanari.
A hierarchy of modal event calculi: Expressiveness and complexity.
In H. Barringer, M. Fisher, D. Gabbay, , and G. Gough, editors, Proceedings of the Second International Conference on Temporal Logic -- ICTL'97, pages 1-17, Manchester, England, 14-18 July 1997.
Kluwer Applied Logic Series.
To appear.
[4] I. Cervesato and A. Montanari.
A general modal framework for the event calculus and its skeptical and credulous variants.
Journal of Logic Programming, 38(2):111-164, 1999.
[5] I. Cervesato, A. Montanari, and A. Provetti.
On the nonmonotonic behavior of the event calculus for deriving maximal time intervals.
International Journal on Interval Computations, 2:83-119, 1993.
[6] L. Chittaro, A. Montanari, and I. Cervesato.
Speeding up temporal reasoning by exploiting the notion of kernel of an ordering relation.
In S. Goodwin and H. Hamilton, editors, Proceedings of the Second International Workshop on Temporal Representation and Reasoning -- TIME'95, pages 73- 80, Melbourne Beach, FL, 26 April 1995.
[7] L. Chittaro, A. Montanari, and A. Provetti.
Skeptical and credulous event calculi for supporting modal queries.
In A. Cohn, editor, Proceedings of the Eleventh European Conference on Artificial Intelligence -- ECAI'94, pages 361- 365.
John Wiley & Sons, 1994.
[8] T. H. Cormen, C. E. Leiserson, and R. L. Rivest.
Introduction to algorithms.
The MIT Press, 1990.
[9] T. Dean and M. Boddy.
Reasoning about partially ordered events.
Artificial Intelligence, 36:375-399, 1988.
[10] R. Fracas.
Uso di algoritmi su grafi per ragionare in modo efficiente su insiemi di eventi parzialmente ordinati (in Italian).
Tesi di Laurea in Scienze dell'Informazione, Universita di Udine, Italy, 1997.
[11] R. Kowalski and M. Sergot.
A logic-based calculus of events.
New Generation Computing, 4:67-95, 1986.
[12] J. van Leeuwen.
Graph algorithms.
In J. van Leeuwen, editor, Handbook of Theoretical Computer Science.
Volume A: Algorithms and Complexity, pages 525-632.
Elsevier, 1990.
Temporal Reasoning in a Meta Constraint Logic Programming Architecture Evelina Lamma, Michela Milano  Paola Mello  DEIS UniversitA di Bologna Bologna 40136, Italy  Istituto di Ingegneria Universitii di Ferrara Ferrara 41100, Italy  Abstract  or in determining an equivalent minimal representation, or in computing the transitive closure of the network.
All the researchers who have worked in the temporal reasoning field have proposed algorithms for solving these problems (see, for example, [2,9,26,29]) but, in general, they did not focus on defining a programming paradigm and architecture for solving these problems.
In this paper, we show how Constraint Logic Programming [14,15] on finite domains (CLP(FD)) can be a flexible and suitable tool for temporal reasoning thanks to its efficient constraint propagation mechanism.
However, CLP presents some limitations in the treatment of both quantitative and qualitative temporal constraints.
On one hand, CLP has a limited embedded propagation mechanism (arc-conszstency) which cannot be changed by the user.
On the other hand, qualitative constraints can be expressed but not propagated (in order to find, for example, the minimal network) in a CLP(FD) framework.
We overcome this drawbacks by integrating both kinds of reasoning (as in the Meiri's Framework [all) in a flexible and modular architecture.
Our meta architecture contains two finite domain constraint solvers which cooperate and exchange knowledge in order to deal with both kinds of constraints: The object level treats quantitative information, while the meta level handles qualitative information by reasoning on the (passive) constraints of the object level.
The contribution of this paper mainly concerns:  Constraint Logic Programming (CLP) is a powerful programming paradigm combining the advantages of Logic Programming and the eficiency of constraint solving.
However, CLP presents some limitations in dealing with temporal reasoning.
First, it uses an "arc consistencym propagation algorithm which cannot be changed by the user and it is too weak in many temporal frameworks.
Second, CLP is not able to deal with qualitative temporal constraints.
In this paper, we show how to overcome these limitations.
In particular, we present a way of performing a path-consistency check without changing the propagation algorithm of the constraint solver.
In addition, we show how to integrate qualitative and quantitative temporal reasoning by using a two module meta CLP architecture.
Each module is a finite domain constraint solver (CLP(FD)).
The object system (extended with the path-consistency algorithm) performs quantitative reasoning, while the meta-level reasons on constraints of the underlying system thus performing qualitative reasoning.
In this way, we can benefit of the eficiency of the constraint handling mechanism of CLP and the modularity, flexibility and scalability of meta-architectures.
1  Introduction  Temporal reasoning plays a central role in many Artificial Intelligence applications such as planning, scheduling, and natural language understanding.
Several frameworks have been proposed in order to deal with this form of reasoning, most notably Allen's Interval Algebra [a], W a i n and Kautz's Point Algebra [as], Dechter, Meiri and Pearl's STP and TCSP (91 and Dean and McDermott's Time Map Management [8].
In addition, some promising approaches have been proposed in order to integrate qualitative and quantitative temporal reasoning [16,21].
In these frameworks the representation of the problem is given in terms of a constraint graph where nodes are the variables of the problem, i.e., points or intervals, and arcs are constraints, i.e., temporal relations between pairs of variables (each n-ary relation among n variables can be represented in terms of binary constraints).
Given a constraint graph, we are interested either in determining whether the graph is consistent,  0  0  a way of handling qualitative temporal constraint propagation by adding a meta constraint solver reasoning on constraints of the underlying system, as proposed by Tsang [25] with metaconstraznt graphs.
In this way, we can benefit of the efficiency of the constraint handling mechanism provided by CLP, and  128 0-8186-7528/96 $5.00 0 1996 IEEE  a way of performing a path-consistency algorithm not by changing the propagation algorithm of the constraint solver as in [6], but by performing an arc-consistency on the so called "arc-variables" and "path-equivalent" constraints;  the modularity, flexibility and scalability of metaarchitectures.
The paper is organized as follows.
In Section 2 we will sketch some features of the CLP(FD) paradigm while focusing on its limitations for treating qualitative temporal reasoning.
Then, we propose how to overcome these limitations.
Section 3 is devoted to the architecture of the general purpose temporal reasoner, i.e., the qualitative and the quantitative modules and their interactions.
Section 4 is devoted to an example.
Conclusions and a mention of future works follow.
of absolute locations on the time line and bound on distance constraints (see [9]).
Temporal intervals can be expressed as pairs of points linked by the relation Ii, < Isup,where Iinf and Isup are the starting and ending points of the interval I .
According to the classification of Meiri [21], we can represent augmented Continuous Point Algebra (CPA) and Point Algebra (PA) networks on discrete domains.
The augmented CPA networks on discrete domains require an arc consistency algorithm for deciding the consistency and an arc+pathconsistency algorithm for computing the minimal domains.
CLP(FD) solvers like CHIP [lo] or ECLiPSe [ll]provide only an arc consistency algorithm.
In section 3.1 we propose a way of performing path consistency not by changing the consistency algorithm of the solver, but by adding what we call "arc-variables" and ('path-equivalent'' constraints.
On the other hand, deciding the consistency of PA networks on discrete domains is a NP-complete problem.
Therefore, we can apply a path consistency algorithm just as a preprocessing technique for reducing the search space.
Another limitation of CLP(FD) concerns its capability of representing qualitative constraints such as x < y, but its inadequacy of reasoning about them.
This inadequacy arises for three reasons that we have identified and called the composataon problem, the zntersectaon problem and the qualatatave-quantitatave problem:  Constraint Logic Programming  2  Constraint Logic Programming (CLP) is a class of programming langua es combining the advantages of Logic Programming fLP) ([ZO]) and the efficiency of constraint solving.
In this paper we focus on CLP on finite domains CLP(FD) [15].
In this framework, variables range on a finite domain which is initially set to [O..max].
These domains are gradually reduced by the propagation of constraints.
Inconsistency is detected when a domain becomes empty.
As an example, let us suppose that we have three variables X, Y , Z ranging on the domain [1..103, and the constraint X < Y which produces the propagation: D X = [1..9] and D y = [2..10] and the constraint Y < Z which produces the propagation: D y = [2..9] and D z = [3..10].
The fact that D y has changed produces the final propagation of constraints leading to the reduction of the domains of X , Y , 2 to: D x = [1..8], D y = [2..9], D z = [3..10].
This propagation is named "arc-consistency" and ensures that, for each value of a variable domain D x , there exists at least one value in each other domain which is consistent with the constraints mentioning the variable X .
2.1  Composition: in the example of Section 2, when the propagation terminates, we are not able to infer information on the relationship that holds between X and Z starting from the domains DX = [1..8] and Dz = [3..10].
CLP is not able to build the transitive closure of the temporal network.
The information that should be inferred from the two constraints X < Y and Y < Z is the relation X < Z .
This relation should be inferred from the transitivity rule, and, in particular, it is the result of the composition of constraints, see for example [2,29].
CLP(FD) and its Limitations for Temporal Reasoning  CLP has a very efficient constraint handling mechanism which can be suitably used for representing and reasoning on constraint-based temporal frameworks.
CLP(FD) is able to express directly quantitative temporal reasoning.
In fact, we can interpret the const raint s: (X E [3..16]) A (y E [1..19]) A ( X - 2 7) A (Z- y 5 16) in a temporal reasoning context.
We can see the variables x and y as temporal points which have a temporal location on the discrete time line (see [3]).
In particular, x is a temporal point located somewhere between 3 and 16 time units, y between 1 and 19.
The last two constraints link the distance of the time points to range between 7 and 16 time units.
The propagation of constraints leads to the reduction of the variables' domains to': (x E [4..16]) A (y E [1..13]) while suspending the two passive constraints (Z - y 2 7) A (X - y 5 16).
Therefore, with CLP(FD) we can express quantitative constraints between temporal points, i.e., ranges  Intersection: the second problem of CLP on finite domains is related to the first and concerns the computation of the minimal network.
The constraints X < Y ,Y < Z and X 5 Z are consistent but not minimal.
A constraint solver should be able, in this case, to compose the first two constraints and intersect the result with the third constraint leading to X < 2.
Qualitative-Quantitative: another limit at ion of CLP on finite domains is that it deletes values according to constraints, but it does not reduce constraints on the basis of variable domain values.
Suppose that we state that X 5 Y , X and Y being temporal points ranging on DX = [1..5] and DY = [6..10] respectively.
It can be seen immediately that X and Y cannot be equal because there does not exist any pair of values from Dx and from D y that "supports" the constraint.
Therefore, the tightest constraint that holds between X and Y is X < Y.
A constraint solver on finite domains propagates constraints and simply concludes that the constraint is satisfied without simplifying it.
These constraints are called active constraints  129  2.2 How t o Overcome these Problems In the previous Section, we have underlined that CLP(FD) presents some limitations for qualitative reasoning and a limited consistency algorithm for quantitative reasoning.
Therefore, we propose an approach based on a meta-architecture [1,5] that solves the above mentioned limitations.
We use a constraint solver on finite domains for handling quantitative constraints, and a meta-constraint solver for qualitative temporal constraints.
On one hand, we extend the object level solver with the concept of "arc-variables" and "path-equivalent" constraints for reaching path-consistency without modifying the algorithm of the constraint solver.
On the other hand, we solve the qualitative reasoning limitations by building a meta-constraint solver where variables are relations.
On meta-level, unary constraints restrict a meta-variable to take its values from a finite domain of relations.
For each pair of variables X and Y (nodes of the graph) we have a meta-variable, namely R x y , ranging on a finite domain of relations.
This domain contains the relations that hold between X and Y.Suppose that X <_ Y , then the domain of the meta-variable Rxy is the powerset of the possible constraints holdin between X and Y , namely D X Y = [{<I,{=I, {<, =,!
0 1 .
Intersection and composition are embedded in the meta-constraint solver as primitive operations.
Therefore, the general purpose temporal reasoner is composed by two constraint solvers propagating their "own" constraints, and interacting each other in order to perform a more powerful propagation.
3  A General Purpose Temporal Reasoner based on CLP  In this section, we explain how the two modules propagate qualitative and quantitative constraints, and how they interact.
An extended version of this paper including theoretical issues and proofs is available [17].
3.1  Quantitative Constraint Solver  The quantitative module is a CLP(FD) constraint solver.
CLP on finite domains can express unary constraints between points such as: Pi E 1 and binary constraints between pairs of points such as: (Pz-Pj)E I .
The qualitative constraints of a CPA (or PA) network can be translated in bound on distance constraints (see [all).
Therefore, we now concentrate on bound on distance constraints.
Let us see an example: if variable X ranges on the domain DX = [10..20] and variable Y ranges on the domain D y = [40..50] and the constraint 2 = Y - X is stated, then the constraint solver is able to associate with the variable 2 the domain D z = [20..40].
Again, if the variable X ranges on the domain Dx = [10..20] and the variable 2 on the domain D z = [30..40], then the propagation of the constraint 2 = Y - X associates with the variable Y the domain D y = [40..60] These constraints, after the propagation, are delayed because variables are not yet instantiated and may cause further propagations.
This propagation is that  performed in the STP framework [9].
In STP we use continuous domains while in this case we use discrete domains.
However, the propagation, in this case, can be performed only on upper and lower bounds of each domain.
Therefore, the mechanism is the same.
Now we explain how bound on distance constraints are propagated in the temporal reasoner.
We refer to an example given by Dechter, Meiri and Pearl in [9].
We have four temporal variables, namely XI, Xz, X 3 , X4, and a time point X O representing the "beginning of the world".
The constraints defined by the problem are: 10 10 60  < Xi-Xo 5  5  X3-x~  2 0 , 30 40 70.
< 20,  2 x4-xo 5  < Xz-Xi 5  x4-x3  5 5  40, 50,  These constraints can be written as: % The f i r s t eleven c o n s t r a i n t s % a r e s t a t e d by t h e problem Xi::10..20, X4 : :60.
.70, 2 2 3 : : 10.
.20  zzi=xz-xi,  Z2i::30..40, 243 9  : :40.
.50, Z43=X4-X3,  z23=x2-x3  5  z42=x4-x2  9  z 4 1 = x 4 - x 1 ,z31=x3-x1,  % The following c o n s t r a i n t s perform % a path-consistency equivalent algorithm 231=221-223, 221'z24-(-241), Z42=z43-223, 241=z42-(-Z21,)  231=(-243)-(-241), 243 =241-z31, z42=241-z21,  221=Z23-(-231), 243=242-(-223)  241=243-(-231)  z23=z21-z31,  9  3  z23=(-z42)-(-z43)  *  where the predicate symbol :: associates a variable Var with a domain Domain in the following way: Var .. .
.
Domain.
The variables 221, 2 2 3 , 243, 242, 241 and 231 are called "arc-variables" because they refer to arcs.
The first eleven constraints define the structure of the problem.
In their paper [9] the authors have shown that the Floyd- Warshall's all-pairs shortest-paths algorithm represents a complete algorithm for determining the consistency of a STP and for computing the minimal network.
Moreover, they have shown that for STP this algorithm is equivalent to the path consistency algorithm.
On the other hand, a CLP(FD) solver performs a lower consistency check: an arc consistency algorithm which does not check all the paths of the network given the variables of the problem.
However, if we add to a program some constraints (the last 12 constraints in the example above, i.e., the "path-equivalent" constraints) describing the paths in the graph and we apply arc-consistency on them, we are able to obtain path consistency.
Theorem 3.1 Let G=(V,A) be a graph where each node represents a varaable (temporal poant) of the problem and each arc, linkzng a paw of vartables, as a bznary constraant definzng the mammal and mammal dastance between the two poants.
Let us consider three varaables X, Y and Z linked b y the following bound on distance constraints: a1  SX -  bi <X c1  130  <z  -Z  5 a2, 5 bp,  - Y  C2,  Y  <  pair of variables X { r l , .
.
.
, r,}Y and X{r' , .. .
, & } Y , whose symbol is =m, is the powerset of the set theoretic intersection between the two vectors { T I , .
.
.
, r,} and { r i , .
.
.
rh}.
A meta-variable representing the possible relations between X and Y is associated with each constraint.
Therefore, the intersection of metavariables Rxy =m R h y (corresponding to the two constraints) imposes the meta-variables to be equal by intersecting their domains.
For example, if the meta-variable RXY has an associated domain: D x y = [{<}, {=}, {<,=}, {}] because of the existence of the qualitative constraint X 5 Y 2 ,and the meta-variable RkY has an associated domain D k y = [{>}, (>, =}, {=}, because of the existence of the qualitative constraint X 2 Y , then the intersection between the two constraints is performed by a meta-unification Rxy =m R k y that leads to the following propagation of constraints: Dxy = Dku = corresponding to the qualitative domain X = Y in the finite domain constraint solver.
The composition should be defined directly in the constraint solver via a transitivity table (see [2,29]).
The composition is represented by the symbol +m and it is defined as following.
The composition of two meta-variables R x z and Rzy admits only those values rSy for which there exists r,, E Dxz and rty E Dzy such that rzy = rzz@rzVwhere the symbol 8 represents the composition of primitive constraints and is defined by the transitivity table on the constraints of D, see [2,29].
The "tighter" relation is represented by the symbol cm.A set of constraints C' is tighter than another set of constraints C" if each pair of values allowed by C' is also allowed by C".
In our case, the relation tighter can be translated into the set inclusion.
It is worth noting that, as in the quantitative solver, we should perform an arc consistency on the metavariables representing arcs of the object level graph.
In fact, the constraint Rxy =m R x z +m Rzy performs a composition of R x z and Rzy in the way defined above, and intersects the computed domain with the domain of R x y .
This constraint behaves in the same way of the constraint X Y = X Z Z Y described in Section 3.1.
Notice that the qualitative constraint solver can work independently from the quantitative part.
For example the following qualitative program:  The path through nodes X, Y and Z is path consistent i f f the following unary constraints are arc consistent: XY :: a1..a2, xz :: bl..b2, ZY :: c1..c2, XY = xz + ZY,  where XY = X-Y, X Z = X-Z and ZY = Z-Y.
Proof.
: Refer to [17].
Without modifying the CLP(FD) solver, we are able to reach the same result obtained by [6] which, instead, modifies the structure of the CLP(FD) solver for achieving path-consistency.
The result of the computation of the abovementioned example consists of delayed constraints in which each variable is associated with the minimal domain:  I}{  E{=}, I}{  X1::10..20, x2::40.,50, X3::20..30, X4::60..70, &1::30..40, &1::10..20, &1::50..60, &3::10..20, &2::20..30, &::40.
.50.
Therefore, with CLP(FD) we are able to solve a STP, to determine its consistency and to compute the minimal network associated with, provided that we add some "path-equivalent" constraints.
We are currently studying a way of performing whatever kind of consistency by adding constraint solvers as meta-levels, see [I81* As concerns the complexity of the above meutioned computation, we have proved [17] that it is equal to the complexity of a path consistency algorithm augmented by the overhead of creating arc variables and path equivalent constraints.
This result is mainly due to the fact that in the STP framework variables ranges on convex intervals, and arc consistency can modify the lower and upper bound of each domain, thus avoiding the domain values enumeration.
3.2  Qualitative Constraint Solver  +  The qualitative module is a meta-constraint solver on finite domains of relations.
A meta-variable Rxy is automatically generated each time a new qualitative constraint between variables X and Y is introduced.
Qualitative constraints are disjunctions of primitive constraints between variables.
If X and Y are points, primitive constraints are point algebra relations [29] (with or without the constraint f ) .
If X and Y are intervals the primitive constraints are interval algebra relations [2].
The intervals will be translated in their starting and ending points.
Therefore, if between variables X and Y we have a relation of the kind: ( X rl Y ) V .
.
.
V ( X r, Y ) which can be written as: X { r l , .
.
.
, r,}Y where ris are primitive qualitative relations, then the system automatically creates a meta-variable Rxy whose domain contains unary constraints which link the variables to Dxy is the powerset of { r l , .. .
, rn}.
take their values from a set of relations, and binary The meta constraint solver has two operations: in2The symbol {} is useful for detecting inconsistencies in qualtersection and composition of qualitative Constraints.
itative reasoning.
The intersection of two constraints, linking the same  131  that should be performed when a change in a quantitative domain influences a qualitative domain.
First of all, every time a new constraint is encountered in the object system, its meta-representation should be added to the meta constraint store.
In addition, each time a variable domain changes due to the object system propagation, a linking rule provides a propagation in the meta system.
This propagation is much more computational expensive and leads to the deletion of constraints that are not supported by variable domain values.
We should check for each constraint linking two variables X and Y , if there exists at least one couples of values 2 E DX and y E D y that supports the constraints between X and Y , represented by the meta-variable Rxy .
In other words, this propagation checks if the set:  constraints containing operations on meta-variables, i.e., intersection and composition, or meta-relations such as the tighter relation.
Therefore, in the metalevel we can express the Vilain and Kautz's Point Algcbra [29] which deals with qualitative relations between temporal points.
It is worth noting that qualitative information, except for the relation #, can be translated into nondisjunctive bound on distance constraints.
For example, the constraint X Y can be translated in -a5 X - Y 5 0.
However, a CLP(FD) solver does not translate this constraint in this way since it applies an "arc-consistency", and therefore the distance of the two variables does not necessarily range on - a .
.
O .
For example, if we have X :: [0..10] and Y :: [-6..30], Y 5 X , 2 = X - Y , the CLP(FD) solver propagates the constraints thus resulting in X :: [0..10], Y :: [-6..10], 2 :: [-lo.. + 161.
Therefore, we need to change the propagation algorithm if we want to achieve 2 :: [0..16].
Second, we gain in expressiveness since the user can express the fact that the relation that holds between variables X and Y should be tighter than the relation that holds between, say, 2 and A'.The user can also query the system by asking a feasible scenario for qualitative temporal information (we will show this feature in the example in section 4 .
This query can be straightforwardly implemented i we have a finite domain of relations at meta level, also using domain dependent heuristics embedded in the meta level.
<  ~ = { ( x , Y ) E DXx D Y I X R X YY} is not empty.
The notation 2 Rxy y represents the "application" of one of the constraints in the domain of the meta-variable Rxy to 2 and y.
For example, suppose that X has an associated domain Dx = [ 3 .
.
5 ] ,Y the domain D y = [6..9] and X and Y are related by the constraint X 5 Y , which is represented by the meta-variable RXY ranging on the domain DXY = [{<}, {=}, {<, =}, {}].
Intuitively, the constraint "equal" is no longer entailed by the values of the domains of X and Y .
In fact, while the set S is not empty for the value < of Rxy because it contains, for example, the couples (3,6), (3,7), (3,8) and so on, the set S is empty for the value = of Rxy.
Therefore, the quantztatzve-guulztatzve propagation deletes from the domain of Rxy the values containing the constraint =.
This process is general, but very heavy handled.
If we work in a specific domain we can make use of special purpose techniques for the interaction between qualitative and quantitative constraints, see [17].
2  3.3  Interactions between Qualitative and Quantitative solvers  One of the most interesting parts of the temporal reasoner concerns the integration of qualitative and quantitative constraint solvers.
This integration concerns the propagation of quantitative constraints when qualitative domains are modified and the propagation of qualitative constraints when quantitative domains are modified.
For this part, our propagation method is different from the Meiri's propagation technique but it is straightforward in a Constraint Logic Programming paradigm.
We have defined two linking rules which ensure that a propagation in the quantitative solver is reflected in the qualitative solver and vice versa.
Due to space limitations, we do not describe formally these linking rules (reported in [17]),but we give an informal idea of the propagation adopted.
The qualatatave-quantztatave propagation, denoted in section 4 with qual-quan, is straightforward.
It is equivalent to the addition in a CLP program of a new qualitative constraint.
For example, if variables X and Y are constrained by X 5 Y , they have an associated meta-variable Rxy ranging on D x y = [{< }, {=}, {<, =j , {)I.
Suppose now that, for the propagation of constraints in the meta constraint solver, the domain D x y is reduced to D x y = [{<},{I].
In the quantitative solver the propagation is performed by adding the new constraint X < Y to the constraint store.
This new constraint may cause further propagations on quantitative domains.
The quantatatzve-qualatatwe interaction, denoted in section 4 with quan-qual, concerns the propagation  4 One Example In this Section we present one example which applies to this architecture, taken from Meiri's paper [all, which handles both qualitative and quantitative temporal knowledge.
In the example, we represent points as CLP domain variables while intervals are represented by their starting and ending points I- arid I+ linked by the constraint I- < I+.
The relations between intervals are the usual Allen's relations [a]: starts (symbol s), durin (symbol d), overlaps (symbol o), before (symbol <?, equal (symbol =), finishes (symbol f ) , meets (symbol m) and their inverses (si, di, 01, >, =, fi, m i respectively).
John and Fred usually work at U local ofice, zn whach case John takes less than 20 mznutes and Fred 1520 manutes to get to work.
Twace a week John works a t the maan ofice, an whach case he takes at least 60 manutes.
Today John left home between 7:05 and 7:lO a.m., and Fred arraved a t work between 7:50 and 7:55 a.m. John and Fred met at the traffic laght on thew way to work.
132  The representation of the problem should be written in a configuration file.
This file contains information about the above-mentioned story: interval(J, (J-,J+)), interval(F, (F-,F+)), point (PO), origin (PO) arc(J, F, [s,si,o,oi,f,fi,d,di,=I), distance(J-, PO, Dl), distance(F+, PO, D2), distance(F+, F-, D3), distance( J+, J-, D4), D1::0..5, D2::50..55, D3::15..20, D4::0..20,60..max.
In this case, the propagation of meta-constraints does not change the meta variable domains.
The quantitative solver transforms the distance constraints in quantitative relations for the CLP(FD) solver.
Moreover, it generates all the possible paths of the graph (see section 3.1) and performs arc consistency on them.
Therefore, the quantitative level constraints are the following:  where J is the interval representing the event John going to work, F is the interval representing the event Fred going to work and origin(P0) is a predicate which holds iff PO is the beginning of the world.
The result of the propagation is the same as that reported by Meiri, but it is interesting to follow the two solvers' steps in order to understand their behavior.
A pre-processing additional module interval-point translates the relations of the Interval Algebra in relations between their starting and ending points.
The interval-interval relation:  The propagation of object level constraints leads to the following domains:  5-::0..5, F+::50..55, D3::15..20, D4::0..20,60..max1 D3 = F+ - F-, D4 = J+ - J-, D5 = F- - J-, D6 = F+ - J+, D7 = F+ - J-, D8 = F- - J+, D5=D8+D4, D5=D7-D3, D6=D7-D4, D6=D3+D8.
5-::0..5, F+::50..55, J+::60..max, F-::30..40, D3::15..20, D4::60..max, D5::25..40, D6::-max..-5, D7::45..55, D8::-max..-20  arc(J, F, Cs,si,o,oi,f ,fi,d,di,=l), The fourth step concerns the qualitative-quantitative and quantitative-qualitative propagations.
The qual-quan interaction (see section 3.3) adds the constraints: J+ > F-, J- < F+, J- < J+, F- < F+ which do not alter the variable domains because they are already consistent with the constraints.
On the other hand, the propagation quan-qual (see section 3.3) alters the meta-variable domains and, in particular, the meta-variables representing the following constraints J- [<,=,>I F-, J+ [<,=,>I F+.
In fact, the domain of J-: : O .
.5 and the domain of F-: :30..40 entail only the constraint J- < Fwhile the domain of J+: :60.
.max and the domain of F+: :50.
.55 entail only the constraint J+ > F+.
The propagation of constraints reduces the meta-variables domains.
This reduction leads to another propagation in the meta-constraint solver but this latest propagation does not alter any other meta variable domain (the meta-constraint graph reaches the quiescence).
In the last step the module interval-point translates the relations between starting and ending points in relations of the Interval Algebra.
In particular, the only relation entailed by the starting and ending point constraints is F during J .
After having determined the minimal network representing the above-mentioned problem, we can be interested in finding a feasible scenario, i.e., an arrangement of the temporal objects along the time line which is consistent with the given constraints, see [27].
is translated into six point-point relations: arc(J-, arc(J+, arc(J+, arc(J-, arc(J-, arc(J-, arc(F-,  F-, [ < , = , > I ) ,  F+, [ < , = , > I ) , F-, [>I), F+, [<I), F+, [<I), J+, [<I),  F+, [<I).
Notice that the constraints J- [<,=,>I F-, and J+ [<,=,>I F+, are not added to the quantitative program because they do not restrict the variable domains.
.__ .__.
These relations are automatically transformed by quan-qual in meta-variables of the meta-constraint  solver:  linked by meta-constraints:  133  values is committed at the end of the computation, thus preventing failures and reducing the search space.
In our architecture this problem can be solved in a straightforward way.
In fact, the CLP(FD) primitive indomain(X), when applied to a domain variable X, instantiates X to a feasible value of its domain.
The propagation then eliminates, from other variable domains, those values which are incompatible with X.
At the end of the propagation, either all the variables are instantiated or another indomain occurs, until there are no more unbounded variables.
In our example, we can instantiate the starting and ending points of the intervals I and J in order to obtain a feasible scenario.
The clause: scenario ([I ) .
scenario ( C V a r I Others] 1 :indomain (Var) ,  5  Conclusions and Future Works  We have presented a Constraint Logic Programming two level architecture for both quantitative and qualitative temporal reasoning.
The architecture is structured into two communicating modules: a finite domain constraint solver for the quantitative reasoning and a meta constraint solver for handling qualitative reasoning.
The object level constraint solver has been extended in order to perform a path consistency check not by changing the solver consistency algorithm but by adding "arc-variables" and "path-equivalent" constraints.
The meta constraint solver reasons on the constraints of the quantitative module and is able to perform operations on them such as composition and intersection and to state relations between constraints, e.g., the tighter relation.
In this paper we have discussed the structure of the two solvers and the relations and interactions between the two.
This architecture has been implemented by using the finite domain library of ECLiPSe [ll],a CLP language developed by ECRC.
Some related works have been studied and discussed in [17].
Here, we list some of the most relevant: As concerns temporal reasoning, there are many related works.
First of all W a i n and Kautz' point algebra, Allen Interval Algebra [a], the Meiri's framework [21] and Tsang work on meta graphs [25] are the starting points for our work.
As far as the integration of CLP(FD) and temporal reasoning is concerned, we have very interesting works.
T .
Fruhwirth in [13] describes how to treat qualitative and quantitative temporal reasoning with Constraint Handling Rules.
Another related work is presented in [24] where the integration of qualitative and quantitative reasoning is performed by extending a single constraint solver.
A third work, described in [19], concerns the integration in a finite domain constraint solver of the Interval Algebra qualitative constraints.
All these approaches are based on a flat architecture while our approach is based on a meta architecture thus gaining in modularity and flexibility.
Finally, as regards the path consistency in CLP FD , a related work is [6] which embeds in a CLP FD solver the path consistency algorithm.
We do not c ange the CLP(FD) propagation algorithm, but we simply add new variables and constraints.
We are generalizing this idea by using a several level meta architecture for reaching whatever consistency algorithm, see [18].
scenario (Others) .
instantiates the temporal points of the problem [Var I Others] to feasible values.
Usually, according to the first f a i l principle [as], the variable to be instantiated next is the variable with the smallest domain (the most constrained variable).
Therefore, in the implementation we used this heuristics.
Other scenarios can be found by using the backtracking mechanism of CLP.
The same problem can be handled in a qualitative network.
Each meta-variable represents a set of relations between variables linked by an arc.
We could be interested in finding a feasible scenario, i.e., an arrangement of the temporal relations between objects which is consistent with the other constraints, see [25].
In this case, the next variable to be instantiated could be the most constrained one, i.e., the variable with the minimum sized domain.
Therefore, the above-mentioned clause s c e n a r i o can be used also for qualitative constraints.
In the example above, the consistent scenario has been already found because meta-variables have no disjunctive constraint but each meta-variable is instantiated to a single primitive relation.
In fact, we have the following qualitative constraint s : J+ > F-, 3- < F+, J- < J + , F- < F + , J- < F-, J+ > F+.
tb  In qualitative networks, a very important problem is to decide which value to try next.
Usually, the ordering of the values according to their restriction is a domain dependent problem.
Tsang [25] proposes some orderings of temporal interval-interval relations, e.g., the more restrictive relations are those imposing a greater number of point-point constraints between  Future work is devoted t o generalize this framework  their starting and ending points or in planning prob-  in order be able to apply it to different domains without changing its structure.
A possible instance can be for example a CLP(Interva1s) as the object-level system [4] and a qualitative solver performing the Interval Algebra reasoning as the meta system.
This is a work in progress.
We intend to apply the system to a real life application of scheduling and planning as the one described in [7].
lems the more restrictive relations are those minimizing the overall duration of the schedules generated.
In this case, the "tighter" relations between the constraints of the qualitative constraint solver can be useful in order to decide which value to try next.
It is worth mentioning that in applications like least-commitment planning or scheduling the qualitative temporal constraints should be instantiated before quantitative constraints.
The choice of the variable  134  Acknowledgments  [14] J.Jaffar, J .L.Lassez, "Constraint Logic Programming", Proceedzngs of the Conference on Prznczple of Programming Languages, 1987.
This work has been partially supported by the M.U.R.S.T.
Project 60% (Minister0 dell'Universitb della Ricerca Scientifica e Tecnologica).
We would like to thank Helene Pokidine who helped us in implementing the system and anonymous referees for very useful comments on the first version of this paper.
[15] J.Jaffar, M.J.Maher, "Constraint Logic Programming: a Survey", Journal of Logzc Programmzng on 10 years of Logzc Programmzng, Vo1.19/20, pp.
503-582, 1994.
References L .Aiello, C.Cecchi, D Sartini, "Represent ation and  [IS] H.A.Kautz, P.B.Ladkin, "Integrating Metric and Qualitative Temporal Reasoning", Proceedzngs AAAI91, pp.241-246, 1991.
Use of Metaknowledge", Proceedings of the IEEE, Vol.
74, NO.
10, pp.
1304-1321, 1986.
[17] E.Lamma, P.Mello, M.Milano, "A Meta Constraint Logic Programming Architecture for Qualitative and Quantitative Temporal Reasoning", Tech.Report DEIS-LIA-95-001, 1995, www-lia.unibo.it/Research/TechReport.html.
J.F.Allen, "Maintaining Knowledge About Temporal Intervals", Communications of the A CM, Vol.
26, pp.
832-843, 1983.
J.F.Allen, P.J.Haves.
"A Common-Sense Theorv of Action'and Tige" Proceedings of IJCAI85, p6.
528-531, 1985.
[18] E.Lamma, P.Mello, M.Milano, "A Multi-Level CLP Architecture for Consistency Techniques", Submztted for pubblzcatzon.
F.Benhamon, D.McAllester, P.Van Hentenryck, "CLP(Interva1s) Revisited", Tech.
Report CS-9418 Computer Sczence Department, Brown University, 1994.
[191 J .Lever, B.Richards, R.Hirsh, "Temporal Reasoning and Constraint Solving", Delzverable CHIC, ESPRIT Project EP5291, IC-Park, 1992  K.A.Bowen, R.A. Kowalski, "Amalgamating Language and Metalanguage in Logic Programming" in Logzc Programmzng, K.Clark and S.Tarnlund Eds., Accademic Press NY, pp.
153-173, 1982.
[20] J.W.Lloyd, Foundataon of Logzc Programmzng", Second Extended Edition, Springer-Verlag, 1987.
[all I.Meiri, "Combining Qualitative and Quantitative Constraints in Temporal Reasoning", Proceedzngs of AAAI91, pp.260-267, 1991.
P.Codognet, G.Nardiello, "Path Consistency in clp(FD)" , in Proceedzngs of the Fzrst Internatzonal Conference Constraint in Computatzonal Logzcs CCL94, pp.
201-216, 1994.
[22] A.K.
Mackworth, "Consistency in Networks of Relations", Artzficzal Intellzgence, vo1.8, pp.
99118, 1977.
A.Dalfiume, E.Lamma, P.Mello, M.Milano, "A Constraint Logic Programming Application to a Distributed Train Scheduling Problem" , Proceedings of the Conference on Practical Applzcatzon of Prolog, pp.163-182, 1995.
[23] R.Mohr, T.C.Henderson, "Arc and Path Consistency Revisited", Artzjiczal Intellagence, Vo1.28, pp.
225-233, 1986.
[24] F.A.Barber, R.Berlanga, R.Fordellas, F. Ibaiiez, G.Martin, F.Toledo, "Integration of Metric and Symbolic Temporal Constraints in CLP" , unpublashed manuscrzpt.
T.L.Dean, D.W .McDermott , "Temporal Data Base Management", Artificzal Intellzgence, Vol.
32, pp.
1-55, 1987.
R.Dechter, I.Meiri, J.Pear1, "Temporal Constraint Networks" , Artificzal Intellzgence, Vol.
49, pp.
6195, 1991.
[25] E.P.K.Tsang, "The Consistent Labeling Problem in Temporal Reasoning" , Proceedzngs of AAAI87, pp.251-255, 1987.
[lo] M. Dincbas, P. Van Hentenryck, M. Simonis, A. Aggoun, T. Graf, F. Berthier, "The Constraint Logic Programming Language CHIP", Proceedings of the Internatzonal Conference on Fzfth Generation Computer System (FGCS88), pp.693702,1988.
[26] P-VanBeek, R.Cohen, "Exact and Approximate Reasoning about Temporal Relations", Computatzonal Intellzgence, V0l.6, pp.132-144, 1990.
[27] P.VanBeek, "Reasoning about Qualitative Temporal Information", in Artzficial Intellzgence, V01.58, pp.
297-326, 1992.
[ll] ECL'PS" User Manual Release 3.3, ECRC, 1992.
[28] P.Van Hentenryck, Constraznt Satzsfactaon zn Logzc Programmzng, MIT Press, 1989.
1121 E.C.
Freuder, "Synthesizing Constraint Expressions", Communzcatzon ofthe ACM, vol.
21, N o l l , 1978, pp.
958-966.
[29] M.B.Vilain, H.Kautz, "Constraint Propagation Algorithms for Temporal Reasoning" Proceedzngs of AAAI89, pp.
377-382, 1989.
[13] T. Friihwirth, "Temporal reasoning with con-  straint handling rules", Tech.
Report ECRC-94-05, ECRC, 1994.
135

Free Schedules for Free Agents in Workow Systems Areas: time in multiple agents, communication, and synchronization; temporal constraint reasoning    Claudio Bettini,  Abstract This paper investigates workow systems in which the enactment and completion of activities have to satisfy a set of quantitative temporal constraints.
Dierent activities are usually performed by autonomous agents, and the scheduling of activities by the enactment service has among its goals the minimization of communication and synchronization among the agents.
The paper formally denes the notion of a schedule for these workow systems and it identies a particularly useful class: free schedules.
A schedule species a time range for the enactment, duration, and completion of each activity in order to satisfy all the temporal constraints in the workow.
In a free schedule, an agent has to start its activity within the range specied in the schedule, but it is free to use any amount of time to nish the activity as long as it is between a minimum and maximum time he has declared when the workow is designed.
No synchronization with other agents is needed.
The paper provides a method to characterize all the free-schedules admitted by a workow specication, and an algorithm to derive them.
1 Introduction A workow is a complete or partial automation of a business process, in which participants (humans or machines) involve in a set of activities according to certain procedural rules and constraints.
The successful completion of the process often depends on the correct synchronization and scheduling of the activities.
The modeling and reasoning tools provided by this paper are intended to address so called production workows with particular emphasis on processes involving loosely coupled, largely  DSI, Universit a di Milano, Italy.
bettini@dsi.unimi.it y Dept.
of Info.& Software Systems Eng., George Mason  University, VA. fxywang, jajodiag@gmu.edu  y  X. Sean Wang,  y  Sushil Jajodia  distributed information sources where the agents cooperating in a workow process are essentially independent from each other.
We consider the inclusion in the workow specication of quantitative temporal constraints on the duration and distances of individual activities.
As a simple example, consider an online vendor workow, including the following activities that must be performed upon the receipt of an order by a customer: (a) order processing, (b) shipping, and (c) payment collection.
These activities have certain conditions concerning their timing that may impose temporal distances (possibly involving different time granularities).
For instance, the order processing must occur within one business day after the order is entered (and the whole workow process is enacted), the order must be transmitted to the shipping sites within ten hours after the end of order processing, and the payment for the merchandise must be made within a time window starting and ending one month before and after delivery, respectively.
The payment collection activity has a duration range (i.e., minimum and maximum time) specied in terms of business days, e.g., the activity can take as little as one business day and as much as 5 business days.
(These requirements are included in the graphical representation of Figure 2 later in the paper.)
The inclusion of temporal constraints naturally leads to questions about how to check the overall consistency of the specication and about how to apply some form of useful temporal reasoning; for example, how can we predict when a certain agent may be asked to perform an activity?
However, these questions can be addressed quite easily applying known techniques in constraint reasoning.
In this paper, we concentrate on a dierent issue, related to the enactment service of the workow system, which has to schedule the dierent activities in order to guarantee a successful completion of the workow.
This must be done considering all the constraints, and the fact that each activity is per-  formed by a relatively autonomous agent, which, in general, may take a dierent amount of time to complete the same activity.
We assume that each agent in the workow system declares a time range (the minimum and the maximum amount of time) it usually needs to nish a particular activity.
It is then desirable to allow the agents to take any amount of time within the declared time range to nish their work.
However, the time each agent actually takes may have some impact on the overall temporal constraints, because there may be constraints relating the ending times of activities.
The question is whether there exists a schedule for the activities' enactment such that, no matter when each agent nishes the activity within the declared time range (i.e., using any amount of time between the declared minimum and maximum), the overall temporal constraints are not violated.
We call this a free schedule.
The main contribution of this paper is a formal characterization of free schedules and an algorithmic method to derive them from the workow specication .
Little attention has been given in the literature to modeling advanced temporal features for workow systems.
Commercial workow systems (as reviewed, e.g., in [ADEM97]) are usually limited in specication of temporal conditions for each individual activity or for the global plan and do not provide temporal reasoning.
There are two recent papers on related issues.
The authors of [MO99] propose a framework for time modeling in production workows.
They also provide an eAcient algorithm to check if a constraint (like a deadline or inter-activity constraint) is \implied" by the given activity duration constraints and by the workow structure.
This is similar to checking in our framework that a free schedule retains its \free" property upon the addition of certain constraints.
However, it does not give a method to derive a free schedule starting with a global set of intra- and interactivity constraints.
The second paper, [EPR99], is concerned with temporal constraints reasoning and management in the context of workows.
Their temporal constraints form a subclass of those considered in this paper, due to the \well structured" requirement in [EPR99].
Their reasoning about the allowed \buer time" for parallel activities, is similar to deriving free schedules, even if using completely dierent methods.
Regarding the general scheduling problem, a rich literature exists on the subject (see [SC93] for a good survey).
However,  our method exploits the particular structure of the constraints, and, for this reason, it has much better computational properties than general purpose scheduling algorithms.
The paper is organized as follows: In the next section, we dene the workow model, and in Section 3, we formally characterize free-schedules and provide methods to derive them.
We conclude the paper in Section 4.
2 The workow model A typical workow management system provides a formalism and tools to specify workow activities.
We follow the consensus workow glossary [WfMC99] for the choice of operators to specify the workow structure.
The relation between activities can be specied by sequential routing, as well as through the use of the operators OR-split, AND-split, OR-join, and AND-join.
An OR-split identies a point within the workow where a single thread of control makes a decision upon which branch to take when encountered with multiple alternative workow branches, while an AND-split is a point where a single thread of control splits into two or more parallel activities.
OR-joins and AND-joins identify points where various branches re-converge to a single path.
Note that an OR-join, as opposed to the AND-join, does not require synchronization, since the ancestors activities are not executed in parallel but they are alternatives.
In this paper, we do not consider loop operators.
Our techniques can still be applied to workow processes involving loops when loops can be modeled as complex activities, estimating time bounds for their execution by the agents in charge.
A workow graphical description is shown in Figure 1.
An arrow between two activities denotes sequential routing, while arrows going through a circle denote OR-split operators (C in the circle denotes the associated condition).
AND-splits are implicit when more than one arrow originates from the same activity.
All joins are implicit when more than one arrow lead to the same activity, where an OR-join is intended when the arrows originate from alternative activities, and an AND-join when from parallel activities.
The dashed arrows mean that details on other activities in that part of the workow are omitted.
Our temporal extension allows a workow designer to include two types of temporal constraints: 2  Order Collection  OCe [0,1]b-day < <  OPb  OPe  Order Processing  < OR  [0,1]b-day  [1,10]hours  Bb  [1,10]hours  C  ISb  <  ISe  DSb  <  <  Be  DSe <  International Shipment  Domestic Shipment  Billing [1,2]b-day  [1,5]b-day  < LDb  Local Delivery  PCe [-1,1]month  PCb  Figure 2: Temporal constraints in a workow  Payment Collection  stands for the fOgrder fCgollection activity, OP for fOgrder fPgrocessing, etc.
The symbol `<' is used as a shortcut of [1; +1].
More than one constraint can be associated with an arc, with conjunction of them being the semantics.
For example, from Bb to Be, [0; 1] b-day forces the end of the Billing activity to occur in the same or next business-day as the beginning, while `<' forces the duration of the activity to be positive, since this is not enforced by the rst constraint.
Each workow constraint graph can be easily decomposed according to the OR-split operators in a set of subgraphs each one representing one possible workow execution thread.
In our running example, the graph in Figure 2 is decomposed into a subgraph taking the left branch of the OR-split (i.e., performing I S b and I S e), and another one taking the right branch (i.e., performing DS b and DS e).
Note that each subgraph denes a constraint problem commonly known as STP (Simple Temporal Problem).
When all constraints are in terms of the same granularity there are eAcient algorithms to check consistency and to derive the minimal network [DMP91].
For uniformity with the CSP literature we call these subgraphs constraint networks.
Figure 1: A workow process description (a) Constraints in each activity's description, as duration constraints and deadline constraints; (b) Constraints in the workow process description, as quantitative constraints on the starting/ending of dierent activities.
We represent these constraints as (binary) distance constraints Xj Xi 2 [m; n]G where Xj and Xi are variables representing the instant starting or ending an activity, m and n are either integers or one of the special symbols 1 and 1, respectively, and G denotes the time granularity in terms of which the distance is measured.
These constraints, known as TCG (Temporal Constraints with Granularity) have been formally studied in [BWJ98, BWJ97].
When G is omitted, the distance is intended in the units of the time domain.
A particular case is that of unary constraints, as Xj 2 [m; n]G, that can force the domain of variables to take values in a specic subset of the time domain.1 We use Dom(Xj ) to denote the domain of Xj .
The constraints informally introduced in Section 1 about our running example, are illustrated using a graph in Figure 2.
Each node in the graph is labeled by the initials of the activity's name followed by `b' for begin or `e' for end.
For example, OC 1 We  LDe  3 The generation of enactment schedules Every time the enactment agent dispatches an activity, it needs to provide a set of constraints on the  use positive integers as the time domain.
3  beginning and ending times to the agent in charge of that activity, such that if each agent satises the constraints, the whole workow can be successfully carried out.
To illustrate, consider a renement of the Domestic Shipment (DS) activity.
Upon completion of the order processing, the online vendor asks directly the suppliers of the requested products to ship them to one of its warehouses located in the area of the customer for their nal delivery.
Obviously, the workow requires some sort of synchronization to ensure that all the products will be delivered to the customer within a certain time to reduce the need of warehouse space.
In Figure 3 we consider the example of two activities S and S corresponding to the shipments to be made by two suppliers.
Each supplier has provided minimum and maximum bounds for the handling and shipping of its products (constraints [3, 7] and [1, 3] respectively).
The two activities must start after order processing and not later than 10 time units from it (constraints [1, 10]).
Also, the nal delivery must begin after all products are available and none of the products must wait more than 5 time units at the warehouse (constraints [1, 5]).
the 5 hours allowed by the constraint.
To solve this problem, we can delay the starting time for S to 11am.
0  3.1  Given the problem description in terms of a set of activities to be enacted, the minimum and maximum durations declared by each agent in charge of an activity, and other temporal constraints involving dierent activities, we want to provide to the enactment service a schedule for these activities.
For the sake of simplicity, in the rest of this paper we restrict our technical investigation to the case of graphs where all constraints are or can be converted in terms of a single granularity.
We formally dene the notion of a schedule.
0  Denition  A schedule for a set of activities A1 ; : : : ; Ak whose constraints are represented in a network N , is a set of triples of intervals on positive integers ([E begini ; Lbegini]; [mi ; ni ]; [E endi ; Lendi ]), one for each activity, such that: if for each activity Ai , a pair (xi ; yi ), with xi in [E begini ; Lbegini ], yi in [E endi ; Lendi ] and (xi ; yi ) satises the constraint [mi ; ni ], is used to instantiate the corresponding pair of beginning and ending nodes for Ai in N , then this assignment can be extended to a solution of N .
OPe [1,10]  [1,10]  Sb  Sab  The rst and last intervals in a schedule identify the allowed beginning and ending instants, respectively (L and E stand for Latest and E arliest, respectively), while the second interval identies the minimum and maximum duration, called the duration constraint.
Hence, if all the involved activities actually begin and end within the given bounds and do not violate the duration constraints, the schedule guarantees that all the constraints in the network are still satisable.
The motivation for this denition is that the agents responsible for the activities should be allowed to act independently from other activities.
Each agent needs only adhere to a \local" constraint and the global constraint should be satisable if each agent satises the local constraint attached to it.
As an example, assume OP e in Figure 3 happens at time 8.
Then the following is a schedule for activities S and S :  [1,3]  [3,7]  Se  Sae [1,5]  [1,5]  Schedules and Free Schedules  LDb  Figure 3: The portion of constraint graph involving the shipping activities S and S 0  In this case, the enactment service cannot assign S and S with the same earliest possible starting times.
Indeed, assume we are using granularity hours and suppose the order processing completed at 8am, and S and S are assigned to start at 9am.
According to duration constraints, activity S could take 7 hours to complete, while activity S could take 1 hour only.
Hence, the products shipped by h([9,9],[3,4],[12,13]),([9,9],[1,2],[10,11])i.
S will have to stand at the warehouse more than Indeed, since S nishes (i.e., S e) either at 12 or at 0  0  0  0  0  4  ([E begink ; Lbegink ]; [mk ; nk ]; [E endk ; Lendk ])i for the activities A1 ; : : : ; Ak within a constraint network N , is called free if (i) the duration constraint in the schedule is the same as that in N (i.e., mi = minDi and ni = maxDi ) for each activity Ai , and (ii) E endi = E begini + mi and Lendi = Lbegini + ni .
13, and S nishes (i.e., S e) either at 10 or 11, in all the possible cases, it's possible to assign LDb a value to satisfy all the constraints in the network.
We say that a schedule is well-formed if there are no \redundant" values in each triple.
Formally, a triple has no redundant values if for each value in the beginning/ending interval there exists one in the other interval such that the pair satises the duration constraint (i.e., the middle interval in the schedules).
Moreover, each value in the duration constraint range should be used by one of these pairs.
For example, ([9; 10]; [3; 4]; [12; 13]) is wellformed since (i) each value in the beginning domain can nd a value in the ending domain to satisfy the constraint (e.g., 9 nds 12 and 10 nds 13), (ii) each value in the ending domain can nd a value in the beginning domain to satisfy the constraint (e.g., 12 nds 9 and 13 nds 9), and (iii) each duration value (i.e., 3 and 4) can nd the beginning and ending pairs to have the exact durations (e.g., 9 and 12 and 9 and 13, respectively).
Note that in the constraint networks and schedules, constraints may involve +1 or 1 and domains may be innite.
For simplicity of presentation, we don't explicitly treat 1 in our exposition.
However, it's not diAcult to slightly extend the arithmetics to take them into account.
Many dierent schedules can exist for a given workow and dierent criteria can be adopted to identify most interesting ones.
When agents for activities operate independently, schedules which do not modify the original duration constraints that these agents declared in the workow specication, seem to be preferable to those which restrict those duration constraints.
We call these schedules free schedules.
If the restriction is unavoidable, a schedule which restricts the maximum duration, (called restricted due-time schedule) is preferable to one forcing a greater minimal duration (called bounded schedule).
In the following, we formally characterize free schedules, and concentrate on the procedure to generate them.
We assume for each activity Ai , the duration constraint from Ai b to Ai e in the network is [minDi ; maxDi ].
0  0  In this case the agent of the activity is just told the set of time instants at which it can begin and it simply has to meet the duration constraints as declared in the workow specication.
Referring to our running example and assuming the time for OP e is 8, we nd one of the free schedules is the one assigning 9 and 11 to the beginning of S and S , respectively.
The schedule can be represented as h([9,9],[3,7],[12,16]), ([11,11],[1,3],[12, 14])i.
This is clearly a schedule by denition.
It is also clear that this is a free one since the duration constraints [3; 7] and [1; 3] are as declared in the constraint network, and condition (ii) is easily veried to be true.
0  3.2  Finding free schedules  We propose a general algorithm for free schedule generation (FSG algorithm from now on) which consists of three main steps: 1. decompose the constraint graph according to OR-split operators and derive the minimal network for each resulting constraint network; 2. for each network characterize the set of free schedules for the given set of activities to be scheduled; 3. derive a particular free schedule from the result of Step 2 above, according to some criteria.
Deriving the minimal network in Step 1 can be easily done by applying a path-consistency algorithm since each constraint network denes an STP.
The minimal network guarantees that none of its constraints can be tightened without loosing a possible solution.
If the original duration constraints for the activities to be scheduled are tightened during Step 1, no free schedule can exist and the algorithm terminates.
Indeed, this will mean that one of the minimum (or maximum) duration declared by an agent cannot be used as part of any solution, i.e., if the activity really uses the minimum (or maximum) amount of time as declared,  Denition A free schedule is a schedule such that for each activity, the associated constraint only imposes a time window for the beginning of the activity, while the duration constraint is the original one in the network, and the ending interval is implicit.
Formally, a schedule h([E begin1; Lbegin1]; [m1 ; n1 ]; [E end1 ; Lend1 ]), .
.
.
, 5  then the whole network is not satisable, and some constraints sooner or later will be violated.
When the FSG algorithm is called for a schedule of the activities S and S in our example, Step 1 derives the implicit constraints and domains depicted in Figure 4.2 In this case, neither durations were tightened, i.e., they maintained the original values.
This means that the use of the minimum or maximum durations are not entirely ruled out by the network, and we can still hope to derive free schedules.
We now want to understand the properties of schedules in terms of sync-free intervals.
Lemma 2 Let S be a well-formed schedule for activities A1 ; : : : ; An in a network N , Ai and Aj be any two of these activities, I (Ai ) the beginning or ending interval in S for Ai , and similarly I (Aj ) for Aj .
Then, the intervals I (Ai ) and I (Aj ) are syncfree with respect to the temporal constraint between the corresponding nodes in N .
Conversely, given a minimal network N including the constraints among n activities, any set of triples ([E begini ; Lbegini]; [mi ; ni ]; [E endi ; Lendi ]) for i = 1 : : : n with the values in the rst and last intervals included in the domain of the corresponding node in N , and the second included in the corresponding constraint in N , is a schedule if the above sync-free property holds on each pair of beginning/ending intervals of dierent activities with respect to the corresponding constraint in N .
0  Dom={8} OPe [1,10]  [1,10]  Dom={[9,18]}  [-4,10]  Sb  Dom={[9,18]}  Sab [-1,11]  [1,3]  [3,7] [-3,7] Dom={[12,25]}  Se  Dom={[10,21]}  Sae  [-4,4]  If we look at the example of activities S and , and we consider free schedules having a single starting value, from Lemma 1 we see that the following conditions must be satised: xS xS  M in(T C (S b; S b)) xS xS  M ax(T C (S b; S b)) xS + minDS xS  M in(T C (S b; S e)) xS + maxDS xS  M ax(T C (S b; S e)) xS + minDS xS  M in(T C (S b; S e)) xS + maxDS xS  M ax(T C (S b; S e)) xS + minDS (xS + maxDS )  M in(T C (S e; S e)) xS + maxDS (xS +minDS )  M ax(T C (S e; S e)) where xS and xS are the starting values, T C (node1 ; node2 ) represents the fTgemporal fCgonstraint assigned to the arc from node1 to node2 , and M in(T C (node1 ; node2 )) and M ax(T C (node1 ; node2 )) represent the lower and upper bounds of the constraint, respectively.
A simple elaboration of these equations and its generalization to k activities provides the basis for Theorem 1.
[1,5]  [1,5]  S  LDb Dom={[13,26]}  0  0  0  Figure 4: The minimal network derived by pathconsistency  0  0 0  0  0  0  0  0  0  0  Step 2 is nontrivial and it is supported by a theoretical result which needs some preliminary observations.
0  0  0  Denition A pair of intervals I and I is called sync-free with respect to a constraint on the dis-  0  0  0  0  0  0  0  0  tance between elements of I and elements of I , if each pair (x; y ) with x 2 I and y 2 I satises the constraint.
0  0  For example, let I = [9; 10] and I = [11; 13].
Then I and I are sync-free with respect to the constraint [1; 4], but not sync-free with respect to [2; 4], nor [1; 3].
A simple test for sync-freeness is given by Theorem 1 Let A1 ; : : : ; Ak be activities to be enacted, and N a minimal network of constraints on Lemma le:sync.
0  0  these and possibly other activities endpoints.
The Lemma 1 A pair of intervals I = [minI ; maxI ], set of all free schedules for A1 , .
.
.
, Ak in N having a single value as the beginning interval is the set I = [minI ; maxI ] is sync-free with respect to conof schedules of the form ([xi ; xi ]; [minDi ; maxDi ]; straint [m; n] from I to I , if and only if: (minI [xi + minDi ; xi + maxDi ]) for each activity Ai , maxI )  m and (maxI minI )  n 2 For the sake of clarity, we do not depict the implicit where minDi and maxDi are the duration bounds constraints originating from the rst and last node since they for Ai in N , and the values x1 ; : : : ; xk satisfy the conditions: won't be used in the algorithms.
0  0  0  0  0  0  6    for each activity Aj , (  ( j ))  (  ( j ))  xj  M in Dom A b    Consider our running example, and, in particular, the minimal network as shown in Figure 4.
Applying Theorem 1, we obtain from the rst condition xS 2 [9; 18], xS 2 [9; 18], and, from the second condition, 2  xS xS  4.
These constraints dene a new network having only two nodes for S and S with the corresponding domains and a single arc from S to S labeled by the constraint [2; 4].
When the propagation algorithm is applied, the resulting minimal network is identical except that the domain of S has been tightened to [11,18].
Indeed, the value 9 and 10 cannot be part of any solution.
According to the procedure illustrated above, the earliest single-valued free schedule is obtained taking as starting instant xS and xS , the values 9 and 11, respectively, as they are the minimal values of the minimal domains in the network representing the solution space.
It is easily checked that it is a free schedule.
Note that this is exactly the example given at the end of Section 3.1.
M ax Dom A b    for each pair of activities hAi ; Aj i with i < j , xi + kij  xj  xi + Kij , where the constants kij and Kij are derived from the following:  0  0  0  = M ax((maxDi minDj + M in(T C (Ai e; Aj e))); M in(T C (Ai b; Aj b)); (M in(T C (Ai b; Aj e)) minDj ); (maxDi M ax(T C (Aj b; Ai e)))), and Kij = M in((minDi maxDj + M ax(T C (Ai e; Aj e))); M ax(T C (Ai b; Aj b)); (M ax(T C (Ai b; Aj e)) maxDj ); (minDi M in(T C (Aj b; Ai e)))).
ij  k  0  0  0  The rst condition in the theorem ensures that starting and ending values are included in their corresponding domains.
The second is derived from Lemma 1 as shown above, and it ensures that the constraints of the network will be satised independently from the specic amount of time taken by each activity within its duration constraint.
Note that all values (included minDi ; maxDi ) used in the calculations of the constant expressions are given by the minimal network computed in Step 1.
Hence, the theorem characterizes all the single-beginning-point free schedules in N , the minimized network.
As mentioned earlier, if the value minDi or maxDi in N are changed by the consistency algorithm, there does not exist any free schedule.
The theorem, however, is still useful since if the system of inequations has any solution, these solutions will give restricted due-time or bounded schedules, depending on whether the consistency algorithm has tightened only maximum duration bounds or some of the minimum ones.
The theorem also provides an eAcient way to nd a free schedule.
Indeed, the inequations given by Theorem 1 can be expressed in a new constraint network with n nodes, with domains of variables bound by the rst inequations and constraints given by the second ones.
We apply the consistency algorithm to this network obtaining a minimal network representing the schedule solution space.
If we take the minimal value from each domain, the resulting set of values is guaranteed to be a solution [DMP91], and, in terms of our problem, this solution provides the earliest free schedule.
Technically, \earliest" here means that this solution identies the point closest to the origin in the n-dimensional space representing all solutions.
Intuitively, it is a schedule with the earliest enactment times.
3.3  Optimizing free schedules  The schedule we have identied in the previous section is not necessarily \maximal", in the sense that we may extend some intervals in a free schedule to still retain the property of freeness.
Intuitively, the larger the intervals, the more relaxed the constraints are on activities.
An interesting problem is to nd maximal free schedules.
To do that, we only need to observe that any (high-dimensional) rectangular region in the solution space given in Theorem 1 yields a free schedule, and to nd a maximal free schedule is to nd the largest rectangular region.
It is easily seen that in some situations, there are many dierent maximal free schedules.
In certain cases, we may be interested in nding an \optimal" free schedule.
In these cases, we may rely on standard optimization algorithms to nd the \optimal" rectangular regions.
4 Conclusion In this paper, we investigated the enactment scheduling problem in the context of workow systems with autonomous agents and temporal constraints among the workow activities.
The notion of free schedule we have introduced is particularly powerful in this context, since, if such a schedule exists, the agents performing the workow activities are essentially free from the need of communicating with each other in order to successfully com7  plete the workow, satisfying the global temporal constraints.
Our scheduling algorithm should be integrated with the ones used by the workow enactment service at runtime, and it should be run each time the workow execution reaches an ANDsplit node, i.e., when parallel activities should be enacted.
We are extending this work in several directions.
One of them considers situations where a free schedule does not exist; intuitively, this is the case when duration bounds provided by agents have a wide range and/or when synchronization among activities is quite strict.
In these cases, the most reasonable schedules become restricted due-time schedules which impose a due-time to the agents in charge of the activities, by restricting the maximum duration.
The algorithm presented in the previous section must be extended to nd these schedules.
Another direction considers the derivation of schedules in the case of constraints in terms of different time granularities.
This becomes necessary when this kind of constraints are given in the specication, and the approximation introduced by a conversion into a common time unit is not acceptable by the workow application.
The results in [BWJ97, BWJ98] give some insight on the problems involved in the extension.
C. Bettini, X. Wang, and S. Jajodia.
Satisability of Quantitative Temporal Constraints with Multiple Granularities.
In Proc.
of 3rd Int.l Conf.
on  J. Eder, E. Panagos, and M. Rabinovich.
Time constraints in workow systems.
In Proc.
of 11th Int.l  [MO99]  O. Marjanovic and M.E.
Orlowska.
On Modeling and verication of Temporal constraints in Production Workows.
Knowledge And Information Systems, 1(2), 1999.
[SC93]  V. Suresh and D. Chaudhuri.
Dynamic scheduling: a survey of research.
In-  [WfMC99] Workow Management Coalition.
Terminology & Glossary.
Document Number WFMC-TC-1011.
1999. http://www.aiim.org/wfmc  (Special Issue on Cooperative Information Systems), 1997.
[BWJ97]  [EPR99]  ternational Journal of Production Economics, 32(1), 1993.
[ADEM97] G. Alonso, D. Agrawal, A. El Abbadi, and C. Mohan.
Functionalities and Limitations of Current Workow Management Systems.
In IEEE Expert  C. Bettini, X. Wang, and S. Jajodia.
A General Framework for Time Granularity and its Application to Temporal Reasoning.
Annals of Mathematics and Articial Intelligence, 22(1,2), 1998.
R. Dechter, I. Meiri, and J. Pearl.
Temporal constraint networks.
Articial Intelligence, 49, 1991.
Conf.
on Advanced Information Systems Enigneering, 1999.
References  [BWJ98]  [DMP91]  Principles and Practice of Constraint Programming, Springer-Verlag LNCS 1330, 1997.
8
(c) 2005 IEEE.
Personal use of this material is permitted.
However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from the IEEE.
Probabilistic Calculation of Execution Intervals for Workflows Johann Eder and Horst Pichler Institute for Informatics-Systems, University of Klagenfurt, Austria [eder|pichler]@uni-klu.ac.at  Abstract The comprehensive treatment of time and time constraints is crucial in designing and managing business processes.
Process managers need tools that help them predict execution durations, anticipate time problems, pro-actively avoid time constraint violations, and make decisions about the relative process priorities and timing constraints when significant or unexpected delays occur.
Variations of activity durations and branching decisions at run-time make it necessary that we treat time management in workflows in a probabilistic way.
Therefore we introduce the notion of probabilistic time management and discuss the application of this new concepts for workflow design as well as time aware, predictive and proactive workflow execution management.
1 Introduction Systems for business process automation, like workflow management or enterprise resource planning systems, are used to improve processes by automating tasks and getting the right information to the right place for a specific job function.
As automated business processes often span several enterprises, the most critical need in companies striving to become more competitive is a high quality of service, where the expected process execution time ranks among the most important quality measures [7].
Additionally it is a necessity to control the flow of information and work in a timely manner by using time-related restrictions, such as bounded execution durations and absolute deadlines, which are often associated with process activities and sub-processes [3].
However, arbitrary time restrictions and unexpected delays can lead to time vi-  olations, which typically increase the execution time and cost of business processes because they require some type of exception handling [9].
Although currently available commercial products offer sophisticated modelling tools for specifying and analyzing workflow processes, their time management functionality is still rudimentary and mostly restricted to monitoring of constraint violations and simulation for process reengineering purposes [1, 6].
In research several attempts have been made to provide solutions to advanced time management problems (e.g.
[1, 2, 6, 8]).
Most of them suffer from the vagueness of information which stems mainly from two aspects: The duration of a task can vary greatly without any possibility of the workflow system to know beforehand.
The second is that in a workflow different paths may be chosen with decisions taking place during the execution.
The main motivations for our probabilistic approach are: a) the improvement of estimations about the (remaining) duration of a workflow (predictive time management), and b) to make forecasts for the likelihood of deadline misses and automatically trigger escalation-avoiding actions if a possible future deadline violation has been detected (proactive time management).
Our calculation algorithms utilize the knowledge about the control flow of a workflow and stochastic information about the uncertainties mentioned above.
2 Timed Workflow Graph A workflow can be defined as directed acyclic graph, which is a collection of nodes and edges between nodes.
Edges determine the execution sequence of nodes, thus a successor can start if its predecessor(s)  Proceedings of the 12th International Symposium on Temporal Representation and Reasoning (TIME'05) 1530-1311/05 $20.00 (c) 2005 IEEE  Start  4 A  4 B  End  3 C  deadline=13  A  deadline  B.st B.d  A.d  B.d 0  4  1  2  3  4 B.eps  5  6 7 B.las  8 9 B.epe  10 11 B.lae  Start  C.d time 12  p = 0.3 AS  p = 0.7  9 B  1 C  p = 0.3 AJ  p = 0.7  2 D End  Figure 2.
Workflow with or-structure  13  Figure 1.
Implicit Time Properties  3 Probabilistic Timed Workflow Graph are finished.
A node can be of type activity, which corresponds to individual tasks of a business process, or a control node (e.g.
start, and-split, and-join, etc.).
Additionally time properties can be attached to each node.
Time properties are either explicit, if defined by the workflow designer, or implicit, if they follow implicitly from the workflows structure and explicit time properties [6].
We use a linear time model, where time is discrete with a universal predefined chronon, called basic time unit.
The time line starts at 0, which denotes the start time of the workflow.
All other points in time are declared or calculated relative to this start time.
Figure 1 visualizes a workflow consisting of three activities executed in sequence.
Explicit time properties are the estimated duration of activities in basic time units, which are A.d = 4, B.d = 4 and C.d = 3 and a deadline of d = 13, stating that the overall workflow execution must not exceed 13 time units.
Based on this information four implicit time properties can be calculated for each node (e.g.
for activity B): a) Considering the sum of durations of preceding activities the earliest possible start of activity B is B.eps = 4. b) The according earliest possible end is B.epe = B.eps + B.d = 8. c) To take the deadline of 13, into account, the point of view has to be reversed, now starting from the end of the workflow.
By subtracting the durations of succeeding activities from the deadline, the latest allowed end B.lae of activity B is determined as B.lae = 13 - 3 = 10.
That means if B ends at 10 it is still possible to reach the overall deadline of 13. d) Analogously the latest allowed start time is B.las = B.lae - B.d = 6.
This information can be utilized to specify a valid time interval for the execution of each activity, which ensures no time violations.
E.g.
we can state that B can not start before 4 and must end until 10 in order to hold the deadline.
Additionally we can state that the expected duration of the workflow is equal to the sum of all activity durations, which is equal to C.epe = 11.
The example presented above is rather simplistic, as some essential problems have not been addressed: a) The expected execution duration of a node is represented as a single average value (without variance, which is unusable for administrative workflows with human participants), b) in and-structures (parallel execution of nodes) the longest of all concurrently executed routes must be considered, and c) in or-structures (conditional execution of several alternative nodes) different paths may be chosen with decisions taking place during the execution, whose outcome we can not know when modelling the workflow.
Therefore, it is impossible to unambiguously determine implicit time constraints or the overall workflow duration, especially if the workflow-graph contains complex control structures.
To tackle these problems we introduced the probabilistic timed graph, which is an extension of the above presented basic model augmented with branching probabilities and time histograms.
Consider the graph in Fig.
2: The or-split after A forces a decision during the execution of the workflow.
One of two possible routes will be chosen, therefore it is impossible to calculate one scalar value for the earliest possible start time of D. According to the branching probabilities (expert estimations or extracted from the log) D will start at 13 with a probability of 30% or at 5 with a probability of 70%.
The same can analogously be stated for the latest allowed end time of A (as latest allowed times are calculated in a reversed fashion starting from the deadline defined on the last activity).
Additionally a workflow modeler will rather use distribution functions to represent activity durations than single scalar values.
Therefore we introduced the concept of a time histogram, which is defined as a set of tuples (p, t) where p is the probability and t is the according time value.
Time histograms are used to represent time properties in the form of probability distributions.
A graph where 2  Proceedings of the 12th International Symposium on Temporal Representation and Reasoning (TIME'05) 1530-1311/05 $20.00 (c) 2005 IEEE  1.0  without violating the overall deadline.
This switches the status to red (for further details we refer to [5]).
According to the new state different escalation actions can be invoked, e.g.
for orange this could be skipping unnecessary (optional) tasks.
At status red an early escalation of this workflow might be invoked which aims at avoiding useless resource consumption of future activities (see also [9]).
The important contribution of this approach is that threshold values can be expressed as the probability of a deadline violation.
green orange  0.5  T.lae  red  5  10  15  now  Figure 3.
Traffic light model for activity T  5 Conclusion each time property is represented by a time histogram is called Probabilistic Timed Workflow Graph.
Details about time histograms, how to cumulate, interpret and query them and how to calculate the probabilistic timed model, considering different types of control nodes, is explained in [5].
How to cope with large histograms by compressing them and how to deal with blocked loop structures is explained in [4].
Probabilistic time management will produce major advantages like better scheduling decisions and improved escalation strategies for workflow execution, as well as it provides the means to implement quality insurance components based on probabilistic time properties.
The integration of probabilistic time management applications into workflow environments are subject of ongoing research.
4 Application areas  References [1] C. Combi and G. Pozzi.
Temporal conceptual modelling of workflows.
LNCS 2813.
Springer, 2003.
[2] P. Dadam and M. Reichert.
The adept wfms project at the university of ulm.
In Proc.
of the 1st European Workshop on Workflow and Process Management (WPM'98).
Swiss Federal Institute of Technology (ETH), 1998.
[3] J. Eder and E. Panagos.
Managing Time in Workflow Systems.
Workflow Handbook 2001.
Future Strategies Inc. Publ.
in association with Workflow Management Coalition (WfMC), 2001.
[4] J. Eder and H. Pichler.
Duration Histograms for Workflow Systems.
In Proc.
of the Conf.
on Engineering Information Systems in the Internet Context 2002, Kluwer Academic Publishers, 2002.
[5] J. Eder and H. Pichler.
Probabilistic Workflow Management.
Technical report, Universitat Klagenfurt, Institut fur Informatik Systeme, 2005.
[6] J. Eder, E. Panagos, and M. Rabinovich.
Time constraints in workflow systems.
LNCS 1626.
Springer, 1999.
[7] M. Gillmann, G. Weikum, and W. Wonner.
Workflow management with service quality guarantees.
In Proc.
of the 2002 ACM SIGMOD Int.
Conf.
on Management of Data.
ACM Press, 2002.
[8] O. Marjanovic and M. Orlowska.
On modeling and verification of temporal constraints in production workflows.
Knowledge and Information Systems, 1(2), 1999.
[9] E. Panagos and M. Rabinovich.
Predictive workflow man-  We differ between predictive and proactive time management applications.
Predictive time management is used to provide users (customers) with predictions about a expected execution durations or the likelihood of coming activity assignments (scheduling forecasts).
Proactive time management tries to asses the current situation, corresponding to possible future time violations, e.g.
deadline misses.
Time histograms are the basis for simple but effective escalation warning mechanisms, using an adaption of the traffic light model introduced in [3]: Two thresholds must be defined on the histogram representing the latest allowed end time of an activity.
The first determines the workflows state change from green (ok) to yellow (warn) and the second one determines the state change from yellow to red (alarm).
As long as the workflows state is green everything is ok, if the state changes something has to happen.
Example: Assume that activity T just finished, at a point in time denoted by now = 12.
Figure 3 shows the descending cumulated time histogram for T.lae with two thresholds defined at 90% and 50%.
Applying a selection-operation on the histogram yields a probability of 30% that the workflow can be finished  agement.
In Proc.
of the 3rd Int.
Workshop on Next Generation Information Technologies and Systems, Neve Ilan, ISRAEL, 1997.
3 Proceedings of the 12th International Symposium on Temporal Representation and Reasoning (TIME'05) 1530-1311/05 $20.00 (c) 2005 IEEE
Localized Temporal Reasoning: A State-Based Approach Shieu-Hong Lin and Thomas Dean Department of Computer Science Brown University, Providence, RI 02912  Abstract  We are concerned with temporal reasoning problems where there is uncertainty about the order in which events occur.
The task of temporal reasoning is to derive an event sequence consistent with a given set of ordering constraints immediately following which one or more conditions have specified statuses.
Previous research shows that the associated decision problems are hard even for very restricted cases.
In this paper, we present a framework of localized temporal reasoning which use subgoals and abstraction to exploit structure in temporal ordering and causal interaction.
We investigate (1) locality regarding ordering constraints that group events hierarchically into sets called regions, and (2) locality regarding causal interactions among regions, which is characterized by subsets of the set of conditions.
The complexity for an instance of temporal reasoning is quantified by the sizes of the characteristic subsets of conditions and the numbers of child regions of individual regions in the region hierarchy.
This demonstrates the potential for substantial improvements in performance.
1 Introduction  We are interested in reasoning about a dynamical system whose state is modeled by a set of conditions P .
Each condition has a status, true or false, at a given point in time and the evolution of the system corresponds to a sequence of state changes.
The evolution of the system depends on the interactions among a set of events E .
An event e in E changes the system state according to a set of causal rules associated with e. A rule associated with an event e is a STRIPS-like operator describing the causal e ects of e when the preconditions of the rule are satisfied.
In this way, each event specifies a state-transition function on the state space determined by P 10].
The ordering of the events is uncertain.
The possible event sequences are determined by a given set of ordering constraints on  the events.
Generally, many event orderings are possible, each of which may result in a di erent evolution of the system.
Given the initial state and a set of goal conditions G , the task of temporal reachability 10] is to (1) detect the existence of a possible event sequence immediately following which the conditions in G have specified statuses, and (2) generate one such event sequence if one exists.
In other words, the task of temporal reachability is to predict the possibility of reaching the goal states where the conditions in G have the specified statuses, and report one possible event sequence which ends in the goal states.
For example, consider a partial plan where the ordering of the components is constrained by some partial order.
Using temporal reachability, we can coordinate the component events to reach the goal states in a way consistent with the inherent ordering constraints.
On the other hand, we can also apply temporal reachability to validate the partial plan by (1) specifying the undesirable states as goal states where the set of goal conditions have the specified statuses, (2) detecting the possibility of reaching undesirable states, and (3) reporting such a possible event sequence as evidence.
This research extends the earlier work of Dean and Boddy 5] on reasoning about partially ordered events.
In special cases, temporal reasoning with uncertainty about the ordering of events is harder than the corresponding planning problem 11].
This happens because in the corresponding planning problem, the planner is free to select an arbitrary set of event instances from a fixed set of event types with no restrictions on the ordering of event instances.
In most cases, if the events are not under the planner's control, however, the problems are computationally equivalent.
Previous research shows that the associated decision problems for temporal reachability 10] and planning are hard even for very restricted cases 1] 2] 3] 5] 6].
We have been trying to understand why temporal reachability and planning are so dicult and what, if any, structure might be extracted from such problems to expedite decision making and inference 10].
In this paper, we present a framework of localized temporal reasoning.
Using this framework, temporal  reachability is viewed as search in the global search spaces associated with individual problem instances.
The sizes of the global search spaces are determined by (1) the total number of the conditions in P , and (2) the total number of the events in E .
Our investigations have focused on the notions of locality in time and the structure of the search spaces.
Locality in time in a particular problem instance is modeled by a hierarchy of regions.
Each region is composed of an encapsulated event subset.
The events outside a region R must occur either before or after the events in R. We allows the child regions of a region to be partially ordered.
Given a hierarchy of regions, we can induce a set of coupling conditions and a set of abstract conditions for each individual region, which reect locality in the associated search space, and enable us to construct local search spaces.
The coupling conditions of a region R are the media for interregional interactions between the events in di erent child regions of R. The abstract conditions of a region R are the media for the interactions between the events in region R and the events not in R. Instead of search in the global search space, we can search the local search spaces and propagate the e ects of local search.
The sizes of the sets of coupling conditions and the numbers of child regions of individual regions determine the complexity for an instance of temporal reachability.
This research utilizes the notions of localized planning 9], subgoals 8], and abstraction 4] 7] 8] 12] 13] to expedite temporal reasoning.
The use of eventbased localized reasoning in planning has been advocated by Lansky in GEM 9].
In GEM, locality regarding domain behavior is also modeled by \regions".
\Regions" in GEM are composed of sets of interrelated events whose behaviors are delimited by event-based temporal logic constraints.
The GEMPLAN planner utilizes localized planning to exploit locality by subdividing the global search space into regional search spaces.
In this paper, we identify the corresponding notion of regions in the temporal reachability problem, which models locality in time.
In addition, we study the dependencies between conditions and regions, and identify the coupling conditions and the abstract conditions associated with individual regions.
These structures model locality in search spaces.
The state-based approach allows us to use knowledge regarding subgoals, abstract events, and local search spaces to conduct and analyze localized reasoning in a clear and elegant way.
The usefulness of subgoals, macro-operators, and abstraction in reducing the search e ort in planning has been investigated by Korf 8].
We identify subgoals and abstract events as useful knowledge for temporal reachability.
We develop techniques to derive this critical knowledge from individual temporal reachability instances.
Our techniques are di erent from previous work on abstraction 4] 7] 13] in that we transform encapsulated event subsets into abstract events, instead of  transforming individual operators into abstract operators.
2 The Temporal Reachability Problem  An instance of the temporal reachability problem is defined by hP  I  G  E  Oi.
 P is a set of conditions modeling the world state.
Each condition in P has a status, true or false, at a given point in time.
 I is a set of condition/status pairs, that specifies the initial statuses of each condition in P .
 G is a set of condition/status pairs, that specifies the goal statuses of all or a subset of the conditions in P .
 E is a set of events, each of whose causal effects are represented by a set of causal rules.
A causal rule r is a STRIPS-like operator, which describes the status changes of a subset of the conditions in P (i.e.
the consequent e ects) when the conditions in another subset of P have the statues specified by r (i.e.
the antecedent requirements).
In this way, each event determines a state-transition function on the state space modeled by P .
 O is a set of arbitrary constraints on E .
The task of temporal reachability is to (1) detect the existence of event sequences of size jEj consistent with O such that the goal specified by G is achieved immediately following the event sequences, and (2) generate one such event sequence if one exists.
Figure 1 describes an example problem instance.
 The world state is modeled by a set of conditions P = fa b c d e f g hg.
 The initial state is described by I .
All the conditions are initially true except that h is false.
 E = fe1  e2  e3 e4 e5  e6g.
Each event in E is associated with a causal rule while in general an event can be associated with multiple causal rules.
 Figure 2 depicts the ordering constraints on E .
(For notational convenience, we use W as an alias name for E .)
The events in the three event subsets X , Y , and Z must occur as three atomic groups, where X = fe1  e2g, Y = fe3  e4 g,Z = fe5 e6 g. The events in Y must occur before the events in X .
Event e1 must occur before event e2 .
Our task is to (1) determine the existence of event sequences consistent with O such that the goal G = f(a false) (b false) (d false) (e false) (h false)g is achieved immediately following the event sequences, and (2) generate one such event sequence if one exists.
3 Locality in Temporal Ordering Initial state:  {(a,true),(b,true),(c,true),(d,true), (e,true),(f,true),(g,true),(h,false)};  Goal : {(a,false),(b,false)(d,false),(e,false),(h,false)}.
W X  e1: If a=true and b=true, then c=true, a=false, b=false; otherwise, the statuses of all conditions remain unchanged.
e2: If a=true and c=true, then b=true, a=false, c=false; otherwise, the statuses of all conditions remain unchanged.
Y  e3 : If d=true and e=true, then f=true, d=false, e=false; otherwise, the statuses of all conditions remain unchanged.
e4 : If d=true and f=true, then e=true, d=false, f=false; otherwise, the statuses of all conditions remain unchanged.
Z  e5 : If a=false and d=true, then a=true, d=false, g=false; otherwise, the statuses of all conditions remain unchanged.
e6 : If a=true and d=false, then d=true, a=false, h=false; otherwise, the statuses of all conditions remain unchanged.
Figure 1: An instance of the temporal reachability problem  W  X e1  e2  Z e5  e3  Y  e4  e6  Figure 2: Ordering constraints on events  Temporal reachability turns out to be NP-Complete even if events are totally unordered (i.e.
O = ) and the associated state space is polynomial in the size of the event set (i.e.
jPj = O(log jEj)) 10].
This complexity result motivates our e ort to exploit inherent locality regarding event ordering and the dependencies among conditions and events.
When totally unordered, events can occur in arbitrary order.
However, there is locality in event ordering if the occurrences of events or subsets of events closely relate to one another.
In this paper, we consider locality in event ordering regarding hierarchical task networks.
We define a hierarchical task network as a partial plan such that  the plan is organized as a hierarchy of tasks, where an individual task may be composed of subtasks that are also tasks in the hierarchy,  the subtasks of an individual task may be constrained by some partial order, and  the leaf tasks of the task hierarchy are events, each of which changes the world states according to a set of associated causal rules.
Each causal rule is a STRIPS-like operator.
For example, we may be working on several programming tasks.
Each task is composed of a set of subtasks, including prototype design, prototype testing, problem reformulation, etc., such that each subtask involves a set of events that we wish to occur as a group.
We allow ourselves to switch between programming tasks, but, once we begin a subtask in a given programming task, we commit to completing all of the events associated with that subtask before switching to any other subtasks.
In the following, we define the concepts of regions , child regions , region hierarchy , and hierarchical ordering constraints.
Given a hierarchical task network, locality in event ordering can be abstractly modeled by a hierarchy of regions, and a set of hierarchical ordering constraints on the regions.
Denition 1.
Given a set of events E , an event sub-  set X of E is a region in E if the events in X occur as an atomic group with the events outside the region occurring either before or after the events inside the region.
E and each individual event in E are regions by themselves.
In a hierarchical task network, a region models the set of descendant events (i.e.
leaf tasks) of a task, which always occur as an atomic group.
In other words, each task is associated with a region.
Denition 2.
Given a set of events E , the regions in E form a region hierarchy E , either they are disjoint  of the other.
if, for any two regions in or one is a proper subset  W  Y  X  e1  e2  e3  {} {a,d}  W  Z  e4  e5  e6  X  Figure 3: A region hierarchy The events in a hierarchical task network form a region hierarchy, since they have the property described in Definition 2.  e1  {a,b,c} {a,b,c}  {a,b,c,d,e,f,g,h}  Local conditions  {a,d}  Subgoal conditions  {a} {a,b,c} {b,c}  {a,d} {a,d} {g,h}  {b,c}  {}  e2  Z  {a,b,c} {a,b,c}  {}  {}  {}  {}  Denition 3.
In a region hierarchy, region X is a  child region of another region Y if Y is the smallest region containing X .
is a 3-tuple hR  fii, where R is a region,  = fR1 R2 : : :g is the set of child regions of R, and fi is a partial order on .
The events in Ri must occur before the events in Rj if Ri fi Rj .
{a,d} {a,d}  e5  e6  {g} {g}  Y  {a,d} {a,d} {h} {h}  {d} {d,e,f} {e,f}  For a task k, the regions associated with k's subtasks are the child regions of the region associated with k.  Denition 4.
A hierarchical ordering constraint  Abstract conditions Coupling conditions  {e,f}  e3  {d,e,f} {d,e,f} {} {}  e4  {d,e,f} {d,e,f} {} {}  A hierarchical ordering constraint models a partial order on the subtasks of a task.
Figure 4: Characteristic condition subsets  In our example problem instance, S XS= fe1 e2g, Y = fe3  e4g, Z = fe5  e6g, W = X Y Z and every individual event are regions.
Figure 3 depicts the corresponding region hierarchy.
The possible event sequences regarding our example problem instance are determined by the following hierarchical ordering constraints: hX fe1  e2g fe1 fi e2 gi, hY fe3  e4g i, hZ fe5 e6g i, and hW fX Y Z g fY fi X gi.
dependent of R. If p is independent of R, we do not need to concern ourselves with p in reasoning about the changes caused by R. In Figure 1, the conditions a, b, c are dependent on events e1 , e2 and region X while the conditions d, e, f , g, h are independent of events e1 , e2 and region X .
4 Locality in Causal Interactions  In this section, we derive useful knowledge regarding the causal interactions among regions.
This knowledge allows us to describe subgoals and abstractions for individual regions, which yields a localized reasoning algorithm in next section.
Our investigation focuses on the causal dependencies among regions and subsets of conditions.
Events that a ect all or even most conditions are rare.
An event tends to a ect or be a ected by a small subset of the set of conditions P .
We say that a condition p is dependent on an event e if (1) event e may change the truth value of condition p immediately following e or (2) the truth value of condition p prior to event e may a ect the truth values of some other conditions immediately following e otherwise, we say that condition p is independent of event e. Similarly, a condition p is dependent on region R if p is dependent on at least one of the events encapsulated in R otherwise p is in-  4.1 Characteristic Condition Subsets  In the following, we generalize this notion of dependency to characterize subsets of conditions for each individual region R in a given problem instance, namely (1) the set of local conditions of R, (2) the set of subgoal conditions of R, (3) the set of abstract conditions of R, and (4) the set of coupling conditions of R. These subsets of conditions characterize the causal interactions among regions.
Figure 4 shows such knowledge derived from the example problem instance in Figure 1.
Local conditions: A condition p is a local condition of region R if p is dependent on R but not dependent on any events outside R. The statuses of the local conditions of R can only a ect and be a ected by the events in R. The initial statuses of the local conditions of R will not be changed until the events in R occur and change their statuses.
Subgoal conditions: The subgoal conditions of R describe the regional subgoal in R. A condition p is a subgoal condition of region R if (1) p is a local con-  dition of R, and (2) p is dependent on more than one child region of R if R has more than one child region.
As soon as all the events in R occur, the statuses of the subgoal conditions of R will not be changed any more.
At that time, the statuses of the subgoal conditions must have the specified conditions given by the goal of the problem instance.
This is the subgoal to be achieved by the events in R. For example, in Figure 1 the status of the subgoal condition d in region Y must be false immediately after region Y is finished.
Abstract conditions: We use the knowledge of the abstract conditions of a region R in deriving the abstraction of R. A condition p is an abstract condition  of region R if p is (1) dependent on at least one event in R, and also (2) dependent on at least one event not in R. Both the events in R and the events outside R can a ect and be a ected by the statuses of the abstract conditions of R. Therefore the set of abstract conditions of R is the media for inter-regional interactions between the events in R and the events outside R. For example, in Figure 1 the events e1 , e2 in region X interact with the other events through the abstract condition a of X .
Coupling conditions: In next section, we use the  knowledge of the coupling conditions of a region R in constructing the local search space for R.  A condition p is a coupling condition of region R if (1) p is an abstract condition of region R, or (2) p is an abstract condition of one or more child regions of R. The coupling conditions of R are the media for (1) the inter-regional interactions between the events in R and the events outside R, and (2) intra-regional interactions among di erent child regions of R. For example, in Figure 1 the three child regions X , Y , and Z of region W interact with one another through the coupling conditions a, d of W .
4.2 Subgoals and Abstractions  Based on the knowledge of the sets of subgoal conditions and abstract conditions of individual regions, we define the concepts of regional subgoals and the abstract events of individual regions, which yields the localized reasoning algorithm in Section 5.
4.2.1 Regional subgoals  The statuses of the subgoal conditions of R must have the specified statuses described by the goal as soon as the events in R have all occurred, and the statuses of the subgoal conditions of R will not be changed after that time.
Denition 5.
The regional subgoal GR is a subset of  the goal G .
GR is composed of the condition/status pairs in G whose condition component is a subgoal condition of region R.  Figure 5 depicts the regional subgoals regarding the example problem instance in Figure 1.
By achiev-  W Regional subgoal  {(a,false),(d,false)} X {(b,false)} e1 {}  Z  Y  e2 {}  {}  {(e,false)} e3 {}  e4 {}  e5 {}  e6 {(h,false)}  Figure 5: Subgoals in individual regions ing the subgoal in every individual region, the goal is achieved.
This describes a problem decomposition for temporal reachability.
Our task in an individual region R is to (1) achieve the subgoals in the children regions of R recursively, (2) determine the ordering of the child regions of R to achieve the regional subgoal of R. For example, the subgoal for Y is f(e false)g, which indicates that the status of condition e must be false immediately after the events in Y have all occurred.
he3  e4 i is the only ordering of e3 , e4 to achieve this subgoal.
4.2.2 The abstractions of individual regions  The e ects caused by the events in a region R are determined by (1) the initial statuses of the local conditions of R, (2) the statuses of the abstract conditions of R immediately before the events in R occur, and (3) the ordering of the events in R, which can be recursively described by the orderings on the child regions of R and R's descendant regions.
In general, there are many possible orderings on the child regions of R to achieve the subgoal in an individual region R. In order to achieve the subgoals in region R, (1) the set of abstract conditions of R must have appropriate statuses immediately before any events in R occur, and (2) the child regions in R must be properly ordered.
In other words, (1) whether or not an ordering on R's child regions may achieve the subgoal in R depends on the statuses of the abstract conditions of R immediately before the events in R occur, and (2) in turn, that ordering also a ects the statuses of the abstract conditions of R after achieving the subgoal in R. An abstract event eR of a region R provides a causal abstraction of R regarding how to achieve the subgoal in R using the events in R. Denition 6.
The abstract event eR of a region R is represented by a set of causal rules.
Each causal rule of eR encodes one possibility regarding (1) the statuses of the abstract conditions of R immediately before any events in R occur, which represent the antecedent requirements to achieve the subgoal, and (2)  W eW : If a=true and d=true, then a=false, d=false.
X e X: If a=true, then a=false.
Z  Y e Y : If d=true, then d=false.
eZ : If a=false and d=true, then a=true, d=false; If a=true and d=false, then d=true, a=false.
Figure 6: Abstract events for individual regions the statuses of the abstract conditions immediately after the regional subgoal is achieved by the events in R, which represent the consequent eects.
The abstract event eR is what we need to reason about the inter-regional interactions between the events in R and the events outside R. The derivation of abstract events for individual regions is investigated in Section 5.
Figure 6 depicts the abstract events of the individual regions in our example problem instance.
For example, the abstract event eY of region Y is associated with a single rule.
This is because (1) the subgoal for Y is f(e false)g, (2) when the condition d is true immediately before e3 and e4 occur, the event ordering he3  e4i can achieve the regional subgoal, and (3) when the condition d is false immediately before e3 and e4 occur, the subgoal cannot be achieved.
5 Localized Temporal Reasoning Using Subgoals and Abstractions  In this section, we first present temporal reachability as search in global search spaces.
We then investigate the use of local reasoning to expedite temporal reasoning by exploiting the structure of a region hierarchy.
Finally, we demonstrate the potential of substantial improvements in performance, which are quantified by the numbers of child regions and the sizes of the sets of coupling conditions of individual regions.
5.1 Temporal Reachability as Search  Given an instance of the temporal reachability problem hP  I  G  E  Oi, we describe the global search space as a directed graph G = (V A) where A node in V indicates (1) the state of the conditions in P , i.e., the status of p for each p in P , and and (2) for each event e in E , whether e has occurred.
An edge (u v) in A models the occurrence of an event e where (1) e can transform the statuses of the conditions in P from the state indicated by u to the state indicated  by v, (2) node u indicates that e has not yet occurred while node v indicates that e has occurred, and (3) e can occur immediately after those events that are marked as occurred at node u without violating the ordering constraints in O.
The task of temporal reachability can be viewed as search for a path from a root node u0 to any goal node t where (1) u0 is the node in which all events are marked as not yet occurred, and the conditions in P have the specified initial statuses given by I , and (2) a node t is a goal node if all events are indicated as occurred and the conditions have the statuses specified by the goal G .
Since an edge in G = (V A) models the occurrence of an event, such a path corresponds to a possible event sequence immediately following which the conditions have the statuses specified in G .
5.2 Local Reasoning in Local Search Spaces  In the following, we consider the kind of problem instances (hP  I  G  E  Oi) where the events in E form a region hierarchy according to a set of hierarchical ordering constraint O.
Instead of reasoning about (1) the whole set of events and (2) the whole set of conditions to achieve the goal, we can conduct local reasoning in each individual region about (1) the abstract events of their child regions and (2) the set of coupling conditions to achieve the regional subgoal.
The goal is attained by incrementally achieving the regional subgoals.
This yields the following localized temporal reasoning algorithms.
5.2.1 Constructing Local Search Spaces  For a region R, the local search space of R embeds the information regarding (1) how the child regions of R interact with one another through R's coupling conditions, and (2) the ordering constraints on the child regions of R. Given (1) the abstract events of region R's child regions and (2) the set of coupling conditions of R, we can construct R's local search space as a directed graph GR = (VR  AR ) in the following way.
 Construct a set of nodes VR such that each node in VR encodes one possibility regarding (1) the statuses of the set of coupling conditions, (2) for each child region R in R, whether the events in R as a whole have occurred.
 Construct an edge (u v) in AR if there exists a child region R of R such that (1) eR , the abstract event of R , can transform the statuses of the coupling conditions indicated by node u to the statuses indicated by node v while achieving the subgoal in R , (2) node u indicates that R has not yet occurred while node v indicates that R has occurred, and 0  0  0  0  0  0  0  0  (3) R can occur immediately after those child regions that are marked as occurred at node u without violating the ordering constraints on the child regions of R. (Note that edge (u v) models the occurrence of the events in the child region R as a whole.)
0  0  5.2.2 Deriving Abstract Events  Given the local search space GR = (VR  AR ) of a region R, we define the following two types of nodes in GR .
Type-I nodes: A node v in VR is a Type-I node if all child regions of R are marked as not yet occurred at v. Type-I nodes: A node v in VR is a Type-II node if (1) at v, all child regions of R are marked as occurred, and (2) at v, the subgoal conditions of R have the statuses specified by the regional subgoal of R. Type-I nodes represent the possible statuses of the coupling conditions of R immediately before any child region of R occurs.
Type-II nodes represent the possible statuses of the coupling conditions of R immediately after all child regions of R occur and the regional subgoal of R is attained.
Property 1.
The local search space GR of a region  R is a directed acyclic graph.
A path in GR from a Type-I node to a Type-II node represents an ordering of the child regions of R to achieve the regional subgoal in R.  Procedure Region-Abstraction Input: (1) the abstract events of the child regions of R, (2) the regional subgoal of R. Output: if the regional subgoal of R can be achieved,  report the abstract event eR  otherwise, stop and report failure in achieving the subgoal.
1.
Construct the local search space GR = (VR AR ).
Derive the reachability information between Type-I nodes and Type-II nodes by searching GR .
2.
If no Type-I nodes can reach Type-II nodes, stop and report failure.
3.
If a Type-I node u can reach a Type-II node v, we encode a rule r associated with the abstract event eR such that (i) the antecedent requirement of r is that the abstract conditions of region R must have the statuses indicated at node u, and (ii) the consequent e ect of rule R is that the abstract conditions of region R must have the statuses indicated at node v.  5.2.3 Achieving Regional Subgoals  For a region R, the following procedure generates a sequence of the events in R to achieve all the subgoals of R and R's descendant regions.
Procedure Generate-Sequence Input: (1) the statuses of R's coupling conditions  before the events in R occur, (2) the abstract events and the subgoals of R and R's descendant regions.
Output: a sequence of the events in R to achieve the subgoals of R and R's descendant regions.
1.
Search the local search space GR for a path from u to v where (i) u is a Type-I node in which R's coupling conditions have the specified initial statuses and, (ii) v is a Type-II node.
2.
According to the derived path (by Property 1), derive (i) an ordering of the child regions of R, (ii) for each child region R of R, the statuses of the coupling conditions of R immediately before the events in R occur.
3.
For each child region R of R, recursively calls procedure Generate-Sequence to generate a sequence of the events in R to achieves the regional subgoals of R and the child regions of R .
According to the derived ordering of R's child regions, concatenate these sequences to generate a sequence of the events in R. 0 0  0  0  0  0  0  5.2.4 A Localized Temporal Reasoning Algorithm  In the following, we present a localized temporal reasoning algorithm for temporal reachability.
Procedure Localized-Reasoning input: a problem instance hP  I  G  E  Oi where the events in E form a region hierarchy.
output: if there exist possible event sequences to  achieve the goal, report one such sequence otherwise, report failure in finding such a sequence.
1.
Derive the knowledge regarding the sets of abstract conditions, coupling conditions, and subgoals of individual regions.
2.
Starting from the bottom level of the region hierarchy, we conduct local reasoning described in Step 3 for the regions at the same level respectively, and then proceed in the same way level by level until we finish the local reasoning in the root region of the region hierarchy.
3.
For each individual region R, call procedure Region-Abstraction to derive the abstract event eR to achieve the regional subgoal of R. If the regional subgoal can not be achieved, stop and report failure otherwise, propagate the knowledge of eR to R's parent region.
4.
In the root region, according to the initial statuses of the coupling conditions call procedure GenerateSequence to generate a solution event sequence.
5.2.5 Quantifying the Computation Eciency Theorem 1.
The time complexityPof the2(Blocalized +C ) temporal reasoning algorithm is O( R 2 R R ) where BR and CR are the number of the child regions and the number of the coupling conditions of an individual region R.  Proof (Sketch).
The local search space GR =  (VR  AR ) is a directed acyclic graph of size O(2BR +CR ).
To search the graph, record and process the reachability information, it takes O(22(BR +CR ) ) time and space.
2 The localized reasoning algorithm is a polynomialtime algorithm if BR and CR are of O(log jEj) size for each individual region R. Since temporal reachability is NP-Complete 10], this demonstrates the potential for dramatic improvements in performance by exploiting inherent locality, which happens when the numbers of child regions (BR 's) and the sizes of the sets of coupling conditions (CR 's) in individual regions are small with respect to the total number of events jEj and the total number of conditions jPj.
The performance degrades to be exponential in jEj when some BR or CR is of O(jEj) size.
This is under our expectation, since (1) a set of totally unordered events corresponds to a single region containing the individual events as child regions, and (2) temporal reachability regarding totally unordered events is NPcomplete 10].
5.2.6 An example.
We illustrate the use of the localized reasoning algorithm in solving our example problem instance in Figure 1.
First, we derive causal knowledge and the subgoals for individual regions depicted in Figure 4 and Figure 5 respectively.
At the bottom level, we conduct local reasoning regarding the six events respectively, which are regions by themselves.
The subgoal in e6 is that h must be false after e6 occurs, while we have null subgoals for the other five events.
The subgoal in e6 is always achieved, since e6 can only make h false and h is false initially.
At the second level of the region hierarchy, we conduct local reasoning regarding the regions X , Y and Z respectively.
The subgoal in region X (Y ) is that b (e) must be true (false) after all the events in region X (Y ) occur, while we have null subgoal in region Z .
We call procedure Region-Abstraction to determine the abstract event eX (eY and eZ respectively).
At the top level, the subgoal in regionW is to make the conditions a and d true after all events occur.
Here the child regions X , Y and Z are treated as the abstract events eX , eY , eZ respectively.
We call procedure Region-Abstraction to determine eW .
Finally, we call procedure Generate-Sequence to derive a solution sequence, given that the coupling conditions a and d of region W are initial true.
Procedure Generate-Sequence recursively generates the ordering heZ  eY  eX i, he5  e6i, he1  e2 i, and he3  e4i for the regions W , Z , Y , X respectively.
We replace eZ ,eY and eX in heZ  eY  eX i with he5  e6 i, he1  e2i, he3  e4i respectively, and derive he5  e6 e1 e2  e3 e4 i as a sequence to achieve the goal.
6 Conclusion  We investigate the locality in (1) the ordering constraints that group events hierarchically into sets called regions, and (2) the dependencies among conditions and regions.
This enables us to describe subgoals and abstractions for individual regions.
We develop a localized temporal reasoning algorithm to exploit locality and demonstrate the potential for dramatic improvements in performance.
References  1] Backstrom, C. and Klein, I., Parallel Non-Binary Planning in Polynomial Time, Proceedings IJCAI 12, Sydney, Australia, IJCAII, 1991, 268{ 273.
2] Bylander, Tom, Complexity Results for Planning, Proceedings IJCAI 12, Sydney, Australia, IJCAII, 1991, 274{279.
3] Chapman, David, Planning for Conjunctive Goals, Articial Intelligence, 32 (1987) 333{377.
4] Christensen, J., A Hierarchical Planner that Generates its own Abstraction Hierarchies, Proceedings AAAI-90, AAAI, 1990, 1004{1009.
5] Dean, Thomas and Boddy, Mark, Reasoning About Partially Ordered Events, Articial Intelligence, 36(3) (1988) 375{399.
6] Gupta, Naresh and Nau, Dana S., Complexity Results for Blocks-World Planning, Proceedings AAAI-91, Anaheim, California, AAAI, 1991, 629{633.
7] Knoblock, Craig A., Search Reduction in Hierarchical Problem Solving, Proceedings AAAI-91, Anaheim, California, AAAI, 1991, 686{691.
8] Korf, Richard, Planning as Search: A Quantitative Approach, Articial Intelligence, 33(1) (1987) 65{88.
9] Lansky, Amy L., Localized Event-Based Reasoning for Multiagent Domains, Computational Intelligence, 4(4) (1988).
10] Lin, Shieu-Hong and Dean, Thomas, Exploiting Locality in Temporal Reasoning, Proceedings of the Second European Workshop on Planning, Vadstena, Sweden, 1993.
11] Nebel, Bernhard and Backstrom, Christer, On the Computational Complexity of Temporal Projection, Planning, and Plan Validation, Articial Intelligence, (1993), To appear.
12] Sacerdoti, Earl, Planning in a Hierarchy of Abstraction Spaces, Articial Intelligence, 7 (1974) 231{272.
13] Yang, Qiang and Tenenberg, Josh D., Abtweak: Abstracting a Nonlinear, Least Commitment Planner, Proceedings AAAI-90, AAAI, 1990, 204{209.
Processing Disjunctions of Temporal Constraints E. Schwalb  R.Dechter  Information and Computer Science Dept.
University of California, Irvine Abstract  All these tasks are known to be NP-complete [4, lo].
The source of complexity stems from allowing disjunctive relationships between pairs of variables.
Disjunctive constraints often arise in scheduling and planning applications.
As an example of a disjunctive relation, consider the constraint specifying that the time it takes to ship cargo depends on whether it is shipped by air or ground transports.
This paper describes new algorithms for processing quantitative Temporal Constraint Satisfaction Problems (TCSP).
I n contrast to discrete CSPs, enforcing path-consistency on TCSPs as exponential due to the fragmentaiton problem.
We present an eficient polynomial constraint propagation algorithms, called Loose Path Consistency, which is shown to improve the performance backtrack search algorithms by orders of magnitude.
The tradeoffs between the eflectiveness and eficiency of L P C are analyzed.
We report the presence of a phase transition on this domain and perform the empirical evaluation on problems which die in the transition region.
1  Example 1 : A large cargo needs to be delivered from New York to Los Angeles within 8-10 days.
From New York to Chicago the delivery requires 1-2 days by air or 10-11 days on the ground.
From Chicago to L.A. the delivery requires 3-4 days by air o r 13-15 days on the ground.
Given the above constraints, we are interested in answering questions such as: "are these constraints satisfiable ?)'
or "when should the cargo be in Dallas ?"
or "can the cargo arrive in L.A. on Jan 8-9 ?".
The model of Temporal Constraint Satisfaction Problems (TCSP) provides a representation with which such questions can be answered.
Introduction  Problems involving temporal constraints arise in various areas such as scheduling [12] and planning with temporal databases [3, 141.
Several formalisms for expressing and reasoning about temporal constraints have been proposed, most notably, Allen's interval algebra l], Vilain, Kautz and van Beek's point algebra [15 , Dean's Time Map Management [ 3 ] ,Dechter, Meiri and Pearl's Temporal Constraint Satisfaction Problems (TCSP) [4] and Meiri's combined model of quantitative and qualitative networks [lo].
Improved algorithms for processing Allen's interval algebra were presented in [9].
Here we extend the work on TCSPs by providing a new algorithm which improves on the algorithms presented in [4,131.
In this paper, we present a new polynomial approximation algorithm for processing quantitative constraint networks, which is extended to process combined qualitative and quantitative constraints.
In the reminder of this paper we use TCSP to refer to the combined qualitative and quantitative model.
i  The central task in constraint processing is to decide consistency of a set of constraints.
Since deciding consistency is intractable, it is common to use approximation algorithms such as path-consistency.
In contrast to discrete CSPs, enforcing path-consistency on TCSPs may be exponential, as noted in [4] and observed empirically in [13].
This motivated a polynomial algorithm for approximating path-consistency] called Upper-Lower-Tightening (ULT).
Here we identify the cause of the exponential behavior and present an improved algorithm called Loose Path-Consistency (LPC).
For randomly generated problems, a phase transition from easy to hard is reported on 3-CNF formulas in [2, 111.
It is observed that the most difficult problems lie in the transition region.
Here we show that randomly generated TCSPs also exhibit a phase transition.
The empirical evaluation is performed on relatively hard problems which lie in the phase transition.
We compare our new algorithm, LPC, and the previously proposed algorithm ULT, with respect to efficiency and effectiveness.
By effectiveness we refer to the ability to detect inconsistencies and by efficiency we refer to the execution time.
We show that LPC is significantly more effective than ULT in detecting in-  The TCSP model facilitates the following tasks: 1.
Finding one or more consistent scenarios.
2.
Finding all feasible times at which a given event can occur.
3 .
Finding all relationships between two given events.
30 0-8186-7528/96 $5.00 0 1996 IEEE  -  consistencies and improving performance of backtrack search by orders of magnitude.
2  Algorithm PC 1.
Q {(i, k , j ) I (i < j ) and (k. # 4.i) 1 2. while Q # { } do 3. select and delete a path ( i , k , j ) from Q 4. if Tz, # @Tk, then 5.
T,, T,, n 8 T~,) 6. if T6,= { } then exit (inconsistency) 7.
Q - Q U { ( i , k , i )I V k # i , ~ l 8. end-if 9. end-while  Processing Constraints  zk  A Temporal Constraint Satisfaction Problem (TCSP) consists of a set of variables XI, .
.
.
, X,, having rational domains, each representing a time point.
Each constraint C is a set of intervals  +  (zk  Figure 1: Algorithm PC.
A unary constraint Ci restricts the domain of the variable X i to the given set of intervals  I ,241  A binary constraint Cij over X i , X j restricts the permissible values for the distance X j - X i ; it represents the disjunction  I0.221 W.331 W.501  (2)  cjj  [1,31 [11.131 [17.191 121.22] m.231 E7291 W .361 I37.391 f44461  [1,221 L23.291  W.461  W.W  (b)  ( C)  Figure 2: The fragmentation problem.
Sf ( U 1 5 xj-xi 5 bl)U.. .u(a, 5 xj-xi 5 b,).
the edges represent a disjunctive constraint, namely the interval label [ a l ,bl] [az,621 [as,b3] representin the constraint X j - X i E [ul, bl] U [uz, b 2 ] U [u3,63f Enforcing path consistency on the network in Figure 3a results in increasing the number of intervals of TI3 from 3 to 10 as shown in Figure 3b.
Clearly, if this pattern is repeated throughout the network, the computation may become exponential in the number of triangles processed.
This was observed empirically in ~31.
All intervals are assumed to be open and pairwise disjoint.
A tuple X = (21,. .
.,E,) is called a solution if the assignment X1 = 21,. .
.
,X , = 2, satisfies all the constraints.
The network is consistent iff at least one solution exists.
Definition 1 : [ composition ] Let T = { 1 1 , I z , .
.
.,IT}and S = { J l , J z , .
..,Js}be two constraints.
The compositron of T and S , denoted by T 8 S , admits only values r for which there exists t E T and s E S such that r = t s.  3  +  Algorithm LPC  h  In algorithm Loose Path-Consastency LPC) trol the fragmentation by replacing t e intersection we conoperator n with the loose intersection operator a.
2.1 Path-Consistency and Fragmentation A commonly used constraint propagation algois rithm enforces path-consistency.
A constraint path-consistent iff zj C nvk(zk @ T k j ) and a network is path-consistent iff all its constraints are pathconsistent.
Path-consistency can be enforced by applying the operator zj +- zj n (Til,8 T k j ) until a fixed point is reached.
z,  Definition 2 : The loose intersection, T a S consists of the intervals { I ; , .
.
.,I;} such that V i I: = [L,,Ui] where [hi,Ui] are the lower and upper bounds of the intersection n S.  The number of intervals in the constraint between X i and X j , denoted E l , is not increased by the operation zj c zj a ( z k 8 T k j ) .
Note that the a operator is asymmetric, namely T a S # S a T .
Lemma 1 : [4] Algorithm PC computes a pathconsastent network and termanates an O ( n 3 R 3 ) , where R as the range of the quantitative constraints.
Algorithm DPC terminates an O(n3R2)steps.
Example 2 : Let T = {[1,4],[10,15]} and S = {[3,11], [14,19]}.
Then T Q S = {[3,4], [lo, 15]}, S a T = {[3,11 while S n T = {[3,41, [lo, 111,[14,151  As we enforce path-consistency on quantitative TCSPs, some intervals may be broken into several smaller subintervals and the number of intervals per constraint (i.e.
disjunction size) may increase.
This may result in exponential blowup (bound by O(n3R3)).This is called a fragmentation problem.
Consider the 3 variable network in Figure 3.
Each node represents a point variable, and the intervals on  Algorithm LPC is presented in Figure 3.
The network N' is a relaxation of N and therefore loosely intersecting it with N results in an equivalent network.
At every iteration if LPC (except the first and the last) at least one interval is removed.
This allows us to conclude that:  31  Algorithm Loose Path-Consistency (LPC) 1. input: N 2.
N " + N 3. repeat 4.
N t N" 5.
Compute N ' by assigning TZ3 = nvk(Cik8 C k j ) , for all i , j .
6.
Compute N " by loosely intersecting T/j = Ci, d T i j , for all i , j .
7 .
until 3 i , j (T:; = 4) ; inconsistency, or or V z , j lT/il= lCi31 ; no interval removed.
8. if 3 i , j (Ti; = 4) then output "inconsistent."
else output: N " .
Figure 3: The Loose Path-Consistency (LPC) algorithm.
Lemma 2 : Algorithm LPC (see Figure 2) computes a network which is equivalent to the input network and terminates in O(n3k3e),where n is the number of variables, k is the maximum number of disjuncts per constraint and e is the total number of constraints.
Polynomid  Example 3 : Consider the constraints: XI  -  xo  Xz - X i XB- Xo Xg - XI Xg - X2  E E E  E E  [lo, 201 U [loo, 1101 [20,40] U [loo, 1301 [80,100] U [150,160] U [180,190] [30,40] U [130,150] [50,70] U [110,120] U [130,140] U [160,190]  Figure 4: The partial order on the effectiveness of the variants of LPC.
algorithms form a spectrum of sound but incomplete propagation algorithms.
After 3 iterations, algorithm LPC terminates with the network: x 1  -xo  X2 - X o X2  -XI  XJ -Xo Xg -Xi x 3  -xz  E E E E E E  4  [l0,20] [30,50] [20,30] [150,160] [130,140] [110,120]  Empirical Evaluation  Algorithm ULT was the first algorithm psoposed for processing disjunctions in TCSPs [13].
This section is organized as follows: In section 4.1, we determine relative efficiency and effectiveness of algorithm ULT, algorithm LPC and its variants DLPC and PLPC.
By effectiveness we refer to the ability to detect inconsistencies and by efficiency we refer to the execution time.
In section 4.2 we examine the effectiveness of LPC and ULT as a forward checking procedure within backtracking search.
The empirical evaluation of these search methods is performed on relatively hard problems which lie in the phase transition.
Problems were generated with the following parameters: n and e are the number of variables and constraints, and k is the number of intervals per quantitative point-point constraint.
These quantitative constraints specify integers in [-R, RI, and the tightness Q of a constraint T = { I l , .
.
.
, I k } is (1111+.. .
+ l I k l ) / 2 R where is the size of Ii; we used uniform tightness for all constraints.
4.1 Comparing LPC and ULT We compare the effectiveness of incomplete constraint propagation algorithms by counting the fraction of cases in which the weaker algorithm detected inconsistency provided that the stronger algorithm also detected inconsistency (recall Figure 4).
PC may  3.1 Variants of LPC A variant of algorithm LPC can be obtained by replacing, in algorithm PC (Figure l),the intersection n operator with loose-intersection Q.
Another variant not presented here is called Directional LPC (DLPC .
To refine the tradeoff between effectiveness and e ciency, we restrict the algorithm to induce constraints from only a subset of the triangles, where at least one of the constraints was non-universal in the input network.
The intuition being that second level constraint recording is relatively week.
Algorithm Partial LPC (PLPC) approximates LPC by applying the relaxation and operation l& +- zj a ( z k 8 T k j ) only in case at least one of z k , T k j is non-universal in the input network.
The partial order between the different constraint propagation algorithms we experimented with is presented in Figure 7.
A directed edge from algorithm AI to A2 indicates that A2 computes a tighter network than .AI on an instance by instance basis, and that its complexity is higher accordingly.
Together these  nl  32  # Op.
#  Acc of  Acc of  PLPC  ULT  LPC  PLPC  150 200  98%  90%  25h'  12K  5h'  99%  27K  17K  8K  250 300 350 400  100% 100% 100% 100%  15% 45% 77% 94% 100%  14K 9h7 7h' 6K  11K  lOI< 8K 7K 6K  #  Op.
8K 7K 6K ~~~  0.546 0.623 0.380 0.287 0.244 0.211  Time PLPC 0.400 0.533 0.350 0.275 0.241 0.212  Time DLPC  Time ULT  0.165  0.132 0.162 0.181 0.164 0.126 0.105  0.259 0.315 0.270 0.235 0.204  ~~  or ULT.
Thereafter, on the tightened network, we re-  be exponential in the number of intervals per constraint in the input network while ULT's execution time is almost constant in the number of intervals.
Nevertheless, ULT is able to detect inconsistency in about 70% of the cases in which PC does [13].
Here we demonstrate that algorithm LPC is capable of detecting even more inconsistencies than ULT.
Therefore, we show LPC computes a better approximation to PC than ULT.
We compare the relative effectiveness and efficiency of algorithms LPC, DLPC, PLPC and ULT.
The results are presented in Table 1.
The columns labeled "Acc < alg >" specify the accuracy of algorithm < alg > relative to LPC, namely fraction of cases algorithm < alg > detected inconsistency given that LPC did.
The columns labeled ''# Op < alg >" describe the number of revision operations made by algorithm < alg >.
The basic revision operation of LPC @ T k j ) , while for ULT we use the is xj +- rj a (3:ll~ relaxation operation given in [13].
This measure is machine and implementation independent, unlike execution time.
The problems generated have 32 variables and constraint tightness of 45%.
Each entry represents the average of 200 instances.
From Table 1 we can conclude that (1) LPC is more effective yet less efficient than UL,T, and (2) PLPC is almost as effective as LPC yet more efficient than LPC.
We conclude that on our benchmarks, algorithm PLPC is the best.
Therefore, in the rest of this paper, whenever we mention LPC we refer to PLPC.
4.2  # Op.
Time DLPC LPC  of  Consts  peat selecting and testing consistency until either inconsistency is detected or a solution is found.
When inconsistency is detected, a dead-end is encountered and the search algorithm backtracks undoing the last constraint labeling.
Constraint propagation algorithms can also be used as a preprocessing phase before backtracking, to reduce the number of dead-ends encountered during search.
After preprocessing with algorithm PC, problems became even harder to solve, due to the increased fragmentation.
In contrast, preprocessing with ULT results in problems on which naive backtrack search is manageable [13].
This algorithms is called "OldBacktrack+ULT"; it was used as our reference point.
In this section, we compare three backtrack search algorithms: (1) "Old-Backtrack+ULT" which uses ULT as a preprocessing phase but no forward checking is used; (2) "ULT-BacktrackSULT" which uses U L T both as a preprocessing phase before backtracking and as a forward checking procedure during the search; (3) "LPC-Backtrack+LPC" which uses LPC both as a preprocessing phase before backtracking and as a forward checking procedure during the search.
In Figure 5 we report an exponential increase in number of dead-ends and execution time at certain regions.
This region, where about half of the problems are satisfiable, is called the transition region [2, 111.
In Figures 5a, 5b we observe a phase-transition when varying the size of the network while in Figures 5c, 5d we observe a similar phenomenon when varying the tightness of the constraints.
Some theoretical insights into this phenomenon in discrete CSPs can be found in [16].
As observed in Figure 6, ULT and LPC are capable of pruning dead-ends and improving search efficiency on our benchmarks by orders of magnitude.
In particular, Old-BacktrackSULT is about 1000 times slower than ULT-Backtrack+ULT, which is about 1000 times slower than LPC-BacktrackSLPC.
The latter encounters about 20 dead-ends on the peak (worst performance) on networks with 12 variables and 66 constraints with 3 intervals to instantiate each (see Figure 10).
Backtracking  To find a solution to a given TCSP, we use a backtracking search algorithm.
A solution of a TCSP is a selection of a single interval or relation from each constraint, called constraint labeling.
Consistency of such a constraint labeling is decided by enforcing pathconsistency.
In the first part of this section we reports results of experiments performed on quantitative TCSPs, while in the second part we focus on qualitative networks.
The experiments were performed with a backtrack search algorithm which uses the constraint propagation algorithms presented above (recall Figure 7) as a forward checking procedure.
Given a TCSP, a single interval or relation is selected from a disjunctive constraint and consistency is tested using either PLPC  33  The faction of consistent instances for complete graphs of different sues.
The ditsculty of various sizes as measured using the ULT-Backtrack algorithm.
-  1000  4  3  100  v.r  10  1  i?
@DAss44%  L  P  2 1  5  15  10  20  5  15  10  Number of V 'ables  taT \  I  The difficulty as tightness is constant.
Difficulty vs Tightness for 10,12,14,16vars, complete graphs, 3 irntervals, 500 reps, for IULT-Backtrack + LPC preprocessing.
60  40  20  0  80  Phase transition for 10,12 variabies, 45,66 constraints, 3 intervals, 500 reps.  0  1M)  20  Ti h ess  40  60  80  100  Ti"h"Tq  f c'f The difficulty as a function of tightness.
Figure 5 Comparing Backtracking Algorithms foi Quantitative Point-Point Networks, 12 vars, 66 consts, 3 intervals, 500 reps.  l ~ o ~  10 7 1 The t m e for ULT-Backtrack + ULT prep at the peak was about 100seconds.
The t m e for LE-Backtrack + LF'C prep at the peak was about 1.5 seconds.
1000 1  4  loo.
10 10  I;  .
10  20  40  60  Tightness  so  0  100  .
20  .
40  .
60  Tightness  Figure 6: A comparison of various backtracking algorithms.
34  '  80  4  100  5  Conclusion  References Allen, J.F., 1983.
Maintaining knowledge about temporal intervals, CACM 26 (11): 832-843.
1991.
Cheesman, P, Kanefsky, B., Taylor, W., Where the Really Hard Problems Are.
Proc.
of IJCAI-91, 163-169.
Dean, T.M., McDermott, D. V., 1987.
Temporal data base management, Artificial Intelligence 32 (1987) 1-55.
Dechter, R., Meiri, I., Pearl, J., 1991.
Temporal Constraint Satisfaction Problems, ArtiJiczal Intelligence 49(1991) 61-95.
We discuss processing combined qualitative and quantitative Temporal Constraint Satisfaction Problems (TCSP).
Using relatively hard problems which lie in the transition region we evaluate the effectiveness of algorithm LPC and its variants and show that they improve efficiency of backtrack search by orders of magnitude, even on small problems.
We identify a fragmentation problem which explains, in contrast to discrete Constraint Satisfaction Problems (CSP , why enforcing path-consistency on quantitative T SPs is exponential.
Identifying this problem allows us to design an efficient yet effective polynomial algorithm for processing TCSPs called Loose Path-Consistency (LPC), and its variants Directional LPC (DLPC) and Partial LPC (PLPC).
d  Freuder, E.C.
1985.
A sufficient condition of backtrack free search.
JACM 32(4):510-521.
Frost, D., Dechter, R., 1994.
In Search of the Best Search: An empirical Evaluation, In Proc.
of AAAI-94, pages 301-306.
Frost, D., Dechter, R., 1994.
Dead-End Driven Learning, In Proc.
of AAAI-94, pages 294-300.
Kautz, H., Ladkin, P., 1991.
Integrating Metric and Qualitative Temporal Reasoning, In Proc.
of AAAI-91, pages 241-246, 1991.
Ladkin, P.B., Reinefeld, A., 1992.
Effective solution of qualitative interval constraint problems, Artificial Intelligence 57 (1992) 105-124.
Meiri, I., 1991.
Combining Qualitative and Quantitative constraints in temporal reasoning In Proc.
AAAI-91, pp.
260-268.
Mitchell, D., Selman, B., Levesque, H., 1992.
Hard and Easy Distributions of SAT Problems, Proc.
of AAAI-92.
Sadeh, N., 1991.
Look-Ahead techniques for Microopportunistic Job Shop Scheduling, Ph.D. thessis, School of Computer Science, Carnegie Mellon University, March 1991.
Schwalb, E., Dechter, R., 1993.
Coping with Disjunctions in Temporal Constraint satisfaction Problems, In Proc.
AAAI-93, 127-132.
Schwalb, E., Dechter, R., 1994.
Temporal Reasoning with Constraints on Fluents and Events, In Proc.
AAAI-94.
Vilain, M., Kautz, H., Van Beek, P., 1989.
Constraint Propagation Algorithms for Temporal Reasoning: A revised Report.
In Readings in Qualitative Reasoning about Physical Systems, J. de Kleer and D. Weld (eds).
1989.
Williams, C.P., Hogg, T., 1993.
A typicality of phase transition search, Computatzonal Intelligence 9(3):211-238.
35
Managing Large Temporal Delays in a Model Based Control System Fano Ramparany  ITMI BP 87, 38244 Meylan, France  Abstract:  In this paper we explain how we have integrated the functionalities of a constraint management system and a temporal data base system to enable a model-based control of systems that exhibit large delays between the events characterizing its behaviour.
Examples of problems where our approach is appropriate, include the monitoring and controlling of complex and geographically distributed systems.
Such applications require a robust modeling of the behaviour of the systems, in terms of causal relationships among its state variables, and the handling of temporal delays that may span between an event and its causal inuences all over the system.
Our approach as been applied to build a KBS for assisting heating central operators to optimize the eciency and profitability of the heating process.
1 Introduction  The main diculty in controlling urban heating systems stems from the geographical distribution of the system to be controlled as it introduces large delays between events and their eects.
Another source of diculty is related to the complexity of the heating system, in terms of the number of its components, the behaviour of these components and the interactions among them.
An example of the topology of an urban heating system is displayed in gure 1.
End user heating stations are depicted with the symbol 1.
Such a network involves more than a hundred stations, all interconnected with heating ow pipes.
When an operator located at Bissy station, modies the temperature of the primary network (heating ow), its eect impacts on the temperature of the secondary network (heated ow), from one to three hours later depending on the speed of the primary ow.
The problem is complicated by the number of parameters which need to be measured, and perhaps  S.V.T.
Heating Central  Bissy  Place de Geneve Sembat  Tercinel Lycee vaugelas  voutes Hopital  Covet Exchanger  Croix des Brigands  Place St.Pierre de Mache Prefecture  Petit Biollay  Figure 1: Part of the map of Chambery city's urban heating system  eventually modied to change the behaviour of the system.
Furthermore the space of control parameters itself contains redundancy.
For example, fullling an increase of the users consumption needs can be realized either by opening the primary network oodgates, or by raising its temperature.
Using a deep model of a system to control and monitor its process has largely been proven a successful approach 3].
Model based control and more generally model based reasoning qualify reasoning processes that draw their inferences upon functional, structural or behavioural models of the system to be controlled.
Several techniques to build and exploit such so called \deep models" have been developed and used so far.
Quantitative models are usually dened as a set of numerical or symbolical constraints that relate relevant variables of the system to each other.
When the number of equations is too high, or if some of them are not available, a qualitative approach is often used.
This constructs an approximate model of the system, and reasons more abstractly about it to derive consistent states which encompass physical qualitative behaviors.
However, none of current \deep" modeling techniques is able to manage large delays between variables.
Instead of extending one of them to t this specic requirement, we propose a more exible approach which integrates a constraint management system with a temporal database system.
The constraint management system enables the causal modeling of the system.
The temporal database system is used to store the temporal histories of some properties of the system that are required when modeling the behaviour of the system.
In this paper, we describe the problems of integrating causal and temporal reasoning.
We then introduce the main concepts of our approach for tackling them.
We show how we have applied this approach for developing a system assisting urban heating power station operators to tune the control parameters of the system, in order to optimize the eciency and protability of the heating process, while satisfying the user consumption requirements.
We nally discuss our approach and compare it to related approaches.
2 Problem Statement  When controlling complex systems with large temporal delays, two types of requirements have to be faced.
Reasoning about time, and reasoning about system functions and behaviour.
More specically, the following capabilities are generally required: Temporal projection: makes it possible to know which properties of the system are known and  what those properties are at some future point in time.
Anticipatory action: enables the planing of an action suciently in advance, so that it eects will occur at the right time.
For example, if we want somebody to receive a parcel within two days, we should better post it now.
Temporal queries: are queries about which property holds within a specic temporal window.
For example, a football match organizer might be interested in checking whether it has rained yesterday evening as to how the terrain grass will look like.
Causal modeling: allows describing the set of parameters that are relevant to describe the state of the system, and to explicitly represent the network of causal relationship among these parameters.
Simulation: completes the description of the system state or infers its subsequent states, based on a partial description of the initial state.
This kind of inference is mainly supported by a causal model of the system to be controlled.
At the requirements analysis level, we can clearly separate representation and reasoning about time, from representation and reasoning about causal dependencies.
Temporal projection, anticipation and queries are typical inferences expected from a temporal reasoning system.
Causal modeling and simulation are typical inferences expected from a causal modeling tool.
From a methodological point of view we propose to preserve the separation between temporal and causal reasoning at the design and implementation stage of the system.
It has been argued that such methodological option allows a modular design of the target system, thus clarifying its conceptual architecture and easing its development as well as its maintenance, specially when dierent development teams are to be involved.
This decomposition approach to design is also a very pragmatic way to \reuse" existing modules within new applications.
In the following sections we will show that reusing and combining existing and standard techniques for handling temporal reasoning and causal reasoning, can reveal a very powerful and ecient development strategy.
3 Integration framework  We describe here the integration framework that has been adopted to enable temporal and causal reasoning to cooperate.
We very briey summarize the basic notions of temporal databases and constraints satisfaction problems as far as they are relevant for this paper.
The reader who is conversant in these areas may skip the respective subsections, or just browse through to pick up the terminology.
3.1 Temporal databases (TDBMS)  As a conventional database system a TDBMS store facts and objects, the specicities of TDBMS is that it provides facilities to describe their temporal properties 10].
For example one can specify the time at which some fact becomes true, and when it becomes false.
So that one can query at which time(s) some fact was true in the past, or will be true in the future.
One can can even ask about which facts are true within a specic temporal window.
The temporal database that we have used is based on Sergot and Kowalski Event Calculus (EC) framework 6].
The EC is a treatment of time in rst-order classical logic, based on the notion of events in order to temporally generate properties that hold for a given time interval.
EC combines the expressive power of situation calculus and the computational power of logic programming.
The EC axioms allow generation of answers to queries about the holding properties.
Although many extensions to the EC framework are possible, such as the handling of dierent temporal granularities and continuous change, we have basically restricted its use to the basic framework.
3.2 Constraint management systems (CMS)  The idea of using constraints as a problem solving and programming paradigm is very useful for model based reasoning, such as diagnosis, simulation and control.
This is because much of the modelling deals with relationships which are naturally described as constraints.
For example, the underlying mathematical models for a range of physical phenomena such as the conservation of heat, uid ow, the relationship between current and voltage, etc.
can be described by constraints.
The constraint management system built here is based on the constraint logic programming language CLP(R) 5, 4].
The essential idea of the constraint programming paradigm is that constraints represent relationships between the objects.
The underlying constraint solver then guarantees the consistency of the constraints as a whole in the system and also solves for the values whenever possible.
When we do not have specic unique values for the variables then \answer constraints" also serve as output to describe the solution space.
Thus when posing constraints to the CMS we do not care if there are values for the variables, we know that those values will be computed either now or later implicitly when more information is obtained or when more constraints are added.
Constraint systems are characterised by the kinds of constraints expressible and the computational power of the constraint solver.
The underlying CLP(R) solves all linear arithmetic constraints directly and completely solves these class of constraints.
More complex non-linear constraints are  not solved directly and are deferred to be solved by local propagation when they become suciently linear.
3.3 Interfacing the TDBMS with the CMS  We will now see how these two paradigms can be associated into a system that can manage a constraint network of temporal properties.
Specifying the interface between the TDBMS and the CMS amounts to dening the conceptual mapping between their respective representation (in terms of what a conceptual primitive in one representation corresponds to the conceptual primitives of the second representation), the functional interface which dene the services that one of the system will request from the other system in runtime, and the control architecture which species the conditions required for such interaction.
3.4 Conceptual Mapping  CMS events, i.e.
variables and their associated values, are simply stored as items in the TDBMS which are dated according to the absolute date of their computation.
Not all variables histories are worth storing in the TDBMS, only those that will eectively be used for causal simulation are.
3.5 Functional Interface  The TDBMS oers its full functionalities to the CMS.
Thus allowing the CMS to post dated events, to perform temporal queries including retrieving of properties that hold at a given time point, or interval.
Conversely, the CMS may be requested by the TDBMS to post new constraints, or variable instantiations (which are considered by the CMS as a special constraint specication), in which case the CMS will automatically propagate the eects of these new constraints on all the variable of the system to derive an updated consistent state, or detect some constraint violations.
This mechanism allows the simulation of eects of future events.
3.6 Control architecture  The control architecture is specied using metarules that identify specic situations that are recognized by one of the reasoning schemes, in which it has to request services from the other reasoning scheme.
The description of the service to be requested is also given by the metarule.
An example of such metarule is:  \ If the temperature of component A has not exceeded a critical threshold Tco during the past 2 hours, then component A exhibits the nominal behaviour, and its output temperature will be twice that of its input temperature."
In this metarule, the situation which is \component A's temperature is below Tco during the last 2 hours" is recognized by the temporal data base, whereas the action to be taken \output temperature  CMS  temporal queries  T  Lrp  TBDMS  temporal assertions  U  Z=3  Bissy Station  Tdrp1  Tarp1  Covet Exchanger Tdrp2  Y < 4  X Z  Vp1  constraints  Vp2 Drp  Y time  Trrp1a  Figure 2: Integration architecture of component A is twice that of its input value" is  interpreted by the causal reasoner.
Metarules can be domain dependent, or task dependent.
In the latter case this metarule can be reused in other domain for similar problem.
The gure 2 summarizes the features of the integration framework.
In the following section we show how it instantiates in the context of a real application.
4 Application  We have implemented a system for providing operators with recommendations on which control actions to take in order to optimize the heating process of Chambery city.
A urban heating system can roughly be described as interconnected networks of heating components such as heat exchangers and urban heating power stations, and mechanical components such as Pump and Flow pipes.
Heat exchangers allow the heating of a secondary ow, using the heat brought by an incoming primary ow.
Urban heating power stations consume fuel and gas to produce heat which is transferred to a circulating ow.
Heating components are linked together through ow pipes carrying liquid that transmit heat from one component to the others.
Hydraulic pumps maintain a controlled ow of the heating liquid that circulate in the pipes.
The basic association of a central and exchanger is shown in gure 3.
During the analysis phase of the system we have interviewed two types of experts: System operators who daily monitor the heating power station and tune its control parameter to satisfy the users requirements, and adapt the system to the weather condition uctuations.
Operators possess implicit models of how some part of the heating network behave.
For example, they know that increasing the speed of the primary ow pump will increase the temperature of the secondary ow through the heat exchanger.
Trrp2  Trrp1  Pc  Figure 3: Example of two components of the urban heating system  System designers who know the physical charac-  teristics of the heating system, such as the length of the various networks, the characteristics of the exchangers.
A rst analysis with the operators helped identify which parameters are relevant for characterizing the state of the heating system.
This preliminary analysis was completed by interviewing the system designers, to rene the relationships between those parameters and make explicit their underlying theoretical justications.
Several additional parameters were introduced by the system designer that were ignored or unconsciously skipped by the operators.
For example, the dependency between the pump speed V p and the temperature of the secondary ow exiting from the heat exchanger Tdrp2 that has been identied by the operators, can be explained with a causal chain comprising four links.
The rst one relates V p to the ow Drp1 circulating in the primary ow pipe.
The second one relates Drp1 to the power transmitted to the exchanger PC.
The third one relates P C to the \loss" of temperature between the primary and secondary ow of the exchanger DTCe, which is itself obviously related to Tdrp2.
Each of these causal link can be formalized as a numerical constraint, that involves the two variable linked and eventually other variables.
For example, the rst causal link described above represents the following equation: Drp1 = Kp  V p where K1 is a constant characterizing the pump.
This two stages analysis phase has produced a behavioural model of the system.
Part of this model is displayed in gure 4, where arrows depict causal links.
Each of the arcs of the network depict a numerical constraint that has been registered in the CMS constraints base.
Prediction of the users power consumption is computed by a specic external module which is not described here.
They are stored in the TDBMS along with the corresponding time of the prediction.
M  Dp  Pay  Evay Prx U  Vp  Drp2  Drp1  Cost  PC  Trrp1  Trrp2 DTrp2  DTrp1 Tarp1  Tdrp2  DTCe  Tdrp1  Tdrpx  Figure 4: Part of the behavioural model of the heating system Pay  Evay  Control variables  U  Vp Drp1  C  Legend  M  Dp  Prpx  input variable  computed variable  Drp2  causal link  PC  Trrp1  Trrp2 DTrp1  DTrp2  Tarp1  Tdrp1  constraint propagation flow  Tdrp2  DTCe Tdrpx  Figure 5: Example of a causal constraint propagation for control recommendation generation The constraints on the prediction of users future power consumption, are registered by the CMS which also propagates the eects of these constraints.
Due to the geographical distribution of the various user stations, the eects of a constraint occur at various instants in time depending on the location of the stations and the speed of the ows that feed them.
This temporal information is stored in the TDBMS.
The CMS ensures the constraints are applied and kept consistent.
As constraints are setup to describe the causal model of the heating system network, the input parameters to the system cause variables which are directly related to be solved, and this eect propagates throughout the causal model which in turn enables solving other variables.
The constraint propagation ends when the control parameters (those used by the operators to tune the heating system) are assigned their values.
Figure 5 displays a trace of such constraint propagation that instantiates the pump speed V p and the temperature of primary ow Tdrp1 exiting from the \Bissy" principal heating power station.
V p and T drp1 are the control parameters of the \Bissy" heating power station.
5 Discussion  The problem of handling of large temporal delays for process control has been identied for some time by  research in control theory 2].
Specic techniques have been introduced for this matter.
Model based approaches of process control problems, have been introduced during the last decade in order to overcome the shortcomings of classical control theory techniques with respect to the needs for high level explanations and incompleteness in the knowledge of the process to control.
We rst looked into qualitative physics and systems like FTQ 1] and QSIM 7], as such models have been used in the application domain of central heating systems 8].
However, FTQ requires variables histories to be continuous, as well as a rather complete knowledge of the system to model in terms of qualitative transfer functions.
On the other hand, QSIM allows more approximations to be done while modeling the physical system and its behaviour, but QSIM doesn't provide for managing large delays.
The power of qualitative modeling technique for modeling transient states was not necessary in our case as the time granularity we worked at, was far coarser than the time constant of thermodynamical phenomena.
Thus steady state equations that don't involve any temporal derivatives are sucient in our case.
Due to the limitation of current causal modeling techniques when reasoning on a phenomenon with large temporal delays, we have proposed a solution that loosely integrates the functionalities of a temporal database management system and that of a constraint management system.
This approach enabled us to combine existing robust temporal and causal reasoning techniques.
For this purpose we have introduced a general framework for integrating heterogeneous representation and reasoning formalisms, that we have applied to the more restricted problem of integrating temporal and causal reasoning.
From a more general perspective, characteristics shared by complex dynamic applications that hamper their deployment include the requirement to handle more than one of the following: evolution of the information with time, incompleteness, unreliability and uncertainty of information which has to be managed.
Complex applications also often require a cooperation between specialized knowledge-based facilities for simulation, fault diagnosis, planning/scheduling, emergency decision making, etc... We believe that such approach of loosely integrating heterogeneous formalisms helps in developing such applications.
Along this direction, we are currently engaged in the ongoing Esprit-III project UNITE 9] which aims at tackling these integration issues.
More specically, we are developing appropriate techniques for integrating heterogeneous knowledge models including incompleteness, uncertainty and time-dependency (internal integration) and for the cooperation between knowledge based facilities (external integration).
These techniques will be implemented in an integrated support environment  composed of a generic platform which supports the development and integration of complex applications.
This support environment and the applicability of our approach are being assessed in the framework of two other pilot applications in the domain of space control centers and neonatal intensive care unit in hospitals.
One main lesson that we have learned through the work described in this paper is that the following three aspects should be carefully specied when integrating heterogeneous formalisms.
Conceptual mapping: \what a conceptual primitive in one representation corresponds to in terms of the conceptual primitives of the second representation?"
Functional interface: \which services will one system request from the other system at runtime?"
Control architecture: \ what the conditions required for interaction are, and which corresponding actions should then be triggered?"
The second lesson is that such loosely integration approach reveals a ecient development strategy, as it allows existing techniques and tools to be eectively reused.
Acknowledgements  Thanks to Renaud Zigmann, Roland Yap and the anonymous referees for their valuable comments on early drafts of this paper.
The research reported here was carried out in the course of a project partially funded by the \Agence pour la Ma^trise de l'Energie".
The partners in this project are Cofreth, LIAC-Lyonnaise des Eaux, SCDC and ITMI.
References  1] P. CALOUD.
Raisonnement Qualitatif.
PhD thesis, INPG, 1989.
2] S. DADEBO and R. LUUS.
Optimal control of time-delay systems by dynamic programming.
Optimal Control Applications & Methods, 1992.
3] F. HAYES-ROTH, D.A.
WATERMAN, and D.B.
LENAT.
Building Expert Systems.
Addison Wesley, 1983.
4] N.C. HEINTZE, J. JAFFAR, S. MICHAYLOV, P.J.
STUCKEY, and R.H.C.
YAP.
The CLP(R) Programmer's Manual, 1992.
5] J. JAFFAR, S. MICHAYLOV, P.J.
STUCKEY, and R.H.C.
YAP.
The CLP(R) language and system.
ACM Transactions on Programming Languages and Systems, 1987.
6] R. KOWALSKI and M. SERGOT.
A logic-based calculus of events.
New Generation Computing, 4, 1986.
7] B. KUIPERS.
Qualitative simulation.
Articial Intelligence, 1986.
8] F. LACKINGER and W. NEJDL.
Diamon: A model-based troubleshooter based on qualitative reasoning.
IEEE Expert, 1993.
9] F. Ramparany and al.
Knowledge integration and interchange issues in imperfect and time dependent information systems.
In Proceedings of  the IJCAI'93 Knowledge Sharing and Information Interchange workshop, 1993.
10] S. M. SRIPADA.
A logical framework for temporal deductive database.
In Proceedings of the 14ht VLDB Conference, 1988.
Believing Change and Changing Belief Peter Haddawy  haddawy@cs.uwm.edu Department of Electrical Engineering and Computer Science University of Wisconsin-Milwaukee Milwaukee, WI 53201  Abstract  We present a rst-order logic of time, chance, and probability that is capable of expressing the relation between subjective probability and objective chance at dierent times.
Using this capability, we show how the logic can distinguish between causal and evidential correlation by distinguishing between conditions, events, and actions that 1) in	uence the agent's belief in chance and 2) the agent believes to in	uence chance.
Furthermore, the semantics of the logic captures commonsense inferences concerning objective chance and causality.
We show that an agent's subjective probability is the expected value of its beliefs concerning objective chance.
We also prove that an agent using this representation believes with certainty that the past cannot be causally in	uenced.
1 Introduction  The ability to distinguish evidential from causal correlation is crucial for carrying out a number of dierent types of problem solving.
To perform diagnosis we must be able to identify the factors that caused an observed failure in order to determine how to repair the faulty device.
If we cannot distinguish causal from evidential correlation, we may end up treating the symptoms rather than the causes of the fault.
When reasoning about plans, an agent may have goals that involve achieving a specied state of the world, or achieving a specied state of knowledge, or a combination of both.
In order to eectively reason about such goals, we need to distinguish actions that in	uence the state of the world from those that only in	uence our state of knowledge of the world.
In this paper we extend Haddawy's 3] logic of time, chance, and action L by adding a subjective probability operator.
We show how the resulting rst-order logic of time, chance, and probability, L , can distinguish between causal and evidential correlation by distinguishing between conditions and events that 1) in	uence the agent's belief in chance and 2) the agent believes to in	uence chance.
Furthermore, the semantics of the logic captures some commonsense inferences tca  tcp  This work was partially supported by NSF grant #IRI9207262.
concerning causality and the relation between objective chance and subjective probability.
We prove that an agent's subjective probability is the expected value of its beliefs concerning objective chance.
We also prove that an agent whose beliefs are represented in this logic believes with certainty that the past cannot be causally in	uenced.
On the other hand, an agent can execute actions that in	uence its subjective beliefs about the past.
2 Ontology  We brie	y present the ontology of the logic, which includes the representation of time, facts, events, objective chance, and subjective probability.
For simplicity of exposition, we will omit the representation of actions and will treat them as events.
For a more detailed development of chance, facts, events, and actions see 3].
Time is modeled as a collection of world-histories, each of which is one possible chronology or history of events throughout time.
A totally ordered set of time points provides a common reference to times in the various world-histories.
We represent an agent's beliefs with subjective probabilities.
Since beliefs may change with time, subjective probability is taken relative to a point in time.
We represent it by dening a probability distribution over the set of world-histories at each point in time.
So an agent can have beliefs concerning temporally qualied facts and events.
We represent causal correlation with objective chance.
Objectively, actions and events can only aect the state of the world at times after their occurrence.
That is to say, at each point in time, the past is xed| no occurrences in the world will cause it to change but at each point in time the future might unfold in any number of ways.
So relative to any point in time, only one objectively possible past exists, but numerous possible futures exist.
Thus we represent objective chance by dening a future-branching tree structure on the world-histories and by dening probabilities over this tree.
Like subjective probability, chance is taken relative to a point in time.
By dening chance in this way, conditions in the present and past relative to a given time are either certainly true of certainly false.
So actions and other events can only aect the chances of future facts and events.
This property distinguishes objective chance from subjective probability.
Subjectively the past can be uncertain but objectively it is completely determined.
The present characterization of objective chance is not to be confused with the frequentist interpretation of probability 10, 11] which is often called objective probability.
Frequentist theories dene probability in terms of the limiting relative frequency in an innite number of trials or events.
The current work does not rely on relative frequencies for its semantics.
Rather it models objective chance by formalizing the properties that characterize objective chance.
Thus while frequentist theories have diculty assigning meaningful probabilities to unique events like a sh jumping out of the water at a given location and time, our model has no problem in assigning nontrivial probabilities to such events.
Our model of objective chance and subjective probability is motivated by the subjectivist theories of objective chance 6, 8, 9], which dene chance in terms of properties that one would expect a rational agent to believe objective chance to possess.
This distinction between the frequentist theory of probability and our conception of objective chance puts the present work in sharp contrast with Bacchus's 1] logic of statistical probabilities which models exactly relative frequency type probabilities.
One telling dierence between the two logics is that Bacchus's logic Lp assigns only probability 0 or 1 to unique events (more precisely, to all closed formulas).
The present logic can assign any chance value to unique events in the future, while events in the past are assigned only chance values 0 or 1, as required by our denition of objective chance.
It is reasonable to expect the subjective beliefs of a rational agent concerning objective chance to obey certain constraints.
Skyrms 7, Appendix 2] has argued for a constraint he calls Millers' principle.
This asserts that an agent's subjective belief in a proposition, given that he believes the objective chance to be a certain value, should be equal to that value.
Skyrms argues that this is a plausible rule for assimilating information about chance.
We will call this relation the subjective/objective Miller's principle.
The world is described in terms of facts and events.
Facts tend to hold and events tend to occur over intervals of time.
So facts and events are associated with the time intervals over which they hold or occur in the various world-histories.
Facts are distinguished from events on the basis of their temporal properties.
A fact may hold over several intervals in any given world-history and if a fact holds over an interval then it holds over all subintervals of that interval.
Events are somewhat more complex than facts.
First, one must distinguish between event types and event tokens.
An event type is a general class of events and an event token is a specic instance of an event type.
Event tokens are unique individuals { the interval over which an event token occurs is the unique interval associated with the event token and an event token can occur at most once in any world-history.
The present work deals with event types, which for brevity are simply referred to as events.
3 The Logic of Time, Chance, and Probability 3.1 Syntax  The language of L contains two predicates to refer to facts and event types occurring in time: HOLDS (FA t1 t2) is true if fact FA holds over the time interval t1 to t2, and OCCURS (EV t1  t2) is true if event EV occurs during the interval t1 to t2 .
Henceforth we will use the symbol t, possibly subscripted, to denote time points , fi, and  to denote formulas and  and  to denote probability values.
In addition to the usual rst-order logical operators, the language contains two modal operators to express subjective probability and objective chance.
The operators are subscripted with a time since according to the ontology subjective probability and objective chance are taken relative to a point in time.
We write P () to denote the subjective probability of  at time t and we write pr () to denote the objective chance of  at time t. Probability is treated as a sentential operator in the object language.
So the probability operators can be arbitrarily nested and combined with one another, allowing us to write complex sentences like: \I believe there was a one in a million chance of my winning the lottery, yet I won."
P 3 (pr 2 (OCCURS (win t1 t2)) = 10;6^ OCCURS (win t1 t2)) = 1 where t1 < t2 < t3.
We also allow conditional probability sentences such as P (jfi) = , which is interpreted as shorthand for P ( ^ fi) =   P (fi).
The language of L is fully rst-order, allowing quantication over time points, probability values, and domain individuals.
A formal specication of the syntax is provided in the full paper 2].
tcp  t  t  t  t  t  t  t  tcp  3.2 Semantics  We describe only the more interesting aspects of the models of L .
The models are completely specied in the full paper.
A model is a tuple hW D, FN, NFN, PFN, FRL, ERL, NRL, FA, EVENTS, EV, R, X , PR , PR , F i, where: fi W is the set of possible world-histories, called worlds.
fi D is the non-empty domain of individuals.
fi FA is the set of facts, a subset of 2(<<) .
A fact is 0 a set of htemporal interval, worldi pairs: fhht1 t1 i w1i ::: hht  t0 i w ig.
If fa is a fact and hht1  t2i wi 2 fa then fa holds throughout interval ht1  t2i in world-history w. fi EVENTS is the set of event tokens, a subset of (<<)  W .
An event token is a single htemporal interval, worldi pair.
fi EV is the set of event types, a subset of 2EVENTS .
An event type is a set of event tokens: fhht1 t01 i w1i ::: hht  t0 i w ig.
If ev is an event and hht1  t2i wi 2 ev then ev occurs during interval ht1 t2i in world-history w. tcp  o  s  W  n  n  n  n  n  n  1.
HOLDS (rf (trm 1  ::: trm ) ttrm1  ttrm 2)]] = true i hh ttrm 1 ]   ttrm 2] i wi 2 F (rf )(trm 1]  :::  trm ] ): 2.
OCCURS (re(trm 1  ::: trm ) ttrm 1  ttrm 2)]] = true i hh ttrm 1 ]   ttrm 2] i wi 2 e for some e 2 F (re)(trm 1 ]  ::: trm ] ): 0 3.  prttrm ()]] =  o ttrm] (fw 2 R ttrm] :  ] = trueg).
4.
Pttrm ()]] =  s ttrm] (fw0 :  ] = trueg).
Mwg  n Mwg  Mwg  Mwg  Mwg  n  Mwg  n Mwg  Mwg  Mwg  Mwg  n  Mwg  Mwg  w  w  Mwg  w  0  Mw g  Mwg  0  Mw g  Mwg  Figure 1: Semantic denitions fi R is an accessibility relation dened on <W W .
R(t w1 w2) means that world-histories w1 and w2 are indistinguishable up to and including time t. If R(t w1 w2) we say a world-history w2 is Raccessible from w1 at time t. The set of all worldhistories R-accessible from w at time t will be designated R .
For each time t, the R partition the world-histories into sets of equivalence classes indistinguishable up to t. fi X is a -algebra over W 1 , containing all the sets corresponding to w's in the language, as well as all R-equivalence classes of world-histories.
fi PR is the objective probability assignment function that assigns to each time t 2 < and worldhistory w 2 W a countably additive probability distribution  o dened over X .
fi PR is the subjective probability assignment function that assigns to each time t 2 < and worldhistory w 2 W a countably additive probability distribution  s dened over X .
Given the models described above, the semantic definitions for the well-formed formulas can now be dened.
Denotations are assigned to expressions relative to a model, a world-history within the model, and an assignment of individuals in the domain to variables.
The denotation of an expression  relative to a model M and a world-history w, and a variable assignment g is designated by  ] .
Figure 1 shows the less familiar semantic denitions.
The remainder are provided in the full paper.
w t  w t  o  w t  s  w t  Mwg  3.2.1 Semantic Constraints  In order to obtain the properties discussed in the ontology, we impose eight constraints on the models.
The future-branching temporal tree is dened in terms of the R relation over world-histories.
To capture the property that the tree does not branch into the past, we say that if two world-histories are indistinguishable up to time t2 then they are indistinguishable up to any earlier time: (C1) If t1t2 and R(t2 w1 w2) then R(t1 w1 w2).
A -algebra over W is a class of subsets that contains W and is closed under complement and countable union.
1  Since R just represents the indistinguishability of histories up to a time t, for a xed time R is an equivalence relation, i.e., re	exive, symmetric, and transitive: (C2) R(t w w) If R(t w1 w2) then R(t w2 w1) If R(t w1 w2) and R(t w2 w3) then R(t w1 w3) As mentioned earlier, facts and events dier in their temporal properties.
This distinction is captured by the following two semantic constraints.
If a fact holds over an interval, it holds over all subintervals, except possibly at the endpoints: (C3) If t1 t2 t3t4 t1 6= t3  t2 6= t4  fa 2 FA and hht1 t4i wi 2 fa then hht2  t3i wi 2 fa : An event token occurs only once in each world-history: (C4) If evt 2 EVENTS , hht1 t2i wi 2 evt, and hht3  t4i wi 2 evt then t1 = t3 and t2 = t4 .
If two worlds are indistinguishable up to a time then they must share a common past up to that time.
And if they share a common past up to a given time, they must agree on all facts and events up to that time.
To enforce this relationship, we impose the constraint that if two world-histories are R-accessible at time t, they must agree on all facts(events) that hold(occur) over intervals ending before or at the same time as t: (C5) If t0 t1 t2 and R(t2  w1 w2) then hht0  t1i w1i 2 A i hht0  t1i w2i 2 A, where A is a fact or event.
The ontology discussed two desired characteristics of objective chance.
The rst is that the chance at a time t be completely determined by the history up to that time.
The second desired characteristic is that the chance of the present and past should be either zero or one, depending on whether or not it actually happened.
These two properties follow as meta-theorems from the following two constraints: (C6) For all 0 X 2 X  tt0 and w w0 such that R(t w w )   (R ) > 0 !
(X ) =   (X jR ).
w t  w t  0  w t0  0  w t  w t0  0  (C7)   (R ) > 0.
Meta-theorem 1 The probability of the present and w t  w t  past is either zero or one.
(R ) = 1 w t  w t  Theorem 7 Objective Miller's Principle (OMP)  1.
(R ) > 0 (C7) 2.
(R ) =   (R jR ) Modus Ponens: (C6),1 3.
(R ) = 1 def of c-prob w t w t w t  w t w t w t  w t  w t  w t  tcp  t  Dening the probabilities in this way makes good intuitive sense if we look at the meaning of R. R designates the set of world-histories that are objectively possible with respect to w at time t. It is natural that the set of world-histories that are objectively likely with respect to w at time t should be a subset of the ones that are possible.
w t  Meta-theorem 2 If two worlds are indistinguishable  up to time t then they have identical probability distributions at that time.
If R(t w w0) then   (X ) =   (X ) w t  1.
2.
3.
4.
5.
0  w t  (R ) > 0 (C2), (C7) (R ) =   (X jR ) Modus Ponens: (C6),1 (X jR ) =   (X jR ) (C2) (R ) = 1 Meta-theorem 1 (X ) =   (X ) def of c-prob  w t w t w t w t 0 w t  0  w t 0 w t  w t  w t  w t  0  w t  w t  0  w t  w t  In the ontology, we argued that subjective probability and objective chance should be related to one another by Millers' principle.
This relation is enforced by the following constraint, which says that the probability of a set of worlds X , given some R equivalence class, should just be the objective chance in that equivalence class.
(C8)  s (X jR ) =  o (X ) w t  w t  w t  4 Theorems  We rst provide several simple theorems that will be used in later proofs.
Then we prove two forms of Miller's principle and provide two associated expected value properties.
Proofs not provided here appear in the full paper.
Theorem 3 From  $ fi infer pr () = pr (fi): t  t  Theorem 4 Stronger sentences have lower probability.
From  !
fi infer P ()  P (fi).
Theorem 5 Certainty cannot be conditioned away from.
P ( ^ fi) = P (fi) !
P ( ^ fi ^  ) = P (fi ^  ) Theorem 6 The present and past are objectively certain.
Let be a fact or event: HOLDS ( t  t0 ) or OCCURS ( t  t0 ) then 8t t  t0 (t0  t) !
pr ( ) = 0 _ pr ( ) = 1] t  t  t  t          t  t      t  All instances of the following sentence schema are valid in L .
8 t0 t1 (t0  t1 ) !
pr 0 ( j pr 1 () = ) =  Proof:    t  The semantic constraints on objective chance give us a version of Miller's principle that relates objective chance at dierent times.
It says that the chance of a sentence  at a time, given that the chance of  at the same or a later time is , should be .
t  We rst prove an expected value property and then use it to prove Miller's principle.
Let t t0 be two time points t  t0 and consider the R-equivalence classes of worlds at time t0 .
Let the variable r range over these equivalence classes.
The r form a partition of W , so the probability of a set X can be written as the integral over this partition: Z  o (X ) =  o (X jr) o (dr)  Since the history up to time t0 determines the probability at time tZ0, this can be written as  o (X ) =  o (X ) o (dr)  where  o denotes the probability at time t0 in equivalence class r. Since the probability at a given time is assumed to be constant over all worlds in an Requivalence class, the probability at a given time is the expected value Z of the probability at any future time:  o (X ) =  o (X ) o (dw0 ): Next we show that Miller's principle is valid in the probability models.
By the expected value property,  o (ZX \ fw0 :  o (X ) = g) =  o (X \ fw0 :  o (X ) = g) o (dw00): Now, by semantic constraints (C6) and (C7) it follows that 8 w 2fw0 :  o (X ) = g  o (fw0 :  o (X ) = g) = 1 8 w 62fw0 :  o (X ) = g  o (fw0 :  o (X ) = g) = 0: So we can restrict the integral to the set fw0 :Z o (X ) = g: =  o (X \ fw0 :  o (X ) = g) o (dw00): w t  w t  r  W  r  W  w t  w t  r t0  w t  r t0  w t0  w t  0  w t  W  w t  w t0  w t0  0  00  w t0  0  w t  W  w t0  w t0  w t0  0  w t0  0  w t0  w t0  w t0  0  0  0  w  00  w t0  t0 0 w 0 fiow0 X t  f  ( )=g  :  0  w t  And by the above property again  o (XZ\ fw0 :  o (X ) = g) = , so =   o (dw00): w t0  00  w t0  f  0  ( )=g  0 w 0 fiow0 X t  :  w t  =    o (fw0 :  o (X ) = g): By the semantic denitions it follows that P ( ^ P () = ) =   P (P () = ): And by a slight generalization of the proof it follows that 8(t  t0) P ( ^ P ()  )    P (P ()  ): 2 From the Objective Miller's Principle it follows directly that current chance is the expected value of current chance applied to current or future chance.
w t  t  w t0  t0  0  t  t  t0  t0  t  t0  Theorem 8 Objective Expected Value Property  All instances of the following sentence schema are valid in L .
8  t1 t2 (t1  t2 ) !
pr 1 (pr 2 ()  )   !
pr 1 ()     ] tcp  t  t  t  As discussed in the ontology, the current subjective probability of a sentence, given that the current or future chance is some value should be that value.
The following theorem shows that this property follows from the semantics of the logic.
Theorem 9 Subjective/Objective Miller's Principle (SOMP)  All instances of the following sentence schema are valid in L .
8 t0 t1(t0  t1 ) !
P 0 (jpr 1 () = ) =  Proof: We rst prove an expected value property and tcp  t  t  then use it to prove Miller's principle.
Let t t0 be two 0 time points t  t and consider the R-equivalence classes of worlds at time t0 .
Let the variable r range over these  equivalence classes.
The r form a partition of W , so the probability of a set X can be written as the integral over this partition:   s (X ) =  Z  w t  r   s (X jr) s (dr) w t    W  w t  Z  w t  r   o (X ) s (dr) r t    W  w t  where  o denotes the objective chance at time t in equivalence class r. Since the chance at a given time is assumed to be constant over all worlds in an Requivalence class, the subjective probability at any time is the expected value of the subjective probability applied to the objective chance at that time: r t   s (x) =  Z  w t   o (X ) s (dw0) w t  W  0  w t  Next we show that the Subjective/Objective Miller's principle is valid in the probability models.
By the above subjective/objective expected value property,  s (ZX \ fw0 :  o (X ) = g) =  o (X \ fw0 :  o (X ) = g) s (dw00) By Objective Miller's Principle,  s (XZ\ fw0 :  o (X ) = g) =   o (fw0 :  o (X ) = g) s (dw00) w t  w t0  w t  0  00  w t0  0  w t  W  w t  w t0  w t  0  00  w t0  W  0  w t  Finally, by the subjective/objective expected value property,  s (X \ fw0 :  o (X ) = g) =  s (fw0 :  o (X ) = g) So by the semantic denitions it follows that 8t t0 (t  t0 ) !
P (jpr () = ) =  w t  w t0  w t  0  w t0  t  0  t0  t  t0  Theorem 10 Subjective/Objective Value Property  Expected  All instances of the following sentence schema are valid in L .
8  t1 t2 (t1  t2) !
P 1 (pr 2 ()  )   !
P 1 ()     ] tcp  t  t  t  5 Distinguishing Evidential and Causal Correlation  We wish to distinguish between two situations in which an agent may believe that two conditions are correlated.
An agent may believe that two conditions are correlated because one is simply evidence for another and an agent may believe that they are correlated because one causes the other.
Let stand for the formula HOLDS ( t  t0 ) or OCCURS ( t  t0 ) and let !
stand for the formula HOLDS (fi t  t0 ) or OCCURS (fi t  t0 ).
We represent evidential correlation as correlation in the subjective probability distribution, which is the standard approach in Bayesian decision theory.
Denition 11 We say that !
is evidence for or   By semantic constraint (C8), this can be written as   s (X ) =  And by a slight generalization of the proof it follows that 8t t0 (t  t0 ) !
P (jpr ()  )   2 From the subjective/objective Miller's principle it follows directly that subjective probability is the expected value of current subjective probability applied to present or future chance.
          against i Pnow ( j !)
= 6 Pnow ( )      (1) It follows from this denition that !
is not evidence for or against i Pnow ( j !)
= Pnow ( ) We represent causal correlation by reference to the objective chance distribution.
We represent an agent's belief that !
causally in	uences by saying that there is some value for the objective chance of such that the agent's belief in given the objective chance of just before !
holds or occurs is not the same as the agent's belief given also knowledge of !.
In other words, knowledge of !
overrides knowledge of the objective chance of .
Denition 12 We say that !
is a cause of i 9 Pnow ( j pr ( ) =  ^ !)
6= : (2) Note that this does not necessarily imply that Pnow ( j!)
6= Pnow ( ).
Thus we may have causal correlation without evidential correlation and, conversely, we may have evidential correlation without causal correlation.
It follows from this denition that !
is not a cause of i 8 Pnow ( j pr ( ) =  ^ !)
= : t  t  5.1 Example  We now present an example demonstrating the use of the denitions and theorems.
We wish to describe the following situation.
You have a coin that may be biased 3:1 towards heads or 3:1 towards tails.
You believe there is an equal probability of each.
You can observe the coin.
If the coin looks shiny, this increases your belief that the coin is biased towards heads.
You also have a magnet that you can use to in	uence the outcome of the coin toss.
Turning on the magnet biases the coin more toward heads.
We can describe the situation with the following set of sentences in which \heads" is the event of the coin landing heads, \shiny" is the event of the coin being observed to be shiny, and \magnet" is the fact that the magnet is on.
(now < t0 < t1 < t2 < t3 < t4 ) Turning on the magnet in	uences the chance of heads.
Pnow (OCCURS (Heads t2 t3)j (3) pr 1 (OCCURS (Heads t2 t3)) = 3=4 ^ HOLDS (Magnet t1  t4)) = 7=8 Pnow (OCCURS (Heads t2 t3)j (4) pr 1 (OCCURS (Heads t2 t3)) = 1=4 ^ HOLDS (Magnet t1  t4)) = 1=2 The probability that the coin is biased toward heads and the probability that the coin is biased toward tails are equal.2 Pnow (pr 1 (OCCURS (Heads t2 t3)) = 3=4) = (5) Pnow (pr 1 (OCCURS (Heads t2 t3)) = 1=4) = 1=2 Observing the coin doesn't in	uence the chance of heads.
(6) 8 t (t > now) !
Pnow (OCCURS (Heads t2 t3) j pr (OCCURS (Heads t2 t3)) =  ^ OCCURS (Shiny t0  t2)) =  Observing the coin gives us knowledge of its bias.
Pnow (pr 0 (OCCURS (Heads t2 t3)) = 3=4 j (7) OCCURS (Shiny t0  t2)) = 5=8 Pnow (pr 0 (OCCURS (Heads t2 t3)) = 1=4 j (8) OCCURS (Shiny t0  t2)) = 3=8 Turning on the magnet does not give us knowledge of the coin's bias.
8 Pnow (pr 1 (OCCURS (Heads t2 t3)) =  j (9) HOLDS (Magnet t1  t4)) = Pnow (pr 1 (OCCURS (Heads t2 t3)) = ) The coin is either biased toward heads or toward tails.
8t pr (OCCURS (Heads t2 t3 )) = 3=4 _ (10) pr (OCCURS (Heads t2 t3)) = 1=4 (11) t  t  t  t  t  t  t  t  t  t  t  It would be more appropriate to say that our belief that the current chance is 3/4 or 1/4 is 1/2 and that in the absence of events that will inuence the chance, chance will remain unchanged till time t1 .
Such an inference would require some kind of theory of persistence, which is beyond the scope of this paper.
2  Using this information, we can make several useful inferences.
First we can derive the unconditional probability that the coin will land heads.
From (5) by SOMP we have Pnow (OCCURS (Heads t2 t3)) = (12) (1=2)(3=4) + (1=2)(1=4) = 1=2 Next, we can use the above information to derive the probability that the coin will come up heads given that it is observed to be shiny.
Instantiating (6) with  = 3=4 and t = t0 and multiplying the result by (7) we get Pnow (OCCURS (Heads t2 t3)^ (13) pr 0 (OCCURS (Heads t2 t3)) = 3=4 j OCCURS (Shiny t0  t2)) = (5=8)(3=4) = 15=32 And instantiating (6) with  = 1=4 and t = t0 and multiplying the result by (8) we get Pnow (OCCURS (Heads t2 t3)^ (14) pr 0 (OCCURS (Heads t2 t3)) = 1=4 j OCCURS (Shiny t0  t2)) = (3=8)(1=4) = 3=32 From (10), (13), and (14) by the law of total probability we get Pnow (OCCURS (Heads t2 t3) j (15) OCCURS (Shiny t0  t2)) = 9=16 We can also derive the probability of heads given that we activate the magnet.
From (3), (5), and (9) we get Pnow (OCCURS (Heads t2 t3)^ (16) pr 2 (OCCURS (Heads t2 t3)) = 3=4 j HOLDS (Magnet t1 t4)) = (1=2)(7=8) = 7=16 From (4), (5), and (9) we get Pnow (OCCURS (Heads t2 t3)^ (17) pr 2 (OCCURS (Heads t2 t3)) = 1=4 j HOLDS (Magnet t1 t4)) = (1=2)(1=2) = 1=4 From (10), (16), and (17) by the law of total probability we get Pnow (OCCURS (Heads t2 t3) j (18) HOLDS (Magnet t1 t4)) = 11=16 t  t  t  t  5.2 The temporal ow of causality  Using our denition of causal in	uence and SOMP we can now show that an agent whose beliefs are represented with L believes that the past cannot be in	uenced.
tcp  Theorem 13 Let be a fact or event: 0 0  HOLDS ( t  t ) or OCCURS ( t  t ) and let !
be a fact or event: HOLDS (fi t  t0 ) or OCCURS (fi t  t0 ).
Then all instances of the following sentence schema are valid in L .
8 t t  t0  t  t0 (t0  t ) ^ (t  t ) !
P ( jpr ( ) =  ^ !)
=              tcp    t      t              Proof: We prove a slightly more general result of which  the above sentence is an instance.
By the Subjective/Objective Miller's Principle, 8 t t0 t  t0 (t0  t0 ) ^ (t  t0) !
(19) P ( ^ pr ( ) = ) =   P (pr ( ) = ) Since valid formulas have probability one, it follows by Theorem 6 that, 8 t t0 t  t0  t  t0 (t0  t0 ) ^ (t  t0) !
(20) P ( ^ pr ( ) =  ^ pr ( ) = 0 _ pr ( ) = 1]) =   P (pr ( ) =  ^ pr ( ) = 0 _ pr ( ) = 1]) Since pr ( ) = 0 and pr ( ) = 1 are mutually exclusive, we have 8 t t0 t  t0 (t0  t0 ) ^ (t  t0) !
(21) P ( ^ pr ( ) =  ^ pr ( ) = 0) + P ( ^ pr ( ) =  ^ pr ( ) = 1) =   P (pr ( ) =  ^ pr ( ) = 0) +   P (pr ( ) =  ^ pr ( ) = 1) Now we have three cases to consider: i)  = 0, ii)  = 1, iii) 0 <  < 1.
      t0  t            t0  t  t0  t0  t  t0  t  t0  t0  t0  t0  t0      t  t0  t  t0    t0 t0  t  t0  t0  t  t0  t0  Case i)  Expression (21) reduces to (22) 8t t0 t  t0  t  t0 (t0  t0 ) ^ (t  t0) !
P ( ^ pr ( ) = 0) = 0  P (pr ( ) = 0) So by Theorem 4 and universal generalization, 8t t0 t  t0  t  t0 (t0  t0 ) ^ (t  t0) !
(23) P ( ^ pr ( ) = 0 ^ !)
= 0  P (pr ( ) = 0 ^ !)
          t0  t      t0  t        t0  t  t0  t  Case ii)  Expression (21) reduces to (24) 8t t0 t  t0 (t0  t0 ) ^ (t  t0) !
P ( ^ pr ( ) = 1) = P (pr ( ) = 1) So by Theorem 5 and universal generalization, 8t t0 t  t0  t  t0 (t0  t0 ) ^ (t  t0) !
(25) P ( ^ pr ( ) = 1 ^ !)
= P (pr ( ) = 1 ^ !)
      t0  t    t          t0  t  t0  t  t0  Case iii)  For 0 <  < 1, P (pr ( ) = ) = 0.
So by Theorem 4 and universal generalization, 8 t t0 t  t0  t  t0 (26) 0 0 0 (t  t ) ^ (t  t ) ^ (0 <  < 1) !
P ( ^ pr ( ) =  ^ !)
= P (pr ( ) =  ^ !)
Therefore we have proven that the following sentence is valid (27) 8 t t0 t  t0  t  t0 (t0  t0 ) ^ (t  t0) !
P ( jpr ( ) =  ^ !)
=  from which it follows that the past cannot be in	uenced.
2 t0  t            t0  t    t    t0    t      t0  6 Related Work  Three outstanding subjective theories of objective chance from the philosophical literature are those of van Fraassen 9], Lewis 6], and Skyrms 7].
van Fraassen's model of objective chance is more constrained than Lewis's model which is more constrained than Skyrms's model.
Thus, in van Fraassen's model, chance has more inherent properties than in either Lewis's or Skyrms's models.
van Fraassen's theory is the only one of the three that is cast in a temporal framework.
All three are semantic theories and do not provide logical languages.
The model of objective chance used in L is based on van Fraassen's 9] model of objective chance.
He presents a semantic theory that models subjective probability and objective chance, using a future-branching model of time points.
van Fraassen places two constraints on objective chance: 1.
The chance of a past is either 0 or 1, depending on whether or not it actually occurred.
2.
Chance at a time is completely determined by history of the world up to that time.
From these assumptions, he shows the following relation between subjective probability and objective chance P (X jY ) = E C (X )] where P is the subjective probability at time t, C is the objective chance at time t, E is the expected value given Y , and provided the truth of Y depends only on the history up to t. This relation entails both Miller's principle and Lewis's principal principle, discussed below.
Note that van Fraassen does not show that a similar relation holds between objective chances at dierent times.
In van Fraassen's models, objective chance can change with time but truth values cannot.
Lewis's 6] theory of objective chance is based on his assertion that ... we have some very rm and denite opinions concerning reasonable credence (subjective probability) about chance (objective chance).
These opinions seem to me to afford the best grip we have on the concept of chance.
He describes a number of intuitive relationships between subjective probability and objective chance and shows that these are captured by his principal principle: Pr(Ajpr (A) =  ^ E ) =  where Pr is subjective probability, pr is objective chance, and E is any proposition compatible with pr (A) =  and admissible at time t. The interesting thing here is the proposition E .
The constraint that E be compatible with pr (A) =  means that Pr(E ^ pr (A) = ) > 0.
Admissibility is less readily dened.
Lewis does not give a denition of admissibility but he does characterize admissible propositions as \the sort of information whose impact on credence about outcomes comes entirely by way of credence about the chances of those outcomes."
So objective chance is invariant with respect to conditioning on tcp  t  Y  t  t  t  Y  t  t  t  t  admissible propositions.
This concept of invariance under conditioning is the central notion of Brian Skyrms's theory of objective chance.
Skyrms 7] works with the notion of resiliency.
A probability value is resilient if it is relatively invariant under conditionalization over a set of sentences.
The resiliency of Pr(q) being  is dened as 1 minus the amplitude of the wiggle about : The resiliency of Pr(q) being  is 1;maxj; Pr (q)j over p1  ::: p , where the Pr 's are gotten by conditionalizing on some Boolean combinationof the p 's which is logically consistent with q. Skyrms then denes propensity (objective chance) as a highly resilient subjective probability.
Independent of his resiliency notion, Skyrms requires that propensities and subjective probabilities be related by Miller's principle: Pr(Ajpr(A) = ) =  where Pr is a subjective probability and pr is a propensity.
He shows that Millers' principle entails that subjective probabilities are equal to the expectation of the subjective probabilities applied to the objective probabilities.
But Skyrms 7, p158] points out that, counter to intuition, independence in every possible objective distribution does not imply independence in the subjective distribution.
This observation provided the motivation for our use of the two probabilities to distinguish causal from evidential correlation.
Halpern 4, 5] presents a probability logic that can represent both statistical and subjective probabilities.
Statistical probabilities represent proportions over the domain of individuals, while propositional probabilities represent degrees of belief.
The two probability operators in the language can be nested and combined freely with other logical operators.
So the language is capable of representing sentences like \The probability is .95 that more than 75% of all birds can 	y."
The models for the language contain a domain of individuals, a set of possible worlds, a single discrete probability function over the individuals, and a single discrete probability function over the possible worlds.
The rst probability function is used to assign meaning to the statistical probability operator, while the second is used to assign meaning to the propositional probability operator.
Although he does not place constraints within the logic on the relation between the two probabilities, he does discuss a form of Miller's principle that relates subjective and objective probabilities.
His version of the principle states that \for any real number r0 the conditional probability of (a), given that the probability of a randomly chosen x satises  is r0, is itself r0."
He points out that this could be used as a rule for inferring degrees of belief from statistical information.
Bacchus 1] presents a logic essentially identical to that of Halpern.
He goes further than Halpern in exploring the inference of degrees of belief from statistical probabilities.
According to his principle of direct inference, an agent's belief in a formula is the expected j  n  i  j  value with respect to the agent's beliefs of the statistical probability of that formula, given the agent's set of accepted objective assertions.
References  1] F. Bacchus.
Representing and Reasoning With Probabilistic Knowledge.
MIT Press, Cambridge, Mass, 1990.
2] P. Haddawy.
Believing change and changing belief.
Technical Report TR-94-02-01, Dept.
of Elect.
Eng.
& Computer Science, University of Wisconsin-Milwaukee, February 1994.
Available via anonymous FTP from pub/tech_reports at ftp.cs.uwm.edu.
3] P. Haddawy.
Representing Plans Under Uncertainty: A Logic of Time, Chance, and Action, volume 770 of Lecture Notes in Arti	cial Intelligence.
Springer-Verlag, Berlin, 1994.
4] J.Y.
Halpern.
An analysis of rst-order logics of probability.
In Proceedings of the International Joint Conference on Arti	cial Intelligence, pages 1375{1381, 1989.
5] J.Y.
Halpern.
An analysis of rst-order logics of probability.
Arti	cial Intelligence, 46:311{350, 1991.
6] D. Lewis.
A subjectivist's guide to objective chance.
In W. Harper, R. Stalnaker, and G. Pearce, editors, Ifs, pages 267{298.
D. Reidel, Dordrecht, 1980.
7] B. Skyrms.
Causal Necessity.
Yale Univ.
Press, New Haven, 1980.
8] B. Skyrms.
Higher order degrees of belief.
In D.H. Mellor, editor, Prospects for Pragmatism, chapter 6, pages 109{137.
Cambridge Univ.
Press, Cambridge, 1980.
9] B.C.
van Fraassen.
A temporal framework for conditionals and chance.
In W. Harper, R. Stalnaker, and G. Pearce, editors, Ifs, pages 323{340.
D. Reidel, Dordrecht, 1980.
10] J. Venn.
The Logic of Chance.
MacMillan, London, 1866.
(new paperback edition, Chelsea, 1962).
11] R. von Mises.
Probability, Statistics and Truth.
Allen and Unwin, London, 1957.
A language to express time intervals and repetition Diana Cukierman and James Delgrande School of Computing Science Simon Fraser University Burnaby, BC, Canada V5A 1S6 fdiana,jimg@cs.sfu.ca  Abstract  We are investigating a formal representation of time units, calendars, and time unit instances as restricted temporal entities for reasoning about repeated activities.
We examine characteristics of time units, and provide a categorization of the hierarchical relations among them.
Hence we dene an abstract hierarchical unit structure (a calendar structure) that expresses specic relations and properties among the units that compose it.
Specic time objects in the time line are represented based on this formalism, including non-convex intervals corresponding to repeated activities.
A goal of this research is to be able to represent and reason eciently about repeated activities.
1 Introduction  The motivation for this work is to ultimately be able to reason about schedulable, repeated activities, specied using calendars.
Examples of such activities include going to a specic class every Tuesday and Thursday during a semester, attending a seminar every rst day of a month, and going to tness classes every other day.
Dening a precise representation and developing or adapting known ecient algorithms to this domain would provide a valuable framework for scheduling systems, nancial systems, process control systems and in general date-based systems.
We search for a more general, and formalized representation of the temporal entities than in previous work.
We explore further the date concept, building a structure that formalizes dates in calendars.
Schedulable activities are based on conventional systems called calendars.
We use as a departure point \calendar" in the usual sense of the word.
Examples of calendars include the traditional Gregorian calendar, university calendars, and business calendars, the last two groups being dened in terms of the Gregorian.
The framework we dene concerns a generic calendar abstract structure, which subsumes the mentioned calendars, and arguably any system of measures based on discrete units.
Calendars can be considered as repetitive, cyclic temporal objects.
We dene an abstract structure that formalizes calendars as being composed of time units, which are related by a  decomposition relation.
The decomposition relation is  a containment relation involving repetition and other specic characteristics.
Time units decompose into contiguous sequences of other time units in various ways.
A calendar structure is a hierarchical structure based on the decomposition of time units.
This structure expresses relationships that hold between time units in several calendars.
We refer to the concept of time unit instances, and distinguish dierent levels of instantiation.
Whereas \month" refers to a time unit class, \June" is referred to as a named time unit.
June is one of the 12 occurrences of the notion of month with respect to year, and viewed extensionally it represents the set of all possible occurrences of June.
Finally, \June 1994" is one specic instance of a month.
2 Related work  Our formalism deals with time units, which are a special kind of time interval with inherent durations.
Therefore we base our work on time intervals as the basic temporal objects 1].
In 21], the time point algebra is developed, based on the notion of time point in place of interval.
Computation of the closure of pairwise time-interval relations in the full interval algebra is NP-complete, whereas the time point algebra has a polynomial closure algorithm.
However the time point algebra is less expressive than the interval algebra: certain disjunctive combinations of relations are not expressible with the time point algebra.
Nonetheless, some applications do not require the full expressive power of the interval algebra, and can benet from ecient (albeit less expressive) representations.
Hence it is of interest to study restrictions of the interval algebra.
The present framework deals with restricted kinds of intervals within a hierarchical structure.
The intent is that the hierarchy provide a basis for obtaining ecient algorithms for certain operations.
(This may be contrasted with 11] which considers a hierarchical structure in the general interval algebra.)
Nonconvex intervals (intervals with \gaps") are employed in 12, 13] when using time units in a repetitive way or when referring to recurring periods.
The time unit hierarchy proposed in our work generalizes 13], in that temporal objects can be represented by any se-  quence of composed time units, as opposed to xed in Ladkin's approach.
Moreover, we are able to combine systems of measurement, and so talk about the third month of a company's business year as corresponding with June in the Gregorian calendar.
13] also does not take into account the varying duration of specic time instances, a matter addressed and formalized in our work.
In addition we address the varying duration of specic time instances.
18] also elaborate on the notions of non-convex interval relations dened in 12, 13] while 16] proposes a generalization of nonconvex intervals.
Leban et.
al.
15] deals with repetition and time units.
This work relies on sequences of consecutive intervals combined into \collections".
The collection representation makes use of \primitive collections" (essentially circular lists of integers), and two basic operators, slicing and dicing, which subdivide an interval and select a subinterval respectively.
Poessio and Brachman 19] are mainly concerned with the implementation of algorithms to detect overlapping repeated activities.
This work relies on temporal constraint satisfaction results and algorithms 8].
19] also introduces the concept of using dates as reference intervals to make constraint propagation further ecient.
We envision our proposed formalism provides a useful framework to follow this idea.
5] exposes a set theoretic structure for the time domain with a calendar perspective.
It formalizes the temporal domain with sets of \constructed intervallic partitions" which have a certain parallel to our decomposition into contiguous sequences of intervals.
However, this formalization is more restricted than ours and can not handle the Gregorian nor general calendars.
3] and 4] propose formalizations that deal with calendars and time units.
These two papers are interestingly related to our research, even though they have evolved independently.
These works are analyzed and compared with our research in the Section 5.
3 Time units and time unit instances  The central element of our formalism is that of a time unit.
Time units represent classes of time intervals, each with certain commonalties and which interact in a limited number of ways.
For example, year and month are time units.
Something common to every year is that it decomposes into a constant number of months.
A characteristic of month is that it decomposes into a non-constant number of days, which vary according to the instance of the month.
Properties that are common to time units determine the time unit class attributes.
In 6, 7], the identier of a time unit class and the (general) duration are presented.
Relative and general durations are compared and the concept of an atomic time unit is introduced.
Time unit instances and their numbering and naming is described as well.
All attributes are formally dened in a functional way.
We here  present a summary of these concepts with examples from the Gregorian calendar.
Identier of a time unit The rst attribute of the time unit class is a unique identier, a time unit name, for example year or month.
We distinguish time unit names when the time units have the same duration but dier in their origin.
For example, years in the Gregorian calendar start in January, but academic years, in university calendars in the northern hemisphere, start in September.
Year and academic year are two dierent time units, which have several properties in common.
Duration Time units inherently involve durations a time unit expressly represents a standard adopted to measure periods of time.
Dierent time unit instances of the same time unit class can have dierent durations.
For example dierent months have different number of days, from 28 to 31.
Accordingly, the attribute representing a duration of a class is a range of possible values.
We represent this range by a pair of integers, the extremes of the range of possible lengths any time unit instance can have.
Thus, month as a class has a duration of (28 31).
A duration referred to as general will be expressed in a basic unit, common to all the time units in one calendar.
For example, using day as a common or basic unit, month has a (general) duration of (28 31) days.
A specic month, for example, February 1994, has a duration of 28 days.
We also dene another kind of duration a duration relative to another time unit.
An important reason why (general) durations of all time units in a calendar are dened based on the same basic measure unit is to be able to compare them.
This notion plays a fundamental role in the partial order among time units in the same calendar or variants of a calendar.
Time units decompose into smaller units up to a nite level at which a time unit is atomic | a non-divisible interval.
When the time interval is non-decomposable it is called a time moment, following 2].
Time units therefore model time in a discrete fashion.
Time units may be considered atomic in one application and decomposable into smaller time units in other applications, depending on the intended granularity for the application 10].
Instance names and numbers The name or number of a time unit instance can be expressed in several ways.
Similar to durations, the name is relative to another time unit, a reference time unit.
For example, a day instance can be named from Sunday to Saturday or numbered 1 to 7 if week is the reference time unit of day.
(Names can be thought of as synonyms for the numbers).
Instances of time units which do not have any reference time unit are numbered relative to a conventional reference point or zero point of the calendar, for example the Christian Era for the Gregorian calendar.
The ordered set of all the possible names the instances of a time unit can take within a reference can be expressed extensionally, by a sequence of names (if there are names associated to the time unit class), or a pair of numbers representing the range of maximum possible instance numbers of a class.
This sequence (or pair of numbers) is an attribute of the time unit class.
A particular instance name or number of a time unit within a reference reects the relative position of the instance within the reference, given a certain origin where counting of instance values starts.
For example, months are counted from 1 to 12, in a year in the Gregorian calendar, starting from 1.
But, in the case of months counted within an academic year, the origin of numbering is not month number 1, but rather the 9th (or September), so the 3rd month of a academic year is the 11th month of all the possible months name sequence (or November).
Circular counting is assumed.
More detail appears in 6].
3.1 Decomposition of time units  The primary relation among time units is that of decomposition.
When A decomposes into B, A will be referred to as the composed time unit, and B will be referred to as the component unit.
For example, a year decomposes into months and a month into days.
Also a month decomposes into weeks, a week into days, etc.
Clearly there are dierent kinds of decompositions: a year decomposes exactly into 12 months, whereas a month decomposes in a non-exact way into weeks, since the extreme weeks of the month may be complete or incomplete weeks.
We propose that all these variations in the decomposition relation can be captured with two dierent aspects of the relation: alignment and constancy.
A time unit may decompose into another in an aligned or non-aligned fashion.
The decomposition is aligned just when the composed time unit starts exactly with the rst component and nishes exactly with the last component.
Consequently, a certain number of complete components t exactly into the composed time unit.
Examples of aligned decompositions include year into months, month into days, and week into day.
Examples of non-aligned decompositions include year into weeks and month into weeks.
Figure 1 shows a graphical picture of aligned and non-aligned decomposition.
A (a) Bx  By A  A  A (b)  Bx  By  Bx  By  Bx  By  Figure 1: Graphical representation of Alignment A time unit may decompose into another in a constant or non-constant fashion.
The decomposition is constant when the component time unit is repeated  a constant number of times for every time unit instance.
Examples of constant decompositions include year into months and week into days.
Examples of non-constant decompositions include month into days and year into days.
There are four possible combinations resulting from these two aspects of alignment and constancy.
These combinations cover examples from the various calendars analyzed.
(Arguably) all the relationships of interest in such systems are covered by the variants resulting from combining alignment and constancy of decomposition.
We analyze the composition (or product), intersection (or sum) and inverse of decomposition relations.
For example, if two aligned decomposition relations are multiplied, the resulting decomposition relation is also aligned.
For example year decomposes into month, month decomposes into day.
These two (aligned) decomposition relations can be composed (or multiplied) to obtain the (aligned) decomposition relation of year into day.
It should be noticed that these three operations (composition, intersection and inverse) are dened among decomposition between time unit classes.
This is to be contrasted to the relational algebras dened in the literature and constraint propagation algorithms, which deal with relations between time intervals in the time line, i.e., at the instance level (for example 1, 18, 14].)
A detailed study of these operations appears in 6].
3.2 Calendar Structures: time unit hierarchies  A calendar structure is a pair: a set of time units and a decomposition relation.
We obtained the following result:  Theorem 1 (Decomposition) The decomposition  relation is a particular case of containment, and constitutes a partial order on the set of time units in a calendar.
Therefore, a calendar structure is dened as a containment time unit hierarchy.
The structure is dened so that variants of a specic calendar can be dened in terms of a basic one.
Such would be the case of a university or business calendar, based on the Gregorian calendar.
Such calendars have the same time units as the basic one, with possible new time units or a dierent conventional beginning point.
For example, many university calendars would have a semester time unit, where the academic year begins in September.
Since calendar structures are partial orders, they can be represented by a directed acyclic graph.
The set of nodes in a calendar structure represents the set of time units.
Edges represent the decomposition relation.
The intended application of use of the calendar structure determines which level of time units is included.
Chains We develop a categorization based on the  subrelations of decomposition, i.e.
considering only constant/aligned decompositions, or aligned only, or constant only.
Calendar substructures, composed of chains result from these subrelations.
The term \chains" appears in 17], however there chains are dened on time intervals on the time line.
In our case we are referring to chains of time unit classes.
Nonetheless, chains of time unit classes are directly related to expressions that represent time unit instances, and inuence greatly in eciency matters.
To give an example, the operation of converting time unit instances from one time unit to another will be more ecient when the time units intervening in the time unit instance expression decompose in a constant/aligned way divisions and multiplications can be done, whereas it is necessary to have some iterative process of additions or subtractions when the decomposition is not exact.
A chain is a consecutive linear sequence of time units, such that each one decomposes into any other in the chain in the same way.
Hence all time units in an aligned chain decompose in an aligned way, etc.
A calendar structure can be organized according to these chains.
There is a dierent subgraph associated to each type of decomposition.
For example, Figure 2 shows the Gregorian calendar structure characterized by two aligned chains: <28-centuries, 4-centuries, century, year, month, day, hour> and <28-centuries, week, day, hour>.
In this gure we can observe there are three common nodes to both chains: 28-centuries,day and hour.
28-Centuries 4-Centuries  Century  Year  Month  Week  Day  Hour  Figure 2: Aligned Gregorian calendar substructure We refer to the subgraph that results from organizing the calendar structures according to a certain type of chain as a calendar substructures of that particular type of decomposition (constant, aligned or constant/aligned).
Therefore a chain of a certain type is composed by all those time units in the hierarchy that form a path in the corresponding calendar substructure.
3.3 A language of the set of time units  We dene a language of time units based on the fact that calendar structures can be organized in constant/aligned, constant or aligned chains.
Together with the organization of calendar structures in chains, we distinguish special time units as primitive.
The set of primitive time units includes one time unit per chain in a minimal way.
Hence, in case that all chains have at least one common node, the primitive set is a singleton.
It was proved 6] that any calendar structure can be extended (i.e.
added time units) so that there is such unique common time unit to all chains, for any type of chain.
For example, if the Gregorian calendar is organized as in Figure 2, there are three minimal sets of primitive time units: fhourg, fdayg or f28-centuriesg.
The main idea of this language is that all time units in the structure can be recursively constructed starting from a set of primitive time units, decomposing or composing them, with decomposition limited to an aspect (for example only aligned).
The symbols that are used in this language are: A set of primitive time units, PRIM = fP P1 P2 : : :g, a set of special symbols, f = ( )g a set of (possibly innitely many) constants, CONS = fsecond minute hour :::g and a language to express durations, DUR = fd j d 2 N + g fi f(d1 d2) j + d1 d2 2 N and d1 < d2g.
The language is dened: 1.
If P 2 PRIM, then P 2 TUS.
2.
If C 2 CONS, then C 2 TUS.
3.
If T 2 TUS and D 2 DUR then (T  D) 2 TUS.
4.
If T 2 TUS and D 2 DUR then (T=D) 2 TUS.
5.
Those are all the possible elements of TUS.
Briey, T = (S  D) when T is composed of a contiguous sequence of D S's and T = (S=D) when D contiguous T's compose S. For example, if we organize the Gregorian calendar with aligned chains, the following is a possible interpretation of the language: PRIM = fdayg month = day  (28 31) year = day  (365 366) = month  12 hour = day=24 twoday = day  2 trimester = month  3: Generally we consider only one time unit as primitive.
It is convenient to include constants into the language.
We abuse notation in that we name constants, which are part of the alphabet of symbols, by the name of the domain elements.
Thus if  is an interpretation function,  : TUS  Calendar ;!
Time units in the calendar.
For example (month Gregorian) = \month", (month  3 University) = \trimester".
As long as it does not lead to ambiguities, we will not make an explicit use of this interpretation function in this paper.
Using the same strings for constants and domain elements appears elsewhere, for example 20].
3.3.1 Named time units  As discussed in previous sections, there is more than one level of time unit instantiation.
For exam-  ple, a subclass of month in the Gregorian calendar could be the fourth month (synonym of April).
This does not represent a specic month yet.
We call April a named-month, and viewed extensionally, it represents the set of all specic instances of the month \April".
A specic instance would be \April 1995".
To be able to express specic instances we dene calendar expressions in the next section.
Named time units are dependent on the time unit, a reference time unit and a position within the reference.
Names and numbers are functions dened on the sets of time units, and some involving instances as well.
We write name(T R X) to represent the Xth \named-T" within the reference time unit R, such that R decomposes into T .
If X is outside permissible values, or if the time unit has no associated names with that reference name(T R X) represents an inconsistent value (?).
Examples of this function include name(month year 3) = \March" name(month academic year 3) = \November" name(day week 8) = ?.
Numbered-time units are dened in an analogous way.
For example number(month year 3) = \3".
A function related to named and numbered time units is the last possible value.
Some cases, as already explained, will not provide a unique value, but a range of last values.
In those cases there exists a unique last value only at the instance level (and not at the class level).
For example, last name(month year) = \December" last number(day month) = (28 31) last number instance(day F ebruary 1994) = 28.
4 Calendar expressions  A goal of our research is to represent and reason with time unit instances, that is, the temporal counterpart of single or repeated activities occurring in the time line.
We want to express these time entities in terms of days, weeks, hours, etc, relative to certain conventional calendars.
Calendar structures above dened provide a formal apparatus of units on which to represent a date-based temporal counterpart of activities.
Intervals, time points and moments in the time line will be represented in terms of these units.
Specic time objects are expressed based on the time units in a calendar structure, via calendar expressions.
A basic calendar expression is dened by a conventional beginning reference point or zero point associated to the calendar, and a nite sequence of pairs: Z 	 (t1 x1) : : : (tn xn)].
Each pair (ti xi) contains a time unit ti from the calendar structure and a numeric expression xi.
Numeric expressions include numbers and variables ranging over the set of integers.
The pairs in the sequence are ordered so that any time unit in a pair decomposes into the time unit in the following pair in the sequence.
We also dene duration expressions.
These are similar to calendar expressions in that they consist of a list of pairs (time unit,value), but have no beginning refer-  ence point nor included variables.
The operation of adding a duration expression to a calendar expression denes a new calendar expression.
4.1 A language for simple and repetitive time unit instances: calendar expressions  The formal denition of the language of calendar expressions is presented next.
The language of duration expressions is not introduced here for space reasons, it follows a similar style as the calendar expressions language.
The decomposition relation is used in the denitions.
The same kind of decomposition used to dene the time units in the calendar structure (and therefore the time units language, TUS) is used in these languages.
The language of calendar expressions CALXS, uses the following: A conventional zero point, Z, a set of special symbols f	 ( )  ] \ fi = Lastg, a set of time units (TUS), a set of variables (VAR), which will be ranging in the set of positive naturals, a set of duration expressions(DURXS), and can be dened as: 1.
If T 2 TUS X 2 N + fi VAR then (Z 	 (T X)]) 2 CALXS.
2.
If T1 T2 2 TUS, such that T1 decomposes into T2 , X2 2 N + fi VAR fi fLastg and Z 	 CX (T1 X1)] 2 CALXS, then (Z 	 CX (T1 X1 ) (T2 X2 )]) 2 CALXS.
3.
If CX(*v ) 2 CALXS, where *v are the variables * * in CX, then (CX( v ) where Exp( v )) 2 CALXS Exp(*v ) is an expression constraining *v .
4.
If CX1 and CX2 2 CALXS then (CX1 fi CX2 ) (CX1 \ CX2 ) (CX1 =CX2) 2 CALXS.
5.
If CX 2 CALXS and DX 2 DURXS then (CX + DX) 2 CALXS.
6.
These are all the possible elements of CALXS.
4.2 Examples of calendar expressions  The following examples express calendar expressions in the Gregorian calendar.
The zero point is the beginning of the Christian Era (CE).
1.
CE 	 (year X) (month 4)].
The fourth month within any year.
(Viewed extensionally, it represents the set of all specic instances of the month \April").
X is a non-quantied variable.
2.
CE 	 (year 1994) (week 17) (day 5)].
The 5th day within the 17th week of the year 1994.
The last time unit in the sequence provides the precision of the time unit instance.
Thus, in Example 1 above, the precision is month.
It can also be observed that a calendar expression with no variables is a single convex interval a time unit instance of the last time unit in the calendar expression.
Consequently, Z 	(t1 x1) : : : (tn xn)] is a (single) tn-instance.
Example 2 above is a (single) day-instance.
A calendar expression with variables represents a  set of time unit instances.
These sets of intervals are  the temporal counterpart of a date-based repeated activity, a specic case of a non-convex interval, as dened in 12] .
Thus, CE 	 (year X) (day 175)] represents a non-convex interval, and the subintervals are the days numbered 175th.
As well, when there are variables in the calendar expression, these can be constrained with logic/mathematical expressions.
This example can be extended to: 3.
CE 	 (year Y ) (day 175)] where (Y > 1992 and Y < 1996).
The 175th day from the beginning of each year after 1992 and before 1996.
Hence starting and ending times of non-convex intervals can be expressed straightforwardly.
Another extension to the language of calendar expressions consists of applying set operations (union, intersection and dierence) to combine non-convex intervals, viewing non-convex intervals as sets of subintervals.
(limit cases could produce an empty set of intervals, which we consider a limit case of calendar expression).
The following illustrate this possibility: 4.
CE 	 (year 1994) (month 11) (day X)] \ CE 	 (year 1994) (week W) (day Tuesday)].
Tuesdays of November 1994.
5.
CE 	 (year 1994) (month 11) (day X)] = (CE 	 (year 1994) (month W) (day Y )] where (Y = 7 or Y = 1) ).
Days of November 1994 which are not Saturdays nor Sundays, i.e., weekdays of November 1994.
Saturday and Sunday are abbreviations of certain days numbers as numbered within week.
= represents set dierence.
Finally we also allow calendar expressions to be moved (or displaced) by adding a certain (convex) interval.
For example, October 1994 plus one month is November 1994.
We use duration expressions to express moved expressions.
For example: 6.
CE 	 (year 1994) (month 11) (day 5)] + (day 5)].
The 5th day of November is moved 5 days, resulting in the 10th day of November 1994.
7.
CE 	 (year 1994) (month X) (day 5)] + (day 5)] The 10th day of every month in 1994.
It is worth noticing that (month 1)] is a duration expression representing one month.
On the contrary, CE 	 (year Y ) (month 1)], stands for the non-convex interval of all January's.
Named time units are precluded as a valid duration.
The following examples show how to represent slightly more elaborated periodicity patterns.
8.
CE 	 (year 1994) (week  2 W ) (day 1)].
Every other Monday of 1994.
9.
CE 	 (year 1994) (month M) (day Last)].
Last day of every month.
\Last" is based on the last number function above presented.
Examples 1, 2 and 8 above are accounted by rules 1 and 2 of the language.
Rule 3 allows to have calendars with variables that are constrained, as in example 3.
Examples 4 and 5 are covered by rule 4.
Finally examples 6 and 7 correspond to rule 5.
4.3 Semantics of calendar expressions  A set-based semantics is proposed for these languages.
Interpretation of duration expressions and calendar expressions is based on an interpreted calendar structure.
Recall that we conventionally name constants in the language of the time units (TUS) as the elements in the domain, i.e.
as the time units in the calendar.
Therefore, for the sake of a more clear presentation we will omit the application of an interpretation function when referring to interpreted time units whenever this does not lead to ambiguities.
Examples will be made within the Gregorian calendar.
Hence the zero point Z will be interpreted as year zero of the Christian Era (CE).
A duration expression is interpreted as a convex interval in the time line.
Comparison, addition and subtraction of duration expressions are dened.
Addition and subtraction can be viewed as translations or displacements in the time line.
Limit cases, circular counting, dierence in precision, etc, are taken into account.
This is not presented in this paper.
Calendar expressions are interpreted as sets of intervals in the time line.
Let !
be a function interpreting calendar expressions.
(We omit here the specication of the calendar to simplify this presentation, and again will provide examples from the Gregorian Calendar), then !
: CALXS ;!
2time intervals.
1.
!
(Z 	 (T X)]) = a.
If X 2 N + , the singleton with the X th occurrence of the interval T starting from !(Z).
For example, !
(Z 	 (year 1995)]) = fyear 1995g.
b.
If X is a variable, Sthe set of time intervals T starting from !
(Z), i.e.
1 X =1 !
(Z 	 (T X)]).
For example, !
(Z +	 (year Y )]) = set with every year since CE = N .
2.
!
(Z 	 CX (T1 X1 ) (T2 X2 )]) = a.
If X2 2 N + , the set of X2th subintervals T2 within each interval in the set !
(Z 	 CX (T1 X1)]).
For example, !
(Z 	 (year Y ) (month April)]) = fApril 1, April 2, : : : g. b.
If X2 = Last, the set of Lth subintervals T2 within each interval in the set !
(Z 	 CX (T1 X1 )]), where L = last number(T2 T1) as dened in Section 3.3.1 above.
For example !
(Z 	 (year Y ) (month Last)]) = f December 1, December 2, : : : g. In case T1 decomposes into T2 in a non-constant way, L depends on X1 and possibly on CX.
For example !
(Z 	 (year 95) (month M) (day Last)]) = fJanuary 31 1995, February 28 1995, : : : g. c. If X is a variable, the set of all time intervals T2 within eachS interval in the set !
(Z 	 CX (T1 X1 )]), i.e.
LX =1 !
(Z 	 CX (T1 X1)]), where L = last number(T2 T1).
For example, !
(Z 	  (year 1995) (month M)]) = fJanuary 1995, February 1995, : : : , December 1995 g. 3.
!
(CX(*v ) where Exp(*v )) = S* * !
(CX(*v )).
v j Exp( v ) For example, !
(Z 	 (year 95) (month M)] where (M > 3 and M < 6) ) = fApril 1995, May 1995g.
4.
!
(CX1 fi CX2 ) = !
(CX1 ) fi !
(CX2 ) !
(CX1 \ CX2 ) = !
(CX1 ) \ !
(CX2 ) !
(CX1 =CX2 ) = !
(CX1 )=!
(CX2 ) See examples in Section 4.2 above.
5.
!
(CX = S T + DX) (DX ) (Interval) 8 Interval 2 !
(CX) Addition of a duration expression is interpreted as a translation (T) or displacement of all the subintervals denoted by the calendar expression, eventually a single one (!
(CX)), by a distance dened by the duration expression (!(DX)).
See examples in Section 4.2 above.
5 Alternative proposals dealing with calendars  3] by Chandra et.al.
present a language of \calendar expressions" to dene, manipulate and query what they call \calendars".
In terms of terminology used, their and our proposals coincidentally refer to \calendars" and \calendar expressions".
The notions however are of a dierent nature the two researches have evolved completely independently.
We refer to \calendar" in the usual sense, where the Gregorian calendar or a university calendar are possible examples.
On the other hand, their \calendars" are structured collections of intervals, as dened in 15].
The dierence between both \calendar expressions" is more subtle.
Both calendar expressions dene, in dierent ways, lists of points and intervals in the time line.
However, our expressions are of a more declarative nature, whereas their expressions are closer to a procedure to obtain such intervals.
For example, we express the collection of Fridays as: CE 	 (year Y ) (week W ) (day 5)].
The calendar expressions in 3] are meant as a way of creating and manipulating their \calendars" (i.e., collections of intervals).
5]/DAYS:during:WEEKS expresses the Fridays collection in their language.
There are two operations involved here: selection and the strict foreach operation (or dicing, as dened in 15]) with the operator during.
We have continued further a formal denition of the domain and languages.
On the other hand, Chandra et al.
's work includes a description of an algorithm to parse their expressions and generate a procedural plan to produce an evaluation plan, implemented in Postgress.
As well, they outline how to incorporate calendar expressions to temporal rules in a temporal data base.
We have not dealt with temporal  databases however it is perceived as an area in which to apply our framework.
Ciapessoni et al.
4] dene a many sorted rst order logic language, augmented with temporal operators and a metric of time.
The language is an extension of a specication language TRIO 9].
We will focus the present discussion on the semantics of their system, specially in the extension they provide to embed time granularities in the language.
At this stage of our research, this is where we see the two works as being comparable.
The semantics of the language in 4] includes a temporal universe which consists of a nite set of disjoint and dierently grained \temporal domains".
Each temporal domain contains temporal instants expressed in the corresponding granularity.
Time domains relate with a granularity (or coarseness) relation and a disjointedness relation.
Very briey, our \time units classes" are related to their \time domains" our \decomposition relation" to their \granularity relation" our \repetition factor" to their \conversion factor", our \aligned decomposition" to their \disjointedness" relation.
Another similar characteristic in both proposals is the distinction between time moments and durations (or displacements in their terminology).
We deal with a more general concept of repetition factor.
In fact we extensively address non-constant decomposition.
We emphasize how to express temporal entities based on dates with the conventions of the Gregorian and other calendars.
At the present stage we specially address the formal representation and reasoning about repetitive temporal terms.
As well, we believe that our abstraction of decomposition with only two characteristics (constancy and alignment) denes the relations of interest between the time unit classes in a more general way, as compared to the distinction of dierent relations in 4].
6 Discussion  The goal of this work is to be able to represent and reason eciently about repeated activities using the formalism.
We have dened a hierarchical structure of time units, emphasizing on decomposition among the time units.
We take into consideration the distinction between time unit classes, named time units and specic time unit instances, (for example \month", \August", and \August 1994").
A characterization of the hierarchical structure considers subrelations of decomposition, according to the dierent aspects of decomposition: aligned, constant, and constant/aligned.
A path in such a substructure is referred to as a chain, where all time units decompose in the same way.
Chains constitute the basis we use to dene a formal language to characterize time units.
We also introduce a language to represent time unit instances, called calendar expressions.
These ex-  pressions provide for a straightforward way of representing the temporal counterpart of repeated activities.
We envision that reasoning with calendar expressions based on time units in one chain will produce very ecient operations.
Constant/aligned chains are expected to be particularly ecient.
Currently we are working on further formalizing useful operations between calendar expressions.
A set based semantics has been provided for such expressions.
We are investigating alternative semantic characterizations.
We are also considering adding more expressive power to the language to consider for example expressions that add uncertainty, as for example \twice a week".
Important future research includes studying algorithms that would best t with this formalism, so that we may obtain ecient inferences when reasoning about repeated activities.
Algorithms developed for qualitative and/or quantitative temporal constraint satisfaction problems, or variations, could be considered in this matter.
A language which allows one to dene time units of a variant calendar, such as a university calendar, in terms of a basic calendar, such as the Gregorian, is under study also.
Finally, we believe that the formalism dened represents a generic approach, appropriate to ultimately reason eciently with repeated events within any measurement system based on discrete units that relate with a repetitive containment relation, such as the Metric or Imperial systems.
References  1] J. F. Allen.
Maintaining knowledge about temporal intervals.
Communications of the ACM, 26(11):832{843, 1983.
2] J. F. Allen and P. J. Hayes.
Moments and points in an interval-based temporal logic.
Computational Intelligence, 5:225{238, 1989.
3] R. Chandra, A. Segev, and M. Stonebraker.
Implementing calendars and temporal rules in next generation databases.
In Proc.
of the International Conference on Data Engineering, ICDE94, pages 264{273, 1994.
4] E. Ciapessoni, E.Corsetti, A. Montanari, and P. San Pietro.
Embedding time granularity in a logical specication language for synchronous real-time systems.
Science of Computer Programming, 20:141{171, 1993.
5] J. Cliord and A. Rao.
A simple, general structure for temporal domains.
In C. Rolland, F. Bodart, and M. Leonard, editors, Temporal aspects of information systems, pages 17{28.
Elsevier Science Publishers B.V. (North-Holland), 1988.
6] D. Cukierman.
Formalizing the temporal domain with hierarchical structures of time units.
M.Sc.
Thesis.
Simon Fraser University, Vancouver, Canada, 1994.
7] D. Cukierman and J. Delgrande.
Hierarchical decomposition of time units.
In Workshop of Temporal and Spatial reasoning, in conjunction with AAAI-94, pages 11{17, Seattle, USA, 1994.
8] R. Dechter, I. Meiri, and J. Pearl.
Temporal constraint networks.
Articial Intelligence, 49:61{ 95, 1991.
9] C Ghezzi, D. Mandrioli, and A. Morzenti.
Trio, a logic language for executable specications of real-time systems.
Journal of Systems and Software, 12(2), 1990.
10] J. Hobbs.
Granularity.
In Proc.
of the Ninth IJCAI-85, pages 1{2, 1985.
11] J.
A. G. M. Koomen.
Localizing temporal constraint propagation.
In Proc.
of the First Inter-  national Conference on Principles of Knowledge Representation and Reasoning, pages 198{202,  12] 13] 14] 15] 16] 17] 18] 19] 20]  21]  Toronto, 1989.
P. Ladkin.
Time representation: A taxonomy of interval relations.
In Proc.
of the AAAI-86, pages 360{366, 1986.
P. B. Ladkin.
Primitives and units for time specication.
In Proc.
of the AAAI-86, pages 354{359, 1986.
P. B. Ladkin and R. D. Maddux.
On binary constraint problems.
Journal of the ACM, 41(3):435{469, 1994.
B. Leban, D. D. McDonald, and D. R. Forster.
A representation for collections of temporal intervals.
In Proc.
of the AAAI-86, pages 367{371, 1986.
G. Ligozat.
On generalized interval calculi.
In Proc.
of the AAAI-91, pages 234{240, 1991.
S. A. Miller and L. K. Schubert.
Time revisited.
Computational Intelligence, 6(2):108{118, 1990.
R. A. Morris, W. D. Shoa, and L. Khatib.
Path consistency in a network of non-convex intervals.
In Proc.
of the IJCAI-93, pages 655{660, 1993.
M. Poesio and R. J. Brachman.
Metric constraints for maintaining appointments: Dates and repeated activities.
In Proc.
of the AAAI-91, pages 253{259, 1991.
R. Reiter.
Towards a logical reconstruction of relational database theory.
In M. L. Brodie, J. Mylopoulos, and J. W. Schmidt, editors, On Conceptual Modeling, pages 191{238.
SpringerVerlag, 1984.
M. Vilain and H. Kautz.
Constraint propagation algorithms for temporal reasoning.
In Proc.
of the AAAI-86, pages 377{382, 1986.
Study and Comparison of Schema Versioning and Database Conversion Techniques for Bi-temporal Databases Han-Chieh Wei and Ramez Elmasri Department of Computer Science and Engineering The University of Texas at Arlington {wei, elmasri}@cse.uta.edu Basically, the difference between schema evolution and schema versioning is that schema evolution only keeps the current schema and corresponding data, whereas schema versioning preserves versions of schema and data during the evolution.
Most current database systems are categorized as snapshot databases, where there exists only one version of a schema and database at any time.
Therefore, whenever a schema is changed, the new schema becomes the current schema and thus the old schema is obsolete and will not be kept.
The legacy data that followed the old schema become inconsistent and must be converted to the new schema.
The problem of schema evolution is that the application programs written against the old schema and affected by the schema changes may need to be modified or at least need to be recompiled.
Obviously this is very time consuming.
Another problem of this technique is that some applications, such as banking, auditing, medical records, and reservation systems require not only current but also past or even future information.
These problems give rise to the approach of schema versioning.
Basically, whenever the schema is changed, a new schema is created and defined as a new version of the schema.
Accordingly, the new version of data is converted from the old data to be consistent with the new schema.
Both the legacy schema and data are still stored in the catalog and databases system as old versions.
As a result, the legacy data is preserved and also the applications defined on the old schema need not be rewritten nor recompiled.
However, the concepts of time and history are still not included in this approach.
In another words, the schema changes can only apply to the current version.
Changes to the past or plans for the future schema still can not be captured.
Bi-temporal databases [18,19], incorporating both transaction time and valid time, can fulfill the requirements of schema versioning not only because they allow the users or application programs to access temporal information but also because they allow retroactive and proactive updates.
Therefore, if the database schema is stored in a bi-temporal catalog, it can provide the most  Abstract Schema evolution and schema versioning are two techniques used for managing database evolution.
Schema evolution keeps only the current version of a schema and database after applying schema changes.
Schema versioning creates new schema versions and converts the corresponding data while preserving the old schema versions and data.
To provide the most generality, bi-temporal databases can be used to realized schema versioning, since they allow both retroactive and proactive updates to the schema and database.
In this paper we first study two proposed database conversion approaches for supporting schema evolution and schema versioning: single table version approach and multiple table version approach.
We then propose the partial table version approach to solve the problems encountered in these approaches when applied to bi-temporal databases.
1.
Introduction Database and software applications usually evolve over time.
This is due to several reasons, such as changes to the modeled reality, the application requirements, or the design specifications.
Hence, a database schema is also subject to change even after careful design.
In [17], quantitative measurements in an actual commercial relational database application are described, which show that changes to the database schema are not only unavoidable but also a significant maintenance concern during and after the process of system design.
It consumes a lot of time and effort to convert the database and application programs whenever such changes occur.
As a result, a mechanism to manage schema changes and maintain the database consistency after these changes would be a valuable addition to database management systems.
Schema evolution [1,9,11,12,13,15,22,23] and schema versioning [4,7,14] are two of the most discussed techniques for managing database evolution in current database systems.
1  flexibility for database schema evolution.
However, there is a cost tradeoff between the flexibility of retroactive and proactive schema changes and the cost of implementing these mechanisms.
The complexity is high because changes not only affect the current versions of data but also the past and even the futute versions which makes the database conversion much more complicated than for conventional snapshot databases.
Currently, there are two main appraoches to convert the database structure after applying schema changes in temporal relational databases: single table version (or complete table) [3,4] (STV) and multiple table version [4] (MTV).
Most previous research [14] discusses versioning for transaction-time only databases.
In [4], bi-temporal database versioning is studied but they only consider the simple cases, especially for the multiple table version approach.
Also there is also no space analysis for the database conversion process for these two approaches.
In this paper we will fully explore the effect of schema changes involving both retroactive and proactive updates in bi-temporal relational databases by comparing these two approaches.
We focus on two basic schema update operations, attribute addition and attribute deletion since these two operations affect the storage space and table structure.
Other operations can be executed using sequences of these two operations.
Algorithms for schema update and database conversion are also defined for these two operations.
The problems and complexity of the two approaches are also discussed.
We then propose a compromise approach, partial multiple table versioning (PMTV), which not only will solve the problems associated with the previous two approaches but also can largely reduce the complexity.
In the next section, we briefly describe bi-temporal databases and the data model used for our presentation in this paper.
The approaches of single table version and multiple table version are discussed in sections 3 and 4.
An example is then given to show the problems and complexity.
The approach of partial multiple table versioning that we propose is presented in section 5.
The comparison with the previous two approaches is given in section 6.
Section 7 concludes this paper.
2  valid-time dimension which records the history of the information in the real world.
Any error corrections and plan changes can be made by modifying the values of temporal data and/or the timestamps.
However, after the modification, the previous values will not be retained and thus the database cannot rollback to the state before the change.
Transaction-time databases incorporate only the transaction-time dimension which records the history of the database activities.
It is impossible to make changes to past information.
Any changes can only apply to the current data versions.
Bi-temporal databases incorporate both transaction-time and valid-time dimensions.
Although this leads to greater complexity, it allows the generality of both retroactive and proactive changes.
We now briefly define the bi-temporal data model adopted in this paper: A bi-temporal relation schema R is defined as follows, R = < ID, A1, A2, ... , An, VS, VE, TS, TE >, in which { A1, A2, ... , An } is the union of the time-varying and fixed-value attributes.
Attribute ID is a system generated entity identifier which has the same value for all versions of a particular entity.
Time in this model is assumed to be discrete and described as consecutive nonnegative integers.
Accordingly, time attributes [VS, VE], and [TS, TE] are atomic-valued timestamped attributes containing a starting and ending valid-time point and a starting and ending transaction-time point, repectively.
The domain of transaction-end time includes a time variable UC (Until Change) [18] and the domain of valid-end time includes time variable now [2].
The validtime interval1 [VS, VE] consisting of valid-time points associated with each tuple means the entity version in the tuple is valid in the modeled reality from VS to VE.
The transaction-time interval [TS, TE] associated with each tuple defines when the tuple is logically existing in the database.
The tuples that have UC as the value of transaction-end time (TE) represent the current versions of an entity.
If the value of valid-end time of some current version of an entity is the time variable now, these current entity versions are valid currently and also are valid in the future until some change occurs.
A tuple is logically deleted by replacing the value of transaction-end time UC with the current time.
If a tuple is modified, the tuple version will first be logically deleted and then a new version is inserted with the new attribute values and with UC as the value of TE.
An example of a bi-temporal database is shown in Figure 1.
This is a bi-temporal catalog for maintaining schema information concerning the database relations and attributes.
Such a bi-temporal catalog needs to be created  Temporal databases  Temporal databases store current information, as well as past information and even information about the future events which are planned to occur.
To serve the requirements of storing and retrieving temporal data, two time dimensions are usually incorporated in the database: transaction time and valid time.
Depending on the time dimensions the database supports, temporal databases can be categorized as valid-time (or historical) databases, transaction-time (or rollback) databases, and bi-temporal databases [6].
Valid-time databases incorporate only the  1  This concept of a fixed time interval is now called a time period in SQL.
2  to maintain the history of schema changes and to be able to update the schema both retroactively and proactively.
Figure 1a is defined for single table version approach, and Figures 1b and 1c are defined for multiple table version and partial multiple table version approaches respectively.
Employee ID Name Salary Position VS VE TS TE t1 1 John 30k P1 10 30 10 20 t2 1 John 30k P1 10 20 20 UC t3 1 John 35k P2 20 50 20 35 t4 1 John 35k P2 20 30 35 UC t5 1 John 40k P3 30 65 35 40 t6 1 John 40k P3 30 60 40 UC t7 1 John 45k P4 60 80 40 UC Figure 2.The state of the Employee relation at 40.
The tuples with '*' are the current versions.
(a) Relation ID  Name  VS  VE  TS  TE  Attribute ID  Name Domain  Rel  VS  VE  TS  TE  TS  TE  Attribute ID Name Domain Rel Rel_Version VS VE  TS  TE  (c) Relation ID  Name Attrbute_of  VS  VE  TS  TE  Attribute ID  Name  Domain  Rel  VS  VE  TS  * * * time  Assume that the following three schema changes are applied to the relation Employee: SC1: At time 50, a new time-varying attribute Bonus is added to Employee, which is valid from time 25 to 65, and John's bonus is recorded as 5% and is valid during [25,65].
SC2: At time 60 another time varying attribute Phone is added to Employee, which is valid during [15,55], and the value of Phone for employee John is 3334567 with valid-time interval [15,55].
SC3: At time 70, attribute Salary is dropped from Employee from time 15 to 50.
(b) Relation ID Name Version Derived_From VS VE  *  TE  Figure 1.
(a) Catalog relations for single table version approach.
(b) Catalog relations for multiple table version approach.
(c) Catalog relations for partial multiple table version approach.
3  Single Table Version approach  In single table version, each table has only one version throughout the lifetime of the database.
This idea is proposed in [11] as complete schemata, which follows the idea of complete table in [3].
A complete schema consists of tables defined over the union of attributes that have ever been defined for them, each with the least general domain which can include all the domains' values.
In cases where a general domain cannot be used, it is necessary to duplicate the attribute with enough domains to hold all necessary values.
For instance, if attributes are added to a relation or the domain size of the attributes is enlarged, the table needs to allocate more space to accomodate the new attribute or the change of the domain.
However, if attributes are dropped, the dropped attribute will still be retained in the database since in append-only temporal databases data will never be deleted.
Therefore the record size, and hence the table size for this approach will only grow but never shrinks.
After the schema update operation SC1, which adds a Bonus attribute, is applied at time 50.
The state of the catalog (which also consists of bi-temporal tables) is shown in Figure 3.
Although a full discussion of catalog implementation is outside the scope of this paper, we would like to briefly discuss the relationship between catalog and database.
The catalog itself can be considered a bitemporal database2 to provide the full generality of schema changes.
The bi-temporal database must be changed to conform to the schema changes as stored in the catalog.
If both catalog and database are bi-temporal, the consistency between them can be maintained since both will allow retroactive and proactive changes.
For the remainder of the paper, we will concentrate only on the database changes (not the catalog).
In this paper we will use the following example to explain and compare the three versioning techniques.
For simplicity, the example only shows the versions of one entity in the relation.
Example 1 Assumes that the bi-temporal relation Employee is created at time 10 with valid-time interval [10, now] and with the attributes employee ID, name, salary, and position.
Figure 2 shows the current state of the relation at time 40, for the versions of Employee 'John'.
Relation_catalog ID 1  2  Although in many cases, a transaction-time database may be sufficient for the catalog implementation.
3  Name Employee  VS 10  VE now  TS 10  TE UC  Bonus is assigned.
Attribute_catalog ID 1 2 3 4 5  Name ID Name Salary Position Bonus  Domain string string real string real  Rel_ID 1 1 1 1 1  VS 10 10 10 10 25  VE now now now now 65  TS 10 10 10 10 50  Employee ID Name Salary Position Bonus 1 John 30k P1 null 1 John 30k P1 null 1 John 35k P2 null 1 John 35k P2 null 1 John 40k P3 null 1 John 40k P3 null 1 John 45k P4 null 1 John 35k P2 null 1 John 35k P2 5% 1 John 40k P3 5% 1 John 45k P4 5% 1 John 45k P4 Null 1 John 30k P1 null 1 John 30k P1 null 1 John 35k P2 null 1 John 35k P2 5% 1 John 40k P3 5% 1 John 40k P3 5%  TE UC UC UC UC UC  Figure 3.
The catalog status at time 50 after the attribute Bonus is added  The new Bonus attribute has a default value 5% which is valid from time 25 to 65.
However, before time 50 there is no value for Bonus, so the unknown value null must be assigned, as shown in Figure 4(a).
In Figure 1, we notice that three of the four current entity versions of employee 1 have their valid-time interval overlap with the valid-time interval [25,65] of the 5% value of Bonus.
For the entity versions whose valid-time intervals partially overlap (P) with [25,65] (namely t4 with [20,30] and t7 with[60,80]), two new tuples need to be derived from each version: these are t8 and t9 from t4, and t11 and t12 from t7.
For the entity versions whose valid-time is included (I) in [25,65] (namely t6 with [30,60]), one new tuple needs to derived from each version: this is t10 from t6.
The newly derived tuples are inserted as shown in Figure 4(b).
The transaction-end time of the overlapped versions is changed from UC to the time when the new attribute value is assigned, which is time 50.
To continue the example, the effect of applying schema change operation SC2 (adding a Phone attribute) is shown in Figure 5.
There are 2 current entity versions whose valid-time intervals are partially overlapping, and two others fully included in the valid-time interval of SC2.
Therefore, six new tuples need to be inserted.
As in SC1, the transaction-end time of the overlapping versions needs to be changed to the current time, which is 60 in this example.
t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12  Employee ID Name 1 John 1 John 1 John 1 John 1 John 1 John 1 John 1 1 1 1 1  John John John John John  Salary Position Bonus VS 30k P1 null 10 30k P1 null 10 35k P2 null 20 35k P2 null 20 40k P3 null 30 40k P3 null 30 45k P4 null 60 Part (a) 35k P2 null 20 35k P2 5% 25 40k P3 5% 30 45k P4 5% 60 45k P4 null 65 Part (b)  VE 30 20 50 30 65 60 80  TS TE 10 20 20 UC 20 35 35 UC 50 P 35 40 40 UC 50 I 40 UC 50 P  25 30 60 65 80  50 50 50 50 50  Phone null null null null null null null null null null null null null 3334567 3334567 3334567 3334567 null  VS 10 10 20 20 30 30 60 20 25 30 60 65 10 15 20 25 30 55  VE 30 20 50 30 65 60 80 25 30 60 65 80 15 20 25 30 55 60  TS 10 20 20 35 35 40 40 50 50 50 50 50 60 60 60 60 60 60  TE 20 UC 60 35 50 40 50 50 UC 60 UC 60 UC 60 UC UC UC UC UC UC UC UC  P  I I P  Figure 5.
The state of Employee relation after time 60.
The effect of SC3 is shown in Figure 6.
As the figure shows, five more tuples are inserted after the change as there are 3 fully including and 1 partially overlapped current versions.
Employee ID 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1  UC UC UC UC UC  Name John John John John John John John John John John John John John John John John John John John John John John John  Salary Position Bonus 30k P1 Null 30k P1 Null 35k P2 Null 35k P2 Null 40k P3 Null 40k P3 Null 45k P4 Null 35k P2 Null 35k P2 5% 40k P3 5% 45k P4 5% 45k P4 Null 30k P1 null 30k P1 null 35k P2 null 35k P2 5% 40k P3 5% 40k P3 5% null P1 null null P2 null null P2 5% null P3 5% 40k P3 5%  Phone Null Null Null Null Null Null Null Null Null Null Null Null null 3334567 3334567 3334567 3334567 null 3334567 3334567 3334567 3334567 null  VS 10 10 20 20 30 30 60 20 25 30 60 65 10 15 20 25 30 55 15 20 25 30 50  VE 30 20 50 30 65 60 80 25 30 60 65 80 15 20 25 30 55 60 20 25 30 50 55  TS 10 20 20 35 35 40 40 50 50 50 50 50 60 60 60 60 60 60 70 70 70 70 70  TE 20 60 35 50 40 50 50 60 60 60 UC UC UC UC 70 UC 70 UC 70 UC 70 UC UC UC UC UC UC  I I I P  Figure 6.
The state of Employee relation after time 70.
There are three problems that need to be addessed if the single table version approach is adopted: space overhead, search overhead, and database availability.
The problem of space overhead results from excessive data duplication and null values whenever the database is converted to conform to the schema change.
Data duplication is a common problem in temporal databases if  Figure 4.
The state of relation Employee at time 50 after the value of Bonus is assigned.
Part (a) shows the relation state before the attribute value is assigned.
Part (b) shows the new inserted tuples after value for attribute  4  the relation is not in temporal normal form [10], i.e., the attributes in a relation do not change their values synchronously.
This is the case here, since every time an attribute is added to or dropped from the relation, all the current entity versions whose valid-time interval overlaps with the valid-time interval of the schema change have most of their attributes duplicated.
As shown in the examples, the information of employee's ID, name, and position are duplicated in all the newly inserted tuples.
In a database with thousands of records, this situation causes a large amount of duplication.
This problem not only wastes storage space but also makes the temporal JOIN operation [5,16,20,21] more complicated.
Another space overhead for this approach is the space of null values in the old tuple versions after the schema change of attribute addition, as shown in Figures 4 and 5(b).
Again, if there are thousands of records in the relation at the time of attribute addition, it will introduce a large quantity of null values.
This will not only waste space at the storage level but also lead to the problems with understanding the meaning of the attributes and with specifying JOIN operation at the logical level.
Moreover, nulls can have multiple conflicting interpretations: 1.
The attribute is valid but its value is unknown.
2.
The attribute is valid and its value is known but not recorded yet.
3.
The attribute is invalid.
In the case of the example, the third interpretation of null above is the correct one.
Searching for current versions of entities that must be changed because of a schema change can be expensive.
These are the versions whose valid time overlaps with the valid time of the schema change.
As a result, in the single table version approach, every time an attribute is added to or dropped from a relation, all current versions need to be checked to see if their valid time interval overlaps with the schema change.
According to [17]'s measurement, the number of attributes in the commercial application used for their experiment increased 274% in only 18 months.
For temporal databases, which usually contain a large amount of data, the search time will be a large overhead.
In addition to the space and time overhead, database availability is another concern.
When a new attribute is added or the type domain of an attribute is generalized, part of (or even the whole) database will not be available for a period of time due to the process of database requires augmentation and conversion, which reorganization of the storage space.
created table.
For conventational nontemporal databases or transaction-time databases, since schema updates can only be applied to the current schema version, the implementation of multiple table version is less complicated than for bi-temporal databases, where schema and database both can be updated retroactively and proactively.
We use the following two figures, 7 and 8, to illustrate the difference between bi-temporal and transaction time databases for this problem.
V0  V1  V2  V3  Transaction time 0  10  20  30  40  50  60  70  Figure 7.
Multiple versioning in the transaction-time relation Employee  Transaction Time  For transaction-time databases3, as shown in Figure 7, there always exists only one current schema version at any time point.
Any schema change will create only one new version which is derived from the current version.
However, in a bi-temporal database, at a certain time point, there may exist more than one current version defined for different valid-time intervals, as illustrated by the following example.
70  V4  V0  V5  V2  V1  V0  V1  V0  S C3 60  V0  V3  V2  S C2 V0  50  V0  V1 S C1  40  30  20  V0  10  10  20  30  40  50  60  70  80  Valid Time  Figure 8.
Multiple versioning in the bi-temporal relation Employee  In Figure 8, we see that version V0 of the relation Employee is defined at (transaction) time 10 and is valid from time 10 to now.
At time 50, SC1 is executed.
Since there is only one current version, V0, that overlaps  4.
Multiple table version 3  We assume that the schema versions in nontemporal databases are identified by the transaction time when they are defined, and thus can also be categorized as transaction-time databases.
In multiple table version, every time a relation schema is changed, it creates a new table version.
Data inserted to the relation after the change will be stored in the newly  5  create new table version Rcur by adding Ax to Rv with valid lifespan Ivy; Ivsc = Ivsc - Ivy; v = v - 1; AddAttribute(Ax, v, Ivsc); // recursive call to older versions  [25,65], only one new version V1 is created.
Now at time 50 after SC1, we have two current table versions, version V0 which is valid during [10,25] and [65,now], and version V1 which is valid during [25,65].
At time 60, SC2 is executed.
However, since the effective time of SC2 is from 15 to 55, it overlaps with both versions V0 and V1.
Two new versions, V2 and V3, need to be created where V2 is derived from V1 and V3 is derived from V0.
SC3 is executed at time 70 and it affects versions V2 and V3.
Therefore, two new versions V5 and V4 are derived from source versions V2 and V3 respectively.
We will call table version Vi a source to a later table version Vj if Vj is derived from Vi because of overlapping valid time intervals.
After new table versions are created, the current entity versions in the source table whose valid-time interval overlaps with the schema change need to be copied into the newly created table along with the value of the new attribute (if the schema change is attribute addition).
To illustrate the problems that may occur, we first give the algorithms for new table version creation and data insertion to the newly created table versions for both attribute addition and attribute deletion.
After the vew table versions are created, data in the source table versions need to be converted to the newly created table versions with the value of the new added attribute Ax.
Inserting tuples in the newly created table versions For each newly created table version Rnew,v after adding the new attribute Ax 1.
Identify the source table version Rv from which Rnew,i is derived.
2.
In Rv, find a set of current versions Si for each entity i, Si = {Vi,1, Vi,2, ... , Vi,n}.
3.
For each entity, compare the valid-time interval, Ii,k, of each current entity versions Vi,k in Si with Ivsc.
If Ii,k [?]
Ivsc = [VT1,VT2] then insert Vi,k as a new tuple into table version Rnew,v along with the new attribute value ax,i, valid-time interval [VT1,VT2], transaction starttime t', and transaction end-time UC.
Figures 8 and 9 show the newly created table versions and data converted from the source versions after SC1 and SC2 are executed.
In Figure 9, after SC1, version V1 is created, the data are converted from the current entity versions marked with '*' in table version V0.
Versions V2 and V3 are created after SC2 is executed.
Version V2 is derived from V1 and its data is converted from entity versions marked with '+' in table version V1.
Version V3 is derived from V0 and its data is converted from entity versions marked with ' ' in table version V0.
Attribute addition At time t, a new attribute Ax is added to table R with value ax,i for entity i.
The valid lifespan of the schema change is Ivsc.
Create new table versions The parameters: v : table version number.
Ivsc : a temporal element which is the valid lifespan of the schema change.
Ivv : a temporal element which is the valid lifespan of table version Rv.
cur : latest table version number.
(a) Employee_V0 (Valid lifspan: Iv0 = [10,now])  v = cur; AddAttribute(Ax, v, Ivsc) // compare Ivv with Ivsc if Ivv [?]
Ivsc // Ivsc included in Ivv then cur = cur + 1; create new table version Rcur by adding Ax to Rv with valid lifespan Ivsc;  ID 1 1 1 1 1 1 1  Name Salary Position John 30k P1 John 30k P1 John 35k P2 John 35k P2 John 40k P3 John 40k P3 John 45k P4  VS 10 10 20 20 30 30 60  VE 30 20 50 30 65 60 80  TS 10 20 20 35 35 40 40  TE 20 UC 35 UC 40 UC UC   *    * *  (b) Employee_V1 (Valid lifspan: Iv1 = [25,65])  else if Ivv [?]
Ivsc = [?]
then v = v -1; AddAttribute(Ax, v, Ivsc); // recursive call to the older versions else if Ivv [?]
Ivsc = Ivy // Ivy is temporal element of the intersection then cur = cur + 1;  ID 1 1 1  Name Salary Position Bonus John 35k P2 5% John 40k P3 5% John 45k P4 5%  VS 25 30 60  VE 30 60 65  TS 50 50 50  TE UC UC UC  (c) Employee_V2 ( Valid lifespan Iv2= {[25,55]} ) ID 1 1  6  Name Salary Position Bonus Phone VS VE TS TE John 35k P2 5% 3334567 25 30 60 UC John 40k P3 5% 3334567 30 55 60 UC  + +  Figures 8 and 10 show the newly created table versions and data converted from the source versions after SC3 is executed.
In Figure 10, versions V4 and V5 are created after SC3 is executed .
Version V4 is derived from V3 and its data is converted from entity versions marked with ' ' in table version V4.
Version V5 is derived from V2 and its data is converted from entity versions marked with '*' in table version V2.
Employee_V3 ( Valid lifespan Iv3= {[15,25]} ) ID Name Salary Position Phone VS 1 John 30k P1 3334567 15 1 John 35k P2 3334567 20  VE 20 25  TS 60 60  TE UC UC  Figure 9.
(a) The original table.
(b) After SC1.
(c) After SC2.
Attribute deletion At t, attribute Ax is dropped from relation R during Ivsc.
(a)Employee_V2 ( Valid lifespan I v2= {[25,55]} ) ID Name Salary Position Bonus Phone VS VE 1 John 35k P2 5% 3334567 25 30 1 John 40k P3 5% 3334567 30 55 1 John 40k P3 5% 3334567 50 55  Create new table versions Find a list of table versions LR in which all the versions include attribute Ax.
The list is ordered by the version number in descending order.
TS TE 60 UC 70 * 60 UC 70 * 70 UC  Employee_V3 ( Valid lifespan Iv3= {[15,25]} ) ID Name Salary Position Phone VS VE TS TE 1 John 30k P1 3334567 15 20 60 UC 70 1 John 35k P2 3334567 20 25 60 UC 70  DropAttribute(Ax, Ivsc, LR) The first version Ri in LR has valid lifespan Ivi; // compare Ivi with Ivsc if Ivi [?]
Ivsc // Ivsc included in Ivv then cur = cur + 1; create new table version Rcur by dropping Ax from Ri with valid lifespan Ivsc; else if Ivi [?]
Ivsc = [?]
then LR = LR - Ri; DropAttribute (Ax, Ivsc, LR); // recursive call to older versions //Ivy is the temporal element else if Ivi [?]
Ivsc = Ivy of the intersection then cur = cur + 1; create new table version Rcur by dropping Ax from Ri with vlaid lifespan Ivy; Ivsc = Ivsc - Ivy; LR = LR - Ri; // recursive call DropAttribute (Ax, Ivsc, LR); to older versions     (b)Employee_V4 ( Valid lifespan I v4= {[15,25]} ) ID Name Position Phone VS VE TS TE 1 John P1 3334567 15 20 70 UC 1 John P2 3334567 20 25 70 UC Employee_V5 ( Valid lifespan I v5= {[25,50]} ) ID Name Position Bonus Phone VS VE TS TE 1 John P2 5% 3334567 25 30 70 UC 1 John P3 5% 3334567 30 50 70 UC  Figure 10.
(a) The table versions before SC3.
(b) After SC3.
From the algorithms and the example above, we can see that the MTV approach does not have the problems of null value and database availability as in the approach of STV.
However, there are still some problem with this approach: data duplication, multischema queries [4], and mandatory version creation.
Although the MTV approach does not introduce null values, it still has the problem of data duplication.
As we can see from the example, if the current entity versions in the source table versions have valid-time interval that overlaps with the schema change, the unchanged attributes need to be duplicated and inserted into the new table versions.
The MTV resolves the problem of database reorganization in STV resulting from attribute addition by creating new table versions.
However, the trade-off is the number of table versions.
As shown in Figure 8, in bitemporal databases, the schema modification is not limited to the latest table version as in the transaction-time databases.
Therefore, the number of newly created versions may be more than one depending on the validtime interval of the schema change.
As the number of table versions increases, more temporal JOIN operations will be needed to process queries.
For example, suppose that at time t, an attribute A is added to table R and the  Inserting tuples in the newly created table versions For each newly created table version Rnew,i after dropping the attribute Ax 1.
Identify the source table version Rv from which Rnew,i is derived.
2.
In Rv, find a set of current versions Si for each entity i, Si = {Vi,1, Vi,2, ... , Vi,n}.
// insert tuples to new table versions 3.
For each entity, compare Ii,k, the valid-time interval of each current entity version in Si, with Ivsc.
If Ii,k [?]
Ivsc = [VT1,VT2] then insert Vi,k as a new tuple into table version Rnew,v with valid-time interval [VT1,VT2], transaction start-time t, transaction end-time UC, and without the dropped attribute Ax.
7  effect of this schema change creates three new table versions of R at time t. Later if a query requires a join operation between table R and S through attribute A as of time t. Then table S needs to be joined with three different table versions of R. Another concern is querying the entity's history.
The system has to access all the different table versions to get the information.
As a result, the output of the query includes different tuple types which makes it difficult to use by applications.
This is called the problem of multischema queries [4].
Another problem of the MTV approach is mandatory version creation.
Let us consider the schema change SC1.
In SC1, the valid lifespan of the newly added attribute Bonus is from time 25 to 65.
At time 45, a new table version V1 is created with attribute Bonus.
However, if the current time becomes 66, the attribute Bonus is not valid any more.
A new table version must be created at this time to maintain the system consistency.
This extra work must either be done by the user or by specifying appropriate triggers.
Applying this approach to the example given in Section 2 after SC1 and SC2 are executed, the results are shown in Figure 11.
Employee ID Name Salary 1 John 30k 1 John 30k 1 John 35k 1 John 35k 1 John 40k 1 John 40k 1 John 45k  Position P1 P1 P2 P2 P3 P3 P4  VS 10 10 20 20 30 30 60  VE 30 20 50 30 65 60 80  TS 10 20 20 35 35 40 40  TE 20 UC 35 UC 40 UC UC  (a) Original Employee relation Emp_Bonus ID Bonus VS 1 5% 25  VE 65  TS 50  TE UC  (b) New created relation for attribute Bonus after SC1 Emp_Phone ID Phone VS 1 3334567 15  VE 55  TS 60  TE UC  (c) New created relation for attribute Phone after SC2  5.
Partial multiple table version Relation_catalog ID Name Attrbute_of VS 1 Employee null 10 2 Emp_Bonus 1 25 3 Emp_Phone 1 15  From the example and discussion in the previous sections, we notice that the complexity of the schema change is highly dependent on the current state of the database.
In this section, we propose the partial multiple table version (PMTV) approach which can make the updates independent from the current state of the database and thus can largely reduce the complexity and also solve the problems with STV and MTV.
For the schema change of attribute addition, instead of enlarging the record size required for the extra storage as in STV or creating a new table version as in MTV, PMTV create a bi-temporal relation with only the new attribute, plus the key attribute (ID) of the relation being modified4.
The complete relation can be later reconstructed by applying the temporal NATURAL JOIN operation5 [20,21].
Techniques for efficient execution of this operation are discussed in [20,21], but they apply to validtime only databases.
For our application, in fact, the type of join needed is more like a TEMPORAL OUTER JOIN.
We are currently working on efficient techniques for this join for bi-temporal databases, since this is crucial to the partial multiple table version technique.
Suppose at time t, attribute Ax is added to relation R with valid-time interval [vt1,vt2].
A new relation R_Ax is created.
Following the data model defined in section 2, we have the following schema: R = < ID, A1, A2, ... , An, VS, VE, TS, TE > R_Ax = < ID, Ax, VS, VE, TS, TE >  VE now 65 55  TS 10 50 60  TE UC UC UC  (d) The state of Relation catalog after SC1 and SC2 Figure 11.Applying SC1 and SC2 using partial multiple table version.
For the schema change of attribute deletion, there are two different cases: the dropped attribute is in the original relation or it was added later.
1.
If the dropped attribute Ax is in the original table R, then the process is the same as single table version approach.
2.
If Ax is an attribute that was added later, then apply the process for single table version approach for table R_Ax only.
2.1 In table R_Ax, for each entity i, find a set of current versions Si where Si = {Vi,1, Vi,2, ... , Vi,n} 2.2 For each entity i, compare Ivsc with Ii,k, the validtime interval of each current entity version in Si If Ii,k [?]
Ivsc = Ivy then insert Vi,k into R_Ax with valid-time interval Ivsc-Ivy; TS = t; TE = UC; change TE of Vi,k from UC to t;  4  This is similar to the technique of vertical fragmentation in distributed database systems.
5 This has also been called temporal intersection join.
8  The following figure shows the result of SC3: Employee ID Name Salary Position 1 John 30k P1 1 John 30k P1 1 John 35k P2 1 John 35k P2 1 John 40k P3 1 John 40k P3 1 John 45k P4 1 John 30k P1 1 John null P1 1 John null P2 1 John null P3 1 John 40k P3  VS 10 10 20 20 30 30 60 10 15 20 30 50  VE 30 20 50 30 65 60 80 15 20 30 50 60  discuss the space complexity of adding attributes.
The following are the definitions of the parameters:  TS TE 10 20 20 UC 70 P 20 35 35 UC I 35 40 40 UC 70 P 40 UC 70 70 UC 70 UC 70 UC 70 UC 70 UC  Parameters Sa : average attribute size.
Ivsc : a temporal element which is the valid lifespan of the schema change.
Na : number of attributes in original relation R. Ntp : number of tuples in table R before any schema changes.
Ne : average number of distinct entities in relation R. NO : average number of current versions of all the entities whose valid-time intervals overlap with Ivsc.
where NO = NP + NF NP : average number of current versions of all the entities whose valid-time intervals partially overlap with Ivsc.
NF : average number of current versions of all the entities whose valid-time intervals included in Ivsc.
Nadd: the number of attribute addition operations.
Now if the schema is changed again at time 75 that the attribute Bonus is dropped from time 30 to 60.
The result will be: Employee_Bonus ID Bonus VS 1 5% 25 1 5% 25 1 5% 60 Employee_Phone ID Phone VS 1 3334567 15  VE 65 30 65  VE 55  TS TE 50 UC 75 75 UC 75 UC  TS 60  P  Assumptions 1.
For the approach of MTV, we use the worst case scenario when computing the number of newly created table versions.
That is, the valid-time interval of the next schema change covers the valid time of the previous schema change, as shown in the following figure:  TE UC  Transaction Time  As we can see from the example, for the schema change of attribute addition, there is no space needed for null values and data duplication.
Searching for the current versions of entities and the valid timespan of their versions is not necessary.
Therefore, this approach largely reduces the space and time complexity.
Futhermore, the problems of database reorganization in the single table version approach and the mandatory version creation in the multiple table version approach do not exist in this approach.
For the schema change of attribute deletion, although in one case the process of database conversion is the same as in the STV approach except that it may be applied to smaller tables.
However, most of the attribute change are attribute addition according to [17]'s measurements.
If the attribute to be deleted was an added attribute, as in the example above, it is much simpler than the other two approaches.
70  60  V7  50  V9  V8  V 10  SC4 V4  40  V6  V5  SC3 V3  30  V2  S C2 V1 S C1  20  V0  10  6.
Comparison of three approaches 10  In this section, we compare the space complexity of the three versioning approaches.
Because of the space limitation, we define the cost formulas and only show the results based on the given example.
From the discussion in the previous sections, we can see that the database conversion for schema changes of attribute addition and attribute deletion is nearly the same.
Here we only  2.
9  20  30  40  50  60  70  80  Valid Time  Therefore, a new schema change creates one more table version than the number of table versions created from the previous schema change.
Since the three approaches were compared using the same example, the value of NO (number of overlapped entity versions) is the same in STV and MTV.
For MTV, we assume the newly created entity  versions are evenly distributed into each of the newly created table versions.
Sdup = Sa* 4 * (2*2 + 1)*1 + Sa* 5 * (2*2 + 2)*1 =50Sa SSTV = 69Sa SMTV = Sa*( (4+1) * (2+1) * 1 )+Sa*[ ( 4+1 ) * 2 * 1 ] + Sa*[ ( 4+2 ) * 2 * 1 ] = 37Sa SPMTV = 2 * [ (Sa * 2) * 1 ] = 4Sa  Single Table Version approach From Section 3, the space required for attribute addition by STV is the space for the null values Snull and for the duplicated attributes Sdup.
For this example, there is a factor of 17 difference between the approaches of STV and PMTV.
Since the space required for the PMTV approach is independent from the number of the attributes, tuples, entities, and the overlapping entity versions.
If we have real a bi-temporal database with reasonable number of entities, tuples, and overlapped entity versions, the difference obviously will become much larger.
Let us consider another simple example: A relation R originally has 5 attributes with average size of 8 bytes, and 15 tuples including 3 entities, i.e., Na = 5, Sa = 8, Ntp = 15, and Ne = 3.
If later 7 attributes are added to R, Nadd = 7, and we assume NP and NF are both equal to 2.
From the result, we found 40 times difference between STV and PMTV and 30 times difference between MTV and PMTV.
= Snull + Sdup SSTV  Snull = ( Sa * Ntp )  // the space for null after the first attribute addition // the space for null after the 2nd attribute addition + Sa * [ Ntp + (2NP + NF) * Ne ] + Sa * [ Ntp + ( 2NP + NF)* Ne + (2NP + NF) * Ne ] +...  =Sa * ( Nadd * Ntp + (2NP + NF) * Ne *  Nadd - 1 i )  [?]
i=0  Sdup = Sa * Na * ( 2NP + NF ) * Ne + Sa * ( Na + 1 ) * ( 2NP + NF ) * Ne + Sa * ( Na + 2 ) * ( 2NP + NF ) * Ne +... Nadd -1   i * ( 2 NP + NF )* Ne = Sa*  Nadd * Na +  i=0   SSTV = Snull + Sdup  [?]
[  ]  = 8 * ( 7 * 15 + ( 2*2 + 2 ) * 3 *  i=0  8*(7*5+ = 3864 + 8064 [?]
12KB  = Sa*      i    j =1  8*  i    [?]
[?]
(5 + j)* i *3 4  j =1  [?]
9KB    NO * Ne  i   [?]
[?]
( N + j ) * a     i =1  7  SMTV =  N = Sa * ( Na + 1)* ( NO* Ne) + Sa*[( Na + 1) + ( Na + 2) ]* O * Ne + 2 i  [?]
i ) * ( (2*2 + 2 ) * 3)  i=0  From Section 4, the space required for attribute addition by MTV is for the overlapped data version duplicated in the new table versions.
Nadd  [?
]i ) +  6  Multiple Table Version approach  SMTV  6  SPMTV = 7 * ( 8 * 2 ) * 3 [?]
0.3KB  Partial Multiple Table Verison approach  7  From Section 5, the space required for attribute addition by PMTV is only the size of two attributes for each entity.
Conclusion  In this paper, we present the study of two schema versioning approaches in a bi-temporal database environment, single table version and multiple table version.
In most of the current literature, only transaction time is considered on schema versioning.
The research that discusses schema versioning involving both transaction time and valid time does not consider some of the more complex problems concerning schema version creation and database conversion.
We first discuss the problems associated with these two approaches when  SPMTV = Nadd * [ ( Sa * 2 ) * Ne ] Now we can compare the space cost for these three approaches based on the given example: Na is 4, Ntp is 7, Ne is 1, and Nadd is 2.
We also have NP for SC1 is 2 and NF is 1, and NP and NF for SC2 are both equal to 2.
Snull = Sa * 7 + Sa * ( 7 + (2*2 + 1) * 1 ) = 19Sa  10  applied to bi-temporal databases, then propose a third approach, partial multiple table version, which makes the database conversion much simpler and does not have the problems found in the previous two approaches.
Furthermore, we also specify formulas to analyze and compare the space cost for the three approaches.
For our proposed partial multiple table version approach, when a new attribute is added, it creates a new bi-temporal relation with only the new attribute, plus the key attribute of the relation being modified.
This way, no null values will be introduced, no searching for the overlapped current versions is needed, no database restructuring and data duplication is required, and no extra effort is needed for the problem of mandatory version creation.
In addition, from section 6, compared with the two previous approaches, the space cost has been largely reduced.
We are currently analyzing the costs of various types of temporal queries when applied to the three methods discussed here.
[11] J. F. Roddick.
Dynamically Changing Schemas within Database Models.
Australian Computer Journal, pages 105-109, 1991.
[12] J. F. Roddick.
Schema Evolution in Database Systems - An Annotated Bibliography.
Technical Report No.
CIS92-004, School of Computer and Information Science, University of South Australia, 1992.
[13] J. F. Roddick.
SQL/SE - A Query Language Extension for Databases Supporting Schema Evolution.
SIGMOD RECORD, pages 10-16, Sep. 1992.
[14] J. F. Roddick.
A survey of schema versioning issues for database systems.
Information and Software Technology, 37(7), 1995.
[15] M. R. Scalas, A. Cappelli, and C. De Castro.
A Model for Schema evolution in Temporal Relational Databases.
In Proceedings of 1993 CompEuro, Computers in Design, Manufacturing, and Production, pages 223 - 231, May 1993.
[16] A. Segev.
Join Processing and Optimization in Temporal Relational Databases.
Chapter 15 of Temporal Databases: Theory, Design, and Implementation, Benjamin/Cummings, 1993.
[17] D. Sjoberg.
Quantifying schema evolution.
Information and Software Technology, 35(1):35 - 44, 1993.
[18] R. T. Snodgrass.
The Temporal Query Language TQuel.
ACM Transactions on Database Systems, pages 247 - 298, June 1987.
[19] R. T. Snodgrass, editor.
The TSQL2 Temporal Query Language, chapter 10.
Kluwer Academic Publishers, 1995.
[20] M. D. Soo, R. T. Snodgrass and C. S. Jensen.
Efficient Evaluation of the Valid-Time Natural Join.
In Proceedings of the 10th International Conference on Data Engineering, IEEE, 1994.
[21] D. Son and R. Elamsri.
Efficient Temporal Join Processing Using Time Index.
In Proceedings of the 8th International Conference on Scientific and Statistical Database Management, pages 252 - 261, June 18 - 20, 1996.
[22] M. Tresch and M. H. Scholl.
Schema transformation without database reorganization.
SIGMOD RECORD, 22(1):21 - 27, 1993.
[23] R. Zicari.
A framework for schema updates in an objectoriented database system.
In Proceedings of the 7th International Conference on Data Engineering, pages 2 - 13, April 1991.
References [1] J. Banerjee, H-T Chou, H. J. Kim , and H.F. Korth.
Semantics and Implementation of Schema Evolution in SIGMOD RECORD, Object-oriented databases.
16(3):311-322, 1987.
[2] J. Clifford, C. Dyreson, T. Isakowitz, C. S. Jensen, and R.T. Snodgrass.
On the Semantics of "now" in Databases.
ACM Transactions on Database Systems, 22(2):171 - 214, June 1997.
[3] J. Clifford and D.S.
Warren.
Formal Semantics for Time in Databases.
ACM Transactions on Database Systems, pages 214-254, 1983.
[4] Christina DeCastro, Fabio Grandi, and Maria Rita Scalas.
Schema Versioning for Multitemporal Relational Databases.
Information Systems, pages 249-290, July 1997.
[5] H. Gunadhi and A. Segev.
Query Processing Algorithms for Temporal Intersection Joins.
In Proceedings of 7th International Conference on Data Engineering, IEEE, 1991.
[6] C. Jensen et al.
A consensus glossary of temporal database concepts.
SIGMOD RECORD, 23(1):52-64, 1994.
[7] W. Kim and H-T Chou.
Versions of schema for objectoriented databases.
In Proceedings of the 14th International Conference on Very Large Databases, pages 148-159, 1988.
[8] B. S. Lerner and A. N. Habermann.
Beyond schema evolution to database reorganization.
SIGPLAN Notices, 25(10):67 - 76, 1990.
[9] N.G.
Martin, S. B. Navathe, and R. Ahmed.
Dealing with Schema Anomalies in History Databases.
In Proceedings of the 13th International Conference on VLDB, 1987.
[10] S. B. Navathe and R. Ahmed.
A Temporal Relation Model and a Query Langue.
Information Sciences, pages 147-175, 1989.
11
Resolution for Branching Time Temporal Logics: Applying the Temporal Resolution Rule Alexander Bolotov and Clare Dixon Department of Computing and Mathematics Manchester Metropolitan University Manchester M1 5GD, UK.
A.Bolotov,C.Dixon@doc.mmu.ac.uk Abstract In this paper we propose algorithms to implement a branching time temporal resolution theorem prover.
The branching time temporal logic considered is Computation Tree Logic (CTL), often regarded as the simplest useful logic of this class.
Unlike the majority of the research into temporal logic, we adopt a resolution-based approach.
The method applies step and temporal resolution rules to the set of formulae in a normal form.
Whilst step resolution is similar to the classical resolution rule, the temporal resolution rule resolves a formula, , that must eventually occur with a set of formulae that together imply that can never occur.
Thus the method is dependent on the efficient detection of such sets of formulae.
We present algorithms to search for these sets of formulae, give a correctness argument, and examples of their operation.
1 Introduction Since it was first proposed in [13] temporal logics have been used extensively in the specification and verification of properties of concurrent and distributed systems [9].
If in this application the ability to refer to a range of possible execution paths is important then the power of the language of branching-time logics is essential [9].
It has been observed that most correctness properties of concurrent programs (that do not deal with fairness) can be expressed in the family of branching time logics called Computation Tree Logics.
The core logic we concentrate on is a Computation Tree Logic (CTL) [4] often regarded as the simplest useful logic of this family.
There are several extensions of CTL of which CTL is the most powerful [8].
Much of the research on the verification of concurrent and distributed systems has centered around the modelchecking technique utilising CTL [9].
Here the satisfiability  of a CTL formula is checked with respect to a model derived from a finite-state program [9].
The underlying research on decision procedures for branching time temporal logics has mostly involved tableau or automata methods [8] with an obvious lack of research into deductive proof methods.
For propositional linear-time temporal logics (PLTL) a clausal resolution method [10] has been developed.
This resolution approach has been extended to CTL in [2, 3].
Its key elements consist of translation to a normal form and a variety of resolution rules.
The normal form is a set of formulae which utilize only anexta, aalwaysa and asometimea temporal operators (all other operators are subsumed within this representation).
Two types of resolution rules are distinguished, namely, resolution within states known as step resolution, and resolution over states known as temporal resolution.
The latter applies when a proposition  occurs at all future moments (known as a loop in ), and if  is also constrained to be false at some point in the future.
The efficient search for such loops is crucial to the temporal resolution method.
In the linear-time case, several algorithms for detecting loops have been developed [5, 6].
In this paper we select one of these loop detection algorithms and extend it to apply to the branching time logic CTL.
The remaining of the paper is organized as follows.
In 2 we overview the logic CTL.
In 3 a clausal resolution method is outlined.
Algorithms to apply the temporal resolution rule and examples of their operation are given in 4.
A correctness argument is outlined in 5.
Finally, in 6 we consider related work and in 7 provide concluding remarks.
2 The logic CTL Here we summarize the syntax and semantics of the core logic, CTL.
2.1.
Syntax and semantics of CTL In the language of CTL we utilize only future-time (always),  (sometime), (next time),  operators (until) and  (unless).
Additionally, we use path quantifiers A (on all future paths) and E (on some future path).
The syntax of CTL distinguishes state ( ) and path (fi ) formulae.
These are defined inductively as follows, where  is any well-formed formula of propositional logic, including classically defined constants fi and fi .
 fi      fi    fi    fi    fi   fi Afi fi E fi  fi  fi  fi    fi     Well formed formulae of CTL are state formulae.
Thus, each CTL formula has a structure where any temporal operator can only be followed by a path operator or a classical operator, while any path operator can only be followed by a temporal operator.
An example of a CTL formula is E  E  meaning athere is a path on which  eventually holds and there is a path on which  always holdsa.
It follows that CTL is weaker than linear-time temporal logic in its expressive capabilities within a path, but is more expressive in that it can quantify over paths themselves.
As an example of its restricted nature, note that no formula de are satisfied on scribing the property aboth  and the same specific patha can be constructed using CTL syntax.
For the detailed description of CTLas theoretical properties, subsystems and extensions see [8].
Before continuing with the semantics of CTL we introduce some notation.
We interpret a well-formed formula of CTL in a tree-like model structure 	   	  	 , where  is a set of states,       is a binary relation over  and  is an interpretation function mapping atomic propositional symbols to truth values at each state.
A path,  , over  , is a sequence of states  	  	  	    such that for all   	 fi 	 fi fi   .
A path Az is called a fullpath.
Given a path  and a state fi   	    we term a finite subsequence   	  	    	 fi a prefix of a path   and an infinite sub-sequence of states  fi 	 fi  	 fi  	    a suffix of a path   abbreviating these respectively with fi   	  	 fi fi (or simply as  	 fi  when it is clear which path this prefix belongs to) and    	 fi fi.
We assume that a CTL model 	 satisfies the following conditions: 1. there is a designated state,  fi   , a root of a structure (i.e.
for all 	  fi 	 fi    fi), 2. every state belongs to some fullpath, i.e.
a path starting at fi , A" It is known that if linear-time temporal logic is interpreted over discrete linear models with finite past and infinite future then adding past-time operators does not give more expressiveness [11].
3. tree structures are of at most countable branching, 4. every state should have a successor state, and, 5. every path is isomorphic to  .
Below we define the satisfaction relation afia which evaluates well-formed CTL formulae at a state   in a model 	.
Postulates s1-s7 define satisfaction relation for the CTL formulae at states while p1-p6 determine evaluation of their subformulae along paths.
s1  		   fi  s2 s3 s4 s5 s6 s7 p1 p2     fi	 .
  fi   		   fi .
  fi     		   fi  and  		   fi  .
  fi     		   fi  or  		   fi  .
  fi     		   fi  or  		   fi  .
  fi A  	  		   fi  .
  fi E     		   fi  .
  fi   		   fi 	 .
  fi  fi       		   	 fi fi fi  .
  fi  fi       		   	 fi fi fi  .
  fi   		   	  fi fi  .
  fi    fi       		   	 fi fi fi           		   	  fi fi .
  fi     		   fi   		   fi    .
  fi fi 	            fi               fi  fi     fi         p3            p4 p5              p6      fi         fi  A well-formed formula,  , is satisfiable if, and only if, there exists a model 	 such that  		  fi  fi  .
A wellformed formula,  , is valid if, and only if,  is satisfied in every possible model, i.e.for each 	,  		  fi  fi  .
Recall that well-formed CTL formulae are state formulae and due to the syntactic requirement (see above) when we evaluate a CTL formula, for example, A  , we reduce the problem  		    fi A  , following s6 above, to the evaluation of  along each path   .
This, acording to p4, means  		    	   fi  .
Since  must be a state formula, applying p1, we obtain from the latter  		   fi  .
Now, if  contains another temporal operator, then this operator will again be preceded by a path operator indicating a specific path context.
For example, if   E  then the problem  		    fi E  will be reduced, according to s7, to evaluating  along some path  AVA" , etc.
2.2.
Closure properties of CTL models When trees are considered as models for distributed systems, paths through a tree are viewed as computations.
The natural requirements for such models would be suffix and fusion closures.
The former means that every suffix of a path is itself a path.
The latter requires that a system, following the prefix of a computation  , at any point  fi   , is able to follow any computation   originating from  fi .
Finally, we might require that if a system follows a computation for an arbitrarily long time, then it can follow a computation forever.
This corresponds to limit closure property, meaning that for any fullpath  Az and any paths   	     (      ) such that Az has the prefix fi 	 fi ,  has the prefix fi 	  ,  has the prefix  	   etc, the following holds (see Figure 1):  The language for indices is based on the set of terms      	    	 	   fi     		   fi    Az    fi  	  fi      iff    Definition 1 (Separated Normal Form for CTL) Given a CTL formula  , the separated normal form for F, SNF (F), is a set of clauses of the form      fi  Thus, E  means that  holds on some path labelled as   .
Indices of the type   fi aim to capture limit closure property.
Once the translation to SNF  has been carried out, all SNF clauses containing a aEa quantifier are labelled with some index (for more details see [2]).
Further, an additional operator,   , which effectively identifies the initial state, is introduced:  A fiAz  	  fi   fi  where each of the afi   a is further restricted as in Figure 2, where each   , fi or  is a literal, fi or fi , and      is some index.
       there exists an infinite path  that is a limit of the prefixes fi 	 fi 	 fi 	  	  	     .
In the following we assume that tree-like models of CTL are suffix, fusion and limit closed  .
    3.
The temporal resolution method for CTL Here we review the temporal resolution method for CTL [3, 2] whose main components are translation into the clausal normal form and application of resolution rules.
3.1.
A normal form for CTL The basic idea behind the normal form for CTL called Separated Normal Form (SNF  ) is to identify the core operators and generate formulae relevant to either the first state in a model, or to all subsequent states in a model.
The transformation procedure uses fixpoint unwinding and subformula renaming in order to reduce an arbitrary formula to SNF .
To preserve a specific path context indices are used.
Az A variety of difficult problems concerning branching-time logics are due to the limit closure property, resulting, for example, in the case of CTL , in the absence of a complete axiomatization.
   A fi    E fi    Figure 1.
Limit closure            fi          an initial clause  fi    fi  fi  an A step clause  fi  a E step clause    fi      A      E   fi  an A sometime clause 	   fi     a E sometime clause  Figure 2.
Form of SNF Clauses The natural intuition here is that the initial clauses provide starting conditions while step and sometime clauses constrain the future behaviour.
For example, a step clause A   A fi means afor any fullpath  and any state       fi, if  is satisfied at a state  then  must be satisfied at the moment, next to  , along each path which starts from   a.
Similarly, interpreting A   E  fi fi, we use the information that aEa is associated with the index   : afor any fullpath  and any state       fi, if  is satisfied at a state  then  must be satisfied at the moment, next to   , along some path associated with    which departs from   a.
Finally, A   E 	fi  fi means afor any fullpath  and any state       fi if  is satisfied at a state  then  must be satisfied at some state, say fi   fi, along some path   associated with the limit closure of    which departs from   a.
All the other operators are subsumed within this representation.
For example, the aA a operator is represented by a (possibly infinite) sequence of aA a operations (see [3]).
For convenience we will omit the outer aA a connective that surrounds the conjunction of clauses and drop the conjunction considering the set of clauses.
a constant fi to indicate this situation and, for example, the conclusion of SRES 2 rule, when resolving fi  A and fi  A  , will be fi  A fi .
Once a contradiction within a state(s) is found, as for example, fi  A fi , then we simplify it applying the following rule:  3.2.
Resolution rules for CTL  The step resolution process terminates when either no new resolvents are derived, or    fi is derived.
Once step resolution has been applied, the temporal resolution rule can be invoked.
The basic idea here is to resolve a set of formulae containing a loop in , i.e.a situation when  occurs at all future moments along some (E-loop in ) or every path (an A-loop in ) from a particular point in a CTL model, with the formula containing a , provided that both refer to the same path.
Thus, identification of loops within given set of SNF  clauses, similar to PLTL, is the crucial part of the temporal resolution method in CTL.
As in PLTL, some loops might be given directly as a set of SNF clauses; in other cases loops might be more difficult to detect.
One of the benefits of the normal form is that it allows us to identify ahiddena loops within some set of clauses.
Once a set of SNF clauses,  , has been obtained, a resolution method is applied to  .
Here we repeatedly apply astepa and atemporala resolution rules, together with various simplification steps.
3.2.1 Step resolution.
Step (classical) resolution can be used between formulae that refer to the initial moment of time or same next moment on some or all paths.
The corresponding step resolution rules are given below SRES 1                    SRES 2  fi  fi  fi        fi    fi    fi  A A A     SRES 3  fi  fi  fi      A E E    fi    fi    fi     fi fi  SRES 4  fi  fi  fi      E E E    fi    fi    fi    fi fi fi  where  is a literal and    is an index.
When an empty clause is generated on the right hand side of the conclusion of the resolution rule, we introduce  fi fi     P fi fi  where P is either of path quantifiers.
The conculsion, fi  fi , in turn, requires that fi must never be satisfied in any moment in time.
This is reflected in generating extra constraints by applying the following rule:  fi        fi  fi fi A fi  3.2.2.
Loops in CTL.
Loops in CTL are defined on sets of merged clauses, which are, in turn, generated from step clauses.
Merging  fi  fi  fi      A A A       fi  fi  fi  fi      A E E        fi  fi  fi  fi  fi      E E E         fi  fi  fi  fi  fi  Note that, similar to SRES 4, we allow merging of two E step clauses if both existential path quantifiers refer to the same path.
Definition 2 (Loop in CTL) A loop in  is a set of merged clauses (possibly labelled) of the form  fi     Pfi  fi   P      then P ,          fi fi  fi  fi               !.
 fi    the following formulae are satisfied       !fi, P only involves one aEa quantifier labelled by      or every P  which involves the aEa quantifier has the same label      then        fi and we have a E-loop in  on the path    fi, otherwise   if for all   we have indicated a E-loop in  on the path   fi, where    is a new index.
For this set of merged clauses, each right hand side implies one or more left hand sides from the side condition on loops.
Each right hand side implies .
Hence, once one of the left hand sides is satisfied, a literal  holds at all future moments on some or all paths (dependent on the type of the path quantifier).
As a simple example consider the following set of clauses (where the first clause on its own gives us a loop in ).
fi  A 	     E  fi (1) By merging both clauses, we obtain    fi    In each of the states          along the path  fi     	  E      	  A,    is empty, and we have an A-loop in    fi        !fi, P is the aAa path quantifier   E          We will abbreviate such loop by P P  fi , where  	   fi        , for all  where fi    and fi    if for all   fi  Az  Figure 3.
Effects of limit closure in CTL: Eloop   	 fi 	     is a path, and each state along this path satisfies   , which gives us the desired E-loop in .
3.2.3.
Temporal resolution rules.
Now, using the expressions fi  A A  and fi  E E  as abbreviations for sets of SNF clauses which together represent these formulae, temporal resolution rules for CTL are defined as follows.
TRES 1  fi        A A  A Afi  fi  TRES 2 fi  fi    (2)  which gives us additionally a E-loop, in , which is linked to the limit closure property of CTL.
Consider a model 	 given in Figure 3, where we arbitrarily chose a fullpath  and let    be the first moment along  which satisfies   .
Therefore, there is a path   associated with    such that fi , the successor of   on this path, satisfies   .
Formula (2) means afor any fullpath and any state     fi, if  is satisfied at  then E fi fi is satisfied at  a.
Due to the fusion closure property, there is a fullpath fi 	   A  (a concatenation of  fi 	   and  ).
Thus, setting  fi 	   A  and    , we conclude that  		 fi  fi E   fi fi .
Therefore, there is a path  associated with    such that there is a state, next to fi , say  , on this path which satisfies   , etc.
Hence, according to the limit closure property, the sequence      A A  E 	fi  Efi  fi 	fi   TRES 3  fi        E E  	fi  A Afi  fi  TRES 4  fi        E E  	fi  E 	fi  Efi  fi 	fi   In each case the resolvent ensures that once  has been satisfied, the conditions, fi , for triggering a -formula are  not allowed to occur, i.e., fi must be false, until the eventuality () has been satisfied.
Although it might be surprising that resolving an A with a E-loop in  results in an Aformula, if the premises of temporal resolution in this case are satisfiable, the satisfiability of the conclusion of TRES 3 is guaranteed by the limit closure property (the corresponding proof is given in [2]).
This system of resolution rules is a complete deductive method for the logic CTL ( [2, 3]).
4.
Applying CTL Temporal Resolution The clausal resolution approach to linear-time logic has been shown to be particularly amenable to efficient implementation [5, 6].
Here we concentrate on one of these algorithms, the Breadth-First Search approach [6], and modify it for use in our branching time setting.
4.1.
Overview of the Loop Detection Method in CTL While in PLTL we have only one temporal resolution rule, in the branching-time framework, a variety of such rules are defined.
Depending on the type of a path quantifier in the asometimea SNF  clause, we look for different types of loops.
In particular, given A, we search for a set of clauses that together imply either A A  or E E  (labelled by any   ), which can be used to apply TRES 1 and TRES 3.
When we are resolving with a asometimea formula containing E labelled by   fi, then we search for a set of clauses that together imply either A A  or E E  fi to apply TRES 2 or TRES 4.
In the latter case use of indices is crucial.
The Breadth-First Search Algorithm constructs a sequence of nodes that are labelled with formulae in Disjunctive Normal Form.
This represents the left hand sides of clauses used to expand the previous node which have been disjointed and simplified.
Clauses are selected for use in the algorithm if they generate the required literal at the next moment in time and their right hand side implies the previous node.
The former ensures the required literal holds and the latter gives the looping required so that the literal always holds.
If we build a new node that is equivalent to the previous one, using this approach, then we have detected a loop.
However, if we cannot create a new node then we terminate without having found a loop.
4.2.
Breadth-first A-loop search algorithm Given a set,  , of SNF clauses we develop an algorithm to detect an A-loop in  by constructing a set of nodes "fi 	 " 	    	 " labelled by formulae  fi 	    	 , where each      !fi is in DNF and the label,   , of the terminating node "  satisfies the following condition:   A A .
Thus, to detect an A-loop in  follow the steps below.
(1) Search for all SNF  clauses of the form #   A , for    to $, disjoin the left hand sides and make the label fi , of the top node " fi equivalent to this, i.e.
"fi   #  	  fi    Simplify "fi .
If  "fi  fi we have found a loop.
(2) Given a node "  , build node "  for   	 	    by looking for clauses or combinations of clauses of the form fi  A fi  fi, for    to % where  fi  " .
Disjoin the left hand sides so that  "       fi fi  fi  and simplify as previously.
(3) Repeat (2) until one of the conditions (a)-(c) holds: (a)  "  fi .
We have found a A-loop and return the label of the terminating node, fi .
(b)  "  " .
We have found a A-loop and return the label of the terminating node, DNF formula "  .
(c)  "  is empty.
Terminate - no loop has been found.
4.3.
Breadth-first E-loop search algorithm To detect a E-loop in  do the following.
(1) Search for all the clauses of the form #   A , or #  E  fi  , for    to $, disjoin the left hand sides, make the top node " fi equivalent to this and label " fi with Ind, i.e.
  "fi     	  fi  #    where Ind is a set of all indices     occurring within #  E  fi  .
Simplify "fi .
If  "fi  fi we terminate having found a loop.
  Not surprisingly, the detection of a A-loop is almost identical to that of Breadth-First Search in linear-time temporal logic.
However, with the detection of an E-loop we must take care when combining clauses (see below).
Az To simplify the presentation below, since a label   of a node    uniquely identifies this node, instead of saying aa label    A" fi    fi   of a node  a we will use the expression   A" fi    fi  .
(2) Given a node "    , build a node "     for   	 	    by looking for combinations of clauses of the form fi  A fi  fi or fi  E fi  fi fi  for    to % where  fi  " .
Disjoin the left hand sides so that     fi    "    fi fi  fi  step (2) of the E-loop search algorithm (i.e.
all the pairs of clauses can be merged and the right hand side contains  as a conjunct and also implies " fi ).
So we disjoin the literals on the left hand sides of the merged clauses 6+1, 7+2 and 8+3 to obtain node  "      where Inda is a set of all indices     occurring within fi  E fi  fi fi  .
Simplify " as previously.
  (3) Repeat (2) until one of the conditions (a)-(c) holds: (a)  "  fi .
We terminate having found a E-loop on path    and return the label of the terminating node, fi , provided that the following condition () is satisfied  if    is the only element of   then we have found a E-loop in  on the path   fi, else  (c)  "   &  fi  E      fi      fi     &  fi  fi   we cannot now combine it with clause 1 as it has a different path index.
We can, however, merge it with either clause 2, 3 or 4 to give  &    $fi &    'fi &    (fi          &    fi &    fi &     fi  E E E    fi     fi  fi   The first two left hand sides will be removed via simplification to leave node "  as  "  &    (fi  $  'fi  is empty.
Terminate - no loop has been found.
In a particular case of the E-loop detection algorithm, given a sometime clause labelled by   fi we search for a loop on the path   fi.
Thus, we only apply merging to those step clauses containing aEa quantifier which are labelled by   .
This will guarantee that if the algorithm terminates by finding a E-loop in , this loop will occur on the desired path   fi.
&  fi  $  'fi  Note that merged clause 7+1 would be removed via simplification.
3.
Clauses 7+2 and 8+3 still satisfy the expansion criteria so we can add $  ' to our new node.
However if we merge clauses 5 and 6 to give   we have found a E-loop in  on the path    fi, where     is a new index.
(b)  "  " .
We terminate having found a E-loop and return the label of the terminating node, DNF formula " provided that condition  above is satisfied;        fi   fi     4.
Now only clauses 7+2 and 8+3 satisfy the expansion criteria so the new node is  "  $  'fi  fi     5.
The same thing happens when we construct the next node so we obtain  "  $  'fi   and terminate with the loop $  'fi  E fi  4.4.
Examples Example 1.
Consider the following set of SNF  clauses in which we are looking for a loop:   	       & $ ' (       E A A A       fi           &  fi $ '       E A A E   & $ $  fi   fi   Noting that here A-loop searching algorithm detects a loop $  A A , we will construct a E-loop.
1.
The clauses 1a4 have either A  or E  on their right hand side.
We disjoin their left hand sides and simplify to give the top node  "fi  &  $  '  (fi  fi     2.
To build the next node, " , we see that the merged clauses 6+1, 7+2 and 8+3 satisfy the expansion criteria in  E  fi  fi   .
Example 2.
Consider the following set of SNF  clauses        	     E A               fi     A E    fi   It is not immediately obvious that this set of SNF  clauses contains a E-loop in , namely,    E E .
1.
The clauses 1 and 2 have either A  or E  on their right hand side, hence, we disjoin their left hand sides and simplify to give the top node  "fi    2.
To build the next node, 1+3 and 2+4, to obtain       E E  "    fi  fi     , we derive merged clauses  fi   fi  fi  fi     These satisfy the expansion criteria, so we add   to the new node.
Note that while we can not merge clauses 1 and 4 as they are labelled by different indices, we can, additionally to the merging 1+3 and 2+4, obtain a merged clause 2+3.
However, its left hand side will be removed via simplification to leave node " as  "       fifi fi   fi   5.
Correctness of the loop searching for CTL Here we outline the soundness and completeness of the loop detection algorithms.
The following lemma is useful.
Lemma 1 Given a series of nodes "  , for   , output by a Breadth-First A-Search for a loop in  then    fi A  3.
Now, as "  "fi we terminate the searching with the loop   fi  E E  	fi   , where     is a new index.
  The model, 	, (Figure 4), which satisfies the set of clauses given in Example 2, in particular, represents this loop.
Pick a fullpath  and a state     that is the first state satisfying   .
"    A    "  fifi  PROOF :  For  *  in step (2) of the algorithm, given node "  , we select clauses fi  A fi  fi, for    to %, where fi fi  " and the new node "  is the disjunction of the fi s. Therefore, for each  fi , fi fi  A " and fi fi  A .
Hence, we obtain fi A  "    A  "  fi    as required.
(END)  fi    satisfies    and E 	          satisfies    and E 	    satisfies  and E 	                  fi A PROOF :       fi     fi  Afi   satisfies    and E 	     E  Lemma 2 Given a series of nodes "  , for   , output by a Breadth-First E-Search for a loop in  then           Az  Figure 4.
E-loop in a combination of step clauses with different labels  Suppose that  		   fi .
Thus, according to merged clause 1+3,  		   fi E   fi fi  , i.e.there must be a path  associated with   	 , such that fi   , the successor of  along  , satisfies  .
Hence,  		 fi  fi and therefore, according to merged clause 2+4,  		  fi  fi E   fi fi  .
Therefore, we must have in the model a path  (possibly different from    	 fi fi), associated with      such that there is a state    , the successor of fi , which satisfies   .
Now, considering  , we apply the same reasoning as in the case of  , etc.
Thus, there is a path )  , the limit closure of the prefixes  	 fi 	 fi 	  	  	  	    such that  		 )  fi E .
Hence,  		   fi E E  as desired.
    "    E    "  fifi  Similar to the A-loop (END)  Theorem 1 [(Soundness)] Let   be a set of step clauses and  be the literal we are searching for a loop in.
For any DNF formula # output by a Breadth-First A-Search (respectively a Breadth-First E-search), on  , fi #  A  A    respectively #  E    E  fi  PROOF : We prove this for the A-Search and the E-Search is similar.
From the termination conditions of the algorithm given in 4.2 and 4.3 we know that either #  fi from step (1) or step (3a), or "   " from step (3b).
For the former either the clause fi  A  is in the clauseset or there are clauses in the clause-set that together imply this.
Again, due to the implicit A -operators surrounding SNF clauses, the clause fi  A  holds in all states on all paths.
Therefore, fi fi  A A  holds as required.
Otherwise "   " and from Lemma 1 we have fi A "  A "  fifi.
Thus, if we terminate with # then fi A #  A #  fifi and fi #  A A  as required.
(END)  Theorem 2 [(Completeness)] Let   be a set of step clauses and  be the literal we are searching for a loop in.
For any set of clauses       that together imply an Aloop, i.e.
fi #  A A , (respectively a E-loop, i.e.
fi #  E E ) the Breadth-First A-Search (respectively E-Search) applied to   outputs a DNF formula #  and #  #  .
PROOF : As we only use A step clauses for the Aloop, completeness for the A-loop can be shown as in the linear-time case [6].
Extract all A step clauses from  , delete the path quantifier and let the set of (linear-time) clauses be    .
Note also that the BreadthFirst A-Search algorithm is identical to that of BreadthFirst Search algorithm for linear-time temporal logic if the path quantifiers are deleted.
To show completeness in the linear-time case a graph is constructed from the clauses in   , whose nodes are valuations of all the propositions in    .
Paths through the graph represent all possible models of    .
It is shown that if a loop exists in   , # , then this is represented in the graph of    by a terminal subgraph where the required literal  holds at each node.
More specifically # is satisfied by each node (a valuation) in the subgraph.
The completeness proof shows that each step in the Breadth-First Search algorithm corresponds to an operation on the graph.
If the Breadth-First Search algorithm detects a loop returning the DNF formula #  then this corresponds to the terminal subgraph representing the largest most comprehensive loop in    .
For more details see [5].
The proof for E-loops is similar but we must construct a graph as described above for each apatha through the branching tree structure.
(END)  6.
Related work Automata-theoretic methods for CTL extend those developed for PLTL [1, 8].
To test formulae of PLTL finite automata on infinite strings are used, the appropriate type of automata in the branching-time setting are finite automata on infinite trees, i.e.
when automaton visits a state it reads an input tree rather then an input word.
Given a CTL formula , a run of the automaton constructed for is considered successful if it meets certain requirements known as aacceptance conditionsa.
This is also known as checking automaton for (non)emptiness.
If the acceptance condition is satisfied then a state structure of a successful run is not empty and it gives a model for .
Alternatively, a run is unsuccessful.
If the automaton does not have a successful run then is not satisfiable.
Note that although the structure of the normal form described in this work is close to alternating tree automata used in [1], there is no direct method of testing these automata for (non)emptiness.
On the contrary, clausal resolution method is effectively applied to a set of clauses of normal form.
Tableau-based method for CTL are outlined in [8].
Using tableau methods, to show that a formula is valid, we negate and apply tableau algorithm.
The algorithm systematically constructs a structure from which a model can be generated.
If the structure is empty then no model can be constructed, the negated formula is unsatisfiable and the  original formula is valid.
The incremental method of the construction of the tableau [8] has been essentially used in showing completeness of the resolution method for CTL [2].
Proof methods for particular modal logics are given in several papers, for example [12, 14].
Ohlbach takes a resolution based approach removing modal operators and replacing them with world path arguments to predicate and function symbols, i.e.
a modal diamond (possibility) is replaced by & meaning there is an accessible world & from here.
Similarly here we annotate E rules with an index to denote which path we are referring to.
Ohlbach requires sophisticated unification algorithms dependant upon the properties of the paths concerned.
In the CTL resolution system matching indices is trivial.
Temporal logics are hard to reason about due to the interand operators encoding, in case action between the of linear-time temporal logic, a simple form of induction i.e.
        fifi      The formulation of induction extended to branching-time temporal logic, which can be found in the axiomatization of CTL [8], is given by a set of formulae of a complex structure, for example, A        E  fifi    A fi  The complex resolution rule, and search method described in this paper is required to deal with the above induction principle.
Note that induction in branching-time logic is additionally complicated by the limit closure property of the underlying tree models and in case of the full branchingtime logic, CTL , represents a main difficulty in axiomatizing the latter.
One of the benefits of the clausal resolution technique is the possibility of invoking a variety of well-developed methods and refinements used in the framework of classical logic.
For example an initial investigation into the development of the set of support for classical logics [15] to that for linear-time temporal logics has been made in [7].
The refinement of this strategy is ongoing work and could potentially be adapted to the branching framework.
7 Conclusions As we have already mentioned, most of the research on the proof methods for branching-time logics has been concentrated around the tableau or automata methods [8].
In this paper we have investigated the application of the clausal resolution method for CTL [2, 3].
The authors know of no other clausal resolution methods developed for branchingtime logics.
Searching for a loop is the crucial part of the  resolution technique.
We have described breadth-first algorithms of identifying A-loops and E-loops and sketched the correctness argument.
This, together with the algorithm for the clausal resolution for CTL [2], makes the resolution method developed practically suitable for implementation.
However, ways to improve the application of the temporal resolution rule must be further investigated.
Here, although we have formulated one method of searching for a loop, this is only the starting point.
The work to be done in this direction concerns, in particular, development of the strategies of the preferred loops to find first, reduction of the search graphs and loop subsumption algorithms.
Again, we expect here to incorporate the related results obtained for PLTL [5, 6].
Also, taking into account that in branchingtime logics we deal with path quantifiers, it might be useful to investigate parallels between loop searching techniques and (rather simple in this case) unification algorithms used in the resolution technique for predicate logic.
Acknowledgements This work has been partially supported by funding from HEFCE, under a PhD studentship and EPSRC, under research grant GR/L87491.
Both authors would like to thank Michael Fisher for his advice and comments on this work.
We are also grateful to anonymous referees for their useful suggestions on improving the presentation.
References [1] O. Bernholtz, M. Vardi, and P. Wolper.
An automatatheoretic approach to branching-time model checking.
In Computer Aided Verification.
Proceedings of 6th International Workshop, volume 818 of Lecture Notes in Computer Science.
Springer-Verlag, 1994.
[2] A. Bolotov.
Clausal Resolution for Branching-Time Temporal Logic.
PhD thesis, The Manchester Metropolitan University, 1999, (submitted).
[3] A. Bolotov and M. Fisher.
A clausal resolution method for ctl branching time temporal logic.
Journal of experimental and theoretical artificial intelligence, (11):77a93, 1999.
[4] E. M. Clarke and E. A. Emerson.
Using Branching Time Temporal Logic to Synthesise Synchronisation Skeletons.
Science of Computer Programming, pages 241a266, 1982.
[5] C. Dixon.
Search Strategies for Resolution in Temporal Logics.
In M. A. McRobbie and J. K. Slaney, editors, Proceedings of the Thirteenth International Conference on Automated Deduction (CADE), volume 1104 of Lecture Notes in Artificial Intelligence, pages 672a687, New Brunswick, New Jersey, July/August 1996.
Springer-Verlag.
[6] C. Dixon.
Temporal resolution using a breadth-first search algorithm.
Annals of Mathematics and Artificial Intelligence, 22, 1998.
[7] C. Dixon and M. Fisher.
The Set of Support Strategy in Temporal Resolution.
In Proceedings of TIME-98 the Fifth International Workshop on Temporal Representation and Reasoning, Sanibel Island, Florida, May 1998.
IEEE Computer Society Press.
[8] E. A. Emerson.
Temporal and Modal Logic.
In J. van Leeuwen, editor, Handbook of Theoretical Computer Science: Volume B, Formal Models and Semantics., pages 996a 1072.
Elsevier, 1990.
[9] E. A. Emerson.
Automated reasoning about reactive systems..
In Logics for Concurrency: Structures Versus Automata, Proc.
of International Workshop, volume 1043 of Lecture Notes in Computer Science.
Springer-Verlag, 1996.
[10] M. Fisher.
A Resolution Method for Temporal Logic.
In Proc.
of the XII International Joint Conference on Artificial Intelligence (IJCAI), 1991.
[11] D. Gabbay, A. Phueli, S. Shelah, and J. Stavi.
On the temporal analysis of fairness.
In Proceedings of 7th ACM Symposium on Principles of Programming Languages, 1980.
[12] H.-J.
Ohlbach.
A Resolution Calculus for Modal Logics.
Lecture Notes in Computer Science, 310:500a516, May 1988.
[13] A. Pnueli.
The Temporal Logic of Programs.
In Proc.
of the Eighteenth Symposium on the Foundations of Computer Science, 1977.
[14] L. A. Wallen.
Matrix Proof Methods for Modal Logics.
In Proc.
IJCAI-87, pages 917a923, Milan, Aug. 1987.
[15] L. Wos, G. Robinson, and D. Carson.
Efficiency and Completeness of the Set of Support Strategy in Theorem Proving.
J. ACM, 12:536a541, Oct. 1965.
Ockhamistic Logics and True Futures of Counterfactual Moments Torben Brauner Centre for Philosophy and Science-Theory Aalborg University Langagervej 6 9220 Aalborg East, Denmark torbenb@hum.auc.dk  Per Hasle Centre for Cultural Research University of Aarhus Finlandsgade 28 8200 Aarhus N, Denmark Phasle@cfk.hum.aau.dk  Peter Ohrstrom Centre for Philosophy and Science-Theory Aalborg University Langagervej 6 9220 Aalborg East, Denmark poe@hum.auc.dk  Abstract In this paper various Ockhamistic logics are compared with the aim of making clear the role of true futures of counterfactual moments, that is, true futures of moments outside the true chronicle.
First we give an account of Prior's original Ockhamistic semantics where truth of a formula is relative to a moment and a chronicle.
We prove that this is equivalent to a semantics put forward by Thomason and Gupta where truth is relative to a moment and a so-called chronicle function which assigns a chronicle to each moment.
This is the case because true futures of counterfactual moments do not matter in Thomason and Gupta's semantics.
Later we discuss how two options considered by Belnap and Green might be formalised.
They come about by assuming either a chronicle or a chronicle function to be given once and for all.
The first of the two options is unable to give an account of certain statements from natural language and the second option invalidates an intuitively valid formula.
We propose a new Ockhamistic semantics where the formula in question is valid, and furthermore, where true futures of counterfactual moments are taken into account.
Finally, we discuss possible applications within Artificial Intelligence.
 This author is supported by the Danish Natural Science Research Council.
1.
Introduction This paper is concerned with certain logics which are Ockhamistic in the sense of involving a notion of true future.
The logics in question are compared with the aim of making clear the role of true futures of counterfactual moments, that is, true futures of moments outside the true future.
The paper is structured as follows.
In the second section of this paper Prior's original Ockhamistic semantics is introduced.
See also [16], p. 126 ff.
A notable feature of this semantics is a notion of "temporal routes" or "temporal branches", called chronicles.
Here truth of a formula is relative to a moment as well as a chronicle - which is to be understood as truth being relative to a provisionally given true future.
In the third section, we give an account of the semantics put forward by Thomason and Gupta in [19].
Here truth of a formula is relative to a moment as well as a socalled chronicle function which assigns a chronicle to each moment - this is to be understood as truth being relative to a provisionally given true future of each moment.
We prove that this is equivalent to Prior's original semantics.
This is so because true futures of counterfactual moments do not matter in the semantics - despite the fact that truth is relative to a moment as well as a chronicle function.
In section four we give an account of two options considered by Belnap and Green in [2].
They come about by assuming either a chron-  icle or a chronicle function to be given once and for all.
We discuss how such assumptions might give rise to formal semantics.
The first of the two mentioned options is unable to give an account of certain statements from natural language in which there are references to true futures of counterfactual moments.
The second option invalidates the intuitively HFq.
In the fifth section we propose a valid formula q new semantics where the model provides a chronicle function once and for all.
Compared to the other Ockhamistic semantics given in this paper, it is a notable feature that the HFq is valid here, and furthermore, true fuformula q tures of counterfactual moments are taken into account.
An essential difference between this new semantics and the traditional Ockhamistic semantics is that the involved notion of possibility only refers to what might happen now whereas possibility in the traditional sense refers to what might happen from now on in general.
This new notion of necessity may be called "immediate necessity".
In the last section we discuss possible applications of the logics presented in this paper within the area of Artificial Intelligence.  )
)  2.
Prior's notion of Ockhamistic necessity In what follows, we shall give an account of the semantics which was put forward by Arthur N. Prior in [16], p. 126 ff., with the aim of formalising some ideas of William of Ockham (ca.
1285 - 1349).
To define Prior's Ockhamistic semantics, we need a set T equipped with a relation < together with a function V which assigns a truth value V t; p to each pair consisting of an element t of T and a propositional letter p. The elements of T are to be thought of as moments and < as the beforerelation.
It is assumed that < is irreflexive and transitive.
Furthermore, to account for the branchingness of time it is assumed that < satisfies the condition  ( )  8t; t ; t : (t < t ^ t 0  00  0  00  < t ) ) (t < t 0  00  _t  00  < t _ t = t ): 00  A relation satisfying this condition is said to be backwards linear.
An important feature of the semantics is a notion of "temporal routes" or "temporal branches" which are to be thought of as possible courses of events.
We shall call such branches chronicles1 .
Formally, chronicles are maximal linear subsets in T; < .
The set of chronicles induced by T; < will be denoted C T; < , and moreover, for any moment t we let t denote the set of chronicles to which t belongs.
Note that the conditions of transitivity and backwards linearity make each chronicle c backwards-closed, c and t0 < t then t0 c. Truth is relative that is, if t to a moment as well as to a chronicle to which the moment belongs - which is to be understood as truth being relative  (  ( ( )  )  )  (  2  1 Some authors call them histories.  )
2  to a provisionally given true future of the moment in question.
In [16], p. 126, Prior calls it a "prima facie" assignment.
By induction, we define the valuation operator Prior as follows:  Prior(t; c; p) , V (t; p), where p is a prop.
letter Prior(t; c; p ^ q) , Prior(t; c; p) ^ Prior(t; c; q) Prior(t; c; :p) , :Prior(t; c; p) Prior(t; c; Pp) , 9t < t: Prior(t ; c; p) Prior(t; c; Fp) , 9t > t: t 2 c ^ Prior(t ; c; p) Prior(t; c; 2p) , 8c 2 (t): Prior(t; c ; p) So Prior(t; c; p) amounts to p being true at the moment t in the chronicle c. A formula p is said to be Prior-valid if and only if p is Prior-true in any structure (T; <; V ), that is, we have Prior(t; c; p) for any moment t and chronicle c such that t 2 c. For instance, let us consider the formula q ) HFq (where Hp is an abbreviation for :P :p).
From the above definitions it is obvious that Prior(t; c; q ) HFq) for any t and any c with t 2 c. Therefore q ) HFq is valid in this system.
Likewise, the formula q ) 2q, where the tense operator F does not occur in q, is valid.
This formula is in 0 0  0  0  0  0  0  good accordance with the medieval dictum "unumquodque, quando est, oportet esse" ("anything, when it is, is necessary"), see [17], p. 191.
It may be doubted whether Prior's Ockhamistic system is in fact an accurate representation of the temporal logical ideas propagated by William of Ockham.
According to Ockham, God knows the contingent future, so it seems that he would accept an idea of absolute truth, also when regarding a statement Fq about the contingent future - and not only prima facie assignments like Prior t; c; Fq .
That is, such a proposition can be made true "by fiat" simply by constructing a concrete structure which satisfies it.
But Ockham would say that Fq can be true at t without being relativised to any chronicle.
It is possible to establish a system which seems a bit closer to Ockham's ideas by taking chronicles to be disjoint and having a relation between chronicles corresponding to identity up to a certain moment.
A system along these lines is in [12] called "Leibnizian".
(  )  3.
Thomason and Gupta's notion of Ockhamistic necessity In this section, we shall compare the traditional Ockhamistic semantics given by Prior to the semantics given by Richmond Thomason and Anil Gupta in [19].
An essential difference is that in the traditional semantics truth is relative to a moment as well as a chronicle whereas in Thomason and Gupta's semantics, truth is relative to a moment as well as a so-called chronicle function which to each moment assigns a chronicle.
We shall prove that the two semantics are equivalent.
This is the case because true futures  of counterfactual moments do not matter in the semantics - despite the fact that truth is relative to a moment as well as a chronicle function.
It should be mentioned that besides the usual Ockhamistic connectives the semantics given in Thomason and Gupta's paper also interprets counterfactual implication - in such a context true futures of counterfactual moments do make a difference.
A formal account of Thomason and Gupta's semantics is based on the same notion of a structure as the one used previously in this paper.
So we need a set T equipped with an irreflexive, transitive and backwards linear relation < together with a function V which assigns a truth value to each pair consisting of a moment and a propositional letter.
Furthermore, it involves a relation on T relating "copresent" (or "pseudo-simultanous") moments and also certain additional machinery to interpret counterfactual implication.
Here we do not consider counterfactual implication so we shall disregard such machinery, and moreover, we adapt Thomason and Gupta's semantics to the previous setting of this paper where no notion of copresentness is taken into account.
In the traditional semantics, truth is relative to a moment and a chronicle to which the moment belongs.
In Thomason and Gupta's semantics, truth is relative to a moment and a chronicle function.
A chronicle function is a function C which assigns to each moment a chronicle such that the following two conditions are satisfied:  8t: t 2 C (t).
(C2) 8t; t : (t < t ^ t 2 C (t)): ) C (t) = C (t ) (C1)  0  0  0  0  The first condition says that the chronicle assigned to a moment has as element the moment itself.
The second condition says that chronicles assigned to later moments of a chronicle coincide with chronicles assigned to earlier ones.
The definition of truth of a formula as relative to a moment and a chronicle function is to be understood as truth being relative to a moment and a provisionally given true future of each moment.
The chronicle function with respect to which truth is relative, is assumed to be normal at the moment at hand.
Given a moment t, a chronicle function C is said to be normal at t if and only if  8t < t: C (t ) = C (t): 0  ()  0  We let N t denote the set of chronicle functions which are normal at the moment t. Normality at a moment means that the moment is in the true future of any earlier moment.
Without restricting attention to chronicle functions normal HFq would not be at a given moment, the formula q valid.
How should 2p be interpreted?
As Thomason and Gupta explains in [19], p. 311, one should not simply say that 2p is true at t with respect to C normal at t if and only if p is true at t with respect to all chronicle functions C 0 normal  )  at t. This is because 2p is supposed to say that p holds no matter how things will be.
Hence, C 0 should differ from C at most at t and moments in the future of t. It follows from normality at t that C 0 also has to be allowed to differ from C at moments in the past of t. This leads to the following definition: The chronicle functions C and C 0 are said to differ at most at t and its past and future if and only if  8t : (t 6= t ^ t fi t ^ t fi t ) ) C (t ) = C (t ): 0  0  0  0  0  0  0   ( )  We let P;F t; C denote the set of chronicle functions which differ from C at most at t and moments in the past and future of t. By induction, we define the valuation operator V as follows:  V (t; C; p) , V (t; p), where p is a prop.
letter V (t; C; p ^ q) , V (t; C; p) ^ V (t; C; q) V (t; C; :p) , :V (t; C; p) V (t; C; Pp) , 9t < t: V (t ; C; p) V (t; C; Fp) , 9t > t: t 2 C (t) ^ V (t ; C; p) V (t; C; 2p) , 8C 2 N (t) \  (t; C ): V (t; C ; p) A formula p is said to be valid if and only if p is true in any structure (T; <; V ), that is, we have V (t; C; p) for any moment t and chronicle function C normal at t. 0  0  0  0  0  0  P;F  0  We shall now show that this notion of validity is equivalent to the traditional notion of Ockhamistic validity.
As alluded to above, this is so because true futures of counterfactual moments do not matter.
In the semantics above, 2q says that q holds no matter how things will be, where "things" do not only refer to the true future of the present moment but also to true futures of future moments.
Thus, even if an Ockhamistic model does provide true futures of counterfactual moments, these futures do not matter if things will be different (and obviously not if things will be the same either).
With the aim of proving the two notions of validity equivalent, we shall first prove a theorem which essentially says that chronicles can be "mimicked" by chronicle functions.
82  Theorem 3.1 Given a chronicle function C such that t c: C t  c there exists a chronicle  ( ) = c.  Proof: In this proof we shall use Zermelo's Theorem and Zorn's Lemma which are both equivalent to the Axiom of Choice.
Zermelo's Theorem says that any set can be wellordered, that is, it can be equipped with a linear partial order such that any non-empty subset has a least element.
By Zermelo's Theorem, we can assume that T is well-ordered by a relation .
For any non-empty subset T 0 of T we let T 0 denote its -least element.
Zorn's Lemma says that if each linear subset of a non-empty partially ordered set A has an upper bound, then A has a maximal element.
By Zorn's Lemma, we can assume that for any moment t there exists a chronicle c0 such that t c0, and hence, by the Axiom of  u  v v  2  Choice, we can assume that there exists a function f which to any moment t assigns a chronicle c0 such that t c0 .
By transfinite induction using , we define a function  2  v C : T !
C (T; <)  8< c C (t) = : C (uft @ tjt 2 C (t )g) f (t) such that  0  0  2  if t c if t0 @ t: t otherwise  9  ()  2 C (t ) 0  2 , 2 2 v  2 = uf j 2 ( )g 2 () v ( ) = () 2 ( ) 2 () v = uf j 2 ( )g v ( )= ( ) 2 ( ) v ( )= ( )  Corollary 3.2 Let a chronicle function C be given.
For any c there exists a moment t and chronicle c such that t c and chronicle function C 0 normal at t such that C 0 t C 0 P;F t; C .
2  ()=  2 ( ) Proof: Let K = ft jt 6= t ^ t fi t ^ t fi t g and note that T n K is itself a structure, that is, irreflexive, transitive and backwards linear.
We have c \ K = ; because t 2 c, so the preceeding theorem can be applied to c considered as a chronicle in T n K to obtain a chronicle function C as appropriate.
Note that any chronicle in T n K is also a chronicle in T .
Furthermore, note that T n K is forwards 0  0  0  0  0  closed.
QED.
The lemma we shall prove now essentially says that given a chronicle function, the semantics does not take into account true futures of counterfactual moments.
Lemma 3.3 Let a formula q be given.
For any chronicle c and chronicle functions C and C 0 normal at a moment t such that C t C 0 t it is the case that V t; C; q V t; C 0; q .
(  ()= ()  )  (  ),  Proof: Induction on the structure of q.
We only check the 2 case; the other cases are straightforward.
Assume that V t; C; 2p .
By definition of the semantics we have V t; C 0; 2p if  (  (  )  )  8C 2 N (t) \  (t; C ): V (t; C ; p): 00  P;F  0  00  It follows from Corollary 3.2 that for any  C  00  2 N (t) \  (t; C ) P;F  0  000  2 N (t) \  (t; C ) P;F  ()= () ( ), ( ( ) 8C 2 N (t) \  (t; C ): V (t; C ; p): Thus, it is the case that V (t; C ; p).  )
such that C t C t .
But V t; C 000; p V t; C 00; p according to the induction hypothesis.
By definition of the semantics we have V t; C; 2p only if 000  00  000  P;F  00  ()=  =  C  000  for any moment t. We only have to check that C satisfies the conditions (C1) and (C2).
Clearly, (C1) is ok.
Assume that t < t0 and t0 C t .
We then have to prove that C t C t0 .
It is straightforward to check that t c t0 c, so without loss of generality we can assume that t = c and t0 = c. Let t0 u t C u .
Thus, t0 is the -least moment where t belongs to the assigned chronicle.
Hence, t C t0 and t0 t, and furthermore, C t0 C t .
C t0 as t0 C t so t00 t0 where t00 Now, t0 0 v t C v .
Thus, t00 t0 and C t00 C t0 .
But t < t0 so t C t00 and hence t0 t00 by definition of t0 .
C t0 .
QED.
We conclude that t0 t00 so C t The theorem above gives rise to an important corollary.
2 ()  there is a  QED.
An important consequence of the preceeding results is the following lemma.
Lemma 3.4 Let a formula q be given.
For any chronicle c and moment t such that t c the existence of a chronicle c and V t; C; q function C normal at t such that C t is equivalent to V t; C 0; q being the case for any chronicle c. function C 0 normal at t such that C 0 t  2  (  )  ()= ()=  (  )  Proof: Follows from Theorem 3.1 and Lemma 3.3.
QED.
We now prove a theorem making clear how truth in the traditional Ockhamistic semantics is related to truth in the new semantics.
Theorem 3.5 Let a formula q be given.
Also, let a moc be given.
Then ment t and a chronicle c such that t Prior t; c; q if and only if for any chronicle function C normal at t such that C t c it is the case that V t; C; q .
(  2  )  ()=  (  )  Proof: Induction on the structure of q.
We proceed on a case by case basis.
The case where q is a propositional letter follows from Theorem 3.1.
The case is straightforward.
The case follows immediately from Lemma 3.4.
The P and F cases follow from Lemma 3.4.
The 2 case goes as follows: By definition Prior t; c; 2p is equivalent to Prior t; c0; p being the case for any c0 such that t c0 .
By the induction hypothesis this is equivalent to V t; C 0; p being the case for any C 0 normal at t such that C 0 t c0 0 0 where c is any chronicle such that t c .
This is equivalent to V t; C 0; p being the case for any C 0 normal at t. But cf.
Corollary 3.2 this is equivalent to V t; C; 2p being the c. QED.
case for any C normal at t such that C t The theorem above enables us to conclude that the traditional semantics is equivalent to Thomason and Gupta's semantics.
^  :  (  (  )  )  (  )  2  ( ) ()=  2  ( ( )=  )  Corollary 3.6 A formula is Prior-valid if and only if it is valid in Thomason and Gupta's semantics.
Proof: Straightforward by the preceeding theorem.
QED.
4.
Belnap and Green's argument In the interesting paper [2], Nuel Belnap and Mitchell Green consider a model where a chronicle is given once  and for all - the true chronicle.
They denote such a chronicle TRL (Thin Red Line).
Belnap and Green argue that this kind of model is unable to give an account of certain statements from ordinary language in which there are references to true futures of counterfactual moments.
This is remedied by assuming that instead of just one true chronicle the model provides one true chronicle for each moment.
But also in this case problems apparently crop up.
In this section we shall discuss how Belnap and Green's arguments might be formalised.
We will first consider the case where the model provides a chronicle once and for all.
Belnap and Green have based their arguments on the following example of a statement from ordinary language: The coin will come up heads.
It is possible, though, that it will come up tails, and then later (*) it will come up tails again (though at that moment it could come up heads), and then, inevitably, still later it will come up tails yet again.
([2], p. 379) As Belnap and Green explain in [2], p. 379, the trouble is that at (*) the example says that tails will happen.
The point is here that (*) is future relative to a counterfactual moment, that is, a moment outside the true chronicle.
Now, how should tenses and possibility in the sentence above be interpreted?
Belnap and Green give the following informal account of the semantics they have in mind: In the semantic theory of branching+TRL the future tense moves you forward along TRL and the past tense moves you backward along it.
Any talk of possibility or necessity or inevitability refers to some histories other than TRL.
([2], p. 379) So any future tensed sentence has to be interpreted with respect to the true chronicle.
But this does obviously not make sense outside the true chronicle.
We agree with Belnap and Green that this is problematic.
Belnap and Green remedies this deficiency by assuming that instead of just one true chronicle the model provides a function which to each moment assigns one true chronicle.
They discuss which conditions such a function C has to satisfy, but rather than condition (C2) they assume the condition  8t; t : t < t ) C (t) = C (t ) 0  0  0  to be satisfied - which amounts to the function being normal at every moment of the model.
This is problematic because it forces the before-relation to be forwards linear, as they do indeed point out2.
In what follows, we shall therefore 2 In [3] we remedied this by suggesting the more appropriate condition (C2).
Later, we realized that this condition was introduced in [19].
In fact, it occurs already in the paper [11] by Vaughn McKim and Charles Davis where, however, it is formulated in a different way.
restrict our attention to chronicle functions in the previous sense of this paper unless otherwise is indicated.
Using a chronicle function, truth of the statement above amounts to truth at the left-most moment of the structure heads  ,, ,@ , heads , @@R,tails @ @@R tails @@ @R tails  where we have used a thick line to represent the future part of a chronicle (the past part needs no representation as it is uniquely determined).
Belnap and Green have argued that the formulae FFq Fq and q PFq should be valid.
They do not give a formal semantics but it is clear that they interpret tenses by moving in the appropriate direction along the true chronicle of the moment at hand.
It is not clear how the interpretation of possibility goes in their semantic theory, but it seems unavoidable to take into account chronicle functions which are different from the given chronicle function at the moment at hand (if that was not the case then the interpretation of possibility could not refer to chronicles other than the true chronicle of the moment at hand).
Hence, truth has to be relative to a moment as well as a chronicle function.
This suggests a semantics like the one put forward by Thomason and Gupta except that a chronicle function is given once and for all and 2p is true at t with respect to C if and only if p is true at t with respect to all chronicle functions C 0 such that C 0 differ from C at most at the moment t. Formally, the chronicle functions C and C 0 are said to differ at most at t if and only if  )  )  8t : t 6= t ) C (t ) = C (t ): 0  0  0  0  0  ( )  We let t; C denote the set of chronicle functions which differ from C at most at t. Then the valuation operator V is defined as follows:  V (t; C; p) , V (t; p), where p is a prop.
letter V (t; C; p ^ q) , V (t; C; p) ^ V (t; C; q) V (t; C; :p) , :V (t; C; p) V (t; C; Pp) , 9t < t: V (t ; C; p) V (t; C; Fp) , 9t > t: t 2 C (t) ^ V (t ; C; p) V (t; C; 2p) , 8C 2 (t; C ): V (t; C ; p) A formula p is then said to be valid if and only if p is true in any structure (T; <; V; C ), that is, we have V (t; C; p) for any moment t. 0  0  0  0  0  )  0  0  Given such a semantics, it is straightforward to check Fq is valid whereas it is not valid if (C2) is that FFq  )  left out.
But q PFq is invalid.
We agree that this is trouHFq is invalid; it blesome.
Furthermore, the formula q is not true at the upper-most moment of the structure  )  q   , ,, @@ R :q @  leads to the following definition: C and C 0 are said to differ at most at t and its past if and only if it is the case that  8t : (t 6= t ^ t fi t) ) C (t ) = C (t ): 0  0  0  0  0  0   ( )  We let P t; C denote the set of chronicle functions which differ from C at most at t and moments in the past of t. We now define the valuation operator V as follows:  Faced with the mentioned problems, Belnap and Green simply abandon the idea of assuming a chronicle function to be given once and for all.
V (t; C; p) , V (t; p), where p is a prop.
letter V (t; C; p ^ q) , V (t; C; p) ^ V (t; C; q) V (t; C; :p) , :V (t; C; p) V (t; C; Pp) , 9t < t: V (t ; C; p) V (t; C; Fp) , 9t > t: t 2 C (t) ^ V (t ; C; p) V (t; C; 2p) , 8C 2 N (t) \  (t; C ): V (t; C ; p) A formula p is said to be valid if and only if p is true in any structure (T; <; V; C ), that is, we have V (t; C; p) for any moment t at which C is normal.
0  0  0  0  0  5.
The new semantics Also in this section we assume that the model provides a chronicle function once and for all - the true chronicle function.
What we are after here is a semantics where the HFq is valid, and furthermore, where true formula q futures of counterfactual moments are taken into account.
With this goal in mind we shall propose a new semantics.
A notable difference between this new semantics and the traditional Ockhamistic semantics is that in the new semantics the involved notion of possibility just refers to what might happen now whereas possibility in the traditional sense refers to what might happen from now on in general.
In other words, in the traditional semantics 2q means that q is true no matter what happens now and in the future whereas in the new semantics 2q means that q is true no matter what happens now.
This new notion of necessity may be called "immediate necessity".
The new semantics corresponds to considering the first occurrence of the word "possible" in Belnap and Green's example as referring to the first tossing of the coin rather than the whole sequence of tossings.
We find this very natural.
In what follows, we shall give an account of the new semantics.
What we do is we take the semantics of the last section where a chronicle function is given once and for all and where truth is relative to a moment and a chronicle function (note that conditions (C1) and (C2) are satisfied here).
We then add the condition that the chronicle function has to be normal at the moment at hand.
This makes the HFq.
The resulting semantics validate the formula q restriction to normal chronicle functions forces us to change the semantics of necessity such that it takes into account not only chronicle functions differing from the given chronicle function at the moment at hand but also chronicle functions differing at moments in the past of the moment at hand.
Thus, the key feature of the semantics is that 2p is true at t with respect to C normal at t if and only if p is true at t with respect to all chronicle functions C 0 normal at t such that C 0 differ from C at most at the moment t and its past.
This  )  )  )  0  P  0  )  It is straightforward to check that both of the formulae Fq and q HFq are valid.
The fact that the last mentioned formula is validated makes us believe that the formal semantics given here is not the one Belnap and Green had in mind when writing the paper [2].
Now, consider the structures  FFq  , :p , ,@ , :p @@R,, @@ @R p  , :p , ,@ , :p @@R,, @@ @R p  Clearly, the formula 3Fp (where 3q is an abbreviation for 2 q) is true in the left-most moment of the first structure whereas it is false in the corresponding moment of the second structure.
But the structures are identical except that the involved chronicle functions differ at a counterfactual moment.
This shows that true futures of counterfactual moments do make a difference in this new semantics.
The new semantics is not equivalent to Prior's Ockhamistic semantics (and thus neither to Thomason and Gupta's semantics).
The new semantics invalidates the formula  ::  F3Fp ) 3FFp  )  which is valid in Prior's semantics (note that in the context of dense time, this formula is equivalent with F3Fp 3Fp).
The formula is not true in the left-most moment of the structure  p ,  -:p,@, @@R :p  So Prior-validity does not imply validity in the sense proposed above.
However, it should be mentioned that all axioms of the modal logic S5 are validated (for an introduction to S5, see [7]).
The invalidity of the above mentioned formula actually reveals a notable feature of this new semantics: It allows for the emergence of possibility in time, that is, it allows for a situation in which the truth of a proposition is possible in the future whereas the future truth of the proposition is impossible.
In [3] we suggested another semantics which also has the property that it allows for the emergence of possibility in time.
The two semantics are not equivalent.
The semantics of the mentioned paper invalidates the formula  H (Gq ) 3Gq) (where Gp is an abbreviation for :F :p).
It is not true in  the lower-most moment of the structure  :q ,  , , @@ ,,  :q R @ q, @@ Rq @  On the other hand, the formula is valid with respect to the new semantics given above.
We find the formula intuitively valid wherefore we consider the semantics of the present paper as an improvement compared to the one of [3].
6.
The relevance to Artificial Intelligence The main concern of this paper has been to work out the formalities of a revised Ockham-semantics as a basis for AIresearch and -applications, rather than actually investigating its use for various purposes.
That remains to be done, but we shall briefly mention some points and areas of application, where an Ockhamistic logic enhanced as suggested is likely to be useful.
This is a distinction arguably required for natural language understanding, regardless of any different metaphysical assumptions one might have about time and the contingent future.
Secondly, within the same frame, linguistic examples as the one given by Belnap and Green can be dealt with.
This also makes the system promising for evaluating counterfactual statements (see also [6]).
6.2.
Planning Branching time temporal logics are used for formalising temporal aspects of planning problems.
Roughly, planning amounts to choosing appropriate actions with the aim of achieving a desirable outcome.
Notable in this respect are the papers [10] and [14].
See also [5] which besides temporal notions also involves chance.
The model of the last mentioned paper differs from the models of the present paper by having disjoint chronicles - which corresponds to the Leibniz system of [12].
The above mentioned applications do not explicitly involve true futures of counterfactual moments, but we conjecture that this notion - and also the naturally associated notion of immediate necessity put forward in the previous section - does have a place in such contexts.
6.3.
Partial information reasoning Branching time as well as chronicle functions occur naturally within the area of partial information reasoning, which recently has been pointed out in the paper [1].
In this context the notion of branching time comes about when considering the future courses of events which are compatible with given (partial) knowledge.
In such a situation one particular future course of events can be singled out by using various criteria such as minimal change principles, probability and typicality.
As explained in [1], such criteria simply correspond to chronicle functions on the underlying branching time structure.
Our notion of immediate necessity is therefore readily available in such contexts.
6.4.
Causal and counterfactual reasoning  6.1.
Natural language understanding Our version of Ockhamistic semantics can at the same time provide two advantages in this field: Firstly, an Ockhamistic logic can, as pointed out in for example [12], make a genuine distinction between the following three statements: 1.
Possibly, Mr. Smith will commit suicide.
2.
Necessarily, Mr. Smith will commit suicide.
3.
Mr. Smith will commit suicide.
The close relation between causal and counterfactual reasoning has been pointed out by a number of authors.
Moreover, both of these areas have a clearly temporal component.
Thus, David Lewis' possible world semantics for counterfactuals [8] was related to branching time in [9].
In [4] it was shown how Lewis' possible world semantics for counterfactuals related to non-monotonic reasoning, and how this could be utilized in AI-contexts (among other things fault diagnosis).
In [18] Shoham made further progress in this direction (subsequently refined in several papers), and showed its relevance in non-monotonic  reasoning - a crucial area in current AI-research.
In [12] Ohrstrom and Hasle presented a system which integrates counterfactual, causal and temporal reasoning, based on an Ockhamistic logic.
See also [13].
The idea of true futures at conterfactual moments plays an important role in these systems, but only for finite models and not explicitly worked out as here.
However, in our view the line of development in causal and counterfactual reasoning since [8] makes it credible that much can be gained from the study of how to apply the logic presented here to current problems in AI, especially those related to causal, counterfactual and temporal non-monotonic reasoning.
6.5.
Knowledge representation From the area of knowledge representation it is relevant to mention Javier Pinto and Raymond Reiter's work on the situation calculus, which is a formalism for representation and reasoning about actions and their effects.
Compared to the formalisms of the present paper, this formalism is based on predicate logic rather than modalities.
In the situation calculus, a branching time structure is implicitly present where the branches are possible sequences of situations starting from an initial situation.
Such a branching time structure is in [15] endowed with a preferred branch describing the world's true evolution.
This is obtained by extending the situation calculus with a predicate singling out situations belonging to the preferred branch.
Given this, it is possible to express and answer certain hypothetical queries like: At time Tp in the past, when you put A on B , could A have been put on C instead?
([15], p. 7) However, it is not possible to express counterfactual queries such as: If I had not paid the bookie, would I have lived to bet again?
([15], p. 12) This is so because the situation calculus of [15] do not take into account true futures of counterfactual moments which the authors call "hypothetical actual" lines of situations.
In fact, they write: The present formalism allows us to express knowledge about what happens in the world ....
Unfortunately, we do not have hypothetical actual lines that would allow us to express knowledge about what would occur if certain non-actual actions were to be performed, for example, counterfactuals in which alternative action occurrences can be postulated.
([15], p. 7) It seems that formalisation of hypothetical actual lines would (explicitly or implicitly) have to involve the notion  of a chronicle function.
This would make the considerations of the present paper relevant.
Acknowledgements: Thanks to Alberto Zanardo for stimulating discussions on topics related to this paper.
References [1] B. Barcellan and A. Zanardo.
Actual futures in Peircean brancing-time logic.
Draft manuscript, 1997.
[2] N. Belnap and M. Green.
Indeterminism and the thin red line.
Philosophical Perspectives, 8:365-88, 1994.
[3] T. Brauner, P. Hasle, and P. Ohrstrom.
Determinism and the origins of temporal logic.
In Proceedings of Second International Conference on Temporal Logic, Applied Logic Series.
Kluwer Academic Publishers, 1998.
22 pages.
To appear.
[4] M. L. Ginsberg.
Counterfactuals.
Artificial Intelligence, 30:35-81, 1986.
[5] P. Haddawy.
A logic of time, chance, and action for representing plans.
Artificial Intelligence, 80:243-308, 1996.
[6] P. Hasle and P. Ohrstrom.
Counterfactuals and branching time in automated text understanding.
In S. Jansen et al., editor, Computational Approaches to Text Understanding.
Museum Tusculanum, Copenhagen, 1992. pp.
13-27.
[7] G. E. Hughes and M. J. Cresswell.
An Introduction to Modal Logic.
Methuen, 1968.
[8] D. Lewis.
Counterfactuals.
Harvard University Press, Cambridge MA., 1973.
[9] D. Lewis.
Counterfactual dependence and time's arrow.
NOUS, 13:455-76, 1979.
[10] D. V. McDermott.
A temporal logic for reasoning about processes and plans.
Cognitive Science, 6:101-55, 1982.
[11] V. R. McKim and C. C. Davis.
Temporal modalities and the future.
Notre Dame Journal of Formal Logic, 17:233-38, 1976.
[12] P. Ohrstrom and P. Hasle.
Temporal Logic: from Ancient Ideas to Artificial Intelligence.
Kluwer Academic Publishers, 1995.
[13] S. A. Pedersen, P. Ohrstrom, and M. Elvang-Goranson.
The structure of causal reasoning in medicine and engineering.
In J. Faye, U. Scheffler, and M. Urchs, editors, Logic and Causal Reasoning.
Akademie Verlag, Berlin, 1994. pp.
23153.
[14] R. N. Pelavin and J. F. Allen.
A formal logic of plans in temporally rich domains.
In Proceedings of the IEEE, volume 74:1364-82, 1986.
[15] J. Pinto and R. Reiter.
Reasoning about time in the situation calculus.
Annals of Mathematics and Artificial Intelligence, 14:251-268, 1995.
[16] A. N. Prior.
Past, Present and Future.
Oxford, 1967.
[17] N. Rescher and A. Urquhart.
Temporal Logic.
Springer, 1971.
[18] Y. Shoham.
Nonmonotonic reasoning and causation.
Cognitive Science, 14:213-52, 1990.
[19] R. H. Thomason and A. Gupta.
A theory of conditionals in the context of branching time.
In W. L. Harper, R. Stalnaker, and G. Pearse, editors, Ifs: Conditionals, Belief, Decision, Chance, and Time.
Reidel, Dordrecht, 1980. pp.
299-322.
Representation and reasoning with disjunctive temporal constraints TRACK 1: Temporal representation and reasoning in AI TOPIC: Temporal constraint reasoning MarAa Isabel Alfonso Galipienso Depto.
Ciencia de la ComputaciAln e Inteligencia Artificial Universidad de Alicante(Spain) e-mail:eli@dccia.ua.es Federico Barber SanchAs Depto.
de Sistemas InformAAticos y ComputaciAln Universidad PolitAScnica de Valencia(Spain) e-mail:fbarber@dsic.upv.es February 21, 2002 Abstract The purpose of this paper is to show the expressiveness provided by the use of a Labelled TCSP model in order to specify and reason about disjunctive temporal constraints.
We use a network based representation.
The reasoning algorithms allows us to manage complex temporal constraints, both in assertion and query processes.
It allows disjunctive assertions, conjunctive and hypothetical queries, and one-to-many constraints.
Additionally, a labelled point-based metric model becomes an adequate support for reasoning on costs associated to the use of resources.
They can be managed by an algorithm that integrates effectively a CSP (Constraint Satisfaction Problem) process into a closure process.
The result is a new parameterizable process that can be applied to practical and real problems.
1  Introduction  A Temporal Constraint Satisfaction Problem (TCSP) is a particular class of CSP problem where variables represent times and constraints represent sets of allowed temporal relations between them.
It requires a temporal model where the goals are to reason about what consequences(T) follow from a set of temporal constraints, 1  "Temporal-constraints j= T?
", or to determine whether a set of temporal constraints is satisfiable, with no assumptions about properties of temporal facts.
A temporal reasoning model is made up by a temporal algebra which determines the expressiveness of the temporal model, and by temporal reasoning algorithms.
Several temporal models have been defined in the literature, which can be considered under two main features [7].
The first one is the representation expressiveness: aWhat temporal information can the model represent as constraints among temporal entities?a.
The second one is related to the temporal reasoning algorithms (ahow to reason on a given type of temporal constraints to obtain what of them can be truea) and their computational complexity.
A clear trade-off between these features exists.
The management of disjunctive constraints allows a higher level of expressiveness, but it implies a higher computational cost [14], [8].
The main classical disjunctive temporal models are [11] point-based,intervalbased, and metric (quantitative) point-based models.
Some efforts have been made to integrate qualitative and quantitative temporal information, but the full integration of qualitative and quantitative information requires managing qualitative and quantitative disjunctive constraints among time points, intervals and durations.
Moreover, some application domains (scheduling, causal reasoning, etc.)
need to manage disjunctive assertions of temporal constraints, conjunctive and hypothetical queries, one-to-many constraints, etc.
This gives rise to the need to manage non-binary constraints.
In order to overcome the limitations outlined above, new models are investigated.
In [5] it has been proposed a temporal model (TCSP), based on a labelled point-based disjunctive metric temporal algebra, which gives rise to a labelledTCN (LTCN).
This model is based on constraint-associated labels so that constraints can be related among them without using hyper-arcs.
In this paper, we intend to show the high expressiveness provided by the Labelled TCSP model, which allows us to specify and reason about a great variety of constraints, such as disjunctive assertions: conjunctive and hypothetical queries, other complex and non-binary time-point constraints, and cost of use of resources.
We also explain an additional reasoning algorithm that integrates a CSP process into a closure process, and introduces some parameters in the resulting process.
The rest of paper is organized as follows.
In section 2, we introduce the labelled TCSP model used.
In section 3, we show the expressiveness of labelled temporal algebra and present several types of complex and non-binary constraints that can be specified.
In section 4, we talk about the reasoning algorithms used to manage effectively the constraints specified.
In section 5, we refer to some applications and evaluation of the model.
Finally, we remark the main conclusions of this work.
2  2  Temporal model of disjunctive constraints  In [5], it has been described a new-labelled temporal algebra, whose main elements are:        labelled disjunctive metric constraints (lij ).
The general form of a constraint + + lij is: (ti f([d ;d ]; flabelij:1 g); :::; ([dij:n ; dij:n ]; flabelij:n g)gtj )1 , with ij:1 ij:k d  d+ij:k , which means: (tj ti  [d+ij:1; d+ij:1 ]) _ ::: _ (tj ti  [dij:n; ij:k + + d ]).
Each [dij:k ; dij:k ] denotes an elemental or canonical constraint ecij:k .
ij:n label sets associated to canonical constraints (flabelij:k g).
Each ecij:k , when it is asserted, has associated a label set flabelij:k g with j flabelij:k g j= 1 that identifies it.
We name the tuple lecij:k = (ecij:k ; flabelij:k g) as labelled canonical constraint.
Each labelij:k can be considered as a unique symbol.
It uses the special label aR0a to denote that an input constraint lij has only one disjunct.
Each labelij:k 2 flabelij:k g of a derived constraint provides information about which input ecij set has been asserted.
sets of inconsistent canonical constraints (I-L-Sets).
An (I-L-Set) is a set of labels flabelij:k g that represents a set of overall inconsistent canonical constraints.
That is, they cannot all simultaneously hold.
Canonical constraints lecij:k 2 lij are pairwise disjoint.
Thus, each 2-length set of labels from each pair of lecij:k is considered an I-L-Set, and is added to the super-set named Inconsistence-Set (I-set).
This model uses a LTCN (Labelled Temporal Constraint Network) based representation, and includes the special temporal points T0 and TF to indicate the beginning and ending of the world.
The reasoning algorithms guarantee the consistency and obtain the minimal LTCN.
These are the updating process, and a total closure process that infers new constraints from those explicitly asserted.
They are based on the following operators:  1    Temporal Inclusion (lc ), that takes into account the inclusion of temporal intervals and the inclusion of their associated label sets.
  Temporal Union ([lc ), that performs the disjunctive temporal union of labelled constraints as the set-union of their canonical constraints.
However, all labelled canonical constraints whose associated labels are I-L-Sets should be rejected.
+ we will use also the form (ti f[dij:1 ; d+ ] ; :::; [dij:n ; dij:n ]flabel gtj ) ij:1 flabelij:1 g ij:n g  3    Temporal Composition ( lc ), and Temporal Intersection based in the operations   and  defined in [9].
  (  lc  ),  which are  The associated label set of each derived canonical constraint references the set of explicitly asserted disjunctions from which the canonical constraint has been derived.
Moreover, the reasoning process maintains inconsistent sets of explicitly asserted disjunctions in the set of I-L-Sets.
It should be taken into account that labelled constraints, associated label sets and the set of I-L-Sets are jointly, integrally managed by the reasoning algorithms.
The computational cost of the closure process is O(n2 l2e ), in which n is the number of nodes in the network, l is the maximal number of disjunctions of input constraints, and e is the number of input constraints updated in the previous LTCN.
This is the bounded cost of each added problem constraint 2 .
This complexity makes infeasible to solve real problems, so we introduce a new parameterized reasoning algorithm that obtain one or several solutions (see section 4).
3  Complex and non-binary constraints  By reasoning on labelled disjunctive constraints, associated label lists and I-L-Sets, the TCSP model offers the capability of representing and managing non-binary disjunctive constraints.
Particularly, logical relations among canonical constraints of different edges (lecij:x 2 lcij ; leckl:y 2 lckl ) can be specified.
This feature will allow us to manage logical expressions of constraints between different pairs of nodes both in assertion and retrieval processes.
Following, we will show how these logical expressions on metric constraints can be represented and managed in the proposed model.
For reasons of simplicity, only two disjunctive elements are shown.
However, more than two elements could be managed in a similar way:    To represent that two canonical constraints (lecij:x 2 lcij , leckl:y 2 lckl ) cannot hold simultaneously, that is :(lecij:x [ leckl:y ) the set of associated labels to lecij:x and leckl:y , fRij:x ; Rkl:y g, should be added to the set of I-LSets.
  To represent that two canonical constraints (lecij:x 2 lcij , leckl:y 2 lckl ) should hold simultaneously, the Cartesian products fRij:x g ffRkl:1 ; :::; fRkl:q g fRkl:y gg and fRkl:y g  ffRij:1 ; Rij:2; ::; Rij:pg fRij:xgg should be added to the set of I-L-Sets.
2  The best case occurs when the algorithm acts as a pure CSP.
Then, the closure process has a polinomial cost.
4    To represent a logical dependency between two canonical constraints, such as "If lecij:x then leckl:y " (where lecij:x 2 lcij ; leckl:y 2 lckl ), the Cartesian product fRij:x g  ffRkl:1 ; Rkl:2 ; :::::; Rkl:q g fRkl:y gg should be added to the set of I-L-Sets.
3.1 Reasoning of sets of constraints and their consequences Let us show a simple flow-shop scheduling example to illustrate some features about the retrieval of logical expressions from constraints and the consequences of such expressions.
Three jobs fJ1 ; J2 ; J3 g share three resources fM1 ; M2 ; M3 g in a given order (Table 1).
The use of Mj by Ji gives rise to the operation Oij .
Table 1: A scheduling example.
J1 J2 J3  Ready-time (Rt)  M1  M2  M3  Due-time (Dt)  0 (rt1 ) 0 (rt2 ) 0 (rt3 )  10a (O11 ) 10a (O21 ) 10a (O31 )  10a (O12 ) 10a (O22 ) 10a (O32 )  10a (O13 ) 10a (O23 ) 10a (O33 )  40a (dt1 ) 50a (dt2 ) 60a (dt3 )  Each operation Oij can be represented by the temporal points Oi1 and Oi+1 , that denote the beginning and ending points of Oij .
We can specify this scheduling problem as the following sets of disjunctive temporal metric constraints:       Ready-time constraints: (T0 f[rti ; 1[fR0 g g Oi1 ) 8 i=1..3 + Flow-shop restrictions: (Oij f[0; 1[fR0 gg Oi(j+1) ) 8 i=1..3, j=1,2  Due-time constraints: (T0 f[0; (dtj )[fR0 g g Oi3 ) 8 i=1..3 + Disjunctive constraints: (Oij f[1; 1[fRij;(i+k)j g ; ] 1; 0]fRji;j(i+k) g g O(i+k)j ) 8 i=1..3, 8 j=1,2,3, 8 k=1,2, with Oij+; O(i+k)j 2 O and fRa ; Rb g is an I-LSet.
Each disjunctive label Ra;b indicates a possible order between operations.
For example the label R12;22 reflects the fact that O12 is scheduled before O22 .
Given this constraint set, we apply the corresponding reasoning algorithms (see sections 2 and 5), and we obtain the minimal LTCN.
It contains all feasible scheduling solutions and we are able to know all possible orders among operations, all starting or ending time of operations, etc.
In addition, we obtain an InconsistenceSet (I-Set) that contain sets of I-L-sets.
This allows us to make queries about some 5  other restrictions that underlie the minimal LTCN, like non-binary constraints.
For instance, we can determine whether a set of canonical constraints of different constraints are overall consistent by checking if the union of their label sets is not an I-L-set.
As an example, suppose that the I-set associated to constraint sets of table 1 contains the I-L-set fR0 ; R11;12 ; R22;21 g. We can formulate the following questions:     It is possible for O11 to be before O12 and for O21 to be after O22 ?
: Under the common problem context (R0 ), the answer is no, since the set of labels associate to these disjunctions fR0 ; R11;12 ; R22;21 g is an I-L-set It is possible for O11 to be before O13 , for O21 to be before O23 and for O21 to be after O22 ?
: The answer will be yes, if the set of labels associates to these disjunctions fR11;13 ; R21;23 ; R22;21 ; R0 g is not an I-L-set nor a superset of an existing I-L-set.
This kind of questions about the feasibility of a conjunctive sets of constraints between different pairs of time points can be solved without propagating their effects to all of the LTCN.
Moreover, the consistency of the partial instantiation of a subset of LTCN variables in their domains can be assured if the union-set of the associated label sets to these instantiations is not an I-L-set.
Thus, partial solutions can be assembled without having to propagate the partial instantiation to all LTCN.
On the other hand, it is also important to know the implications of these choices.
These implications can be considered as consequences of hypothetical queries on the LTCN: aWhat happens if ....?a.
Thus, the consequences which follow from the fulfillment of a set of canonical constraints can be obtained by means of the associated label sets to these canonical constraints.
For example: we ask if O22 can meet with O21 , and if O12 can meet with O11 due to some optimal criteria.
Suppose + f:::; [0; 0]fR0 ;R2;R5 ;:::g; :::g O21 ), (O12+ that we have in the minimal LTCN: (O22 f:::; [0; 0]fR0 ;R2;R3;:::g; :::g O11 ).
If the union set of labels fR0 ; R2 ; R3; R5 ; :::g is not an I-L-set, then we know that both of these choices can hold.
In this case, we know also that these choices about the operations (O22 ; O21 ) and (O12 ; O11 ) imply that the associated disjunctions to the corresponding union-set of labels should also hold.
It must be taken into account that making certain choices about orders or assignment times for operations can prevent the feasibility of making other choices.
Thus, we can easily know the feasibility of a certain decision, its consequences and which other choices (orders among operations, possible times, etc.)
become unavailable by means of associated labels of such a decision and without having to update or propagate it.
This feature is important in order to analyse and optimize feasible scheduling solutions.
6  3.2 Reasoning on complex time-point constraints A specific kind of non-binary constraints (disjunctive one-to-many constraints) disjunctively restricts the temporal occurrence of a time-point with other time-points.
For instance, a time point can be temporally restricted to the maximum/ minimum temporal occurrence of a set of time-points.
These complex constraints are useful in scheduling problems where the ending time-point of each order should be associated to the ending time-point of the last task in the order.
Moreover, these constraints can also be useful in areasoning about changea processes.
In a typical causal relation (C 1; C 2; C 3 !
E ), the effect E holds while all causes fC 1; C 2; C 3g hold.
Thus, the concluded effect E should be temporally constrained to the overlapping temporal interval where all causes fC 1; C 2; C 3g hold.
That is, E = max (C 1 ; C 2 ; C 3 ), E + = min(C 1+ ; C 2+ ; C 3+ ).
Constraints of this kind, which also appear with non-disjunctive constraints, cannot be managed by usual models so that ad hoc procedures are needed [6], [13].
Thus, by using labelled temporal constraints, "the time-point tf is at the maximum occurrence of the set of time points ft1 ; t2 ; ::; tn g" can be represented as:  f  g )8 2f g is an I-L-Set.
(max(t1 ; t2 ; ::; tn ) [0; 0]  tf  ti  g : ( f[0 0]f The I-L-Set f  t1 ; ::; tn  ti  ;  RAi  g ; [1; 1]f  RBi  g gt  f  )  and fRB 1 ; RB 2 ; ::; RBn RB 1 ; RB 2 ; ::; RBn g specifies that the time point tf has the constraint [0,0] with, at least, a time point ti .
Moreover, the constraint (ti f[0; 0]gtf ) can only exist between tf and those time points ti that also allow the constraint [0,0] among them.
Afterwards, when all time points become fully constrained on the time line, tf becomes constrained with only the maximum time-points tmax : (tmax {[0,0]} tf ).
In another case, the constraint (tmax {[1, 1]gfRBi g } tf ) would fail.
The one-to-many constraint (max(t1 ; t2 ; ::; tn ) {[0,0]} tf ) is bidirectional: (i) T0 becomes constrained at the maximum time-point of ft1 ; t2 ; ::; tn g, and (ii) by constraining the upper-bound of the occurrence of tf , the upper-bounds of occurrences of the time-points ft1 ; t2 ; ::; tn g also become constrained.
For example, time-point tf can be constrained at the end of the latest operation (use of the resource P3: {O13 ; O23 ; O33 }) of the scheduling problem of Table 1: +  +  +  f g  (max(O13 ; O23 ; O33 ) [00]  tf )    +  f f[00] f[00]  1[ g [11[ g [11[ g  (O13 [00]RA1 ; [1 + (O32 + (O33  RA2 ; RA3 ;  RB 1  tf );  RB 2  tf );  RB 3  tf );  and fRB 1 ; RB 2 ; RB 3 g is an I-L-Set.
Without any more constraints in the problem, tf becomes constrained (T0 {[50 60]} tf ), where [50, 60] are the minimum and maximum possible due times for 7  scheduling.
Thus, we can estimate the optimum scheduling due time as the lower bound of tf .
Moreover, we can restrict the constraint of tf with T0 so that the resource/operation assignment times and the operation ordering also become more constrained.
Therefore, we can force a desired scheduling due time in [50 60].
Otherwise, when all scheduling times become fixed, time point tf will be also fixed to the final scheduling due time.
Similarly,  f  g )8 2f g : ( f[0 0]f g ] 1 1]f g is an I-L-Set.
Likewise, the constraint "no is at [  (min(t1 ; t2 ; ::; tn ) [0; 0]  tf  ti  t1 ; ::; tn  ti  and fRB 1 ; RB 2 ; ::; RBn + [d2 ; d+ 2 ],..., [dn ; dn ] from the maximum time point represented:  f  +  (max(t1 ; t2 ; ::; tn ) [d1  ; d1  ]; [d2  Finally, given the causal relation I1 ; I2 ; :::; In (max(I1  ; I2 ; :::; In  f  g  ) [0; 0]  I0  +  ; d2  !
+  ;  RAi  ;  RBi  g gt  +  ] " could also be d1 ; d1  t1 ; t2 ; ::::; tn  +  g  ]; :::; [dn ; dn ]  I0  ); (min(I1  ;  t f ):  , the following constraints: +  +  ; I2 ; :::; In  f  g  ) [0; 0]  +  I0  );  restrict the interval I0 to the temporal overlapping of intervals {I1 ; I2 ; :::; In }.
Moreover, the constraint (I0 f[11]gI0+ ) implies that I0 has a non-null duration, so that forces {I1 ; I2 ; :::; In } to overlap, which is an usual requirement in causal chaining processes.
3.3 Cost constraints The labels of the model allows us to incorporate several additional information that can be managed in an integrated manner with the reasoning algorithms.
This is important because we can specify more wide range of problems in the same manner and solve it without changing the reasoning algorithms applied.
As an example of this feature, we show how specify cost associated to resources required to carry out the corresponding actions depending on the time in which the actions are carried out.
The form of a cost constraint would be: "The resource rk have a cost associated of x if it is used between the temporal points t1 and t2 ".
We denote the cost of use of the resource rk by means the labelled temporal interval [t1 ; t2 ]Rck .
In order to incorporate it into the network, we add the constraint (t0f[ 1; t1 1]Ra ; [t1 ; t2 ]Rck ; [t2 + 1; 1]Rb gT F ), in which Ra and Rb represent zero cost value [4].
Thus, heuristics applied can use label Rck in order to calculate the corresponding operation costs.
This will allow us to use the same resolution method to solve the CSP problem, with or without associated costs to actions.
8  f  )  4  Reasoning algorithms  In addition to reasoning algorithms outlined in section 2, a new reasoning process has been proposed in [2] that integrates effectively the CSP process into a limited closure process: not interleaving them but as a part of the same process.
It is an iterative algorithm, in which we add a new constraint each time.
It results in a flexible model that can be tailored by two parameters:     The maximal number of indecisions maintained in the network.
The variable and value heuristics used to prune the search space.
The first parameter allows us to perform as a pure CSP process (if the number of disjunctions maintained is equal to zero), as a pure closure process (if this number is not limited), or as a convenient mixed closure-CSP method.
The greater the number of indecisions maintained, more solutions are also maintained, and fewer backtrackings are needed to obtain a solution.
On the other hand, we obtain a more incremental method in that we reduce the need to know in advance all constraints.
Moreover, the process varies itself automatically with the number of maintained disjunctions and which of them are maintained in every moment.
The second parameter, the set of heuristics to apply, provides efficiency to the method in that they can take better decisions.
Each input constraint acts as a variable, and each disjunction in a constraint represents a possible value for that variable [12].
So, in each iteration we have to decide the next constraint to add (by means a variable heuristic), and which disjunctions (by means a value heuristic) have to be maintained in the network.
We can use the named mixed heuristics [3], that combine the results of apply several heuristics to major information derived in the LTCN by the closure process.
Moreover, when we add a disjunctive constraint, we can maintain several disjunctions, and leave (delay) the corresponding decisions for later, if we consider that the information supplied by the heuristics it is not sufficient to decide.
The computational cost of the closure process is O(n2 l2e ), in which n is the number of nodes in the network, l is the maximal number of disjunctions of input constraints, and e is the number of input constraints updated in the previous LTCN.
This is the bounded cost of each iteration of the algorithm Closure-CSP 3 .
This complexity makes infeasible to solve real problems, so we introduce the parameter to maintain a maximal number of indecisions, and the use of convenient variable and value heuristics.
3 The best case occurs when the algorithm acts as a pure CSP.
Then, the closure process has a polinomial cost.
9  5  Applications and evaluation  Due to the reasoning algorithm outlined in section 4, we are able to apply it both in the framework of temporal reasoning (for example causal reasoning) and CSP problems [4].
For example, this model has been investigated under several types of scheduling problems, and it has been capable of specifying sets of constraints not contemplated in previous approximations, such as setup and maintenance periods of the resources, several work flows, consider the cost of use of resources, and other types of scheduling problems such as production lots [1].
On the other hand, the outlined mixed closure-csp process proposed results in an incremental reasoning process that is useful when temporal constraints are not initially known but are successively deduced from an independent process; for instance, in an integrated planning and scheduling system [10].
This process has been evaluated using both several instances from a known benchmark of scheduling problems, and randomly generated instances.
The results obtained [2], [3], are promising in order to apply it to solve real problems.
6  Conclusions  We have shown the expressiveness of a Labelled TCSP model in order to specify and reason about disjunctive temporal constraints.
This model allows us to:       Represent and manage non-binary disjunctive constraints both in assertion and retrieval processes.
More precisely, we have seen how to handle several types of disjunctive one-to-many constraints.
Perform hypothetical queries and assemble partial solutions without having to propagate the partial instantiation to all LTCN.
So, we can easily know the feasibility of a certain decision, its consequences and which other choices become unavailable by means of associated labels of such a decision.
This feature is important in order to analyse and optimize feasible solutions.
Incorporate information cost of use of resources, that can be managed in an integrated manner.
References [1] M.I.
Alfonso.
Un modelo de integraciAln de tAScnicas de clausura y CSP de restricciones temporales: aplicaciAln a problemas de scheduling.
PhD thesis, Dep.
Ciencia de la Comput.
e I.
A. Universidad de Alicante, Spain, 2001.
10  [2] M.I.
Alfonso and F. Barber.
A mixed closure-csp method to solve scheduling problems.
In Proceedings of the 14th IEA/AIE 2001, volume 2070 of Lecture Notes in A. I., pages 559a570, Berlin, July 2001.
Springer Verlag.
[3] M.I.
Alfonso and F. Barber.
Nuevas heurAsticas y medidas de textura para resolver problemas de scheduling mediante clausura y csp.
In Proceedings of the 9th CAEPIAa01, volume 2, pages 833a842, GijAln, Spain, Nov 2001.
[4] M.I.
Alfonso and F. Barber.
Complex constraints management.
Submitted for publication in ECAIa02, Juny 2002.
[5] F. Barber.
Reasoning on interval and point-based disjunctive metric constraints in temporal contexts.
Journal of A. I.
Research, 122:35a86, 2000.
[6] F. Barber, V. Botti, E. Onaindia, and A. Crespo.
Temporal reasoning in reakt: An environment for real-time knowledge-based systems.
AI Communications, 7(3):175a202, 1994.
[7] L. Chittaro and A. Montanari.
Trends in temporal representation and reasoning.
The knowledge engineering review, 11(3):281a288, 1996.
[8] R. Dechter.
From local to global consistency.
Artificial Intelligence, 55:87a 107, 1992.
[9] R. Dechter, I. Meiri, and Perl J. Temporal constraint networks.
Artificial Intelligence, 49:61a95, 1991.
[10] A. Garrido, E. Marzal, L. SebastiAA, and Barber F. A model for planning and scheduling integration.
In Proceedings of the 8th CAEPIAa99, pages 1a9, Murcia, Spain, 1999.
[11] E. Schwalb and Ll.
Vila.
Temporal constraints: A survey.
Constraints, 3(23):129a149, 1998.
[12] K. Stergiou and M. Koubarakis.
Backtracking algorithms for disjunctions of temporal constraints.
Artificial Intelligence, 120:81a117, 2000.
[13] P. Terenziani.
Integrating calendar dates and qualitative temporal constraints in the treatment of periodic events.
IEEE Trans.
on knowledge an data engineering, 9(5):763a783, 1997.
[14] M. Vilain and H. Kautz.
Constraint propagation algorithm for temporal reasoning.
In 5th.
National Conference on A.I., pages 377a382, Philadelphia, PA, 1986.
11
Compositional Temporal Logic Based on Partial Order Adrianna Alexander  Wolfgang Reisig  Humboldt-UniversitA$?t zu Berlin Unter den Linden 6 10099 Berlin, Germany alexander@informatik.hu-berlin.de  Abstract The Temporal Logic of Distributed Actions (TLDA) is a new temporal logic designed for the specidZcation and veridZcation of distributed systems.
The logic supports a compositional design of systems: subsystems can be specidZed separately and then be integrated into one system.
TLDA can be syntactically viewed as an extension of TLA.
We propose a different semantical model based on partial order which increases the expressiveness of the logic.
1.
Introduction Temporal logic has established itself as an appropriate tool for describing and proving properties of distributed systems.
The idea of specifying a system as a logical formula was dZrst proposed by Pnueli [16].
This means that all the possible courses of actions (or states) of the a system are exactly the models of the formula.
A property of a system will also be represented as a logical formula.
Thus, no formal distinction will be made between a system and a property.
Hence, proving that a system possess a property is reduced to proving a logical implication.
This is a fundamental benedZt of this approach.
There are also some other considerable advantages: Compositional reasoning will be eased signidZcantly: Large systems are composed of smaller components.
Properties of the composed system should be derivable from the properties of its components.
The components are represented as logical formulas.
It can be shown that parallel composition of the components can basically be represented by conjunction of the formulas representing the components.
Furthermore, it is often desirable to express that a higherlevel system is implemented by a lower-level one.
This can simply be represented in logical terms by implication.
However, on the other hand, to describe composition of concurrent systems in temporal logic is not a simple task  reisig@informatik.hu-berlin.de  (cp.
[6]): A system is usually described by help of variables: A variable updates its value in the course of time.
To represent a possible system execution, one usually assumes temporal snapshots of the actual values of the variables in a system.
Such a snapshot is most often called a global state, Val, with Var the set of formally a mapping state : Var variables and Val the set of (potential) values of the system.
A sequence s0 s1 s2 s3 of global states is called a state sequence of a system.
Each pair s i si1 of adjacent global states forms a step.
We specify such a system S with a temporal formula, IS, whose models are exactly the state sequences of S. IS will be called a specidZcation of S. Suppose now that we wish to use the system S as a part of some modular systems in which other components are working in parallel.
A state sequence of the composed system might possibly involve steps at much more frequent intervals than a state sequence of S. Hence, the values of the variables under the control of S are not updated during these intermediate steps (for the sake of simplicity suppose that the variables of S are unaffected by the other components).
Thus, there is no guarantee that the state sequences of the composed system will still be models of IS.
Consequently, the composed system cannot be specidZed by conjunction of IS and the formulas representing the other components.
A similar problem arises with implementation.
There are several solutions to these problems [6, 8, 9, 15, 12].
In this paper, we are considering one of them: stuttering invariance.
Lamportas Temporal Logic of Actions (TLA, [12, 3]) is based on this idea (another example is the Modular Temporal Logic, MTL [15]).
In this approach a temporal formula representing a system will be forced to be stuttering invariant, i.e.
its truth is preserved under addition or removal of a dZnite number of repetitions of the same state in a state sequence.
As a result of this syntactic restriction a specidZcation formula of S remains true even though another system is running in parallel  Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIMEa04) 1530-1311/04 $20.00 AS 2004 IEEE  with S. This makes it actually possible to specify composition of concurrent systems as conjunction.
On the other hand, however, stuttering causes some undesirable effects which will be described in more detail in Section 2.
As a solution for the composition (and implementation) problem described above we suggest a new temporal logic, called Temporal Logic of Distributed Actions (TLDA).
TLDA is syntactically similar to TLA, but has a semantic model different from that of TLA, called a run, which is based on a partial order.
A run consists of two components: Firstly, the history of each variable, i.e.
the sequence of its updates, secondly, the synchronization of updates.
This extends the information provided by sequences of global states.
Hence, the composition and implementation between systems can be specidZed in our logic as conjunction and implication, respectively (Section 5).
Furthermore, due to the partial order based semantic model we can explicitly distinguish between concurrent and the nondeterministic variable updates (see [5]).
Moreover, it can be determined, whether an update of a variable does not change the value of the variable or whether the variable is not updated at all (Section 4).
is intuitively unfair and is therefore to be excluded from the set of sequences representing the system M. Hence, (1) and (2) should be distinguishable in a formalism used for specifying M. Unfortunately, they can not be distinguished in a formalism based on stuttering sequences like TLA, since sequences, in which a dZnite number of iterations of the same global state is added or removed, are equivalent.
Consequently, an action changing no variable values, like A 1 in the above example, can not be described.
This implies that we cannot detect whether or not such an action is treated fair in a computation.
Note, that we could detect this for the action A1 in case the initial values of x and y happened not to be equal.
This type of actions, however, is quite common in programming languages and it seems reasonable to expect that they could be described in a specidZcation formalism.
[17] addresses this problem as causing troubles when specifying semantics of rewriting languages.
2.
Motivation  We suggest a different model instead.
We represent a (distributed) system not as a set of sequences of global states, but as a set of partially ordered sets of local updates, called runs.
The updates of a single variable are obviously totally ordered.
Updates of different variables are partially ordered: They occur in a run either concurrently or are sometime enforced to occur coincidently.
This principle depicts reality more faithfully than stuttering.
We will depict each update of a variable explicitly as a box.
In our example, the action A 1 coincidently updates the variables x and y and the action A 2 coincidently updates the variables y and z.
Hence,  We start with a simple example, which demonstrates a problem arising from stuttering and justidZes our approach.
2.1.
A Problem with Stuttering Let M be a system with three variables x, y and z.
There are two actions in M, A 1 and A2 , which are performed nondeterministically: A1 swaps the values of x and y, A 2 reads the (current) value of y and assigns to z the value z  y  1.
The variables x, y and z have initially the values 1, 1 and 0, respectively.
Additionally, assume a (weak) fairness requirement for M stating that every action eventually continuously enabled in the system would also be indZnitely often executed.
Both actions, A1 and A2 , are continuously enabled in M. Thus, both of them have to be executed indZnitely often.
is an execution of M As an example, A 1 A2 A1 A1 A2 satisfying the fairness requirement.
This execution generates the following fair state sequence of M: x: y: z:  1 1 1 1 1 1 1 1 1 1 1 1 0 0 2 2 2 4  (1)  violates the fairness In contrast, the execution A 2 A2 A2 requirement, since the action A 1 is not performed at all.
Thus, the generated state sequence x: 1 1 1 y: 1 1 1 z: 0 2 4  (2)  2.2.
A Partial Order Solution  D1 : x: 1 y: 1 z: 0 C0 C1  t1  1  t3  1  1 t2  1  1 ...  1  1  2  1 ... 4 ...  C2  (3) is the run representing the execution A 1 ; A2 ; A1 ; A1 ; A2 (the labels C0 C2 and t1 t3 will be explained in Section 3.1) and D2 :  x: y:  1 1  1  1  z:  0  2  4  ...  (4)  the run representing A 2 ; A2 ; A2 .
Obviously, there are no concurrent updates in the system M. An example of a run of another system, in which all y-updates would be concurrent  Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIMEa04) 1530-1311/04 $20.00 AS 2004 IEEE  to the z-updates is shown in (5).
D3 :  y: z:  update of one variable or a synchronized update of several variables; technically a mapping:  1  t11  1  1  ...  0  t22  2  4  ...  (5)  For the sake of compositionality, we assume, as TLA does, that the set of variables Var of a system as well as the set of values Val are indZnite.
Hence, we will explicitly describe updates of a dZnite subset of the variables only.
These variables will be called system variables.
Thus, a run consists of indZnitely many variables and we always graphically outline only the dZnite part of it concerning updates of the system variables, called the restriction of the run to the system variables.
We assume that the values of all other variables change arbitrarily.
Therefore, the set of all runs of a system will always be indZnite.
This set will be called the behavior of the system.
The next section provides the foundations of our formalism, in which we are able to specify such behaviors.
The specidZcation of the sample system M will be presented in Section 4.
3.
The Logic TLDA In this section we introduce the representation of the semantic model of TLDA, followed by its syntax and semantics.
t :V  where 0/  V fi Var is dZnite.
V  dom t  includes all variables that are involved in the transition t. t x  i denotes that the ith value in the history of x is updated by t. In the run D 1 in (3) the transition t 1 depicts a synchronized update of the variables x and y.
Hence x and y are involved in t 1 .
The transition t11 of the run D 3 in (5) is an update of the variable y.
Thus, y is the only variable involved in t11 .
Transitions in a run are (partially) ordered.
We dedZne an immediate successor relation  for t u T by t  u  H : Var  Vala is a history.
Transitions Updates of different variables in a history may synchronize, i.e.
occur coincidently.
A transition is an  iff there exists a variable x with : t x  u x  1  dom t   dom u  For example, t 1  t2  t3     holds in D1 , t11  t22 does not hold in D 3 .
Let  denote the transitive closure of .
Transitions in a run which are not related by  are called concurrent.
For instance, t 11 and t22 are concurrent in D 3 .
We require that for every transition t there is a dZnite number of transitions t i with ti  t. This completes the notions required for the dedZnition of a run: Runs Let H be a history and let T be a set of transitions.
D  H T  is a run iff  	  3.1.
The Semantic Model The semantic model of a TLDA formula is a run as already intuitively introduced and exemplidZed above in (3)a (5).
The notation of a run resembles that of an occurrence net known from Petri netas theory (see [7] for instance).
A run consists of a history of each variable and a set of transitions.
History In a run D of a system each variable x Var evolves its history.
A history of x is a dZnite or indZnite sequence H x  x0 x1 x2 of values xi Val.
xi is the local state of x at index i.
We abbreviate H x to H x .
l Hx  denotes the length of a sequence H x .
As an example, Hz  0 2 4 is the history of the variable z, and H z 0  0, Hz 1  2 etc.
The histories of the variables constitute the history of the run: Let Val and ValD denote the set of all non-empty, dZnite and indZnite sequences of values, respectively, and let Val a  Val  ValD .
Then   0  	 	  For every variable x Var and for all i with 0   i  l Hx   1, there exists exactly one transition t T with t x  i.
For all t T and for all variables x dom t  holds: 0   t x  l Hx   1.
The relation  on T is irredZexive.
The runs D1 aD3 fuldZll these properties.
In the rest of this section we assume a run D  H T  with a history H and a set of transitions T .
Since in a run D the relation  on T is transitive (by definition of a transitive closure) and irredZexive (by dedZnition of the run),  constitutes a partial order on the transition set of D .
Cuts and Steps  can canonically be generalized to the local states of variables.
A set of local states which are not related by  forms a cut.
Formally, a mapping C : Var   0 is called a cut in D iff for each t holds:  T and all x y  dom t   if t x  C x then t y  C y For instance, C0 with C0 x  C0 y  C0 z  0, as well as C2 with C2 y  2 and C2 x  C2 z  1 are cuts in D1 .
Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIMEa04) 1530-1311/04 $20.00 AS 2004 IEEE  C0 : Var 0 is obviously a cut in every run D and will be called the initial cut in D .
We say that a transition t  T occurs at C if t updates variables at the local states belonging to C, i.e.
if t x  C x for each x  dom t .
From the dedZnition of a cut arises an important observation: Any two transitions that occur at C are concurrent in D. When one or more transitions occur, another cut will be reached.
For example, from C 0 in D1 the cut C1 is reached by the occurrence of t 1 .
Let UC be the set of transitions that occur at C. For each cut C, the successor cut C Az of C will be reached by occurrence of all transitions from UC .
CAz is for each x  Var dedZned by: C Az x   C x  1 C x  if x  dom t  for some t  UC otherwise  It is quite easy to prove that C Az is a cut in D , too.
A cut C and its successor cut C Az , together with the transition set UC form a step SC .
Thus, SC will canonically be dedZned by the cut C. Note that not every cut of a run D can be reached by taking such maximal steps from C 0 .
Hence, the cuts do not constitute the run; they rather may be conceived as observations of the run.
3.2.
Syntax of TLDA Vocabulary A vocabulary of TLDA is given by the following sets: a set of function symbols , a set of predicate symbols (the symbol for equality  is one of them in particular), a set of special symbols and a set of variables.
Additionally, TLDA expressions can include brackets, which we use in order to overwrite the binding priorities or just to increase readability.
Each predicate symbol and each function symbol has an arity.
Constants can be thought of as 0-arity functions.
The set of special symbols consists of the standard boolean connectives  and fi, the quantidZer  and the temporal operator Az.
The sets , and the set of special symbols should be pairwise disjoint.
An indZnite set of variables Var all is partitioned into indZnite disjoint sets of:       rigid variables Var rigid       ,  dZexible variables Var  x y    ,  primed dZexible variables Var Az  xAz  x  Var x Az y Az    ,   a  0/ 	 a   Var  and -variables Var   x y x y    .
 x and  x y are abbreviated to x and xy,  respectively.
   Terms The terms of our logic, like in the classical predicate logic, are made up of variables and functions applied to them:   Any variable from Var rigid  Var  VarAz is a term.
 If t1 t2    tn are terms, f  has arity n, then f t1 t2    tn  is a term.
Formulas Based on the terms we can continue to dedZne the formulas of our logic in the common way.
The formulas are divided into two classes: the step formulas and the run formulas.
The set of step formulas over and is inductively dedZned as follows:     If P is a predicate taking n arguments, n  1, and if t1 t2    tn are terms over , then P t 1 t2    tn  is a step formula.
 is a step formula.
 Any variable from the set Var  If F and G are step formulas, then so are F and F fi G.  If x  Varrigid and if F is a step formula, then so is x F. (We omit here the quantidZcation over dZexible and variables, which is also dedZned in TLDA, since this would extend the scope of this paper.)
and is inductively The set of run formulas over dedZned as follows:     Any step formula F is a run formula.
 If F and G are run formulas, then so are F, F fi G and AzF.
 If x  Varrigid and if F is a run formula, then so is x F. We use some conventional arithmetical and logical abbreviations in TLDA, including boolean abbreviations true (for P  P), false,  and , as well as Az (for Az).
xAz  x  1,  z fi   0  z  15    x2 , where  is a rigid variable, are examples of step formulas  Az z  xAz  x  5 are in our logic.
Az z   Azxy examples of run formulas,  is a rigid variable.
3.3.
Semantics of TLDA Now we explain briedZy the difference between the sets of variables introduced above.
Rigid variables stand for an unknown but dZxed value.
Flexible variables will be mostly called program variables.
They are intended to describe changes in our systems: Every program variable has a value in a particular cut C of a system run.
A value of any given program variable in the successor cut C Az will be described by a corresponding primed program variable.
The partition of variables into rigid and dZexible variables is a well known idea (see for example [14]) and primed variables have also been used before for describing values of variables in a successor state ([12, 14]).
Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIMEa04) 1530-1311/04 $20.00 AS 2004 IEEE  The -variables are new.
They are independent from the values assigned to the program variables in a cut of a system run: The -variables can only take boolean values and provide information about the synchronization of variable updates.
Some subsequent examples will clarify this concept.
The semantics of the logic resembles those for other temporal logics.
We assume a non-empty set Val of concrete values, called the universe, we interpret each function symas a concrete function on Val, and each predicate bol in symbol in as a predicate over Val.
Formally, the inter consists of the following set of data: pretation I of          a non-empty set Val, for each n-ary f  a function f I : Valn  Val, and for each P fi with n arguments a subset P I fi Valn .
  Evaluating terms Let r be a mapping r : Var rigid  Val which associates with every rigid variable m a value r m of the universe.
The values of all program variables and primed program variables depend on a run.
Terms will be evaluated in steps of a run as follows: Let D  H T  be a run and let SC be a step of D taken from the cut C. To each rigid variable we assign its value according to the mapping r. To each program variable x  Var we assign the value Hx C x, i.e.
the value assigned to x at the C xth index in its history Hx .
This is intuitively the value of x in the global state C. To each primed program variable xAz  VarAz we assign the value Hx CAz x.
Intuitively, each variable xAz gets the value of x in the succeeding global state CAz .
Formally, to compute the value of a term in S C under the interpretation I and with respect to r we inductively dedZne a mapping rC as follows:  rC  rC m  r m if m  Varrigid rC x  Hx C x if x  Var rC xAz   Hx CAz x if xAz  VarAz and f t1    tn   f I rC t1     rC tn  if t1    tn are terms  Evaluating step formulas A model of a step formula consists of a step SC of a run D and an interpretation I of  (for convenience, we will write simply S C in place of SC I  when a model of a step formula is concerned).
Let r be a valuation mapping of rigid variables.
We dedZne the notion SC r D of D holding in SC with respect to r for each step formula D by structural induction on D :       SC r P t1    tn  iff rC t1     rC tn   PI .
SC r a iff a fi dom t  for any t  UC , i.e.
we replace a by the boolean value true, if a is a subset of the variables involved in a transition t occurring at C, and by false otherwise.
  SC r F, SC r F  G and SC Varrigid are standard.
r x F  for x    Examples Let SC0 and SC1 be steps of D1 as given in (3).
In SC0 holds z because no transition of S C0 involves z.
In SC1 holds yz, since there is a transition t 2 in which both y and z are involved.
This implies in particular that z is involved in this transition, so z is true in SC1 too.
The same holds for the variable y.
In contrast, in the initial step of the run D 3 in (5) holds y and z, because there are apparently transitions t 11 and t22 , in which y and z are involved, respectively.
But in this step yz does not hold.
Evaluating run formulas Now we extend the semantics to run formulas.
A model of a run formula is a pair D C consisting of a run D and a cut C of D , and an interpretation I for  (we will write for convenience simply D C  instead of D C I ).
Analogously to step formulas, we dedZne now the notion D C  r D of D holding in D C with respect to a valuation mapping r of rigid variables for each run formula D by structural induction on D :        D C r F iff SC r F. D C r F, as well as F  G and xF for x  Var rigid are standard.
D C r AzF iff D CAL   F for every cut C AL of D with CAL x 	 C x for all x  Var.
Notations We usually omit an explicit denotation of the mapping r and write simply D C  IS for D C  r IS.
Furthermore, if a run formula IS holds in D at the initial cut C0 , i.e.
D C0   IS, we write D  IS.
The set of all models of IS will be denoted by IL IS.
Hence, IL IS is the behavior of the system specidZed by IS.
Finally, V IS   x  Var x xAz occurs in IS or a occurs in IS and x  a denotes the set of Var-variables occurring in IS.
4.
Specifying Systems in TLDA With the logic of Section 3 we are now ready to specify systems.
In this section we revise our motivating example and describe the behavior of the system M based on the informal description from Section 2.
We describe the initial values of the system variables x, y and z of M by the formula: Minit   x  1y  1z  0  Recall that there are two actions in the system M which are performed nondeterministically: The action A 1 swaps the values of x and y, the action A 2 reads y and changes z according to the current value of y.
Each occurrence of A 1 or A2 will be represented in a run of M by a transition involving  Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIMEa04) 1530-1311/04 $20.00 AS 2004 IEEE  both x and y or y and z, respectively.
We describe an update of a variable in case this variable is involved in a transition: x is involved only together with y, and the value of x will be set on the previous value of y.
This will be expressed by the xy x Az  y.
We likewise describe what hapformula x pens if y and z are involved in a transition.
Additionally, we claim that x and z are never involved in the same transition.
Hence, the following formula specidZes the updates of M:    Mnext  Az          x xy xAz  y y xy yAz  x yz yAz  y z yz zAz  z  y  1 xz        HRinit HRnext  Az  AzAzz  LAz x      For simplicity we omit here the general dedZnition of fairness.
Hence, the system M is specidZed by Minit Mnext L. Note that the runs D 1 and D2 can now be well distinguished: The run D 1 is a model of this specidZcation, while D2 is not, since x does not hold in D 2 .
Consequently, D 2 is excluded from the behavior of M.  AzAz  5.
Composing SpecidZcations In this section, we focus on parallel composition of systems and their specidZcation in TLDA.
Let S 1 and S2 be systems specidZed by formulas IS 1 and IS2 , respectively.
Parallel composition of S 1 and S2 is dedZned as the intersection of their behaviors IL IS 1  IL IS2 .
From this dedZnition follows immediately by logical reasoning that the specidZcation formula of the composed system is the conjunction IS1 IS2 of the specidZcation formulas of the components.
The idea of composition as conjunction has been suggested in [4, 1, 2, 13].
Works on compositional semantics based on partial order are [10, 11].
We introduce the basic concepts with a simple version of a clock composition.
We borrow this example from [13].
An hour clock displays the hours; for this purpose we assume the variable hr to display sequences such as 22 23 00 01 .
Likewise, a minute clock with the variable min displays sequences such as 58 59 00 01 .
fi    22  23  00  01   hr  0 23  hr  suchr hr   Az    with suchr 23  0 suchr n  n  1 if n  23  We focused so far on the safety part of the specidZcation of M only.
Now we consider the liveness condition for M, stating that each of the actions A 1 and A2 should be executed indZnitely often.
In order to satisfy this condition it sufdZces to require that both x and z are indZnitely often involved in a transition of a run (such requirement for y would be redundant):  hr:  hr, including min, may change arbitrarily.
These variables constitute the environment of hr.
A run of the minute clock resembles (6) with some obvious modidZcations.
We start with specifying the hour clock: The formula HRinit specidZes the clockas initial state, viz the initial value of the variable hr to vary between 0 and 23.
HR next specidZes the clockas updates: Each update increases hr by one, with the exception that 23 is followed by 0.
...  (6)  is a run of the hour clock.
Recall that this run actually consists of indZnitely many variables.
All variables other than  Since the hour clock is a detached component which should later work as a part of a bigger system, we specify it in a way allowing arbitrary synchronization with any other subsystem.
Such specidZcation will be called environment invariant.
Formally, a formula IS will be called environment invariant iff for all runs D with D  IS holds: D  IS for all runs D such that the restrictions of D and of D to the variables V IS are identical.
We give here a sufdZcient syntactical condition for environment invariance: A formula IS is environment invariant if either IS false, or one of the cases 1a3 holds:  	        	      1. no primed or -variables occur in IS.
2.
IS has the form a I" where I" is a step formula such that either I" false (i.e.
IS is equivalent to a), or if v or vAz occurs in I" then v a and if v occurs in I" then v a  0.
/ 3.
IS has the form I" a where I" is a step formula such that no -variables occur in I", and V I"  v for a variable v Var and v a.
  fi              Lemma 1 Let IS and I" be formulas.
If IS and I  are environment invariant, then so are IS I  and IS.
  Az  These properties are very useful for writing system specidZcations.
Observe that the formula Mnext in Section 4 is environment invariant while x x Az  x  1 and xy xAz  y are not.
Now, we come back to the clock example.
To allow hr an arbitrary synchronization with its environment, we require that the formula HR next is to be applied only in system steps which involve the variable hr.
Observe that this fuldZlls the syntactical condition given above.
Hence, the specidZcation of the hour clock is  Az    HR  HRinit   Az hr  Az    HRnext   The specidZcation of the minute clock strongly resembles the  Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIMEa04) 1530-1311/04 $20.00 AS 2004 IEEE  specidZcation of the hour clock: MIN init  min 0    59 MIN next  minAz sucmin min with sucmin 59 0 sucmin n n  1 if n  59  MIN init fi Az min  MIN next  MIN One easily observes that every run of the hour-minute clock, such as (7), is a model of the conjunction HR fi MIN.
This conjunction, however, specidZes the hour and the minute clock working really in parallel.
Thus, it additionally admits models that are not proper runs of the hour-minute clock.
hr: 22 23 00 ... 59 00 01 ... 59 00 ... min: 58 (7) The unwanted models do not properly synchronize the updates of hr and min.
Hence we strive for an additional formula, SYNC, to express additional constraints on the models.
SYNC talks about synchronized updates of hr and min by help of the variable hrmin.
Two properties are required: Firstly, the variables hr and min synchronize their updates iff min 59.
Secondly, each update of hr is synchronized with an update of min: SYNC  Azhrmin  min    59 fi Azhr  hrmin  Note that the formula SYNC is environment invariant, too.
(But this is not always necessary.)
It can easily be shown by transforming hrmin  min 59 into an equivalent formula hrmin  min 59 fi min 59  hrmin fuldZlling the syntactical condition given above, and then by applying Lemma 1.
Hence, the hour-minute clock will be specidZed by HR fi MIN fi SYNC Since this specidZcation is by Lemma 1 also environment invariant, the hour-minute clock can effortless be used as a component for a further system.
The above clock example shows how a system composed of independent interacting components will be usually specidZed: Firstly, an environment invariant specidZcation will be given for each component.
Since the components are independent they always have disjoint system variables.
Secondly, a synchronization formula employing primarily -variables will be added to dedZne the interactions between the components.
6.
Conclusion We suggest a new temporal logic, TLDA, for specifying and verifying distributed systems.
The logic can syntactically be conceived as a variant of TLA.
TLDA, however,  is interpreted on partial order semantics.
This renders the logic more expressive.
Furthermore, we have shown that TLDA supports a compositional system design: subsystems can be specidZed separately and then be integrated into one system.
References [1] M. Abadi and L. Lamport.
Decomposing specidZcations of concurrent systems.
In E.-R. Olderog, editor, Proc.
of the Working Conference on Programming Concepts, Methods and Calculi (PROCOMET a94), volume A-56 of IFIP Transactions, pages 327a340.
North-Holland, 1994.
[2] M. Abadi and L. Lamport.
Conjoining specidZcations.
ACM Transactions on Programming Languages and Systems, 17(3):507a534, May 1995.
[3] M. Abadi and S. Merz.
On TLA as a logic.
In M.Broy, editor, Deductive Program Design, NATO ASI series F. Springer-Verlag, 1996.
[4] M. Abadi and G. Plotkin.
A logical view of composition.
Theoretical Computer Science, 114(1):3a30, 1993.
[5] A. Alexander and W. Reisig.
Logic of involved variables system specidZcation with Temporal Logic of Distributed Actions.
In Proc.
of the 3rd International Conference on Aplication of Concurrency to System Design (ACSDa03), pages 167a176, Guimaraes, Portugal, 2003.
[6] H. Barringer, R. Kuiper, and A. Pnueli.
Now you may compose temporal logic specidZcations.
In Proc.
of the 16th Annual ACM Aymposium on Theory of Computing, pages 51a 63, 1984.
[7] E. Best and C. Fernandez.
Nonsequential processes a a Petri net view.
In W. Brauer, G. Rozenberg, and A. Salomaa, editors, EATCS Monographs on Theoretical Computer Science, volume 13.
Springer-Verlag, 1988.
[8] A. Cau and W.-P. d. Roever.
A dense-time temporal logic with nice compositionality properties.
In Proc.
of the 6th International Workshop on Computer Aided Systems Theory EUROCASTa97, Las Palmas de Gran Canaria, Spain, volume 1331 of LNCS, pages 123a145.
Springer, February 1997.
[9] J. Fiadeiro and T. Maibaum.
Sometimes atomorrowa is asometimea: Action redZnement in a temporal logic of objects.
In D. Gabbay and H. Ohlbach, editors, Proc.
of the 1st International Conference on Temporal Logic ICTLa94, volume 827 of LNAI, pages 48a66.
Springer-Verlag, 1994.
[10] D. Gomm, E. Kindler, B. Paech, and R. Walter.
Compositional liveness properties of EN-systems.
In M. Marsan, editor, Applications and Theory of Petri Nets 1993, 14th International Conference, volume 691 of LNCS, pages 262a281.
Springer-Verlag, June 1993.
[11] E. Kindler.
A compositional partial order semantics for Petri net components.
In P. AzASma and G. Balbo, editors, Application and Theory of Petri Nets 1997, 18th International Conference, volume 1248 of LNCS, pages 235a252.
SpringerVerlag, June 1997.
[12] L. Lamport.
The Temporal Logic of Actions.
ACM Transactions on Programming Languages and Systems, 16(3):872a 923, May 1994.
Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIMEa04) 1530-1311/04 $20.00 AS 2004 IEEE  [13] L. Lamport.
Composition: A way to make proofs harder.
In A. W.P.de Roever, H.Langmaack, editor, Compositionality: The SignidZcant Difference, International Symposium, COMPOSa97, volume 1536 of LNCS, pages 402a423, September 1997.
[14] Z.
Manna and A. Pnueli.
The temporal logic of Reactive and Concurrent Systems: SpecidZcation.
Springer, 1992.
[15] A. Mokkedem and D. Mery.
A stuttering closed temporal logic for modular reasoning about concurrent programs.
In D. Gabbay and H. Ohlbach, editors, Temporal Logic, Proc.
of the 1st International Conference on Temporal Logic ICTLa94, volume 827 of LNAI, pages 382a397.
SpringerVerlag, 1994.
[16] A. Pnueli.
The temporal semantics of concurrent programs.
Theoretical Computer Science, 13(1):45a61, 1981.
[17] M. Reynolds.
Changing nothing is sometimes doing something: Fairness in extensonal semantics.
ulr: citeseer.nj.nec.com/ reynolds96changing.
html, 1996.
Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIMEa04) 1530-1311/04 $20.00 AS 2004 IEEE
Analysis of timed processes with data using algebraic transformations Michel A. Reniers Yaroslav S. Usenko Department of Mathematics and Computer Science, Technical University of Eindhoven, P.O.
Box 513, 5600 MB Eindhoven, The Netherlands  Abstract The language of timed uCRL is an extension of an ACP-style process algebra-based language uCRL with time-related features.
In this paper we describe this language and its equational axiomatization, and give an example specification.
We outline the method of simplifying transformations based on this equational axiomatization, and illustrate it on this example.
This transformation method allows for a time-free abstraction of the specification, which in turn enables the use of tools and techniques for verification of untimed systems.
We prove some properties of the example using a known invariants technique.
Key words: Process Algebra, Real Time, Axioms, Verification.
1  Introduction  The language uCRL, see [13], offers a uniform framework for the specification of data and processes.
Data are specified by equational specifications (cf.
[5, 19]): one can declare sorts and functions working upon these sorts, and describe the meaning of these functions by equational axioms.
Processes are described in process algebraic style, where the particular process syntax stems from ACP [6, 4, 9], extended with data-parametric ingredients: there are constructs for conditional composition, and for data-parametric choice and communication.
As is common in process algebra, infinite processes are specified by means of (finite systems of) recursive equations.
In uCRL such equations can also be data-parametric.
As an example, for action a and adopting standard semantics for uCRL, each solution for the equation X = a * X specifies (or "identifies") the process that can only repeatedly execute a, and so does Y(17) where Y(n) is defined by the data-parametric equation Y(n) = a * Y(n + 1) with n [?]
Nat.
Several timed extensions have been proposed for different kinds of process algebras.
For an overview of ACP extensions with time we refer to [3].
According to [3], timed process algebras can be categorized by three criteria.
Discrete vs. continuous time; relative vs. absolute time; two-phase vs. timed-stamped model.
In [11] the language uCRL is extended with time, and in [21] a sound and complete axiomatization of timed uCRL is presented.
In [15] some examples of specification and reasoning in timed uCRL are given.
Timed uCRL makes use of absolute time, timed stamped model, and the time domain can be defined by the user (both discrete and continuous domains are possible).
For a way to interpret timed automata [1] in timed uCRL we refer to [23, Chapter 6].
In this paper we present a method to describe and analyze real-time systems using uCRL and timed uCRL.
It is assumed that some real-time system is described by means of a timed uCRL specification (see Section 2).
Mostly such descriptions will contain operators such as parallel composition that complicate analysis.
As a first step towards the analysis of such systems, we linearize the given description using the algorithm from [22] (see Section 3).
The result is a Timed  1  Linear Process Equation (TLPE) which is equivalent to the original description and has a very simple structure.
On this TLPE it is already possible to perform some analysis.
Next, in Section 4, we describe how a TLPE can be transformed into an LPE, i.e., a linear process equation without time.
This transformation, called time-free abstraction, has been used for a more restricted class of timed uCRL specifications in [21].
Crucial steps in this transformation are that the TLPE is first transformed into a well-timed TLPE and second that it is transformed into a deadlock-saturated well-timed TLPE.
Finally, all time-stamping is captured in the parameters of atomic actions.
The result is an LPE for which the machinery of untimed uCRL can be put to use for further analysis.
These are based on symbolic analysis of the specifications, such as invariants, term rewriting and theorem proving, or on explicit state space generation and model-checking.
We illustrate the respective steps of the proposed method on the Bottle Filling System from [15, Section 3] and [3, Section 4.2.5].
In comparison to those expositions, we apply a proved transformation method [22] and a general analysis technique based on invariants [7].
2  uCRL and Timed uCRL  Timed uCRL specifications contain algebraic specifications of several abstract data types.
The only data types that are required are booleans and time.
The algebraic specifications of booleans are standard and can be found for instance in [8, Chapter IV].
We assume a constant t and functions !
: Bool - Bool and [?]
: Bool x Bool - Bool .
Time Time can be represented in many different ways.
In timed uCRL the time domain has to satisfy a set of properties.
We present these properties as an algebraic specification of sort Time by defining its signature and the axioms.
The signature of sort Time consists of: a constant 0; functions leq, eq : Time x Time - Bool , which are often abbreviated as <= and =, respectively; function if : Bool x Time x Time - Time; and functions min, max : Time x Time - Time.
The axioms of sort Time are presented below.
Many of the axioms are taken from, or inspired by [11, 16].
The axioms say that <= is a total order on the Time domain, and 0 is the least element.
(Time10 )  t<=u[?
]u<=w [?]t<=u[?]u<=w[?
]t<=w t<=u[?]!
w <=u[?]t<=u[?]!
w <=u[?]!
w <=t  (Time100 )  !
u<=t[?
]u<=w [?]!
u<=t[?]u<=w[?]!
w <=t  (Time1000 )  0<=t[?
]t  (Time2)  t<=u[?]u<=t[?
]t  (Time3)  eq(t, u) [?]
t <= u [?]
u <= t  (Time5)  min(t, u) [?]
if (t <= u, t, u)  (Time6)  max (t, u) [?]
if (u <= t, t, u)  (Time60 )  if (t, t, u) [?]
t  (Time7) (Time80 )  if (!b, t, u) [?]
if (b, u, t) if (b1 [?]
b2 , t, u) [?]
if (b1 , t, if (b2 , t, u))  (Time9)  if (b1 , if (b2 , t, u), w) [?]
if (b1 [?]
b2 , t, if (b1 , u, w))  (Time10)  if (t <= u [?]
u <= t, t, u) [?]
u  (Time11)  t <= if (b, u, w) [?]
(b [?]
t <= u) [?]
(!b [?]
t <= w)  (Time12)  if (b, u, w) <= t [?]
(b [?]
u <= t) [?]
(!b [?]
w <= t)  (Time13)  The last seven axioms allow to eliminate if from any boolean expression containing subterms of sort Time.
Every term of sort Time can be represented as 0, a variable, or as if (b, t, u) where t and u are terms of sort Time.
The above mentioned form has two extremes: one where all boolean terms b are variables, and another, where every variable of sort time (and 0) occurs at most once.
The latter form is useful for proving time identities in the following way: if we order the time variables occurring in a term as 0 < t1 < .
.
.
< tn , then with the help of the axioms we can transform every term of sort Time to the form if (b1 , ti0 , if (b2 , ti1 , .
.
.
if (bm , tim-1 , tim ) .
.
. ))
with indices such that tik < tik+1 .
Moreover, the conditions b1 , .
.
.
, bm can be made pairwise distinct,  2  i.e.
having the property that i 6= j - bi [?]
bj [?]
f .
In addition, the conditions b1 , .
.
.
, bm can be made such that if eq(tik , tik+1 ) [?]
t, then bk [?]
t. This gives us a method for proving identities of sort Time.
Other Data Types.
Any other data type in uCRL is specified in a similar way by providing a signature and axioms from which all other identities are derived.
Other data sorts have generally different axioms, and sometimes induction principles (cf.
[14]) are required to describe them.
Processes Next we define the binding-equational theory of timed uCRL by defining its signature and the axioms.
The signature of timed uCRL consists of data sorts (or 'data types') including Bool and Time as defined above, and a distinct sort Proc of processes.
Each data sort D is assumed to be equipped with a binary function eq : D x D - Bool .
(This requirement can be weakened by demanding such functions only for data sorts that are parameters of communicating actions).
The process operations are the ones listed below: - - - - * actions a : Da - Proc where a [?]
ActLab is an action label and Da is a list of parameter types of a.
It is assumed that the signature of timed uCRL is parameterized by the finite set of action labels ActLab.
* deadlock d :- Proc.
The constant d models inaction, or the inability to perform actions.
* alternative composition + : Proc x Proc - Proc.
The process p + q behaves like p or like q, depending on which of the two performs the first action.
* sequential composition * : Proc x Proc - Proc.
The process p * q first performs the actions of p, until p terminates, and then continues with the actions from q.
* conditional operator _ C _ B _ : Proc x Bool x Proc - Proc.
The process term p C b B q behaves like p if b is equal to t, and if b is equal to f it behaves like q. P * alternative P quantification d:D : Proc - Proc, for each data variable d of sort D. The process d p behaves like p[d1 /d] + p[d2 /d] + * * * , i.e., as the possibly infinite alternative composition of processes p[di /d] for any data term di of sort D. * at-operator , : Proc xTime - Proc.
A key feature of timed uCRL is that it can be expressed at which time a certain action must take place.
This is done using the at-operator.
The process p,t behaves like the process p, with the restriction that the first action of p must start at time t. The process p , t can delay till at most time t. If p consists of several alternatives, then only those with the first actions starting at time t will remain in p , t. The alternatives that start earlier than t will express that p , t can delay till that earlier time.
The alternatives that start later than t will express that p , t can wait till time t (but not till that later time).
* initialization operator  : Time xProc - Proc and weak initialization operator  : Time x Proc - Proc.
The initialization operator tp expresses the process in which all alternatives of p that start earlier than t are left out, but an alternative to delay till time t is added.
The weak initialization operator t [?]
p expresses the process in which all alternatives of p that start earlier than t are replaced by the ability to delay till those earlier times.
Thus the process t [?]
p can delay till the same time as p, while t  p can delay till at least time t, which can be longer than p could delay.
* parallel composition k : Proc x Proc - Proc, left-merge T : Proc x Proc - Proc, and communication merge | : Proc x Proc - Proc.
The process p k q can first perform an action of p, first perform an action of q, or start with a communication, or synchronization, between p and q.
The process p k q exists at time t only if both p and q exist at time t. The process p T q is as p k q, but the first action that is performed comes from p. The action can only be performed if the other party still exists at that time.
The process p | q also behaves as  3  the process p k q, except that the first action must be a communication between p and q.
Furthermore, these actions must occur at the same time and have the same data parameters.
The action resulting from such a communication is defined by the partial commutative and associative function g : ActLab x ActLab - ActLab such that g(a1 , a2 ) [?]
ActLab implies that a1 , a2 and g(a1 , a2 ) have parameters of the same sorts.
It is assumed that the signature of timed uCRL is parameterized by this function g. * encapsulation [?
]H : Proc - Proc, for H [?]
ActLab.
The process [?
]H (p) behaves as the process p where the execution of actions from the set H is prohibited.
* ultimate delay [?
]U : Proc - Proc.
The ultimate delay operator [?
]U(p) expresses the process, which can delay as long as p can, but cannot perform any action.
* before operator  : Proc - Proc.
The before operator p  q expresses the process in which all alternatives of p that start later than [?
]U(q) are replaced by the abilities to delay till [?]U(q).
Thus p  q cannot delay longer than both p and q.
The ultimate delay [?
]U(p) of process p can be expressed in terms of  as d  p. This process cannot perform actions and can delay as long as p could (because d can delay till any time).
Another key feature of timed uCRL is that it can be expressed that a process can delay till a certain time.
The process p + d , t can certainly delay till time t, but can possibly delay longer, depending on p. Consequently, the process d , 0 can neither delay nor perform actions, and the process d can delay for an arbitrary long time, but cannot perform any action.
We follow the intuition that a process that can delay till time t can also delay till an earlier moment, and a process that can perform a first action at time t can also delay till time t. P The descending order of binding strength of operators is: ,, *, {, [?
], }, {k, T, |}, CB, , +.
In Appendix A the axioms of timed uCRL are given.
Many of these axioms are taken from, or inspired by [21, 12].
To prove identities in timed uCRL we use a combined many-sorted calculus, which for the sort of processes has the rules of binding-equational calculus, for the sorts of booleans and time has the rules of equational calculus, while other data sorts may include induction principles which could be used to derive process identities as well.
We note that the derivation rules of binding-equational calculus do not allow to substitute terms containing free variables if they become bound.
The operational semantics (SOS) of timed uCRL and soundness and completeness proofs of the axiomatization are presented in [21].
The axiomatization used here is an extension of the axiomatization in [21] with a number of axioms that are derivable in the setting of [21] for all closed terms.
These extra axioms are needed to prove correctness of the linearization.
Timed uCRL Specifications For the purpose of this paper we restrict to the timed uCRL specifications that do not contain left merge (T), communication (|), ultimate delay ([?
]U), and before () operators explicitly.
These operators were introduced to allow the finite axiomatization of parallel composition (k) and timing constructs in the bisimulation setting, and they are hardly used explicitly in timed uCRL specifications.
We consider systems of process equations with the right hand sides from the following subset of timed uCRL terms P - - - - p ::= a( t ) | d | Y( t ) | p + p | p * p | p k p | d:D p | p C c B p | [?
]H (p) | p , t | t  p | t [?]
p - - For a system of process equations G containing a process equation for X, (X( t ), G) is a process - - definition if t is a list of data terms that corresponds to the type of process X.
The combination - - of the given data specification with a process definition (X( t ), G) of process equations determines a timed uCRL specification.
Such a specification depends on a finite subset Act of ActLab and on Comm, an enumeration of g restricted to the labels in Act.
4  2.1  Example: Bottle Filling System  This example is taken from [15, Section 3] and [3, Section 4.2.5].
We start from the informal specification from [3, page 153]: "Bottles on a conveyor belt are filled with 10 liters of liquid poured from a container with a capacity of m liters.
The container is filled at a constant rate of r liters per second.
When a bottle is under the container, a tap is opened and the bottle is filled at a rate of 3 liters per second until the container becomes empty.
From that moment, the bottle is filled at the same rate as the container.
When the bottle is full, the tap is closed and the conveyor belt starts moving to put the next bottle under the container which takes 1 second.
Obviously, it is highly preferable that overflow (of the container) never occurs.
Of course, it is also preferable that the container does not get empty during the filling of each bottle."
Specification in Timed uCRL The time domain used in this specification is nonnegative rational numbers.
For the specification of the conveyor belt we distinguish three 'modes of operation': mv - moving, nf - normal filling, and sf - slow filling.
CBmv (t:Time) =!start , (t + 1) * CBnf (t + 1) X CBnf (t:Time) = ?empty , (t + t0 ) * CBsf (t + t0 , 3t0 ) C t0 < 10/3B  t0 :Time !stop , (t + 10/3) * CBmv (t + 10/3) CBsf (t:Time, l:Q) =!stop , (t + (10 - l)/r) * CBmv (t + (10 - l)/r) The process CBmv executes the !start action and then behaves as the process CBnf .
The latter process can synchronize with the container process by ?empty at time period t + t0 , where t0 [?]
[0, 10/3), or it can synchronize by !stop action at time t + 10/3.
The further behavior of CBnf depends on which action it synchronized.
For the specification of the container also three modes are distinguished: inc - increasing the amount of liquid, dec - decreasing, and dry - liquid goes through the empty container directly into the bottle.
X Cinc (t:Time, h:Q) = ?start,(t + t0 ) * Cdec (t + t0 , h + rt0 ) C t0 < (m - h)/rB  t0 :Time !overflow , (t + (m - h)/r) * d , (t + (m - h)/r) X Cdec (t:Time, h:Q) = ?stop,(t + t0 )*Cinc (t + t0 , h - (3 - r)t0 ) C t0 <= h/(3 - r) B d,0 t0 :Time  + !empty , (t + h/(3 - r)) * Cdry (t + h/(3 - r)) X Cdry (t:Time) = ?stop ,t0 * Cinc (t0 , 0) t0 :Time  The process Cdec behaves nondeterministically at time t + h/(3 - r).
Depending on the parallel process it can either perform ?stop or !empty in order to synchronize with that process.
Using the above descriptions of the conveyor belt and the container, the system can be given as: T(t:Time, h:Q) = [?
]H (CBmv (t) k Cinc (t, h)) where g(?s, !s) = g(!s, ?s) = s for s [?]
{start, stop, empty} and g is undefined otherwise, and H = {?s, !s | s [?]
{start, stop, empty}}.
In words, the system is a parallel composition of the conveyor belt and the container processes, that are forced to synchronize on all actions except !overflow.
Let G contain all of the above equations.
Then the process definition (T(0, h), G) forms the specification of the bottle-filling system.
5  3  Linearization  - - The problem of linearization of a timed uCRL specification defined by (X( t ), G) consists of generation of a new timed uCRL specification which * depends on the same Act and Comm, * contains all data definitions of the original one, and, possibly, definitions of the auxiliary data types, - - * is defined by (Z(mX ( t )), L), where L contains exactly one process equation for Z in linear form (defined later), and mX is a mapping from the parameters of X to the parameters of X.
- - - - such that all processes that are solutions of (X( t ), G) are also solutions of Z(mX ( t )), L).
It is not possible to linearize a timed uCRL specification which is unguarded, e.g.
X = X cannot be brought to the linear form.
The exact notion of guardedness in uCRL is rather complicated.
In a nutshell, in a guarded process every occurrence of a recursive call is preceded (with sequential composition) by an action.
We refer to [22, Section 3.6] for a precise definition.
We define Timed Parallel Greibach Normal Form (TPEGNF) and Timed Linear Process Equation (TLPE) as special forms of process equations in timed uCRL.
TPEGNF and TLPE are similar to the Greibach Normal Form [10] for context-free languages.
A timed uCRL process equation is in TPEGNF if it is of the form: XX --- - --- - --- --- --- X(d:D) = ai ( fi (d, ei )) , ti (d, ei ) * pi (d, ei ) C ci (d, ei ) B d,0 --- i[?
]I - ei :Ei  +  X X  - --- - --- --- aj ( fj (d, ej )) , tj (d, ej ) C cj (d, ej ) B d,0  --- j[?
]J - ej :Ej  +  X  --- --- d , td (d, ed ) C cd (d, ed ) B d,0  ---- ed :Ed  --- where I and J are disjoint, and all pi (d, ei ) have the following syntax: - - - - - - p::=a( t ) | d | Y( t ) | p * p | p k p | [?
]H (pkp) | [?
]H (Y( t )) | p,t | t  p | t [?]
p A timed uCRL process equation is called Timed Linear Process Equation (TLPE) if it is of the --- --- - same form as above, but the terms pi (d, ei ) are recursive calls of the form X(- gi (d, ei )) for some - - function vectors gi .
- - The equation is explained as follows.
The process X, being in a state vector d , can for any - - - - - --- - - ei , that satisfy the condition ci (d, ei ), perform an action ai parameterized by fi (d, ei ) at the --- --- - - absolute time ti (d, ei ), and then proceed to the state - gi (d, ei ).
Moreover, it can for any - ej , that --- - --- - satisfy the condition cj (d, ej ), perform an action aj parameterized by fj (d, ej ), and then terminate --- ---- successfully.
The last summand indicates that for any ed :Ed , that satisfies cd (d, ed ), the process --- can wait till the absolute time td (d, ed ).
- - As input for the linearization procedure we take a timed uCRL process definition (X( t ), G).
Further on, the process goes through a number of intermediate forms, including TPEGNF, and finally we get to TLPE.
All the steps are described in [22, Chapter 6] and are proved to transform a system of process equations in timed uCRL to an equivalent one.
3.1  Linearization of the Example  In this subsection we illustrate some of the linearization steps on our example.
The example does not contain double bound variables, so we can start with reducing right hand sides of the equations  6  with the help of conventional term rewriting [2].
This step is described in [22, Section 6.2.2].
By doing this step the equations for CBnf and Cinc change to the following: X CBnf (t:Time) = ?empty , (t + t0 ) * CBsf (t + t0 , 3t0 ) C t0 < 10/3 B d,0 t0 :Time  + !stop , (t + 10/3) * CBmv (t + 10/3) X Cinc (t:Time, h:Q) = ?start , (t + t0 ) * Cdec (t + t0 , h + rt0 ) C t0 < (m - h)/r B d,0 t0 :Time  + !overflow , (t + (m - h)/r) * d , (t + (m - h)/r) In both equations we move the alternative composition operator (+) outside the sum operator and eliminate the sum in the second summand.
At this point all our equations, except the one for T are in TPEGNF.
We proceed by guarding [22, Section 6.2.4] the equation for T. We have to consider the term [?
]H (CBmv (t) k Cinc (t, h)) and apply the guarding procedure to it.
We get the following: [?
]H (CBmv (t) k Cinc (t, h)) = start , (t + 1) * [?
]H (CBnf (t + 1) k Cdec (t + 1, h + r)) C h < m - r B d,0 + !overflow , (t + (m - h)/r) * d , (t + (m - h)/r) C m - r <= h B d,0 Here we use the fact that only !overflow action is not forced to synchronize, and the rest of the actions have to.
It is interesting to see how the !overflow action gets its condition.
It has to happen before time t + 1, otherwise the action !start , (t + 1) should occur first.
This means that t + (m - h)/r <= t + 1 should hold, which is equivalent to m - r <= h. Now we consider the term [?
]H (CBnf (t)kCdec (t, h)) and apply the guarding procedure to it.
Here we again use the fact that the all the actions of the two processes have to synchronize.
We get the following: [?
]H (CBnf (t) k Cdec (t, h)) = stop , (t + 10/3) * [?
]H (CBmv (t + 10/3) k Cinc (t + 10/3, h - 10(3 - r)/3)) C 10(3 - r)/3 <= h B d,0 +empty , (t + h/(3 - r)) * [?
]H (CBsf (t + h/(3 - r), 3h/(3 - r)) k Cdry (t + h/(3 - r))) C h < 10(3 - r)/3 B d,0 Now we consider the term [?
]H (CBsf (t, l) k Cdry (t)) and apply the guarding procedure to it.
We get the following: [?
]H (CBsf (t, l) k Cdry (t)) = stop , (t + (10 - l)/r) * [?
]H (CBmv (t + (10 - l)/r) k Cinc (t + (10 - l)/r, 0)) At this point we are ready to make a single equation for the whole system (cf.
[22, Section 4.3]).
Here we use the fact that only the parallel processes are reachable.
We define a new sort  7  State = {mv_inc, nf_dec, sf_dry, dl} and use it as a parameter of the resulting process T: T(s:State, t:Time, h, l:Q) = start , (t + 1) * T(nf_dec, t + 1, h + r, 0) C s = mv_inc [?]
h < m - r B d,0 + !overflow , (t + (m - h)/r) * T(dl, t + (m - h)/r, 0, 0) C s = mv_inc [?]
m - r <= h B d,0 + stop , (t + 10/3) * T(mv_inc, t + 10/3, h - 10(3 - r)/3, 0) C s = nf_dec [?]
10(3 - r)/3 <= h B d,0 + empty , (t + h/(3 - r)) * T(sf_dry, t + h/(3 - r), 0, 3h/(3 - r)) C s = nf_dec [?]
h < 10(3 - r)/3 B d,0 + stop , (t + (10 - l)/r) * T(mv_inc, t + (10 - l)/r, 0, 0) C s = sf_dry B d,0 + d , t C s = dl B d,0 This equation is in TLPE format.
Let system of equations L contain the above equation only.
Then (T(mv_inc, 0, h, 0), L) is the linearized specification of the bottle-filling system.
4  Time-free Abstraction and Analysis  - - An important notion of timed uCRL processes is well-timedness.
A term a( t ) , t * p is well-timed - - if p [?]
t  p. If t is such that c(t) [?]
t implies p [?]
t  p, then a( t ) , t * p C c(t) B d , 0 is also - - well-timed.
Terms a( t ) , t and d , t are also well-timed.
If p and q are well-timed terms, then p + q, P d:D p and p C c B d,0 are also well-timed terms.
- - An equation in TPEGNF is well-timed if for all i [?]
I the terms ai ( ti ) , ti * pi C ci B d , 0 are well-timed.
The linearization method for timed uCRL ensures that the resulting TLPE is well-timed, e.g., in our example, t  T(s, t, h, l) [?]
T(s, t, h, l).
The time-free abstraction (cf.
[21, Section 4.2]) of well-timed TLPEs can be used for further analysis with methods that are designed for untimed uCRL.
For instance, strong bisimilarity of time-free abstractions of two well-timed TLPEs is equivalent to the timed bisimilarity of them.
In the initial timed uCRL specification time has a direct influence on the specified behavior, for instance on the interleavings of parallel components (for example a , 1 k b , 2 [?]
a , 1 * b , 2 in timed uCRL).
This is why performing the time-free abstraction on the initial specification will not work (because a(1) k b(2) 6[?]
a(1) * b(2) in uCRL).
However, after linearization the influence of time on the specified behavior is encoded in the parameters and conditions of resulting TLPE, i.e.
time becomes just a conventional data type in untimed uCRL.
Applying time-free abstraction to our example gives us the following uCRL equation: T(s:State, t:Time, h, l:Q) = start(t + 1) * T(nf_dec, t + 1, h + r, 0) C s = mv_inc [?]
h < m - r B d + !overflow(t + (m - h)/r) * T(dl, t + (m - h)/r, 0, 0) C s = mv_inc [?]
m - r <= h B d + stop(t + 10/3) * T(mv_inc, t + 10/3, h - 10(3 - r)/3, 0) C s = nf_dec [?]
10(3 - r)/3 <= h B d + empty(t + h/(3 - r)) * T(sf_dry, t + h/(3 - r), 0, 3h/(3 - r)) C s = nf_dec [?]
h < 10(3 - r)/3 B d + stop(t + (10 - l)/r) * T(mv_inc, t + (10 - l)/r, 0, 0) C s = sf_dry B d + [?
](t) C s = dl B d Analysis We try to prove some properties of the bottle-filling system.
For this we assume that in the initial state m > r > 0.
It is easy to see that both r and m do not change in T, and 8  therefore these properties are invariants of T. It is also easy to see that h >= 0 is an invariant of the system.
Having assumed that, we see that if h >= m - r in the initial state, then the overflow is eminent at time (m - h)/r.
It is interesting to see what happens if h < m - r in the initial state.
To this end, we can see that the following formula is an invariant (cf.
[7]) of the LPE: r <= 30/13 [?]
(s = mv_inc == h < m - r) [?]
(s = nf_dec == h < m) This gives us the fact that if r <= 30/13, then overflow is not reachable provided h < m - r holds in the initial state.
In case r > 30/13, the process T is equal to T(s:State, t:Time, h, l:Q) = start(t + 1) * stop(t + 13/3) * T(mv_inc, t + 13/3, h + r - 10(3 - r)/3, 0) C s = mv_inc [?]
h < m - r B d + !overflow(t + (m - h)/r) * T(dl, t + (m - h)/r, 0, 0) C s = mv_inc [?]
m - r <= h B d + [?
](t) C s = dl B d This is because r > 30/13 and h >= 0 implies that h+r < 10(3-r)/3 is always false.
It is clear that the value of h increases with every sequence of start, stop actions by a constant, so the overflow is eminent.
The next question is whether the container may become empty.
From the previous analysis follows that this can only happen if r <= 30/13 and h < m - r holds in the initial state.
If r < 30/13, the value of h will decrease with each sequence of start, stop actions by a constant.
So, eventually, its value will become smaller than 10(3 - r)/3 and the container will become empty.
In case r = 30/13, the value of h is constant in state nf_dec and equal to the initial value of h plus 10(3 - r)/3 = 30/13, so, the container does not become empty in this case.
5  Conclusions and Future Work  We presented the language of timed uCRL with an example specification.
We outlined the method of simplifying transformations based on equational axiomatization, and illustrated it on the example.
This transformation allows for a time-free abstraction of the specification, which in turn enables the use of tools and techniques for verification of untimed systems.
For proving properties of the presented example we used known invariant [7] techniques.
An interesting direction for future work is in adopting efficient real-time abstraction techniques similar to the regions and zones methods [1] for timed automata.
Another interesting approach is to make use of model checking techniques, similar to the ones available for timed automata in tools like UPPAAL [18].
A symbolic model checking approach for untimed uCRL has been recently proposed in [17].
It looks more applicable to the time setting than the explicit model checking [20] of modal mu-calculus formulas.
In order to apply any of these methods for timed setting a well-thought extension of modal mu-calculus (or another action-based temporal logic) to real-time is needed.
References [1] R. Alur.
Timed automata.
In Proc.
CAV'99, LNCS 1633, pages 8-22, 1999.
[2] F. Baader and T. Nipkow.
Term Rewriting and All That.
Cambridge University Press, August 1999.
[3] J. C. M. Baeten and C. A. Middelburg.
Process Algebra with Timing.
Monographs in TCS.
Springer, 2002.
9  [4] J. C. M. Baeten and W. P. Weijland.
Process Algebra.
Cambridge Tracts in TCS 18.
Cambridge University Press, 1990.
[5] J.
A. Bergstra, J. Heering, and P. Klint, editors.
Algebraic Specification.
ACM Press, ACM Press Frontier Series, 1989.
[6] J.
A. Bergstra and J. W. Klop.
Process algebra for synchronous communication.
Information and Computation, 60(1/3):109-137, 1984.
[7] M. A. Bezem and J. F. Groote.
Invariants in process algebra with data.
In B. Jonsson and J. Parrow, editors, Proc.
CONCUR'94, LNCS 836, pages 401-416.
Springer, 1994.
[8] S. N. Burris and H. P. Sankappanavar.
A Course in Universal Algebra.
Number 78 in Graduate Texts in Mathematics.
Springer-Verlag, 1981.
[9] W. J. Fokkink.
Introduction to Process Algebra.
Texts in TCS.
An EATCS Series.
Springer, 2000.
[10] S. A. Greibach.
A new normal-form theorem for context-free phase structure grammars.
JACM, 12(1):42-52, 1965.
[11] J. F. Groote.
The syntax and semantics of timed uCRL.
Report SEN-R9709, CWI, Amsterdam, 1997.
[12] J. F. Groote and S. P. Luttik.
Undecidability and completeness results for process algebras with alternative quantification over data.
Report SEN-R9806, CWI, Amsterdam, July 1998.
Available from http://www.cwi.nl/~luttik/.
[13] J. F. Groote and M. A. Reniers.
Algebraic process verification.
In J.
A. Bergstra, A. Ponse, and S. A. Smolka, editors, Handbook of Process Algebra, chapter 17, pages 1151-1208.
Elsevier, 2001.
[14] J. F. Groote and J. J. v. Wamel.
Algebraic data types and induction in uCRL.
Report P9409, University of Amsterdam, Programming Research Group, 1994.
[15] J. F. Groote and J. J. v. Wamel.
Analysis of three hybrid systems in timed uCRL.
SCP, 39:215-247, 2001.
[16] J. F. Groote and J. J. v. Wamel.
The parallel composition of uniform processes with data.
TCS, 266(1-2):631-652, 2001.
[17] J. F. Groote and T. A. C. Willemse.
A checker for modal formulae for processes with data.
In F. S. de Boer, M. M. Bonsangue, S. Graf, and W.-P. de Roever, editors, Proc.
FMCO'04, LNCS 3188, pages 223-239, 2004.
[18] K. G. Larsen, P. Pettersson, and W. Yi.
UPPAAL in a nutshell.
International Journal on Software Tools for Technology Transfer, 1(1-2):134-152, 1997.
[19] J. Loeckx, H.-D. Ehrich, and M. Wolf.
Algebraic specification of abstract data types.
In S. Abramsky, D. Gabbay, and T. S. E. Maibaum, editors, Handbook of Logic in Computer Science, Vol 5, chapter 4, pages 217-316.
Oxford University Press, 2000.
[20] R. Mateescu and M. Sighireanu.
Efficient on-the-fly model-checking for regular alternation-free mu-calculus.
SCP, 2002.
[21] M. A. Reniers, J. F. Groote, J. J. v. Wamel, and M. B. v. d. Zwaag.
Completeness of Timed uCRL.
Fund.
Inf., 50(3-4):361-402, 2002.
[22] Y. S. Usenko.
Linearization in uCRL.
PhD thesis, Eindhoven University of Technology, December 2002.
[23] T. A. C. Willemse.
Semantics and Verification in Process Algebras with Data and Timing.
PhD thesis, Eindhoven University of Technology, 2003.
10  A  Axioms of Timed uCRL  We assume that * x, y, z are variables P of sort Proc; c, c1 , c2 are variables of sort Bool ; d, d1 , dn , d0 , .
.
.
are data variables (but d in d:D is not a variable); and t, u, w are variables of sort Time.
- - * b stands for either a( d ), or d; - - - - - - 1 n * d = d0 is an abbreviation for eq(d1 , d0 ) [?]
* * * [?]
eq(dn , d0 ), where d = d1 , .
.
.
, dn and -0 - 1 n d = d0 , .
.
.
, d 0 ; * the axioms where p and q occur are schemata ranging over all terms p and q of sort Proc, including those in which d occurs freely; * the axiom (SUM2) is a scheme ranging over all terms r of sort Proc in which d does not occur freely.
11  x+y [?
]y+x  (A1)  x + (y + z) [?]
(x + y) + z  (A2)  x+x[?
]x  (A3)  (x + y) * z [?]
x * z + y * z  (A4)  (x * y) * z [?]
x * (y * z)  (A5)  x + [?
]U (x) [?]
x  (A6T)  d + [?
]U (x) [?]
d  (A6T0 )  d*x[?
]d  x k y [?]
(x T y + y T x) + x | y  b , t T y [?]
(b , t  y) * y  (b , t * x) T y [?]
(b , t  y) * ((t  x) k y) (x + y) T z [?]
x T z + y T z 0  0  xCtBy [?
]x  (Cond1) (Cond2)  x C c B y [?]
x C c B d , 0 + y C !c B d , 0  0  (x + y) | z [?]
x | z + y | z  (Cond3T)  (ATA8)  0  otherwise  x|y [?
]y|x  (x C c B d , 0) * y [?]
(x * y) C c B d , 0  (Cond6T)  (x T y) T z [?]
x T (y k z)  (x + y) C c B d , 0 [?]
x C c B d , 0 + y C c B d , 0  (Cond7T)  (x C c B d , 0) T y [?]
(x T y) C c B d , 0  (Cond8T)  (x C c B d , 0) | y [?]
(x | y) C c B d , 0  p C eq(d, e) B d , 0 [?]
p[d := e] C eq(d, e) B d , 0 X x[?
]x  (PET)  x | (y T z) [?]
(x | y) T z  xTd [?
]x*d  x | d [?]
[?
]U (x)  r[?]
e:D  X d:D  X  (r[e := d])  X  p+p  X X d:D  X d:D  X  X  X  d:D  (p * x) [?]
(  X  q  (SUM4) (SUM5)  d:D  (p T x) [?]
( (p | x) [?]
(  (SCT2) (AT1) (AT2) (ATA10 )  (x + y) , t [?]
x , t + y , t  (ATA2)  (x * y) , t [?]
x , t * y X X ( p) , t [?]
p,t  (ATA3)  d:D  (ATA4)  d:D  (SUM6)  (x C c B d , 0) , t [?]
x , t C c B d , 0  (ATA50 )  p) | x  (SUM7)  t[?]x[?]
tx[?]t[?
]x+d,t X x , u C t <= u B d ,0  (ATD0)  X  (SUM8)  xb[?
]x  X d:D  X  (x , t T y) C u <= t B d , 0  x , t , u [?]
x , t C t = u B d , 0 + [?
]U (x) , min(t,u)  d:D  p) * x  x,t  b,t*y [?
]b,t*ty  (SUM3) p+  X  (SC5) (SCD1)  t:Time  d:D  (p + q) [?]
d:D  d:D  (SUM2)  d:D  p[?]
d:D  X  x[?]
(SC1) (SC4)  (SCT1)  (x , t T u [?]
y) C u <= t B d , 0 [?]
d:D  X  (CF2)  (SCDT2)  x , t T y [?]
(x T y) , t  (SUM1)  (CF1)  (SC3)  (x | y) | z [?]
x | (y | z)  (Cond9T) (ScaT)  X  (CM8)  (x | y) , t [?]
x | y , t - -0 - -0 - - - - - 0 0 - a( d ) | a ( d ) [?]
g(a, a )( d ) C d = d B d  (x C c1 B d , 0) + (x C c2 B d , 0) [?]
x C c1 [?]
c2 B d , 0 (Cond5T)  (x C c B d , 0) * (y C c B d , 0)[?
](x * y) C c B d , 0  (CM7) (ATA7)  - -0 - - 0 a( d ) | a ( d ) [?]
d  (Cond4T)  (CM4)  (x | y) , t [?]
x , t | y  if g(a, a ) is defined  (x C c1 B d , 0) C c2 B d , 0 [?]
(x C c1 [?]
c2 B d , 0)  (CM3T) (CM5)  (b * x) | (b * y) [?]
(b | b ) * (x k y)  (A7)  xCf By [?
]y  0  (b * x) | b [?]
(b | b ) * x  (CM1) (CM2T)  p) T x  d:D  ([?
]H (p)) [?]
[?
]H (  (ATB0)  u:Time  p)  d:D  (p C c B d , 0) [?]
(  d:D  X  p) C c B d , 0  (SUM12T)  d:D  (ATC10 )  x  (y + z) [?]
x  y + x  z  (ATC2)  x  (y * z) [?]
x  y X X x p[?]
xp  (ATC3)  - - [?
]H (b) [?]
b if b = a( d ) and a [?]
/H  (D1)  [?
]H (b) [?]
d otherwise  (D2)  x  (y C c B d , 0) [?]
(x  y) C c B d , 0+x , 0  [?
]H (x + y) [?]
[?
]H (x) + [?
]H (y)  (D3)  [?
]H (x * y) [?]
[?
]H (x) * [?
]H (y)  (D4)  x  (y T z) [?]
x  (y  z)  [?
]H (x C c B d , 0) [?]
[?
]H (x) C c B d , 0 [?
]H (x , t) [?]
[?
]H (x) , t  d:D  x  (y | z) [?]
x  (y  z)  (D5T) (D7)  x  ([?
]H (y)) [?]
x  y X (x  y) , u C u <= t B d , 0  x  (y , t) [?]
u:Time  12  (ATC4)  d:D  (ATC50 ) (ATC6) (ATC7) (ATC8) (ATC11)
Department of  Computer Science  Research Report No.
RR-01-02  ISSN 1470-5559  The Event Calculus Assessed Sergio Brandano  March 2001  The Event Calculus Assessed Sergio Brandano Department of Computer Science Queen Mary College University of London sb@dcs.qmw.ac.uk  Abstract  implements temporal inertia.
In this paper, the range of apclass in the plicability of FEC is proven to be the K Features and Fluents taxonomy.
The proof is given with respect to the original definition of this preference logic, where no adjustments of the language or reasoning method formally captures all of the were necessary.
As K above characteristics, this assessment result implies that the claims on the expressiveness and problem-solving power of FEC were indeed correct.
The general meaning of this assessment result is that the assessed logic is guaranteed, or certified to be correctly applicable to all reasoning problems in the class, i.e.
the logic always gives the correct, intended set of conclusions when applied to any reasoning problem in that class.
As the Full Event Calculus is the first of a family of other similar definitions, also involving important implementation issues, this assessment result discloses knowledge on how to certify the expressiveness and problem-solving power of these logics.
Assuming the given implementation is correct, the final user would then be guaranteed on its fitness for a particular purpose 1 , unlike all other products of similar nature.
Finally, a word on the Frame Problem.
K ad, obtained by remits an important sub-class, K stricting K to the case of purely deterministic actions.
In 1986 [4, 5] Hanks and McDermott pointed out that none of the reasoning methods developed so far, including predicate circumscription, were correctly addressing the Frame Problem.
They used the Yale Shooting Problem as a diagnostic example.
In 1994 [14, page 168] Sandeclass wall classified this problem, for which the K resulted to be the smallest class including a correct solution for it.
As FEC is correctly applicable to K - , and K  K - , then FEC implements a provably correct solution to the Hanks-McDermott problem.
sp IA  The range of applicability of the Full Event Calculus is proven to be the K class in the Features and Fluents taxonomy.
The proof is given with respect to the original definition of this preference logic, where no adjustments of the language or reasoning method were necessary.
The result implies that the claims on the expressiveness and problem-solving power of this logic were indeed correct.
sp IA  sp IA  1 Introduction We consider two well established approaches to Nonmonotonic temporal Reasoning about Actions and Change: the Event Calculus approach by Shanahan [16] and the Features and Fluents approach by Sandewall [14, 15].
It turns out that, although the design of suitable preference logics is a common task to both approaches, Sandewallas approach emphasises the systematic classification of these logics, via formally proven assessments of their range of applicability, while Shanahanas approach does not use any similar methodology.
The aim of this paper is to extend the benefit of Sandewallas systematic methodology to Shanahanas approach.
As a case study, we show that the most useful among all definitions of the Event Calculus, the Full Event Calculus (FEC), is a preference logic to which Sandewallas systematic methodology applies.
Shanahan originally proposed FEC as suitable, i.e.
adequate in expressiveness and problem-solving power, for correctly solving a number of NRAC reasoning problems with the following characteristics.
The information about actions is accurately and completely specified, actions succeed only if their preconditions are satisfied, successful actions may have a nondeterministic effect, state variables are truth-valued, the initial state of the world is accurately and completely specified, and there is no information at any later state than the initial one.
The time structure consists in the set of natural numbers with their standard order relation.
The reasoning  sp IA  sp IAd  sp IA  sp IAd  sp IA  sp IAd sp IA  1 Any software licence agreement includes the following clause: THE SOFTWARE IS PROVIDED AS-IS WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, ANY IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.
1  2 Preliminaries We assume the reader familiar with the Features and Fluents systematic methodology.
Readers with no preliminary knowledge in the topic are invited to consult [1, 15, 14].
Any concept not explicitly defined in this paper refers to [1].
The research task in this paper is precisely described as follows, with some preliminaries.
Definition 2.1 (Preference Logic) [20, pages 73-77] Let L be a standard logic, i.e.
a logic with the usual compositional model-theoretic semantics.
   Let  be a strict partial order on interpretations for L. Intuitively, I1  I2 means that the interpretation I2 is preferred over the interpretation I1 .
L and  define a new logic L .
We call such logics preference logics.
;  be in L.  preferentially entails  , written    , if for any M , if M   then M   or, Let  equivalently, if the models (preferred and otherwise) for  are a superset of the preferred models for .
 L is monotonic if for all ; ;  2 L, if    then also  ^    .
fi Definition 2.2 (Range of Applicability) [1, definition 2.9] Let L be a preference logic, let  be a scenario description and let  be the mapping defined in terms of  that selects those members of the classical model set [ ]] which are minimal according to , so that the maximally preferred models are the selected ones.
We say that aL is correct for a iff the preferred model set for  and the intended model set for  are identical, i.e.
iff 2  ([[]]) =  ().
We call arange of applicability of L a the class of all  such that L is correct for .
We call aclassification of L a the formally proven assessment of the range of applicability of L .
fi Within model-theoretic AI, Shohamas 1986 [19] notion of model preference is a generalisation [20, pages 83-85] of McCarthyas 1980 [10] predicate circumscription, which in turn is a generalisation [12] of Clarkas 1978 [2, 3] predicate completion.
Shanahanas Full Event Calculus is a preference logic; in fact, as summarised in definition 3.1, it uses classical first-order logic as base logic and predicate circumscription as model-preference criterion.
As Shanahanas Full Event Calculus is a preference logic, the research task in this paper then consists in formally assessing its range of applicability.
However, it is required by definition 2.2 that  and  use the same language for .
As meeting this requirement is not possible in the present note that  and  are defined in terms of , hence they speak the same language.
2 Please  case, we extend the notion of correctness by redefining it in terms of an immersion operator.
We then say that aL is correct for a iff  ([[T ()]]) =  (), where  is written in the underlying language and T () is the translation of  in the language of L .
If T is the identity operator, then T ()   and the previous definition of correctness applies.
The following is the underlying language for .
Definition 2.3 (Underlying Language) [1, section 3.1.3] Let T be the time-point domain [1, section 3.1.1], F the set of all feature symbols, V the domain of all feature values, and E the set of all action symbols.
Let hH; vi be the lattice whose elements, called observations, are members of H = T  F  2O  V and the order relation v applies as follows: ht1 ; f1 ; f: : :g; v1 i v ht2 ; f2 ; f: : :g; v2 i iff t1 v t2 .
The tuple ht; f; f: : :g; unknowni is an abbreviation for i ht; f; f: : :g; vi i, varying i over all possible tuples ht; f; f: : :g; vi i in H. Let hD; vi be the lattice whose elements, called rigid occurrences of actions, are members of D = TTE and the order relation v applies as follows: hs1 ; t1 ; A1 i v hs2 ; t2 ; A2 i iff s1 v s2 .
The order relation @ is an abbreviation for v ^ 6=.
The relation hs1 ; t1 ; A1 i = hs2 ; t2 ; A2 i simply means that A1 and A2 start at the same time-point, while hs1 ; t1 ; A1 i @ hs2 ; t2 ; A2 i means that A1 starts earlier than A2 .
Let  be a scenario description.
W      The OBS part of  is a sub-lattice of hH ; vi, whose elements are members of H = T F 2O V  H, where F is the set of all features explicitly occurring in .
The SCD part of  is a sub-lattice of hD; vi.
Each tuple in SCD specifies the starting time, the ending time and the action symbol of an action scheduled for execution.
The function V : D !
2H maps each scheduleas occurrence in a set of non-empty lattices of observations.
The function V is parametric on the action type, and the LAW part of  consists in the definition of V as a set of action-laws in Full Trajectory Normal Form, one law for each action type.
The Full Trajectory Normal Form for the action-laws is a mapping hs; t; Ai V ni=1 mj=1 Sij for which the action occurrence hs; t; Ai is expanded into a formula in Full Disjunctive Normal Form, that is into a disjunction of conjunctions of trajectory formulas Sij , each of which corresponds to the feature fj in the alternative i.
A trajectory formula for a given feature fj in F is the firstorder formula 8  2 [s; t]  T : [ ]fj =  'j ( ) where 'j is a partial fluent defined over D  [s; t]  T , and s 6= t. fi  W V  The underlying language is very expressive.
The assessment will reveal how much of that expressivity the specific logic is capable of using.
3 Definition  4 Classification  The following definition first appeared in [17, section 3] then in [18, page 209].
The definition extends [16, chapter 16] and [17, section 1] to the case of actions with duration, and derives from Kowalskias 1992 [6] simplification of the 1986 [7] Kowalski and Sergot original Event Calculus.
We shall now proceed to the assessment of the range of applicability of this logic.
Are the underlying semantics and the logicas semantics equivalent?
Is the intended model set for  equal to the set of logical consequences EC (T ())?
Let the relation ht; f; v i 2  () be a shorthand for aexists an interpretation hM; H i such that hB ; M; H; P ; Ci 2 Mod() and H (t; f ) = va, according to the known definition of intended model set.
Let the relation ht; f; truei 2 EC (T ()) be a shorthand for  ^  HoldsAt(f; t), and the relation ht; f; falsei 2 EC (T ()) be a shorthand for  ^  :HoldsAt(f; t), where (1)  is the conjunction of axioms A1 : : : A7 (def.
3.1), (2) is the conjunction CIRC [S1 ; Initiates; T erminates; Releases] ^ CIRC [S2 ; Happens] ^ S3 (def.
3.1), and (3) all formulae in S1 and S2 are in T () (definition 4.1).
Definition 3.1 (Full Event Calculus) The calculus uses classical first-order logic as base logic, augmented with the formulas in table 1 and axioms in table 2 for representing the specific problem domain of interest and for controlling deduction, and uses McCarthyas 1986 [11] predicate circumscription 3 with forced separation as modelpreference criterion.
The language of the calculus is defined in table 1.
Let S1 be a conjunction of Initiates, T erminates and Releases formulae, let S2 be a conjunction of InitiallyP , InitiallyN , Happens and temporal ordering formulae, and let S3 be a conjunction of Uniqueness of Names Axioms for actions and fluents.
The set of logical consequences of the calculus are defined as being the set of logical consequences of  ^ , according to the classical, Tarskian definition of logical consequence, written f :  ^  g, where  is the conjunction of axioms A1 : : : A7 in table 2, is the conjunction CIRC [S1 ; Initiates; T erminates; Releases] ^ CIRC [S2 ; Happens] ^ S3 where CIRC is the circumscription of the given predicates, and  is either a positive or negative HoldsAt formula.
The minimisation of Happens corresponds to the default assumption that there are no unexpected event occurrences.
The minimisation of Initiates, T erminates and Releases corresponds to the default assumption that actions have no unexpected effects.
fi As the essence of the Frame Problem is how do we use logic to represent the effects of actions without having to explicitly represent all their non-effects, the above method is a solution to the Frame Problem.
The conceptual basis of the above model-preference criterion is the partitioning of the set of premises and the application of different selection functions to the classical model set of the resulting and distinct sets of premises.
The set of selected models is then chosen by filter preferential entailment, using predicate circumscription as selection function.
The filtering technique was first described by Sandewall in 1989 [13], and occurs within the Event Calculus literature as the principle of forced separation [16, chapter 16 and page 81].
3 The generalisation of the 1980 [10] definition, allowing predicates, functions and constants to vary, and allowing many predicates to be minimised in parallel.
Definition 4.1 (Immersion Operator) Let L1 be the underlying language (definition 2.3), and let L2 be the language of the logic (definition 3.1).
The immersion operator T : L1 !
L2 is defined as follows:   T (h0; f; truei) = InitiallyP (f ) and T (h0; f; falsei) = InitiallyN (f );  T (hs; t; Ai) = Happens(A; s; t);  T (hs; t; Ai V Wni=1 Vmj=1 Sij ) is translated into a set  of formulas, one Initiates(A; f; s) formula for any fluent f becoming true as the effect of a deterministic action A, one T erminates(A; f; s) formula for any fluent f becoming false as the effect of a deterministic action A, one Releases(A; f; s) formula for any fluent f becoming randomised (true or false) as the effect of a non-deterministic action A, one HoldsAt(f; s) formula for any positive precondition (hs; f; truei) to the successful execution of the action A, and one :HoldsAt(f; s) formula for any negative precondition (hs; f; falsei) to the successful execution of the action A. Preconditions are explicit conditions for the truth of Initiates, T erminates and Releases formulae.
fi  The following two propositions by Lifschitz [8] are needed for the assessment.
We reproduce them as in Shanahan [16, page 280].
(x); ] is equivProposition 4.1 CIRC [ ^ 8 x:(x) alent to  ^ 8 x:(x) $ (x) if  and (x) do not mention the predicate .
Proposition 4.2 [8, page 341, proposition 7.1.1] Let  be the tuple of predicate symbols 1 ; : : : ; n .
If all occurrences  in  of the predicate symbols in  are positive 4 , then  CIRC [; ] = CIRC [; 1 ] ^ : : : ^ CIRC [; n ]  Theorem 4.1 (assessment) For all  2 K ht; f; vi 2 H , the following relation holds: EC (T ()) , ht; f; vi 2 K - ().
sp IA  sp-IA  and  ht; f; vi 2  P ROOF.
The following standard reduction applies.
By proposition 4.2, the second-order formula CIRC [S1 ; Initiates; T erminates; Releases] reduces to the second-order formula CIRC [S1 ; Initiates] ^ CIRC [S1 ; T erminates] ^ CIRC [S1 ; Releases].
By proposition 4.1 each CIRC minimisation, including CIRC [S2 ; Happens], reduces to first-order predicate completion.
In what follows, this reduction is used at each EC-evaluation, and the reference to an EC-axiom involves the application of the Uniqueness of Names Axioms in S3 .
The proof is by induction.
(a) If all preconditions for the action E are successfully met (i.e.
all HoldsAt and :HoldsAt test conditions for Initiates, T erminates and Releases clauses are met by axioms A3 and A6), or no precondition exists at all (in which case the above tests are trivially met), then action E is successfully executed.
Only one of the following three situations may then occur.
t    1.
The ego-world game starts at time  = 0.
The initial state of the world is represented by means of tuples h0; f; truei or h0; f; falsei in the OBS part of .
This results either in HoldsAt(f; t) 2 EC (T ()) by axiom A1, or in :HoldsAt(f; t) 2 EC (T ()) by axiom A4.
  2.
The world player persists until the ego player communicates its intention to perform an action, so that no tuples occur in SCD whose starting time is the present time  .
This trivially results in temporal inertia, by either axiom A1 or A4 depending on how f was initialised, or by axiom A2 or A5 depending on how was it last modified.
3.
The ego player, suddenly, adds the tuple h; E i to the current-action set C , where  is the point in time where this update occurs.
Then the world player executes the action and terminates it at  0 by removing the tuple h; E i from C and adding the tuple h;  0 ; E i to the past-action set P .
The ego may also decide to terminate E earlier, let say at  00 2 (;  0 ), so that it may autonomously remove the tuple h; E i from C and add h;  00 ; E i to P .
Let show what are the corresponding logical consequences of EC, pointwise.
By definition 4.1, we know it exists a single formula Happens(E; ;  0) (or Happens(E; ;  00 )) to refer to.
If the feature f does not belong to the set of those features which would be modified by a successful ex= Infl(E; t)), then the feature ecution of E (i.e.
f 2 is neither Clipped nor Declipped, and the situation described at point 2 then occurs up to  0 (or  00 ).
Otherwise, occurrence of a predicate symbol in a formula  is positive if it is in the scope of an even number of negations in the equivalent formula that is obtained by eliminating the connectives and from .
4 An  !
$  =  : then is either InitiallyP (f ) by T (), :Clipped(0; f; t) by axiom A3 and HoldsAt(f; t) 2 EC (T ()) by axiom A1, or InitiallyN (f ) by T (), :Declipped(0; f; t) by axiom A6 and :HoldsAt(f; t) 2 EC (T ()) by axiom A4.
< t <  0 : then is either Declipped(; f;  0) (if Initiates(a; f;  ) _ Releases(E; f;  )), or Clipped(; f;  0 ) (if T erminates(a; f;  ) _ Releases(E; f;  )), so that it is neither HoldsAt(f; t) 2 EC (T ()) by axiom A2, nor is :HoldsAt(f; t) 2 EC (T ()) by axiom A5 respectively, i.e.
inertia is not assumed in (;  0 ) (occlusion).
t =  0 : then is either (1) Initiates(a; f;  ) by T (), then is HoldsAt(f;  0 ) by axiom A2, (2) T erminates(a; f;  ) by T (), then is :HoldsAt(f;  0 ) by axiom A5, or (3) Releases(a; f;  ) by T (), then is both Declipped(; f;  0 ) and Clipped(; f;  0 ), so that it is neither HoldsAt(f; t) 2 EC (T ()) by axiom A2, nor is :HoldsAt(f; t) 2 EC (T ()) by axiom A5, i.e.
inertia is not assumed after  0 (nondeterminism).
The case for  00 in place of  0 is identical.
(b) If there is at least one precondition which is not met, then the action is executed without any effect, and the situation described at point 2 occurs up to  0 (or  00 ).
4.
The ego-world game ranges to infinity, where the intended-model set is defined.
Due to the choice of assumptions, the situations described at point 2 and 3 repeat themselves to the infinity, for both semantics, the semantics mirroring the underlying semantics.
fi Corollary 4.1 For all  2 K P ROOF.
[ ]].
fi  sp-IA, is EC (T ())  [ ]].
EC (T ()) = Ksp-IA ()   K-IA ()   The use of this preference logic for solving the HanksMcDermott [4, 5] problem and the Russian Shooting Problem is explained in [17, 16].
Theorem 4.1 gives a more general insight into how this is done, and guarantees that the reasoning method indeed gives the correct answers for these specific reasoning problems, as well as for all other class.
problems in the K -  sp IA  5 Conclusion In this paper, the range of applicability of Shanahanas Circumscriptive Full Event Calculus is proven to be the K - class in the Features and Fluents taxonomy.
The assessment is proven by referring to the original definition of this preference logic, where no adjustments of the language or reasoning method were necessary.
The result implies that the claims on the expressiveness and problemsolving power of this logic were indeed correct.
class is that subclass of Kwhere acThe K curate and complete information about actions (K), complete knowledge about the initial state of the world ( ) and no information at any later state than the initial one ( ), together with strict inertia in integer time ( ) of possibly non-deterministic actions ( ), are the assumed characteristics.
Time-points are natural numbers, and features are truth-valued ( ).
The extension of the Full Event Calculus so to encompass the full Kclass, which is the broadest class defined in [14], involves allowing backward (abductive) reasoning.
This extension is already available, it is called Abductive Event Calculus [18] [16, chapter 17], and its range of applicability is currently being investigated.
sp IA  sp IA  IA  s  A  I  I  p  IA  Acknowledgements We are grateful to Murray Shanahan for the helpful clarifications.
References [1] S. Brandano.
On the meta-theoretic approach to nonmonotonic reasoning, its extension to the continuum case and relation with classical Newtonian Mechanics.
LinkoEping Electronic Articles in Computer and Information Science, 5(42), 2000. http://www.ep.liu.se/ea/cis/2000/042/.
[2] K. L. Clark.
Negation as Failure.
In H. Gallaire and J. Minker, editors, Logic and Data Bases, Proceedings of the Workshop, pages 293a322.
Plenum Press, New York, 1978.
[3] K. L. Clark.
Predicate Logic as a Computational Formalism.
PhD thesis, University of London, Queen Mary College, Department of Computer Science, 1980.
Partially published as [2].
[4] S. Hanks and D. McDermott.
Default Reasoning, Nonmonotonic Logics, and the Frame Problem.
In Artificial Intelligence, Proceedings of the National (USA) Conference, pages 328a333, 1986.
Best Paper Award.
[5] S. Hanks and D. McDermott.
Non-monotonic Logic and Temporal Projection.
Artificial Intelligence, 33:379a412, 1987.
[6] R. A. Kowalski.
Database Updates in the Event Calculus.
Journal of Logic Programming, 12:121a146, 1992.
[7] R. A. Kowalski and M. Sergot.
A Logic-based Calculus of Events.
New Generation Computing, 4(1):67a95, 1986.
[8] V. Lifschitz.
Circumscription.
In D. M. Gabbay, C. J. Hogger, and J.
A. Robinson, editors, Handbook of Logic in Artificial Intelligence and Logic Programming, volume 3, chapter 6.
Oxford University Press, 1994.
[9] V. Lifschitz, editor.
Formalizing Common Sense: Papers by John McCarthy.
Intellect, Exeter, England, 1998.
[10] J. McCarthy.
Circumscription a A form of Non-Monotonic Reasoning.
Artificial Intelligence, 13(1-2):27a39, 171a172, 1980.
Reprinted in [9].
[11] J. McCarthy.
Applications of Circumscription to formalising Common Sense Knowledge.
Artificial Intelligence, 28(1):89a116, 1986.
Reprinted in [9].
[12] R. Reiter.
Circumscription implies Predicate Completion (sometimes).
In Artificial Intelligence, Proceedings of the National (USA) Conference, pages 418a420, 1982.
[13] E. Sandewall.
Filter Preferential Entailment for the logic of action in almost continuous worlds.
In Artificial Intelligence, Proceedings of the International Joint Conference (IJCAI), pages 894a899, 1989.
[14] E. Sandewall.
Features and Fluents: The Representation of Knowledge about Dynamical Systems, volume 1.
Oxford University Press, 1994.
[15] E. Sandewall and Y. Shoham.
Non-monotonic Temporal Reasoning.
In D. M. Gabbay, C. J. Hogger, and J.
A. Robinson, editors, Handbook of Logic in Artificial Intelligence and Logic Programming, volume 4, chapter 7.
Oxford University Press, 1994.
[16] M. P. Shanahan.
Solving the Frame Problem: A Mathematical Investigation of the Common Sense Law of Inertia.
MIT Press, 1997.
[17] M. P. Shanahan.
The Event Calculus Explained.
In M. Wooldridge and M. Veloso, editors, Artificial Intelligence Today: Recent Trends and Developments, volume 1600 of Lecture Notes in Artificial Intelligence, pages 409a 430.
Springer-Verlag, 1999.
[18] M. P. Shanahan.
An Abductive Event Calculus Planner.
Journal of Logic Programming, 44(1-3):207a239, 2000.
[19] Y. Shoham.
Reasoning about Change: Time and Causation from the Standpoint of Artificial Intelligence.
PhD thesis, Department of Computer Science, Yale University, 1986.
Published as [20].
[20] Y. Shoham.
Reasoning about Change: Time and Causation from the Standpoint of Artificial Intelligence.
MIT Press, second edition, 1988.
Table 1.
The Language of the Event Calculus Formula  Meaning What is true when (OBS):  InitiallyP (f ) InitiallyN (f )  Fluent f holds from time 0 Fluent f does not hold from time 0 What happens when (SCD):  Happens(a; t1; t2)  Action a starts at time t1 and ends at time t2 What actions do (LAW):  Initiates(a; f; t) T erminates(a; f; t) Releases(a; f; t)  Fluent f starts to hold after action a at time t Fluent f ceases to hold after action a at time t Fluent f is not subject to inertia after action a at time t Temporal Constraints:  t1 < t2, t1  t2 HoldsAt(f; t) Clipped(t1; f; t2) Declipped(t1; f; t2)  standard order relations between natural numbers Logical Machinery: Fluent f holds at time t Fluent f is terminated between times t1 and t2 Fluent f is initiated between times t1 and t2  The intuition behind Initiates(A; f; s), T erminates(A; f; s) and Releases(A; f; s) formulae is that the effect of the action A, starting at time s and ending at time t, is exerted on the fluent f at time t only.
Note.
Table 2.
The Axioms of the Event Calculus  HoldsAt(f; t) HoldsAt(f; t) Clipped(t1; f; t4)  :HoldsAt(f; t) :HoldsAt(f; t) Declipped(t1; f; t4) Happens(a; t1; t2)  InitiallyP (f ) ^ :Clipped(0; f; t) t2 < t ^ Happens(a; t1; t2) ^ Initiates(a; f; t1)^ :Clipped(t1; f; t) !9 a; t2; t3 [ t1 < t3 ^ t2 < t4 ^ Happens(a; t2; t3)^ [T erminates(a; f; t2) _ Releases(a; f; t2)]] InitiallyN (f ) ^ :Declipped(0; f; t) t2 < t ^ Happens(a; t1; t2) ^ T erminates(a; f; t1)^ :Declipped(t1; f; t) !9 a; t2; t3 [ t1 < t3 ^ t2 < t4 ^ Happens(a; t2; t3)^ [Initiates(a; f; t2) _ Releases(a; f; t2)]] !t1  t2  (A1) (A2)  (A3)  (A4) (A5)  (A6)  (A7)

Temporal XML?
SQL Strikes Back!
Fusheng Wang  Carlo Zaniolo and Xin Zhou  Siemens Corporate Research 755 College Road East, Princeton, NJ 08540 fusheng.wang@siemens.com  Computer Science Department University of California, Los Angeles {zaniolo, xinzhou}@cs.ucla.edu  Abstract While the introduction of temporal extensions into database standards has proven difficult to achieve, the newly introduced SQL:2003 and XML/XQuery standards have actually enhanced our ability to support temporal applications in commercial database systems.
We illustrate this point by discussing three approaches that use temporally grouped representations.
We first compare the approaches at the logical level using a common set of queries; then we turn to the physical level and discuss our ArchIS system that supports the three different approaches efficiently in one unified physical implementation.
We conclude that the approaches of managing transaction-time information using XML and SQL can be integrated and supported efficiently within the current standards, and claim that the proposed approach can be extended to valid-time and bitemporal databases.
1.
Introduction In this paper, we seek to support historical information management and temporal queries without extending current standards.
Our insistence on using only current standards is inspired by the lessons learned from the very history of temporal databases, where past proposals failed to gain much acceptance in the commercial arena, in spite of great depth, breadth [1, 2] and technical elegance [3, 4].
An in-depth review of the technical (and often non-technical) reasons that doomed temporal extensions proposed in the past would provide an opportunity for a very interesting and possibly emotional discussion; but such a discussion is outside the scope of this paper.
Here, we simply accept the fact that temporal extensions to existing standards are very difficult to sell, in spite of the growing pull by temporal applications; then, we move on from there by exploring solutions that do not require extending current standards.
This low-road approach is hardly as glamorous as the "new temporal standards" approach pursued in the past, but it is not without interesting research challenges and opportunities, as we will show in this paper.
In particular, new opportunities are offered by two recent developments that have taken information systems well beyond SQL:1992 which, in the past, supplied the frame of reference for temporal database research.
The first development is the introduction of  XML/XQuery standards, that have gained wide acceptance by all DBMS vendors, and the second is the introduction of SQL:2003 [5, 6, 7], which contains new advanced features such as nested relations and OLAP functions.
The benefits of XML in temporal information management include: i) XML can be used to represent data in a temporally grouped data model, ii) XQuery provides an extensible and Turing-complete language [8], where new temporal functions can be defined in the language itself.
These features make it possible to use XML to represent the history of relational databases by timestamping the grouped attribute histories of each table, and XQuery to express complex temporal queries [9, 10, 11, 12].
This approach requires no extension to current standards, and it is very general, insofar as it can be used to represent and query the transaction-time, valid-time and bitemporal history of databases[11], and arbitrary XML documents [13].
Therefore, contrasting this experience with the past one focusing on SQL, we might simply conclude that XML and its query languages are more supportive to historical information management and temporal queries than SQL.
However, there are many reasons for which we are not prepared to give up on SQL.
Indeed, relations represent a simple and intuitive data model which comes with (i) a built-in graphical representation in form of tables, (ii) WYSIWYG query languages such as QBE, and (iii) unique areas of commercial strength, such as OLAP applications and data warehousing.
By contrast, (i) there is no built-in graphical rendering for an XML document, and this must be provided by the user via stylesheets, (ii) WYSIWYG XML query languages require further research and development, and (iii) XQuery is more complex than SQL, and its commercial application areas are still emerging.
There is also the critical issue of performance.
In particular, in supporting transaction-time history of relational databases in XML [10, 12], we compared the two approaches of (i) implementing temporal queries directly in a native XML system, and (ii) recasting these views into historical tables, whereby the original XQuery statements are then mapped into equivalent SQL (or SQL/XML [14]) queries.
Our experiments show that the second approach tends to be significantly more efficient [10, 12].
Therefore, both logical and physical considerations point to the conclusion that SQL is to remain the database language of choice, for a long time to come-- particularly in data warehousing and business-intelligence applications-- and every effort should be made to assure efficient management for historical information and temporal queries in SQL:2003.
Toward this goal, we will take full advantage of the lessons learned in supporting temporal queries in XML and seek an efficient support for temporal queries independent of whether they are expressed in SQL or XQuery.
The paper is organized as follows.
After the discussion of related work, in Section 3, we study the problem of representing relational database history in XML, along the lines proposed in [9, 12].
In the core of the paper, we seek to apply the lessons learned on XML to SQL:2003.
Thus, in Section 4, we try to use the nested relations constructs provided by SQL:2003 to represent temporally grouped representations of database table histories.
We use the same basic temporal queries to compare this approach to the XMLbased approach and the SQL:2003-based approach of Section 5, where we support a temporally grouped representation using an OLAP-inspired view based on null values and flat tables.
The representation used in Section 5 dovetails with data warehousing and business intelligence applications, and it is also capable of reconciling the event-based and state-based views of temporal databases.
Finally, we consider efficient implementation issues and, in Section 6, we describe the architecture of our ArchIS system that unifies the support for both SQL-based and XML-based temporal views and queries at the physical level.
2.
Related Work Time in XML.
Interesting research work has recently focused on the problem of representing historical information in XML.
Approaches to support temporal XML documents by extending XML or its query languages have been proposed in [15, 16, 17, 18, 19].
For instance in [15], valid time on the Web is supported by introducing a new <valid> tag.
The t XQuery language proposed in [19], extends XQuery with new constructs for temporal support.
An archiving technique for scientific data was presented in [20], but XML query language support is not provided.
Temporal Databases and Grouped Representations.
There is a large number of temporal data models and query languages, including those discussed in [3, 21]; thus the design space for the relational data model has been exhaustively explored [1].
A useful taxonomy was introduced by Clifford et al.
[22] who classified them into two main categories: temporally ungrouped and temporally grouped data models.
Clifford had also suggested that the latter representation has more expressive power and is more natural since  empno salary title deptno 1001 60000 Engineer d01 1001 70000 Engineer d01 1001 70000 Sr Engineer d02 1001 70000 Tech Leader d02  start 1995-01-01 1995-06-01 1995-10-01 1996-02-01  end 1995-05-31 1995-09-30 1996-01-31 1996-12-31  Table 1.
The table employee history it is history-oriented [22, 23].
The basic representation used in ungrouped data models is tuple timestamping.
As shown in Table 1, a new timestamped tuple is generated whenever there is a change in any of the attribute values.
The well-know problem with this approach is that coalescing is needed when some of the attributes are projected out [24].
Much research has focused on this problem, and the solutions proposed include the TSQL2 [3] approach, and the point-based temporal model [25].
Versioning of DBMS was discussed in [26], and techniques for accurate time-stamping of transactions were discussed in [27] and Immortal DB [28].
Recently Oracle implemented Flashback [29], which supports the rollback to old versions of tables in case of errors.
However, these systems do not provide much support for temporal queries.
SQL:2003.
SQL:2003 [5], the latest release of SQL standards, is similar to SQL:1999, but provides significant extensions from SQL:1992.
In particular, SQ:2003 O-R features include multiset, nested collection types (supported by both Oracle [7] and Informix [30]), and user-defined types.
Another major feature is SQL/XML [14], which defines how SQL can be used together with XML in a database, and is supported by major database vendors.
Publishing functions provided by SQL/XML can directly construct query results as XML documents or fragments.
3.
Viewing Database History in XML The use of XML to publish the history of database relations has been discussed in [10, 11, 31, 12], using a temporally grouped representation such as that of Figure 1 (which we call H-document) for the employee table as shown in Table 1.
Powerful temporal queries on such representations can be expressed using XQuery.
(In the remainder of this paper, our granularity for time is a day; however, all the techniques we present are equally valid for any granularity used by the application.
For finer granularity, techniques in [27, 28] can be used.
Furthermore, throughout this paper, we assume that relation keys remain invariant.)
3.1.
Temporal Queries with XQuery A full spectrum of queries was presented in [10] to illustrate the effectiveness of the approach--including temporal projection, temporal snapshot, temporal slicing, temporal join, and temporal aggregate.
Because of space limitations, we restrict ourselves to the following three examples that will be used throughout the paper.
<employees tstart="1995-01-01" tend="1996-12-31">  element empno {$e/empno/text()}, element salary{$s/text()}, $ol  <employee tstart="1995-01-01" tend="1996-12-31"> <empno tstart="1995-01-01" tend="1996-12-31">1001</empno> <salary tstart="1995-01-01" tend="1995-05-31">60000</salary> <salary tstart="1995-06-01" tend="1996-12-31">70000</salary> <title tstart="1995-01-01" tend="1995-09-30">Engineer</title> <title tstart="1995-10-01" tend="1996-01-31">Sr Engineer</title> <title tstart="1996-02-01" tend="1996-12-31">Tech Leader</title> <deptno tstart="1995-01-01" tend="1995-09-30">d01</deptno> <deptno tstart="1995-10-01" tend="1996-12-31">d02</deptno> </employee> </employees>  }  Here overlapperiod($a, $b) is a user-defined function that returns an element PERIOD with overlapped period as attributes(tstart, tend); if there is no overlap, then no element is returned which satisfies the XQuery built-in function empty().
3.2.
Discussion Figure 1.
H-document: XML-based Representation of Employees' History  Q UERY Q1.
Temporal Projection.
Retrieve the title history of employee "1001": element title_history{ for $t in doc("employees.xml")/employees/ employee[empno="1001"]/title return $t }  Observe that no coalescing is needed after this projection, since the history of titles is temporally grouped.
Q UERY Q2.
Temporal Snapshot.
Retrieve titles and salaries of all employees on 1994-05-06: for $e in doc("employees.xml")/employees/ employee let $t:=$e/title[ tstart(.)
<= date("1994-05-06") and tend(.)
>= date("1994-05-06") ] let $s:=$e/salary[ tstart(.)
<= date("1994-05-06") and tend(.)
>= date("1994-05-06") ] return <employee>{$e/empno,$t,$s}</employee>  In this query, we need to check only the timestamps of the leaf nodes, since the H-document has a temporal covering constraint, i.e., the interval of a parent node always covers that of its child nodes.
Here, date() is a built-in function of XQuery (for simplicity, we omit the namespace fn).
Instead, tstart() and tend() are user-defined functions to shield the user from the complexity of the underlying representation, since, e.g., 'now' [32] requires special representation and special handling (in ArchIS [10] we use the end-of-time to represent 'now').
date() is a built-in function of XQuery ( for simplicity, the namespace fn is omitted here).
Q UERY Q3.
Retrieve the salary history of employees in dept.
"d01", while they were in that department.
for $e in doc("employees.xml")/employees/ employee[deptno="d01"] for $s in $e/salary for $d in $e/deptno[.="d01"] let $ol:=overlapperiod($s, $d) where not (empty($ol)) return element salhistory{  The previous examples illustrate that XQuery is capable of expressing complex temporal queries, but the expression of these queries can be greatly simplified by a suitable library of temporal functions.
The ArchIS system [10], discussed in Section 6, supports a rich set of functions, including the simple scalar functions described above, and also complex functions, including temporal aggregates and coalesce functions.
A significant benefit offered by the XML/XQuery-based approach to temporal information management is that it is very general and can handle the history of arbitrary XML documents that have evolved through successive versions [13].
The approach can also be extended to valid-time databases and bitemporal databases [11].
On the other hand, the ease of use of XQuery is questionable, and the problem of displaying the results of temporal queries in user-friendly ways can be a real challenge, since the tagged representations, such as that of Figure 1, are not suitable for casual users.
To produce visually appealing representations, the query designer might have to code a stylesheet, using XSL [33]--possibly a different one for each query.
This problem is far from trivial, and the visual rendering of temporal information poses interesting research challenges.
Finally, the growing popularity of XML in web-oriented applications does not change the fact that SQL remains the cornerstone of database applications, and its importance in areas such as business intelligence and data warehouses is growing every day.
For these reasons, efficient support for temporal information and queries in SQL remains critical [?].
Therefore, we explore two approaches: one based on nested relations, which is discussed next, and another based on OLAP tables, which is discussed in Section 5.
4.
DB History and Nested Relations Nested relations are part of the latest SQL:2003 standards, and also supported by some commercial database vendors [7, 30].
Therefore a temporally grouped representation, similar to that used with XML, can also be achieved within SQL standard.
For instance, for our employee history example, we can use the following schema containing the nested table ('n-table' for short, or 'n-view' if it is a nested view) n employee:  CREATE TYPE salary_typ AS OBJECT( salary NUMBER(7), timep PERIOD ); ...
CREATE TYPE salary_tbl AS TABLE OF salary_typ; ...
CREATE TABLE n_employee( empno VARCHAR2(8), timep PERIOD, n_name name_tbl, n_salary salary_tbl, n_title title_tbl, n_deptno deptno_tbl) NESTED TABLE n_salary STORE AS n_salary, NESTED TABLE n_title STORE AS n_title, NESTED TABLE n_deptno STORE AS n_deptno;  This definition uses the user-defined type PERIOD, which can be defined in SQL:2003 as follows: CREATE TYPE PERIOD AS OBJECT( tstart DATE, tend DATE );  The same temporal queries that we have expressed on XML using XQuery can now be expressed on nested tables using SQL:2003, as follows: Q UERY Q1n.
History projection.
Retrieve the title history of employee "1001": SELECT FROM WHERE  t.* n_employee e, TABLE(e.n_title) AS t e.empno='1001'  Q UERY Q2n.
Temporal Snapshot.
Retrieve titles and salaries of all employees on 1994-05-06: SELECT t.title, s.salary FROM n_employee e, TABLE(e.n_title) AS t, TABLE(e.n_salary) AS s WHERE tstart(t.timep) <= '1994-05-06' AND tend(t.timep) >= '1994-05-06' AND tstart(s.timep) <= '1994-05-06' AND tend(s.timep) >= '1994-05-06'  Here too we use the functions tstart() and tend() to isolate the user for the internal representation of time, including 'now'.
(Support for user-defined scalar functions is now available in all commercial OR-DBMSs.)
Q UERY Q3n.
Retrieve the salary history of employees in dept.
"d01", while they were in that department.
SELECT e.empno, overlapperiod(d.timep, s.timep), s.salary FROM n_employee AS e, TABLE(e.n_dept) AS d, TABLE (e.n_salary) AS s WHERE d.deptno = 'd01' AND overlaps(d.timep, s.timep)  Here overlaps() is defined to return true if two periods overlap, and false otherwise; overlapperiod() is defined to return the overlapped PERIOD.
In addition to scalar functions, such as overlapperiod(), temporal aggregates (e.g., the temporal version of min and sum [3] ) will be required by temporal queries.
These new functions could be easily built into commercial systems by the vendors, or by the users, since commercial OR-DBMSs now support the introduction of new scalar and aggregate functions coded in a procedural language.
(In the ATLaS system [34], user-defined aggregates can also be introduced natively in SQL, with no recourse to external PLs.)
The new temporal aggregates that must be introduced include, the rising function of TSQL2 [3], and also the tcoalesce aggregate for temporal coalescing-- since the temporally grouped representation made possible by nested tables has greatly reduced the need for coalescing, but not eliminated it all together (and the same is true for XML).
Assuming that a library containing the basic temporal functions is available, the complexity of writing temporal queries in SQL:2003 and nested tables is about the same as writing them in XQuery and XML.
Both approaches present users with more alternatives in presenting data than flat relations.
For instance, the join of nested employees and departments tables can be represented by a one-level hierarchy where the department and employee attributes are at the same level, or as a hierarchy where employees are grouped inside departments, or vice-versa.
We next return to the 'Spartan simplicity' of flat relations, in which the alternatives are fewer and the problem is simplified.
5.
An OLAP-Inspired Representation A temporally grouped representations can also be obtained by using null values in flat tables such as those returned by OLAP aggregates.
Thus, the transaction-time history of employees, that was described by tuple timestamping in Table 1, and as an XML document in Figure 1, is now described as a flat table with null values as shown in Figure 2, where the null value is represented by the question mark, "?".
This representation can be defined by a ROLLUP operation on Table 1, defined by the following SQL statement (again here we use timep to represent the period of tstart and tend).
As in the case of OLAPs, we might also want to represent the null values generated by the rullup operation differently from those representing null values in the original table.
CREATE VIEW e_employee AS SELECT empno, tcoalesce(timep,salary,title,deptno) FROM employee_history GROUP BY GROUPING SETS(empno,(empno,salary), (empno,title),(empno,deptno) )  We refer to the representation shown in Figure 2 as an 'etable' (or 'e-view' if it is a view) because this captures the event-history for employees, as it will be discussed in Section 5.1.
Moreover, temporal queries on e-tables preserve  empno  tstart  tend  salary  title  deptno  1001  1995-01-01  1995-05-31  60000  ?
?
1001  1995-06-01  1996-12-31  70000  ?
?
1001  1995-01-01  1995-09-30  ?
Engineer  ?
1001  1995-10-01  1996-01-31  ?
Sr Engineer  ?
1001  1996-02-01  1996-12-31  ?
Tech Leader  ?
1001  1995-01-01  1995-09-30  ?
?
d01  1001  1995-10-01  1996-12-31  ?
?
d02  1001  1995-01-01  1996-12-31  ?
?
?
Figure 2.
The history view e employees the traditional style of SQL queries: Q UERY Q1e.
History projection.
Retrieve the title history of employee "1001": SELECT title, tstart, tend FROM e_employee WHERE empno= '1001' AND title IS NOT NULL  Q UERY Q2e.
Temporal Snapshot.
Retrieve titles and salaries of all employees on 1994-05-06: SELECT FROM WHERE AND AND OR  e.empno, e.title, e.salary e_employee AS e tstart(e.timep) <= '1994-05-06' tend(e.timep) >= '1994-05-06' e.title IS NOT NULL e.salary IS NOT NULL  This query assumes that we only want to retrieve the information, without reformatting it.
However, if we want to reformat the information derived into a join table, then we also want to join the titles and salaries of all employees at that date into a flat relation as follows: SELECT FROM WHERE AND AND AND AND AND AND  s.empno, t.title, s.salary e_employee AS s, e_employee AS t tstart(t.timep) <= '1994-05-06' tend(t.timep) >= '1994-05-06' tstart(s.timep) <= '1994-05-06' tend(s.timep) >= '1994-05-06' s.empno=t.empno t.title IS NOT NULL s.salary IS NOT NULL  Q UERY Q3e.
Retrieve the salary history of employees in dept.
"d01", while they were in that department: SELECT n1.empno, n1.salary, overlapperiod(n1.timep,n2.timep) FROM e_employee n1, e_employee n2 WHERE n1.empno = n2.empno AND n1.salary IS NOT NULL AND n2.deptno IS NOT NULL AND n2.deptno = "d01" AND overlaps(n1.timep, n2.timep)  This query illustrates the use of temporal joins, with intersection of overlapping periods; these are required for query Q3 in all three representations.
While the complexity of queries is similar for our three temporally-grouped  approaches, e-tables offer unique advantages that are discussed next.
5.1.
Event-Oriented Histories An advantage of this last representation is that grouping can be easily controlled by the ORDER BY clause in SQL.
For instance, the representation of Figure 2, where the history of each employee attribute is grouped together, is produced by the following clause: SELECT empno, timep, salary, title, deptno FROM e_employee ORDER BY empno, salary, title, deptno, tstart(timep)  Since the null value is assumed to be the last value in each domain, this ORDER BY clause indeed produces the table of Figure 2.
Assume now that we want to view the history of events, pertaining to employees' salaries and departments, that have occurred in the company; then we can just list them in ascending chronological order as follows: SELECT timep, empno, salary, deptno FROM e_employee WHERE title IS NULL ORDER BY tstart(timep),empno,salary,deptno  However, in order to visualize the salary history of employees in a given department, we need first to write a query similar to that of Example Q3e to derive a table (or a view) depthist(deptno, empno, salary, timep), on which we can write following query: SELECT deptno, timep, empno, salary FROM depthist ORDER BY deptno,tstart(timep),empno,salary  This last statement returns all the events grouped by department and arranged in chronological order.
The visual presentation of historical data and query results is much simpler using e-tables than using n-tables or H-tables (which is discussed later in Section 6.1).
This is because flat tables come with their built-in graphical representation, while, e.g., XML requires the user to write a style sheet to visualize data.
Moreover, as demonstrated by the previous examples, restructuring on e-tables can be realized by simply reordering the tuple using an ORDER BY clause, whereas it might require complex nesting and unnesting in the other representations.
In most temporal database approaches, including TSQL2 [3], a temporal relation can be either declared as a state table or as an event table but the two views are not easily combined.
A simple mapping between the two views is highly desirable since, in everyday life, states and events are two facets of the same evolving reality.
Moreover, many advanced applications, such as time-series analysis [35], sequence queries [36], and data stream queries [37], view the database as a sequence of events, rather than a sequence of states.
The e-tables just described, make it possible the unification of state-based and event-based representations by simply using SQL ORDER BY construct.
For instance, say that we want to find employees who have been transferred from a department to another, and from this, back to the old one.
To answer this query by perusing the history of employees, we would probably start by carefully viewing the results of the following query: Q UERY Q4.
Reordering to detect round-trip transitions between departments: SELECT empno, timep, depno, salary, title FROM e_employee ORDER BY empno, tstart(timep)  Then, the immediate sequence of any three tuples with non-null deptno column, would satisfy the query-- provided that the first department is equal to the third (and that there was no interruption in the employee's employment).
Although this query is conceptually simple, it requires the detection of three successive tuples--an operation that is rather complex and inefficient to express in standard SQL.
A first solution to this problem is to write a user-defined aggregate (UDA); in fact UDAs can easily express statebased computations [38].
Moreover, several event-patterns and sequence languages for time-series analysis have been proposed in the literature [39, 35, 36] and would work very nicely with the representation discussed here.
For instance, using SQL-TS [36] our query could be expressed as follows: Q UERY Q5.
From department A to B and back, with no other change in between: SELECT A.empno, A.title FROM e_employee [ORDER BY empno, tstart(timep)] AS (A,B,C) WHERE A.deptno = C.deptno AND B.deptno IS NOT NULL  internal level should be provided for these multiple external views.
At UCLA, we have been developing the ArchIS system that unifies the support for multiple external temporal models into one architecture [10, 12].
The basic architecture of ArchIS [10] is shown in Figure 3.
ArchIS is designed to preserve and archive the history of the database by preserving the evolution of its content, either by using active rules attached to the database or by periodically visiting their update logs.
ArchIS then supports alternative logical views of the database history described in the previous sections, by mapping queries against these views into equivalent queries against the history database.
In our previous work on the implementation of storing H-documents [10, 12], we have compared the use of a native XML DBMS such as the Tamino XML Server [40], against the approach of shredding these documents and storing them into RDBMSs.
The second approach was found to offer substantial performance advantages and will be used here.
(In our implementation, the 'current database' and the archived one are managed by the same system.
But the results are easily generalized to the situations where these two are separate and even remote.)
In the next sections we first discuss the structure of the Key & Attribute History Tables, used at the internal level and then we describe the problem of mapping external queries into internal ones.
We finally describe the temporal clustering and indexing techniques used in improving the performance of such queries.
The problem of supporting XML views through stored RDBMS tables is hardly new since it has recently provided a major focus for database research [41, 42, 43].
However, here we do not need to support all XML documents and queries, but only historical views of database tables and temporal queries on such tables; thus, specialized techniques can be used for more efficient storage, and optimized query mapping.
6.1.
History Tables  Here, the FROM clause specifies that, given the ordering described above, A, B and C are three successive tuples that are also related by the conditions specified in the WHERE clause.
Space limitation prevents us from delving into languages as SQL-TS [36], although they represent a very interesting and pertinent topic in temporal database research.
Here, it suffices to observe that these languages rely on tuples being arranged in a suitable order--which is easier to achieve with e-tables than with H-tables or n-tables.
with key empno.
The history of employee is preserved by following tables in ArchIS:  6.
Efficient Implementation  The Key Table:  In the previous sections, we have discussed the pros and cons of alternative representations for temporal history.
In reality, these are likely to be supported together, rather than as alternatives, since database vendors are gung ho on supporting both SQL and XML in their systems.
Practical considerations also suggest that a unified implementation at the  Since empno will not change along the history, the period (tstart,tend) in the key table also represents the valid period of the employee.
The use of keys is for easy joining of all attribute histories of an object such as an employee.
The history of each relation is preserved by a set of tables: one table for each attribute, and an additional table for the primary key of the original relation.
Each tuple in the tables is timestamped with the two attributes tstart and tend.
For example, consider our evolving DB relation employee(empno, salary, title, deptno)  employee empno(empno, tstart, tend)  Attribute History Tables:  Current Database  However, this is only the first step of the translation performed by ArchIS which also adds conditions to exploit the temporal clustering and indexing discussed later.
Relational data  SQL queries  Active rules/ update logs  Temporal data  A R  XML view  C  n-view  H  e-view  I H-tables  Temporal queries  S  Figure 3.
ArchIS: Archival Information System employee_salary(empno, salary, tstart,tend) employee_title (empno, title, tstart,tend) employee_deptno(empno, deptno, tstart,tend)  The values of empno in the above tables are the corresponding key values, thus indexes on such empno can efficiently join these relations.
When a new tuple is inserted, the tstart for the new tuple is set to the current timestamp, and tend is set to now.
When there is a delete on a current tuple, we simply change the tend value in that tuple as current timestamp.
An update can be viewed as a delete followed by an insert.
We will later refer to these as key & attribute history tables (Htables for short).
H-tables could also be viewed as yet another candidate representation at the logical level; we have not considered them here because they do not provide real query advantages with respect to e-tables, and they make tasks such as reordering and visualization harder.
In addition to these, we also store information about the schema in a global relation: relations(relationname, tstart, tend)  Our design builds on the assumption that keys (e.g., Otherwise, a system-generated surrogate key can be used.
empno) remain invariant in the history.
6.2.
Query Mapping Mapping from e-views to H-tables.
Mapping from Htables to the e-views (or e-tables) of Figure 2 is simple, since the latter can be obtained taking the union of the Htables after padding them with null values.
This simple correspondence simplifies the translation and optimization of queries expressed on e-tables into equivalent queries on Htables.
The pattern of null values associated with the query plays an important role in the translation.
Take for instance QUERY Q1e.
There, the condition that title IS NOT NULL implicitly determines that salary and department must be null, and attribute table employee title will appear in the WHERE condition of mapped query.
Thus our original query is translated into: SELECT T.title, T.tstart, T.tend FROM employee_title as T WHERE T.empno = '1001'  Mapping from n-views to H-tables.
In DBMS that support nested relations, n-views (or n-tables) can be supported directly at the physical level.
But even so, we might prefer to 'shred' and store them into flat H-tables, to simplify support for alternative external views (in particular, e-views), of for performance reasons, e.g., to take advantage of the clustering techniques available for H-tables, that will be discussed later.
A simple approach to achieve this is to define a nested object-view (as defined in SQL:2003) on H-tables, as follows: CREATE VIEW n_employee OF employee_t WITH OBJECT IDENTIFIER (empno) AS SELECT e.empno, PERIOD(e.tstart, e.tend) AS timep, CAST(MULTISET( SELECT s.salary, s.tstart, s.tend FROM employee_salary s WHERE s.empno = e.empno) AS salary_tbl) ) AS n_salary, ... FROM employee_empno e;  With such a mapping, temporal queries on n-views are automatically translated by the DBMS into queries on Htables through view definitions.
Mapping from XML-views to H-tables.
The mapping from XML-views (or H-documents) to H-tables is significantly more complex.
The problem of supporting XQuery on H-tables is similar in the sense that we have to generate efficient SQL queries, but more complex insofar as XML documents must be structured as output.
Therefore, we use SQL/XML [14], whereby the results of SQL queries can be efficiently assembled into XML documents for output.
Many database vendors now support efficient SQL/XML implementations, in which tag-binding and structure construction are done inside the relational engine for best performance [44].
In ArchIS [10], we compile XQuery statements on temporal XML-views, and optimize their translation into SQL/XML on the H-tables in five main steps, as follows: 1.
Identification of variable range.
For each distinct tuple variable in the original query, a distinct tuple variable is created in the FROM clause of the SQL/XML query, which refers to a certain key table or attribute table.
2.
Generation of join conditions.
There is a join condition T.empno and N.empno for any pair of distinct tuple variables.
3.
Generation of the WHERE conditions.
These are the conditions in WHERE clause of XQuery or specified in the XPath expressions.
4.
Translation of built-in functions.
The built-in functions (such as overlaps($a,$b)) are simply mapped into  the corresponding SQL built-ins we have implemented for ArchIS.
5.
Output generation.
This is achieved through the use of the XMLElement and XMLAgg constructs defined in SQL/XML [14].
Scheme Feature External Schema Temporal  Ungrouped  XML  Flat Tables  XML View  Ungrouped  SQL  Nested  OLAP  Relations  Tables  Nested  Null-filled  Tables  Flat Tables Flat  Grouped  Grouped  Grouped  XQuery  SQL:2003  SQL  Model  For instance, the SQL/XML translation of Query Q1 is: SELECT XMLElement (Name "title_history", XMLAgg (XMLElement (Name "title", XMLAttributes (T.tstart as "tstart", T.tend as "tend"), T.title))) FROM employee_title as T WHERE T.empno = '1001'  Query Language Temporal Coalescing Event  Offten ten Very O  Needed Needed Very No No  Needed  Seldom Seldom Needed No No  Needed  Seldom Seldom Needed No No  Needed  Seldom Seldom Needed Yes Yes  ALL  Many  Many  Some  Some  ALL  Support DBMS  ALL  ALL  Support  6.3.
Clustering and Indexing Efficient support for historical queries requires support for temporal clustering and indexing; in ArchIS, this is achieved by a simple usefulness-based scheme whereby the H-tables are partitioned into segments [10].
For each table, the usefulness of its current segment is defined as the percentage of the segment tuples that have not expired yet (i.e., whose tend timestamp is still 'now').
The usefulness of the current segment is monotonically decreasing with time, and as soon as it falls below a user-specified percentage, the whole segment is archived, and a new segment is started containing only those tuples whose timestamps are 'now'.
The segment number then can become part of the search keys supported by the indexes used in the database.
Thus, a request to find the salary of a given employee at certain time, could involve finding the corresponding segment in a small memory-resident index, and then using the (segment no, empno) pair in the index search.
This usefulness-based scheme achieves temporal clustering through redundancy.
Since there is no update in the archived tuples of a transaction-time database (unlike validtime databases), redundancy does not generate additional execution costs.
For reasonable usefulness values the extra storage costs are modest (e.g., 30% storage overhead for 33% usefulness [10]); this cost represents a minor drawback, because of the fast decreasing cost of storage, and the applicability of compression techniques which has been proven in [10].
(The cost of re-compressing after updates is not present for archived data, since these are not updated.)
On the other hand, the usefulness-based approach expedites archival search in a predictable and controllable fashion.
For instance, for usefulness of 33% (1/3) we are assured that, when searching in the corresponding segments for records with a given timestamp, at least one of the three records visited has the right timestamp.
Therefore, the time required to regenerate the past snapshot of a relation can be expected to be less than three times of that needed to generate the current snapshot from the current database [10].
Also, observe that the joining of H-tables require little extra time since they are already sorted on empno.
The architecture and performance of ArchIS is covered in [10].
Figure 4.
Temporal Scheme Comparison  6.4.
Summary Only the skeleton of ArchIS is currently operational, and many improvements are planned for the future; even so, its realization confirms the practicality of supporting both SQL-based and XML-based temporal views and queries with a unified and efficient internal representation.
ArchIS can now run on top of IBM DB2 and the ATLaS system [34].
We are currently working on extending it to run on commercial DBMS that support nested relations [7, 30], and explore any performance improvement that can be gained with this approach.
We also plan to experiment with additional storage structures, such as R-trees, to better support valid-time and bitemporal databases.
7.
Conclusion and Future Work An important conclusion emerges from the research presented in this paper: a unified multi-model support for transaction-time databases can be achieved effectively using a temporally grouped data model.
This requires the introduction of new temporal functions and aggregates, but no extension to the current standards.
A unified efficient implementation for the three external models relies on well-understood query mapping/optimization techniques, and temporal clustering/indexing techniques at the internal level.
In practice, the ArchIS approach is desirable since it provides a low-cost approach to address a wide range of applications.
In particular, XML-based views dovetail with web applications, while nested-relations are more natural for object-oriented applications, and the null-filled flat tables are best for traditional database applications, decisionsupport applications, and event-oriented queries.
This last approach provides a simple framework for the presentation of the data, which can require significantly more effort when XML is used.
Figure 4 summarizes the features of the temporally-grouped schemes proposed, comparing them to the basic ungrouped scheme.
While we have concentrated here on transaction-time databases, it was recently shown that, for XML, this ap-  proach can be extended to bitemporal representations and queries as well [11].
Support for valid-time and bitemporal views and queries using nested relations and nullfilled tables represents an important topic of forthcoming research.
Many research issues also remain open at the physical level, including the use of nested relations and of clustering schemes that support updates on historical data (such updates are not present in transaction-time databases).
Acknowledgments This work was partially supported by a gift of NCR Teradata.
The authors would also like yo thank the referees for many useful suggestions.
References [1] G. Ozsoyoglu and R.T. Snodgrass.
Temporal and Real-Time Databases: A Survey.
TKDE, 7(4):513-532, 1995.
[2] F. Grandi.
An Annotated Bibliography on Temporal and Evolution Aspects in the World Wide Web.
In TimeCenter Technique Report, 2003.
[3] R. T. Snodgrass.
The TSQL2 Temporal Query Language.
Kluwer, 1995.
[4] R. T. Snodgrass, M. H. Bohlen, C. S. Jensen, and A. Steiner.
Transitioning Temporal Support in TSQL2 to SQL3.
Lecture Notes in Computer Science, 1399:150-194, 1998.
[5] Database Languages SQL, ISO/IEC 9075-*:2003.
[6] A. Eisenberg, J. Melton, K. Kulkarni, J. Michels, and F. Zemke.
SQL:2003 has been published.
SIGMOD Rec., 33(1):119-126, 2004.
[7] SQL 2003 Standard Support in Oracle Database 10g, otn.oracle.com/products/database/ application development/pdf/SQL 2003 TWP.pdf.
[8] S. Kepser.
A Proof of the Turing-Completeness of XSLT and XQuery.
In Technical report SFB 441, Eberhard Karls Universitat Tubingen, 2002.
[9] F. Wang and C. Zaniolo.
An XML-Based Approach to Publishing and Querying the History of Databases.
To Appear in World Wide Web: Internet and Web Information Systems.
[10] F. Wang, X. Zhou, and C. Zaniolo.
Using XML to Build Efficient Transaction-Time Temporal Database Systems on Relational Databases.
Technical Report 81, TimeCenter, Mar.
2005.
[11] F. Wang and C. Zaniolo.
XBiT: An XML-based Bitemporal Data Model.
In ER, 2004.
[12] F. Wang and C. Zaniolo.
Publishing and Querying the Histories of Archived Relational Databases in XML.
In WISE, 2003.
[13] F. Wang and C. Zaniolo.
Temporal Queries in XML Document Archives and Web Warehouses.
In TIME, 2003.
[14] ISO.
Information technology - Database languages - SQL Part 14: XML-Related Specifications.
2003.
[15] F. Grandi and F. Mandreoli.
The Valid Web: An XML/XSL Infrastructure for Temporal Management of Web Documents.
In ADVIS, 2000.
[16] T. Amagasa, M. Yoshikawa, and S. Uemura.
A Data Model for Temporal XML Documents.
In DEXA, 2000.
[17] C.E.
Dyreson.
Observing Transaction-Time Semantics with TTXPath.
In WISE, 2001.
[18] S. Zhang and C. Dyreson.
Adding Valid Time to XPath.
In DNIS, 2002.
[19] D. Gao and R. T. Snodgrass.
Temporal Slicing in the Evaluation of XML Queries.
In VLDB, 2003.
[20] P. Buneman, S. Khanna, K. Tajima, and W. Tan.
Archiving scientific data.
TODS, 29(1):2-42, 2004.
[21] J. Chomicki, D. Toman, and M.H.
Bohlen.
Querying ATSQL Databases with Temporal Logic.
TODS, 26(2):145- 178, June 2001.
[22] J. Clifford, A. Croker, F. Grandi, and A. Tuzhilin.
On Temporal Grouping.
In Recent Advances in Temporal Databases, pages 194-213.
Springer Verlag, 1995.
[23] J. Clifford, A. Croker, and A. Tuzhilin.
On Completeness of Historical Relational Query Languages.
ACM Trans.
Database Syst., 19(1):64-116, 1994.
[24] C. Zaniolo, S. Ceri, C.Faloutsos, R.T. Snodgrass, V.S.
Subrahmanian, and R. Zicari.
Advanced Database Systems.
Morgan Kaufmann Publishers, 1997.
[25] D. Toman.
Point-based Temporal Extensions of SQL.
In DOOD, pages 103-121, 1997.
[26] M. Stonebraker.
The Design of the POSTGRES Storage System.
In VLDB, 1987.
[27] C. S. Jensen and D. B. Lomet.
Transaction Timestamping in Temporal Databases.
In VLDB, 2001.
[28] D. Lomet, R. Barga, M. F. Mokbel, G. Shegalov, R. Wang, and Y. Zhu.
Immortal DB: Transaction Time Support for SQL Server.
In SIGMOD, 2005.
[29] Oracle Flashback Technology.
http://otn.oracle.com/deploy/availability /htdocs/flashback overview.htm.
[30] Informix Universal Server.
http://www.ibm.com/informix.
[31] F. Wang, X. Zhou, and C. Zaniolo.
Temporal Information Management using XML.
In ER, 2004.
[32] J. Clifford, C.E.
Dyreson, T. Isakowitz, C.S.
Jensen, and R.T. Snodgrass.
On the Semantics of "Now" in Databases.
TODS, 22(2):171-214, 1997.
[33] The Extensible Stylesheet Language (XSL).
http://www.w3.org/Style/XSL/.
[34] ATLaS.
http://wis.cs.ucla.edu/atlas.
[35] Chang-Shing Perng and D. S. Parker.
SQL/LPP: A Time Series Extension of SQL Based on Limited Patience Patterns.
In DEXA, 1999.
[36] R. Sadri, C. Zaniolo, A. Zarkesh, and J. Adibi.
Expressing and Optimizing Sequence Queries in Database Systems.
TODS, 29(2):282-318, 2004.
[37] C. Zaniolo Y.-N. Law, H. Wang.
Query Languages and Data Models for Database Sequences and Data Streams.
In VLDB, pages 492-503, 2004.
[38] H. Wang and C. Zaniolo.
Using SQL to Build New Aggregates and Extenders for Object-Relational Systems.
In VLDB, 2000.
[39] P. Seshadri, M. Livny, and R. Ramakrishnan.
SEQ: A Model for Sequence Databases.
In ICDE, pages 232-239, 1995.
[40] H. Schoning.
Tamino - a DBMS Designed for XML.
In ICDE, 2001.
[41] M. Carey, J. Kiernan, J. Shanmugasundaram, and et al.
XPERANTO: A Middleware for Publishing ObjectRelational Data as XML Documents.
In VLDB, 2000.
[42] D. DeHaan, D. Toman, M. P. Consens, and M. T. Ozsu.
A Comprehensive XQuery to SQL Translation Using Dynamic Interval Encoding.
In SIGMOD, 2003.
[43] M. F. Fernandez, A. Morishima, D. Suciu, and W. C. Tan.
Publishing Relational Data in XML: the SilkRoute Approach.
IEEE Data Engineering Bulletin, 24(2):12-19, 2001.
[44] J. Shanmugasundaram and et al.
Efficiently Publishing Relational Data as XML Documents.
In VLDB, 2000.
