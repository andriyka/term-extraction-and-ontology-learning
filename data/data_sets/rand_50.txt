The dark side of Interval Temporal Logic: sharpening the undecidability border Davide Bresolin1 , Dario Della Monica2,3 , Valentin Goranko4 , Angelo Montanari3 , Guido Sciavicco5,6 1 University of Verona, Italy, 2 University of Salerno, Italy, 3 University of Udine, Italy, 4 Technical University of Denmark, 5 University of Murcia, Spain, 6 UIST Ohrid, Macedonia  Abstract Unlike the Moon, the dark side of interval temporal logics is the one we usually see: their ubiquitous undecidability.
Identifying minimal undecidable interval logics is thus a natural and important item in the research agenda in the area.
The decidability status of a logic often depends on the class of models (in our case, the class of interval structures) in which it is interpreted.
In this paper, we have identified several new minimal undecidable logics amongst the fragments of Halpern-Shoham logic HS, including the logic of the overlaps relation alone, over the classes of all and finite linear orders, as well as the logic of the meet and subinterval relations, over the class of dense linear orders.
These, together with previously obtained undecidability results, delineate quite sharply the border of the dark side of interval temporal logics.
1.
Introduction Temporal reasoning plays a major role in computer science.
In the most standard approach, the basic temporal entities are time points and temporal domains are represented as ordered structures of time points.
The interval reasoning approach adopts another, arguably more natural perspective on time, according to which the primitive ontological entities are time intervals instead of time points.
The tasks of representing and reasoning about time intervals arises naturally in various fields of computer science, artificial intelligence, and temporal databases, such as theories of action and change, natural language processing, and constraint satisfaction problems.
Temporal logics with interval-based semantics have also been proposed as a useful formalism for the specification and verification of hardware [19] and of real-time systems [11].
Interval temporal logics feature modal operators that correspond to (binary) relations between intervals usually known as Allen's relations [1].
In [13], Halpern and Shoham introduce a modal logic for reasoning about inter-  val structures (HS), with a modal operator for each Allen's relation.
This logic, which we denote as HS, turns out to be undecidable under very weak assumptions on the class of interval structures [13].
In particular, undecidability holds for any class of interval structures over linear orders that contains at least one linear order with an infinite ascending (or descending) sequence of points, thus including the natural time flows N, Z, Q, and R. For a long time, such a sweeping undecidability result have discouraged attempts for practical applications and further research on interval logics.
A renewed interest in the area has been recently stimulated by the discovery of some interesting decidable fragments of HS [6, 7, 8, 9, 10].
Gradually, the quest for expressive decidable fragments of HS has become one of the main points of the current research agenda for (interval) temporal logic.
In this quest, many fragments of HS have already been shown to be undecidable [3, 4, 5, 16].
The main aim of this paper is to contribute to the delineation of the boundary between decidability and undecidability of the satisfiability problem for HS fragments, by establishing new undecidability results.
In particular, here we exhibit the first known case of a single-modality fragment of HS which is undecidable in the class of all linear orders, as well as in the class of all finite linear orders, thus also strengthening our previous results [4, 5].
Besides, most undecidability proofs given so far exploit the existence of a linear ordering with an infinite (ascending or descending) sequence of points; here we show how this assumption can be relaxed.
For lack of space, proofs are omitted or only sketched1 .
Details of the proofs and a complete picture of the state of the art about the classification of HS fragments with respect to the satisfiability problem can be found in [12].
On the web page http: //itl.dimi.uniud.it/content/logic-hs, it is also possible to run a collection of web tools, allowing one to verify the status (decidable/undecidable/unknown) of any fragment with respect to the satisfiability problem, over various classes of linear orders (all, dense, discrete, and finite).
1 In  the submitted version they are put in a technical appendix.
2.
Preliminaries Let D = hD, <i be a linearly ordered set.
An interval over D is an ordered pair [a, b], where a, b [?]
D and a <= b. Intervals of the type [a, a] are called point intervals; if these are excluded, the resulting semantics is called strict interval semantics (non-strict otherwise).
Our results hold in either semantics.
There are 12 different non-trivial relations (excluding the equality) between two intervals in a linear order, often called Allen's relations [1]: the six relations depicted in Table 1 and their inverse relations.
One can naturally associate a modal operator hXi with each Allen's relation RX .
For each operator hXi, we denote by hXi its transpose, corresponding to the inverse relation.
Halpern and Shoham's logic HS is a multi-modal logic with formulae built over a set AP of propositional letters, the propositional connectives [?]
and !, and a set of modal unary operators associated with all Allen's relations.
For each subset {RX1 , .
.
.
, RXk } of these relations, we define the HS fragment X1 X2 .
.
.
Xk , whose formulae are defined by the grammar: ph ::= p | p | !ph | ph [?]
ph | hX1 iph | .
.
.
| hXk iph, where p is a modal constant, true precisely at point intervals.
We omit p when it is definable in the language or when the strict semantics is adopted.
The other propositional connectives, like [?]
and -, and the dual modal operators [X] are defined as usual, e.g., [X]ph [?]
!hXi!ph.
The semantics of an interval-based temporal logic is given in terms of interval models M = hI(D), V i, where I(D) is the set of all intervals over D and the valuation function V : AP 7- 2I(D) assigns to every p [?]
AP the set of intervals V (p) over which it holds.
The truth of a formula over a given interval [a, b] in a model M is defined by structural induction on formulae: * M, [a, b]  p iff a = b; * M, [a, b]  p iff [a, b] [?]
V (p), for all p [?]
AP; * M, [a, b]  !ps iff it is not the case that M, [a, b]  ps; * M, [a, b]  ph [?]
ps iff M, [a, b]  ph or M, [a, b]  ps; * M, [a, b]  hXi ips iff there exists an interval [c, d] such that [a, b] RXi [c, d], and M, [c, d]  ps, Satisfiability is defined as usual.
The notion of sub-interval (contains) can be declined into two variants, namely, proper sub-interval ([a, b] is a proper sub-interval of [c, d] if c <= a, b <= d, and [a, b] 6= [c, d]), and strict sub-interval (when both c < a and b < d).
Both variants will play a central role in our technical results; notice that by sub-interval we usually mean the proper one.
3.
Brief summary of undecidability results In this section, we first briefly summarize and reference the main undecidability results for fragments of HS.
Then,  a hAi  [a, b]RA [c, d] = b = c  hLi  [a, b]RL [c, d] = b < c  hBi  [a, b]RB [c, d] = a = c, d < b  hEi  [a, b]RE [c, d] = b = d, a < c  hDi  [a, b]RD [c, d] = a < c, d < b  hOi  [a, b]RO [c, d] = a < c < b < d  b c  d c  d  c d c d c  d c  d  Table 1.
Allen's interval relations and the corresponding HS modalities.
we state the main results of this paper, extending the previous ones in two directions: (i) we prove a number of new undecidability results for proper sub-fragments of logics that had already been shown to be undecidable, and (ii) we show how to adapt various undecidability proofs to a more general class of linear orders.
The first undecidability result, for full HS, was obtained in the original paper by Halpern and Shoham [13].
Since then, several other results have been published, starting from Lodaya [15], that proved the undecidability of the fragment BE, when interpreted over dense linear orders, or, alternatively, over ho, <i, where infinite intervals are allowed.
In [3], Bresolin at al.
proved the undecidability of a number of interesting fragments, such as AD* E* , AD* O, AD* B* , AD* O, BE, BE, and BE, where, for each X [?]
{A, L, B, E, D, O}, X* denotes either X or X.
In [4], the undecidability of all (HS-)extensions of the fragment O (and thus of O), except for those with the modalities hLi and hLi, has been proved when interpreted in any class of linear orders with at least one infinite ascending (or descending) sequence.
In [5], the one-modality fragment O alone has been proved undecidable, but assuming discreteness.
Recently, Marcinkowski et al.
have first shown the undecidability of B* D* on discrete and on finite linear orders [17], and, then, strengthened that result to the one-modality fragments D and D [16].
Here, we extend and complete the results from [4, 5], by providing an undecidability proof that assumes neither discreteness nor the presence of any infinite ascending or descending sequence.
Second, we claim that all other undecidability proofs for HS-fragments that required infinity of the structures (i.e., A* D* , B* E* ), appeared in detail in [12] for specific cases, can actually be relaxed in a similar way and, thus, generalized.
As a consequence, we depict a very sharp decidability/undecidability border for the family of HS-fragments, as the undecidability for the mentioned logics holds over the class of all finite linear orders as well as over the classical orders based on N, Z- , Z, Q, and R.  Theorem 3.1.
The satisfiability problem for the HS fragments O, O, A* D* , B* E* is undecidable in any class of linear orders that contains, for each n > 0, at least one linear order with length greater than n. In summary, as far as the (un)decidability classification is concerned, the above theorem leaves as open only one more problem, namely, the decidability/undecidability status of D and/or D in the class of all linear orders, which cannot be trivially derived neither from the undecidability in the finite and discrete cases, nor from the decidability in the dense case [18].
Due to space constraints we only show in detail the case of O.
First, we show how to relax the discreteness hypothesis, and, then, we provide the necessary changes required to relax also the hypothesis of having at least one infinite sequence in the model.
We refer the reader to [12] for full details.
4.
Undecidability of O 4.1.
Intuition As in [4, 5], our undecidability proof is based on a reduction from the so-called Octant Tiling Problem (OTP).
This is the problem of establishing whether a given finite set of tile types T = {t1 , .
.
.
, tk } can tile the second octant of the integer plane O = {(i, j) : i, j [?]
N [?]
0 <= i <= j}.
For every tile type ti [?]
T , let right(ti ), lef t(ti ), up(ti ), and down(ti ) be the colors of the corresponding sides of ti .
To solve the problem, one must find a function f : O - T such that right(f (n, m)) = lef t(f (n + 1, m)) and up(f (n, m)) = down(f (n, m + 1)).
By exploiting an argument similar to the one used in [2] to prove the undecidability of the Quadrant Tiling Problem, it can be shown that the Octant Tiling Problem is undecidable too.
Given an instance OT P (T ), where T is a finite set of tiles types, we build an O-formula PhT in such a way that PhT is satisfiable if and only if T tiles O.
The proof is structured as follows.
First, we focus on the (sub)set G[a,b] of all and only those intervals that are reachable in the language of O from a given starting interval [a, b], by defining a suitable global operator [G].
Then, we set the tiling framework by forcing the existence of a unique infinite chain of u-intervals (i.e., intervals satisfying a designated proposition u) on the underlying linear ordering; the elements of such u-chain will be used as cells to arrange the tiling, and we will define in the language a derived modality to capture exactly the next u-interval from the current one.
Third, we encode the octant by means of a unique infinite sequence of Id-intervals (Idchain), each one of them representing a row of the octant.
An Id-interval is composed by a sequence of u-intervals; each u-interval is used either to represent a part of the plane  or to separate two consecutive rows; in the former case it is labelled with tile, while in the latter case it is labelled with *; fourth, by setting suitable propositions, we encode the above-neighbor and right-neighbor relations, which connect each tile in a row of the octant with, respectively, the one immediately above it and the one immediately at its right, if any.
The encoding of such relations must be done in such a way to respect the commutativity property (Def.
4.1 below).
Throughout, if two tiles t1 and t2 are connected by the above-neighbor (resp., right-neighbor) relation, we say that t1 is above-connected (resp., right-connected) to t2 , and similarly for tile-intervals (when they encode tiles of the octant that are above- or right-connected, respectively).
Definition 4.1 (commutativity property).
Given two tileintervals [c, d] and [e, f ], if there exists a tile-interval [d1 , e1 ], such that [c, d] is right-connected to [d1 , e1 ] and [d1 , e1 ] is above-connected to [e, f ], then there exists also a tile-interval [d2 , e2 ] such that [c, d] is above-connected to [d2 , e2 ] and [d2 , e2 ] is right-connected to [e, f ].
4.2.
Technical details in the infinite case Let [a, b] be any interval of length at least 2 (i.e., such that there exists at least one point c with a < c < b).
We define G[a,b] as the set of all and only those intervals [c, d] of length at least 2 such that c > a, d > b.
Accordingly, the modality [G] defined as [G]p [?]
p [?]
[O]p [?]
[O][O]p refers to all, and only those intervals that are in G[a,b] .
Because all formulae that we will use in the encoding will be prefixed with hOi, [O], or [G], hereafter we only refer to intervals in G[a,b] ; all others will be irrelevant.
Definition of the u-chain.
The definition of the u-chain is the most difficult step in our construction, due to the extreme weakness of the language.
It involves three, related, aspects: (i), the existence of an infinite sequence of u-intervals [b0 , b00 ], [b1 , b01 ], .
.
.
, [bi , b0i ], .
.
., with b <= b0 and b0i = bi+1 for each i [?]
N; the existence of an interleaved auxiliary chain [c0 , c00 ], [c1 , c01 ], .
.
.
, [ci , c0i ], .
.
., where bi < ci < b0i , bi+1 < c0i < b0i+1 , and c0i = ci+1 for each i [?]
N, composed by k-intervals (each one of them overlapping exactly one u-chain), used to make it possible for us to reach the 'next' u-interval from the current one (see Fig.
1); (iii) guaranteeing that both chains are unique.
This third aspect is the most difficult one.
To obtain uniqueness, we show that under certain conditions, the language of O can express properties of proper sub-intervals; in particular, we show that whenever p is a so-called disjointly-bounded proposition (see Def.
4.3 below), it is possible to express properties such as "for each interval [a, b], if [a, b] satisfies p then no proper sub-interval of [a, b] satisfies p".
Let M be a model over the set AP of propositional letters - hereafter called just 'propositions', for short - and let  u2 , k1 , and k2 to be disjointly-bounded.
b0 u b1 u b2 u b3 u b4 u b5 u b6 u b7 k  c0  k  c1  k  c2  k  c3  k  c4  k  c5  k  c6  c7  !u [?]
!k [?]
[O](!u [?]
!k) [G]((u - u1 [?]
u2 ) [?]
(k - k1 [?]
k2 )  Figure 1.
Encoding of the u-chain.
[?]
(u1 - !u2 ) [?]
(k1 - !k2 ))  (4) (5)  [G]((u1 - [O](!u [?]
!k2 ))[?
](u2 - [O](!u [?]
!k1 ))) (6) [a, b] be our starting interval (which automatically defines the universe G[a,b] ).
Definition 4.2.
The propositions p, q [?]
AP are said to be disjoint if, for every pair of intervals h[c, d], [e, f ]i such that [c, d] satisfies p and [e, f ] satisfies q, either d <= e or f <= c. The proposition q is called disjoint consequent of p if p and q are disjoint and any p-interval is followed by a q-interval, that is, for each interval [c, d] [?]
G[a,b] that satisfies p, there exists an interval [e, f ] [?]
G[a,b] , with e >= d, that satisfies q.
Definition 4.3.
The proposition p is said to be disjointlybounded in G[a,b] (w.r.t.
a disjoint consequent q) if: (i) [a, b] neither satisfies p nor overlaps a p-interval, that is, p (possibly) holds only over intervals [c, d], with c >= b; (ii) p-intervals do not overlap each other, that is, there exist not two intervals [c, d] and [e, f ] satisfying p and such that c < e < d < f ; (iii) p has a disjoint consequent q.
[G]((k1 - [O](!k [?]
!u1 ))[?
](k2 - [O](!k [?]
!u2 ))) (7) [G]((hOiu1 - !hOiu2 ) [?]
(hOik1 [?]
!hOik2 )) [G]((u1 - hOik1 ) [?]
(k1 - hOiu2 ) [?]
(u2 - hOik2 ) [?]
(k2 - hOiu1 )) (4) [?]
.
.
.
[?]
(9)  - [G](p - [O](hOiq - - p )) - [G](!p [?]
[O](hOiq - - p ) - insidep )  (2)  [G]((insidep - !hOip) [?]
(p - !hOiinsidep ))  (3)  (1)  Lemma 4.4.
Let M be a model, [a, b] be an interval over M , and p, q [?]
AP two propositions such that p is disjointly-bounded in G[a,b] w.r.t.
q.
If M, [a, b]  (1) [?]
(2) [?]
(3), then, in G[a,b] , there are no p-intervals properly contained in other p-intervals.
From now on, for any given disjointly-bounded proposition p, we will use non-sub(p) to denote the (global) property that no p-interval is sub-interval of another p-interval.
By means of the following formulae, we force the letter u1 ,  (9) (10)  Lemma 4.5.
Let M be a model, and [a, b] and interval over M such that M, [a, b]  (10).
Then u1 , u2 , k1 , and k2 are disjointly-bounded.
Thanks to the above lemma, we are justified to use the formulae non-sub(u1 ), non-sub(u2 ), non-sub(k1 ), non-sub(k2 ).
Finally, to build the u-chain, we state the following formulae.
hOihOi(u1 [?]
first)  Now, whenever we can prove that a certain proposition p is is disjointly-bounded in G[a,b] w.r.t.
a disjoint consequent q, we may set an auxiliary proposition insidep in such a way that it is true over all proper sub-intervals (in G[a,b] ) of p-intervals; after that, by simply asserting that insidep intervals and p-intervals cannot overlap each other, we will be able to guarantee that p-intervals are never proper subintervals of other p-intervals.
To define insidep for the (disjointly bounded) letter p, we exploit the existence of its dis- joint consequence q, plus an auxiliary proposition - p , which we make true over a suitable subset of interval starting inside a p-interval and ending outside it.
(8)  (11)  [G](u [?]
k - [O]!first [?]
[O][O]!first)  (12)  [G]((first - u1 ) [?]
(first - [O][O]!first))  (13)  non-sub(u1 ) [?]
non-sub(u2 ) [?]
non-sub(k1 ) [?]
non-sub(k2 ) (14)  [G](u [?]
k - [O]hOi(u [?]
k))  (15)  (11) [?]
.
.
.
[?]
(15)  (16)  Lemma 4.6.
Let M be a model and [a, b] and interval over M such that M, [a, b]  (10) [?]
(16).
Then: (a) there exists an infinite sequence of u-intervals [b0 , b00 ], [b1 , b01 ], .
.
.
, [bi , b0i ], .
.
., with b <= b0 , b0i = bi+1 for each i [?]
N, and such that M, [b0 , b00 ]  first, (b) there exists an infinite sequence of k-intervals [c0 , c00 ], [c1 , c01 ], .
.
.
, [ci , c0i ], .
.
.
such that bi < ci < b0i , bi+1 < c0i < b0i+1 , and c0i = ci+1 for each i [?]
N, and (c) every other interval [c, d] [?]
G[a,b] satisfies neither of u, k, or first, unless c > bi for every i [?]
N. Within this framework, an operator hXu i, used to step from any given u-interval to the next one in the sequence, becomes now definable: hXu iph [?]
(!u[?]hOihOi(first[?]ph))[?](u[?]hOi(k[?]hOi(u[?
]ph))) Definition of the Id-chain.
In order to define the Id-chain,  we make use of the following set of formulae:  a) (17)  bw t15 t25 t35 t45 t55 fw t14 t24 t34 t44  (18)  bw t13 t23 t33 fw t12 t22  [G]((u - * [?]
tile) [?]
(* - !tile))  (19)  bw t11  [G](* - hOi(k [?]
hOiId))  (20)  [G](Id - hOi(k [?]
hOi*))  (21)  [G]((u - !hOiId) [?]
(Id - !hOiu))  (22)  [G](hOi* - !hOiId)  (23)  non-sub(Id)  (24)  (17) [?]
.
.
.
[?]
(24)  (25)  !Id [?]
!hOiId [?]
[G](Id - !hOiId) hXu i(* [?]
hXu i(tile [?]
Id [?]
hXu i* [?]
[G](* - hXu i(tile [?]
hXu itile))))  Lemma 4.7.
Let M, [a, b]  (10) [?]
(16) [?]
(25) and let b <= b01 < c01 < b11 < .
.
.
< bk11 -1 < ck11 -1 < bk11 = b02 < c02 = ck11 < b12 < .
.
.
< bk22 = b03 < .
.
.
be the sequence of points, defined by Lemma 4.6, such that [bij , bi+1 j ] satisfies u and [cij , ci+1 ] satisfies k for each j >= 1, 0 <= i < kj .
Then, j for each j >= 1, we have: (a) M, [b0j , b1j ]  *; (b) M, [bij , bi+1 j ]  tile for each 0 < i < kj ; (c) M, [b1j , b0j+1 ]  Id; (d) k1 = 2, kl > 2 for each l > 1; and no other interval [c, d] [?]
G[a,b] satisfies * (resp., tile, Id), unless c > bij for each i, j > 0.
Above-neighbor relation.
We proceed now with the encoding of the above-neighbor relation (Fig.
2), by means of which we connect each tile-interval with its vertical neighbor in the octant (e.g., t22 with t23 in Fig.
2).
For technical reasons, we need to distinguish between backward and forward rows of O using the propositions bw and fw: we label each u-interval with bw (resp., fw) if it belongs to a backward (resp., forward) row (formulae (26)-(27)).
Intuitively, the tiles belonging to forward rows of O are encoded in ascending order, while those belonging to backward rows are encoded in descending order (the tiling is encoded in a zig-zag manner).
In particular, this means that the left-most tile-interval of a backward level encodes the last tile of that row (and not the first one) in O.
Let a, b [?]
{bw, fw}, with a 6= b: hXu ibw [?]
[G]((u - bw [?]
fw) [?]
(bw - !fw))  (26)  [G]((a[?
]!hXu i* - hXu ia)[?](a[?
]hXu i* - hXu ib)) (27) (26) [?]
.
.
.
[?]
(27)  (28)  Lemma 4.8.
If M, [a, b]  (10) [?]
(16) [?]
(25) [?]
(28), then the sequence of points defined in Lemma 4.7 is such that M, [bij , bi+1 j ]  bw if and only if j is an odd number, and M, [bij , bi+1 j ]  fw if and only j is an even number.
Furthermore, we have that no other interval [c, d] [?]
G[a,b] satisfies bw or fw, unless c > bij for each i, j > 0.  b) z bw }| {z last  fw }|  {z last  bw }| last  {z  fw }|  { last  1 1 2 3 2 1 1 2 3 4 * t1 * t2 t2 * t3 t3 t3 * t4 t4 t4 t4  Figure 2.
Encoding of the above-neighbor relation.
We make use of such an alternation between backward and forward rows to use the operator hOi in order to correctly encode the above-neighbor relation.
We constrain each up rel-interval starting from a backward (resp., forward) row not to overlap any other up rel-interval starting from a backward (resp., forward) row.
The structure of the encoding is shown in Fig.
2, where up rel-intervals starting inside forward (resp., backward) rows are placed one inside the other.
Consider, for instance, how the 3rd and 4th level of the octant are encoded in Fig.
2b.
The 1st tile-interval of the 3rd level (t33 ) is connected to the second from last tile-interval of the 4th level (t34 ), the 2nd tile-interval of the 3rd level (t23 ) is connected to the third from last tile-interval of the 4th level (t24 ), and so on.
Notice that, in forward (resp., backward) level, the last (resp., first) tile-interval has no tile-intervals above-connected to it, in order to constrain each level to have exactly one tile-interval more than the previous one (these tile-intervals are labeled with last).
Formally, we define the above-neighbor relation as follows.
If [bij , bi+1 j ] is a tile-interval belonging to a forward (resp., backward) row, then we say that it is abovej+2-i+1 connected to the tile-interval [bj+2-i ] (resp., j+1 , bj+1 j+2-i-1 j+2-i [bj+1 , bj+1 ]).
To do so, we label with up rel the j+2-i j+2-i-1 interval [cij , cj+1 ] (resp., [cij , cj+1 ]).
Moreover, we distinguish between up rel-intervals starting from forward and backward rows and, within each case, between those starting from odd and even tile-intervals.
To this end, we bw use a new proposition, namely, up relbw o (resp., up rele , fw fw up relo , up rele ) to label up rel-intervals starting from an odd tile-interval of a backward row (resp., even tileinterval/backward row, odd/forward, even/forward).
Moreover, to ease the reading of the formulae, we group up relbw o bw bw and up relbw (up relbw - up relbw e in up rel o [?]
up rele ), and similarly for up relfw .
Finally, up rel is exactly  one among up relbw and up relfw (up rel - up relbw [?]
up relfw ).
In such a way, we encode the correspondence between tiles of consecutive rows of the plane induced by the above-neighbor relation.
Let a, b [?]
{bw, fw} and g, d [?]
{o, e}, with a 6= b and g 6= d:  lae (46)-(49)): [G](tile - hOiup rel)  (38) a  [G](a - [O](up rel - up rel )) [G](up rel - hOib)  (40) bw  [G](hOi* - !
(hOiup rel  !up rel [?]
!hOiup rel  (29)  [G]((up rel - up relbw [?]
up relfw ) a  [?]
(up rel - up  rela o  [?]
up  rela e ))  [G]((k [?]
* - !hOiup rel) [?]
(up rel - !hOik))  (30) (31)  a b [G](u [?]
hOiup rela g - !hOiup reld [?]
!hOiup rel ) (32)  [G](up rela - !hOiup rela )  (33)  [G](up rel - hOiId)  (34)  [G](hOiup rel - !hOifirst) [G](up  rela g  - hOi(tile [?]
hOiup  (29) [?]
.
.
.
[?]
(36)  (35) relbg ))  (36) (37)  (39)  a  [G](tile [?]
hOiup  rela g  fw  [?]
hOiup rel ))  (41)  [?]
hXu itile  (42)  - hXu i(tile [?]
hOiup rela d )) [G](last - tile)  (43)  [G]((* [?]
bw - hXu ilast) [?]
(fw [?]
hXu i* - last)) (44) [G]((last [?]
fw - hXu i*) [?]
(bw [?]
hXu ilast - *)) (45) [G](* [?]
fw - hXu i(tile  (46)  [?]
hOi(up rel [?]
hOi(tile [?]
hXu i*)))) [G](last [?]
bw - hOi(up rel  (47)  [?]
hOi(tile [?]
hXu i(tile [?]
hXu i*)))) [G](k [?]
hOi(tile [?]
hOiup rela g) - [O](hOiup rela g [?]
hOi(k [?]
hOi(tile  Lemma 4.9.
If M, [a, b]  (10) [?]
(16) [?]
(25) [?]
(28) [?]
(37), then the sequence of points defined in Lemma 4.7 is such that, for each i >= 0, j > 0, the following properties hold: 0 a) if [c, d] satisfies up rel, then c = cij and d = cij 0 for 0 0 some i, i , j, j > 0; that is, each up rel-interval starts and ends inside a tile-interval.
More precisely, it starts (resp., ends) at the same point in which a k-interval starts (resp., ends); 0 b) [cij , cij 0 ] satisfies up rel if and only if it satisfies exactly 0 one between up relbw and up relfw and [cij , cij 0 ] satisfies up relbw (resp., up relfw ) if and only if it satisfies exbw actly one between up relbw o and up rele (resp., between fw fw up relo and up rele ); 0 c) for each a, b [?]
{bw, fw} and g, d [?]
{o, e}, if [cij , cij 0 ] a satisfies up relg , then there is no other interval starting b at cij satisfying up relbd such that up rela g 6= up reld ; bw fw d) each up rel -interval (resp., up rel -interval) does not overlap any other up relbw -interval (resp., up relfw interval); 0 bw fw e) if [cij , cij 0 ] satisfies up relbw o (resp., up rele , up relo , fw up relfw e ), then there exists an up relo -interval (resp., fw bw up rele -interval, up relo -interval, up relbw e -interval) 0 starting at cij 0 .
Now, we constrain each tile-interval, apart from the ones representing the last tile of some level, to have a tile-interval above-connected to it.
To this end, we label each tileinterval representing the last tile of some row of the octant with the new proposition last (formulae (43)-(45)).
Next, we force all and only those tile-intervals not labelled with last to have a tile-interval above-connected to them (formu-  [?]
hOiup  relbd  [?]
!last)) - hOiup  (48)  rela d ))  [G](up rel - !hOilast)  (49)  (38) [?]
.
.
.
[?]
(49)  (50)  Lemma 4.10.
If M, [a, b]  (10) [?]
(16) [?]
(25) [?]
(28) [?]
(37) [?]
(50), then the sequence of points defined in Lemma 4.7 is such that the following properties hold: 0 a) for each up rel-interval [cij , cij 0 ], connecting the tile0  0  i +1 i interval [bij , bi+1 j ] to the tile-interval [bj 0 , bj 0 ], if bw fw i i0 [cj , cj 0 ] satisfies up rel (resp., up rel ), then [bij , bi+1 j ] 0  0  satisfies bw (resp., fw) and [bij 0 , bij 0+1 ] satisfies fw (resp., bw); b) (strict alternation property) for each tile-interval [bij , bi+1 j ], with i < kj - 1, such that there exists an bw fw up relbw o -interval (resp., up rele -interval, up relo -infw i terval, up rele -interval) starting at cj , there exists bw fw an up relbw e -interval (resp., up relo -interval, up rele fw i+1 interval, up relo -interval) starting at cj ; c) for every tile-interval [bij , bi+1 j ] satisfying last, there is no up rel-interval ending at cij ; 0 d) for each up rel-interval [cij , cij 0 ], with 0 < i < kj , we have that j 0 = j + 1.
Lemma 4.11.
Each tile-interval [bij , bi+1 j ] is aboveconnected to exactly one tile-interval and, if it does not satisfy last, then there exists exactly one tile-interval which is above-connected to it.
Right-neighbor relation.
The right-neighbor relation connects each tile with its horizontal neighbor in the octant, if any (e.g., t23 with t33 in Fig.
2).
Again, in order to encode  the right-neighbor relation, we must distinguish between forward and backward levels: a tile-interval belonging to a forward (resp., backward) level is right-connected to the tile-interval immediately to the right (resp., left), if any.
For example, in Fig.
2b, the 2nd tile-interval of the 4th level (t24 ) is right-connected to the tile-interval immediately to the right (t34 ), since the 4th level is a forward one, while the 2nd tile-interval of the 3rd level (t23 ) is right-connected to the tile-interval immediately to the left (t33 ), since the 3rd level is a backward one.
Therefore, we define the right-neighbor relation as follows: if [bij , bi+1 j ] is a tile-interval belonging to a forward (resp., backward) Id-interval, with i 6= kj - 1 (resp., i 6= 1), then we say that it is right-connected to the i+2 i-1 i tile-interval [bi+1 j , bj ] (resp., [bj , bj ]).
Lemma 4.12 (Commutativity property).
If M, [a, b]  (10) [?]
(16) [?]
(25) [?]
(28) [?]
(37) [?]
(50), then the commutativity property holds over the sequence defined in Lemma 4.7.
Tiling the plane.
The following formulae constrain each tile-interval (and no other interval) to be tiled by exactly one tile (formula (51)) and constrain the tiles that are rightor above-connected to respect the color constraints (from (52) to (54)): [G]((  k _  k ^  ti - tile) [?]
(  i=1  !
(ti [?]
tj ))  (51)  i,j=1,i6=j  [G](tile -  _  Definition of the u-chain.
The main difference from the reduction of the octant tiling problem described in the previous section is the finiteness of the rectangular area.
This requires the existence of an arbitrarily long, but not infinite, u-chain.
Hence, we introduce an auxiliary propositions lastu to denote the last u-interval of the (finite) u-chain.
The properties of lastu are defined as follows.
hOihOilastu  (56)  [G](lastu - * [?]
[O](!u [?]
!k) [?]
[O][O](!u [?]
!k)) (57)  (ti [?]
hOi(up rel [?]
hOitj ))) (52)  up(ti )=down(tj )  [G](tile [?]
fw [?]
hXu itile -  _  (ti [?]
hXu itj )) (53)  right(ti )=lef t(tj )  [G](tile [?]
bw [?]
hXu itile -  _  (ti [?]
hXu itj )) (54)  lef t(ti )=right(tj )  (51) [?]
.
.
.
[?]
(54)  allow us to conclude that O is undecidable when interpreted in the class of all finite linear orders.
The Finite Tiling Problem is formally defined as the problem of establishing if a finite set of of tile types T , containing a distinguished tile type t$ (blank) with the same color on all sides, can tile the entire Z x Z plane, under the restriction that at least one, but only finitely many tiles are not blank.
This problem has been first introduced and shown to be undecidable in [14].
In this section we concentrate on an equivalent variation of it, defined as the problem of establishing if T can tile a finite rectangular area (of unknown size) whose edges are colored by blank, using at least one non-blank tile.
Indeed, if this is the case then we can extend the tiling to the entire plane by putting the blank tile on all the remaining cells.
Conversely, if we can tile the entire plane using only finitely many non-blank tiles, then we can identify a finite rectangular portion of it containing all non-blank tiles and whose edges are blank.
(55)  Given the set of tile types T = {t1 , t2 , .
.
.
, tk }, let PhT be the formula (10) [?]
(16) [?]
(25) [?]
(28) [?]
(37) [?]
(50) [?]
(55).
Lemma 4.13.
The formula PhT is satisfiable if and only if T can tile the second octant O.
4.3.
Extending undecidability to finite linear orders In this section, we show how to adapt the construction of the previous section in order to encode the Finite Tiling Problem.
This provides us with an undecidability proof for the fragment O that works in any class of strongly discrete linear orders - that is, linear orders satisfying the property that every interval contains only finitely many points - that contains arbitrarily (finitely) long orders.
In particular, this  Now, we analyze the formulae used in the previous section, showing only those that need to be changed for the finite case.
Formula (9) is replaced by (58) in order to guarantee the existence of the u- and k-chains.
[G]((u1 [?]
!lastu - hOik1 ) [?]
(k1 - hOiu2 ) [?]
(u2 [?]
!lastu - hOik2 ) [?]
(k2 - hOiu1 ))  (58)  Since u1 - and u2 -intervals (resp., k1 - and k2 -intervals) do not infinitely alternate with each other in the finite case, we introduce the new proposition cons, and we force it to be a disjoint consequent of u and k. In this way, we can force u1 , u2 , k1 , and k2 to be disjointly-bounded.
!cons [?]
[O]!cons [?]
[G](u [?]
k - hOihOicons)  (59)  [G](hOiu [?]
hOik - !hOicons)  (60)  [G]((u [?]
k - !hOicons) [?]
(cons - [O](!u [?]
!k))) (61) Finally, we replace formula (15) with (62).
[G](u [?]
k - [O](hOihOilastu - hOi(u [?]
k)))  (62)  Notice that formulae (56), .
.
.
, (62) guarantees the existence of the u-chain also when interpreted over arbitrary linear orders, but that the strong discreteness assumption is  crucial to guarantee the finiteness of the chain.
As a counterexample, consider the model over Q depicted in Figure 3, 1 ] such where u1 holds over every interval [2 - 21n , 2 - 2n+1 1 1 ] that n is even, u2 holds over every interval [2- 2n , 2- 2n+1 such that n is odd, the sequence of k1 - and k2 -intervals are defined consistently, and lastu holds over the interval [2, 2 + 21 ].
Such a model satisfy formulae (56), .
.
.
, (62), but contains an infinite u-chain.
k1 u1  u2 1 2  1  k2 k1  u1 , lastu  u1 3 4  7 8  2  2+  1 2  Figure 3.
Infinite u-chain counterexample.
Definition of the Id-chain.
To guarantee that Id is a disjointly-bounded proposition, we exploit the fact that, by definition, cons is also a disjoint consequent of Id.
Moreover, as for the u-chain, we have to make sure that the chain is finite: to this end, we introduce the proposition lastId to denoting the last Id-interval of the (finite) Id-chain.
[G]((lastId - Id)[?](Id[?]hOi(k[?
]hOilastu ) - lastId )) (63) Finally, we redefine formulae (18) and (20) as follows.
hXu i * [?
][G](* - hXu itile)  (64)  [G](* [?]
!lastu - hOi(k [?]
hOiId))  (65)  Above-neighbor relation.
In the finite case, every row has exactly the same number of tiles; therefore, the formulae (43), (44), (45), (47), and (49) can be dismissed.
Formulae (36), (38), and (48) are replaced by the following ones.
[G](up rela g - (hOitile [?]
(hOihOi(* [?]
!lastu ) - hOi(tile [?]
hOiup relbg )))) [G](tile [?]
hOihOi(* [?]
!lastu ) - hOiup rel) [G](k [?]
hOi(tile [?]
hOiup [?]
hOiup  (67)  rela g)  - [O](hOiup rela g [?]
hOi(k [?]
hOi(tile relbd ))  (66)  - hOiup  (68)  rela d ))  Finally, it is not difficult to complete the construction by adding the color constraints on the border of the region and the existence of at least one non-blank tile.
Therefore, undecidability of O is proven also for finite linear orders.
References [1] J. F. Allen.
Maintaining knowledge about temporal intervals.
Communications of the ACM, 26(11):832-843, 1983.
[2] E. Borger, E. Gradel, and Y. Gurevich.
The Classical Decision Problem.
Springer, 1997.
[3] D. Bresolin, D. Della Monica, V. Goranko, A. Montanari, and G. Sciavicco.
Decidable and Undecidable Fragments of Halpern and Shoham's Interval Temporal Logic: Towards a Complete Classification.
In Proc.
of LPAR'08, volume 5330 of LNCS, pages 590-604.
Springer, 2008.
[4] D. Bresolin, D. Della Monica, V. Goranko, A. Montanari, and G. Sciavicco.
Undecidability of Interval Temporal Logics with the Overlap Modality.
In Proc.
of TIME'09, pages 88-95.
IEEE Computer Society Press, 2009.
[5] D. Bresolin, D. Della Monica, V. Goranko, A. Montanari, and G. Sciavicco.
Undecidability of the logic of Overlap relation over discrete linear orderings.
ENTCS, 262:65 - 81, 2010.
Proc.
of M4M-6, 2009.
[6] D. Bresolin, V. Goranko, A. Montanari, and P. Sala.
Tableaux for logics of subinterval structures over dense orderings.
J. of Logic and Computation, 20:133-166, 2010.
[7] D. Bresolin, V. Goranko, A. Montanari, and G. Sciavicco.
Propositional interval neighborhood logics: Expressiveness, decidability, and undecidable extensions.
Annals of Pure and Applied Logic, 161(3):289-304, 2009.
[8] D. Bresolin, A. Montanari, and P. Sala.
An optimal tableaubased decision algorithm for Propositional Neighborhood Logic.
In Proc.
of STACS'07, volume 4393 of LNCS, pages 549-560.
Springer, 2007.
[9] D. Bresolin, A. Montanari, P. Sala, and G. Sciavicco.
Optimal Tableaux for Right Propositional Neighborhood Logic over Linear Orders.
In Proc.
of JELIA'08, volume 5293 of LNAI, pages 62-75.
Springer, 2008.
[10] D. Bresolin, A. Montanari, and G. Sciavicco.
An optimal decision procedure for Right Propositional Neighborhood Logic.
J. of Automated Reasoning, 38(1-3):173-199, 2007.
[11] Z. Chaochen, C. A. R. Hoare, and A. P. Ravn.
A calculus of durations.
Inf.
Processing Letters, 40(5):269-276, 1991.
[12] D. Della Monica.
Expressiveness, decidability, and undecidability of Interval Temporal Logic.
PhD thesis, University of Udine, 2011.
Available at: http: //www.dia.unisa.it/dottorandi/dario.
dellamonica/pubs/thesis/phd_thesis.pdf.
[13] J. Halpern and Y. Shoham.
A propositional modal logic of time intervals.
Journal of the ACM, 38(4):935-962, 1991.
[14] J. Kari.
Reversibility and surjectivity problems of cellular automata.
Journal of Computer Systems and Science, 48:149-182, 1994.
[15] K. Lodaya.
Sharpening the undecidability of interval temporal logic.
In Proc.
of ASIAN'00, volume 1961 of LNCS, pages 290-298.
Springer, 2000.
[16] J. Marcinkowski and J. Michaliszyn.
The ultimate undecidability result for the Halpern-Shoham logic.
Proc.
of LICS'11, 2011.
[17] J. Marcinkowski, J. Michaliszyn, and E. Kieronski.
B and D are enough to make the Halpern-Shoham logic undecidable.
In Proc.
of ICALP'10 - Part II, volume 6199 of LNCS, pages 357-368, July 2010.
[18] A. Montanari, G. Puppis, and P. Sala.
A decidable spatial logic with cone-shaped cardinal directions.
In Proc.
of CSL'09, volume 5771 of LNCS, pages 394-408, 2009.
[19] B. Moszkowski.
Reasoning about digital circuits.
Tech.
rep. stan-cs-83-970, Dept.
of Computer Science, Stanford University, Stanford, CA, 1983.
A.
Proof details A.1.
Proof of Lemma 4.4 Proof.
Suppose, by contradiction, that there exist two intervals [c, d] and [e, f ] (belonging to G[a,b] ) satisfying p and such that [e, f ] is sub-interval of [c, d].
By definition of subinterval, we have that c < e or f < d. Without loss of generality, let us suppose that c < e (the other case is analogous).
Since [e, f ] [?]
G[a,b] , then there exists a point in between e and f , say it e0 .
The interval [c, e0 ] is a sub-interval of [c, d].
Moreover, it cannot satisfy p, since it overlaps the p-interval [e, f ] (and p is a propositional letter disjointlybounded in hM, [a, b]i).
By (1) and by the fact that q is a disjoint consequent of p, each interval starting in between - c and d, and ending inside a q-interval, satisfies - p .
Thus, - - 0 [c, e ] satisfies !p and [O](hOiq - p ).
By (2), it must also satisfy insidep .
But this contradicts (3), hence the thesis.
A.2.
Proof of Lemma 4.6 Proof.
For the sake of simplicity, we will first prove a variant of points (a) and (b), that is, respectively, (a') there exists an infinite sequence of u-intervals [b0 , b00 ], [b1 , b01 ], .
.
.
, [bi , b0i ], .
.
., with b <= b0 , b0i <= bi+1 for each i [?]
N, and such that M, [b0 , b00 ]  first, (b') there exists an infinite sequence of k-intervals [c0 , c00 ], [c1 , c01 ], .
.
.
, [ci , c0i ], .
.
.
such that bi < ci < b0i , bi+1 < c0i < b0i+1 , and c0i <= ci+1 for each i [?]
N. Then, we will prove point (c).
Finally, we will force b0i = bi+1 and c0i = ci+1 for each i [?]
N, actually proving the original version of points (a) and (b).
As for the proof of points (a') and (b'), it is simple to see that formulae (4), (5), (6), (7), (9), and (11) are enough to guarantee the existence of the u- and k-chains with the desired properties.
We must show, now, that each other interval satisfies neither u nor k. As a preliminary step, it is useful to show that an u-interval (resp., k-interval) belonging to G[a,b] cannot be sub-interval of u-intervals or k-intervals.
Formula (8) guarantees that it cannot exist an u1 -interval (resp., k1 -interval) that is sub-interval of an u2 interval (resp., k2 -interval) or, vice versa, an u2 -interval (resp., k2 -interval) that is sub-interval of an u1 -interval (resp., k1 -interval).
Moreover, since, by Lemma 4.5, u1 , u2 , k1 , and k2 are disjointly bounded, then (14) guarantees that no u1 -interval (resp., u2 -interval, k1 -interval, k2 interval) can be sub-interval of another u1 -interval (resp., u2 -interval, k1 -interval, k2 -interval).
So far, we have shown that no u-interval (resp., k-interval) can be sub-interval of any u-interval (resp., k-interval).
It remains to show that no u-interval can be sub-interval of any k-interval, and vice versa.
Suppose, by contradiction, that the u-interval [c0 , d0 ]  is sub-interval of the k-interval [c00 , d00 ].
By (9), there must exist a k-interval, say it [c000 , d000 ], starting in between c0 and d0 .
Then, we either have (i) d000 <= d00 and the k-interval [c000 , d000 ] is sub-interval of the k-interval [c00 , d00 ], contradicting the previous statement, or (ii) d000 > d00 and the k-interval [c00 , d00 ] overlaps the k-interval [c000 , d000 ], contradicting (7).
With a similar argument, one can show that no k-interval can be sub-interval of a u-interval.
Thus, we can state that u-intervals (resp., k-intervals) cannot be subintervals of u- or k-intervals.
Now, let us focus on the point (c) of the lemma.
Suppose, by contradiction, the existence of the u-interval [c, d], belonging to G[a,b] and such that [c, d] 6= [bi , b0i ] for any i [?]
N. By (4), it must be c >= b.
Now, let us distinguish the following cases: * if b <= c < b0 , then one of the following: - if d < b00 , then (12) is contradicted, - if d >= b00 , then the u-interval [b0 , b00 ] is subinterval of the u-interval [c, d], * if c = bi for some i [?]
N, then one of the following: - if d < b0i , then the u-interval [c, d] is sub-interval of the u-interval [bi , b0i ], - if d = b0i , then we are contradicting the hypothesis "per absurdum" that [c, d] 6= [bi , b0i ] for any i [?]
N, - if d > b0i , then the u-interval [bi , b0i ] is subinterval of the u-interval [c, d], * if bi < c < b0i for some i [?]
N, then one of the following: - if d <= b0i , then the u-interval [c, d] is sub-interval of the u-interval [bi , b0i ], - if d > b0i , then the u-interval [bi , b0i ] overlaps the u-interval [c, d], contradicting (6), * if b0i <= c < bi+1 for some i [?]
N, then one of the following: - if d <= bi+1 , then the u-interval [c, d] is subinterval of the k-interval [ci , c0i ], - if bi+1 < d < b0i+1 , then the u-interval [c, d] overlaps the u-interval [bi+1 , b0i+1 ], contradicting (6), - if d >= b0i+1 , then the u-interval [bi+1 , b0i+1 ] is sub-interval of the u-interval [c, d].
Thus, there cannot exist an u-interval [c, d] [?]
G[a,b] such that [c, d] 6= [bi , b0i ] for any i [?]
N. A similar argument can be exploited to prove that there cannot exist a k-interval [c, d] [?]
G[a,b] such that [c, d] 6= [ci , c0i ] for any i [?]
N. In addition, suppose, by contradiction, the existence of the interval [c, d], belonging to G[a,b] , satisfying first, and such that [c, d] 6= [b0 , b00 ].
By the first conjunct of (13), it must be [c, d] = [bi , b0i ] for some i [?]
N, with i 6= 0.
Thus, the second conjunct of (13) is contradicted.
Finally, suppose, by contradiction, that it is the case that b0i < bi+1 for some i [?]
N. By the previous argument, there must be bi , ci , c0i , b0i+1 such that bi < ci < b0i , bi+1 < c0i <  b0i+1 , and [ci , c0i ] satisfies k. By point (c), there cannot exist an u- or k-interval starting in between ci and bi+1 .
Then, the interval [bi , b0i ] contradicts (15), since it overlaps the interval [ci , bi+1 ] that, in turn, does not overlap any u- or k-interval.
Thus, it must be b0i = bi+1 for each i [?]
N. In a very similar way, it is possible to show that it must also be c0i = ci+1 for each i [?]
N.  A.3.
Proof of Lemma 4.7 Proof.
First of all, we show that Id is a disjointly-bounded propositional letter.
By (17), it is easy to see that Id meets the first two requirements of Definition 4.3.
By (22) and (23), * and Id are disjoint, and, by (21), * is a disjoint consequent of Id.
Thus, Id is a disjointly-bounded propositional letter.
The proof proceeds case by case.
(a) Observe that there exists an infinite sequence of *intervals, thanks to (18), (20), and (21).
Let us denote by [b01 , b11 ], [b02 , b12 ], .
.
.
, [b0j , b1j ], .
.
.
such a sequence.
By the first conjunct of (19), we can assume that, for each j > 0, there is no *-interval between [b0j , b1j ] and [b0j+1 , b1j+1 ].
(b) By (19), each interval satisfying * or tile is an u-interval and each u-interval satisfies either * or tile.
Then, the u-intervals between two consecutive *-intervals (if any) must be tile-intervals.
(c) By (20), for each k-interval [c0j , c1j ] overlapped by a *interval, there exists an Id-interval [c, d], with c0j < c < c1j < d. We show that c = b1j and d = b0j+1 .
Suppose that c < b1j .
Then, the u-interval [b0j , b1j ] overlaps the Id-interval [c, d], contradicting (22).
On the other hand, if c > b11 , then we distinguish two cases.
* j = 1.
In this case, by (18), we have that [b1j , b2j ] is the Id-interval representing the first level of the octant.
Now, if d > b21 , then the u-interval [b11 , b21 ] overlaps the Id-interval [c, d], contradicting (22); otherwise, if d <= b21 , then the Id-interval [c, d] is a sub-interval of the Id-interval [b11 , b21 ], contradicting (24) (recall that Id is a disjointly-bounded propositional letter).
* j > 1 ([b1j , b2j ] is not the last tile-interval of the jth level).
In this case, the k-interval [c1j , c2j ] does not overlap a *-interval (since [b2j , b3j ] is a tileinterval).
Thus, due to (21), it must be d > c2j , and the u-interval [b1j , b2j ] overlaps the Id-interval [c, d], contradicting (22).
Hence, it must be c = b1j .
Now, we have to show that d = b0j+1 , that is, the Id-interval starting immediately after the *-interval [b0j , b1j ] ends at the point in which the next *-interval starts.
Suppose, by contradiction, that d 6= b0j+1 .
Suppose that j = 1.
In this case, if d < b02 (resp., d > b02 ), then the Id-interval [c, d] (resp.,  [b11 , b21 ]) is a sub-interval of the Id-interval [b11 , b21 ] (resp., [c, d]), contradicting (24).
So, let us suppose j > 1, and consider the following cases: k -1 * if d <= cj j , then (21) is contradicted, since either [c, d] does not overlap any k-interval or it overlaps a k-interval that does not overlap any *interval; k -1 < d < b0j+1 , then the Id-interval [c, d] * if cj j k -1  k  overlaps the u-interval [bj j , bj j ], contradicting (22); * if b0j+1 < d < b1j+1 , then the Id-interval [c, d] overlaps the u-interval [b0j+1 , b1j+1 ], contradicting (22); * if d >= b1j+1 , then (23) is contradicted, since the interval [a0 , c0j+1 ], where a0 is a generic point in between a and b, overlaps both the *-interval [b0j+1 , b1j+1 ] and the up rel-interval [c, d].
Hence, it must be d = b0j+1 .
(d) By (18), it immediately follows that k1 = 2 and kl > 2 when l > 1.
Finally, suppose, by contradiction, that there exists an Idinterval [c, d] [?]
G[a,b] such that [c, d] 6= [b1j , b0j+1 ] for each j > 0 and that c <= bij for some i, j > 0.
By (17), the interval [a, b] neither satisfies Id nor overlaps an interval that satisfies Id, thus c >= b, and one of the following cases arise.
1.
If b <= c < b01 , then, by (21), it must be d > c01 , and (23) is contradicted.
2.
If b0j <= c < c0j for some j > 0, then (23) is contradicted.
3.
If c0j <= c < b1j for some j > 0, then, due to (21), it must be d > c1j and the u-interval [b0j , b1j ] overlaps the Id-interval [c, d], contradicting (22).
4.
If c = b1j for some j > 0, then we have already shown that it must be d = b0j+1 .
5.
If b1j < c < b0j+1 for some j > 0, then: (a) if d <= b0j+1 , then the Id-interval [c, d] is subinterval of the Id-interval [b1j , b0j+1 ], contradicting (24), (b) if d > b0j+1 , then the Id-interval [b1j , b0j+1 ] overlaps the Id-interval [c, d], contradicting (17).
The fact that no other interval [c, d] [?]
G[a,b] satisfies * or tile, unless c > bij for each i, j > 0 can be proved by a similar argument.
A.4.
Proof of Lemma 4.9 Proof.
We only proof point a), that is the less intuitive.
Let [c, d] be an up rel-interval.
First, we show that it must be 0 c = cij , for some i, j > 0.
Then, we prove that d = cij 0 , for 0 0 some i , j > 0.
Notice that we want to exclude also the case in which c = c0j (resp., d = c0j 0 ) for some j > 0 (resp., j 0 > 0), since this would imply the existence of an up rel-interval  starting (resp., ending) inside a *-interval.
This is done by means of (31) (first conjunct) and (36).
Now, we show that c = cij , for some i, j > 0.
By (29), it must be c >= b and, by (35) and (36), it follows c >= c01 .
Moreover, by (31) and (36), it cannot be the case that bij <= c < cij for any i >= 0, j > 0.
It only remains to exclude the case in which cij < c < bi+1 j for some i >= 0, j > 0.
Thus, suppose, by contradiction, that for some i >= 0, j > 0.
If d > ci+1 cij < c < bi+1 j j , then (31) is contradicted; otherwise, if d <= ci+1 , then, by (34), [c, d] j overlaps an Id-interval.
As a consequence, there should be i i+1 an Id-interval starting at bi+1 j , that means that [bj , bj ] is a *-interval.
This lead to a contradiction with (31), since the *-interval [bij , bi+1 j ] overlaps the up rel-interval [c, d].
Thus, we have that c = cij for some i, j > 0.
Now, we want to 0 prove that d = cij 0 for some i0 , j 0 > 0.
It is easy to see that, 0 if d 6= cij 0 for any j 0 , i0 > 0, then there would be an up relinterval overlapping a k-interval, contradicting (31), hence the thesis.
A.5.
Proof of Lemma 4.10 0  Proof.
a) Let [cij , cij 0 ] be an up rel-interval connecting 0  0  i +1 i the tile-interval [bij , bi+1 j ] to the tile-interval [bj 0 , bj 0 ].
bw i i0 Suppose that [cj , cj 0 ] satisfies up rel (the other case is symmetric) and that [bij , bi+1 j ] satisfies fw.
Then, (39) is 0  0  contradicted.
Similarly, if [bij 0 , bji 0+1 ] satisfies bw, then (40) is contradicted.
b) Straightforward, by (42); c) Straightforward, by (49); 0 d) Let [cij , cij 0 ] be an up rel-interval, with 0 < i < kj , and suppose, by contradiction, that j 0 6= j + 1.
Suppose that 0 [cij , cij 0 ] is an up relbw -interval (the other case is symmetric).
By point a) of this lemma, we have that [bij , bi+1 j ] 0  0  satisfies bw and that [bij 0 , bij 0+1 ] satisfies fw.
Two cases are possible: i0 +1 i0 (i) if j 0 = j, then [bij , bi+1 j ] and [bj 0 , bj 0 ] belong to the same Id-interval.
By Lemma 4.8, they must be both labelled with bw or fw, against the hypothesis; (ii) if j 0 > j + 1, then consider a tile-interval [bhj+1 , bh+1 By j+1 ] belonging to the (j + 1)-th level.
Lemma 4.8, we have that [bhj+1 , bh+1 ] satisfies j+1 fw (since [bij , bi+1 j ] satisfies bw) and, by (38) and (39), we have that there is an up relfw -interval 0 starting at chj+1 and ending at some point chj 00 for 00 some j > j + 1, (by point (i)).
Consider the *-interval [b0j+2 , b1j+2 ].
We have that the interval [a0 , c0j+2 ], where a0 is a generic point in between a and b, overlaps the *-interval [b0j+2 , b1j+2 ], 0 the up relfw -interval [chj+1 , chj 00 ], and the up relbw -  0  interval [cij , cij 0 ], contradicting (41).
Hence, the only possibility is j 0 = j + 1.
A.6.
Proof of Lemma 4.11 Proof.
First of all, we observe that each tile-interval is above-connected with at least one tile-interval, by (38) and by Lemma 4.9, item a).
Now, suppose, by contradiction, that there exists a tile-interval [bij , bi+1 j ] not satisfying last and such that there is no tile-interval above-connected to it.
The proof proceeds by induction.
Base case.
If [bij , bi+1 j ] is the rightmost interval of the jth Id-interval not satisfying last and it satisfies fw (resp., bw), then we have that i = kj - 2 (resp., i = kj - 1).
Formula (47) (resp., (46)) guarantees the existence of an up rel-interval ending at cij , leading to a contradiction.
Inductive step.
Otherwise, if [bij , bi+1 j ] is not the rightmost interval of the j-th Id-interval not satisfying last, then the inductive case applies.
So, we can assume the inductive hypothesis, that is, there is an up rel-interval end0 ing at ci+1 and starting at some point cij-1 .
We want to j show that there exists also an up rel-interval ending at cij .
0 Without loss of generality, suppose that [cij-1 , ci+1 j ] satisfies up relfw .
Then, by Lemma 4.9, item e), there exists o i+1 an up relbw -interval starting at c and, by the strict alo j ternation property (Lemma 4.10, item b)), there exists an i up relbw e -interval starting at cj .
We show that, by applying 0  0  -1 i (48) to the k-interval [cij-1 , cj-1 ], we get a contradiction.
0  0  -1 i Indeed, [cij-1 , cj-1 ] satisfies k [?]
hOi(tile [?]
hOiup relfw o ) 0 and it overlaps [bij-1 , bij ], which satisfies the following formulae: fw i+1 i0 * hOiup relfw o : [cj-1 , cj ] satisfies up relo ; * hOi(k [?]
hOi(tile [?]
hOiup relbw e [?]
!last)): the interi val [ci-1 , c ] satisfies k and overlaps the tile-interval j j i i+1 [bj , bj ], which does not satisfy last (by hypothesis) and overlaps an up relbw e -interval (that one starting at cij ).
0  We show that [bij-1 , bij ] does not satisfy the formula hOiup relfw e , getting a contradiction with (48).
Suppose that there exists an interval [e, f ] satisfying up relfw e and such 0 that bij-1 < e < bij < f .
We distinguish the following cases: 0 * if f > ci+1 and e > cij-1 , then the up relfw o -interval j 0 fw [cij-1 , ci+1 ] overlaps the up rel -interval [e, f ], cone j tradicting Lemma 4.9, item d); 0 * if f > ci+1 and e = cij-1 , then there are an up relfw o j fw i0 and an up rele -interval starting at cj-1 , contradicting Lemma 4.9, item c); fw fw * if f = ci+1 j , then there are an up relo - and an up rele i+1 interval ending at cj and, by Lemma 4.9, item e),  bw there are an up relbw o - and an up rele -interval starting i+1 at cj , contradicting Lemma 4.9, item c); * finally, if f = cij , we have a contradiction with the hypothesis.
Thus, there exists no such an interval, contradicting (48).
This proves that each tile-interval is above-connected to at least one tile-interval and, if it does not satisfy last, then there exists at least one tile-interval above-connected to it.
Now, we show that such connections are unique.
Suppose, 00 0 by contradiction, that for some [cij , cij+1 ] and [cij , cij+1 ], 0 00 0 00 with cij+1 < cij+1 (the case cij+1 > cij+1 is symmet0 00 ric), we have that both [cij , cij+1 ] and [cij , cij+1 ] are up relintervals.
By Lemma 4.9, we have that they both satfw isfy the same propositional letter among up relfw o , up rele , bw bw fw up relo , and up rele , say up relo (the other cases are 0 00 symmetric).
Then, both cij+1 and cij+1 start an up relbw o interval by Lemma 4.9, item e).
By the strict alternation i0 +1 property, an up relbw e -interval starts at the point cj+1 .
Since 0  0  +1 i +2 [bij+1 , bj+1 ] does not satisfy last (it is neither the rightmost nor the leftmost tile-interval of the (j + 1)-th Id-interval), then, as we have already shown, there exists a point c such 0 +1 that [c, cij+1 ] is an up rel-interval.
By Lemma 4.9, items e) 0  i +1 and c), we have that [c, cj+1 ] is an up relfw e -interval.
We show that the existence of such an interval leads to a contradiction: i0 +1 * if c < cij , then the up relfw e -interval [c, cj+1 ] over00 i i laps the up relfw o -interval [cj , cj+1 ], contradicting Lemma 4.9, item d); * if c = cij , then cij starts both an up relfw o - and an fw up rele -interval, contradicting Lemma 4.9, item c); i i0 * if c > cij , then the up relfw o -interval [cj , cj+1 ] 0  i +1 overlaps the up relfw e -interval [c, cj+1 ], contradicting Lemma 4.9, item d).
In a similar way, we can prove that two distinct up relintervals cannot end at the same point.
ral networks that may be used in any system relying upon such networks and giving account of some particular structure in which a <<strict decomposition>> may appear.
[LePape,90] C. Le Pape - A Combination of Centralized and Distributed Methods for Multi-Agent Planning and Scheduling.
Proc.
IEEE Robotics and Automation, 1990.
7 .
References  [LePape,94] C. Le Pape - Implementation of Resource Constraints in ILOG SCHEDULE: A Library for the Development of Constraint-Based Scheduling Systems.
to appear in <<Intelligent Systems Engineering>>, 1994.
[Boddy,93] M. Boddy - Temporal Reasoning for Planning and Scheduling.
SIGART Bulletin, 4(3), 1993.
[Collinot,87] A. Collinot & C. Le Pape - Controlling Constraint Propagation.
Proc.
10th IJCAI, Milan (It) 1987.
[Collinot,88] A. Collinot, C. LePape, G. Pinoteau - SONIA: A Knowledge-Based Approach to Industrial Job-Shop Scheduling.
International Journal for Artificial Intelligence in Engineering, 3(2), 1988.
[Dean,86] T. Dean - Intractability and TimeDependent Planning.
"Reasoning About Actions and Plans", 1986.
[Dechter,89] R. Dechter, I.Meiri, J.Pearl - Temporal Constraint Networks.
Technical Report, Oct 1989.
[Erschler,91] J. Erschler, P. Lopez & C. Thuriot - Raisonnement Temporel sous Contrainte de Ressource et Problemes d'Ordonnancement.
Revue d'Intelligence Artificielle, 5(3), 1991 [in french].
[French,82] S. French - Sequencing & Scheduling: an Introduction to the Mathematics of Job-Shop.
Whiley, 1982.
[Ghallab,89] M. Ghallab & A. Mounir-Alaoui Managing Efficiently Temporal Relations Through Indexed Spanning Tree.
Proc.
11th IJCAI, 1989.
[Ghallab,94] M. Ghallab & T. Vidal - Focusing on a Sub-Graph for Managing Efficiently Numerical Temporal Constraints.
LAAS Report 94303, Jan. 1994.
[Vidal,94] T. Vidal, M.Ghallab & R.Alami - Dynamical Allocation of Predefined Tasks to a Large Team of Robots.
LAAS Report n 94303, Aug 1994.  action brings some real duration value that is propagated to the following expected actions, thanks to an arc-consistency algorithm that runs efficiently in O(m.r) in worst case, where m is the number of time-points in the graph managed by the execution process, and r the number of robots.
3.
For big databases, the initial off-line process of expanding the graph, with initial propagations became really costly.
But the global in-line allocation/execution process behaves with nearly the same experimental complexity, thus paying off for the initial preprocessing.
One can notice that in fact allocation and execution processes work on the same graph, but not exactly on the same sub-set of time-points: the sliding interleaving technique leads to a certain time-lag between both.
Thus, we assume a robust interleaving process, with the quality of the solution produced depending upon the level of imprecision characterising the instance of the application.
***  If we have a lot of imprecision, we cannot require a tight discrimination criteria between two robots(i), because the execution would let this imprecision unresolved, and the process would stop.
So, we need to adapt our requirement of quality to the level of imprecision.
We should tune it, with statistical or learning methods for example, such that each robot is given at each time an horizon of one up to three tasks, thus maintaining a sufficient time-lag between allocation and execution.
The application presented throughout this paper describes techniques for allocating predefined identical tasks to a large team of identical robots.
This is done at a high-level of abstraction, with centralised techniques, in order to give to each robot a global description of the tasks it has to perform.
Path planning and trajectory control capabilities are managed in a distributed way in the ESPRIT project MARTHA.
6 .
Experimental Results and Conclusion  The need for near-optimal solutions, considering the imprecision of temporal information, led us to adopt an allocation/execution interleaving process, which gave birth to the necessity of finding highly efficient temporal management techniques.
This was made possible thanks to  Our method was implemented in CommonLisp in a SunSparc environment.
The formal details about the tests can be found in [Vidal,94].
We can summarize our results as follows: 1.
With less imprecise data, we get better near-optimal solutions.
2.
The temporal propagation processes appear to be negligible compared to the global allocation process, which complexity itself appears to be strictly bounded and runs in the order of the second(ii).
i. formally a short overlapping of the two intervals corresponding to the possible values of the availability times of the two robots  * the separation between heuristic choices and temporal management.
* the use of the same graph, with distinct propagation algorithms, for managing allocation as well as execution steps, * and essentially, the proof of some important property of the path-consistency mechanism made it possible to restrict it to local propagations, keeping the global completeness of the propagation process with respect to the application needs.
This is a general property of tempoii.
which has to be compared to durations of actions in the order of 10 to 15 minutes at the less.
Thus we have to run the two decision steps that are usually addressed in search techniques especially when backtrack-free search is required, and that will be made through heuristic functions that are briefly introduced here (see [Vidal,94] for more details): 1.
<<variable ordering>>: which task to schedule next, i.e.
in our case which container to take next.
We will look for the "most critical" container (in a temporal sense).
This search runs in linear time in the number of reachable containers, which are generally few, as far as the containers are <<highly stacked>> in the conveyors.
2.
<<value ordering>>: which resource and temporal placement to choose for this task, i.e.
in our case which robot will take care of the container.
We will look for the robot that is likely to arrive first at the unloading area.
This search runs in linear time in the number of robots.
*** Once the choices have been made, temporal precedence constraints will be added * between the last availability time-point of the robot and the beginning of its new task, * and ordering constraints for the PICKUP and PUTDOWN operations.
This constraints adding process goes with temporal propagations as it is depicted in section 3, leading to updating the graph, and then to new values on the availability date of the robot and the temporal windows on each unloading/loading operation.
In [Vidal,94], we detail a least-commitment approach, mixed with some heuristic choice when needed, to obtain a backtrack-free near-optimal ordering of the unloading/loading operations.
To summarize, the global allocation process runs through the following steps: 1.
Heuristic choice of the most critical container: CONT.
2.
Heuristic choice of the best robot to convey it: ROB.
3.
Strict ordering of the last PUTDOWN made by ROB, with associated temporal propagations.
4.
Temporal propagation in the sub-graph of the currently allocated task.
5.
Strict ordering of the PICKUP of the currently allocated task, with associated temporal propagations.
One can easily denote the separation between heuristic decisions and temporal management, which allows flexibility of the global system: it is always possible to improve the heuristic functions, without challenging the overall system.
5 .
The Execution Process and the Global Control Loop As we have sketched out in the introduction, one cannot allocate the robots to all the tasks, because of lack of precision in the numerical constraints given in input.
The more tasks we give to a robot, the more imprecision we get in its new availability time, because of imprecision growing within successive propagation processes.
We reach a point when it is no more possible to choose between two robots in a sufficiently deterministic way: we can only make an unreliable choice that could lead to a future need to backtrack.
We then decide to stop the allocation loop and wait until the execution process provides more precise values: each executed  turn propagated in that neighbour cluster.
The process goes on that way until no constraint is modified.
Hence we will not compute the complete graph, but we can easily prove that every constraints included in a cluster will be minimal at the end of the process.
This intuitive behaviour is summarized through the two following properties, that can be easily proved: 1.
The task-graphs defined above represent a strict decomposition of the overall graph of the application.
2.
The clusterised propagation is complete: it always detects a global inconsistency if there is one, and moreover provides minimal constraints (thus new precedence constraints as well) within each cluster.
The only constraints requested by the allocation process (see above) appear, from the definition of the task-graph, to be confined to those taskgraphs.
Thus we can say that our clusterised propagation process behaves in the same way as a global propagation would do, regarding the global process we have to address.
Concerning the efficiency, the decomposition leads to the definition of sub-graphs each containing 14 time-points.
We thus get a complexity of O(k.143) at each constraint addition, with k being the number of sub-graphs that will have to be propagated in a recursive way.
As we only allocate within a short-term horizon, the total number of clusters in the graph corresponding to the tasks being allocated is strictly bounded, thus k is strictly bounded, which leads to nearly constant time propagation process.
To end with this section, let us look at the global temporal management process during the allocation process.
At the beginning, the global graph is expanded, with all the task-graphs in parallel.
Initial propagation in each sub-graph (not yet connected one to another) is also made.
Each time a robot is allocated to a mission, the precedence constraints corresponding to the interactions with other clusters are added (as in our graph example), launching clusterised propagation steps.
Thus, the global graph evolves from a highly parallel one to a more sequential one, where, as usually in scheduling problems, tasks for a robot are incrementally ordered, and constraints between those robot sequences of tasks appear because of the ordering of the unloading/loading operations.
4 .
The Allocation Decision-Making Loop Just remember that in our interleaving approach, we have to dynamically allocate one robot to predefined tasks of handling containers, until some threshold in time is reached, that will require to wait for results of the execution of the allocated task (see part 5).
So we will define a loop whose incremental steps are allocation of one robot to one container, with the aim of not backtracking on these decisions.
At each of these steps, we are given: * (a) the current partially ordered overall graph, * (b) the current position and <<availability time>> of each robot (i.e.
the resources availability), * (c) the current stacks of containers remaining on each conveyor, and the loading area for each container (i.e.
the set of tasks requiring sequencing and resource allocation).
We want to get some near-optimal (or <<good quality>>) solution according to the criteria which appears to be preeminent in our application: minimising the earliest end time of the overall plan (as in [LePape,90]).
In other words, we wish to process all the unloading/loading tasks with a minimal loss of time, in order to get the best chances of having unloaded/loaded all the containers in a conveyor before its leaving.
Thus, the global graph appears to be composed of task-graphs like in the first figure of last page, some of them connected one to another by precedence constraints.
Let us define more precisely those task-graphs: each task is associated to a cluster of 14 timepoints containing: * the origin of time 0 (reference frame of the dates), * the end time-point of the last task assigned to the same robot, if there is one.
* the initial and final points of each action in the task (8 time-points), * the current temporal windows for PICKUP and PUTDOWN actions (4 time-points).
Those intersecting clusters are represented in the second figure in last page, by means of circles, not including, for clarity concerns, the timepoint 0.
For the same reason, the precedence constraints between 0 and some conveyor availability beginning time-points are not represented.
In fact, the only temporal constraints that are useful for the global mission allocation process, as we will see in next part, are: * the date of availability of a robot, i.e.
the date of the end of the last task allocated.
* the temporal windows for the PICKUP and PUTDOWN operations.
This means that the only minimal constraints that are useful are constraints between timepoints belonging to the same cluster.
In other words, we will never need to know the temporal distance between the beginning of PICKUP(Rob1,Cont2,Boat) and the end of GOTO(Rob4,Park1,Train), for example.
Thus we only need to get minimal constraints within those clusters to guide the allocation decisions.
Let us now briefly present the properties that are at the heart of our process:  << Definition 1 >> * A strict minimal precedence (i,j) is a precedence constraint that cannot be entailed by another (by transitivity).
* A <<strict decomposition>> D of a graph G is a set of clusters (Cu)u=1,..c , corresponding to overlapping sub-sets of time-points {iu1, ..., ium}, such that:  [?
]i [?
]G, [?]
Cu such that i [?
]Cu (complete partition upon time-points),  [?
]i [?
]G, [?
]j [?
]G, such that (i,j) is a strict minimal precedence, then necessarily [?]
at least one u such that i [?
]Cu, and i [?
]Cu (complete partition upon constraints).
* The <<neighbour clusters>> of a cluster Cu are Cv1, ..., Cvm such that for each Cvj, [?]
a timepoint ivj such that ivj [?
]Cvj and ivj [?
]Cu at the same time.
The <<clusterised>> propagation process will then be as follows: * If D is a strict decomposition of G, if (i,j) is a temporal constraint being modified, then the propagation process will be the following recursive process: execute a propagation step within Cu and within each of its neighbour clusters Cv1, ..., Cvm, and then repeat the same process for each modified cluster, until there is no cluster modified.
Let us illustrate those definitions through the graph in the second figure of last page.
The arrows represent strict precedences of the graph.
We can see that all of them are included in at least one cluster.
Then, a modification of such a constraint will be surely propagated to all the other constraints of the cluster, that will be updated.
Some modified constraints are included in other clusters as well: those are the precedence constraints between tasks, or the ordering constraints between PICKUP/PUTDOWN actions (in dotted lines in the figure).
They are in  all the minimal constraints (i.e.
where only values globally consistent with the other constraints are kept), and at the same time gives account of new precedence constraints.
As our application involves large number of robots and containers, n is quickly in the order of a few thousands time-points (about ten times as many as the number of containers).
Hence a cubic complexity will end in unbearable time consuming algorithms.
We can take some advantage of the particular structure of the temporal network in our application.
The global decision-making process consists of: * allocating each robot to successive tasks, which leads to a strict ordering of tasks for each robot, * and adding constraints on PUTDOWN and PICKUP operations (following the constraints on crane actions), thus inducing precedence constraints between tasks.
3 .
The Temporal Graph Decomposition Scheme  u1 IN_AREA (boat) u2  [0',10']  i1  0  GOTO [10',15']  [300',360']  i3 PICKUP i4  i2  i1 time-point action simple precedence constraint sequencing actions conveyor availability temporal window [0',10'] numerical temporal constraint  x  x  x  PICKUP  x  IN_AREA (boat)  x  0  GOTO  x  GOTO  x  x  x  x  x  time-point action simple precedence within a task-graph conveyor availability temporal window simple precedence between distinct task-graphs task-graph  l1  IN_AREA (train) [90',120']  PUTDOWN  x  x  x  x x  x  x x  [6',10']  x  x x  i7PUTDOWN i8  i6  IN_AREA (train)  x  x  GOTO [20',30']  [60',75']  x x  i5  [5',8']  x  x  x  x  x  x  x  x  x  x  x  x  x  x  x x  x  x x  l2  Our proposal is to decompose the global temporal graph into sub-parts (or clusters) in which propagation can be confined.
Our decomposition scheme relies upon the application-dependent structure of the graph, whereas for example [Dean,86] proposes a temporal decomposition based upon complex problem characterisation techniques.
We prove that our method meets some important property that keeps its algorithmic completeness (i.e.
it always detects an inconsistency when there is one); it also gives back the minimal constraints [Dechter,89], when considering only the constraints that are required by the decision-making process.
We then achieve a highly reliable temporal constraints manager whose propagation runs in nearly constant time, keeping at the same time the total expressiveness of the [Dechter,89] temporal constraint networks.
One important thing to notice is that we take advantage of a separate temporal constraint management system, which can be compared to the [Erschler,91] system and also to [Boddy,93] system relying on the Dean's TMM.
Another important feature is that our method is incremental, which means that ordering constraints are added one by one, updates and consistency checking being made at each step.
2 .
Application Context and Basic Representation Issues The global mission in our application domain is the following: a large team of robots have to load/unload ships in a harbour (ESPRIT project MARTHA).
Containers (a few hundreds per mission) arrive in boats or trains (we will use the generic term of <<conveyor>>).
They are to be brought to some other place: stockage areas, or conveyor areas.
Those tasks are to be carried out by a crew of robots (about 50), all identical, through predefined routes linking the different areas in the harbour.
A mission allocation system has to decide which robot is going to take  care of which container.
The generic task performed by a robot is a sequence of the four actions: * Moving from the robot current position to the unloading area (GOTO action).
* Picking up the assigned container by use of a crane located in this area (PICKUP).
* Moving to the loading area (GOTO).
* Putting down the container (PUTDOWN).
Each of these actions have an associated duration, and the PICKUP and PUTDOWN actions have to be made during the availability temporal window of the corresponding conveyor.
The containers are arranged inside a conveyor by stacks, which defines an initial partial order on unloading operations.
But there is just one crane per area, which means that PICKUP and PUTDOWN operations have to be strictly ordered in the final plan for each area.
Moreover, cranes are not explicitly represented but implicitly managed throughout the temporal ordering of PICKUP/PUTDOWN operations.
*** The representation of time within our temporal system IxTeT relies upon a graph-based structure with time-points (the nodes in the graph) that are constrained by precedence constraints (directed edges in the graph, see [Ghallab,89]), and also by imprecise numerical constraints (durations and dates) given as possible durations labelling the precedence edges (see [Dechter,89] and [Ghallab,94]).
This leads to represent the above general task with the conveyors availability temporal windows as it is shown in the first figure of next page.
As far as numerical constraints are concerned, a path-consistency propagation algorithm, running in O(n3), n being the total number of timepoints in the overall graph, is required.
It allows to check for global consistency, making explicit  Efficient Temporal Management through an ApplicationDependent Graph Decomposition Thierry Vidal Malik Ghallab thierry@laas.fr malik@laas.fr LAAS-CNRS - 7, avenue du Colonel-Roche, 31077 Toulouse, France  Abstract In the MARTHA project, a large number of robots in a harbour are given the global task of transporting standardized containers from one area to another (ships, trains, stocking areas).
The global decision-making process consisting of allocating robots to those predefined tasks can be viewed as a scheduling and resource allocation problem, which is addressed here in a centralised way.
Imprecision of temporal constraints (expected arrival and leaving times of ships and trains, durations of actions) make it meaningless to search for a strict optimal schedule.
Our approach interleaves task allocation and execution, scheduling in a sliding short-term horizon, as the execution process runs, and providing near-optimal solutions.
For large applications as our, the complexity of temporal management is a crucial issue.
We present here a technique of graph decomposition, leading to nearly-constant time temporal propagation, without any loss of information.
We finally relies on a complete, highly expressive and efficient temporal reasoner used by our global decision-making loop.
1 .
Introduction and Related Work When addressing scheduling and resource allocation problems in the context of real applications involving dynamical uncertain worlds, one has to manage temporal constraints as well as resource constraints, trying to get a robust schedule of good quality as fast as possible.
In our application context, we have to allocate predefined tasks to a number of identical robots.
It is in fact what [LePape,94] calls a joint problem, mixing constraint-based scheduling and resource allocation.
We are mainly concerned with the imprecision of temporal constraints on expected events and goals.
To decide which robot to allocate to a task, we use heuristic functions approximating a given optimality criteria along a greedy search approach, thus leading to near-optimal solutions.
Here, the global objectives that have to be met are temporal ones (earliest end time of the overall plan), hence the choices are guided by the temporal constraints.
As those are imprecise, short term predictions and objectives are usually quite reliable whereas long term ones are not.
This lead us to adopt a dynamic approach (as it is defined in [French,82]), interleaving task allocation and execution, like in [Collinot,88], or in [Dean,86].
The efficiency of the allocation process is a crucial issue.
Like in [Erschler,91] or in [Collinot,87], the temporal constraints propagation process is at the heart of the problem, making explicit new constraints that will help guide the scheduling choices, which will in turn add new constraints that will need to be propagated.
But [Erschler,91] as most authors use an O(n3) complexity algorithm that is too expensive in our application, whereas [Collinot,87] uses focusing techniques that, although enhancing the propagation efficiency, do not achieve its completeness any longer, and then leads to <<deviations>> in the decision process.
A Constraint Database System for Temporal Knowledge Roman Gross and Robert Marti Institut fur Informationssysteme ETH Zentrum, CH-8092 Zurich Switzerland (gross,marti)@inf.ethz.ch  Abstract  This paper describes how the technology of deductive constraint database systems and constraint query languages can be used to represent and reason with temporal knowledge.
First, we summarize our approach to manipulating constraints over reals within deductive database systems.
This approach is based on the compile-time rewriting of clauses which are not admissible.
Then, we show how the timestamping of facts and rules in temporal databases can be mapped to constraints over reals.
Subsequently, we present a more ecient and elegant approach which is based on a special temporal constraint solver.
Keywords: temporal databases, temporal reasoning, deductive databases, constraint query languages  1 Introduction  Current database management systems ultimately return a set of ground substitutions for the variables occurring in a query, that is, equations of the form X = c where X is a variable and c is a constant.
In other words, every answer corresponds to a tuple of constant values, typically of type integer, real or string.
However, many real-world problems can not be solved by associating constant values to all of the variables e.g.
because insucient information is available in order to compute a precise answer.
Instead, some variables may take part in constraints in the form of complex equations or inequalities, or they may even be completely free.
Problems which naturally give rise to such \partial" answers which are subject to certain constraints include con guration tasks, circuit design and temporal reasoning.
In a similar vein, conventional database systems typically fail to answer queries if at least one of the subqueries has in nitely many solutions.
Hence, the answer set cannot be enumerated as it would be in ordinary systems.
Typical problems of this kind are  periodically recurring events such as the weekly departure of a particular ight.
FlightNr DepDay DepTime `SR100' 1994=12=10 13:05 `SR100' 1994=12=17 13:05 .. .. .. .
.
.
However, the solutions may be subject to certain conditions which can be represented as a  nite collection of constraints.
The tuples in the example above have in common that the DepDay is equal to 1994/12/10 modulo 7 days.
F lightNr DepDay DepT ime `SR100' 1994/12/10 mod 7 days 13:05 Jaar et al.
have successfully merged constraint solving techniques to deal with the above mentioned problems with logic programming 9].
In the last few years, many systems based on the approach of constraint logic programming (CLP) have been implemented, e.g.
CLP(R).
This approach has been extended to temporal annotated CLP 6] to handle temporal constraints as investigated in 13].
A drawback of these systems is that they are main memory based and can only handle a limited amount of data (eciently).
Moreover, these approaches cannot handle concurrent access by several users adequately Therefore, 12] have proposed the concept of a constraint query language (CQL) in order to represent and handle constraints in deductive databases.
CQLs are similar to CLP-languages in that they support the management of non-ground facts and non-allowed rules as well as non-ground answers.
However, to the best of our knowledge, no complete and reasonably ecient implementation of 1a CQL exists with the exception of our own DeCoR system (8], see below).
During the same period, a lot of research has been done in the areas of temporal databases (see e.g.
17, 1]) and temporal reasoning (e.g.
13]).
Temporal databases not only contain information about the 1 DeCoR stands for DEductive database system with COnstraints over Reals.
present state of the real world, but also the history leading up to this state and/or possible future developments.
Most of these eorts are based on the relational model and SQL.
In this paper, we argue that the constraint handling technology of DeCoR can elegantly be applied to managing temporal information, providing the functionality associated with typical temporal database systems.
Moreover, temporal information which is incomplete and/or potentially in nite can be represented by the system.
The paper is structured as follows: Section 2 gives an overview of DeCoR and how it deals with constraints.
Section 3 sketches how temporal knowledge can be represented in the existing DeCoR system, using constraints over reals.
Section 4 presents a more elegant approach which relies on building a special purpose constraint solver which can deal with constraints over both time points (instants) and time intervals.
2 Overview of DeCoR  The DeCoR system is a prototype of a deductive constraint database system implemented as a strongly coupled front-end (written in Prolog) running on top of an SQL-based commercial database product (Oracle).
As a result of this architecture, the DeCoR system features full database functionality (i.e., concurrency control, logging and recovery, authorization and physical data independence).
A DeCoR database consists of a set of clauses (facts and rules) which are mapped to SQL base tables and views in a straightforward way.
Rule bodies and queries are translated into SQL statements which can then be executed by the underlying relational database system (DBMS).
The queries are evaluated in a bottom-up fashion in an attempt to minimize the calls to the DBMS.
In addition to the components of typical deductive database systems, the DeCoR system also contains components to store and handle constraints.
2.1 Deductive Database with Constraints  In the following we assume familiarity with deductive database systems, as e.g.
described in 5, 19, 15].
These systems usually handle equations and inequalities (= 6= <  > fi) on arithmetic terms (with + ; ) only if the variables occurring in these expressions are ground and therefore the arithmetic expressions can be evaluated.
DeCoR generalizes the treatment of arithmetic relations by allowing variables to be non-ground.
Hence, these built-in predicates can be viewed as constraints on potential values for those variables.
Definition 2.1 A constraint is a relation (t1 t2 ) where t1  t2 are arithmetic terms and the symbol  represents any of the symbols (= 6= <  > fi).
Definition 2.2 A generalized clause is an implication of the form  p0 (X0 )  p1(X1 ) : : : pk(Xk ) cl (Xl ) : : : cm (Xm ): where pi are user dened predicates and cj are constraints.
The Xi are vectors of variables.
A generalized clause does not have to be range-restricted.
This de nition follows that given by Kanellakis et al.
12] for a generalized fact.
Definition 2.3 A system of generalized clauses form  a database with constraints, called a constraint database.
Formulas in the DeCoR system may contain conjunctions (), existential quanti ers (9) as well as negation (:).
Disjunctions have to be represented by multiple clauses.
Without loss of generality, we assume in the following that clauses and queries are in standard form, i.e.
all arguments in atoms are variables and each variable occurs at most once in a userde ned predicate, c.f.
8, 10].
Example 2.1 The standard form of the user-dened clause \p(X Y )  q(X 5) r(X Y Y ):" is  p(X Y )  q(X1  Z) r(X2  Y1 Y2) Z =5 X =X1  X1 =X2  Y =Y1  Y1 =Y2 :  2.2 Types of Constraints  In deductive database systems which demand allowedness, all variables become instantiated (ground) during bottom-up evaluation 5].
This is no longer guaranteed in constraint database systems because some variables might only be constrained by inequalities or even completely free.
Therefore, two types of variables are distinguished according to their groundness which can be determined in a bottom-up fashion.
Definition 2.4 A variable X in the body of a clause is ground hgi if   X is bound to a constant c.   X can be bound to a term f(Y1  : : : Yn) by solving the constraints of the clause.
(For this to be possible, the Yi have to be ground.)
X appears in a user-dened predicate at an argument position which is ground.
(The groundness of the arguments is determined by the groundness patterns of the body predicates, see below.)
Otherwise the variable is non-ground hni.
The arguments in the head of a clause inherit the groundness information from the body.
The groundness of the arguments in turn determines the groundness patterns of the predicates.
Example 2.2 The groundness pattern hggngi for a predicate determines that the rst, second and fourth arguments are ground whereas the third argument is non-ground.
Based on the groundness information of the variables, constraints can be separated into evaluable and non-evaluable ones.
Definition 2.5 A constraint C is evaluable if   C is an equation which can be transformed into X = f(Y1  : : : Yn ) where each Yi is ground.
C is an inequality not containing any nonground variable.
Otherwise, the constraint is non-evaluable.
It can easily be seen that evaluable constraints correspond exactly to those allowed in \ordinary" deductive database systems.
As shown e.g.
in 5, 4] these constraints can be translated into selection conditions in relational algebra in a straightforward way.
This is not the case with non-evaluable constraints which have to be manipulated separately.
2.3 Constraint Lifting Algorithm  The constraint database system DeCoR delays the evaluation of non-evaluable constraints until they become evaluable (if ever).
For that purpose, the clauses are rewritten at compile-time in such a way that at run-time, only evaluable parts have to be dealt with 8].
This is achieved by propagating nonevaluable constraints into dependent clauses.
This process is denoted as constraint lifting (CL).
The constraint lifting algorithm (CLA) consists of the following 5 steps which are applied iteratively on each clause C in a bottom-up fashion on the reduced dependency tree.
1.
Rewrite clause C into standard form.
In doing so, all equality constraints become explicit.
2.
For each literal in the body of C, lift the nonevaluable constraints occurring in its respective de nitions into C. 3.
Simplify (solve) the resulting conjunction of constraints.
4.
Split the simpli ed constraints into evaluable and non-evaluable ones.
5.
Fold the evaluable parts (user-de ned predicates and evaluable constraints) into a unique auxiliary predicate which contains as arguments the variables of the non-evaluable part (nonevaluable constraints).
Step 3 of the CLA depends on a domain-speci c constraint solver which has to meet some special requirements 7].
For example, it must be able to deal with variables for which it is only known whether or not they will become ground at run-time.
The DeCoR system contains a solver for constraints over reals based on Kramer's rule and Fourier elimination.
The following example shows the abilities of the CLA in the DeCoR system.
Example 2.3 All facts to the predicate tax rate are supposed to be ground.
They contain the lower and  upper bound of income ranges and the tax rate to be applied.
Min Max Rate 0 5 000 0 5 000 25 000 0:03 25 000 55 000 0:07  tax(Inc Tax)  tax rate(Min Max Rate) Min  Inc Inc < Max Tax = Rate  Inc Min < 10 000:  w w w   (1)  Steps 1 to 3 of the CLA do not aect the rewriting of this clause because tax rate is a base predicate.
Steps 4 and 5 result in  tax(Inc Tax)  tax aux(Inc T ax Min Max Rate) Min  Inc Inc < Max (2) Tax = Rate  Inc: (3) tax aux(   Min Max Rate)  tax rate(Min Max Rate) Min < 10 000:  (2) contains the non-evaluable parts of the original clause and (3) the evaluable ones.
The query ?
{ tax(I T ) is also rewritten by the CLA.
In step 2, the body of (2) is lifted into the query and replaces the predicate tax rate.
The resulting query ?
{ tax aux(I T Min Max Rate)  Min  I I < Max T = Rate  I:  does not have to be processed further and can be answered by  I  T 0  Constraints 0  I I < 5 000 5 000  I I < 25 000 T = 0:03  I  As a consequence of this approach, the run-time query evaluation mechanism of the DeCoR system only has to deal with the evaluable parts of the clauses and hence can be realized using well known techniques developed for deductive databases.
In particular, the usual optimization techniques developed for standard bottom-up evaluation (e.g.
Magic Template 14], Constraint Pushing 16]) can be combined with the CLA.
3 Managing Temporal Knowledge in DeCoR  Managing temporal knowledge can be considered as an application of the general-purpose constraint database system DeCoR.
The DeCoR system provides the functionality to implement most of the  typical features of temporal databases.
Similar to the constraint-based framework of 13], the temporal database should support time points (following 11], we will use the term instant) and time intervals as well as relations between objects of these types.
As mentioned earlier and in contrast to our approach the system described in 13] is main-memory based and therefore it is limited to handle large amount of data.
In this paper we restrict ourselves to relations with only one temporal argument which can be considered as the valid-time of a tuple.
Nevertheless, our ideas can easily be generalized to relations with multiple temporal arguments such as bitemporal relations.
In the following, familiarity with temporal databases as e.g.
described in 17, 1] is assumed.
Moreover, we attempt to adhere to the terminology introduced in 11].
In order to improve readability, a special notation for temporal terms is used: Instant variables (event variables) will be denoted by E1 E2 : : : whereas interval variables will be denoted by T1 T2 : : :.
The start (end) point of a time interval T will be denoted by T s (T e ).
A concrete instant is represented as e.g.
/1994/5/2213:40:13.6.
An instant such as /1994/6/1500:00:00 will be abbreviated to /1994/6/15.
Finally, time intervals are considered as closed at the lower bound and open at the upper one.
The format for such an interval is /1994/6/15 { /1994/6/23).
3.1 Temporal Datatypes and Relations  While the DeCoR system supports the datatypes string, integer and real, only constraints over reals can currently be solved.
To apply constraint solving ability to temporal terms, an instant E can be mapped to a value of type real.
This is done by calculating the number of seconds between E and a reference point, e.g.
/1970/1/1.
Example 3.1 The predicate rate shows the instant and the exchange rate between two currencies rate(=1993=4=23  13:45:12:4 `US$' `SFR' 1:14) m  rate(7:251183e + 08 `US$' `SFR' 1:14) A consequence of the above mapping is that the best precision is obtained for instants near the reference point.
This is usually not a problem because most databases contain instants within a few decades only or do not need a temporal granularity smaller than microseconds.
In historical databases, each fact is usually timestamped with a valid time interval which represents the knowledge that the fact is true at every instant within this interval.
The distinction between a valid time interval and all the instants within the interval leads to two dierent representations of intervals in the DeCoR system.
The implicit representation emphasizes the validity of the fact at every instant in the interval.
The explicit representation emphasizes the  time interval and gives direct access to the bounds of the interval.
Definition 3.1 The implicitsrepresentation of a fact p(X) valid in the interval T {T e ) is p(X E )  T s  E  E < T e: The explicit representation of the same fact is p(X T s  T e ): The advantage of the implicit representation is that the temporal conjunction (p(X E ) q(X E )), which requires the intersection of the valid times associated with p and q, can be directly performed by the constraint solver of the constraint database system.
On the other hand, it is not possible to extract the bounds of the interval.
Hence, it is not possible to calculate the duration of the interval.
This drawback disappears if the intervals are represented explicitly.
However, this representation requires that temporal relations have to be de ned explicitly too.
The advantages and disadvantages of these two representations are similar to those of instants respectively time intervals.
(We refer to 1] for the discussion of time intervals versus instants and further references.)
In contrast to \ordinary" deductive database systems, the temporal relations can easily be represented in a constraint database system such as DeCoR.
As a consequence, the explicit representation of time intervals is preferred.
The following three examples serve to convey the idea how temporal relations can be realized as clauses in a constraint database.
Example 3.2 The interval T3s {T3e ) is the s(none  empty) temporal intersection of the intervals T1 {T1 ) and T2s {T2e )  =  inter(s T1se T1es T2se T2es T3se T3e ):  s= s e e inter(T1  T1  T2  T2  T1  T1 )  T1 fi T2  T1  T2 : inter(T1s  T1e T2s  T2e  T1s  T2e )  T1s fi T2s  T1e > T2e : inter(T1s  T1e T2s  T2e  T2s  T1e )  T1s < T2s  T1e  T2ee : inter(T1s  T1e T2s  T2e  T2s  T2e )  T1s < T2s  T1e > T2 : Example 3.3 The relation T1s {T1e ) before T2s{T2e ) can be written as  before(T1s  T1e  T2s T2e )  T1e < T2s : Example 3.4 The duration D of an interval calculated in seconds and can be written as  T  is  duration(T s  T e  D)  D = T e ; T s : The temporal relations can be used as ordinary user-de ned predicates.
This can be seen at the following example adapted from 1].
Example 3.5 Supposing a system contains thes timee  stamped relation works in(EmpNo DepNo T  T ), the query: \In what period (T 3s T 3e)) did the person with the employee number 1354 and 245 work in  the same department (Dep) and how many days (D) were this?"
would be written as: ?
{ 9 T1s T1e T2s T 2e S  (works in(1354 Dep T1s T 1e) works in(245 Dep T2s T 2e) inter(T1s T 1e T2s T2e T3s T 3e) duration(T3s T3e S) S = 86400  D): The last constraint S = 86400  D is required because the predicate duration calculates the dierence of T3s and T3e in seconds (S) and not in days (D) as demanded in the query.
If the predicate works in contains ground facts only, all variables in the query will be ground and hence all constraints lifted from inter and duration turn out to be evaluable.
However, this query will return a partially instantiated answer if there is a person for whom only the starting point of his or her employment in a department is known.
Such an answer cannot be returned by conventional temporal database systems because they do not have a constraint component.
This weakness of conventional system can not be removed even by introducing a value \forever".
3.2 Temporal Reduction (Coalescing)  As pointed out e.g.
in 2, 3, 1], it is necessary for temporal complete systems to allow coalescing time intervals for value-equivalent facts (facts which are identical with the exception of their temporal arguments).
They refer to this operation as temporal reduction.
Top-down constraint handling systems such as described in 6, 13] do not coalesce value-equivalent facts because they are tuple oriented.
Hence, in contrast to our approach, these systems are not temporally complete 3].
As shown in 3] such systems are less expressive than temporally complete ones.
Example 3.6 If a historical database contains the timestamped relation works in Name Dep  Start  End  `Mary' `R&D' 1982=01=01 1988=01=01 `Mary' `Eng' 1988=01=01 1995=01=01 it is necessary to coalesce the timestamps to generate the intended answer to the query \Who worked more than 10 years in the company".
Of course, we have to perform temporal reduction in our representation of a temporal database as well.
Because the reduction operator is second order and because it is not integrated in the underlying DeCoR system, the reduction has to be programmed explicitly for every user-de ned predicate.
In the following, the reduction scheme for a generic predicate p(X T s  T e ) is presented.
The reduction requires two additional relations.
p clos(X T s  T e ) contains the transitive closure obtained by pairwise coalescing the intervals pertaining to value-equivalent  tuples.
The relation p red(X T s  T e ) contains the temporally reduced facts.
p clos(X T s T e )  p(X T s  T e ): p clos(X T1s T2e )  p(X T1s  T1e ) p clos(X T2s T2e ) T1s < T2s  T2s  T1e  T1e < T2e : p red(X T s T e )  p clos(X T s T e ) :9T1s T1e (p clos(X T1s  T1e ) T1s  T s  T e  T1e  :(T1s = T s  T e = T1e )):  3.3 Assessment  The typical functionality associated with temporal database systems can be implemented as an application of the DeCoR system.
Moreover, the possibility to manipulate partially instantiated facts and answers as supported by the DeCoR system can be exploited nicely.
Unfortunately, this approach has some serious drawbacks.
First, it is not very ecient because the CLA has to lift many constraints.
In particular, each temporal conjunction results in the lifting of four constraint parts.
This overhead is not acceptable for such a frequent operation.
Second, as seen in the examples above, it is clumsy to explicitly write two arguments for every time interval.
Both drawbacks are primarily due to the facts that time intervals are represented by two points and that the database system knows nothing about the special semantics of these two arguments as interval bounds.
4 A Temporal Extension of DeCoR  The drawbacks mentioned above disappear if temporal semantics are directly built into the underlying database system.
This is done in the TDeCoR system which is an extension of the constraint database system DeCoR.
The extensions consist of temporal datatypes, constraints relating temporal and nontemporal variables and the incorporation of the reduction algorithm.
The two main goals are improving (1) the eciency of the evaluation of temporal queries and (2) the readability of temporal clauses and queries.
4.1 Constraints over Temporal Domains  The DeCoR system is extended by the two datatypes instant and interval.
Constants of one of these types are denoted as described at the beginning of Section 3.
In contrast to the ChronoLog system 2, 1], there is no special syntax for temporal arguments.
However, as shown below, the uni cation of temporal arguments has a special semantics.
We mention in passing that this design decision supports the de nition of multiple temporal arguments, e.g.
to represent valid and transaction time.
T1 precedes T2 T1 follows T2 T1 contains T2 T1 equals T2 T1 overlaps T2  Table 1: interval  interval constraints T1 before T2 T1 starts T2 T1 during T2 T1 ends T2 T1 after T2  E before T E starts T E during T E ends T E after T  Table 2: interval  interval or instant  interval constraints Similar to the constraints (= 6= > < fi ) between terms of type real currently supported in the DeCoR system, we introduce the temporal constraints shown in Tables 1 and 2 (as suggested in 13]).
As seen in Table 2, some constraints are overloaded in the sense that they represent relations between two intervals as well as between an instant and an interval.
For example, the constraint starts can not only be used to relate intervals with the same start point, but also to extract or set the start point of an interval.
In addition to the above constraints, we introduce relations to extract dierent properties of an instant (see Table 3).
year(E  I) month(E  I) day(E  I) Ith day of the month weekday(E  I) Ith day of the week hour(E  I) minute(E  I) second(E  I) Table 3: instant  integer constraints Note that constraints are relations rather than functions and can therefore be used in a bidirectional way.
Example 4.1 In the query \in which month is Peter  born" the instant argument E is the given argument for predicate month ?
{ 9E birthday('Peter' E ) month(E  Month): whereas in the rule \Paul has to present the progress of his work every Monday" the instant argument E is not known: duty('Paul' 'present work' E )  weekday(E  1):  As seen in the example above, it is possible to express periodical knowledge in the TDeCoR system.
(Indeed, the constraints listed in Table 3 are somewhat similar to the modulo relations of 18].)
The constraint duration (interval  integer  unit) calculates the duration of an interval.
The third argument which has to be given determines the granularity in which the duration is measured.
Example 4.2 The constraint duration can be used to dene the span between the two instants E1 , E2 to be 6 month  starts T  duration(T  6 month) E2 ends T The constraint intersect(T1,T2,T3) determines the explicit intersection T3 of two time intervals T1 and T2.
In addition to the above datatypes and constraints, special syntax expresses which parts of a formula have to be temporally coalesced.
The TDeCoR system does not make an implicit reduction because in some cases, the user does not want to perform a reduction.
Moreover the system is more ecient if the coalescing is performed only on demand.
Following 1], the formula F which has to be coalesced must be enclosed by braces fg.
(If multiple temporal arguments are present the user has to specify which ones have to be reduced by supplying a second argument within the braces) E1  Example 4.3 The query \Who worked in our company for more than 25 years" needs coalescing ?
{ 9 T  Y (f(9Dep works in(Emp Dep T )) T g  duration(T  Y year) Y fi 25): The reduction algorithm realized in the TDeCoR system is similar to the approach described in 2].
In addition, it is extended in a straightforward way to temporally reduce facts with multiple temporal arguments in each user speci ed direction.
This is for example necessary to reduce bitemporal relations.
4.2 Handling of Temporal Constraints  The constraint lifting algorithm presented in Sect.
2.3 is not restricted to constraints over variables of type real.
Indeed, similar to the CLP scheme 9], the CLA depends on a concrete domain.
Ultimately, the constraint solver invoked in step 3 of the CLA only determines which constraints can be simpli ed and solved.
The integration of the constraint solver of DeCoR with a temporal constraint solver will be investigated in the next section.
In order to apply the CLA on clauses with temporal constraints, it is necessary to de ne (1) the standard form of a temporal formula and (2) when a temporal constraint is evaluable.
Non-temporal formulas are rewritten into standard form by replacing the second and every further occurrence of a variable X by a new unique variable Xi and a constraint X = Xi (see Example 2.1).
However, if a  variable of type time interval occurs multiple times in a formula, this transformation is not so useful since it imposes the constraint that the two intervals be exactly the same.
In temporal databases, it is usually much more interesting to merely impose the constraint that time intervals associated with dierent facts overlap (i.e., have a non-empty intersection).
As a result, multiple occurrences of variables denoting time intervals are related via the intersect constraint.
(Note that this strategy corresponds to the notions of temporal join respectively temporal conjunction.)
Example 4.4 The temporal formula  emp(ENo Name T ) work in(ENo  T ) salary(ENo Sal T )  will be rewritten in standard form as  emp(ENo Name T ) work in(ENo1   T1) salary(ENo2  Sal T2) ENo = ENo1  ENo = ENo2 intersect(T  T1 T3) intersect(T3  T2 ) As a consequence of this approach, the readability is improved.
As a case in point, consider the query of Example 3.5 which will be written works in(1354 Dep T ) works in(245 Dep T ) duration(T  D day):  In order to de ne the evaluability of a temporal constraint, the de nition of the groundness of a variable has to be extended.
The terms ground or nonground given in Def.
2.4 do not allow to adequately categorize variables bound to partially instantiated intervals.
Definition 4.1 A partially instantiated interval is an interval with one bound ground and the other nonground.
Definition 4.2 A variable X bound to the partially instantiated interval T is start-ground (s) if the start point of T is ground.
If the end point of T is ground then the variable X is end-ground (e).
A ground interval variable is both start-ground and end-ground.
Example 4.5 A variable bound to the partially instantiated interval /1994/8/2312:24:13.8 { E ] is start-ground whereas a variable bound to the interval E { /1996/1/1] is end-ground.
Table 4 shows which minimal groundness tags are required for every argument of a temporal constraint in order to guarantee its evaluability.
(Note that the following partial order holds for groundness tags: n < s, n < e, s < g, e < g. Also, instant variables can only be ground or non-ground.)
Example 4.6 The temporal constraint T1 before T2  can be evaluated if the end point of T1 and the start  before after precedes follows during contains equals starts ends overlaps year month : : : duration intersect  hesi hsei heni hnsi hsni hnei hggi hgni hsei hesi hngi hsni hnsi heni hnei hegi hgni hgni hsgi hegi hggni hsgei hegsi hnggi hgsei hesgi hgesi hsegi hgngi  Table 4: Groundness patterns for evaluable temporal constraints point T2 are known.
The following situation would evaluate to true.
T1  )    T2  4.3 Constraint Solving  For space reasons, the speci cation of a temporal constraint solver can not be given in this paper.
However, a general speci cation for a solver in a constraint database system is given in 7].
Nevertheless, a temporal constraint solver can not simply be added to the existing non-temporal one.
Instead, the two solvers have to collaborate by passing information about freshly bound variables between them.
This is illustrated in example 4.7.
Example 4.7 In the query \at which instant would  John have worked twice as long in the sales department as in the engineering department?"
?
{ 9 T1 T2 M1 M2 (works in(`John' `Eng' T1) works in(`John' `Sal' T2)  duration(T1 M1 month) M2 = 2  M1  duration(T2 M2 month) E ends T2 ):  the temporal variable E (the result we are looking for) depends on the non-temporal variable M2 .
M2 depends on M1 which in turn depends on the interval variable T1.
The rst call of the temporal solver determines that the constraint \duration(T1  M1 month)" is evaluable so that M1 becomes ground.
The temporal constraint \duration(T2  M2 month)" remains nonevaluable because the rst argument is suggested to be only start-ground and the second argument is non-ground.
In the next step, the non-temporal solver determines \M2 = 2  M1 " as evaluable and  therefore M2 as ground.
This enables the temporal constraint solver to identify the constraints \duration(T2 M2 month)" and \E ends T " as evaluable.
This example shows that it is necessary to iterate steps 3 and 4 of the constraint lifting algorithm.
During this iteration ostensivebly non-ground variables are determined to be ground because new constraints are identi ed to be evaluable.
This may in turn lead to the solving of further constraints, and so on.
The iteration terminates when no new nonground variables can be determined as ground.
If m non-temporal or event variables and n interval variables are present this is the case after at most max(m 2  n) steps.
This iteration takes place during the constraint lifting algorithm which on its own is executed at compiletime.
At query-evaluation time no further manipulation of the temporal constraints is necessary.
5 Conclusions  In this paper, we have  rst shown how a deductive database system which handles generalized facts and non-allowed rules can be realized.
Subsequently, we have demonstrated how its ability to manipulate constraints over reals can be used to represent the timestamping information typical of temporal databases.
Finally, we have presented a more sophisticated approach to represent temporal knowledge which is based on the development and integration of a special purpose temporal constraint solver.
References  1] Michael B!ohlen.
Managing Temporal Knowledge in Deductive Databases.
PhD thesis, ETH Zurich No.
10802, 1994.
2] Michael B!ohlen and Robert Marti.
Handling temporal knowledge in a deductive database system.
In Datenbanksysteme in Buro, Technik und Wissenschaft, 1993.
3] Michael B!ohlen and Robert Marti.
On the completeness of temporal database query languages.
In Proc.
1st Int.
Conf.
on Temporal Logic, July 1994.
4] Jan Burse.
ProQuel: Using Prolog to implement a deductive database system.
Technical Report TR 177, Departement Informatik ETH Z!urich Switzerland, 1992.
5] Stefano Ceri, Georg Gottlob, and Letizia Tanca.
Logic Programming and Databases.
Surveys in Computer Science.
Springer Verlag, 1990.
6] Thom Fruehwirth.
Temporal logic and annotated constraint logic programming.
In IJCAI Workshop on Executable Temporal Logic, 1993.
7] Roman Gross and Robert Marti.
Compile-time constraint solving in a constraint database system.
In Workshop Constraints and Databases, Int.
Logic Programming Symposium, Ithaca, November 1994.
8] Roman Gross and Robert Marti.
Handling constraints and generating intensional answers in a deductive database system.
Journal of Computers and Articial Intelligence, 13(2-3):233{256, 1994.
9] Joxan Jaar and Jean-Louis Lassez.
Constraint logic programming.
In Proc.
of the 14th ACM Symposium on Principles of Programming Languages, pages 111{119, January 1987.
10] Joxan Jaar and Michael J. Maher.
Constraint logic programming: A survey.
Journal of Logic Programming, 19/20:503{582, 1994.
11] C. S. Jensen, J. Cliord, R. Elmasri, S. K. Gadia, P. Hayes, and S. Jajodia (eds).
A glossary of temporal database concepts.
SIGMOD Record, 23(1), March 1994.
12] Paris C. Kanellakis, Gabriel M. Kuper, and Peter Z. Revesz.
Constraint query languages.
In Proc.
9th ACM Symp.
on Principles of Database Systems (PODS), pages 299{313, Nashville,  1990.
13] Itay Meiri.
Combining qualitative and quantitative constraints in temporal reasoning.
In AAAI, pages 260{267, 1991.
14] Raghu Ramakrishnan.
Magic Templates: A spellbinding approach to logic programs.
In Proc.
Int.
Conf.
on Logic Programming, pages 140{159, 1988.
15] Raghu Ramakrishnan, Divesh Srivastava, S. Sudarshan, and P. Seshadri.
Implementation of the CORAL deductive database system.
In Proc.
ACM Conf.
on Management of Data (SIGMOD), 1993.
16] Peter J. Stuckey and S. Sudarshan.
Compiling query constraints.
In Proc.
ACM Symp.
on Principles of Database Systems (PODS), 1994.
17] Adbullah Uz Tansel, James Cliord, Shashi K. Gadia, Sushil Hajodia, Arie Segev, and Richard Snodgras.
Temporal Databases: Theory, Design and Implementation.
Benjamin/Cummings Publishing Company, Inc., 1993.
18] David Toman, Jan Chomicki, and David S. Rogers.
Datalog with integer periodicity constraints.
In Proc.
Int.
Logic Programming Symposium, November 1994.
19] J. Vaghani, K. Ramamohanarao, David Kemp, Z. Somogyi, and Peter Stuckey.
Design overview of the Aditi deductive database system.
In Proc.
7th Int.
Conf.
on Data Engineering, pages 240{ 247, 1991.
Inferred Validity of Transaction-Time Data Cristina De Castro  Maria Rita Scalas  C.I.O.C.-C.N.R.
and Dipartimento di Elettronica, Informatica e Sistemistica Universita di Bologna Viale Risorgimento 2, I-40136 Bologna, Italy Tel.
+ 39 (51) 644.
{3542,3544} Fax: +39 (51) 644.3540 E-mail: {cdecastro,mrscalas}@deis.unibo.it Abstract: Temporal databases can support at least two kinds of independent time dimensions: transaction-time, which tells when an event is recorded in a database, and valid-time, which tells when an event occurs, occurred or is expected to occur in the real world.
According to the semantics of transaction-time, when data are retrieved from a transaction-time relation, the only temporal information that can be associated to such data is the time when they were stored, updated or deleted.
No conjecture on their validity in the real world is straightforward.
Nevertheless, when transaction-time data are used, a validity is implicitly assigned to them.
In this paper we propose three possible interpretations of the validity of transaction-time data.
Such proposals can be useful in a temporal heterogeneous environment, where relations of different temporal format must interoperate: as a matter of fact, the proposed solutions allow a conversion of temporal data to the bitemporal format, and thus they provide a common format for the execution of the operations.
All the three proposed solutions assume that data can be considered valid at least since they were stored.
Such semantics is intrinsic of transaction-time DBs, which are accordingly named historical, thus we can say that, when data validity is not specified, the knowledge actually stored in the database is taken into account.
The second assumption is that any conjecture on the  effective beginning of the validity of data preceding their insertion time would create false information and must thus be avoided.
These criteria fix the beginning of the inferred validity.
The end of validity can be defined in three distinct ways, depending on how much it can span along the valid-time axis.
A comparison among the three types of inference is provided at the end, on the basis of the possible use of such data, because the inferred validity can be used for the translation of data both to the valid-time or the bitemporal format.
The differences among the three inferences look remarkable.
Keywords: Transaction-Time, Inferred Valid-Time Extension  Valid-Time,  1.
Introduction and Notation In current bibliography on temporal databases there is a common agreement for the support of at least two kinds of time dimensions: transactiontime, which tells when an event is recorded in a database, and valid-time, which tells when an event occurs, occurred or is expected to occur in the real world [Soo91,TSC+93].
Further proposals include the temporal dimension event-time [KC93], which allows the distinction between retroactive and proactive updates, impossible with transaction and valid-time only.
In this paper we are concerned with the possible deductions that can be made when only transaction-time is  supported and thus, for the beginning, we do not take into account event time.
If we consider transaction- and valid-time, according to the temporal dimensions they support, temporal databases can be classified as monotemporal (transaction- or valid-time) , bitemporal or snapshot [JCE+94].
Transaction-time DBs record all the versions of data inserted, deleted or updated in successive transactions (current and non current versions).
The temporal information of a transaction-time relation concerns the time when data are recorded, updated or deleted from the database.
In this sense, transaction-time is the database time.
Valid-time DBs maintain the most recently inserted versions of data, each relative to a distinct valid-time interval (current versions only).
The temporal information of a valid-time relation concerns the time when events actually happen in the real world.
In this sense, valid-time is the time of the miniworld to represent.
Bitemporal DBs support both transaction and valid-time and thus maintain all the valid-time versions recorded in successive transactions (current and non current versions).
Snapshot DBs do not support time: they maintain only the most recently inserted (current) version.
When data are retrieved from a database where their validity is not represented, the user implicitly assigns them a validity interval.
For instance, when you get the latest version of the telephone book, you can use the telephone numbers and thus you consider them valid.
This process is, of course, risky, but reflects commonly used deductions made on every type of available data.
In this paper we consider a transaction-time database and propose three distinct ways for inferring data validity from their represented transaction-time.
Even if transaction- and validtime are independent (orthogonal) dimensions, the above considerations justify this type of inference.
Not only the proposed methods give criteria for making inferences on the validity of data, but also they allow the conversion of data to the bitemporal format.
This use is necessary in a temporal heterogeneous environment, where the interoperability is required of relations of different temporal format [DGS94,DGS95].
The temporal representation adopted in this paper is the Bitemporal Conceptual Data Model (BCDM in [JCE+94]), where time-stamps are represented by Temporal Elements.
In the following, we provide a concise description of the BCDM formalism and of the adopted notations.
Time is represented by means of temporal elements, which consist of sets of chronons.
As in [JCE+94] a chronon is a non-decomposable time interval of some fixed, minimal duration.
A duration p represents the chosen granularity of time.
A (bi)temporal chronon is an ordered pair of unitary-length chronons, one relative to transaction-time, the other to valid-time: for instance (ti, tj) is a bitemporal chronon, where ti denotes transaction-time, and tj denotes validtime.
Bitemporal elements are sets of bitemporal chronons of the type tb = {(ti, tj), ... , (tl, tm)}.
In general, the symbol tX denotes a transaction-time (tt), valid-time (tv) or bitemporal (tb) element.
A monotemporal element can always be represented by the union of disjoint component subsets, each represented by its endpoints (IN, OUT for transaction-time, FROM, TO for validtime) and containing contiguous chronons only: every chronon tj, such that tmi <= tj <= tni, belongs to the component subset {tmi .. tni}.
For instance, the general representation of a transaction-time temporal element is tt = [?
]i tti = [?
]i {tmi .. tni}.
The paper is organized as follows: in section 2 we present the three inferences, discuss the criteria on which they are based and provide some examples.
In section 3 we carry on the discussion and focus the attention on the different results obtained when the data produced by each inference are selected at distinct transaction-time instants.
2.
Three Inferences on the validity of transaction-time data In this section we present the three different inferences, named Square Inference, Stripe Inference and L-Shaped Inference respectively, and discuss the criteria on which each of them is based.
A comparison among the three is carried on.
The common assumption concerns the  beginning of validity of data: transaction-time (historical) data can be considered valid from the instant they were recorded.
Probably such validity precedes this instant, but, in the absence of further information, it would be absolutely unsafe and arbitrary to make further conjectures.
The difference among the three solutions is in the extent data validity is allowed to span along the valid-time axis.
The following notation is adopted: if the non-temporal attributes are denoted by r and [?]
denotes the operation of tuple concatenation, a version of an object along the temporal dimension X can be expressed as rX = r [?]
(tX).
*  Transaction-time semi-axis: {T0 ..
T[?]}
*  Valid-time semi-axis: {t0 ..
t[?]}
*  Current transaction-time: Tnow  *  If rt is a transaction-time record, rb' , rb'' and rb''' will denote the results in the bitemporal format obtained by using the first, second and third inference made on the validity of rt.
The transaction-time relation T-Employee in Tab.1 will be used in the examples.
For the sake of simplicity, in the examples the transaction-time temporal elements have a single component.
The granularity of time chosen for the examples is one year, thus, for instance, {90 .. 92} starts at the beginning of 1990 and finishes at the end of 1992.
2.1.
Square Inference The criterion on which the Square Inference is based is that when data are retrieved from a transaction-time relation they can be considered valid no less and no more than in their transaction-time interval.
This type of inference is represented in Fig.1.
The Square-inferred valid-time pertinence equals the transaction-time temporal element of each record: the transaction-time record r [?]
(tt) is thus transformed to the bitemporal format as follows:  r [?]
(tt) - r [?]
(tt) [?]
(tv) inferred validity is:  where the tv [?
]def tt  If tt is the union of disjoint intervals tt = [?
]i tti, the above definition must be applied to each component transaction-time temporal element tti: r [?]
{[?
]i tti} - r [?]
( [?
]i tti) [?]
( [?
]i tvi ) where tvi [?
]def tti 2.2.
Stripe Inference The criterion on which the Stripe-Inference is based is that when data are retrieved from a transaction-time relation they can be considered valid since they were stored and indefinitely valid in their transaction-time interval.
The Stripe inference reconstructs the bitemporal pertinence of each record taking into account that every record was current before it was updated.
Therefore, when a record had not been archived yet, it could be considered undefinitely valid.
This type of inference is represented in Fig.2.
The Stripe-inferred valid-time pertinence spans the whole valid-time axis starting from the minimum chronon of the transaction-time temporal element of each record: the transactiontime record r[?
](tt) is thus transformed as follows: where r [?]
(tt) - r [?]
(tt) [?]
(tv) def tv [?]
{min {tt} ..
t[?]}
Again, if tt = [?
]i tti , the above definition must be applied to eachtti: r [?]
( [?
]i tti) - r [?]
( [?
]i tti) [?]
( [?
]i tvi) tvi [?
]def {min {tti} ..
t[?]}
where  2.3.
L-Shaped inference A transaction-time tuple r [?]
(tt) is said to be current if Tnow [?
]tt (in this case OUT = T[?
]); it is said to be archived if Tnow >= min{tt}+1 (in this case OUT < T[?]).
A current tuple r [?]
({IN ..
T[?]})
(e.g.
r3 in Fig.3) can be considered valid since it  NAME  JOB  SALARY  tt  Ann  Engineer  2800  {85 .. 90}  Ann  Manager  3000  {91 ..
T[?]}
John  Engineer  1500  {90 .. 92}  John  Engineer  2000  {93 ..
T[?]}
Table 1: transaction-time relation T-Employee Valid-Time Axis T0 r0  T[?]
t2 = T2 t3 = T3  r 0 'b  T1  T1 r1  T2 T3  t1= T1  T0  T2 r2  r 1 'b  T3  r3  r 2 'b r 3 'b  T[?]
T[?]
Transaction-Time Axis a)  b)  Figure 1: (a) transaction-time pertinence; (b) corresponding Square-inferred validity  NAME  JOB  SALARY  tb = tt x tv  Ann  Engineer  2800  {85 .. 90} x {85 .. 90}  Ann  Manager  3000  {91 ..
T[?]}
x {91 ..
t[?]}
John  Engineer  1500  {90 .. 92} x {90 .. 92}  John  Engineer  2000  {93 ..
T[?]}
x {93 ..
t[?]}
Table 2: Square-inferred validity of T-Employee  Valid-Time Axis T0  T0  t1 = T1  r0  r 0 ''  T1  b  T1 r1  r 1 ''  T2  b  T2 r2  T3  T[?]
t2 = T2 t3 = T3  r 2 ''  T3  b  r3  r 3 '' b  T[?]
T[?]
Transaction-Time Axis a)  b)  Figure 2: (a) transaction-time pertinence; (b) corresponding Stripe-inferred validity  NAME  JOB  SALARY  tt x tv  Ann  Engineer  2800  {85 .. 90} x {85 ..
t[?]}
Ann  Manager  3000  {91 ..
T[?]}
x {91 ..
t[?]}
John  Engineer  1500  {90 .. 92} x {90 ..
t[?]}
John  Engineer  2000  {93 ..
T[?]}
x {93 ..
t[?]}
Table 3: Stripe-inferred validity of T-Employee  T0  T0 r0  T1  r 0 ''' b T1  r1 T2 T3  t1 = T1  Valid-Time Axis t2 = T2 t3 = T3 T[?]
T2 r2  T3  r3 T[?]
Transaction-Time Axis a)  r 1 ''' b r 2 ''' b r 3 ''' b  T[?]
b)  Figure 3: (a) transaction-time pertinence; (b) corresponding L-shaped inferred validity  Valid-Time Axis T0  t1= T1  T0 r0  T [?]
t2 = T2 t3 = T3  r 0 'b  T1  T1  T = T* r1 T2 T3  r 1 'b  T2 r2  r3 T = Tnow T [?]
Transaction-Time Axis  r 3 'b T [?]
T0  T0  r 2 'b  T3  t1 = T1  r0  r 0 ''b  T1  T1  T [?]
t2 = T2 t3 = T3  T = T* r 1 '' b  r1 T2  T2 T3  r2  r 2 ''b  T3  r 3 ''b  r3 T = Tnow T [?]
T [?]
T0  T0 r0  T1  t1 = T1  T[?]
t2 = T2 t3 = T3  r 0 ''' b T1  T = T* r1 T2 T3  T2 r2  T3  r 1 ''' b r 2 ''' b  r3 T = Tnow T[?]
r 3 ''' b T[?]
Figure 4: projection along the valid-time axis of the inferred validity at T = T* < T[?]
and at T = Tnow  was stored and indefinitely valid, since it cannot be forecasted if an update transaction would ever occur and archive such tuple.
An archived tuple r [?]
({IN .. OUT}) can be considered as created by a transaction with effect in [IN ..
T[?])
x [IN ..
T[?])
and modified by a successive transaction with effect in [OUT ..
T[?])
x [OUT ..
T[?]).
The update transaction cuts the initial time pertinence of the tuple to an ``L-shaped'' region (e.g.
r0b''', r1b''', r2b''' in Fig.3 and their corresponding ones).
This inference criterion was proposed in [DGS93] and fits the bitemporal view of a diagonal user (for the concept of user see [BG93]).
As in the Stripe-inferred case, the L-shaped inferred valid-time pertinence spans the whole valid-time axis starting from the minimum chronon of the transaction-time temporal element of each record; The difference is that this second criterion takes into account that, before a record was updated it were unknown if it would have been updated and when.
As a consequence, before the update, not only could the valid-time interval be interpreted as covering the whole valid-time axis from the time IN, but the same hold for the transaction-time pertinence of the record itself (OUT = T[?]
before the update).
The transactiontime record r [?]
(tt) is transformed as follows: r [?]
(tt) - r [?]
(tb), where tb [?
]def {min{tt} ..
T[?]}
x {min{tt} ..
t[?]}
{max{tt}+1 ..
T[?
]}x {max{tt}+1 ..
t[?]}
When r [?]
(tt) is current, {max{tt}+1 ..
T[?
]}x {max{tt}+1 ..
t[?]}
= [?]
Again, if tt = [?
]i tti , the above definition must be applied to each tti: r [?]
( [?
]i tti) - r [?]
( [?
]i tbi) where tbi [?
]def {min{tti} ..
T[?
]}x {min{tti} ..
t[?]}
- {max{tti}+1 ..
T[?
]}x {max{tti}+1 ..
t[?]}
3.
Further Discussion and Conclusions  In this paper we have considered how data validity can be inferred in transaction-time databases.
We proposed three distinct solutions: the Square Inference, the Stripe Inference and the L-Shaped Inference.
The Square Inference reflects only the knowledge of the database: data are valid within their transaction-time interval.
The StripeInference takes into account that archived data were current before they were updated and, before the update, data could be considered indefinitely valid.
A further deduction is made in the L-shaped solution: a portion of data is considered as still current.
As far as the use of the inferred data is concerned, a distinction must be made between their use in the bitemporal or in the valid-time format.
It can be noticed that the Square Inference can be used not only for the bitemporal view of data, but also for the valid-time view, since the Square-inferred valid-time intervals do not overlap; on the contrary, the other two solutions can be used only in the bitemporal format, since the Stripe- or Lshaped- inferred valid-time intervals overlap on the valid-time axis.
Furthermore, if we consider the bitemporal representations in Figs.1, 2, 3 and project (see Fig.4) the valid-time data inferred in each solution at T = T* < T[?]
and at T = Tnow, we find out another important difference.
At the generic time T = T* < T[?
], the Square Inference and the Stripe Inference return only the data whose original transaction-time pertinence contains T*, i.e.
they return the information which was available in the considered transaction-time interval; the L-shaped inference returns all the data whose transactiontime pertinence contains or precedes T*, i.e.
they return all the information which was available at time T*, independently of the transaction-time pertinence of such data.
If the projection is performed at the current time Tnow, the Square inference and the Stripe inference return only the current data, whereas the L-shaped inference returns all the original data, both current and archived.
These considerations can guide the use of the data produced in each type of inference.
The Square method is most in  harmony with the semantics of both transactionand valid-time, thus it allows to use the inferred data for the translation to both the bitemporal- and the valid-time format in quite a safe way, just being aware that the validity were not explicitly defined.
Also the Stripe method returns, for each transaction-time instant, only the data which were current at that time.
In this sense, it is in harmony with the semantics of transaction-time.
On the other hand, the inferred data can not be used with no transaction-time reference, because they overlap along valid-time.
The L-shaped inference, if used with no transaction-time reference, is the most risky of the three.
Its use can be justified by the fact that transaction-time can only grow, thus no retroactive insertion is possible.
As a consequence, the original transaction-time data are, in each interval, the only one version which was ever recorded and thus, in this sense, the most recently inserted one.
References: [BG93] Bhargava G., Gadia S.K., ``Relational Database Systems with Zero Information Loss'', IEEE Trans.
on Knowledge and Data Engineering, Vol.
5, No.
1, Feb. 1993.
[DGS94] De Castro C., Grandi F., Scalas M.R.
: ``Semantic Interoperability of Multitemporal Relational Databases'', in Entity-Relationship Approach - ER '93, Lecture Notes in Computer Science, Vol.
823, Springer-Verlag, 1994.
[DGS94] De Castro C., Grandi F., Scalas M.R.
: ``Meaning of Relational Operations in Temporal Environment'', accepted for presentation at the Basque International workshop on Information Technology (BIWIT 95), to be held in San Sebastian (Spain), July 1995.
[JCE+94] Jensen C., Clifford J., Elmasri R., Gadia S.K., Hayes P., Jajodia S. (editors), Dyreson C., Grandi F., Kafer W., Kline N., Lorentzos N., Mitsopoulos Y., Montanari A., Nonen D., Peressi E., Pernici B., Roddick J.F., Sarda N.L., Scalas M.R., Segev A., Snodgrass R., Soo M.D., Tansel A., Tiberio P., Wiederhold G.: ``A Consensus Glossary of Temporal Database Concepts", SIGMOD RECORD, Vol.
23, No.
1, March 1994 [KC93] Kim S.K., Chakravarthy S.: ``Modeling Time: Adequacy of Three Distinct Time Concepts for Temporal Databases", Proc.
of 12th International Conference on  Entity-Relationship Approach, Arlington, December 1993, also in Lecture Notes in Computer Science, Springer-Verlag.
[Soo91] Soo M., ``Bibliography on Temporal Databases,'' ACM SIGMOD Record, Vol.
20, No.
1, Mar.
1991.
[TSC+93] Tansel A., Snodgrass R., Clifford J., Gadia V., Segev A.
(eds), Temporal Databases: Theory, Design and Implementation, The Benjamin/Cummings Publishing Company, Redwood city, California, 1993.
Mapping Calendar Expressions into Periodical Granularities* Claudio Bettini Sergio Mascetti DICo - University of Milan, via Comelico 39, I-20135 Milan, Italy Abstract An effort has been devoted in the recent years to study and formalize the concept of time granularity and to design applications and services using the formalization.
Among other proposals, a calendar algebra has been defined to facilitate the specification of new granularities and to perform conversions among them.
This paper shows how granularities defined as algebraic calendar expressions can be represented as periodical sets of instants.
More precisely, the paper shows how each algebraic operator changes the periodical structure of the granularities given as operands.
These results have an immediate application enabling users to easily specify new granularities and using them in the only constraint solver supporting time granularities that is currently available.
1.
Introduction Temporal information in applications is often expressed differently for different purposes.
For internal representation purpose, integers and integer intervals are a common choice.
For example, in UNIX systems, time is expressed internally as the number of seconds elapsed since Jan 1, 1970.
Other examples include temporal databases [EJS98], where integer intervals are often used internally as timestamps, and temporal contraint propagation [BWJ02], where sets of integer intervals are used.
On the other hand, when presented to users, temporal information is best expressed in terms of commonly used, organization-specific, or even user-specific calendars.
A particular time such as "Wed Feb 11 21:56:32 EST 2004", expressed in the common calendar, is much easier for a human user than "1,076,554,592 seconds since 00:00:00 1970-01-01 UTC" is, while the latter is easier for computers to deal with.
The same holds for "every other Friday" *  This work has been partially supported by Italian MIUR (FIRB "WebMinds" project N.RBNE01WEJT 005).
Wang's work has also been supported by US NSF under the career award 9875114.
X. Sean Wang Department of Computer Science Univ.
of Vermont, Burlington, VT, USA versus a set of intervals of integers, each being a day, that gives all these Fridays.
In this paper, we consider the calendar algebra [NWJ02] as a symbolic representation tool at the user level.
The algebra can be used to easily define commonly used granularities like day, week, month, and year, as well as specialized granularities like every other Friday, and first week of all semesters.
We develop a method to convert calendar algebra expressions to internal integer interval representations (formally defined later).
As a particular application of our results, users of the only system currently available for solving networks of temporal constraints with granularities (GSTP, [BWJ02]) can use calendar algebra expressions to define granularities appearing in the constraints.
The main contribution of the paper is that we show how to map an algebraic expression defining a time granularity into a periodic set representation of the same granularity.
In particular, for each algebraic operator, the periodicity of the set corresponding to its application is derived from the periodicity of the argument expression.
In the paper we also highlight a problem with the definition of the altering tick algebra operator in [NWJ02], and we propose a solution.
Despite several formalisms have been proposed for symbolic representation of granularities and periodicity (among which [LMF86, Nie92, NWJ02, Ter03]), and some work has been done on comparing and enhancing the expressive power of some of them (e.g., [BD00]), no mapping was provided in these papers to identify how each operator changes the mathematical characterization of the periodicity of the argument expressions.
The problem of finding these mappings is not trivial for some operators, and its solution enables interesting applications based on the mathematical characterization of periodic expressions.
In the next section we define granularities and discuss their internal symbolic representation.
In Section 3 we outline our main task of conversion, identifying the periodicity for each algebraic operator.
The derivation of the integer intervals corresponding to explicit granules in one period is shown in Section 4 and we conclude in Section 5.
2.
Granularities and Their Symbolic Representation Time granularities include very common ones like hours, days, weeks, months and years, as well as the evolution and specialization of these granularities for specific contexts or applications.
Trading days, banking days, and academic semesters are just few examples of specialization of granularities that have become quite common when describing policies and constraints.
A comprehensive formal study of time granularities and their relationships can be found in [BJW00].
In this paper, for lack of space we only introduce notions that are essential to show our results.
In particular, we report here the notion of labeled granularity which was proposed for the specification of a calendar algebra [BJW00, NWJ02].
Granularities are defined by grouping sets of instants into granules.
For example, each granule of the granularity day specifies the set of instants included in a particular day.
A label (or index) is used to refer to a particular granule.
The whole set of time instants is called time domain, and for the purpose of this paper the domain can be an arbitrary infinite set with a total order relationship, <=.
Definition.
A labeled granularity is a pair (L, G), where L is a subset of the integers, and G is a mapping from L to the subsets of the time domain such that for each pair of integers i and j in L with i < j, if G(i) 6= [?]
and G(j) 6= [?
], then (1) each element in G(i) is less than every element of G(j), and (2) for each integer k in L with i < k < j, G(k) 6= [?].
When L is exactly the integers, the granularity is called "full-integer labeled".
When L = Z+ we have the same notion of granularity as used in several applications (e.g., [BWJ02]).
For example, following this labeling schema, if we assume to map day(1) to the subset of the time domain corresponding to January 1, 2001, day(32) would be mapped to February 1, 2001, b-day(6) to January 8, 2001 (the sixth business day), and month(15) to March 2002.
The generalization to arbitrary label sets has been introduced mainly to facilitate conversion operations in the algebra, however our final goal is the conversion of a labeled granularity denoted by a calendar expression into a "positive-integer labeled" one denoted by a periodic formula.
Related to the notion of labeled granularity is the one of label-aligned subgranularity.
Formally, G1 is a labelaligned subgranularity of G2 if the label set LG1 of G1 is a subset of the label set LG2 of G2 and for each i in LG1 such that G1 (i) 6= [?
], we have G1 (i) = G2 (i).
Intuitively, G1 has a subset of the granules of G2 and those granules have the same label in the two granularities.
Several interesting relationships can be defined among granularities.
The first of these is called group into and defines a partial order over the set of all granularities.
If G and H are granularities, then G is said to group into H, denoted G / H, if for each non-empty granule H(j), there exists a (possibly infinite) set S of labels of G such that S H(j) = i[?
]S G(i).
Intuitively, G / H means that each granule of H is a union of some granules of G. For example, day / week since a week is composed of 7 days and day / b-day since each business day is a day.
Granularities are said to be bounded when L has a first or last element or when G(i) = [?]
for some i [?]
L. We assume the existence of an unbounded bottom granularity, denoted by G[?]
which is full-integer labeled and groups into every other granularity in the system.
We also say that G partitions H if G / H and there are no granules of G other than those included in granules of H. For example, both day and b-day group into b-week, but day does not partition b-week, while b-day does.
Since we are particularly interested in granularities which can be expressed as periodic repetitions of granules of other granularities (in particular a bottom granularity), we formally define the following relationship1 Definition.
A labeled granularity G groups periodically into a labeled granularity H if G groups into H and there exist positive integers N and P such that (1) for each label i of H, i + N is a label of H unless i + N is greater than the greatest label of H, and (2) for each label i of H, if Sk H(i) = r=0 G(jr ) and H(i + N ) is a non empty granSk ule of H then H(i + N ) = r=0 G(jr + P ), and (3) if H(s) is the first non-empty granule in H (if exists), then H(s + N ) is non empty.
Intuitively, condition (1) says that labels of H have N as a periodic pattern (i.e., if i [?]
LH , then i + a * N [?]
LH with a [?]
Z if H is unbounded), condition (2) says that the granules of H repeat along the time domain by "shifting" each granule by P granules of G, and condition (3) simply says that there is at least one of these repetitions.
Several examples will be shown in Section 3.
We call each pair P and N in Definition 2, a period and its associated period label distance.
Note that period and period label distance are not unique.
More precisely, we inG G dicate with PH the period of H in terms of G and with NH the period label distance of H in terms of G; the form PH and NH is used when G = G[?]
.
In general, this relationship guarantees that granularity H can be finitely described (in terms of granules of G) providing the following information: (i) a value for P and N ; (ii) the set L of labels of H in one period of length P ; (iii) 1  This is simply an extension to labeled granularities of the analogous relation defined for "regular" granularities (see e.g., [BJW00]).
for each j [?]
L, the finite set Sj of labels of G, describing the composition of H(j); (iv) the labels of first and last non-empty granules in H, if their values are not infinite.
If L is the set of labels of H with values in {b, .
.
.
, b + N - 1}, and we assume H to be unbounded, the description of an arbitrary granule j [?]
LH can be obtained by the following formula.
Given j 0 = [(j - 1) mod N ] + 1 and  fi b-1  fi  * N + j0 if b-1 * N + j0 >= b  N N k=   fi b-1  + 1 * N + j 0 otherwise N  corresponds to Monday, i.e., the first day of a week.
Note that G partitions G0 , and G0 is also a full-integer labeled granularity.
Proposition 1 If G0 = Groupm (G), then PG0 = m * PG and NG0 = NG Example 1 Figure 1 shows an example of the grouping operation: granularity day is the bottom granularity and its period is 1; week is defined as Group7 (day), so it has a period of 7 days.
we have  H(j) =  [  i[?
]Sk         j-1 k-1 G G G PH * + i - PH * .
N N  Day Week  1  2  3  4  1  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21  2  3  Figure 1.
Group operation example  3.
Deriving periodicity from algebraic operations We now briefly illustrate each calendar algebra operation, identifying a pair of values P and N that characterize the periodicity of the granularity resulting from its application.
Since we assume to start from the bottom granularity G[?]
, which is trivially periodic in terms of itself, these results applied recursively to a calendar expression, allow us to characterize the periodicity of the resulting granularity in terms of G[?]
.
With respect to the algebra defined in [NWJ02] we need to limit the altering tick operation to finite values (no [?]
in its parameters), and we exclude the subset operation if not simply applied as the last operation in an algebraic expression.
In the following, symbols G0 , G, G1 , and G2 stand for granularities.
G, G1 and G2 are usually operand granularities, while G0 is the resulting granularity; Symbols LG0 , LG , LG1 , and LG2 stand for the label set associated to G0 , G, G1 , and G2 , respectively; Symbols PG0 , P , PG1 , and PG2 stand for periods of G0 , G, G1 , and G2 respectively; NG0 , NG ,NG1 , and NG2 are the period label distances of G0 , G, G1 , and G2 , respectively.
3.1.
The grouping operation Let G be a full-integer labeled granularity, and m a positive integer.
The grouping operation Groupm (G) gives the granularity G0 such that for each integer i, G0 (i) =  i*m [  G(j).
j=(i-1)*m+1  For example, given granularity day, week can be generated by week = Group7 (day) if we assume that day(1)  3.2.
The altering-tick operation Let G1 , G2 be full-integer labeled granularities, and l, k, m integers, where G2 partitions G1 , and 1 <= l <= m. The m altering-tick operation Alterl,k (G2 , G1 ) generates a new granularity by periodically expanding or shrinking granules of G1 in terms of granules of G2 .
Since G2 partitions G1 , each granule of G1 consists of some contiguous granules of G2 .
The granules of G1 can be partitioned into mgranule groups such that G1 (1) to G1 (m) are in one group, G1 (m+1) to G1 (2m) are in the following group, and so on.
The goal of the altering-tick operation is to modify the granules of G1 so that the l-th granule of every aforementioned group will have |k| additional (or fewer when k < 0) granules of G2 .
For example, if G1 represents 30-day groups (i.e., G1 = Group30 (day)) and we want to add a day to the third month every 12 month (i.e., to make March to have 31 12 days), we may perform Alter3,1 (day, G1 ).
Specifically, for all i = l + m * n, where n is an integer, G1 (i) denotes the granule to be shrunk or expanded.
The granules of G1 are split into two parts at G1 (0).
When i > 0, G1 (i) expands (or shrinks) by taking in (or pushing out) later granules of G2 , and the effect is propagated to later granules of G1 .
On the contrary, when i <= 0, G1 (i) expands (or shrinks) by taking in (or pushing out) earlier granules of G2 , and the effect is propagated to earlier granules of G1 .
The altering-tick operation can be formally described as follows.
For each integer i such that G1 (i) 6= [?
], let bi and i ti be the integers such that G1 (i) = [?
]tj=b G2 (j).
(The i integers bi and ti exist because G2 partitions G1 .)
Then G0 = Alterm l,k (G2 , G1 ) is the granularity such that for each integer i, let G0 (i) = [?]
if G1 (i) = [?
], and otherwise let  G0 (i) = b0i  =  St0i  j=b0i    G2 (j) where:  bi + (h - 1) * k, if i = (h - 1) * m + l bi + h * k, otherwise  and t0i = ti + h * k, having h = b i-l m c + 1.
The original definition of altering-tick given in [NWJ02] as reported above, has the following problems when an arbitrary negative value for k is used: (1) It allows the definition of a G0 that is not a full integer labeled granularity and (2) It allows the definition of a G0 that does not even satisfy the definition of granularity.
After considering several options, we decided that a reasonable restriction that avoids these undesired behaviors is imposing  Combine(G1 , G2 ) generates a new granularity G0 by combining all the granules of G2 that are included in one granule of G1 into one granule of G0 .
More formally, for each i [?]
LG1 , let s(i) = [?]
if G1 (i) = [?
], and otherwise let s(i) = {j [?]
LG2 |[?]
= 6 G2 (j) [?]
G1 (i)}.
Then G0 = Combine(G1 , G2 ) is the granularity with the label set LG0 =S{i [?]
LG1 |s(i) 6= [?]}
such that for each i in LG0 , G0 (i) = j[?
]s(i) G2 (j).
As an example, given granularities b-day and month, the granularity for business months can be generated by b-month = Combine(month, b-day).
k > -(mindist(G1, 2, G2) - 1)  Proposition 4 If G0 = Combine(G1 , G2 ), then: PG0 = lcm(PG1 ,PG2 )NG1 lcm(PG1 , PG2 ) and NG0 = PG  where mindist() is formally defined in [BJW00].
Intuitively, mindist(G1, 2, G2) represents the minimum distance (in terms of granules of G2) between two consecutive granules of G1.
Example 3 Figure 3 shows an example of the combining operation: G0 = Combine(W eek, G2 ).
Day is the bottom granulairty, the period of week is 7 and the period of G2 is 3, therefore the period of G0 is 21.
1  m Proposition 2 If G0 = Alterl,k (G2 , G1 ), then: PG0 = m * PG1 * PG2 * NG2 + k * NG1 * (PG2 )2 and NG0 = m * NG1 * NG2 * PG2 .
Example 2 Figure 2 shows an example of the altering tick operation: day is the bottom granularity; G1 = Group3 (Day) and G0 is defined as 2 Alter2,-1 (Day, G1 ).
The period of G0 is 5 and NG0 = 2.
Day G1 G'  1  2  3  4  5  1 1  6  7  2 2  8  Day  1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21  1  Week G2  3  1  2  3  1  2 4  5  6  7  8  2  3 9 10  11 12  13 14  3  G'  Figure 3.
Combining operation example  9 10 11 12 13 14 15 16 17 18 19 20 21  3 3  4 4  5 5  6 6  7  7 8  Figure 2.
Alter-tick operation example  3.3.
Shifting operation Let G be a full-integer labeled granularity, and m an integer.
The shifting operation generates a new granularity G0 = Shiftm (G) such that for each integer i, G0 (i) = G(i + m).
Note that G0 is full-integer labeled.
Proposition 3 If G0 = Shif tm (G1 ), then PG0 = PG1 and NG0 = NG1 .
3.4.
Combining operation Let G1 and G2 be granularities with label sets LG1 and LG2 respectively.
The combining operation  Anchored grouping operation Let G1 and G2 be granularities with label sets LG1 and LG2 respectively, where G2 is a label-aligned subgranularity of G1 , and G1 is a full-integer labeled granularity.
The anchored grouping operation Anchored-group(G1 , G2 ) generates a new granularity G0 by combining all the granules of G1 that are between two granules of G2 into one granule of G0 .
More formally, G0 = Anchored-group(G1 , G2 ) is the granularity with the label set LG0 = LG2 such that for i0 -1 each i [?]
LG0 , G0 (i) = [?
]j=i G1 (j), where i0 is the next label of G2 after i.
Proposition 5 If G0 = Anchored-group(G1 , G2 ), then lcm(PG1 ,PG2 )*NG2 PG0 = lcm(PG1 , PG2 ) and NG0 = PG 2  Example 4 Figure 4 shows an example of the anchored grouping operation: the USweek (i.e., a week starting with a Sunday) is defined by the operation anchored-group(day, Sunday).
Using day as the bottom granularity, the period of USWeek is 7.
Day  1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21  7  Sunday USWeek  7  0  14 7  21 14  Figure 4.
Anchored grouping operation example  3.5.
Subset operation The subset operation is designed to generate a new granularity by selecting an interval of granules from another granularity.
Let G be a granularity with label set L, and m, n integers such that m <= n. The subset operation G0 = Subsetnm (G) generates a new granularity G0 = Subsetnm (G) with the label set LG0 = {i [?]
L | m <= i <= n}, and for each i [?]
LG0 , G0 (i) = G(i).
Note that G0 is a label-aligned subgranularity of G, and G0 is not a full-integer labeled granularity even if G is.
We also allow the extensions of setting m = -[?]
or n = [?]
with semantics properly extended.
This operation returns a finite granularity.
It may still be periodic, with same period as its argument granularity, if it includes granules that cover exactly a multiple of its period, provided that we introduce bounds for the description of the periodic granularity.
Clearly, these bounds are determined by the values of m and n when different from -[?]
and +[?
], respectively.
However, in general, either the operation will lead to a finite non-periodic granularity (whose granules are all explicitly described), or to a finite semiperiodic granularity [BJW00], where the periodic part has the same period as its argument granularity.
3.6.
Selecting operations There are three selecting operations: select-down, selectup and select-by-intersect.
To facilitate the description of these operations, the [?
]lk (S) notation is used.
Intuitively, if S is a set of integers, [?
]lk (S) selects l elements starting from the k-th one.
Select-down operation.
For each granule G2 (i), there exits a set of granules of G1 that is contained in G2 (i).
The operation Select-downlk (G1 , G2 ), where k 6= 0 and l > 0 are integers, selects granules of G1 by using [?
]lk (*) on each set of granules (actually their labels) of G1 that are contained in one granule of G2 .
More formally, G0 = Select-downlk (G1 , G2 ) is the granularity with the label set LG0 = [?]i[?
]LG2 [?
]lk ({j [?]
LG1 | [?]
= 6 G1 (j) [?]
G2 (i)}),  eration generates a new granularity G0 by selecting the granules of G1 that contain one or more granules of G2 .
The Select-by-intersectlk (G1 , G2 ) operation, where k 6= 0 and l > 0 are integers, selects granules of G1 by applying for each granule G2 (i) the [?
]lk (*) operator to the set of labels of LG1 such that the corresponding granules intersect G2 (i).
In both cases, G0 is a label-aligned subgranularity of G1 .
Proposition 6 If G0 = Select-downlk (G1 , G2 ) G0 = Select-up(G1 , G2 ) or G0 = Select-by-intersect(G1 , G2 ) then PG0 lcm(PG1 ,PG2 )NG1 lcm(PG1 , PG2 ) and NG0 = .
PG  or =  1  Example 5 Consider a calendar with day as the bottom granularity.
Sunday could be defined as Select-down17 (day, week).
Hence, the period of the granularity Sunday is 7.
3.7.
Set operations The set operations, union, intersection, and difference, can be defined on G1 and G2 only if there exists a granularity H such that G1 and G2 are both label-aligned subgranularities of H. Union.
The union operation G1 [?]
G2 generates a new granularity G0 by collecting all the granules from both G1 and G2 .
More formally, G0 = G1 [?]
G2 is the granularity with the label set LG0 = LG1 [?]
LG2 , and for each i [?]
LG0 ,  G1 (i), i [?]
LG1 , 0 G (i) = G2 (i), i [?]
LG2 - LG1 .
Note that G1 and G2 are label-aligned subgranularities of G0 .
Example 6 Given granularities Saturday and Sunday, the granularity WeekendDay can be generated by WeekendDay = Sunday [?]
Saturday.
The operations intersection [?]
and difference (-) are defined similarly.
Since a set operation is valid if the granularities used as argument are both labeled aligned granularity of another granularity, the following property is used.
Proposition 7 If G is a labeled aligned subgranularity of NH G H, then N PG = PH .
Proposition 8 If G0 = G1 [?]
G2 or G0 = G1 [?]
G2 or G0 = G1 - G2 then PG0 = lcm(PG1 , PG2 ) and NG0 = lcm(PG1 ,PG2 )NG1 lcm(PG1 ,PG2 )NG2 = .
PG PG 1  2  and for each i [?]
LG0 , G0 (i) = G1 (i).
4.
Completing the periodical characterization  Select-up and Select-by-intersect operations.
Two analogous operations can be defined.
The Select-up(G1 , G2 ) op-  In Section 3 we have derived for each operation the values P and N characterizing the periodicity of the resulting  granularity in terms of one of the operand granularities.
In order to fully characterize the granularity we still have to provide a description of the granules in one of the periods, assuming, of course, that we have one for the operand granularities.
Moreover, we should identify possible lower and upper bounds.
For what concern the bounds, we know that each operation, except subset, results in an unbounded granularity if the granularities used as arguments are unbounded.
From this consideration and since we assume that the bottom granularity is unbounded, all the granularities that could be generated with the calendar operation (excluding the subset operation) are unbounded.
Since we assume that the subset operation could be only used as the last operation in an algebraic expression, we can assure that for each operation, except the subset, the resulting granularity is unbounded.
For the subset operation it has been indicated in the previous section how to calculate the lower and upper bounds.
Since we do not have to deal with bounds and since the period can be easily obtained, in order to fully characterize a periodical granularity we only need the explicit representation of the granules in one of its periods.
4.1.
Deriving the explicit granules in one period Observe that our algebra operations are always defined in two step, first the label set, and second the granules for each label.
In order to derive the explicit granules in one period, we first need to identify the set of labels LG0 for the granules of the resulting granularity in one period of G0 , and then compute the value of each of them according to the operation definition.
In order to check the relationship among explicit granules as directed by the algebra operation definitions, for each granularity G, we define LG as the set of labels corresponding to granules of G which contain at least one of G[?]
(1), .
.
.
, G[?]
(PG ).2 LG is equal to LG if G[?]
(0) 6[?]
G(min(LG )), otherwise LG = LG - {max(LG )}.
To compute LG0 we still need to extend both explicit granules and labels of each operand granularity to the period of G0 , and then use the operation definitions to derive LG0 and then LG0 .
Given a granularity G with period PG , the corresponding set of labels LG can be extended to a period P by considering P instead of PG in the definition of L. We denote the resulting set with LP G. The idea is to start with LG[?]
= {1}, and then show for each operation how L of the resulting granularity G0 is calculated given the L value (possibly extended to the common period PG0 ) of the operand granularities.
2  Note that the cardinality of LG will be equal to the number of granules in one period or have one extra label (in the case a granule includes both G[?]
(0) and G[?]
(1)).
If G0 = Shif tm (G1 ) then LG0 = {i [?]
Z|i + m [?]
LG1 } If G0 = Groupm (G1 ) then LG0 = {min(LG1 ), .
.
.
, min(LG1 ) + NG0 } m If G0 = AlterT ickl,k (G2 , G1 ) then LG0 = {min(LG2 ), .
.
.
, min(LG2 ) + NG0 } If G0 = Combining(G1 , G2 ) then P P 6 G2 (j) [?]
G1 (i)} [?
]i [?]
LGG1 0 s(i) = {j [?]
LGG2 0 | [?]
= PG0 0 and LG = {i [?]
LG1 |s(i) 6= [?]}
If G0 = select downlk (G 1 , G2 ) then n o S P LG0 = i[?
]LPG0 [?
]lk j [?]
LGG1 0 |[?]
= 6 G1 (j) [?]
G2 (i) G2  If G0 = select n up(G1 , G2 ) then  o P P LG0 = j [?]
LGG1 0 |[?
]j [?]
LGG2 0 ([?]
= 6 G2 (j) [?]
G1 (i)) If G0 = select by intersect(G 1 , G2 ) then n o S P l LG0 = i[?
]LPG0 [?
]k j [?]
LGG1 0 |[?]
= 6 G1 (j) [?]
G2 (i) G2  If G0 = G1 [?]
G2 then P P LG0 = LGG1 0 [?]
LGG2 0 , and similarly for [?]
and -.
Example 7 Consider a calendar having day as the bottom granularity; let week be defined with Pweek = 7, Nweek = 1 and Lweek = {1}.
We define the granularity including all the Saturdays and Sundays (called wday) as wday = select down26 (day, week).
By Proposition 6, Pwday = 7 and Nwday = 7.
In order to compute Lwday Pwday we first need to calculate Lday = {1, 2, 3, 4, 5, 6, 7}, P  wday and Lweek = {1}.
Hence, by definition of the combinPwday ing operation Lwday = [?
]26 (j [?]
Lday |[?]
= 6 day(j) [?]
week(1)) = {6, 7}.
Since LG0 is trivially derived from LG0 , we can calculate the explicit granules in one period by applying the formulas specific to the operation for each label in LG0 .
Referring to Example 7, Lwday = Lwday = {6, 7}, and using the definition of the select down operation we derive wday(6) = day(6) and wday(7) = day(7), which together with the values of Pweek and Nweek fully characterize week.
If we need to characterize all granularities as periodic expressions in terms of the bottom granularity, we can simply recursively apply the steps we have shown for each algebraic operation.
Example 8 Continuing with Example 7, suppose we define weekend as combine(week, wday).
By Proposition 4 Pweekend = 7 and Nweekend = 1; moreover from the formulas to derive L: Lweekend = Lweekend = {1}.
By definition of the combining operation, s(1) = {6, 7}, S and hence weekend(1) = wday(j).
Since j[?
]{6,7} wday(6) = day(6) and wday(7) = day(7), then weekend(1) = day(6) [?]
day(7).
4.2.
Computing arbitrary values from the periodic representation Given LG0 , the explicit granules, and the values of PG0 and NG0 for the granularity G0 derived by an algebraic operation where G is the operand granularity which periodically groups into G0 , we can easily compute the composition of an arbitrary granule G0 (j) with j [?]
LG0 in terms of granules of G using the formula given at the end of Section 2.
Example 9 Referring to Example 7, we know that Lwday = {6, 7} and wday(6) = day(6) and wday(7) = day(7).
wday(14) can be computed using the formula observing that j = 14, b = min(Lwday ) = 6, G N = PH = 7, and S6 = {6} and S7 = {7}.
Then we derive k = j 0 = 7, and hence wday(14) = day(14).
4.3.
Reducing the periodic representation to positive-integer labeled granularities Each granularity obtained from a calendar expression as explained above can be easily converted into a full- or positive-integer labeled granularity in order to satisfy the definition used in some of the existing applications (e.g., GSTP).
The formal definition of this process is described in [BJW00] and is not reported here for lack of space.
Example 10 Referring to Example 7, the granules with the labels 6 and 7 in Lwday are relabeled with 1 and 2, respectively.
While the value of the period remains the same, the value of Nwday is changed into 2, the number of granules in each period.
It is easily seen that the formula provided to compute arbitrary values is still valid.
5.
Conclusions We have shown how, given a calendar algebra expression, we can derive an equivalent granularity characterization in terms of periodic sets of granules of the bottom granularity.
The periods and period label distances we obtain are not always minimal.
We are investigating algorithms to reduce the period value or even to find the minimum values.
The optimal period values have more general applications; for example it would be very useful even to further reduce the representations given by users that directly define their granularities in terms of periodic sets, as it currently happens for the GSTP system [BWJ02].
We are also working on XML versions of algebraic and periodical representations, to enable a rich set of time granularity web services that could be useful for many applications handling time-dependent data.
References [BJW00] C. Bettini, S. Jajodia, and X. Wang.
Time Granularities in Databases, Temporal Reasoning, and Data Mining.
Springer, 2000.
[BD00] C. Bettini, R. De Sibi.
Symbolic Representation of UserDefined Time Granularities, Annals of Mathematics and Artificial Intelligence, 30(1-4):53-92, 2000.
[BWJ02] C. Bettini, X. Wang, S. Jajodia, Solving MultiGranularity constraint networks, Artificial Intelligence, 140(12):107-152, 2002.
[EJS98] O. Etzion, S. Jajodia, and S. Sripada (Eds.)
Temporal Databases: Research and Practice.
Springer-Verlag Heidelberg, Lecture Notes in Computer Science 1380, 1998.
[LMF86] B. Leban, D. Mcdonald, and D. Foster, A representation for collections of temporal intervals, in Proc.
of the American National Conference on Artificial Intelligence, pp.
367- 371, AAAI Press, 1986.
[Nie92] M. Niezette and J. Stevenne, An efficient symbolic representation of periodic time, in Proc.
of International Conference on Information and Knowledge Management, pp.
161- 168, ACM Press, 1992.
[NWJ02] P. Ning, X. Wang, S. Jajodia.
An Algebraic Representation of Calendars.
Annals of Mathematics and Artificial Intelligence 36(1-2): 5-38, 2002.
[Ter03] Paolo Terenziani, Symbolic User-Defined Periodicity in Temporal Relational Databases.
IEEE Trans.
on Knowledge and Data Engineering, 15(2): 489-509, 2003.
2010 17th International Symposium on Temporal Representation and Reasoning  Local Polynomial Regression Models for Average Traffic Speed Estimation and Forecasting in Linear Constraint Databases Hang Yue, Elizabeth Jones* Nebraska Transportation Center Civil Engineering Department University of Nebraska-Lincoln Email: yuehang366@gmail.com *ejones1@unl.edu  Peter Revesz Computer Science & Engineering Department University of Nebraska-Lincoln Email: revesz@unl.cse.edu  difficult to see the intuitive relationship among the important traffic attributes [4].
Time discontinuity is another significant deficiency in relational databases.
Users expect that, regardless of the magnitude of change, the complete snapshot produced at each time slice could duplicate all the unchanged data in the database.
However, relational databases with time discontinuity [3] cannot store the complete information of moving objects, such as vehicles.
Constraint databases are viewed as a special kind of post-relational databases, although they share with relational databases some important features, such as, formal model-theoretic semantics, and various highlevel query languages including SQL and Datalog [5].
Constraint databases have some additional useful features like the ability to represent infinite relations by various types of constraints, to describe continuous temporal and spatiotemporal data in arbitrarily highdimension [6].
Linear equations over rational numbers form a type of constraint in constraint databases (other constraints include linear inequalities over rational numbers or polynomial equations over real numbers).
Once linear equations support the storage of continuous curve, constraint databases can serve as a more useful tool for traffic data archiving and query operations.
As a nonparametric method, local polynomial regression is a real-time curve model without data pre-classification.
It follows the curved tendency of the raw data over the entire estimating region and learns the functions from the dataset [7].
Further, a complicated global regression model can be easily approximated by a local polynomial regression model using the band width and weight.
The aim of our study is to develop the local polynomial regression models to estimate and predict nonlinear average traffic speed with a continuous time-line in linear constraint databases.
The development of these new models means that  Abstract Constraint databases have the specific advantage of being able to represent infinite temporal relations by linear equations, linear inequalities, polynomial equations, and so on.
This advantage can store a continuous time-line that naturally connects with other traffic attributes, such as traffic speed.
In most cases, vehicle speed varies over time, that is, the speed is often nonlinear.
However, the infinite representations allowed in current constraint database systems are only linear.
Our article presents a new approach to estimate and forecast continuous average speed using linear constraint database systems.
Our new approach to represent and query the nonlinear average traffic speed is based on a combination of local polynomial regression and piecewise-linear approximation algorithm.
Experiments using the MLPQ constraint database system and queries show that our method has a high accuracy in predicting the average traffic speed.
The actual accuracy is controllable by a parameter.
We compared the local linear regression model with the local cubic model by using a field experiment.
It was found that the local cubic model follows more closely the raw data than the linear model follows.
1.
Introduction Today relational databases are popularly applied to many traffic systems [1, 2], such as traffic management systems, public transit systems and Advanced Traveler Information Systems (ATISs).
However, when the relational model is used to handle spatial data, the points, lines and polygons in space are discretely saved in tables and often lose some of the important spatial relationships [3].
Having only a finite set of tuples in the relational tables may make it  1530-1311/10 $26.00 (c) 2010 IEEE DOI 10.1109/TIME.2010.24  154  There is a basic difference between a parametric approach and a nonparametric approach.
The former assumes a pre-specified functional form for the density estimator, while the latter does not.
The density estimation in nonparametric regression can effectively describe the overall pattern in a set of data.
Suppose that in a sample of random pairs (x1, y1),..., (xn, yn), the response variable yi is assumed to satisfy [13]:  constraint databases have the capability to model and store continuous nonlinear data.
In addition, constraint databases have far-reaching potentials to evaluate and analyze the information of moving traffic objects (vehicles and pedestrians) on the basis of statistical nonparametric methods.
Section 2 reviews the related literature.
Section 3 discusses the local polynomial regression models and piecewise-linear approximations.
Section 4 presents the experiments that test the accuracy of using constraint databases to predict the average traffic speed.
Finally, Section 5 gives a brief discussion and some concluding remarks.
yi = m( xi ) + u 1/2 ( xi )e i  (1)  where m is a function to be estimated; u is a variance function; ei is an independent random variable with zero mean and unit variance; xi is a random variable having common density f; i =1,..., n. A local polynomial estimator ; , [19, 20, and 21] can be developed via "locally" fitting a pth degree polynomial [?]
b to (xi, yi) using weighted least squares.
Bandwidth h is assumed to approach zero at a rate slower than n -1, that is:  2.
Literature review Local polynomial regression without data preclassification has several advantages.
It can avoid the drawbacks of the traditional kernel regression methodologies, such as the Nadaraya-Watson estimator [8, 9] and the Gasser-Muller estimator [10].
The Nadaraya-Watson estimator produces an undesirable bias, and the Gasser-Muller estimator must pay a price in variance to manipulate a random design model.
Moreover, local polynomial regression with high curvature adapts well to the bias problems at boundaries [11, 12].
As opposed to the local model, the global model deviates from the data pattern and requires the offline training [7], such as neural networks and time series models.
The local modeling is not the approximate function with more accuracy from it, and this feature avoids negative interference exhibited by the global models.
Moreover, the local linear method is preferable to the local constant regression in traffic data analysis [7, 13].
About local constant regression, Smith et al.
[14] and Faouzi [15] respectively implement the k-nearest neighbor method and kernel estimator in transportation.
Travel time estimation has been an interesting research area related to traffic short-term prediction.
[16] describes how speed-based travel time estimation has two application contexts: on-line real-time prediction and off-line historic data analysis.
[17] emphasizes the combination of real-time and historic data for dynamic travel time prediction.
The method of travel time estimation in [18] relies solely on average speeds.
lim h = 0  lim nh = [?]
n-[?]
n-[?]
The function of local polynomial estimator for the true function Y is shown below:  ^ ( x; p, h) Y =  m = e1T ( X xTWx X x )-1 X xTWx y = e1T b^ = b^0  (2)  where e1 is a (p+1)x1 vector having 1 in the first entry and zero elsewhere;  y = ( y1 , [?][?][?
], yn )T is a vector of responses; Wx = diag{Kh ( x1 - x), [?][?][?
], Kh ( xn - x)} is an nxn diagonal matrix of weights;  [?
]1 x1 - x L ( x1 - x) p [?]
[?]
[?]
X x = [?
]M M O M [?]
[?
]1 xn - x L ( xn - x) p [?]
[?]
[?]
is an nx(p+1) design matrix, n is the number of observations;  b^ = (b^0 , [?][?][?
], b^p )T is able to minimize the locally weighted polynomial regression  3.
Method  p  n  [?]
{ y - [?]
b ( x - x) } K i =1  i  j =0  j 2  j  i  h  ( xi - x) ;  K h ([?])
= K ([?]
/ h) / h  is a kernel function scaled by h  3.1.
Local polynomial regression  (kernel function is usually a unimodal symmetric probability with K ( x) dx = 1 ).
[?]
3.1.1.
Definition  155  of several important assumptions, [31] clarifies the rules and calculation steps about DPI bandwidth.
Figure 1 illustrates the important aspects of local polynomial regression theory.
Y (green curve) is the true model, and Y (red curve) is the result of the local polynomial regression.
The bandwidth h is a nonnegative number controlling the size of the local neighborhood.
Kh(xi - x) is the weight assigned to yi , and this weight depends on the height of the kernel function centered about the particular point x.
The data closer to x carry more influence in the value of m(x), not assuming a specific form of the regression function m(x).
There are some shape choices about kernel function [22], such as Epanechnikov, Biweight, Triweight, Normal, Uniform, Triangular, etc.
The shape choice about kernel function is not that important for data estimation and analysis as the bandwidth selection.
3.1.3.
Order choice  In terms of the order of polynomial fit for the ; , , [13] shows that asymptotic performance of fitting the polynomials of higher order leads to a possible bias reduction and a variance increase, and odd order fits are preferable to even order fits in the problem of the variability increase.
Further, even order fits achieve a lower efficiency in a bias reduction, especially in boundary regions and highly clustered design regions.
According to the practical performance in many cases, the order of polynomial fits, which are beyond cubic fit, need a very large sample to actualize a significant improvement.
Therefore, our study proposes to use p=1 or p=3.
A local cubic fit (when p=3) has more degrees of freedom for estimating a high curve region in a set of data than a linear fit (when p=1), although a cubic fit has a higher requirement concerning its calculation and sample variability than a local fit does [22].
3.2.
Piecewise-linear approximation In a piecewise-linear approximation [32] data points (xi, yi) with i=1,2,...,n, the relation between the piecewise-linear function f(xi) and yi satisfies: | f(xi) - yi | <= Ps for each (xi, yi)  (3)  The maximum error threshold Ps controls the maximum difference between the original data points and the piecewise-linear function.
It means that the original data points are always within a narrow band with width Ps around the piecewise-linear function, as shown in Figure 2.
Figure 1.
Local polynomial regression model.
3.1.2.
Bandwidth selection  The bandwidth choice is particularly important to highlight the significant structure in a set of data.
[23] executes a survey of several bandwidth selections for the density estimation, and these selectors are Biased Cross-validation (BCV) [24], Least Squares Crossvalidation (LSCV) [25], Rule-of-Thumb (ROT), Solve-the-equation (STE) [19, 26, 27, 28, and 29], and Smoothed Bootstrap [30], and [23] summarizes that ROT has a small variance yet an unacceptable large mean; LSCV has a good mean yet a too large variance; BCV suffers from unstable performance; both STE and smoother bootstrap have a correctly centered distribution in mean and an acceptable variance.
[31] compares three plug-in bandwidth selection strategies [13], such as ROT, STE, and Direct Plug-in (DPI) via data simulation and analysis, and the result is that DPI and STE have the same appealing performance.
Moreover, DPI does not need the extra complication of requiring a root-finding procedure and minimization.
Hence, DPI is selected as the approach of the bandwidth calculation in our paper.
On the basis  Figure 2.
Piecewise-linear approximation.
The piecewise-linear approximation compresses the discrete data points into a piecewise-linear function for data interpolation and faster query.
Our study innovatively uses this algorithm to approximately transform a curve into a piecewiselinear form.
156  computed for an estimation of accuracy.
Their definitions are shown in the following equations, where n is the number of the average speed reported by the detector station from 4:30 pm to 6:30 pm (n=25), Yi is the average speed from the detector station, and Yi is the speed of the speed-time pairs in the local linear or cubic model:  4.
Experimental results 4.1.
Data collection The values of vehicle speeds used in this research were collected by Cambridge Systematics Inc. at the detector station (717490) of U.S. Highway 101 in Los Angeles, California on June 8, 2005 [33].
Figure 3 shows a schematic of the detector placement for the five lane highway section of this detector station.
Each station records the time that each vehicle occupies the detector as it travels over it.
The detector also counts the number of vehicles passing over it.
The occupancy and the flow rate of vehicles are then used along with the mean vehicle length to determine speed (speed = flow / occupancy / mean vehicle length).
These data for occupancy, flow and speed are aggregated and computed over a specified period of time and only the aggregated data are stored.
The data at this station are averaged over 5 minute periods.
The data points (the open circles in Figure 4) represent the sequential fiveminute average speed values in miles per hour (mph).
Figure 4.
Local linear and cubic models for traffic speed data.
Figure 3.
Loop detector station  4.2.
Model implementation The local polynomial models with the bandwidth h=11.5 and Normal shape estimate the average speed of vehicles in all lanes.
Moreover, the models alter the discrete vehicle speed points into the continuous speed curves.
Figure 4 shows the local linear model (black curve) and the local cubic model (red curve).
The solid circles in this figure represent the average speed of five lanes reported at the detector station.
Figures 5 and 6 respectively display the speedtime pairs in the local linear and cubic models with those average speed values from the detector station (the continuous curve about the local linear or cubic regression model is divided into 361 data points, and these data points are called speed-time pairs).
Mean Square Error (MSE), Root Mean Square Error (RMSE), and Mean Absolute Error (MAE) are  Figure 5.
Speed-time pairs in local linear model and average speed from detector.
The accuracy estimations about the linear and cubic models are given in Table 1.
The results show that the cubic model is closer to the speed values recorded by the detector station than the linear model.
The data patterns in Figures 4, 5, and 6 display that the local cubic fit has more degrees of freedom for estimating a high curve region, which is consistent with the description in [22].
Due to the traffic congestion at the rush hours, the speed is the lowest  157  around 5:00 pm, i.e.
x=30, and then the speed begins to rise and has a tendency to level off after 6:00 pm, i.e., x=90.
From 4:30 pm (x=0) to 4:40 pm (x=10) and from 5:20 pm (x=50) to 5:40 pm (x=70), the local linear and cubic models show significantly different results (see Figures 5 and 6), and the local cubic model is more accurate in following the raw data.
curves in Figure 4 into the corresponding linear functions / constraints with high accuracy for the storage and query of the speed curves.
More importantly, the accuracy can be adjusted and controlled via the error threshold Ps, and the accuracy is higher with a smaller error threshold.
The query design and results are displayed using the MLPQ system [6].
MLPQ allows Datalog queries, minimum and maximum aggregation operators over linear objective functions, and some other operators.
MSE, RMSE, and MAE are also applied for an estimation of accuracy about each sub-function of the piecewise function, where n is the number of the speed-time pairs related to a certain sub-function, Yi is the speed of the pairs, and Yi is the speed calculated by the sub-function.
Based on the piecewise-linear algorithm with the error threshold Ps=0.05, the speed-time pairs' data points in Figures 5 and 6 are compressed into some linear functions, which are respectively shown in Tables 2 and 3.
The MSE, RMSE, and MAE columns summarize the accuracy analysis of every piecewiselinear function calculated by the piecewise-linear approximation algorithm.
The piecewise-linear segments listed in Tables 2 and 3 are actualized to exert the data analysis in constraint databases.
In Figure 7, the MLPQ database system shows the curves similar to those in Figure 4.
Table 4 lists the model speed values evaluated by the local linear and cubic regressions and the query results from constraint databases.
The query results are very close to the velocity values in the local linear and cubic models.
Table 4 also shows that the cubic regression has a better result than the linear regression.
Figure 6.
Speed-time pairs in local cubic model and average speed from detector.
Table 1.
Model estimation  4.3.
Database implementation and queries After using the speed-time pairs in the local polynomial models, the piecewise-linear approximation can transform the continuous speed  158  Table 3.
Local cubic model  Table 4.
Result comparison  159  Figure 7.
Piecewise model speed in MLPQ.
spatiotemporal data analysis in constraint databases.
Moreover, the analysis of other important traffic parameters, such as volume, flow rate, and queue length, would be also necessary in databases.
5.
Conclusions Linear constraint databases are able to store and query discrete data points or linear data.
However, research related to the use of constraint databases for nonlinear data storage and advanced statistical modeling is not well developed.
Our article proposes a novel and flexible approach to use the local polynomial regression and piecewise-linear approximation algorithm for the estimation and prediction of nonlinear traffic average speed in constraint databases.
The experiment results show that this approach has a high accuracy in the storage of continuous nonlinear data in linear constraint databases for a transportation application.
Our study details the definition of local polynomial regression models, their bandwidth selection, and their order choice.
The local cubic fit has more degrees of freedom for estimating a high curve region in traffic speed data than the linear fit, and yet the local cubic fit has a higher requirement concerning its calculation.
Fortunately, the development of the software package can execute the complex calculations concerning the local cubic model, and overcome this demerit to satisfy traffic data operation in a large data size.
More importantly, our approach can make a piecewise-linear function with a high accuracy for any nonparametric regression model.
Future research would concentrate on the microsimulation of traffic moving objects by storing traffic geographical information and vehicular trajectory data extracted from traffic video.
The development of vehicular trajectory data models need to implement  6.
Acknowledgements This research was supported by Nebraska Department of Roads Project No.
ITS-STWD (66).
The authors are solely responsible for the contents of this paper.
References [1] Bob McQueen, Kan Chen and Rick Schuman.
Advanced Traveler Information Systems.
Artech House Publishers, 2002.
[2] Schofer, J.L., F.S.
Koppelman, and W.A.
Charlton.
Perspectives on Driver Preferences for Dynamic Route Guidance Systems.
Transportation Research Record 1588, pp.
26-31, 1997.
[3] Linxin Li, Xingyou Zhang, Reinhard Piltner.
A Spatiotemporal Database for Ozone in the Conterminous U.S.
Proceeding of the Thirteenth International Symposium on Temporal Representation and Reasoning (TIME'06).
Budapest, Hungary, June 15-17, 2006.
[4] Fleming CC, Von Halle B. Handbook of Relational Database Design.
Addison-Wesley, Reading, pp.605, 1989.
[5] Peter Revesz.
Constraint Databases: A Survey.
Semantics in Databases, Springer Berlin, Vol.
1358, pp.
209-246, 1998.
[6] Peter Revesz.
Introduction to Database: From Biological to Spatiotemporal.
Springer, New York, 2010.
160  [7] Hongyu Sun, Henry X. Liu, Heng Xiao, Rachel R. He, and Bin Ran.
Use of local linear regression model for short-Term traffic forecasting.
Transportation Research Board, No.
1836, Vol.
18, pp.59-71, 2003.
[8] Nadaraya, E.A.
On estimating regression.
Theory Probab.
Appl.
10, pp.186-190, 1964.
[9] Wathson, G. W. and Leadbetter, M.R.
Hazard analysis I. Biometrika 51, pp.175-84, 1964.
[10] Gasser, T. and Muller, H.-G. Kernel estimation of regression functions.
In Smoothing Techniques for Curve Estimation (eds.
T. Gasser and M. Rosenblatt).
Springer-Verlag, Heidelberg, pp.
23-68, 1979.
[11] Silverman, B.W.
Density estimation for statistics and data analysis.
Chapman and Hall, London, 1986.
[12] Fan, J. and Marron, J.S.
Fast implementations of nonparametric curve estimators.
J. Comput.
Graphical Statist.
3, pp.35-56, 1994.
[13] Fan J. and Gijbels I.
Local polynomial modeling and its applications.
Chapman & Hall, London, New York, 1996.
[14] Smith, B., Williams, B.
& Oswald, K. Parametric and nonparametric traffic volume forecasting.
Transportation Research Record.
CDROM, 1999.
[15] Faouzi, E. Nonparametric Traffic Flow Prediction Using Kernel Estimator.
Transportation and Traffic Theory: Proceedings of the 13th International Symposium on Transportation and Traffic Theory, Lyon, France, pp.24-26, July, 19, 1996.
[16] Ruimin Li, Geoffrey Rose, and Majid Sarvi.
Evaluation of speed-based travel time estimation models.
Journal of Transportation Engineering, Vol.
132, ASCE, 2006.
[17] Steven l-Jy Chien and Chandra Mouly Kuchipudi.
Dynamic travel time prediction with real-time and historic data.
Journal of Transportation Engineering, Vol.
129, ASCE, 2003.
[18] Lu Sun, Jun Yang, and Hani Mahmassani.
Travel time estimation based on piecewise truncated quadratic speed trajectory.
Transportation Research Part A, Vol.
42, Page 173-186, 2008.
[19] Scott, D.W., Tapia, R.A. and Thompson, J.R. Kernel density estimation revisited.
Nonlinear Anal.
Theory Meth.
Applic.
1, pp.339-372, 1977.
[20] Cleveland, W. Robust locally weighted regression and smoothing scatterplots.
Journal of the American Statistical Association.74, pp.829-836, 1979.
[21] Fan, J. Design-adaptive nonparametric regression.
[22] [23]  [24]  [25] [26]  [27] [28]  [29]  [30]  [31]  [32]  [33]  161  Journal of the American Statistical Association.
87, pp.998-1004, 1992.
Wand, M. P. and Jones, M. C. Kernel Smoothing.
Chapman and Hall, London, 1995.
Jones, M.C.
Marron J.S.
and Sheather S. J.
A brief survey of bandwidth selection for density estimation.
Journal of the American Statistical Association.
91, No.
433, 1996.
Scott, D.W. and Terrell, G.R..
Biased and unbiased cross-validation in density estimation.
Journal of the American Statistical Association.
82, pp.1131-1146, 1987.
Bowman, A. W. An alternative method of crossvalidation for the smoothing of density estimates.
Biometrika 71, pp.353-360, 1984.
Sheather, S.J.
An improved data-based algorithm for choosing the window width when estimating the density at a point.
Comp.
Statist.
Data Anal.
4, pp.6165, 1986.
Park, B. U. and Marron, J.S.
Comparison of data-driven bandwidth selectors.
Journal of the American Statistical Association.
85, 66-72, 1990.
Sheather, S.J.
and Jones, M.C.
A reliable data-based bandwidth selection method for kernel density estimation.
J. Roy.
Statist.
Soc.
Ser.
B 53, pp.683-690, 1991.
Engel, J., Herrmann, E. and Gasser, T. An iterative bandwidth selector for kernel estimation of densities and their derivative.
J. Nonparametric Statist.
to appear, 1995.
Faraway, J.J., and Jhun, M. Bootstrap choice of bandwidth for density estimation.
Journal of the American Statistical Association.
85, pp.1119-1122, 1990.
Ruppert, D., Sheather, S. J. and Wand, M. P. An effective bandwidth selector for local least squares regression.
Journal of the American Statistical Association, 90, pp.1257-1270, 1995.
Peter Revesz, Rui Chen, Min Ouyang.
Approximate Query Evaluation Using Linear Constraint Databases.
Proceeding of the Eighth International Symposium on Temporal Representation and Reasoning (TIME'01).
Cividale Del Friuli, Italy, June 14-16, 2001.
Cambridge Systematics, Inc.
Prepared for Federal Highway Administration.
NGSIM U.S. 101 Data Analysis (4:30 p.m. to 6:30 p.m.), June 8, 2005.
2013 20th International Symposium on Temporal Representation and Reasoning  Incremental, Inductive Model Checking Aaron R. Bradley Mentor Graphics Corporation & University of Colorado at Boulder Email: bradleya@colorado.edu  Abstract--IC3, a model checking algorithm for invariance properties, has inspired a fair amount of research since it was first noticed in 2011 and is now widely used in the EDA industry.
It is rooted in the deductive approach to verification, central to which is the application of mathematical induction.
IC3 applies induction in two ways: in the typical manner, to detect convergence to an inductive strengthening of the property; and in an incremental manner, to discover relatively inductive lemmas in response to concrete error states.
Core ideas in IC3 have been lifted to algorithms for model checking LTL and CTL properties and for analyzing infinite-state systems.
to a SAT database--essentially symbolically executing the system starting from the initial states--a technique referred to as "unrolling" the relation.
At each iteration, the error is temporarily asserted in the final time frame, directing the SAT solver to search for a counterexample of the given length.
While revolutionary for disproving specifications and finding shortest counterexample witnesses, BMC's practical incompleteness spurred the development of k-induction [19].
The technique of k-induction generalizes standard induction.
In k-induction, one proves that all loop-free sequences of k P states have only P -state successors.
If a given k is inadequate, one tries a larger k. Interpolation-based model checking (ITP) introduced the use of Craig interpolants as a means of achieving a propertydirected over-approximating abstraction [17].
For a given set of states represented by formula F , if no F -state can reach a !P -state (an error) in k steps, then the k-step unrolling rooted in F and targeting !P is unsatisfiable.
A Craig interpolant then exists over variables x that over-approximates states reachable from F in one time-step, which provides an abstract version of the strongest post-condition operator.
Iterating this construction can result in a spurious intersection with error states, in which case k is increased.
Convergence to an inductive strengthening of P occurs when the formed over-approximation implies the disjunction of previous overapproximating formulas.
I.
BACKGROUND Model checking [9], [21], [22], in practice, is the task of deciding whether a design satisfies its logically-provided specification.
Specifications have traditionally been written in LTL [21] and CTL [9]--or in practical variants of these temporal logics.
Techniques for deciding satisfaction of a model checking query include algorithmic [9], [23] and deductive [16].
Symbolic state exploration is now standard [18].
Common to both logics--and, in practice, most widely used--is the class of invariance properties.
An invariance property asserts that all reachable states satisfy some propositional or first-order assertion; alternately, no state that satisfies the negation of the assertion is reachable.
Because of the prominence of the invariance-checking problem, there has been steady progress in the scalability of algorithms for it.
For this discussion, consider a finite-state transition system S : (i, x, I, T ) with primary inputs i, state variables x, initial condition I(x), and transition relation T (x, i, x ), where x represent the values of the state variables in the next state and formulas are propositional.
The oldest method of proving an invariance property P (x) is induction [16], in which one shows that 1) the invariance property P includes the initial condition: I(x) = P (x); 2) and P -states have only P -state successors: P (x) [?]
T (x, i, x ) = P (x ).
Here, F = G is shorthand for [?]
* .
F - G. Failure of the inductive step does not imply that P does not hold, only that P is not "inductive."
A basic goal of any induction-based invariance-checker is to find a strengthening formula F such that P [?]
F is inductive.
Bounded model checking (BMC) introduced the use of propositional logic (SAT) solvers to the model checking problem [2].
For checking an invariance property, ever more time-shifted copies of the transition relation are appended 1530-1311/13 $26.00 (c) 2013 IEEE 1550-1311/13 DOI 10.1109/TIME.2013.9  II.
A N I NCREMENTAL , I NDUCTIVE I NVARIANCE C HECKER Since BMC, k-induction, and ITP unroll the transition relation, they all suffer when k, the number of time frames, becomes large.
IC3 (sometimes called PDR) departs from this BMC-inspired line of model checkers, instead refining overapproximating stepwise sets incrementally [3], [4].
Queries to the SAT solver involve only one step of the transition relation and are thus relatively easy.
In practice, hundreds to thousands of SAT queries are solved per second, and memory consumption tends to be small and stable.
IC3 maintains a sequence of over-approximations Fi to sets of states reachable within i steps, for 0 <= i <= k, where Fk is the "frontier" and k grows incrementally until convergence or a counterexample is found.
It is always the case that Fi (x) = Fi+1 (x) and Fi (x) [?]
T (x, i, x ) = Fi+1 (x ).
Each Fi is a conjunction of the property P with an initially empty set of clauses.
For each k > 0, IC3 refines the Fi 's as needed to prove inductiveness of P relative to Fk , i.e., 53  that Fk (x) [?]
T (x, i, x ) = P (x ).
This refinement is propertydriven: a counterexample to the inductiveness (CTI) of the property, which is an Fk -state with a !P -state as a successor, triggers IC3 to derive a clause to block it.
If successful, it applies induction to generalize the clause to block many more states than the CTI alone.
It then adds the generalized clause to Fi for all i <= k. Otherwise, it explores (transitive) predecessors of the CTI to derive supporting strengthening clauses until the original CTI can itself be addressed relative to Fk .
This exploration of concrete predecessors is guided by a priority queue of pairs of states and frame indices: (s, i) represents the obligation that state s must be inductively excluded relative to Fi , i.e., proved unreachable for at least i + 1 steps.
Obligations are handled in lowest-index-first order, guaranteeing termination.
IC3 aggressively generalizes from states: once it addresses (s, i) by finding a clause c [?]
!s that is inductive relative to some Fj , j >= i, IC3 adds obligation (s, j + 1) to the queue if j < k. This aggressive strategy not only facilitates early discovery of mutually inductive clauses, it also allows IC3 to find deep counterexamples even when k is small.
When no CTIs remain (for Fk ), IC3 checks each clause of each Fi to determine if it can be propagated forward, i.e., if it has become inductive relative to Fi since its creation because of subsequent strengthening of Fi .
In the process, IC3 determines whether any Fi has become an inductive strengthening of the property, in which case the property is declared to hold.
If not, it "bootstraps" the new frontier Fk+1 with all clauses that are inductive relative to Fk and increments k. This process continues until IC3 finds an inductive strengthening of the property or finds a counterexample by following a sequence of CTIs back to an initial state.
While complicated in detail, the overall ideas are simple: refine i-step over-approximating sets F1 , .
.
.
, Fk until they are sufficiently strong to show that Fk 's successors are P -states, which allows the frontier to progress one time frame.
Generate clauses that are inductive relative to these sets and that block explicit states that can reach errors.
Finally, use induction to aggressively generalize the blocking of one or a few states to the blocking of many related states.
tal, inductive" philosophy of refining by constructing, using induction, lemmas from (sets of) explicit states.
IC3-like algorithms have been developed for other decidable problem domains, including timed systems [13], [14], Petri nets (and a broader class of infinite-state transition systems) [15], and finite-state safety games [20].
Incomplete extensions to infinite-state systems are possible when combined with predicate generation [7], [13], [26].
The technique of state-based inductive generalization [5], as opposed to interpolation, is useful in its own right and has been applied in an interpolant-generating algorithm [25].
Finally, IC3 has the potential to impact application domains outside of verification that apply model checking or reachability algorithms, e.g., planning.
R EFERENCES [1] J. Baumgartner, A. Ivrii, A. Matsliah, H. Mony.
IC3-guided abstraction.
In FMCAD, Nov. 2012.
[2] A. Biere, A. Cimatti, E. M. Clarke, and Y. Zhu.
Symbolic model checking without BDDs.
In TACAS, 1999.
[3] A. R. Bradley.
k-step relative inductive generalization.
Technical report, CU Boulder, Mar.
2010. http://arxiv.org/abs/1003.3649.
[4] A. R. Bradley.
SAT-based model checking without unrolling.
In VMCAI, Jan. 2011.
[5] A. R. Bradley and Z.
Manna.
Checking safety by inductive generalization of counterexamples to induction.
In FMCAD, Nov. 2007.
[6] A. R. Bradley, F. Somenzi, Z. Hassan, and Y. Zhang.
An incremental approach to model checking progress properties.
In FMCAD, Nov. 2011.
[7] A. Cimatti and A. Griggio.
Software model checking via IC3.
In CAV, July 2012.
[8] H. Chockler, A. Ivrii, A. Matsliah, S. Moran, Z. Nevo.
Incremental formal verification of hardware.
In FMCAD, Nov. 2011.
[9] E. M. Clarke and E. A. Emerson.
Design and synthesis of synchronization skeletons using branching time temporal logic.
In Logic of Programs, May 1981.
[10] N. Een, A. Mishchenko, R. Brayton.
Efficient implementation of property directed reachability.
In FMCAD, Nov. 2011.
[11] Z. Hassan, A. R. Bradley, and F. Somenzi.
Incremental, inductive CTL model checking.
In CAV, July 2012.
[12] Z. Hassan, A. R. Bradley, and F. Somenzi.
Better generalization in IC3.
Submitted.
[13] K. Hoder and N. Bjorner.
Generalized property directed reachability.
In SAT, June 2012.
[14] R. Kindermann, T. A. Junttila, I. Niemela.
SMT-based induction methods for timed systems.
In FORMATS, Sept. 2012.
[15] J. Kloos, R. Majumdar, F. Niksic, R. Piskac.
Incremental, inductive coverability.
In CAV, July 2013.
[16] Z.
Manna and A. Pnueli.
Temporal Verification of Reactive Systems: Safety.
Springer-Verlag, New York, 1995.
[17] K. L. McMillan, Interpolation and SAT-based model checking.
In CAV, July 2003.
[18] K. L. McMillan.
Symbolic Model Checking.
Kluwer, Boston, MA, 1994.
[19] M. Sheeran, S. Singh, and G. Stalmarck.
Checking safety properties using induction and a SAT-solver.
In FMCAD, Nov. 2000.
[20] A. Morgenstern, M. Gesell, K. Schneider.
Solving games using incremental induction.
In IFM, June 2013.
[21] A. Pnueli.
The temporal logic of programs.
In FOCS, Nov. 1977.
[22] J. P. Quielle and J. Sifakis.
Specification and verification of concurrent systems in CESAR.
In International Symposium on Programming, April 1982.
[23] M. Y. Vardi and P. Wolper.
An automata-theoretic approach to automatic program verification (preliminary report).
In LICS, June 1986.
[24] Y. Vizel, O. Grumberg, S. Shoham.
Lazy abstraction and SAT-based reachability in hardware model checking.
In FMCAD, Nov. 2012.
[25] Y. Vizel, V. Ryvchin, A. Nadel.
Efficient generation of small interpolants in CNF.
In CAV, July 2013.
[26] T. Welp and A. Kuehlmann.
QF BV model checking with property directed reachability.
In DATE, March 2013.
III.
I NCREMENTAL , I NDUCTIVE M ODEL C HECKING IC3 continues to inspire new research, some of which is outlined here.
Most obvious are refinements of the core algorithm.
Lifting CTIs before attempting to block them provides another opportunity for generalization [8], [10].
IC3 can be used within an incremental verification framework, in which the analysis of small changes to designs or families of properties can be accelerated based on previous executions, as the generated clauses of the frames Fi can be filtered and reused [8].
The generalization mechanism has been improved [12].
IC3 offers new opportunities for localization-reduction, including offering direction on how to refine [1] and allowing for lazy abstraction [24].
The ideas of IC3 have been lifted to LTL [6] and CTL [11] model checking.
Both algorithms follow the "incremen-  46

Real Time Properties for Interrupt Timed Automata B. BeErarda   S. HaddadaA  a  UPMC, aA ENS  M. Sassolasa   LIP6/MoVe, CNRS UMR 7606, Paris, France de Cachan, LSV, CNRS UMR 8643, Cachan, France  MeFoSyLoMa June 18, 2010  Outline Real Time Properties for ITA Mathieu Sassolas  1  The context: timed and hybrid systems  2  The Interrupt Timed Automata Model  3  The model checking problem  4  Decidable fragments  5  Conclusion  (Lip6/MoVe) 2010/06/18  Introduction The ITA model The model checking problem Decidable fragments Conclusion  2 / 21  Outline Real Time Properties for ITA Mathieu Sassolas  1  The context: timed and hybrid systems  2  The Interrupt Timed Automata Model  3  The model checking problem  4  Decidable fragments  5  Conclusion  (Lip6/MoVe) 2010/06/18  Introduction The ITA model The model checking problem Decidable fragments Conclusion  3 / 21  Context Real Time Properties for ITA Mathieu Sassolas (Lip6/MoVe) 2010/06/18  Introduction The ITA model The model checking problem Decidable fragments  Modelling and verification of hybrid systems I Hybrid automaton = finite automaton + variables aV Variables evolve in states and can be tested and updated on transitions.
aV Clocks are variables with slope 1 in all states aV Stopwatches are variables with slope 0 or 1  I Timed automaton = finite automaton + clocks with guards x + c ./ 0 and resets x := 0  Example (The gas burner)  Conclusion  Leaking x a$?1 yE = 1 4 / 21  x a$?
1, stop, x := 0 Not leaking yE = 0  x aL 30, start, x := 0  Previous results Real Time Properties for ITA  Hybrid automata = Stopwatch automata [Cassez, Larsen 2000] HA = SWA  Mathieu Sassolas  TA  (Lip6/MoVe) 2010/06/18  Timed automata  Introduction The ITA model The model checking problem Decidable fragments Conclusion  5 / 21  I The reachability problem is undecidable for a timed automaton with one stopwatch [Henzinger et al.
1998].
I Model checking timed automata with stopwatch observers is undecidable for WCTL (a weighted extension of CTL) [Bouyer et al.
2006].
I Reachability and model checking TCTL is decidable on TA [Alur, Dill 1990] [Alur, Courcoubetis, Dill 1993].
Motivations Real Time Properties for ITA Mathieu Sassolas (Lip6/MoVe) 2010/06/18  Introduction The ITA model The model checking problem Decidable fragments Conclusion  6 / 21  I Theoretical aV To express more than timed automata aV To obtain decidability results  I Practical aV In operating systems, tasks are scheduled according to their priority level.
aV A higher priority task can interrupt a lower priority task.
I An interrupt clock can be seen as a restricted type of stopwatch: only one evolves at a given time.
Clock interruptions Real Time Properties for ITA Mathieu Sassolas  level 4  (Lip6/MoVe) 2010/06/18  level 3  Introduction The ITA model The model checking problem Decidable fragments Conclusion  7 / 21  level 2 level 1 dLZ  xi := 0  dLZ dLs 0 dLZ 0 dLs 1.5 dLZ dLZ dLs dLZ dLdeg 0 dLt aaa dLdeg 0  dLZ dLs 1.5 0 dLs 2.1 dLZ dLs a aa dLZ dLdeg 0 dLt 0  x4 := 0 x3 := 0 x2 := 0 dLZ dLs 1.5 0 dLs 1.7 dLZ dLs a aa dLZ dLdeg 2, 1 dLt 0  dLZ dLs 1.5 0 dLs 2.2 dLZ dLs a aa dLZ dLdeg 2.1 dLt 1.7  ... dLs 3.7 0 dLs dLs 0 dLt 0  Outline Real Time Properties for ITA Mathieu Sassolas  1  The context: timed and hybrid systems  2  The Interrupt Timed Automata Model  3  The model checking problem  4  Decidable fragments  5  Conclusion  (Lip6/MoVe) 2010/06/18  Introduction The ITA model The model checking problem Decidable fragments Conclusion  8 / 21  Interrupt Timed Automata Real Time Properties for ITA Mathieu Sassolas (Lip6/MoVe)  q1 , 2 x1 < 1, a  The ITA model  Guard  Level  2010/06/18  Introduction  x1 + 2x2 = 2, b, x2 := 21 x1 + 1  Action  Update  q0 , 1  x2  The model checking problem Decidable fragments  1  b x1 = 1  Conclusion  9 / 21  0  a  1  x1  +2 x2  =  2  2  x1  q2 , 2  ITA and TA are incomparable Real Time Properties for ITA Mathieu Sassolas (Lip6/MoVe) 2010/06/18  ITA A1 cannot be simulated by a TA  x2 = x1 , a, x2 := 0  q1 , 2  q0 , 1  A1 accepts words made of as separated always by the same amount of time  x = 1, a, x := 0  TA A2 cannot be simulated by an ITA  x1 > 0, a, x2 := 0 (a, 0.7) (a, 1.4) (a, 2.1) (a, 2.8) (a, 3.5) (a, 4.2)  Introduction The ITA model The model checking problem Decidable fragments Conclusion  A2 accepts timed words with a a at each time unit, a b between each a, and the b gets closer to the a each time.
q0  q1 0 < x < 1, b, y := 0 q2  0<x , b, y := 0 y <1 10 / 21  x = 1, a, x := 0 q3  (a, 1) (a, 2) (a, 3) (a, 4)  (b, 1.87) (b, 2.42) (b, 3.37) (b, 4.23)  Expressiveness and decidability trade-off Stopwatch automata  Real Time Properties for ITA  SWA  Mathieu Sassolas  TA  (Lip6/MoVe) 2010/06/18  Timed automata  ITA  Interrupt timed automata  Introduction The ITA model The model checking problem Decidable fragments Conclusion  Previous results I SWA: Reachability and model checking undecidable I TA: Reachability and model checking decidable I ITA: Reachability decidable  11 / 21  What about model checking on ITA ?
Outline Real Time Properties for ITA Mathieu Sassolas  1  The context: timed and hybrid systems  2  The Interrupt Timed Automata Model  3  The model checking problem  4  Decidable fragments  5  Conclusion  (Lip6/MoVe) 2010/06/18  Introduction The ITA model The model checking problem Decidable fragments Conclusion  12 / 21  Timed CTL Real Time Properties for ITA Mathieu Sassolas (Lip6/MoVe) 2010/06/18  Introduction The ITA model The model checking problem Decidable fragments  I aNo error in the first 50 time unitsa y .
(A AZ error U y > 50) I aA normal state is reached when the clock of level 2 is greater than the one of level 1a E > U normal aSS x2 aL x1 or EF normal aSS x2 aL x1 I aWe never leave level 1 for more than 5 time unitsa AG (AZ`1 a z.
(AF `1 aSS z < 5)) I Timed CTL with explicit clocks: X ai AV xi + b ./ 0 | y .D | D ::= p | y + b ./ 0 | iaI  Conclusion  A D U D | E D U D | D aSS D | AZD I Given a formula D and an ITA A, does A  D ?
Theorem 13 / 21  Model checking TCTL formula on ITA is undecidable.
Model checking TCTL on ITA is undecidable Real Time Properties for ITA  I A two-counter machine: for e a {c, d} aV ae++ goto la, aV aif e > 0 then e-- goto l1 else goto l2a, aV aHalta.
Mathieu Sassolas (Lip6/MoVe) 2010/06/18  Introduction The ITA model The model checking problem Decidable fragments  I The halting problem of a two-counter machine is undecidable Does  the two-counter machine M  reach the Halt label ?
Does  automaton AM  reach its final state ?
Conclusion  14 / 21  Does  IM ITA  A  IM    TM TA  D  Model checking problem Only 2 external clocks ?
Outline Real Time Properties for ITA Mathieu Sassolas  1  The context: timed and hybrid systems  2  The Interrupt Timed Automata Model  3  The model checking problem  4  Decidable fragments  5  Conclusion  (Lip6/MoVe) 2010/06/18  Introduction The ITA model The model checking problem Decidable fragments Conclusion  15 / 21  TCTL without external clocks Real Time Properties for ITA Mathieu Sassolas (Lip6/MoVe) 2010/06/18  Introduction The ITA model The model checking problem Decidable fragments Conclusion  16 / 21  I Only  P  iaI  ai AV xi + b ./ 0 comparisons.
I For example E > U normal aSS x2 aL x1 I The truth value of the comparison can be abstracted by regions.
I A classical CTL model checking algorithm can be applied.
Theorem Model checking TCTL without external clocks on ITA can be done in 2-EXPSPACE and PSPACE when the number of clocks is fixed.
Example of model-checking procedure Real Time Properties for ITA  x1 + 2x2 = 2, b  q1 , 2  q2 , 2  x1 < 1, a  Mathieu Sassolas  E > U q1 aSS x2 aL x1  (Lip6/MoVe)  x1 < 23 x is fixed 0 < x1 < a 12 x11 + 1 = x2  q0 , 1  2010/06/18  q0 , Z01  Introduction  a  q1 , Z01 0 = x2 < 1  q1 , Z01 0 < x2 < 1  q1 , Z01 0 < x2 = 1  q1 , Z01 0 < 1 < x2  x1 = 0  b  The ITA model The model checking problem Decidable fragments  q2 , Z01 0 < x2 = 1  q0 , Z11  a  q1 , Z11 , Z02  q1 , Z11 , Z12  q1 , Z11 , Z22  q1 , Z11 , Z32  when x1 = 23 , x1 + 2x2 = 2 m x1 = x2  q2 , Z01 0 < 1 < x2  q1 , Z11 , Z42  q1 , Z11 , Z52  0 < x1 < 23  b q2 , Z11 , Z42  q0 , Z21  a  q1 , Z21 0 = x2 < 23  q1 , Z21 0 < x2 < 23  q1 , Z21 0 < x2 = 23  q2 , Z11 , Z52  q1 , Z21 0 < 23 < x2  x1 = 23  b  Conclusion  q2 , Z21 0 < x2 = 23  q0 , Z31  a  q1 , Z31 , Z62  q1 , Z31 , Z72  q1 , Z31 , Z82  q2 , Z21 0 < 23 < x2  q1 , Z31 , Z92  2 q1 , Z31 , Z10  2 q1 , Z31 , Z11  q2 , Z31 , Z92  2 q2 , Z31 , Z10  2 q2 , Z31 , Z11  2<x 1 3  b q2 , Z31 , Z82  17 / 21  q0 , Z41  q0 , Z51  q0 , Z61  q0 , Z71  A fragment of TCTL with only one external clock Real Time Properties for ITA Mathieu Sassolas (Lip6/MoVe) 2010/06/18  Introduction The ITA model The model checking problem Decidable fragments Conclusion  18 / 21  I A particular case of TCTL with 1 external clock.
I Clock conditions can only restrict the Until operator with urgency (y a$?
b or y < b) or delay (y aL b or y > b).
I There can be no imbrication of Untils.
I For example y .
(A AZ error U y > 50)  Theorem Model checking this fragment of TCTL on ITA is decidable.
Outline Real Time Properties for ITA Mathieu Sassolas  1  The context: timed and hybrid systems  2  The Interrupt Timed Automata Model  3  The model checking problem  4  Decidable fragments  5  Conclusion  (Lip6/MoVe) 2010/06/18  Introduction The ITA model The model checking problem Decidable fragments Conclusion  19 / 21  Summary and future work Real Time Properties for ITA  Stopwatch automata  Mathieu Sassolas  SWA  (Lip6/MoVe) 2010/06/18  Introduction  TA  ITA  Timed automata  Interrupt timed automata  The ITA model The model checking problem Decidable fragments Conclusion  ITA+ I ITA allow reasoning on systems with interruptions.
I Its expressive power is incomparable with the TA model.
I Unfortunately model checking of full TCTL is impossible.
I Nevertheless some interesting fragments are still decidable.
20 / 21  Thank you Real Time Properties for ITA Mathieu Sassolas (Lip6/MoVe) 2010/06/18  Introduction The ITA model The model checking problem Decidable fragments Conclusion  21 / 21  Any questions ?
Estimating Positions and Paths of Moving Objects Kate Beard and H. Mustafa Palancioglu NCGIA and Department of Spatial Information Science and Engineering University of Maine Orono, ME 04469 beard and hpalanci@spatial.maine.edu  Abstract Several applications require support for representing and analyzing moving objects.
Such applications include wildlife tracking, emergency dispatch, vehicle navigation, fleet management, storm tracking, and military applications to name a few.
Enhanced data collection technologies such as high resolution satellite imagery, videogrammetry, and GPS in combination with wireless communications are rapidly increasing the feasibility of obtaining information on moving objects and fueling new research and development.
In these applications there is a need to efficiently answer questions about moving objects such as where they are at a specific time, where they have been in the past, where they will be in the future, and their relationships to static or other moving objects.
Complete knowledge of object movements is not possible but movements can be predicted with some degree of reliability.
Positions and paths of moving objects can be estimated from a set of observations.
This paper reports on an approach that uses movement profiles, and movement histories in addition to observations to more reliably estimate positions and paths of moving objects.
1.
Introduction and Background Management of information on moving objects requires database support and there has been substantial research activity in this area recently.
Relevant research covers spatiotemporal databases, moving object databases [3], [18], [19], spatio-temporal indexing [12], indexes  for moving objects [7], [13], representations on the uncertainty of moving objects [11], [10], and ontological considerations for movement and moving objects [4].
A challenge in spatiotemporal modeling comes from incorporating "space" and "time" dimensions in the same database.
Temporal databases [6] are characterized as managing records of time varying information.
Spatial databases [5], [14] contain records on space varying phenomena.
Spatio-temporal databases are characterized by the management of information on phenomena that are both time and space varying.
Research in spatio-temporal systems has taken the form of both temporal extensions to spatial databases and spatial extensions to temporal databases.
Modeling, indexing, and query languages for spatio-temporal data have all received attention.
Langran [9] addressed a set of practical issues concerned with data representation, incremental updates, and system longevity.
Langran [8] examined the concept of combined spatial and temporal dimensions and suggested that dimensional dominance must be determined for the optimization of data and the algorithms.
Snodgrass [16] described the need to consider the evolution of spatial objects in addition to retroactive or post active changes.
Spatio-temporal indexing raises significant challenges.
Temporal indexing often includes two aspects of time, both valid time and transaction time.
When both times are included this is referred to as a bitemporal database [6].
Full spatio-temporal support is assumed to include these two temporal aspects as well as two or three spatial dimensions.
Spatio-temporal indexing has typically used one of two approaches [12]: 1)  1 0-7695-0756-5/00 $10.00 a 2000 IEEE  overlapping index structures that index spatial objects at different times or 2) the addition of time as another dimension to an existing spatial index.
In an index for moving objects, a spatial index used to record continuously changing locations requires the index to be continuously updated which is not a satisfactory solution.
Query languages have been the subject of related research.
Spatial query languages [2], [14]  supporting various spatial operations have been developed.
Likewise temporal query languages have been evolving.
TQUEL [15] incorporates notions of time that extend the semantics to allow the formulation of historical queries.
Spatiotemporal query language extensions need to develop in parallel with these spatial and temporal query language developments.
The following table summarizes and compares databases from traditional to moving object databases.
Table 1.
Overview of database evolutions from traditional to moving object databases.
Traditional DBMS  Spatial DBMS  Temporal DBMS  Spatio-temporal DBMS  Moving Objects DBMS  Static non-spatial nontemporal supports attribute data  Static non-temporal supports spatial and attribute data  Static and Dynamic non-spatial supports temporal and attribute data  Static and Dynamic supports spatial, temporal and attribute data  Static and Dynamic supports spatial, temporal and attribute data extends spatiotemporal by adding support for continuously time and space varying data  Time Stamping  None  None  Tuple Attribute  Geometry and Attribute  Geometry, Attribute and Location  Answers  what  what where  what when  what where when  what where-when  Data Type  Moving objects fall directly within the purview of spatio-temporal databases.
Moving object databases are a specialization of spatiotemporal databases for discrete and continuously varying spatial and temporal information.
A continuous model for continuous movements may be desirable but not practical in the near term.
Data observation streams are not fully continuous and there are difficulties in storing and indexing continuous movements.
Wolfson et al [19] identify a set of capabilities for managing moving objects that are not provided by current databases.
These deficiencies include location modeling,  query support, indexing, and uncertainty issues.
Within a conventional relational database the position of a moving object may be recorded and periodically updated.
The assumption however is that the position of the object is constant between updates.
Updates to an object's position can be made at frequent intervals but continuous updates of locations are not feasible.
Additionally frequent updates create performance problems if they require frequent updates of the index in response to the updated positions.
In terms of query languages, traditional query languages such as SQL are not satisfactory for spatio-temporal range  2 0-7695-0756-5/00 $10.00 a 2000 IEEE  and spatio-temporal join queries and can be particularly problematic for moving object queries.
Moving object queries also need to accommodate semantics for uncertainty.
Since frequent updates to an object's location can be costly and because position is assumed to be constant between periodic updates, query responses from the database will not accurately reflect a moving object's position.
Interpolated results are also necessarily uncertainty.
Morira et al [10] have proposed Superset and Subset semantics as one approach to address the uncertainty in queries regarding moving object trajectories.
The Superset returns the set of tuples that potentially rather than definitely satisfies a query predicate.
Several current research activities [19], [13] address enhancements for support of moving object information.
Wolfson et al [19] incorporate the notion of a dynamic attribute, one whose value is updated continuously as time passes.
They also incorpoarte a higher level data abstraction refereed to as an objects motion plan.
The dynamic attribute is only explicitly updated in response to changes in the motion plan.
Saltenis et al [13] propose a time parameterized R -tree which indexes the current and anticipated future positions of moving objects.
They use a linear function with parameters that include position and a velocity vector.
We propose a similarly discrete approach that is dictated by the assumption of a set of discrete observations and two higher level abstractions referred to as mobility profiles and movement histories.
The next sections describe our definition of movement and moving objects and an approach to modeling the positions and paths of moving objects.
The model assumes linear spatio-temporal functions applied to rigid objects.
2.
Movement and Movement Patterns We consider moving objects to be real world objects capable of voluntary or involuntary movement.
Moving objects may include those that are frequently at rest to those that exhibit continuous movement.
Erwig et al [3] characterize all geometric change as movement including changes in shape (growing or shrinking).
We separate change to an object's boundary as a shape change and distinct from movement.
We define movement specifically as a rotation about an axis or a translation (the same definition used by Moreira et al [10].
This distinction captures important behavioral  differences between real world objects for example lakes and cars.
Lakes frequently shrink and expand but are not subject to rotation or translation and hence we would say are not subject to movement (except possibly in a catastrophic event).
Cars on the other hand frequently change their position but do not change their shape.
Both rigid and non-rigid objects may exhibit both types of change: movement and boundary reconfiguration and there can be reason and need to treat these as separate dimensions of change.
Galton [4] describes movement as occurring whenever the same object occupies different positions in space at different times.
He describes the position of an object as the total region of space that it occupies at a time.
By this definition he suggests that an object's position is congruent with the region of space occupied by the body itself.
This definition allows for the translation and rotation of objects as movement but also includes scaling (an object's expansion and contraction).
As described above, moving object representations can create storage problems in conventional DBMS because their locations require frequent updates.
We address this problem by storing only the static positions associated with observations and computing non-observed positions in response to queries that involve a direct or indirect request for a location or path.
Examples of such queries include: a) Where was the object an hour ago?
b) Where will the object be in an hour?
c) Where is the object now?
d) What paths has the object followed?
e) What paths may the object follow?
Slight variations on such queries may request a time, a set of moving objects or a common time and location for a set of objects as in the following examples.
f) When will the object arrive at a location?
g) What objects are within ten minutes of a location ?
h) Where and when might two object's paths cross?
This paper focuses on the first type of query (queries a-e).
Our approach employs a method called GetPosition (funct, id_list, t, s) with parameters that can include a location function, a list of object ids, a time, and location and which can return a time, a location for an object, or set  3 0-7695-0756-5/00 $10.00 a 2000 IEEE  of objects depending on which parameters are supplied.
Return of an object's position (which may be represented as a point, line or region) requires specification of a time (as a point or interval) and conversely return of a time requires specification of a location.
Query a above is an example of the former type and query f an example of the later.
The time parameter may be specified relative to current time (now) or a specific time in an absolute temporal reference frame.
If the query is, "Where was the object an hour ago?"
the method GetPosition (funct, id_list, t, s) is activated with a time of NOW - 1 hour.
All previous approaches to estimating moving object positions rely on a time ordered sequence of observations.
Several approaches have indicated the need to amend recorded positions with additional information at higher levels of abstraction.
Wolfson et al [19] add a motion plan which is a sequence of way time points (pi, ti) indicating where (pi) a moving object will be at time ti.
This approach assumes that the system has prior information on an object's expected movements which may not be case.
We assume that observations may be irregularly spaced in time, from different sources, and that no planned route information is available.
To compute a position and expected path for a moving object a set of observations reporting an object's position are used.
To improve the estimated locations and paths, which can suffer from flaws in the observation set, two additional pieces of information are used.
One piece of information is an object's prototypical behavior.
Prototypical behavior is associated with an object class rather than a particular object.
A third input incorporates summaries of past movements that we refer to as movement histories.
Movement histories are summaries over all or consecutive subsets of past movements of a particular object.
A method GetPosition relies on these three sources of information.
2.1 Detection of Movement: the role of Observations Observations are essential components for the computation of a moving object's position.
Prototypical movement profiles (described in the next section) provide information on general movement behaviors.
Observations on the other hand serve as sightings of a particular object at specific locations and times.
Observations refer to any measurements that capture the location of one or more real world objects.
Observation types  include satellite images, aerial photographs, video sequences, GPS observations or telemetry observations.
Moreira et al [10] refer to observation based systems as sensor systems that capture location data in an ordered sequence at regular time intervals.
Representations of these observations are stored in a database.
For each observation we record an associated spatial footprint that describes a 2D projection of the observation into a spatial reference frame [1].
Such projections or footprints may take the form of points, lines, or regions.
A GPS observation has a point footprint while a satellite image has a region footprint.
A video clip may have a point, linear, or regional footprint.
Observations additionally have time stamps that may be points or intervals.
Because the observation footprints and timestamps are static they are amenable to spatial, temporal or spatio-temporal indexing.
2.2.
Spatial and Movement Registers A GPS observation is typically associated with a single real world object (e.g.
a ship).
Satellite or other imagery, on the other hand, contain representations of multiple real world objects and their positions.
Objects and their positions are extracted from images using various feature extraction methods.
We use what is referred to as a spatial register to maintain the association between an observation and an object position.
[object_id, observation_id, observation_time_stamp, point or region] A point or region specification depends on the extent of an object in the observation.
This association is important for accuracy assessment as the accuracy of the position is a function of characteristics of the observation and the applied feature extraction method.
In comparison with GPS observations, images are less effective observation sources for identifying and tracking specific moving objects.
In certain contexts, however, imagery including that from video and surveillance cameras may be useful sources for obtaining moving object positions.
In our approach typically the only stored positions for moving objects are those recorded by an observation.
Once an instance of an object, say B, is extracted from an image we describe its location  4 0-7695-0756-5/00 $10.00 a 2000 IEEE  as given in the observation by a point or by the center of the object's minimum bounding rectangle (MBR).
B:position: (x,y,z,th)Oti describes B's location in the observation Oti with time stamp ti; x, y, and z are coordinates describing the MBR center, and th is an initial azimuth for the object if a record of azimuth is pertinent.
Two such observations are required to detect movement.
We say movement has occurred if for [?
]t (the difference in time stamps between two observations), there is a [?
]X, [?
]Y, [?
]Z, or [?
]th greater than zero, and we define a unit movement as the set of deltas between two observation tuples.
media constraints.
Directional axis movement constraints describe the ability of an object to move in the X, Y or Z axis where these are aligned with respect to the object itself as shown in Figure 1.
We assume an object has a fixed orientation: a front and back aligned with theY axis and a left and right aligned with the X axis.
Z is the vertical axis aligned with the gravitational field.
As examples, helicopters are capable of Y movement (forward and backward)), Z movement (up, down), and X movement (sideways), a submarine has Z and Y movement, an elevator only Z movement, and a car just Y movement.
B:movement O1,O2 = ([?
]X, [?
]Y, [?
]Z, [?
]th) A unit movement has the properties of duration ([?
]t), path ([?
]X, [?
]Y, [?
]Z, or [?
]th) and velocity ([?
]X, [?
]Y, [?]Z)/[?
]t, or [?]th/[?]t.
The unit movement vectors are maintained in what is referred to as a movement register.
If we assume a uniform space-time surface or volume and no constraints on an object's movements, a set of observed positions could be sufficient to compute an object's probable trajectory between observations or a position at any unmeasured space-time coordinate.
All real world object movements are however subject to various spatial and temporal constraints.
Knowledge of object's movement constraints in combination with observations can improve the ability to more reliably estimate a moving object's location or path.
We model constraints on object movements through development of prototypical movement profiles and movement histories.
2.3 Prototypical movement profiles Prototypical movement profiles generally identify constraints on the movement behaviors of a class of objects.
A movement profile for a class of objects identifies directional axis constraints on movement, medium constraints on movement, general size, shape and weight characteristics than constrain movement, as well as prototypical average or maximum speed for a class of object.
These sets of constraints are associated with class hierarchies.
As an example classes of vehicles, classes of animals or classes of storms can be globally assigned average or maximum speeds.
Subsets of classes with different behaviors can be assigned to subclasses distinguished by different directional movement constraints or different  Z  Front  Y  Left Right  X  Back  Figure 1.
Axes used to describe the directional axis movement constraints of objects.
Media constraints refer to the surface or volumetric media on or within which an object is capable of moving.
Very few objects are unconstrained with respect to media.
Obvious examples of media constraints include roads for automobiles, tracks for trains, snow for sleds and snowmobiles and water for ships.
Animals, people and all terrain vehicles are examples of less media constrained moving objects.
Within a media type additional constraints may apply to subclasses of objects.
For example the class and condition of a roadway may restrict certain subclasses of vehicles such as heavy trucks.
Constraints on prototypical movements can take the form of either spatial or temporal constraints.
An animal's spatial constraints can be that it can not move up steep gradients, through thick underbrush, or over water.
Its temporal constraints may be that it routinely moves only during the day or only during the night.
Media may have their own constraints that a moving object may inherit.
For example all classes of moving objects have a prototypical speed.
Cars constrained to the road media inherit the speed limits assigned to the roads which in turn may override their prototypical speed  5 0-7695-0756-5/00 $10.00 a 2000 IEEE  2.4 Movement histories While prototypical movement profiles characterize general movement constraints at a class level, a time ordered sequence of observations on a particular instance of a real world moving object provides information on the object's movement history.
Movement histories are summaries of collections of unit movements for example the collection of unit movements associated with a single object B as extracted from a set of observations Oi, i=1,..n. B: movement = (([?
]T, [?
]X, [?
]Y, [?
]Z, [?
]th)(O1,O2) ([?
]T [?
]X, [?
]Y, [?
]Z, [?
]th) (O2,O3) ([?
]T [?
]X, [?
]Y, [?
]Z, [?
]th) (O3,O4)).
Any contiguous subset of unit movement vectors can form a movement history.
A collection of movements can be described by the overall duration [?
]T (O1,On); path ([?
]X, [?
]Y, [?
]Z, [?
]th) (O1,On, an average direction or heading and an average velocity for the overall duration ([?
]X, [?
]Y, [?
]Z, [?
]th) (O1,On))/ [?
]T (O1,On).
Large subsets of unit movements describe longer histories that can begin to reveal spatial and temporal patterns such as an animal tracing a similar path at similar times of the day.
With larger collections of unit movements we can build summaries that describe spatio-temporal patterns in the collection such as whether the movements are systematic, organized, regular, or periodic [20].
Movement histories may or may not be stored.
3.
Estimation of Locations and Paths From the three sources of information: observations, profiles, and histories responses to queries regarding moving objects can be generated.
Assume a situation in which a dispatcher is tracking fleet vehicles (of different types) and must respond to any number of possible queries concerning vehicle locations and paths.
One query might be "Where was Vehicle 37 15 minutes ago?"
For this example we assume every vehicle is equipped with a GPS receiver and transmitter and reports its position every hour.
If the current time is 11:00am the request is for the vehicle's position at 10:45am.
As a first step the object's movement profile is retrieved to obtain its movement constraints.
Assume a subset of the vehicle's profile is as follows:  Object class: truck max speed: 80 mph media: road (classes) directional axis constraint: Y The profile indicates that this vehicle is constrained to roads, has only Y (forward and backward) movement capability and a maximum speed of 80mph.
A search is them made on the spatial and movement registers for recorded positions and unit movements of the truck that are closest to the requested time.
Three recorded positions for 10:10am, 9:10am and 8:10am and two unit movement vectors are returned.
The Average summary operation over the unit movement vectors provides the recent history information that the truck's averaged heading is South with an average speed of 50 mph.
Figure 2 illustrates the three recorded positions and the unit movement vectors between the pairs of observed positions.
Using the media constraint from the movement profile which is roads (possibly of a certain class) and the general heading South, a standard geographic information system (GIS) network allocation routine is used to compute possible positions.
The start node for the network allocation is the last observed position.
This last position had a time stamp of 10:10 am and the requested time is 10:45 indicating the need to estimate positions over the last 35 minutes.
Each road link in the network has an associated impedance value, a value that represents a cost to traverse the link.
This value can be expressed in time or distance units.
Using a time impedance value, road links from the start position are accumulated until the desired impedance value of thirty-five minutes is reached.
At each intersection in the road network the vehicle could take a number of different paths.
Using the recent movement history information that the average heading has been south and the average speed has been 50 mph, the most likely paths of the vehicle can be constrained to links in the network with south, south-west and south-east azimuth values.
6 0-7695-0756-5/00 $10.00 a 2000 IEEE  histories should enhance the ability to more reliably estimate moving object positions and paths, but more testing is need to demonstrate this.
In continuing research we are investigating the number of observations needed to respond to a query and the number of unit movements needed to build useful summaries for a particular context.
These numbers are expected to be functions of the numbers of observations on an object within a time interval and the time frame of the query.
Summary operations over unit movement vectors is another area of on going research.
Summaries of subsets of vectors are required to identify individual object movement constraints but additionally they can be used to build descriptions of periodic, cyclical, or other spatio-temporal patterns.
Such patterns may subsequently be used as search templates for similar patterns.
Figure 2.
Observed positions and unit movement vectors for Truck 37.
One area of particular interest is to more formally estimate the uncertainties associated with the estimation of object positions and paths using the three information sources.
The computation of either a location or an expected arrival time can not be exact and thus the reported result and presentation should provide some indication of the degree of uncertainty.
Future research will investigate metrics for reporting spatial or temporal uncertainties and visual presentations of such uncertainties.
Acknowledgment Support from NIMA for this research under grant number NMA202-98-1-1113 is gratefully acknowledged.
Figure 3.
Possible paths and most probable locations for truck 37 given its profile constraints and recent history.
Figure 3 highlights the set of links representing zero to thirty-five minutes of travel time from the truck's last observed position.
The two small darker segments represent the most likely positions of the truck 15 minutes ago given the constraints indicated by the movement histories.
4.
Summary and Future Work This paper reflects work in progress.
Clearly additional research is needed for a fully functional approach to managing moving objects and estimating unrecorded movements and paths.
The addition of movement profiles and movement  5.
References [1] Beard, K. T. Smith, L.Hill.
1997.
Meta-information Models for Georeferenced Digital Library Collections.
Proceedings IEEE 2nd International Conference on Metadata.
Silver Spring, MD.
[2] Egenhofer M. 1994.
Spatial SQL: A Query and Presentation Language.
IEEE Transactions on Knowledge and Data Engineering Vol.
6.
No.
1 pp.
8695.
[3] Erwig, M. Guting, R. H. M. Schneider and M. Vazirgiannis.
1997.
Spatio-temporal Data Types: An Approach to Modeling and Querying Moving Objects in Databases.
Chorochronos Technical Paper Series.
[4] Galton, A., 1995.
Towards a Qualitative Theory of Movement, In A. U. Frank and W. Kuhn (eds.
), Spatial Information Theory: A Theoretical Basis for GIS  7 0-7695-0756-5/00 $10.00 a 2000 IEEE  (Proceedings of International Conference COSIT'95, Semmering, Austria, Springer-Verlag, pp.
377-396  [5] Guting, R.H. 1994.
An Introduction to Spatial Database Systems.
VLDB Journal 4. pp.
357-399.
[19] Wolfson O., P. Sistla, B. Xu, J. Zhou, S. Chamberlain, Y. Yesha, and N. Rishe, 1999.
"Tracking Moving Objects Using Database Technology in DOMINO", Springer-Verlag Lecture Notes in Computer Science, No.
1649, Proceedings of NGITS'99.
pp.
112-119.
[6] Jensen, C. S. and R. Snodgrass.
1999.
Temporal Data Management.
IEEE Transactions on Knowledge and Data Engineering.
Vol.
11, No.1.
pp.
36-43.
[7] Kollios, G., D. Gunopulos, and V. Tsotras.
1999.
[20] Yuan, M. 1998.
Representing Spatio-temporal Processes to Support Knowledge Discovery in GIS databases.
8th International Symposium on Spatial Data Handling 2: 431-440.
On Indexing Mobile Objects.
In Proceedings of the PODS Conference.
pp.
261-272.
[8] Langran, G., 1988, Temporal Design Tradeoffs.
In proceedings of GIS/LIS'88, Volume 2 Falls Church, VA: ACSM, pp.
890-899.
[9] Langran, G., 1993, Time in Geographic Information Systems Bristol, PA: Taylor & Francis.
[10] Moreira J, R. Cristina, and J.-M. Saglio, 1999.
"Representation and Manipulation of Moving Points: An Extended Data Model for Location Estimation", Cartography and Geographic Information Systems, Vol.
26, No.
2, pp.
109-123.
[11] Pfoser, D and C. S. Jensen.
1999.
Capturing the Uncertainty of Moving Object Representations.
Proceedings of the SSDBM Conference.
pp.
111-132.
[12] Saltenis, S. and C. S. Jensen.
1999.
R-Tree Indexing of General Spatio-Temporal Data.
Chorochronos Techncial Report Series CH-99-18.
[13] Saltenis, S. C. S. Jensen, S. T. Leutenegger and M. A. Lopez.
1999.
Indexing the Positions of Continuously Moving Objects.
Chorochronos Techncial Report Series CH-99-19.
[14] Shekar.S.
S. Chawla, S. Ravada, A. Fetterer, X. Liu, and C.-t,Lu.
1999.
Spatial DatabasesAccomplishments and Research Needs.
IEEE Transactions on Knowledge and Data Engineering.
Vol.
11 No.
1 pp.
45-54.
[15] Snodgrass R. 1987.
The Temporal Query Language TQuel.
TODS 12(2): 247-298.
[16] Snodgrass, R and I. Ahn.
1985.
A Taxonomy of Time in Databases.
Proceedings ACM SIGMOD.
Austin, TX.
pp.
236-246.
[17] Sistla P., O. Wolfson, S. Chamberlain, S. Dao.
1997.
Modeling and Querying Moving Objects.
Proceedings of the 13th International Conference on Data Engineering, [18] Wolfson O., B. Xu,, S. Chamberlain.
L. Jiang.
1998.
Moving Objects Databases: Issues and Solutions.
Proceedings of the 10th International Conference on Scientific and Statistical Database Management (SSDBM98) Capri, Italy.
pp.
111-122.
8 0-7695-0756-5/00 $10.00 a 2000 IEEE
Fixing the Semantics for Dynamic Controllability and Providing a More Practical Characterization of Dynamic Execution Strategies Luke Hunsberger Vassar College Poughkeepsie, NY 12604-0444 USA hunsberg@cs.vassar.edu  Abstract Morris, Muscettola and Vidal (MMV) presented an algorithm for checking the dynamic controllability (DC) of temporal networks in which certain temporal durations are beyond the control of the planning agent.
Their DC-checking algorithm is based on rules for inferring new constraints based on the real-time context within which execution decisions must be made.
This paper presents a counter-example to demonstrate that some of the inference rules are, in fact, not sound.
The paper fixes the problem by strengthening the definition of dynamic execution strategies to correctly capture the central prohibition against advance knowledge of future events.
The new definition enables MMV's soundness proof to go through with minimal changes.
It then uses the stronger definition to derive an equivalent, alternative characterization of dynamic execution strategies that highlights the real-time execution decisions that a planning agent must make.
The procedural strategy used by MMV in their completeness proof is shown to satisfy the stronger definition, thus ensuring that the DC-checking algorithm is also complete with respect to the stronger definition.
As a result, the paper puts MMV's DC-checking algorithm on a more solid theoretical foundation, while also providing a more practical characterization of dynamic execution strategies.
1.
Introduction Dechter et al.
[1] introduced Simple Temporal Networks (STNs), a practical formalism for representing and managing time-points and temporal constraints.
Vidal and Ghallab [8] argued that certain processes are only initiated by a planning agent, but their durations are beyond the agent's control.
They augmented STNs to include contingent constraints and defined the controllability of such networks.
Vidal and Fargier [6, 7], and later Vidal [5], presented more concise definitions of networks with contingent  constraints--called Simple Temporal Networks with Uncertainty (STNUs)--and three kinds of controllability: weak, strong and dynamic.
Of these, dynamic controllability has the most practical use.
Loosely speaking, a network is dynamically controllable (DC) if there exists a dynamic execution strategy (DES) that an agent can use to execute the time-points under its control that will guarantee the consistency of the network no matter how the contingent durations turn out.
Crucially, a DES is not allowed to depend on advance knowledge of future events.
In particular, real-time execution decisions cannot depend on the values of contingent durations that have not yet completed.
Morris, Muscettola and Vidal [3]--henceforth MMV-- further refined the semantics of dynamic controllability and presented a pseudo-polynomial-time algorithm for checking the dynamic controllability of arbitrary STNUs.
Their proof of the soundness of their DC-checking algorithm is based on rules for inferring new constraints that reflect the real-time context within which execution decisions must be made.
Their completeness proof demonstrates that any network accepted by the algorithm has a viable DES.
Later, Morris and Muscettola [4] presented a more concise, O(n5 )-time DC-checking algorithm, and Morris [2] presented an even faster, O(n4 )-time algorithm.1 The correctness of the later algorithms is based on the MMV semantics and the correctness of the MMV algorithm; thus, this paper focuses on the MMV semantics and algorithm.
1.1.
This paper This paper begins by presenting a counter-example to the soundness of MMV's DC-checking algorithm.
The problem is two-fold.
First, MMV's definition of a DES--and hence of dynamic controllability--does not adequately capture the prohibition against advance knowledge of future events that lies at the heart of our pre-theoretic notion of dynamic controllability.
Second, MMV's soundness proof relies on a 1n  is the number of time-points in the network.
are as in an STN, and C [?]
E is a subset of the edges: the contingent links.
(The rest of the edges are called requirement links.)
For each contingent link, e [?]
C, the bounds are assumed to satisfy 0 < l(e) < u(e) < [?].
If e is a contingent link from A to C, then C is called a contingent time-point, and A is called the activation time-point for C. In this paper, contingent links are indicated as follows:  property of DESs that is not entailed by their definition.
The paper fixes the problem by augmenting MMV's definition of DES to include a stronger property that correctly captures the above-mentioned prohibition.
It then proves that MMV's soundness proof, with minimal changes, goes through using the new definitions.
No changes to the DCchecking algorithm are required; it is sound with respect to the new definitions.
Finally, the paper uses the new definition of dynamic execution strategies to derive a more practical, alternative characterization of such strategies.
The new characterization is based directly on the kinds of real-time execution decisions that a planning agent must make.
Each real-time decision has one of two forms.
They can be glossed as "wait until something happens" or "if nothing happens before time t , then execute the time-points in the set kh".
Such strategies can be derived using the sorts of incremental computations that are reflected in the procedural strategy used by MMV in their completeness proof.
This paper demonstrates that their procedural strategy satisfies the new definition of a dynamic execution strategy, thereby ensuring that the existing DC-checking algorithm is also complete with respect to the new definition of dynamic controllability.
Although some of the elements of the alternative characterization of strategies developed in this paper bear some similarity to notions defined by Vidal and Fargier [6, 7], they are quite distinct.
In addition, the strategies based on real-time execution decisions are novel, their properties are rigorously analyzed, and they are proven to be equivalent to the (revised) MMV definitions.
[l(e), u(e)] A  C  Let N c [?]
N denote the set of contingent time-points and N x = N - N c - {Z} the set of executable time-points.
To execute a time-point means to fix its value to the current time.
The planning agent directly controls the execution of only the executable time-points.
Nature is presumed to control the duration of each contingent link.
A contingent time-point, C, is said to be activated if its activation time-point, A, has been executed.
Situations and Projections.
Let N be an STNU whose contingent links are e1 , .
.
.
, eq and whose corresponding labels are [x1 , y1 ], .
.
.
, [xq , yq ].
The space of situations for N is the cross-product, N = [x1 , y1 ] x .
.
.
x [xq , yq ].
Each o [?]
N is called a situation.
Each situation specifies durations for all of the contingent links in N .
For a situation, o = (o1 , o2 , .
.
.
, oq ), the projection, No , is the STN (not STNU) derived from N by replacing each contingent link, ei , with a requirement link labeled by [oi , oi ], thereby fixing its duration to the value oi .
1.2.
Summary of MMV's approach  Schedules.
Given an STNU, N = hN, E, l, u, Ci, a schedule is a mapping T : N - R (i.e., a complete set of variable assignments for all of the time-points in N ).
For convenience, the shorthand Tt is used instead of T (t) to denote the time at which t is executed according to the schedule T .
The set of schedules for N is denoted by TN .
A schedule T is consistent with N (resp., a projection No ) if the assignments in T satisfy all of the constraints in N (resp., No ).
Henceforth, we restrict attention to schedules in which TAi < TCi for each contingent link, Ai Ci .
This section summarizes the definitions used by MMV, but also draws from Vidal [5] and Vidal and Fargier [7].
Simple Temporal Network (STN).
An STN is a 4-tuple, hN, E, l, ui, where N is a finite set of nodes (or timepoints), E is a set of directed edges, and l : E - R[?]{-[?]}
and u : E - R [?]
{[?]}
are functions that map edges to lower and upper bounds, respectively.
An edge, e [?]
E, from the time-point X to the time-point Y, represents the constraint, Y - X [?]
[l(e), u(e)], indicated as follows:  Pre-histories.
Given a schedule T for an STNU N , and a time-point x [?]
N , the pre-history of x with respect to T is:  [l(e), u(e)] X  Y  T<x = {(Ci , TCi - TAi ) : Ai Ci [?]
C, TCi < Tx } Thus, T<x specifies the durations of all contingent links that execute before x according to the schedule T .
The edges are also called links.
One of the time-points, called the zero time-point (or Z), is fixed at the value 0.
All other time-points are constrained to occur after Z.
Execution Strategies.
An execution strategy for an STNU N is a mapping, S : N - TN , from situations to schedules.2 S is called viable if for each situation o [?]
N ,  Simple Temporal Network with Uncertainty (STNU).
An STNU is a 5-tuple, hN, E, l, u, Ci, where N, E, l and u 2  [10, 20]  [10, 20]  C  A  (a)  C - B <= 15, should C happen to execute after time 15.
But executing B sometime after 0 risks violating the constraint, C - B >= 10, should C happen to execute at time 10.
For N !
, the DC-checking algorithm infers the (impossible) constraint, C - B [?]
[5, 0], and thus rejects N !
.
But the strategy, S !
, defined below, is a viable DES for N !
.
C  A B  [2, 15]  B  (b)  [-4, 7]  Figure 1.
Sample triangular networks C0  [10, 20]  [2, 3]  C  For any situation, o, if ZC <= 15 in o, let [S !
(o)]B = 0; otherwise, let [S !
(o)]B = 5.
[10, 15]  S !
clearly violates the important prohibition against decisions that depend on advance knowledge of future events.
In particular, the execution time for B, which must be chosen before C executes, depends on the execution time of C. However, this strategy is viable, since all of the schedules it generates are consistent with the corresponding projections.
Furthermore, it is a DES.
To see this, notice that the pre-history for any schedule that assigns B the value 0 is necessarily empty, but the pre-history for any schedule that assigns B the value 5 necessarily contains information about C0 --since ZC0 is constrained to occur within [2, 3].
Thus, any schedules having the same pre-histories for B assign the same value to B, which is what the DES definition requires.
Thus, according to MMV's definition, N !
is DC, although the DC-checking algorithm rejects it.
Thus, N !
contradicts the soundness of the DC-checking algorithm.
This counter-example employs what should be an irrelevant contingent time-point, C0 , to take advantage of a loophole in the DES definition.
The presence of C0 causes the different sets of schedules to have different pre-histories for B, thereby escaping the main DES requirement.
In so doing, the execution time of B is allowed to depend on advance knowledge of the future execution of C. In view of this contradiction, MMV's proof of the soundness of the DC-checking algorithm must have an error.
In particular, their proof depends on the following claim:3  B  Z  Figure 2.
A counter-example, N !
the schedule S(o) is consistent with the projection No .
S is called a dynamic execution strategy (DES) for N if [S(o 0 )]<x = [S(o 00 )]<x  ==  [S(o 0 )]x = [S(o 00 )]x  for all situations, o 0 , o 00 , and executable time-points, x.
Thus, if two schedules generated by a DES have identical pre-histories for x, they must assign the same value to x.
Dynamic Controllability.
An STNU, N , is dynamically controllable (DC) if there exists a viable DES for N .
The DC-Checking Algorithm.
MMV's DC-checking algorithm infers constraints that any viable DES must satisfy.
Some of the constraints are ternary constraints, called waits.
For example, the networks in Fig.
1 each contain a contingent link, AC, and a requirement link, BC.
For (a), the DC-checking algorithm infers the constraint, B-A [?]
[5, 8].
For (b) it infers a wait: while C is unexecuted, B must wait until at least 13 units after A (i.e., B - A >= 13).
The algorithm also propagates waits.
MMV prove that the rules for generating and propagating waits are sound (i.e., the resulting waits must be satisfied by any viable DES).
They also prove that any STNU accepted by the algorithm has a viable DES (i.e., that the algorithm complete).
2  Let S be a DES, o a situation, t 6= C a time-point, and AC a contingent link where C 6[?]
[S(o)]<t .
Let o 0 be a situation that differs from o only in its value for the duration of AC--but such that C 6[?]
[S(o 0 )]<t .
Then [S(o 0 )]t = [S(o)]t .
Counter-example  Although this property holds in the case of triangular networks, it does not hold for N !
.
In particular, S !
is a DES, o = (2, 10) is a situation (i.e., C0 = 2, C = 10), [S !
(o)]B = 0, and C 6[?]
[S !
(o)]<B = [?].
However, o 0 = (2, 20) is a situation that differs only in the value it assigns to C, C 6[?]
[S !
(o 0 )]<B = {(C0 , 2)}, and yet [S !
(o 0 )]B = 5 6= 0 = [S !
(o)]B .
The STNU, N !
, shown in Fig.
2, contains two contingent links--ZC0 and ZC--and one requirement link: BC.
Since Z is fixed at 0, the duration of ZC is the same as the value of C; and the duration of ZC0 equals the value of C0 .
N !
does not satisfy our intuitive notion of dynamic controllability since there is no safe time to execute B.
For example, executing B at time 0 risks violating the constraint,  3 In  MMV's discussion of the Precede case: "Since C is not in T<B or T<A , TB cannot depend on AC.
Therefore TA and TB are unchanged if the projection is mutated to a projection p0 where AC equals y."
A similar assumption is made in their Lemma 1, which covers propagation of waits.
2 MMV define strategies as mappings from projections to schedules, not  situations to schedules.
However, since situations correspond one-to-one to projections, this difference is purely superficial.
3  3.
Closing the loophole  of generality, let Ci be a contingent time-point such that [S(o 0 )]Ci = k0 , but [S(o 00 )]Ci > k0 .
Let Ai be the activation time-point for Ci .
Then [S(o 0 )]Ai < [S(o 0 )]Ci = k0 implies that [S(o 0 )]Ai = [S(o 00 )]Ai , by the choice of k0 .
The problem with the DES definition is that it compares pre-histories relative to time-point variables.
The revised DES definition, given below, instead compares pre-histories relative to fixed times, k [?]
R. To facilitate comparison, the revised definitions for DES and DC are marked by asterisks.
Lemma 2.
If S is a DES* for N , then it is a DES for N .
Proof 2.
Suppose [S(o 0 )]<x = [S(o 00 )]<x for some o 0 and o 00 , and executable x.
Let k0 , Ai and Ci be as in Lemma 1.4 Since S(o 0 ) and S(o 00 ) agree about Ai , but not Ci , it follows that Ci 6[?]
[S(o 0 )]<x = [S(o 00 )]<x .
Thus, Ci occurs at or after x in both S(o 0 ) and S(o 00 ).
Thus, x occurs before k0 in both.
Thus, [S(o 0 )]x = [S(o 00 )]x .
Pre-histories relative to a number k. Given an STNU N , a schedule T , and some k [?]
R, then T <k denotes the pre-history of T relative to k, which specifies the durations of the contingent links in N that finish before k in T : T <k = {(Ci , TCi - TAi ) : Ai Ci [?]
C, TCi < k}  Lemma 3.
Suppose [S(o 0 )]<k = [S(o 00 )]<k for a DES* S, and some k, o 0 and o 00 .
Then for all time-points t, if [S(o 0 )]t < k or [S(o 00 )]t < k, then [S(o 0 )]t = [S(o 00 )]t .
Furthermore, for all executable time-points x, if [S(o 0 )]x <= k or [S(o 00 )]x <= k, then [S(o 0 )]x = [S(o 00 )]x .
Thus, if S(o 0 ) and S(o 00 ) have the same pre-histories relative to k, then they must agree about the execution times of all time-points before k, and all executable time-points at k.  Dynamic Execution Strategy* .
A strategy, S, is called a dynamic execution strategy* (DES* ) if for any situations, o 0 , o 00 [?]
N , and executable time-point x in N : if [S(o 0 )]x = k and [S(o 0 )]<k = [S(o 00 )]<k , then [S(o 00 )]x = k. In other words, if the strategy S in the situation o 0 assigns the value k to the executable time-point x, then S must also assign k to x for any other situation o 00 whose pre-history relative to k matches that of o 0 .
Proof 3.
Let k0 , Ai and Ci be as in Lemma 1.
By choice of k0 , S(o 0 ) and S(o 00 ) agree about all executions before k0 .
If k0 < k, then Ci executes before k in S(o 0 ) or S(o 00 ).
But then Ci [?]
[S(o 0 )]<k = [S(o 00 )]<k , which implies [S(o 0 )]Ci = [S(o 00 )]Ci , contradicting the choice of Ci .
Thus, k <= k0 , and S(o 0 ) and S(o 00 ) agree about all timepoints executing before k. Finally, suppose [S(o 0 )]x = k or [S(o 00 )]x = k for some executable x.
If k < k0 , then [S(o 0 )]x = [S(o 00 )]x by the choice of k0 ; if k = k0 , then [S(o 0 )]x = [S(o 00 )]x by Lemma 1.
Dynamic Controllability* .
An STNU is called dynamically controllable* (DC* ) if there is a viable DES* for it.
Although the strategy, S !
, is a DES for the network, N !
, it is not a DES* .
To see this, first note that [S !
(o)]<0 = [?]
for all situations o, since no contingent time-point can execute before 0.
Next, note that for some o 0 , [S !
(o 0 )]B = 0.
Thus, for S !
to be a DES* would require [S !
(o)]B = 0 for all situations o.
However, for some o 00 , [S !
(o 00 )]B = 5.
In general, being a DES* requires a strategy to make the same decisions about executable time-points in the same real-time contexts.
As will be shown subsequently, the existing DC-checking algorithm is both sound and complete with respect to the new DES* and DC* definitions.
Lemma 4 shows that the DES* definition correctly captures the prohibition against advance knowledge of future events, which is the property on which MMV based their soundness proof for their DC-checking algorithm.
Lemma 1.
Let S be a DES* , and o 0 and o 00 situations such that S(o 0 ) 6= S(o 00 ).
Let k0 [?]
R be the first time at which S(o 0 ) and S(o 00 ) differ.
Then S(o 0 ) and S(o 00 ) agree about which executable time-points execute at k0 ; however, for some contingent link, Ai Ci , one of S(o 0 ) and S(o 00 ) says Ci executes at k0 , while the other says Ci executes after k0 .
In any case, [S(o 0 )]Ai = [S(o 00 )]Ai < k0 .
Lemma 4.
Let S be a DES* , o a situation, t 6= C a timepoint, and AC a contingent link such that C 6[?]
[S(o)]<t .
If o 0 is the same as o, except that the duration of AC is changed, and C 6[?]
[S(o 0 )]<t , then [S(o 0 )]t = [S(o)]t .
Proof 4.
Let k0 , Ai and Ci be as in Lemma 1.
Thus, S(o) and S(o 0 ) disagree about the duration of Ai Ci .
Thus, AC must be the link Ai Ci .
Without loss of generality, suppose [S(o)]C = k0 and [S(o 0 )]C > k0 .
Since C 6[?]
[S(o)]<t , C must occur at or after t in S(o) (i.e., [S(o)]t <= k0 ).
If t is executable, or if t is contingent with [S(o)]t < k0 , then  Proof 1.
By construction, [S(o 0 )]<k0 = [S(o 00 )]<k0 which, given that S is a DES* , implies that S(o 0 ) and S(o 00 ) agree about the executable time-points that execute at k0 .
Thus, S(o 0 ) and S(o 00 ) can only disagree about the execution of contingent time-points at k0 .
Without loss  4 Proofs  4  of Lemmas 2, 3, 4 and 8 ignore trivial case, S(o 0 ) = S(o 00 ).
[S(o)]t = [S(o 0 )]t by Lemma 1.
If t is contingent with [S(o)]t = k0 , then S(o) and S(o 0 ) must agree about the execution of t's activation time-point.
Furthermore, since t 6= C, S(o) and S(o 0 ) must agree about the duration of the contingent link ending in t. Thus, [S(o)]t = [S(o 0 )]t .
T (t), is also used for partial schedules.
For convenience, T may be viewed as a set of elements of the form, (t, Tt ).
Let u(T ) = max{Tt : t [?]
Dom(T )} denote the maximum execution (MaxEx) time of time-points appearing in T .
If t 6[?]
Dom(T ), then t is unexecuted in T .
The set of time-points that are unexecuted in T is denoted by U (T ).
Similarly, U x (T ), U c (T ) and U a (T ) respectively denote the sets of executable, contingent and activated time-points that are unexecuted in T .
Note that U a (T ) [?]
U c (T ), since only contingent time-points can be activated.
Corollary 4.1.
MMV's DC-checking algorithm is sound with respect to the definitions of DES* and DC* .
4.
Alternative Characterization of a DES*  Respect.
Let T be a (possibly partial) schedule* for an STNU N .
Let o = (o1 , .
.
.
, oq ) [?]
N .
T respects o if for each contingent link, Ai Ci , one of the following holds:  Defining strategies as mappings from (complete) situations to (complete) schedules obscures the real-time features of typical execution scenarios.
For example, an agent typically becomes aware of the unfolding situation only incrementally, over time.
As more contingent durations complete, the space of possible situations contracts.
In addition, when making real-time execution decisions, an agent knows the execution times of only those time-points that have already executed.
Finally, the DES* definition obscures the kinds of execution decisions an agent can make.
This section introduces partial schedules to represent not only the contexts within which an agent must make realtime execution decisions, but also the outcomes of those decisions.
A partial schedule specifies the execution times of some, but not all of the time-points.
However, since partial schedules represent what has actually happened so far, it is important to restrict attention to partial schedules that respect (i.e., are consistent with) at least one situation.
Two kinds of real-time execution decisions (RTEDs) are defined: WAIT and (t, kh).
These can be glossed as: "Wait until some contingent duration completes" or "If nothing happens before t , then execute the (executable) time-points in kh."
The outcome of an RTED depends on the situation, and is represented by a partial schedule that specifies the execution of one or more additional time-points.
The outcome of a WAIT decision involves the execution of only contingent time-points; the outcome of a (t, kh) decision can involve the execution of contingent or executable time-points.
An RTED-based strategy is defined as a mapping from partial schedules to real-time execution decisions.
This section proves that RTED-based strategies correspond one-toone to DES* s. In addition, an RTED-based strategy is used to verify that MMV's DC-checking algorithm is complete with respect to the new DES* and DC* definitions.
(1) neither Ai nor Ci appear in T ; (2) only Ai appears in T , and TAi + oi > u(T ); or (3) both Ai and Ci appear in T , and TAi + oi = TCi .
For each T , the set of situations respected by T is denoted by (T ).
T is called respectful if it respects at least one situation in N (i.e., if (T ) 6= [?]).
If T is both respectful and partial, it is called a respectful, partial schedule (RPS).
A strategy S is respectful if for each o, S(o) respects o.5 The WAIT Decision.
Let T be some RPS for N such that U a (T ) is non-empty (i.e., there is at least one contingent time-point that is activated, but not yet executed in T ).
Then WAIT is an allowable RTED for T .
The Outcome of a WAIT Decision.
If U a (T ) 6= [?]
and o [?]
(T ) is a situation respected by T , then the time at which the next contingent time-point will execute (according to T and o) is defined by: tnc(T , o) = min{TAi + oi : Ci [?]
U a (T )} Since U a (T ) 6= [?
], tnc(T , o) is well defined; and since T respects o, tnc(T , o) > u(T ).
Next, let kha (T, o) [?]
U a (T ) denote the non-empty set of activated contingent time-points that, according to T and o, will execute next, at the time tnc(T , o): kha (T, o) = {Ci [?]
U a (T ) : TAi + oi = tnc(T , o)} Then O(T, o, WAIT) denotes the outcome of the WAIT decision for T in the situation o, which is defined to be: T [?]
{(Ci , tnc(T , o)) : Ci [?]
kha (T , o)} Note that O(T, o, WAIT) is a schedule* that augments T to include all of the contingent time-points that execute at the time, tnc(T , o).
Thus, the MaxEx time for this outcome is tnc(T , o).
In addition, T [?]
O(T, o, WAIT).
Schedules* .
Given an STNU, N = hN, E, l, u, Ci, a schedule* is a (possibly partial) mapping T : N - R. Let Dom(T ) [?]
N denote the domain of T .
If Dom(T ) = N , then T is a (complete) schedule as previously defined; otherwise, T is a partial schedule.
If t [?]
Dom(T ), then t is said to appear in T .
The shorthand notation, Tt instead of  5 A viable strategy is necessarily respectful since S(o) being consistent with No requires S(o) to respect all durations in o; however, a respectful strategy need not be viable, since it need not satisfy all constraints in N .
5  A (t, kh) Decision.
Let T be some RPS for N such that U x (T ) is non-empty (i.e., at least one executable time-point is unexecuted in T ).
If t > u(T ) and kh is a non-empty subset of U x (T ), then (t, kh) is an allowable RTED for T .
Lemma 6.
Let R be an RTEDS, and o some situation.
Then R and o determine a unique sequence of schedules* , T 0 = {(Z, 0)} [?]
T 1 [?]
T 2 [?]
.
.
.
[?]
T a where for each i < a, T i is partial and respects o, T i+1 = O(T i , o, R(T i )), and u(T i ) < u(T i+1 ); and where T a is a complete schedule that respects o.
The Outcome of a (t, kh) Decision.
Let o [?]
(T ) be a situation respected by T .
Then O(T, o, (t, kh)) denotes the outcome of the decision, (t, kh), for T in the situation o.
The outcome depends on the relationship between the numbers tnc(T , o) and t .
For simplicity, let t c = tnc(T , o), and let kha = kha (T, o).
(If U a (T ) = [?
], let t c = [?].)
If t c < t , the outcome involves the execution of only the (contingent) time-points in kha ; if t < t c , the outcome involves the execution of only the (executable) time-points in kh; if t c = t , the outcome involves the execution of the time-points in both kha and kh.
In particular, O(T, o, (t, kh)) is defined by:  if t c < t  T [?]
{(C, t c ) : C [?]
kha }, T [?]
{(x, t ) : x [?]
kh}, if t < t c  T [?]
{(C, t c ) : C [?]
kha } [?]
{(x, t ) : x [?]
kh}, if t c = t  Proof 6.
Given some T i and o, the outcome T i+1 = O(T i , o, R(T i )) deterministically augments T i to include the execution of at least one more time-point.
Thus, the sequence is unique and terminates in a complete schedule, T a .
Furthermore, u(T i ) < u(T i+1 ).
Finally, since T 0 respects all situations, including o, Lemma 5 inductively ensures that each T i , including T a , respects o. Lemma 7.
Any DES* S and situation o together determine a unique sequence of schedules* , s0 = {(Z, 0)} [?]
s1 [?]
s2 [?]
.
.
.
[?]
sb = S(o) where u(s0 ) < u(s1 ) < .
.
.
< u(sb ), and for each i < b, the time-points that appear in si+1 - si are all executed at the time u(si+1 ).
This sequence of schedules* is henceforth called the signature sequence of schedules* for S(o).
If S(o) respects o, then each si respects o.
Note that the MaxEx time of the outcome is min{t c , t }.
In addition, T [?]
O(T, o, (t, kh)).
Lemma 5.
If T is a partial schedule that respects the situation o, and d is an RTED that is allowed for T , then the outcome O = O(T, o, d) also respects o.
Proof 7.
The schedule S(o) involves |N | execution events, some of which may occur simultaneously.
Let 0 = t0 < .
.
.
< tb be the distinct times of those events, where b <= |N |.
For each i, let si+1 = si [?]
{(t, [S(o)]t ) : [S(o)]t = ti+1 }.
Then u(si ) = ti for each i <= b.
If S(o) respects o, then it satisfies condition (3) of respect for each contingent link, implying that each si [?]
S(o) respects o.
Proof 5.
Let Ai Ci be some contingent link.
Case 1: Neither Ai nor Ci appear in T .
Thus, Ci 6[?]
U a (T ) and Ci is unexecuted in O.
If Ai is still unexecuted in O, then condition (1) of the definition of respect is satisfied; otherwise, condition (2) is satisfied, since oi > 0.
Case 2: Ai appears in T , but not Ci (i.e., Ci [?]
U a (T )).
Suppose Ci [?]
kha (T, o).
If d is the WAIT decision or d = (t, khc ) and tnc(T , o) <= t , then Ci is executed in O and condition (3) in the definition of respect is satisfied, since TAi + oi = tnc(T , o).
Otherwise, Ci is not executed in O and condition (2) is satisfied, since t < tnc(T , o).
On the other hand, if Ci 6[?]
kha (T, o), then Ci will not execute until after tnc(T , o).
Thus, Ci is not executed in O and condition (2) is satisfied.
Case 3: Ai Ci is finished and respected by T .
Thus, Ai Ci is finished and respected by the outcome, O [?]
T .
Reachable schedules.
Let S be a respectful DES* and o a situation.
Let s0 [?]
.
.
.
[?]
sb be the signature sequence of schedules for S(o).
Each si is called reachable by S(o).
R(S(o)) = {si : i < b} denotes the S set of reachable partial schedules for S(o).
R(S) = o[?]
R(S, o) denotes the set of reachable partial schedules for the strategy S. By Lemma 7, each s [?]
R(S) respects at least one situation.
Lemma 8.
Let S be a respectful DES* .
If T is reachable by S(o 0 ), and T respects o 00 , then T is reachable by S(o 00 ).
Proof 8.
Let {si0 } and {si00 } be the signature sequences of schedules* for S(o 0 ) and S(o 00 ), respectively.
Let j be the smallest index for which sj0 6= sj00 .
Since T is reachable by S(o 0 ), T = si0 for some i.
Case 1: T = si0 for some i < j.
But then T = si00 and, hence, is reachable by S(o 00 ).
RTED-based Strategy (RTEDS).
An RTED-based strategy for an STNU, N , is a mapping, R, from respectful partial schedules to real-time execution decisions.
Thus, if T is a partial schedule that respects at least one situation, then R(T ) is a real-time execution decision for T .
6  Case 2: T = si0 for some i >= j.
Thus, sj0 [?]
T .
Since T respects o 00 , so does sj0 .
And sj00 respects o 00 by Lemma 7, since S is respectful.
Let u = min{u(sj0 ), u(sj00 )}.
By construction, u is the first time at which sj0 and sj00 differ.
Thus, [S(o 0 )]<u = [sj0 ]<u = [sj00 ]<u = [S(o 00 )]<u .
Since S is a DES* , this implies that S(o 0 ) and S(o 00 ) agree about which executable time-points execute at u.
But since both sj0 and sj00 respect o 00 , they cannot disagree about the execution times for any contingent time-points at u.
Thus, sj0 = sj00 , which contradicts the choice of j.
Proof A.
Let S be a respectful DES* .
Define a mapping R from RPSs to RTEDs, as follows.
First, if T is reachable for S (i.e., T [?]
R(S)): * If x (T ) = [?
], then let R(T ) = WAIT.
* If x (T ) 6= [?
], then let R(T ) = (t x , kh), where t x is the time of the next execution event for any o [?]
x (T ), and kh is the set of executable time-points that execute at the time t x in any such o.
The uniqueness of t x and kh is guaranteed by Lemma 9; thus, R(T ) is well-defined for T [?]
R(S).
For any other respectful partial schedule, T , if U a (T ) is non-empty, let R(T ) = WAIT; otherwise, let R(T ) = (u(T ) + 1, {x}), where x is some time-point in U x (T ).
Finally, for any situation o, let s0 [?]
.
.
.
[?]
sb = S(o) be the signature sequence of schedules* for S(o); and let T 0 [?]
.
.
.
[?]
T a be the unique sequence of outcomes determined by R and o.
It suffices to show that a = b and T i = si for each i--and hence that T a = sb = S(o).
Base Case: s0 = {(Z, 0)} = T 0 .
Recursive Case: si = T i for some i < a.
Then T i is reachable for S, and [T i ]t = [S(o)]t for all t [?]
Dom(T i ).
Thus, for any Cj [?]
U a (T i ), TAi j + oj = [S(o)]Aj + oj = [S(o)]Cj .
If x (T i ) = [?
], then R(T i ) = WAIT and o [?]
c (T i ), which implies that U a (T i ) 6= [?].
Thus, tnc(T i , o) = min{[S (o)]Cj : Cj [?]
U a (T i )} equals t c , the time of the next contingent execution event in S(o), and kha (T i , o) is the set of contingent time-points executing at that time in S(o).
Thus, the outcome, T i+1 , equals si+1 .
However, if x (T i ) 6= [?
], then R(T i ) = (t x , kh).
If o [?]
x (T i ), then the outcome includes the execution of the time-points in kh.
Otherwise, o [?]
c (T i ), and t c < t x implies that the outcome involves the execution of contingent time-points as in the WAIT case.
In either case, the outcome agrees with S(o), implying that T i+1 = si+1 .
Corollary 8.1.
Let T [?]
R(S) be a reachable partial schedule for a respectful DES* S. For any o 0 , o 00 [?]
(T ), T is reachable for S(o 0 ) and S(o 00 ).
Thus, the signature sequences for S(o 0 ) and S(o 00 ) are the same up to T , and hence for each t [?]
Dom(T ), [S(o 0 )]t = Tt = [S(o 00 )]t .
The sets c (T ) and x (T ).
For each T [?]
R(S), let c (T ) denote the set of situations, o [?]
(T ), such that the next execution event in S(o) after u(T ) involves only contingent time-points; and let x (T ) denote the set of situations, o [?]
(T ), such that the next execution event in S(o) after u(T ) involves at least one executable time-point.
Note that (T ) = c (T ) [?]
x (T ).
Lemma 9.
Suppose T [?]
R(S) for a respectful DES* S. If o 0 , o 00 [?]
x (T ), then the time of the next execution event after u(T ) is the same for S(o 0 ) and S(o 00 ).
Call that time t x .
Also, the sets of executable time-points that execute at t x in S(o 0 ) and S(o 00 ) are the same.
Finally, if o c [?]
c (T ), then the time, t c , of the next contingent execution after u(T ) according to S(o c ) is less than t x .
Proof 9.
Let t 0 and t 00 be the times of the next execution events after u(T ) in the schedules S(o 0 ) and S(o 00 ), respectively.
Suppose t 0 <= t 00 .
By Corollary 8.1, S(o 0 ) and S(o 00 ) are the same up to the time u(T ).
Thus, by construc0 0 tion, [S(o 0 )]<t = [S(o 00 )]<t .
Thus, by Lemma 3, S(o 0 ) and S(o 00 ) must agree on all executable time-points that execute at t 0 , which implies that t 0 = t 00 .
Let t x = t 0 = t 00 .
x x Suppose t c >= t x .
Then [S(o 0 )]<t = [S(o c )]<t 0 c which, by Lemma 3, implies that S(o ) and S(o ) must agree on all executable time-points that execute at or before time t x , contradicting the choice of o c [?]
c (T ).
Theorem B.
Each RTEDS is a respectful DES* .
Proof B.
Let R be any RTED-based strategy for N .
Given any o, define SR (o) to be the terminal schedule in the unique sequence of outcomes determined by R and o which, by Lemma 6, is guaranteed to respect o.
Let o 0 , o 00 be a pair of situations for which the DES* property fails.
Let k be the earliest time of such a failure.
Thus, for some executable x, k = [SR (o 0 )]x and [SR (o 0 )]<k = [SR (o 00 )]<k , but [SR (o 00 )]x 6= k. Let {T1j } and {T2j } be the sequences of outcomes determined by following R in the situations o 0 and o 00 , respectively.
Let T1i+1 and T2i+1 be the first pair of outcomes in these sequences that differ.
Thus, T1i = T2i .
Hence, the governing decision at step i was the same: R(T1i ) = R(T2i ).
Theorem A.
Let S be a respectful DES* for N .
Then there exists an RTED-based strategy, R, that is equivalent to S in the sense that for each situation o [?]
N , the signature sequence of schedules* for S(o) is identical to the unique sequence of outcomes determined by R and o.
7  Let T = {(Z, 0)} be the initial partial schedule.
If U (T ) = [?
], then DONE!
If U x (T ) = [?
], let d(T ) = WAIT.
Go to Step 4.
For each x [?]
U x (T ), let [m(x), M (x)] be the current time-window for x in N .
If every contingent link, Ai Ci , for which x has a wait, w(Ai , Ci , x), is activated in T , let W (x) = max{TAi + w(Ai , Ci , x) : Ci [?]
U a (T )}; otherwise, let W (x) = [?].
Let floor (x ) = max{m(x ), W (x )} and go(x ) = min{floor (x ), M (x )}.
Let d(T ) = (t x , kh), where t x = min{go(x ) : x [?]
U x (T )} and kh = {x [?]
U x (T ) : t x = go(x )}.
4.
If d(T ) = WAIT, then wait until some contingent time-point executes.
Otherwise, d(T ) = (t x , kh).
If nothing happens before time t x , then execute the time-points in kh; otherwise, observe the contingent time-points executed at some t c < t x .
5.
Update T to include the execution events from Step 4.
Update N to include the corresponding constraints.
Go to Step 1.
0.
1.
2.
3.
required.)
floor (x ) is the earliest time x can be executed without violating its lower bound, m(x), or any of its relevant waits.
go(x ) is the same except that it enforces the constraint that x not violate its upper bound, M (x).
(MMV, in effect, prove that a conflict between floor (x ) and M (x) is not possible for an STNU accepted by their algorithm.)
In Step 4, the agent waits to see what the outcome of the decision d(T ) will be.
In Step 5, the agent updates T and N to reflect that outcome, before returning to Step 1.
Eventually, the procedure terminates when all of the timepoints have been executed.
For any situation, o, the procedure in Fig.
3 leads the agent through the characteristic sequence of outcomes described in Lemma 6.
Thus, it determines an RTED-based strategy which, by Theorem A, is necessarily a DES* .
Therefore, the strategy generated by MMV's procedure is necessarily a DES* .
Thus, their proof that that strategy is viable ensures that their DC-checking algorithm is complete with respect to the definitions of DES* and DC* .
Figure 3.
MMV's strategy as an RTEDS Let u = min{u(T1i ), u(T2i )}.
Note that u is the earliest time at which SR (o 0 ) and SR (o 00 ) differ.
Case 1: u < k. Then the DES* property holds at u and [SR (o 0 )]<u = [SR (o 00 )]<u implies, by Lemma 3, that SR (o 0 ) and SR (o 00 ) agree about all executable time-points at u, and all time-points before u.
But then [SR (o 0 )]<k = [SR (o 00 )]<k implies that SR (o 0 ) and SR (o 00 ) agree on all contingent time-points at u, too, contradicting choice of u.
Case 2: k <= u.
Then SR (o 0 ) and SR (o 00 ) agree about all time-points before k. Since x is executed at k in SR (o 0 ), the decision R(T1i ) = R(T2i ) must have been (k, kh) for some kh containing x.
But then T2i+1 must be an outcome at time k, which implies that x [?]
kh is also executed in T2i+1 at time k, contradicting that [SR (o 00 )]x 6= k.  5.
Conclusions This paper fixed a technical flaw in MMV's semantics for dynamic controllability.
It presented an alternative characterization of strategies based on real-time decisions and showed that MMV's existing DC-checking algorithm is sound and complete with respect to the revised semantics.
References [1] R. Dechter, I. Meiri, and J. Pearl.
Temporal constraint networks.
Artificial Intelligence, 49:61-95, 1991.
[2] P. Morris.
A structural characterization of temporal dynamic controllability.
In Principles and Practice of Constraint Programming (CP 2006), volume 4204 of Lecture Notes in Computer Science, pages 375-389.
Springer, 2006.
[3] P. Morris, N. Muscettola, and T. Vidal.
Dynamic control of plans with temporal uncertainty.
In 17th Int'l.
Joint Conf.
on Artificial Intelligence (IJCAI-01), pages 494-499, 2001.
[4] P. H. Morris and N. Muscettola.
Temporal dynamic controllability revisited.
In 20th National Conference on Artificial Intelligence (AAAI-2005), pages 1193-1198, 2005.
[5] T. Vidal.
A unified dynamic approach for dealing with temporal uncertainty and conditional planning.
In Fifth International Conference on Artificial Intelligence Planning Systems (AIPS-2000), pages 395-402, 2000.
[6] T. Vidal and H. Fargier.
Contingent durations in temporal csps: from consistency to controllabilities.
In Proceedings of the TIME-97 Workshop, 1997.
[7] T. Vidal and H. Fargier.
Handling contingency in temporal constraint networks: from consistency to controllabilities.
Journal of Experimental and Theoretical Artificial Intelligence, 11(1):23-45, 1999.
[8] T. Vidal and M. Ghallab.
Temporal constraints in planning: Free or not free?
In CONSTRAINT '95 Workshop, 1995.
RTED-based Version of MMV's Verification Strategy.
To prove that any STNU accepted by their DC-checking algorithm is dynamically controllable, MMV presented not a mapping from situations to schedules, but a procedure for incrementally generating a single schedule in response to the unfolding situation.
Space limitations preclude duplicating that procedure here.
Instead, Fig.
3 presents an equivalent procedure for generating real-time execution decisions.
Although the computations are equivalent, they are structured around partial schedules and RTEDs.
The procedure in Fig.
3 starts with an initial partial schedule, T = {(Z, 0)}.
It then computes an RTED, d(T ), in Step 2 or Step 3, as follows.
If all executable time-points have already been executed, then d(T ) = WAIT (Step 2).
Otherwise, d(T ) = (t x , kh) based on the values for t x and kh computed in Step 3.
These values are based on the current time-windows for the as-yet-unexecuted executable timepoints, which can be computed using an all-pairs, shortestpath algorithm.
(No generation or propagation of waits is 8
Performance of Querying Temporal Attributes in Object-Relational Databases Carsten Kleiner, Udo W. Lipeck University of Hannover Institute for Information Systems 30159 Hannover, Germany {ck | ul}@informatik.uni-hannover.de  Abstract In this paper we evaluate a model for temporal data utilizing the benefits of object-relational database systems (ORDBS).
In particular we show how attribute timestamping can be efficiently implemented in state-of-theart ORDBS.
The attribute timestamping concept is based on introducing user-specific types for temporal versions of datatypes.
Moreover on the physical level we make use of user-defined index structures; in particular adapted spatial indexes based on generalized search trees are applied.
These index structures greatly improve performance of important operators on both valid and bitemporal datatypes and show the effectiveness of this approach.
Keywords: ORDBS, temporal datatypes, attribute timestamping, physical design, user-defined index structures  1.
Introduction and Related Work Many recent complex applications involve time-varying information.
Dependency on time is in most cases not a feature of a real-world object alone but also of its attributes.
Temporal data is treated differently from spatial data; whereas the spatial position or extent of an object is usually perceived as a regular attribute of an object, time is more fundamental.
All information (i. e. all attributes, standard as well as spatial) may be subject to change over time.
Time seems to be the governing dimension behind all information.
In the spirit of object-oriented modeling using time on the attribute level as opposed to the class level should be preferred.
This facilitates one-to-one modeling of real-world objects.
It essentially means using attribute-timestamping as opposed to the widely used tupletimestamping.
We even propose to go one step further in modeling and suggest that combinations of attribute values and time should be seen as basic value units; thus we say that the datatypes to be used in temporal appli-  cations should be temporal versions of basic datatypes, e. g. vt_integer.
The reason for using 'datatype-timestamping' is the more adequate modeling capability.
Due to space constraints we only give a brief overview of the new concepts; details can be found in [6] as well as [4].
The theoretical foundation of the conceptual model for attribute-timestamping is derived from the bitemporal conceptual data model ([3]).
ORDBS were introduced and described in detail in [8].
Temporal extensions of ORDBS based on tuple timestamping have been presented recently by [1] and [12].
In contrast to those, our approach is rather based on attribute timestamping; such models were e. g. described in [9], but not considered in recent years since they are difficult to implement efficiently in purely relational database systems1 .
The connection of user-defined datatypes, attribute-timestamping and ORDBS has to the best of our knowledge never been researched before.
Generalized search trees (GiST) which we will use as foundation were introduced in [2] as extensible indexing structures for databases.
The idea of using spatial index structures for indexing pure temporal data has been used in several papers and is described in [7].
In this work we extend that idea to the newly defined temporal datatypes and embed it into an extensible indexing framework.
2.
Physical Design The physical model is developed by defining temporal versions of datatypes (conceptual and logical model are explained in [6]).
2.1.
Operators For temporal datatypes as combinations of temporal and standard datatypes (e. g. vt_integer for valid time dependent integer values or bt_integer for bitemporal integer values), queries can be differentiated by which dimen1 or internally modeled by tuple timestamping  2.2.
Indexing By using the extensibility features of ORDBS like Oracle9i as described in [4] together with the generalized search tree approach ([2]) one can add appropriate index structures.
We use attribute timestamping here and implement the index in an extensible framework (GiST).
A similar approach for tuple timestamping without using the GiST framework has been described for Informix in [1].
Similarly no extensible framework was used for other temporal index structures such as RI-Tree, GR-Tree and 4R-Tree.
Also these indexes only use the temporal information but not the thematic information.
Thus only temporal queries will be efficiently supported.
Thematic and combined queries will perform poorly (cf.
spatial indexes on temporal queries evaluated in section 3, such as 2D R* -Tree on valid time intervals).
Since temporal datatypes combine values from different domains or dimensions into a single item to be indexed, the use of index structures from spatial databases seems straightforward.
One can view the standard value (such as salary) as one dimension and the valid (as well as the transaction) time information as another dimension2.
Thus in the case where the standard datum is from a onedimensional domain (such as integer) we obtain twoor three-dimensional domains for temporal datatypes which may be indexed by spatial indexes.
E. g. the temporal integer value (3000,[10,30)) is treated like the two-dimensional interval [(3000,10),(3000,30)].
Since the standard datum may not remain one-dimensional (e. g. spatio-temporal data) it is not recommended to use the 2D (or at best 3D) spatial indexes shipped with current ORDBS, but rather to use user-defined extensible indexes which may be adapted to as many dimensions as required.
2 Special properties of the valid and transaction time dimension such as now or UC should be simple to add due to the extensibility features of the index structures used.
3.
Performance Evaluation 3.1.
Efficiency for Valid Time Data For the temporal datatype vt_integer performance tests with different index structures and query types were conducted on synthetically generated data as well as on data generated using the S PY T IME-benchmark.
Comprehensive results can be found in [6], figure 1 shows sample results on range//range queries of different selectivities.
14 Without Index Built-in B-tree (salary) User-defined B-tree (salary) Built-in B-tree (start validtime) User-defined R*-tree (valid time) User-defined 2D R*-tree User-defined 2D RSS-tree  12  10  response time  sions the desired results are restricted to, and whether they query for a range in a dimension or a point.
For these query types we can use the notational conventions of [10], e. g. 'range//point/range' means querying for a range in the standard attribute domain, a certain point in valid time and a range in transaction time.
In addition, our experiments showed that the size of the range in a range query is an important measure.
Therefore experiments on range queries were performed with several different selectivities.
The sample results for temporal queries in section 3 are presented with the objective of finding the optimal index structure for all possible query types.
This is important, since in most cases only one index can be built on a single column, and it should provide for good query performance of all temporal, thematic and combined operators.
8  6  4  2  0 0  5  10 selectivity in %  15  20  Figure 1.
Index Performance on vt_integer Using no index at all or simply a B* -tree on salary (built-in or user-defined) led to inacceptable results as expected.
The creation of a B* -tree on start point of the valid time interval showed acceptable performance but requires a complex recoding of queries to be able to use the index.
Also this approach would not be scalable to more complex datatypes.
The 2D RSS-Tree3 outperforms all other indexes for all selectivities, since it provides good clustering and subdivision of space in each dimension, not just in one as several others.
The distance based clustering leads to outperforming the 2D R* -Tree.
A little bit surprising is the good performance of the R* -Tree on valid time intervals which almost outperforms its 2D counterpart.
This is due to the low execution times for the operator on the integer component and would not scale well for more complex datatypes.
The idea of using two-dimensional indexes significantly improves query performance, especially for the 2D RSS-Tree by factors of between 2 and 6.
For range//point queries we obtained a similar picture but this time the R* -Tree on valid time was slightly faster than 2D RSS-Tree and 2D R* -Tree, since it can optimally exploit the query point information in valid time.
Point//range queries showed exactly reversed results with 3 A combination of R* - and SS-Tree, see [5, 11] for details.
the 2D RSS-Tree being faster by a much bigger difference.
Thus the 2D-RSS-Tree should be used, since it provides a good performance over all possible query types.
3.2.
Efficiency for Bitemporal Data The bitemporal datatype bt_integer can be interpreted as multidimensional data in the form of rectangles in three-dimensional space.
Figure 2 summarizes performance of the different indexing options exemplarily for range//range/range queries.
14 Without Index User-defined B-tree (salary) User-defined 2D R*-tree (bitemporal) User-defined 2D R*-tree (valid time) User-defined 3D R*-tree User-defined 3D RSS-tree  12  response time  10  8  4.
Future Work Our work focused on the physical implementation of temporal information.
In order to use it for a complete temporal database it needs to be embedded into a temporal query language and user-friendly environment.
The experiments with spatial indexes need to be extended to different temporal datatypes.
In particular the case of complex base types, leading to higher dimensional temporal types would be interesting.
Results for spatiotemporal data (valid time only) will be reported in [4] but more work is required.
Also, details about user-defined selectivity estimation will have to be investigated.
Choosing the appropriate execution plan will become more important with more complex datatypes.
Finally the special characteristics of now and UC (until changed ) in valid and transaction time, respectively, have to be taken into account in more detail in future work.
6  References 4  2  0 0  5  10  15  20  selectivity in %  Figure 2.
Index Performance on bt_integer Options that already performed poorly on datatype vt_integer, such as no index, built-in or user-defined B* -tree on salary, again performed poorly and are not considered any further.
The three-dimensional indexes outperform all other indexes significantly by factors between 2 and 6 where results for 3D R* -Tree and 3D RSS-Tree were almost the same this time.
Among the better indexes are user-defined 2D R* -Trees (either on valid time intervals or on bitemporal rectangles).
If point//range/range queries are also considered the pure temporal indexes perform extremely bad since they do not support the strongest restriction on salary.
In this case the B* -Tree index on salary is the most efficient index as expected, but the three-dimensional indexes also perform pretty good.
To support all kinds of operators on bt_integer the only choice can be the 3D indexes; differences among R* Tree and RSS-Tree are not significant.
Coupled with the results for vt_integer the RSS-tree should be preferred.
Similarly to vt_integer the index entries are not approximations of real objects but rather the objects themselves.
Thus query results can be directly taken from the index-based filter step and no refinement is necessary.
This greatly improves index performance since the call overhead associated with exact operators is absent.
Thus no userdefined selectivity estimation is required since an indexbased execution is always faster than a full table scan.
[1] R. Bliujute, S. Saltenis, G. Slivinskas, and C. S. Jensen.
Developing a datablade for new index.
In Proceedings of 15th International Conference on Data Engineering (ICDE'99) [2] J. Hellerstein, J. Naughton, and A. Pfeffer.
Generalized search trees for database systems.
In Proceedings of the 21th International Conference on Very Large Data Bases 1995 [3] C. Jensen, M. Soo, and R. Snodgrass.
Unifying temporal data models via a conceptual model.
Information Systems, 19(7):513-547, 1994.
[4] C. Kleiner.
Modeling Spatial, Temporal and SpatioTemporal Data in Object-Relational Database Systems.
PhD thesis, Universitat Hannover, 2002. in preparation.
[5] C. Kleiner and U. W. Lipeck.
Efficient index structures for spatio-temporal objects.
In Proceedings of 11th International Workshop DEXA 2000 [6] C. Kleiner and U. W. Lipeck.
Natural and efficient modeling of temporal information with object-relational databases.
Technical Report 01-2002, Universitat Hannover, Apr.
2002.
[7] Y. Manolopoulos, Y. Theodoridis, V. J. Tsotras.
Advanced Database Indexing.
Kluwer Academic Publishers, 2000.
[8] M. Stonebraker and P. Brown.
Object-Relational DBMSs - Tracking the Next Great Wave (Second Edition).
Morgan Kaufmann Publishers, 2nd edition, 1999.
[9] A. Tansel, J. Clifford, S. Gadia, S. Jajodia, A. Segev, and R. Snodgrass, editors.
Temporal Databases: Theory, Design, and Implementation.
Benjamin/Cummings, 1993.
[10] V. J. Tsotras, C. S. Jensen, and R. T. Snodgrass.
An extensible notation for spatiotemporal index queries.
SIGMOD Record, 27(1):47-53, Mar.
1998.
[11] S. Wang, J. M. Hellerstein, and I. Lipkind.
Near-neighbor query performance in search trees.
Technical Report CSD98-1012, University of California, Berkeley, Sept. 1998.
[12] J. Yang, H. C. Ying, and J. Widom.
Tip: A temporal extension to informix.
In Proceedings of the ACM SIGMOD International Conference on Management of Data 2000
Free Schedules for Free Agents in Workow Systems Areas: time in multiple agents, communication, and synchronization; temporal constraint reasoning    Claudio Bettini,  Abstract This paper investigates workow systems in which the enactment and completion of activities have to satisfy a set of quantitative temporal constraints.
Dierent activities are usually performed by autonomous agents, and the scheduling of activities by the enactment service has among its goals the minimization of communication and synchronization among the agents.
The paper formally denes the notion of a schedule for these workow systems and it identies a particularly useful class: free schedules.
A schedule species a time range for the enactment, duration, and completion of each activity in order to satisfy all the temporal constraints in the workow.
In a free schedule, an agent has to start its activity within the range specied in the schedule, but it is free to use any amount of time to nish the activity as long as it is between a minimum and maximum time he has declared when the workow is designed.
No synchronization with other agents is needed.
The paper provides a method to characterize all the free-schedules admitted by a workow specication, and an algorithm to derive them.
1 Introduction A workow is a complete or partial automation of a business process, in which participants (humans or machines) involve in a set of activities according to certain procedural rules and constraints.
The successful completion of the process often depends on the correct synchronization and scheduling of the activities.
The modeling and reasoning tools provided by this paper are intended to address so called production workows with particular emphasis on processes involving loosely coupled, largely  DSI, Universit a di Milano, Italy.
bettini@dsi.unimi.it y Dept.
of Info.& Software Systems Eng., George Mason  University, VA. fxywang, jajodiag@gmu.edu  y  X. Sean Wang,  y  Sushil Jajodia  distributed information sources where the agents cooperating in a workow process are essentially independent from each other.
We consider the inclusion in the workow specication of quantitative temporal constraints on the duration and distances of individual activities.
As a simple example, consider an online vendor workow, including the following activities that must be performed upon the receipt of an order by a customer: (a) order processing, (b) shipping, and (c) payment collection.
These activities have certain conditions concerning their timing that may impose temporal distances (possibly involving different time granularities).
For instance, the order processing must occur within one business day after the order is entered (and the whole workow process is enacted), the order must be transmitted to the shipping sites within ten hours after the end of order processing, and the payment for the merchandise must be made within a time window starting and ending one month before and after delivery, respectively.
The payment collection activity has a duration range (i.e., minimum and maximum time) specied in terms of business days, e.g., the activity can take as little as one business day and as much as 5 business days.
(These requirements are included in the graphical representation of Figure 2 later in the paper.)
The inclusion of temporal constraints naturally leads to questions about how to check the overall consistency of the specication and about how to apply some form of useful temporal reasoning; for example, how can we predict when a certain agent may be asked to perform an activity?
However, these questions can be addressed quite easily applying known techniques in constraint reasoning.
In this paper, we concentrate on a dierent issue, related to the enactment service of the workow system, which has to schedule the dierent activities in order to guarantee a successful completion of the workow.
This must be done considering all the constraints, and the fact that each activity is per-  formed by a relatively autonomous agent, which, in general, may take a dierent amount of time to complete the same activity.
We assume that each agent in the workow system declares a time range (the minimum and the maximum amount of time) it usually needs to nish a particular activity.
It is then desirable to allow the agents to take any amount of time within the declared time range to nish their work.
However, the time each agent actually takes may have some impact on the overall temporal constraints, because there may be constraints relating the ending times of activities.
The question is whether there exists a schedule for the activities' enactment such that, no matter when each agent nishes the activity within the declared time range (i.e., using any amount of time between the declared minimum and maximum), the overall temporal constraints are not violated.
We call this a free schedule.
The main contribution of this paper is a formal characterization of free schedules and an algorithmic method to derive them from the workow specication .
Little attention has been given in the literature to modeling advanced temporal features for workow systems.
Commercial workow systems (as reviewed, e.g., in [ADEM97]) are usually limited in specication of temporal conditions for each individual activity or for the global plan and do not provide temporal reasoning.
There are two recent papers on related issues.
The authors of [MO99] propose a framework for time modeling in production workows.
They also provide an eAcient algorithm to check if a constraint (like a deadline or inter-activity constraint) is \implied" by the given activity duration constraints and by the workow structure.
This is similar to checking in our framework that a free schedule retains its \free" property upon the addition of certain constraints.
However, it does not give a method to derive a free schedule starting with a global set of intra- and interactivity constraints.
The second paper, [EPR99], is concerned with temporal constraints reasoning and management in the context of workows.
Their temporal constraints form a subclass of those considered in this paper, due to the \well structured" requirement in [EPR99].
Their reasoning about the allowed \buer time" for parallel activities, is similar to deriving free schedules, even if using completely dierent methods.
Regarding the general scheduling problem, a rich literature exists on the subject (see [SC93] for a good survey).
However,  our method exploits the particular structure of the constraints, and, for this reason, it has much better computational properties than general purpose scheduling algorithms.
The paper is organized as follows: In the next section, we dene the workow model, and in Section 3, we formally characterize free-schedules and provide methods to derive them.
We conclude the paper in Section 4.
2 The workow model A typical workow management system provides a formalism and tools to specify workow activities.
We follow the consensus workow glossary [WfMC99] for the choice of operators to specify the workow structure.
The relation between activities can be specied by sequential routing, as well as through the use of the operators OR-split, AND-split, OR-join, and AND-join.
An OR-split identies a point within the workow where a single thread of control makes a decision upon which branch to take when encountered with multiple alternative workow branches, while an AND-split is a point where a single thread of control splits into two or more parallel activities.
OR-joins and AND-joins identify points where various branches re-converge to a single path.
Note that an OR-join, as opposed to the AND-join, does not require synchronization, since the ancestors activities are not executed in parallel but they are alternatives.
In this paper, we do not consider loop operators.
Our techniques can still be applied to workow processes involving loops when loops can be modeled as complex activities, estimating time bounds for their execution by the agents in charge.
A workow graphical description is shown in Figure 1.
An arrow between two activities denotes sequential routing, while arrows going through a circle denote OR-split operators (C in the circle denotes the associated condition).
AND-splits are implicit when more than one arrow originates from the same activity.
All joins are implicit when more than one arrow lead to the same activity, where an OR-join is intended when the arrows originate from alternative activities, and an AND-join when from parallel activities.
The dashed arrows mean that details on other activities in that part of the workow are omitted.
Our temporal extension allows a workow designer to include two types of temporal constraints: 2  Order Collection  OCe [0,1]b-day < <  OPb  OPe  Order Processing  < OR  [0,1]b-day  [1,10]hours  Bb  [1,10]hours  C  ISb  <  ISe  DSb  <  <  Be  DSe <  International Shipment  Domestic Shipment  Billing [1,2]b-day  [1,5]b-day  < LDb  Local Delivery  PCe [-1,1]month  PCb  Figure 2: Temporal constraints in a workow  Payment Collection  stands for the fOgrder fCgollection activity, OP for fOgrder fPgrocessing, etc.
The symbol `<' is used as a shortcut of [1; +1].
More than one constraint can be associated with an arc, with conjunction of them being the semantics.
For example, from Bb to Be, [0; 1] b-day forces the end of the Billing activity to occur in the same or next business-day as the beginning, while `<' forces the duration of the activity to be positive, since this is not enforced by the rst constraint.
Each workow constraint graph can be easily decomposed according to the OR-split operators in a set of subgraphs each one representing one possible workow execution thread.
In our running example, the graph in Figure 2 is decomposed into a subgraph taking the left branch of the OR-split (i.e., performing I S b and I S e), and another one taking the right branch (i.e., performing DS b and DS e).
Note that each subgraph denes a constraint problem commonly known as STP (Simple Temporal Problem).
When all constraints are in terms of the same granularity there are eAcient algorithms to check consistency and to derive the minimal network [DMP91].
For uniformity with the CSP literature we call these subgraphs constraint networks.
Figure 1: A workow process description (a) Constraints in each activity's description, as duration constraints and deadline constraints; (b) Constraints in the workow process description, as quantitative constraints on the starting/ending of dierent activities.
We represent these constraints as (binary) distance constraints Xj Xi 2 [m; n]G where Xj and Xi are variables representing the instant starting or ending an activity, m and n are either integers or one of the special symbols 1 and 1, respectively, and G denotes the time granularity in terms of which the distance is measured.
These constraints, known as TCG (Temporal Constraints with Granularity) have been formally studied in [BWJ98, BWJ97].
When G is omitted, the distance is intended in the units of the time domain.
A particular case is that of unary constraints, as Xj 2 [m; n]G, that can force the domain of variables to take values in a specic subset of the time domain.1 We use Dom(Xj ) to denote the domain of Xj .
The constraints informally introduced in Section 1 about our running example, are illustrated using a graph in Figure 2.
Each node in the graph is labeled by the initials of the activity's name followed by `b' for begin or `e' for end.
For example, OC 1 We  LDe  3 The generation of enactment schedules Every time the enactment agent dispatches an activity, it needs to provide a set of constraints on the  use positive integers as the time domain.
3  beginning and ending times to the agent in charge of that activity, such that if each agent satises the constraints, the whole workow can be successfully carried out.
To illustrate, consider a renement of the Domestic Shipment (DS) activity.
Upon completion of the order processing, the online vendor asks directly the suppliers of the requested products to ship them to one of its warehouses located in the area of the customer for their nal delivery.
Obviously, the workow requires some sort of synchronization to ensure that all the products will be delivered to the customer within a certain time to reduce the need of warehouse space.
In Figure 3 we consider the example of two activities S and S corresponding to the shipments to be made by two suppliers.
Each supplier has provided minimum and maximum bounds for the handling and shipping of its products (constraints [3, 7] and [1, 3] respectively).
The two activities must start after order processing and not later than 10 time units from it (constraints [1, 10]).
Also, the nal delivery must begin after all products are available and none of the products must wait more than 5 time units at the warehouse (constraints [1, 5]).
the 5 hours allowed by the constraint.
To solve this problem, we can delay the starting time for S to 11am.
0  3.1  Given the problem description in terms of a set of activities to be enacted, the minimum and maximum durations declared by each agent in charge of an activity, and other temporal constraints involving dierent activities, we want to provide to the enactment service a schedule for these activities.
For the sake of simplicity, in the rest of this paper we restrict our technical investigation to the case of graphs where all constraints are or can be converted in terms of a single granularity.
We formally dene the notion of a schedule.
0  Denition  A schedule for a set of activities A1 ; : : : ; Ak whose constraints are represented in a network N , is a set of triples of intervals on positive integers ([E begini ; Lbegini]; [mi ; ni ]; [E endi ; Lendi ]), one for each activity, such that: if for each activity Ai , a pair (xi ; yi ), with xi in [E begini ; Lbegini ], yi in [E endi ; Lendi ] and (xi ; yi ) satises the constraint [mi ; ni ], is used to instantiate the corresponding pair of beginning and ending nodes for Ai in N , then this assignment can be extended to a solution of N .
OPe [1,10]  [1,10]  Sb  Sab  The rst and last intervals in a schedule identify the allowed beginning and ending instants, respectively (L and E stand for Latest and E arliest, respectively), while the second interval identies the minimum and maximum duration, called the duration constraint.
Hence, if all the involved activities actually begin and end within the given bounds and do not violate the duration constraints, the schedule guarantees that all the constraints in the network are still satisable.
The motivation for this denition is that the agents responsible for the activities should be allowed to act independently from other activities.
Each agent needs only adhere to a \local" constraint and the global constraint should be satisable if each agent satises the local constraint attached to it.
As an example, assume OP e in Figure 3 happens at time 8.
Then the following is a schedule for activities S and S :  [1,3]  [3,7]  Se  Sae [1,5]  [1,5]  Schedules and Free Schedules  LDb  Figure 3: The portion of constraint graph involving the shipping activities S and S 0  In this case, the enactment service cannot assign S and S with the same earliest possible starting times.
Indeed, assume we are using granularity hours and suppose the order processing completed at 8am, and S and S are assigned to start at 9am.
According to duration constraints, activity S could take 7 hours to complete, while activity S could take 1 hour only.
Hence, the products shipped by h([9,9],[3,4],[12,13]),([9,9],[1,2],[10,11])i.
S will have to stand at the warehouse more than Indeed, since S nishes (i.e., S e) either at 12 or at 0  0  0  0  0  4  ([E begink ; Lbegink ]; [mk ; nk ]; [E endk ; Lendk ])i for the activities A1 ; : : : ; Ak within a constraint network N , is called free if (i) the duration constraint in the schedule is the same as that in N (i.e., mi = minDi and ni = maxDi ) for each activity Ai , and (ii) E endi = E begini + mi and Lendi = Lbegini + ni .
13, and S nishes (i.e., S e) either at 10 or 11, in all the possible cases, it's possible to assign LDb a value to satisfy all the constraints in the network.
We say that a schedule is well-formed if there are no \redundant" values in each triple.
Formally, a triple has no redundant values if for each value in the beginning/ending interval there exists one in the other interval such that the pair satises the duration constraint (i.e., the middle interval in the schedules).
Moreover, each value in the duration constraint range should be used by one of these pairs.
For example, ([9; 10]; [3; 4]; [12; 13]) is wellformed since (i) each value in the beginning domain can nd a value in the ending domain to satisfy the constraint (e.g., 9 nds 12 and 10 nds 13), (ii) each value in the ending domain can nd a value in the beginning domain to satisfy the constraint (e.g., 12 nds 9 and 13 nds 9), and (iii) each duration value (i.e., 3 and 4) can nd the beginning and ending pairs to have the exact durations (e.g., 9 and 12 and 9 and 13, respectively).
Note that in the constraint networks and schedules, constraints may involve +1 or 1 and domains may be innite.
For simplicity of presentation, we don't explicitly treat 1 in our exposition.
However, it's not diAcult to slightly extend the arithmetics to take them into account.
Many dierent schedules can exist for a given workow and dierent criteria can be adopted to identify most interesting ones.
When agents for activities operate independently, schedules which do not modify the original duration constraints that these agents declared in the workow specication, seem to be preferable to those which restrict those duration constraints.
We call these schedules free schedules.
If the restriction is unavoidable, a schedule which restricts the maximum duration, (called restricted due-time schedule) is preferable to one forcing a greater minimal duration (called bounded schedule).
In the following, we formally characterize free schedules, and concentrate on the procedure to generate them.
We assume for each activity Ai , the duration constraint from Ai b to Ai e in the network is [minDi ; maxDi ].
0  0  In this case the agent of the activity is just told the set of time instants at which it can begin and it simply has to meet the duration constraints as declared in the workow specication.
Referring to our running example and assuming the time for OP e is 8, we nd one of the free schedules is the one assigning 9 and 11 to the beginning of S and S , respectively.
The schedule can be represented as h([9,9],[3,7],[12,16]), ([11,11],[1,3],[12, 14])i.
This is clearly a schedule by denition.
It is also clear that this is a free one since the duration constraints [3; 7] and [1; 3] are as declared in the constraint network, and condition (ii) is easily veried to be true.
0  3.2  Finding free schedules  We propose a general algorithm for free schedule generation (FSG algorithm from now on) which consists of three main steps: 1. decompose the constraint graph according to OR-split operators and derive the minimal network for each resulting constraint network; 2. for each network characterize the set of free schedules for the given set of activities to be scheduled; 3. derive a particular free schedule from the result of Step 2 above, according to some criteria.
Deriving the minimal network in Step 1 can be easily done by applying a path-consistency algorithm since each constraint network denes an STP.
The minimal network guarantees that none of its constraints can be tightened without loosing a possible solution.
If the original duration constraints for the activities to be scheduled are tightened during Step 1, no free schedule can exist and the algorithm terminates.
Indeed, this will mean that one of the minimum (or maximum) duration declared by an agent cannot be used as part of any solution, i.e., if the activity really uses the minimum (or maximum) amount of time as declared,  Denition A free schedule is a schedule such that for each activity, the associated constraint only imposes a time window for the beginning of the activity, while the duration constraint is the original one in the network, and the ending interval is implicit.
Formally, a schedule h([E begin1; Lbegin1]; [m1 ; n1 ]; [E end1 ; Lend1 ]), .
.
.
, 5  then the whole network is not satisable, and some constraints sooner or later will be violated.
When the FSG algorithm is called for a schedule of the activities S and S in our example, Step 1 derives the implicit constraints and domains depicted in Figure 4.2 In this case, neither durations were tightened, i.e., they maintained the original values.
This means that the use of the minimum or maximum durations are not entirely ruled out by the network, and we can still hope to derive free schedules.
We now want to understand the properties of schedules in terms of sync-free intervals.
Lemma 2 Let S be a well-formed schedule for activities A1 ; : : : ; An in a network N , Ai and Aj be any two of these activities, I (Ai ) the beginning or ending interval in S for Ai , and similarly I (Aj ) for Aj .
Then, the intervals I (Ai ) and I (Aj ) are syncfree with respect to the temporal constraint between the corresponding nodes in N .
Conversely, given a minimal network N including the constraints among n activities, any set of triples ([E begini ; Lbegini]; [mi ; ni ]; [E endi ; Lendi ]) for i = 1 : : : n with the values in the rst and last intervals included in the domain of the corresponding node in N , and the second included in the corresponding constraint in N , is a schedule if the above sync-free property holds on each pair of beginning/ending intervals of dierent activities with respect to the corresponding constraint in N .
0  Dom={8} OPe [1,10]  [1,10]  Dom={[9,18]}  [-4,10]  Sb  Dom={[9,18]}  Sab [-1,11]  [1,3]  [3,7] [-3,7] Dom={[12,25]}  Se  Dom={[10,21]}  Sae  [-4,4]  If we look at the example of activities S and , and we consider free schedules having a single starting value, from Lemma 1 we see that the following conditions must be satised: xS xS  M in(T C (S b; S b)) xS xS  M ax(T C (S b; S b)) xS + minDS xS  M in(T C (S b; S e)) xS + maxDS xS  M ax(T C (S b; S e)) xS + minDS xS  M in(T C (S b; S e)) xS + maxDS xS  M ax(T C (S b; S e)) xS + minDS (xS + maxDS )  M in(T C (S e; S e)) xS + maxDS (xS +minDS )  M ax(T C (S e; S e)) where xS and xS are the starting values, T C (node1 ; node2 ) represents the fTgemporal fCgonstraint assigned to the arc from node1 to node2 , and M in(T C (node1 ; node2 )) and M ax(T C (node1 ; node2 )) represent the lower and upper bounds of the constraint, respectively.
A simple elaboration of these equations and its generalization to k activities provides the basis for Theorem 1.
[1,5]  [1,5]  S  LDb Dom={[13,26]}  0  0  0  Figure 4: The minimal network derived by pathconsistency  0  0 0  0  0  0  0  0  0  0  Step 2 is nontrivial and it is supported by a theoretical result which needs some preliminary observations.
0  0  0  Denition A pair of intervals I and I is called sync-free with respect to a constraint on the dis-  0  0  0  0  0  0  0  0  tance between elements of I and elements of I , if each pair (x; y ) with x 2 I and y 2 I satises the constraint.
0  0  For example, let I = [9; 10] and I = [11; 13].
Then I and I are sync-free with respect to the constraint [1; 4], but not sync-free with respect to [2; 4], nor [1; 3].
A simple test for sync-freeness is given by Theorem 1 Let A1 ; : : : ; Ak be activities to be enacted, and N a minimal network of constraints on Lemma le:sync.
0  0  these and possibly other activities endpoints.
The Lemma 1 A pair of intervals I = [minI ; maxI ], set of all free schedules for A1 , .
.
.
, Ak in N having a single value as the beginning interval is the set I = [minI ; maxI ] is sync-free with respect to conof schedules of the form ([xi ; xi ]; [minDi ; maxDi ]; straint [m; n] from I to I , if and only if: (minI [xi + minDi ; xi + maxDi ]) for each activity Ai , maxI )  m and (maxI minI )  n 2 For the sake of clarity, we do not depict the implicit where minDi and maxDi are the duration bounds constraints originating from the rst and last node since they for Ai in N , and the values x1 ; : : : ; xk satisfy the conditions: won't be used in the algorithms.
0  0  0  0  0  0  6    for each activity Aj , (  ( j ))  (  ( j ))  xj  M in Dom A b    Consider our running example, and, in particular, the minimal network as shown in Figure 4.
Applying Theorem 1, we obtain from the rst condition xS 2 [9; 18], xS 2 [9; 18], and, from the second condition, 2  xS xS  4.
These constraints dene a new network having only two nodes for S and S with the corresponding domains and a single arc from S to S labeled by the constraint [2; 4].
When the propagation algorithm is applied, the resulting minimal network is identical except that the domain of S has been tightened to [11,18].
Indeed, the value 9 and 10 cannot be part of any solution.
According to the procedure illustrated above, the earliest single-valued free schedule is obtained taking as starting instant xS and xS , the values 9 and 11, respectively, as they are the minimal values of the minimal domains in the network representing the solution space.
It is easily checked that it is a free schedule.
Note that this is exactly the example given at the end of Section 3.1.
M ax Dom A b    for each pair of activities hAi ; Aj i with i < j , xi + kij  xj  xi + Kij , where the constants kij and Kij are derived from the following:  0  0  0  = M ax((maxDi minDj + M in(T C (Ai e; Aj e))); M in(T C (Ai b; Aj b)); (M in(T C (Ai b; Aj e)) minDj ); (maxDi M ax(T C (Aj b; Ai e)))), and Kij = M in((minDi maxDj + M ax(T C (Ai e; Aj e))); M ax(T C (Ai b; Aj b)); (M ax(T C (Ai b; Aj e)) maxDj ); (minDi M in(T C (Aj b; Ai e)))).
ij  k  0  0  0  The rst condition in the theorem ensures that starting and ending values are included in their corresponding domains.
The second is derived from Lemma 1 as shown above, and it ensures that the constraints of the network will be satised independently from the specic amount of time taken by each activity within its duration constraint.
Note that all values (included minDi ; maxDi ) used in the calculations of the constant expressions are given by the minimal network computed in Step 1.
Hence, the theorem characterizes all the single-beginning-point free schedules in N , the minimized network.
As mentioned earlier, if the value minDi or maxDi in N are changed by the consistency algorithm, there does not exist any free schedule.
The theorem, however, is still useful since if the system of inequations has any solution, these solutions will give restricted due-time or bounded schedules, depending on whether the consistency algorithm has tightened only maximum duration bounds or some of the minimum ones.
The theorem also provides an eAcient way to nd a free schedule.
Indeed, the inequations given by Theorem 1 can be expressed in a new constraint network with n nodes, with domains of variables bound by the rst inequations and constraints given by the second ones.
We apply the consistency algorithm to this network obtaining a minimal network representing the schedule solution space.
If we take the minimal value from each domain, the resulting set of values is guaranteed to be a solution [DMP91], and, in terms of our problem, this solution provides the earliest free schedule.
Technically, \earliest" here means that this solution identies the point closest to the origin in the n-dimensional space representing all solutions.
Intuitively, it is a schedule with the earliest enactment times.
3.3  Optimizing free schedules  The schedule we have identied in the previous section is not necessarily \maximal", in the sense that we may extend some intervals in a free schedule to still retain the property of freeness.
Intuitively, the larger the intervals, the more relaxed the constraints are on activities.
An interesting problem is to nd maximal free schedules.
To do that, we only need to observe that any (high-dimensional) rectangular region in the solution space given in Theorem 1 yields a free schedule, and to nd a maximal free schedule is to nd the largest rectangular region.
It is easily seen that in some situations, there are many dierent maximal free schedules.
In certain cases, we may be interested in nding an \optimal" free schedule.
In these cases, we may rely on standard optimization algorithms to nd the \optimal" rectangular regions.
4 Conclusion In this paper, we investigated the enactment scheduling problem in the context of workow systems with autonomous agents and temporal constraints among the workow activities.
The notion of free schedule we have introduced is particularly powerful in this context, since, if such a schedule exists, the agents performing the workow activities are essentially free from the need of communicating with each other in order to successfully com7  plete the workow, satisfying the global temporal constraints.
Our scheduling algorithm should be integrated with the ones used by the workow enactment service at runtime, and it should be run each time the workow execution reaches an ANDsplit node, i.e., when parallel activities should be enacted.
We are extending this work in several directions.
One of them considers situations where a free schedule does not exist; intuitively, this is the case when duration bounds provided by agents have a wide range and/or when synchronization among activities is quite strict.
In these cases, the most reasonable schedules become restricted due-time schedules which impose a due-time to the agents in charge of the activities, by restricting the maximum duration.
The algorithm presented in the previous section must be extended to nd these schedules.
Another direction considers the derivation of schedules in the case of constraints in terms of different time granularities.
This becomes necessary when this kind of constraints are given in the specication, and the approximation introduced by a conversion into a common time unit is not acceptable by the workow application.
The results in [BWJ97, BWJ98] give some insight on the problems involved in the extension.
C. Bettini, X. Wang, and S. Jajodia.
Satisability of Quantitative Temporal Constraints with Multiple Granularities.
In Proc.
of 3rd Int.l Conf.
on  J. Eder, E. Panagos, and M. Rabinovich.
Time constraints in workow systems.
In Proc.
of 11th Int.l  [MO99]  O. Marjanovic and M.E.
Orlowska.
On Modeling and verication of Temporal constraints in Production Workows.
Knowledge And Information Systems, 1(2), 1999.
[SC93]  V. Suresh and D. Chaudhuri.
Dynamic scheduling: a survey of research.
In-  [WfMC99] Workow Management Coalition.
Terminology & Glossary.
Document Number WFMC-TC-1011.
1999. http://www.aiim.org/wfmc  (Special Issue on Cooperative Information Systems), 1997.
[BWJ97]  [EPR99]  ternational Journal of Production Economics, 32(1), 1993.
[ADEM97] G. Alonso, D. Agrawal, A. El Abbadi, and C. Mohan.
Functionalities and Limitations of Current Workow Management Systems.
In IEEE Expert  C. Bettini, X. Wang, and S. Jajodia.
A General Framework for Time Granularity and its Application to Temporal Reasoning.
Annals of Mathematics and Articial Intelligence, 22(1,2), 1998.
R. Dechter, I. Meiri, and J. Pearl.
Temporal constraint networks.
Articial Intelligence, 49, 1991.
Conf.
on Advanced Information Systems Enigneering, 1999.
References  [BWJ98]  [DMP91]  Principles and Practice of Constraint Programming, Springer-Verlag LNCS 1330, 1997.
8
A Temporal Relational Algebra Based on Multiple Time-Lines Mehmet A. Orgun Department of Computing, Macquarie University Sydney, NSW 2109, Australia  @  mehmet:orgun mq:edu:au  Abstract  tended SQL-92 to support mixed granularities with respect to a granularity lattice.
They provided two operations, scale and cast, that move times within the granularity lattice.
Euzenat [6] gives an algebraic approach to granularity in time representation, which uses granularity change operators for converting (upward and downward) qualitative time relationships from one granularity to another.
A related issue is the notion of time-varying relations defined over multiple time-lines, for instance, multiple time series [4] or financial data defined over time-lines with varying rates of sampling.
Therefore there is a need for a model of time that can differentiate between different levels of time.
Caspi and Halbwachs [1] proposed a model of events in reactive systems, in which there is a global time-line, but it is the actual events that define time.
Events are timestamped with their dates of occurrence.
Such a model of time requires the synchronization of data based on multiple time-lines.
In [8], the maintenance of historical data along multiple lines of time evolution has been extensively discussed, but little attention has been paid to representing and querying historical data based on multiple time-lines.
This paper proposes a clocked temporal algebra, called <, in order to deal with time-varying relations based on multiple time-lines.
In the underlying data model, temporal relations are a collection of ordinary relations, defined over clocks that represent multiple time-lines.
The set of possible moments in time is called the global time-line, whose interpretation depends on the application at hand.
Our model of time is similar to that of [1, 14] in that the clock of a temporal relation is the collection of time-stamps (dates of occurrence or validity) from its defining events (tuples).
In <, data values from different moments in time and from different time-lines are combined through the use of temporal operators, not by explicit references to time.
The meaning of an operation (temporal or otherwise) depends on the clocks of the relations involved in the operation as well as the relations; however, no new times are produced as a result of an operation.
The data model of < is based on a temporal logic called TLC [9] in which each predicate symbol is assigned a clocked temporal relation and tempo-  A clocked temporal relational algebra, called <, which supports temporal relations based on multiple time-lines is proposed.
Temporal relations are defined over clocks which are subsequences of an assumed global time-line.
The algebra is a consistent extension of the relational algebra, and it includes a number of temporal operators to combine data based on different time-lines.
The meaning of an operation of < depends on the clocks of the relations involved in the operation as well as the relations.
We outline a formal interpretation of expressions of <, and sketch a naAaEve expression evaluation method.
1 Introduction The relational algebra [3] operates on a model in which each relation reflects the current reality as it is best known of the enterprise being modeled.
The model cannot deal with the notion of a history, or how each relation has evolved into what it is now in a natural manner.
Having recognized the need to incorporate the time dimension into the relational model, a significant number of research efforts have been directed towards studying various aspects of this problem.
Included in that effort are temporal extensions of Coddas relational algebra [11].
For more details on temporal databases and temporal query languages, we refer the reader to the literature [16, 2].
Most of the proposed algebras are based on a uniform representation of time (with regards to granularity and scale).
An important and unresolved issue is that the relations in a temporal database are not necessarily defined on the same granularity of time or scale (e.g., days, weeks, or months).
Some events occur at irregular intervals, and it seems unnatural to force them all onto a prescribed notion of time.
Wiederhold, Jajodia and Litwin [18] recognized the problem, and provided an algebra in which data with multiple granularities are converted to a uniform model of data based on time intervals.
Dyreson and Snodgrass [5] ex1  Definition 2 (v) For any given clocks C1 and C2 , we write C1 v C2 if for all t 2 C1 , we have t 2 C2 .
ral operators are used to combine values from different moments in time.
At this stage, < does not deal with relations based on multiple granularities of time.
There are other proposals for temporal algebras based on temporal logic.
Tuzhilin and Clifford [17] proposed a temporal algebra (called TA) which offers the operators of the relational algebra plus two temporal linear recursive operators.
TA is equivalent in expressive power to a temporal calculus based on a temporal logic with operators since and until.
Gabbay and McBrien [7] considered a refinement of TA which is also based on a temporal logic with since and until.
These algebras do not deal with multiple granularity of time or multiple time-lines; their main motivation is to propose a temporal relational algebra which can be used as a basis for temporal relational completeness.
Orgun and MuEller [13, 12] proposed a temporal algebra called T RA based on a discrete temporal logic with operators first and next.
When all relations are defined over the given global time-line, < degenerates into T RA.
The rest of the paper is organized as follows.
Section 2 introduces a model of time based on clocks and the notion of a clocked database.
Section 3 discusses the operators of < and their semantics.
It is in particular shown that the meaning of a given expression of < over a given clocked database is a clocked relation defined over the clock of the expression.
Section 4 discusses temporal integrity constraints.
Section 5 concludes the paper with a brief summary.
It can be shown that the set of clocks, denoted by CK, is a complete lattice in which the global clock is the maximum element and the empty clock is the minimum element.
We now define two operations on clocks that are analogous to set intersection and union.
Let C1 ; C2 2 CK.
C1 u C2  g:l:b:fC1; C2 g C1 t C2  l:u:b:fC1; C2 g  These operations can be generalized to arbitrary sets of clocks.
Note that the g.l.b.
(greatest lower bound) of two given clocks can be obtained by taking the common moments in time from both clocks whereas the l.u.b.
(least upper bound) of two given clocks can be obtained by taking all the moments from each clock.
2.2 Clocked Databases    Let a relation scheme (or just scheme) be a finite set of attribute names, where for any attribute name A 2 , dom A is a non-empty domain of values for A.
A tuple on is any map t !
[A2 dom A , such that t A 2 dom A , for each A 2 .
Let  denote the set of all tuples on .
( )  ( )      ( ) ()    ( )  Definition 3 A relation R on scheme is any subset of  .
We let P be the set of all relations on .
()  ()    For computability reasons, we stipulate that relations are finite sets of tuples.
Clocked relations are an indexed collection of ordinary relations defined over a given clock:  2 Clocked Data Model 2.1 Clocks The set of all moments in time (i.e., the time-line) is modeled by the set of natural numbers !
f ; ; ; ; : : :g. Then clocks are defined as sequences over !
, each of which representing a different time-line.
= 0123   : : () Note that T (0) is the value (or extension) of R at time C (0), T (1) at time C (1), and so on.
For instance, if C = h1; 5; 7; 18; : : :i, then the clocked relation has defined values at moments in time C (0) = 1; C (1) = 5; C (2) = 7; C (3) = 18 and so on.
However, it is not defined at those Definition 4 A clocked relation R on scheme is a tuple hC; T i where C !
!
!
is a clock and T !
!
P .
: (2)  Definition 1 (Clocks) A clock C !
!
!
is a strictly increasing sequence of natural numbers.
(it must satisfy the condition that C < C < C < : : :).
(0)  :    (1) We use the notation h0 7!
t0 ; 1 7!
t1 ; 2 7!
t2 ; : : :i where  moments in time which are not on its clock.
We define an ordering relation on clocked relations as follows: If hC; T i and hB; S i are clocked relations on the same scheme, then we say that hC; T i v hB; S i if C v B and T i  S i for all i 2 C .
A clocked database is a collection of clocked relations.
In the following exposition, we use the notation X !
Y to denote the set of functions from set X to set Y .
i 7!
ti means that the value i is mapped to the value ti .
We simply write ht0 ; t1 ; t2 ; : : :i to denote a clock where ti is the time indexed by i.
For finite clocks, we write ht0 ; t1 ; : : : tn i.
The global (time-line) clock is the sequence h0; 1; 2; 3; : : :i.
The empty clock is the empty sequence h i.
Given a clock C , we write dom(C ) to denote the domain (index set) of C , and range(C ) to denote the range (image) of C .
The notation rank(t; C ) refers to the index of time t on clock C , i.e., C (rank(t; C )) = t. Let C be a clock.
We write t 2 C if time t occurs in C , or t 2 range(C ) (t is a moment in time on clock C , not an  ()  ()  [  ]  Definition 5 Let Rel be a countable set of relation symbols, such that for any symbol p 2 Rel , p is a scheme.
Then a clocked database dbRel for Rel is a tuple hRel; i where for all p 2 Rel,  p is a clocked relation on scheme p (i.e.,  p CK  !
!
P p .)
  index value).
We now define an ordering relation on clocks as follows.
( ):  2  [  ()  ( )]    ()  We also write dbRel p to refer to the clocked relation assigned to p 2 Rel by the clocked database db.
0 7,!
Example 1 Suppose that there is a library which only holds books.
Book acquisitions are dealt with once each week.
Once a book is in stock, it remains in stock forever.
The library also has customers (borrowers) whose names and addresses are also kept on record.
The library database has the following relation schemas: stock (CALLNO, NAME) acquisition (CALLNO, NAME, PRICE) onloan (CALLNO, CUST NAME) customer (CUST NAME, ADDRESS)  TK17 QA75  War and Peace Oliver  TK17 1 7,!
QA75 HF97  War and Peace Oliver The Hobbit :::  6 7,!
TK17 QA75 HF97  War and Peace Oliver The Hobbit  7 7,!
TK17 QA75 HF97 QA12  War and Peace Oliver The Hobbit Mobbydick  14 7,!
TK17 QA75 HF97 QA12 BC52 CH91  =  The set of relation symbols in the database is Rel fstock, acquisition, onloan, customerg with their associated relation schema; keys are underlined.
The clocked database dbRel hRel; i assigns a clocked relation to each of the given relations.
Suppose that in dbRel the relations stock, onloan and customer are all assigned the clock h ; ; ; ; : : :i and acquisition is assigned the clock h ; ; ; : : :i.
Here the first clock may represent the days of the week (monday, tuesday etc) and the second clock mondays.
Suppose that dbRel assigns the clocked relations shown partially in figure 1 to stock and in figure 2 to acquisition.
In the figures, the notation C i 7!
T i is used to show the value of the relations at moment C i , not at an index value i.  :::  =  0123 0 7 14  ()  War and Peace Oliver The Hobbit Mobbydick Germinal HTML Guide :::  Figure 1.
The stock relation  () ()  3 Clocked Relational Algebra  0 7,!
TK17 HF97  War and Peace The Hobbit  7 7,!
TK17 BC52  Mobbydick Germinal  14 7,!
CH91  An expression of < consists of relation symbols, and their compositions using point-wise extensions of the operators of the relational algebra and temporal operators.
21 7,!
fg  158.95 25.05 33.35 85.10  HTML Guide  299.99  :::  Figure 2.
The acquisition relation  3.1 Point-wise Operators An operator of < is called pointwise if the value of any expression involving that operator at any time t depends entirely on the operand values at the same time t. Note that the notion of a point-wise operation has been extensively discussed in the literature [14, 13].
In the following we write to refer to the clock component of a given operation and to the relation component of a given operation.
For any n-ary operator on relations defined in the relational algebra [3], we let ~ be an n-ary pointwise operator of <, such that  time, and the resulting clocked relation is an indexed collection of the results from each moment in time.
~ , ~\, ,~ , ~ Thus, ~ and ~F are unary operators and [ ~ and ./ are binary operators of <.
Let x  .
The aggregation operators are sum ~ x , avg ~ x, ~ x with their obvious interpretations.
~ , max count ~ x and min For example, if < C; T > is a time-varying relation, then  1  () 	()     ~ (< C; T >) = hC; hsumx (T (i)) j i 2 dom(C u C )ii  sumx  where each sumx  	(~ ) = R1 ; : : : ; Rn:s:(	(R1 )(s); : : : ; 	(Rn)(s)) (~ ) = R1; : : : ; Rn:s: u f(R1); : : : ; (Rn )g  0  (T (i)) is a single-valued relation.
(  )  Example 2 The expression sum ~ PRICE acquisition can be used to find the total cost of the books acquired in a given week.
The result depends on when the query is evaluated.
For instance, if the time of evaluation is 7, then the answer  In other words, at each moment in time, the operator of the relational algebra is applied to operands at that moment in 3  118 45  is the relation fh : ig.
If the time of evaluation is 14, then the answer is the relation fh : ig.
Example 3 Consider the query aWhich books were in the stock in moment 6?a The index of moment 6 is also 6 on the clock of stock, so we have the following expression:  299 99  An important criterion for a temporal algebra is that it should be a consistent extension of Coddas relational algebra [11].
It can be easily shown that < satisfies the criterion.
Therefore < inherits (pointwise) analogues of all properties of the relational algebra, such as distributive and associative laws, and the definitions of the other relational operators such as fi, -join, and 1, and so on.
These properties, together with some other properties involving temporal operators, can be used in query optimization.
[6] (~NAME (stock)) The temporal operator(s) first next[6] move the context to moment indexed by 6.
Then the answer is the relation first next  fh War and Peace i; h Oliver i; h The Hobbit ig at any given  main idea is to find the value of R at a time on its clock that is closest to time s.  More formally we can define the meaning of first by providing the definitions of its relation component, first and its clock component, first .
The clock of the resulting relation is the same as the clock of the input relation.  )
Example 4 Consider the query aWhich books have been acquired this week but not yet placed in stock?a Since book acquisitions are only handled on a certain day of each week, this information would not be accessible on other days of the week unless the acquisition relation is sampled using current.
(first) = R:s:	(R)(0) (first) = R:s:(R)(s)  ~NAME ((current  The operator next is the tomorrow operator as it permits looking one step in the future of its operand.
So, next R is the tuple  ) ,~ stock)  acquisition  There is no need to apply current operator to stock because it already runs on the global clock.
hC; hT (1); T (2); T (3); : : :ii:  012  When C is an infinite clock, the clock of the resulting relation is the same as the clock of the input relation:  Note that the sequence h ; ; ; : : :i of all time points is infinite only to the right and not to the left.
If, in addition, it were also infinite to the left, for example, if the sequence of valid time points were the set Z of integers h   ; , ; , ; ; ; ;   i, we could have had a unary operator prev (yesterday operator) defined as  	(next) = R:s:	(R)(s + 1) (next) = R:s:(R)(s + 1)  2 1012  We need to consider the case that, when the clock of the given relation is finite, the last moment on the clock will not have a next moment defined for it.
Let k f R g, that is, the number of moments in time on a finite clock R .
Then we have:  	(prev) = R:s:	(R)(s , 1) (prev) = R:s:(R)(s , 1)  = card ( )  ( ) 	(next) = R:s < k , 1: 	(R)(s + 1) if (R) is finite (next) = R:s < k , 1: (R)(s + 1) if (R) is finite  ( )  where R is assumed to be infinite in the negative direction.
We could also provide the definition of prev when R is finite in the negative direction; we omit the details.
Observe that prev is the complete inverse of next.
However, such an operator is not possible now, because the above functions are not defined for s over !
.
We solve the problem by introducing a binary operator fby, which uses the index 0 value of its first operand only in  ( )  From the definition, we can see that the clock of next R will have one less moment then the clock of R. We write next k for k-folded applications of next.
In case k , next k is the empty string.
=0  0  0  	(current) = R:s: if k = 0 then ; else 	(R)(k , 1) (current) = R:s:s where k = cardf(R)(i)  s j i 2 dom((R))g. The  = hC; hT (0); T (0); T (0); : : :ii: )  0  We would also like to refer to the most recent or current value of a clocked relation (e.g., most recent employees) with respect to a given time of evaluation.
This is achieved by the current operator that samples its operand onto the global time-line (clock):  An operator of < is called temporal if it allows looking into the future or past values of the operands in arriving at the current value of an expression.
< offers four temporal operators explained below.
The unary operator first results in the propagation of the value of its operand at the first moment of its clock.
Thus, for any clocked relation R hC; T i, first R is the tuple  (  0  moment at which the query is evaluated.
3.2 Temporal operators  	(  0  0  =0  [] []  4  =0  DB(E ) at time t.  	(fby) = R1; R2 :s: if s = 0 then 	(R1)(0) else 	(R2)(s + k , 1) (fby) = R1 ; R2:s: if s = 0 then (R1 )(0) else (R2 )(s + k , 1) where k = card(fx j x 2 (R2 ) & x > (R1 )(0)g).
( ( ))  ( ( ))  Now prev can be defined in terms of fby as follows: prev  R =df ;  fby  R  where ; is the empty clocked relation.
Example 6 Consider the query given in example 3.
If t  2  (DB((first next[6] (~NAME stock))), we can evaluate the expression.
The clock of the expression is the clock of stock, so t 2 (stock).
We now compute the rank of t in the clock of the expression, that is, find k = rank(t; DB(E )).
Then we proceed to the evaluation of the  Example 5 Consider the query aWhich books were in the stock yesterday?a Here is the time-dependent expression: ~NAME prev stock .
At time 0, there is no yesterday, so the answer is the empty relation; at time 1, the answer is the relation fh0 War and Peace0 i; h0 Oliver0 ig and so on.
(  ( ( ))  We first have to check if t 2 DB E using the clocks of the relation symbols that occur in E .
For this purpose, we need to consult the clock component definitions of each operator that appear in E (we omit the details).
If t 2 DB E , the expression E is guaranteed to have a defined value at time t. If not, we stop the evaluation of the expression.
If t 2 DB E , then we first transform t into appropriate moments in time for the immediate subexpressions of E , again using the clock component definitions, then obtain the values of the sub-expressions at those moments in time, and finally apply the primary operator to the values to obtain the value of E at time t.  that special case s .
Let R1 and R2 be clocked relations on the same scheme.
Then we have:  )  expression:  3.3 Interpretation of Expressions  	(DB(first next(~NAME stock)))(k) = 	(first DB(next[6](~NAME stock)))(k) = 	(DB(next[6](~NAME stock)))(0) = 	(next[6](DB(~NAME stock)))(0) = 	(DB(~NAME stock))(6) = 	(~NAME DB(stock))(6) = ~NAME (	(DB(stock))(6)) = ~NAME (	((stock))(6)) = NAME (hTK17; War and Peace i; : : :g) = fh War and Peace i; h Oliver i; h The Hobbit ig  An expression of < is interpreted over a given clocked database.
Then the meaning of an expression is a clocked relation over some clock determined by the clocks of relation symbols appearing in the expression.
=  Definition 6 Given a clocked database DB hRel; i, and an expression E over DB, then the meaning of each kind of expression of < over DB is defined as follows.
0   DB(r) = (r) for all r 2 Rel.
 DB(~ (E1 ; : : : ; En )) = ~ (DB(E1 ) : : : DB(En )) for ~ of < with arity n. any operator   0  0  0  0  0  0  0  Again the resulting value will be the same whenever the query is evaluated.
It can be shown that all operations of the algebra are closed, that is, the meaning of a legal expression over a given clocked database is a clocked relation.
The closure property is one of the important criteria that temporal algebras should satisfy [11].
4 Integrity Constraints Recall that < is based on the temporal logic TLC [9].
TLC would also allow us to express temporal integrity constraints that the data in a clocked database must satisfy (see [15] for a more extensive discussion on temporal relationships and constraints).
A clocked database provides the basis for an interpretation for formulas in TLC, because predicate symbols in a given formula would correspond to relation symbols in a clocked database.
The meaning of a predicate symbol is just like a clocked relation in a clocked database; we omit the details.
Suppose that TLC is also extended with temporal modalities 2 (from now on) and 3 (now or sometime in the future).
These modalities would allow us to express temporal relationships and temporal constraints succinctly.
For instance, an integrity constraint that says aall acquired books  = ( )  Theorem 1 Given a clocked database DB hRel; i, and a legal expression E over DB, then DB E is a clocked relation.
Ignoring the storage structures for storing clocked relations, expressions of < can be evaluated using the naive evaluation method discussed below.
Since clocked relations are inherently infinite when defined over infinite clocks, we restrict the evaluation method to particular moments in time or event-based intervals.
Given a clocked database DB hRel; i, an expression E over DB, and a time t, we want to find out the value of  =  5  must be available in stock soon after they are acquireda could be expressed by the following formula:  [2] J. Chomicki.
Temporal query languages: A survey.
In Proc.
of The First International Conference on Temporal Logic, volume 827 of LNAI, pp.506a534, 1994.
Springer-Verlag.
[3] E. F. Codd.
A relational model of data for large shared data banks.
Communications of the Association for Computing Machinery, 13(6):377a387, 1970.
[4] W. Dreyer, A. K. Dittrich, and D. Schmidt.
Research perspectives for time series management systems.
SIGMOD Record, 23(1):10a15, 1994.
[5] C. E. Dyreson and R. T. Snodgrass.
Temporal granularity.
In R. T. Snodgrass, editor, The TSQL2 Temporal Query Language, pages 347a383.
Kluwer Academic Press, 1995.
[6] J. Euzenat.
An algebraic approach to granularity in time representation.
In Proc.
of TIMEa95: 2nd International Workshop on Temporal Representation and Reasoning, Melbourne Beach, Florida, USA, April 1995.
[7] D. Gabbay and P. McBrien.
Temporal logic & historical databases.
In Proc.
of the 17th Very Large Data Bases Conference, pages 423a430.
Morgan Kauffman, 1991.
[8] G. M. Landau, J. P. Schmidt, and V. J. Tsotras.
Historical queries along multiple lines of time evolution.
VLDB Journal, 4:703a726, 1995.
[9] C. Liu and M. A. Orgun.
Dealing with multiple granularity of time in temporal logic programming.
Journal of Symbolic Computation, 22(5&6):699a720, 1996.
[10] C. Liu and M. A. Orgun.
Embedding a timing system into TLC.
In Proc.
of TIMEa98: 5th International Workshop on Temporal Representation and Reasoning, pages 105a112.
IEEE Computer Society Press, 1998.
[11] L. E. McKenzie Jr. and R. Snodgrass.
Evaluation of relational algebras incorporating the time dimension in databases.
ACM Computing Surveys, 23(4):501a543, 1991.
[12] M. A. Orgun.
Incorporating an implicit time dimension into the relational model and algebra.
RAIRO Theoretical Informatics and Applications, 30(3):231a260, 1996.
[13] M. A. Orgun and H. A. MuEller.
A temporal algebra based on an abstract model.
In Proc.
of the 4th Australian Database Conference, pages 301a316.
World Scientific, 1993.
[14] J. Plaice.
Nested clocks: The LUSTRE syncronous dataflow language.
In Proc.
of the 1989 International Symposium on Lucid and Intensional Programming, pages 1a17, Arizona State University, Tempe, U.S.A., 1989.
[15] E. Rose and A. Segev.
Toodm - a temporal object-oriented data model with temporal constraints.
In Proc.
of the 10th International Conference on the Entity Relationship Approach, oct 1991.
[16] A. U. Tansel et al., editors.
Temporal Databases: Theory, Design, and Implementation.
Benjamin/Cummings Publishing Company, Redwood City, CA, 1993.
[17] A. Tuzhilin and J. Clifford.
A temporal relational algebra as a basis for temporal relational completeness.
In Proc.
of the 16th International Conference on Very Large Data Bases, pages 13a23, August 13a16 1990.
Morgan Kaufmann.
[18] G. Wiederhold, S. Jajodia, and W. Litwin.
Dealing with granularity of time in temporal databases.
In Proc.
of the Third International Conference CAiSEa91, pages 124a140, May 13a15 1991.
Springer-Verlag.
2(8C; N; P )(acquisition(C; N; P ) !
3stock(C; N )) And the constraint that says aonce a book in in stock, it remains in stock forevera could be expressed by the formula:  2(8C; N )(stock(C; N ) !
2stock(C; N )) A clocked database satisfies given integrity constraints if the formulas representing the constraints are true under the interpretation that corresponds to the given clocked database.
For the library database, it is easy to see that integrity constraints given above are both valid.
However, as the database is being updated, there may be a period during which the integrity constraints are not true, for instance, when a book is just acquired, but not immediately processed.
In this case, the constraints may be flagged for future processing.
5 Concluding Remarks We have outlined a temporal algebra < that supports relations based on different time-lines such as multiple time series data [4].
Temporal operators are used to navigate through multiple time-lines, and change the clocks of resulting relations.
Temporal operators behave in a uniform manner over their operands, keeping their intuitive meanings, but the end result varies with the clocks of the operands involved.
All of this is transparent to the end-user.
Future work includes some extensions of < to deal with clock alignments and multiple granularities.
Sometimes various relations may be based on different clocks with the same periodicity, for instance, clocks h ; ; ; ; : : :i and h ; ; ; ; : : :i.
It may be desirable to align such relations without using the sampling operator.
Liu and Orgun [10] suggested an embedding of timing systems with multiple granularities into TLC so that it can be used to represent and reason about relations based on multiple levels of granularity.
We may be able to use their approach to embed a timing system into < so that it can be used to manipulate clocked relations with multiple granularities.
1 8 15 22  0 7 14 21  Acknowledgements This work has been supported in part by The Australian Research Council (ARC) and The Department of Education, Training and Youth Affairs (DETYA).
Thanks are also due to L. Flax and C. Swe for many useful discussions.
References [1] P. Caspi and N. Halbwachs.
A functional model for describing and reasoning about time behavious of computing systems.
Acta Informatica, 22:595a627, 1986.
6
Index Based Processing of Semi-Restrictive Temporal Joins Donghui Zhang, Vassilis J. Tsotras Computer Science Department University of California Riverside, CA 92521  donghui, tsotras  @cs.ucr.edu  Abstract Temporal joins are important but very costly operations.
While a temporal join can involve the whole time (and/or key) domain, we consider the more general case where the join is defined by some time-key rectangle from the whole space (i.e., when the user is interested in joining portions of the ausually largea temporal data).
In the most restrictive join, objects (within this rectangle) are joined together based on key equality and interval intersection.
This paper concentrates on semi-restrictive joins, i.e., when either the key equality (equi-join) or the interval intersection (timejoin) predicates are used.
Given the large relations created by the ever increasing time dimension, we assume that each temporal relation is indexed and examine efficient ways to process semi-restrictive temporal joins.
Utilizing an index is helpful since it directs the join towards the objects that are within the time-key rectangle.
A straightforward approach is to perform an unsynchronized join.
An index selection query on each relation identifies all objects within the time-key rectangle which are then joined.
Although simple, this approach ignores the data distribution in the other relation.
Instead, in a synchronized join, both indices are concurrently traversed as the join is computed.
Synchronized semi-restrictive join algorithms can be performed utilizing traditional indices like B+-trees or R-trees.
The drawback of this approach is that traditional indices do not achieve good temporal data clustering.
Better clustering is achieved by temporal indices through record copying.
Nevertheless, record copies can greatly affect the correctness and effectiveness of join performance.
In this paper we introduce correct and efficient algorithms for performing semi-restrictive temporal joins using temporal indices.
An extensive experimental comparison shows that the newly proposed algorithms have the best performance.
While the paper concentrates on using the Multiversion B+-tree, our algorithms apply equally to other efficient tree-based temporal indices.
  This work was partially supported by NSF (IIS-9907477, EIA9983445) and the Department of Defense.
1 Introduction A temporal record has a key, some attributes and a time interval during which the record is valid.
Temporal join predicates may involve the key and/or time spaces [9].
Examples include the T-Join (join two records if their intervals intersect), the E-Join (join two records in their keys are equal), and the TE-Join (keys are equal and intervals intersect).
Due to large volume of temporal data, a user may be interested in a portion rather than the whole time-key data space.
This portion is typically retrievable via a rangeinterval selection query: afind all records with keys in range fi and whose intervals intersect interval  a.
By intergrating the range-interval selection condition into the join predicate, we get the general T-Join, E-Join and TE-Join (in short, GT-, GE- and GTE-Joins).
Note that the plain T-Join, E-Join and TE-Join are then special cases when the query range fi is the whole key space and the query interval  is the whole time space.
Temporal join research has focused on non-indexed algorithms for plain joins [9, 15, 18, 23, 19, 20, 26] and typically involves a sequential scan of both relations.
This is prohibitive for general temporal joins.
Instead, an indexing scheme is beneficial since it can quickly direct the join towards the objects of interest.
With an index present in each joined relation, a straightforward approach is to perform unsynchronized joins.
Each index is first used to identify the objects of the relation that is within the query rectangle.
The retrieved objects are then joined (using existing algorithms for plain temporal joins).
A drawback of this approach is that each selection is performed independently, ignoring the data selectivity of the other relation.
Instead, in a synchronized join, the selection and the join phases are integrated.
Both indices are traversed concurrently, taking advantage of the join selectivities and thus leading to more robust performance.
For example, a synchronized traversal algorithm can quickly identify that no pair of records should be reported (if for example no record from the first relation qualifies) thus finishing the join fast.
When considering the index used on each temporal re-  lation, various possibilities exist.
One approach is to use traditional indices like B+-trees and R-trees [5].
Such indices are widely implemented and lead to straightforward synchronized joins.
However, such joins suffer from the ineffectiveness of the B+-tree and (to a lesser extent) the R-tree on clustering temporal data.
Temporal data is inherently multidimensional having typically long intervals on the time dimension.
Even the R-tree (and its most efficient variation, the R*-tree [5]) are known to be problematic in clustering long intervals [12, 24].
Better clustering is achieved by temporal access methods that create many copies for records with long time intervals [17, 3].
This leads to fast processing of range-interval selection queries, but record copies can greatly affect join processing (for example, duplicate join results).
In [27], we have proposed efficient synchronized join algorithms that utilize temporal indices for the GTE-Join (i.e., when both the key-equality and interval-intersection conditions are applied).
As a follow-up to that work, in this paper we address the semi-restrictive GT-Join and GE-Join.
That is, we look at temporal joins when either the key-equality (GE-join) or the interval-intersection (GT-join) predicates are used.
Due to the inherent difference among the join conditions of the three problems, the GTE-Join algorithms proposed in [27], either do not apply or need to be modified so as to solve the GT- and GE-join problems.
In the rest of the paper we use the Multiversion B+-tree (MVBT) [3] as a temporal index.
However, the proposed algorithms can be applied to other efficient temporal access methods [17, 25].
The main contributions of this paper are summarized below:  We propose synchronized, temporal index based join algorithms for the GT-Join and GE-Join.
Top-down and sideways traversal algorithms are presented.
 We examine other approaches, including the unsynchronized approach, the B+-tree and R-tree based synchronized approaches, and an approach based on spatial partitioning (using the TP-Index [21]).
Results from an extensive experimental evaluation are also presented.
Our experimental study shows that the sideways, linkbased, synchronized traversal is the most robust among the examined methods.
Depending on the join characteristics, other methods can be competitive and should be considered by a temporal database optimizer.
The rest of this paper is organized as follows.
Section 2 formally defines the join problems that we address.
Section 3 reviews related work.
Section 4 proposes algorithms for the GT-Join, while section 5 addresses the GE-Join.
Section 6 shows the experimental results over all compared approaches while section 7 concludes the paper.
2 Problem Definition In the following, a key range fi is specified by its fi 	  and fi   keys while a time interval  is described by its   fi and    time instants.
A range and an interval create a fi  	 in the 2-dimensional key-time space.
A record contains a key, a time interval and various attributes that may change over time.
We assume the First Temporal Normal Form [22] which implies that no two records exist in a given temporal relation that have equal keys and intersecting intervals.
A record with time interval  is called  	 "!
 for all time instants in  .
Moreover, we assume the transaction-time model [11] which implies that record updates arrive in increasing time order.
When a data record is inserted in a relation at time  , the end time of its interval is yet unknown and is thus initiated to   (a variable representing the ever increasing current time).
Record deletions are logical, i.e., records are marked as deleted and are retained in the relation.
Hence, if a record is deleted, its end time is changed from   to its deletion time.
An attribute update to record with key # at time  is treated by a (logical) deletion of the old record at  and the subsequent insertion of a new record awith key # , but updated attributesa and an interval starting at  .
The temporal joins examined in this paper are defined as: 1.
General T-Join (GT-Join): given temporal relations $ , % , key ranges fi& , fi' , and time interval  , find all (*),+.-0/ % where )213$ and -41 such that: (1) )  #  -51 fi& and  6      fi  	 )  !
intersects  ; (2) -  #  -41 fi ' and )   6fi !
 	 intersects  ; and (3) )   6fi !
 	 intersects -   6fi !
	 .
2.
General E-Join (GE-Join): given temporal relations % $ , , key range fi , and time intervals  & ,  ' , find % all ()7+.-0/ where )819$ and -:1 such that: (1) )  #  -31 fi and )   6fi !
 	 intersects  & ; (2) -  #  -31 fi and )   6fi !
 	 intersects  ' ; and (3) )  #  -<;=-  #  - .
Note that in general, the key ranges (respectively, time intervals) restricting each of the two relations in the GTJoin (respectively, GE-Join) can be different.
An example GT-Join query is: afind employees whose last names start with aBa and who co-worked during 1995 with the employees whose last names start with aSa a.
An example GEJoin query is: afind the 1998 IBM employees who were UC Riverside students in 1995 a.
3 Related Work In our previous work [27], we proposed synchronized join algorithms based on temporal indices for the General % TE-Join: given temporal relations $ , , key range% fi , and time interval  , find all ()7+>-?/ where )21@$ and -41 such that: (1) )  #  -@1 fi and )   6fi !
 	 intersects  ; (2) -  #  -51  fi and )   6fi !
 	 intersects  ; and (3) )  #  -A;B-  #  - and )   6fi !
	 intersects -   6fi !
 	 .
Except for [27], research on temporal joins has focused on non-indexed algorithms.
[18] assumed that the smaller relation fits in memory and proposed seven nested-loop (plain) T-Join algorithms.
[9] provided sort-merge (plain) T-Join and TE-Join algorithms when one or both relations are sorted.
[15] assumed that the relations are sorted on the start time of the record intervals and discussed how to merge them in a stream-processing manner.
Each iteration of the algorithm reads in buffer one record whose start time is the smallest among non-read records.
This record is joined with the in-buffer records and the in-buffer records which will not join with further records are removed.
[19] also assumed the relations are sorted and discussed how to merge them.
Besides the nested-loop and sort-merge temporal join algorithms, partition-based algorithms have also been proposed.
In static partitioning [26], a record is copied to all partitions that intersect its interval.
A partition of records in one relation needs to join with one partition of the other relation.
In dynamic partitioning [23], a record is assigned only to one partition (the last partition that intersects the recordas interval).
After a pair of partitions is joined, the records that may possibly join with some records in the unprocessed partitions are retained in the join buffer.
[20] used this dynamic partitioning algorithm while utilizing the Time Index [7] to determine the exact partitioning intervals so that each partition fits in memory.
[16] proposed a (plain) T-Join algorithm based on spatial partitioning.
Here a recordas interval  is mapped to a point (  fi ,    -  fi ) in a two-dimensional space.
These points are then indexed by an R-tree like method (the TP-Index [21]) which partitions the space.
However, a partition in one relation may be joined with many partitions in the other relation.
When an R-tree is used as an index, a temporal join can be considered as a special case of a spatial join.
[4] presents a depth-first while [10] proposes a breath-first synchronized R-tree join algorithm.
[8] proposed join algorithms based on Generalization Trees.
[2] developed a plane-sweeping algorithm that unifies the index-based and non-index based approaches.
The plane-sweeping phase of the algorithm needs to read records from the joining relations in nondecreasing order regarding one dimension (and then the stream processing of [15] can be applied).
For the nonindexed environment, an initial sorting is sufficient.
When the R-tree index exists, it is exploited to directly extract the data in sorted order according to the plane-sweep direction.
This algorithm is an extension to the scalable sweepingbased spatial join (SSSJ) [1] to the case of indexed inputs.
4 Synchronized GT-Join Algorithms 4.1 GT-Join based on Traditional Indices A one-dimensional index like the B+-tree, clusters data primarily on a single attribute.
Consider first a B+-tree that clusters on the interval start time.
Because of the transaction-time environment, records are inserted in increasing time order; thus such an index can easily take advantage of sequential I/O.
However, this scheme will be clearly inefficient for the GT-Join.
Given a query interval  , a record fi may intersect  as long as fi fiDC    .
To answer a GT-Join query we have to scan the B+-tree for all such records, although most of them do not intersect  (since they are completely to the left of  ).
Clustering primarily on record  times is similarly inefficient.
Alternatively, we can utilize a B+-tree which clusters by keys.
Nevertheless, synchronized join algorithm is not possible in this case.
Besides the range-interval selection query, the join condition of the GT-Join only specifies that the joining records must have intersecting intervals.
Since the B+-trees do not cluster records on time attributes, the range-interval selection query will give un-sorted output (in the sense that the selection result is not clustered by time attribute, either).
This implies finding the records first and then join them in a separate phase (as in an unsynchronized approach).
With a multidimensional index like the R-tree, records are clustered by both key and time.
Then a temporal join can be addressed as a special case of a spatial join.
The depth-first [4] and breath-first [10] R-tree join algorithms follow the Synchronized Tree Traversal (STT) scheme.
Initially the pair of root nodes is pushed into the stack.
To process a pair of nodes that is popped from the stack, every record in the first node is joined with every record in the second node (if they satisfy a given condition).
Eventually, at the leaf level, records in leaf nodes are joined.
The R-tree join algorithms can be used to solve the GT-Join, with the following modifications:  The original algorithms [4, 10] join two complete Rtrees.
In our case, we are interested in records within the query rectangle defined by range fi and interval  .
Thus whenever a page is examined, only records that intersect the query rectangle are considered.
 We modify the condition for two tree nodes to join with each other.
The original R-tree join algorithms join two nodes as long as they intersect, with the goal to eventually find pairs of leaf records which intersect with each other.
In our GT-Join case, we want to find pairs of records whose intervals intersect; thus at higher levels of the tree, we join two nodes as long  as they intersect in the time dimension, regardless of whether they intersect in the key dimension.
The disadvantage of the R*-tree based approaches emanates from their difficulty in storing the typically long time-intervals.
Such intervals tend to increase the size of their bounding rectangles which introduces overlapping and in turn affects the join performance.
4.2 GT-Join using Temporal Indices Temporal indices like the MVBT [3] achieve better clustering by introducing record copies: long intervals are broken into smaller ones that are stored in multiple places.
Thus special care is needed for the range-interval selection query algorithm to avoid duplicates, i.e.
reporting multiple copies of the same record.
[6] proposed two rangeinterval selection query algorithms, a top-down and a sideways (link-based) one.
We also present a variation of the link-based approach, the plane-sweep algorithm.
4.2.1 Top-Down GT-Join  selection algorithm first finds the leaf pages which intersect the right border of the query rectangle, and then follows the predecessor links to find the other leaf pages that intersect the query rectangle.
Once these leaf pages are identified, the task to select records from them is trivial.
We propose a new link-based algorithm for the GT-Join.
This is a simpler solution than the link-based approach used for the GTE-Join [27].
The idea of the algorithm is to perform the link-based selection algorithm synchronously on the two MVBTs.
First, it finds pairs of data pages: (1) whose rectangles intersect the right border of the query rectangle; and (2) whose intervals intersect each other.
Then it follows predecessor records synchronously to find other pairs of data pages intersecting with the query rectangle and whose intervals intersect.
To follow predecessor records synchronously, the following procedures are utilized: to choose a pair of data pages to join, use a priority queue to choose the one whose  time is the latest.
While a pair of data pages are examined, if their fi times are different, the page with the smaller fi time is joined with the predecessors of the other page; if their fi times are equal, the predecessors of one page are joined with the predecessors of the other.
The idea of the top-down GT-Join algorithm using MVBT is to perform the depth-first range-interval selection query on both MVBTs synchronously.
Note that an MVBT can be viewed as a forest of ordered trees.
The root node of each tree corresponds to a time interval, where for any root node E except the last one, E  ; EGF,& fi .
For more details of the MVBT, we refer to [3].
To join two relations indexed by MVBTs, we join every pair of root nodes, one from each MVBT, whose time intervals intersect each other and intersect the query interval  .
To join each such pairs ( ,& + ' / , we perform STT of the two trees rooted by ,& and  ' .
Here the condition for two pages to join is that their intervals should intersect.
Similar to the above depth-first join algorithm, we can have a breadth-first join algorithm, which finishes one level of the MVBT before going to the next level.
These two algorithms are similar to the top-down algorithms for the GTE-Join [27] (with the exception of the different join condition) and are omitted.
Furthermore, both the balancing condition optimization and the virtual height optimization proposed there can be applied.
The plane-sweep join algorithm is similar to the link-based join algorithm in that it also starts with finding the data pages intersecting with the right border of the query rectangle, and it also proceeds by following predecessor links to find the other data pages.
The difference is that instead of keeping pairs of data pages in a single priority queue, we now keep a priority queue of individual pages.
Furthermore, we maintain a buffer of records which may join with records to be identified later.
At each step, we locate the leaf page which has the largest  time and select records from the page to buffer, and we insert the predecessors of the leaf page into the priority queue.
Once a record is put into the buffer, we join it with the in-buffer records of the other relation.
Also, as leaf pages are taken out from the queue, we locate garbage records (whose fi is larger than the largest  time of any page in the queue) which are removed from the buffer.
4.2.2 Link-based GT-Join  5 Synchronized GE-Join Algorithms  The link-based range-interval selection algorithm of [6] utilizes the concept of predecessors.
Each leaf page of the MVBT corresponds to a rectangle in the two-dimensional key-time space; if for two pages H and I , their key ranges overlap and H is right before I in the space (i.e.
the  of H as time interval is equal to the Jfi of I as interval), then H is a predecessor of I .
The link-based range-interval  4.2.3 Plane Sweep GT-Join  5.1 GE-Join based on Traditional Indices Using the B+-tree to cluster by time attributes is not efficient for the same reason as discussed in section 4.1.
However, unlike the GT-Join case, it is possible to perform synchronized GE-Join based on B+-tree indices which cluster  by keys.
Since tree leaf pages are linked and records in them are ordered, the join algorithm starts with the leaf page in each tree that contains fi 	  , and proceeds by performing a sort-merge join until leaf pages with keys larger than fi    are met.
The major shortcoming of this simple join algorithm is that it may encounter many records that do not participate in the join since their intervals do not intersect the query interval.
If the relations are indexed by R-trees, we can use the modified R-tree join algorithms.
The modification from the original version [4, 10] is similar to our discussion in the GT-Join case (section 4.1).
The difference is that in order for two tree nodes to join, instead of requiring them to intersect in the time dimension, we now require them to intersect in the key dimension.
5.2 GE-Join using Temporal Indices 5.2.1 Top-Down GE-Join Again, we can have depth-first and breadth-first join algorithms based on the MVBT.
The algorithms are similar to the top-down approach of the GT-Join and are omitted.
The difference is that to join two pages, instead of using the condition that they should intersect in the time dimension, we now require that they should intersect in the key dimension.
5.2.2 Link-based GE-Join The link-based GE-Join algorithm is completely different from the link-based GT-Join algorithm.
The reason is that now two leaf pages join even if their intervals do not intersect (as long as their key ranges overlap).
Consider the leaf pages in a MVBT that intersect the query rectangle.
They form a set of linked lists where the anexta pointer between each pair of nodes is a predecessor link between two leaf pages; and the aheada of each linked list is a leaf page which intersects the right border of the query rectangle.
Consider the joining of two such linked lists, one from each joining MVBT.
Every node in one linked list should be joined exactly once with every node in the other.
We proceed with discussing an algorithm to join two linked lists and then relate the link-based GE-Join problem with it.
The idea of joining linked lists is to join every node in linked list H with the whole linked list I .
More formally, we give the algorithm as follows.
Algorithm LinkedListJoin( Linked-list H , I ) 1.
Push( K   # , [ H L  , I L  ] ); 2. while( not IsEmpty( K   # ) ) do 3.
[  , M ] = Pop( K   # ); 4.
Join  with M ; 5.
Push( K   # , [  , M   )  ] ) if ( M   ) O; N NULL );   ; N NULL and Push( K  # , [ P  )  , M ] ) if ( P  ) 4 M ;RI  S ); Q 7. endwhile  6.
Consider each leaf page from the first MVBT which intersects the right border of the query rectangle.
By following the predecessor links from it until the left border of the query rectangle is reached, we get a sequence of nodes.
We consider such a sequence as a linked list.
Similarly, we can get a linked list from the second MVBT.
We thus can join the two lists using the above algorithm.
If we consider every possible linked list, we can find every pair of leaf pages whose key ranges overlap.
The issue which remains is how to avoid duplicates.
This important issue arises due to the fact that a leaf page in the MVBT may have more than predecessors.
Correspondingly, different linked-lists may share nodes.
We need to make sure that the same pair of nodes are not joined multiple times.
key  key  pd1  page 1 pd1  pd2 page 2  page 1  page 0  pd2  time  (a) The PD condition  page 2  time  (b) Case to release the PD condition  Figure 1.
The PD condition and when it should be released.
To avoid duplicates, we borrow from [6] the concept of reference point and predecessor condition (PD).
The reference point is defined as the lower left intersection of the query rectangle and the predecessor page rectangle.
Then the predecessor condition (PD) is defined as: agiven record  , a predecessor record T  in page(  ) and a query rectangle fi , the predecessor record is visited only if its reference point with respect to fiU falls in the key range of  a.
For example, in figure 1a, a predecessor record T  & in page 1 and a predecessor record T  ' in page 2 point to the same page.
Since the specified intersection point (the black dot) lies in the key range of page 2 and not page 1, only T ' is followed.
However, to apply the PD condition without further consideration will result in loss of join results.
Figure 1b reveals the scenario.
Suppose pages 1 and 2 belong to the first MVBT and page 0 belongs to the second MVBT.
Assume the query rectangle is the whole key-time space.
Obviously, the GE-Join algorithm should join page 0 with page 1 and with page 1as predecessor page (their key ranges overlap).
However, the PD condition specifies that page 1as predecessor page will only be visited while examining page 2.
Since  5.2.3 Pipelined Sort-Merge GE-Join Unlike the GT-Join case, if we perform the plane sweep algorithm for the GE-Join, it would be very inefficient.
The reason is that the plane sweep scheme sweeps on the time dimension (from right to left), while the GE-Join condition does not require the joining records to have intersecting intervals.
For the previous GT-Join case, due to the fact that joining records must have intersecting intervals, we can remove records from memory buffer as long as the sweep line is moved to the left of their fi time.
In our GE-Join case, however, records join as long as their keys are equal, irrelavent to whether their time intervals intersect or not.
Thus we have to maintain all records in buffer.
Instead, we consider the following sort-merge with pipelining technique.
It can be thought of as a semisynchronized approach.
The idea is as follows.
the GE-Join.
The reasons are discussed in sections 4 and 5.
We also implemented a spatially partitioned GT-Join using the approach in [16].
For each implemented algorithm, we mention the section where the algorithm is discussed.
6.2 Experimental Setup The algorithms were implemented in C and C++ using GNU compilers.
The programs were run on a Sun Enterprise 250 Server machine with two UltraSPARC-II processors using Solaris 2.8.
To compare the performance of the various algorithms we used the estimated running time.
This estimate is commonly obtained by multiplying the number of I/Oas by the average disk block read access time, and then adding the measured CPU time.
We measured the CPU cost by adding the amount of time spent in VWfi and  - JX mode as returned by the  YfiVW   system call.
A random access was counted as 5ms on average.
7000 6000  Total Time (#sec)  page 2 does not join with page 0 at all (their key ranges do not overlap), applying the PD condition in this case results in lose of joining page 0 with page 1as predecessor.
In general, when joining leaf H with leaf I from the other MVBT, if H as low key is no less than I as low key, we should join H with I as predecessor page regardless of whether the PD condition is true (i.e.
the PD condition is released in this case).
1.
Perform range-interval selection queries on the first relation, sort the results, and then fill the memory buffer with the sorted selection result.
If the selection result is larger than the memory buffer, part of it should be kept on disk.
It is trivial to calculate the key range of the on-disk records.
3.
After the selection query on the second relation is performed, join the on-disk records from relation one with the retained records from relation two.
4000 IO  3000  CPU  2000 1000  0 mvbt mvbt mvbt mvbt r*_df r*_bf _df _bf _link _ps  spj  (a) R/I ratio = 10 1500  1250  Total Time (#sec)  2.
As the range-interval selection query is performed on the second relation, for each result that is generated, join it with the in-memory records from the first relation.
Then, the record can simply be discarded if its key does not belong to the key range of the on-disk records from the first relation.
Otherwise, retain it.
5000  1000  750  IO  CPU  500  250  0 mvbt mvbt mvbt mvbt r*_df r*_bf _df _bf _link _ps  spj  (b) R/I ratio = 0.1  6 Performance Analysis 6.1 Implemented Algorithms Table 1 lists the algorithms we implemented.
For each algorithm, the index structure used can be seen from the notation.
The plane sweep algorithm based on MVBT is only implemented for the GT-Join, while the pipelined sortmerge join and the B+-tree join are only implemented for  Figure 2.
Performance of GT-Join, varying the R/I ratio.
For every index, an LRU buffering was used.
For the R Z -tree joins, besides using a LRU buffer, for each tree we also buffered all the nodes along the path from the root to the most recently accessed node.
For the breadth-first joins,  Notation:  [ \J^"_ `ba ] [O\J^"_ ^a []\J^"_ cedgf h []\J^"_ ikj []\J^"_ j.
[ ^l m `ga m  ^a jin  GT-Join:  GE-Join:  Meaning:  4.2.1  5.2.1  Depth-first traversal  4.2.1  5.2.1  Breadth-first traversal  4.2.2  5.2.2  Link-based traversal  4.2.3  Plane-sweep traversal 5.2.3  Pipelined sort-merge join  5.1  Synchronized, find the start point using index and sort-merge on leaf pages  4.1  5.1  Depth-first traversal using R -tree  4.1  5.1  Breadth-first traversal using R -tree  3      Spatially partitioned join [16]  Table 1.
Implemented Algorithms.
other relation.
Also, the performance of SPJ deteriorates as the R/I ratio reduces.
This is because the query rectangle covers less key space while the SPJ joins the whole key space.
900 800  Total Time (#sec)  700 600 500 IO  400  CPU  300 200 100  0 mvbt mvbt mvbt mvbt _df _bf _link _sm  b+  r*_df r*_bf  (a) R/I ratio = 10 2500 2250 2000  Total Time (#sec)  we used 15% of the memory buffer for storing and sorting the intermediate join results.
We created and joined two datasets: a uni-S and a uniSM dataset.
They were were first created using the TimeIT software [13] and then transformed to add record keys.
Each dataset has 10 million records.
Each actual record is 128 bytes long.
The #  - , fi and  attributes are each 4 bytes long.
A dataset contains 50,000 unique keys where each key has on average 200 intervals.
The uni-S dataset contains only short intervals (length about 1/10000 of the time space), while the uni-SM dataset contains 25% medium (length about 1/1000 of the time space) intervals and 75% short intervals.
Here the time space is from 1 to 20 million.
Each experiment reports the average response over 10 randomly generated query rectangles with fixed rectangle shape and size.
The shape of a query rectangle is described by the R/I ratio, where o is the length of the query key range divided by the length of the key space and p is the length of the query time interval divided by the length of the time space.
The query rectangle size (QRS) is described by the percentage of the query area in the whole key-time space.
Unless otherwise stated, the default parameters we used are: buffer size = 10MB, page size = 8KB, QRS = 0.1% and R/I ratio = 1.
1750 1500 1250  IO  1000  CPU  750 500 250  6.3 Join Performance Figure 2 compares the GT-Join performance.
For both cases, the Link-based and the plane sweep MVBT algorithms have very similar performance (much faster than the competitors).
The breadth-first and the depth-first MVBT based approaches do not perform well since they join two many index pages.
The R-tree based join algorithms are much slower.
The difference in clustering temporal data is apparent.
The SPJ performs the worse since a partition in one relation needs to be joined with many partitions in the  0 mvbt mvbt mvbt mvbt _df _bf _link _sm  b+  r*_df r*_bf  (b) R/I ratio = 0.1  Figure 3.
Performance of GE-Join, varying the R/I ratio.
For the GE-Join query (figure 3), the link-based MVBT algorithm performs the best.
(Recall that the plane-sweep MVBT algorithm is very inefficient for GE-Join a see sec-  tion 5.2.3).
Interestingly, we observe that when the R/I ratio is small, the pipelined sort-merge MVBT algorithm becomes a competitor.
The reason is that it does not need to read a page from the indices more than once, while all the synchronized algorithms do.
When the R/I ratio is small, the query rectangle intersects many pages from each relation with similar key ranges.
Since the time attribute is not involved in the join predicate, most of these pages will join.
Thus the problem for the synchronized algorithms worsens as the R/I ratio gets smaller.
The performance of the other algorithms is drastically affected by the R/I ratio.
The B+tree algorithm performs relatively better as the R/I ratio reduces.
But it is still not as good as the link-based one.
7 Conclusions We studied the problem of efficiently processing semirestrictive temporal joins (GT- and GE-join) when indices are available.
We argued that traditional indexing schemes, like a B+-tree or an R*-tree do not lead to efficient join processing, due to their ineffectiveness in clustering temporal data.
Instead we used a temporal index and we proposed various synchronized join algorithms.
While we have concentrated on using the MVBT, our findings apply to other temporal indices as well.
Our experimental results verified that temporal index based joins are more efficient than the B+-tree and R*-tree based joins.
In particular, for both the GT-Join and GE-Join, the newly proposed link-based join algorithm has the most robust performance.
It showed multi-fold improvement over the B+-tree/R*-tree joins.
References [1] L. Arge, O. Procopiuc, S. Ramaswamy, T. Suel and J. Vitter, aScalable Sweeping-Based Spatial Joina, Proc.
of VLDB, 1998.
[2] L. Arge, O. Procopiuc, S. Ramaswamy, T. Suel, J. Vahrenhold and J. Vitter, aA Unified Approach For Indexed and Non-Indexed Spatial Joinsa, Proc.
of EDBT, 2000.
[3] B. Becker, S. Gschwind, T. Ohler, B. Seeger and P. Widmayer, aAn Asymptotically Optimal Multiversion B-Treea, VLDB Journal 5(4), 1996.
[4] T. Brinkhoff, H. Kriegel and B. Seeger, aEfficient Processing of Spatial Joins using R-treesa, Proc.
of SIGMOD, 1993.
[5] N. Beckmann, H. Kriegel, R. Schneider and B. Seeger, aThe R* tree: An Efficient and Robust Access Method for Points and Rectanglesa, Proc.
of SIGMOD, 1990.
[6] J. van den Bercken and B. Seeger, aQuery Processing Techniques for Multiversion Access Methodsa, Proc.
of VLDB, 1996.
[7] R. Elmasri, G. Wuu and Y. Kim, aThe Time Index: An Access Structure for Temporal Dataa, Proc.
of VLDB, 1990.
[8] O. GuEnther, aEfficient Computation of Spatial Joinsa, Proc.
of ICDE, 1993.
[9] H. Gunadhi and A. Segev, aQuery Processing Algorithms for Temporal Intersection Joinsa, Proc.
of ICDE, 1991.
[10] Y. Huang, N. Jing and E. Rundensteiner, aSpatial Joins Using R-trees: Breadth-First Traversal with Global Optimizationsa, Proc.
of VLDB, 1997.
[11] C. Jensen and R. Snodgrass, aTemporal Data Managementa, TKDE 11(1), 1999.
[12] C. Kolovson and M. Stonebraker, aSegment Indexes: Dynamic Indexing Techniques for Multi-Dimensional Interval Dataa, Proc.
of SIGMOD, 1991.
[13] N. Kline and M. Soo, aTime-IT, the Time-Integrated Testbeda, ftp://ftp.cs.arizona.edu/timecenter/time-it0.1.tar.gz, Current as of August 18, 1998.
[14] A. Kumar, V. J. Tsotras and C. Faloutsos, aDesigning Access Methods for Bitemporal Databasesa, TKDE 10(1), 1998.
[15] T. Leung and R. Muntz, aStream Processing: Temporal Query Processing and Optimizationa, in Temporal Databases: Theory, Design, and Implementation, (ed.)
A. Tansel, J. Clifford, S. Gadia, S. Jajodia, A. Segev and R. Snodgrass, Benjamin/Cummings, 1993.
[16] H. Lu, B. Ooi and K. Tan, aOn Spatially Partitioned Temporal Joina, Proc.
of VLDB, 1994.
[17] D. Lomet and B. Salzberg, aAccess Methods for Multiversion Dataa, Proc.
of SIGMOD, 1989.
[18] S. Rana and F. Fotouhi, aEfficient Processing of Timejoins in Temporal Data Basesa, Proc.
of Int.
Conf.
on Database Systems for Advanced Applications (DASFAA), 1993.
[19] S. Ramaswamy and T. Suel, aI/O-Efficient Join Algorithms for Temporal, Spatial, and Constraint Databasesa, Bell Labs TechReport, URL: http://www.bell-labs.com/user/sridhar/ftp/suelrep.ps.
gz, 1996.
[20] D. Son and R. Elmasri, aEfficient Temporal Join Processing using Time Indexa, Proc.
of SSDBM, 1996.
[21] H. Shen, B. Ooi and H. Lu, aThe TP-Index: A Dynamic and Efficient Indexing Mechanism for Temporal Databasesa, Proc.
of ICDE, 1994.
[22] A. Segev and A. Shoshani, aThe Representation of a Temporal Data Model in the Relational Environmenta, Proc.
of SSDBM, 1988.
[23] M. Soo, R. Snodgrass and C. Jensen, aEfficient Evaluation of the Valid-Time Natural Joina, Proc.
of ICDE, 1994.
[24] B. Salzberg and V. J. Tsotras, aComparison of Access Methods for Time-Evolving Dataa, Computing Surveys 31(2), 1999.
[25] P. Varman and R. Verma, aAn Efficient Multiversion Access Structurea, TKDE 9(3), 1997.
[26] T. Zurek, aOptimization of Partitioned Temporal Joinsa, Ph.D. thesis, University of Edinburgh, 1997.
[27] D. Zhang, V. J. Tsotras and B. Seeger, aEfficient Temporal Join Processing using Indicesa Proc.
of ICDE, 2002.
A Calculus of Macro-Events: Progress Report  Iliano Cervesato  Angelo Montanari  Advanced Engineering and Sciences Division ITT Industries, Inc. Alexandria, VA 22303-1410, USA  Dipartimento di Matematica e Informatica Universita di Udine Via delle Scienze, 206 { 33100 Udine, Italy  iliano@itd.nrl.navy.mil  montana@dimi.uniud.it  plicit axioms to model interaction among concurrent actions, e.g.
[4, 5, 13, 14, 18, 23, 26].
If we also remove the assumption that actions are instantaneous, occurrences of actions may partially overlap which can inuence the outcome of the interacting actions [20].
However, the ability of dealing with concurrent and/or temporally extended actions is not suAcient for many realistic applications.
Modeling real-world domains may indeed require representing complex patterns of actions, which, besides sequentiality and concurrency, involve additional relations among actions, such as the occurrence of just one action in a given set, the iteration of occurrences of the same action or of a pattern of actions.
The notion of process has been introduced to dene compound actions in terms of patterns of simpler actions.
As an example, the action of dialing a ten-digit number can be dened as a set of ten basic actions in strict sequence.
Foundational work on process modeling, which inspired research in many Computer Science areas, including knowledge representation, has been done by Hoare [15] and Milner [21].
Limited contributions to (discrete) process modeling in the temporal representation and reasoning area have been proposed by Evans [12], Belegrinos and George [6], Lesperance et al.
[17], and by Lin and Dean [19].
In this paper, we present preliminary results on the denition of a Macro-Event Calculus, an extension of Kowalski and Sergot's Event Calculus, EC [16].
Our proposal allows expressing basic forms of process interaction, including temporal delays between processes, sequential, simultaneous, and alternative occurrences of processes, and process iteration.
This proposal builds on work by Chittaro and Montanari [10] on modeling discrete processes.
The set of constructors of the current version of the Macro-Event Calculus is similar to the path expression operators of [3].
Originally developed for modeling operating system behavior, path expressions have been successfully used in several areas of Computer Science.
In this paper, we show how their formalization within the Event Calculus can be usefully  Abstract  The need of constraining the temporal relationships among sets of related events arises in several temporal reasoning tasks, including monitoring, plan validation, planning, and diagnosis.
Process constructors provide an eective way of packaging up related events into individual conceptual chunks, called macro-events.
In this paper, we present a rst attempt at dening a Calculus of Macro-Events that extends Kowalski and Sergot's Event Calculus with process constructors to express eects triggered by complex combinations of event occurrences.
We apply this language to model the operations of a simple gas heater, and present a Prolog implementation.
1  Introduction  Classical formalisms for reasoning about actions and change make the simplifying assumptions that (i) only one action can be performed at any given time (some formalisms actually allow simultaneous actions under the assumption that they do not inuence each other), and (ii) actions are instantaneous.
When we remove the rst assumption, we must take into account that concurrent actions may interact.
Interactions may lead both to synergistic results (their combined eect is more than the sum of their individual outcomes) and to interferences (their individual eects may be partially or totally canceled).
For example [2], if one agent lifts one end of a piano, while another agent lifts the other end, then the entire piano is lifted o the oor; instead, if one agent pushes a door open, while the other is pushing it closed, the two actions cancel each other out.
To handle these situations, many formalisms support ex The rst author is supporting the Formal Methods Section of the Naval Research Laboratory under contract N001496-D2024.
The second author was partially supported by the MURST project Software Architectures and Languages to Coordinate Distributed Mobile Components.
47  Form Approved OMB No.
0704-0188  Report Documentation Page  Public reporting burden for the collection of information is estimated to average 1 hour per response, including the time for reviewing instructions, searching existing data sources, gathering and maintaining the data needed, and completing and reviewing the collection of information.
Send comments regarding this burden estimate or any other aspect of this collection of information, including suggestions for reducing this burden, to Washington Headquarters Services, Directorate for Information Operations and Reports, 1215 Jefferson Davis Highway, Suite 1204, Arlington VA 22202-4302.
Respondents should be aware that notwithstanding any other provision of law, no person shall be subject to a penalty for failing to comply with a collection of information if it does not display a currently valid OMB control number.
1.
REPORT DATE  3.
DATES COVERED 2.
REPORT TYPE  2000  00-00-2000 to 00-00-2000  4.
TITLE AND SUBTITLE  5a.
CONTRACT NUMBER  A Calculus of Macro-Events: Progress Report  5b.
GRANT NUMBER 5c.
PROGRAM ELEMENT NUMBER  6.
AUTHOR(S)  5d.
PROJECT NUMBER 5e.
TASK NUMBER 5f.
WORK UNIT NUMBER  7.
PERFORMING ORGANIZATION NAME(S) AND ADDRESS(ES)  8.
PERFORMING ORGANIZATION REPORT NUMBER  ITT Industries Inc,Advanced Engineering and Sciences Division,Alexandria,VA,22303-1410 9.
SPONSORING/MONITORING AGENCY NAME(S) AND ADDRESS(ES)  10.
SPONSOR/MONITORaS ACRONYM(S) 11.
SPONSOR/MONITORaS REPORT NUMBER(S)  12.
DISTRIBUTION/AVAILABILITY STATEMENT  Approved for public release; distribution unlimited 13.
SUPPLEMENTARY NOTES  The original document contains color images.
14.
ABSTRACT 15.
SUBJECT TERMS 16.
SECURITY CLASSIFICATION OF:  17.
LIMITATION OF ABSTRACT  a.
REPORT  b. ABSTRACT  c. THIS PAGE  unclassified  unclassified  unclassified  18.
NUMBER OF PAGES  19a.
NAME OF RESPONSIBLE PERSON  12  Standard Form 298 (Rev.
8-98) Prescribed by ANSI Std Z39-18  Safety disable  Lighter  Power  Desired Temperature  Gas Main  Figure 1.
The Gas Heater: a Useras Perspective  system start up procedure, a switch (Power ) used to enable or disable system operation, a knob (Desired Temperature ) used to select the desired temperature for the environment where the gas heater is placed, a tap (Gas Main ) used to allow or prevent the supply of gas to the heater and a plug to supply electricity to the heater.
The user typically starts up the system by: connecting the plug to a socket, opening the Gas Main tap, turning on the Power switch, pressing the Safety Disable button together with the Lighter button and keeping them pressed until he/she sees the pilot light turning on, releasing then the two buttons.
When the pilot light is on, heating is automatically controlled by the system, which burns gas when the room temperature is lower than desired and keeps only the pilot light on otherwise (details below).
employed to reason about complex events.
Compared to other approaches to the formalization of processes in temporal reasoning, our proposal is characterized by a logical bias: we aim at developing a calculus that gives a logical meaning to constructs which are operational in nature.
We reconcile the two views by providing a logic programming implementation of the Macro-Event Calculus.
The paper is organized as follows: in Section 2, we describe the Gas Heater Problem, that we will use as our case study throughout this paper.
In Section 3 we formalize versions of the Event Calculus that allow explicit time and event durations.
In Section 4, we dene macro-events and extend our specication of EC to handle them.
We give a Prolog implementation of these various calculi in Section 5, and outline directions of future work in Section 6.
2  Looking inside the gas heater (Figure 2), six main components deserve attention.
The Power Switch allows or prevents the supply of electrical power to the Thermostat and to the Lighter System.
The Lighter System is devoted to producing sparks in order to light up the pilot light during the start up procedure.
The Safety System prevents dangerous gas leaks into the environment: this thermo-mechanical device closes the Safety Valve when it is not heated by the pilot light and opens it when the pilot light is on.
If the Safety Disable button is pressed, the Safety Valve allows gas to ow through the pipe connected to the pilot light (thus allowing, if needed, to ignite the pilot light with the Lighter ) but not through the pipe connected to the Thermostatic Valve.
The Thermostat senses room  Case Study  We will illustrate the use of EC and of the proposed extensions by modeling some of the operations of a simple gas heater [10].
We now give an informal description of this case study, and later formalize it as we introduce concepts and denitions.
In [22], we have successfully applied a variant of the MacroEvent Calculus to a larger-scale example, the Dagstuhl Steam-Boiler problem [1].
The gas heater presents to its user an interface (shown in Figure 1) consisting of two buttons (Safety Disable and Lighter ) which must be used during the 48  Lighter system  Safety system  Gas Main  Safety valve Thermostatic valve  Power switch  Thermostat  Figure 2.
The Gas Heater: an Engineeras Perspective  temperature and opens the Thermostatic Valve if the temperature is one degree or less lower than the Desired Temperature preset by the user; it closes the valve when temperature is at least one degree higher than the Desired Temperature.
When both the Safety Valve and the Thermostatic Valve are open, a huge quantity of gas is allowed to reach the main burner and be burnt, possibly ignited by the pilot light.
3  time have the eect of initiating or terminating the validity of properties when given preconditions are met.
The time-independent aspects of a situation are formalized by means of an ECT-structure.
It alters the notion of PEC -structure given in [8] only by the addition of a temporal domain.
Denition 3.1 (ECT-structure) A structure for the Event Calculus with Explicit Time (ECT-structure for short) is a quintuple H = (E; P; [ji; hj]; T ) such that:  Explicit Time and Event Duration  We will now introduce the avors of EC we will consider in this paper.
In Section 3.1, we formalize EC with explicit time.
In Section 3.2, we extend this model to admit non-instantaneous events.
Finally, in Section 3.3, we apply these notions to our case study.
  = fe1 ; : : : ; en g and P = fp1 ; : : : ; pm g are nite sets of event types and properties, respectively.
Elements of 2P are called contexts and the properties in them are referred to as preconditions.
E   [ji : P  2P !
2E  While the original informal presentation of the Event Calculus [16] anchors the occurrence of instantaneous events to explicit time points, recent work on the formalization of EC [7, 8, 9] abstracted the time line and only considered events that are ordered relative to one another.
We will extend this denition so to admit explicit time.
We will operate on the formulation in [8], that embeds preconditions.
The Event Calculus with Explicit Time and Instantaneous Events (ECT ) aims at modeling situations that consist of a set of events, whose occurrences over    and  hj] : P  2P !
2E  are respectively the initiating and terminating map of H. For every property p 2 P , [pjC i and hpjC ] represent the set of events that initiate and terminate p, respectively, in case all preconditions in C hold at their occurrence time.
3.1 Explicit Time  The temporal domain T is some ordered set (T; ).
In our implementation, we used the natural numbers N with their usual ordering.
The instance-specic part of a specication is captured through the following notion of time-structure : 49  Denition 3.2 (Time-structure) Given an ECT-structure H = (E; P; [ji; hj]; T ), a time-structure for H is a set T  E  T of event instances, where a pair (e; t) 2 T expresses the fact that an event of type e has occurred at time t.  3.2 Event Duration  Although thinking of events as instantaneous is a suAcient abstraction for some situations, in many cases the occurrence of an event happens over a period of time [24].
Capturing this possibility enables ner models, as we can now reason about changes that take place while an event is in the process of occurring.
In particular, we must revise our notion of precondition as it can now have at least two meanings: a property that should hold in order for an event to start happening, or a property that ought to be uninterruptedly valid during the entire duration of the occurrence of an event.
In our formalization below, we will allow both options, keeping the name precondition for the former, and calling the latter constraint [10].
Given the formalization of a situation in terms of an ECT -structure and a time-structure, ECT provides means to check whether a given property p is valid at a given point t in time [predicate holdsAt(p; t) below].
This problem is easily reduced to the determination of the maximal validity intervals, abbreviated MVI, over which a given property holds [mvi(p; t1 ; t2 )]: a property p holds maximally over an interval [t1 ; t2 ] if i) t1  t2 , ii) an event e initiating p subject to preconditions C1 has occurred at t1 and all properties in C1 hold at t1 [initiate(p; t1 )], iii) dually, an event e terminating p subject to preconditions C2 has occurred at t2 and all property in C2 hold at t2 [terminate(p; t2 ], and iv) p is not initiated or terminated at any time point between t1 and t2 [:broken(p; t1 ; t2 )].
Denition 3.4 (ECTD-structure) A structure for the Event Calculus with Event Duration (ECTD-structure for short) is a sextuple H = (E; P; [jji; hjj]; T; A ) such that:    Denition 3.3 (ECT-model) Let H = (E; P; [ji; hj]; T ) be a ECT-structure and T be a time-structure over H. We dene the following recursive predicates:  9t1 ; t2 2 T:  holdsAt(p; t)  i  mvi(p; t1 ; t2 )  i t1  t2  initiate(p; t)  i  mvi(p; t1 ; t2 )  t1   [jji: P  2P  2P !
2E and hjj]: P  2P  2P !
2E are respectively the initiating and terminating map of H. For every property p 2 P , [pjC jK i and hpjC jK ] represent the set of events that initiate and terminate p, respectively, in case all preconditions in C hold at the time they start occurring, and all constraints in K hold for the entire duration of their occurrence.
 t  t2 ^  ^ initiate(p; t1 ) ^ terminate(p; t2 ) ^ :broken(p; t1 ; t2 )  terminate(p; t) i broken(p; t1 ; t2 ) i  and P are the sets of event types and properties, respectively.
E  9e 2 E: 9C 2 2P : (e; t) 2 T ^ p 2 [ejC i ^ 8q 2 C: holdsAt(q; t) 9e 2 E: 9C 2 2P : (e; t) 2 T ^ p 2 hejC ] ^ 8q 2 C: holdsAt(q; t) 9t 2 T: t1  t ^ t  t2 ^ (initiate(p; t) _ terminate(p; t)).
   T = (T; ; +; 0) is the temporal domain, where the ordered set (T; ) considered above is augmented with a monoid component (T; +; 0).
: E !
T is a duration function, that gives the duration of every event type.
A  Observe that ECT -structures are degenerate ECTD structures where no constraints are present, and A (e) = 0 for every event e 2 E .
We do not need to modify the denition of time-structure.
In spite of the drastic changes in Denition 3.4 with respect to the notion of ECT -structure, the modications to the specication of how to check whether a property holds at a given time point are localized to the predicates initiate and terminate.
Let us focus on the former: in order for initiate(p; t) to hold, there must be an event e with preconditions C and constraints K that initiates p. If e starts occurring at time t0 , it must be the case that t is the endpoint of the occurrence  The meta-predicates holdsAt, mvi, initiate, terminate and broken are mutually recursive in the above denition.
In particular, an attempt at checking properties or computing MVIs by simply unfolding their denition is non-terminating in pathological situations [8].
However, most ECT problems encountered in practice satisfy syntactic conditions ensuring the termination of this procedure.
See [8] for a detailed discussion of these restrictions.
The above denition can be directly transcribed in the programming language Prolog [25].
The resulting code is given in Section 5.1.
50  interval of e (i.e.
t = t0 + A (e)).
Moreover, every precondition in C must hold at its startpoint t0 , and every constraint in K must be valid without interruption throughout the interval [t0 ; t].
Notice in particular that an event has eectively initated a property at the end of its occurrence interval.
Similar considerations apply to the case of terminate.
This is captured in the following denition, where the predicates holdsDuring and happens are accessory.
3.3 Example a Part I  The rst phase of the representation of the gas heater in EC consists in the identication of the relevant types of events.
We consider eleven types: eight of them represent possible actions of the user of the gas heater, i.e.
open or close the Main Gas tap (gasOn, gasO), turn on or o the Power switch (powerOn, powerO), press or release the Safety Disable button (prDisable, relDisable), and press or release the Lighter button (prLighter, relLighter).
In this simple model, we will not represent the action of setting the Thermostat to the desired temperature.
Two other types of event represent the possible temperature changes in the environment as reported by a Thermometer (coolDown, warmUp).
In order to represent the properties that are initially valid in the system [11], we add the ctitious event start which represent the beginning of time.
A possible time structure T built using the above types of events is the following:  Denition 3.5 (ECTD-model) Let H = (E; P; [jji; hjj]; T; A) be a ECTDstructure and T be a time-structure over H. We modify Denition 3.3 by overriding the specication of initiate and terminate given there with the clauses below, and adding the predicates happens and holdsDuring.
initiate(p; t)  i  9e 2 E: 9t0 ; d 2 T: 9C; K 2 2P : happens(e; t0 ; d) ^ t = t0 + d ^ p 2 [ejC jK i ^ 8q 2 C: holdsAt(q; t0 ) ^ 8q 2 K: holdsDuring(q; t0 ; t)  terminate(p; t)  (start; (coolDown; (gasOn; (powerOn; (prDisable; (prLighter; (relLighter;  i  9e 2 E: 9t0 ; d 2 T: 9C; K 2 2P : happens(e; t0 ; d) ^ t = t0 + d ^ p 2 hejC jK ] ^ 8q 2 C: holdsAt(q; t0 ) ^ 8q 2 K: holdsDuring(q; t0 ; t)  happens(e; t; d)  i (e; t) 2 H  holdsDuring(p; t1 ; t2 ) i  ^  d  0): 0): 1): 2): 3): 3): 5):  (relDisable; (warmUp; (coolDown; (warmUp; (powerO ; (gasO ; (coolDown;  6): 8): 10): 11): 18): 19): 25):  In this scenario, the user notices a drop in temperature (time 0) and takes all the actions needed in order to ignite the pilot light: she opens the Gas Main (time 1), switches the power on (time 2), and presses the Lighter and Safety Disable buttons simultaneously (time 3).
She releases these buttons at time 5 and 6, respectively.
Between time 8 and 11 the thermometer reports some temperature changes in the environment.
At time 18 the user interrupts the supply of gas, and shortly after she turns the power o.
Eventually, the rooms becomes cold again (time 25).
The second step in the representation of the gas heater is the identication of the interesting properties that are initiated and terminated by events.
We model this system by means of nine properties: whether gas is owing into the heater (gas), whether electrical power is supplied (power), whether the room is cold (cold), whether the thermostatic and the safety valves are open (thermoVOpen and safetyVOpen, respectively), whether the lighter system produces sparkles (sparkling), whether the pilot light is on (pilotOn), and whether gas is burning in the main combustion chamber (burning).
Finally, it will be convenient to know when the pilot light is o (pilotO).
These properties fall in three classes: properties ini-  = A (e)  9t0 ; t3 2 T: mvi(p; t0 ; t3 ) ^ t0  t1 ^ t2  t3 :  It is easy to verify that, whenever H corresponds to an ECT -structure, the denition reduces to Denition 3.1.
It is interesting to observe that, in the absence of constraints, the Event Calculus with Event Duration can be compiled into the formalism presented in Section 3.1.
Every lasting event e is translated into a pair of instantaneous events start e and end e, and a property occ e. Instances (e; t) are mapped to the instantaneous instances (start e; t) and (end e; t + A (e)).
Finally, if p 2 [ejC ji, we get occ e 2 [start ejC i, moreover occ e 2 hend ej], nally p 2 [end eji.
The termination map is processed analogously.
The treatment of constraints is problematic because of the requirement that they ought not to be interrupted during the event's occurrence interval.
Prolog code implementing Denition 3.5 is given in Section 5.2.
51  events.
We will address this shortcoming of ECTD by allowing the validity status of properties to be changed on the basis of the occurrence of structured conglomerations of events that we will call macro-events.
In Section 4.1, we dene this notion and extend ECTD structures to accommodate it.
It takes into account the cumulative eects of a set of related events, but for simplicity, excludes interference issues.
In Section 4.2, we simultaneously address the problems of determining whether a macro-event has occurred, and extend the specication of validation of a property (Denition 3.5) to these entities.
In Section 4.3, we complete the treatment of our case-study.
tiated or terminated by the simple occurrence of an event; properties initiated or terminated by the occurrence of an event in a specic context; properties initiated or terminated by a combination of events.
Properties gas, power, cold, and thermoVOpen fall in the rst class: they are unconditionally initiated and terminated by the events gasOn and gasO, powerOn and powerO, coolDown and warmUp, and coolDown and warmUp, respectively.
This is captured in EC as follows: [gasjji [powerjji [coldjji [thermoVOpenjji hgasjj] hpowerjj] hcoldjj] hthermoVOpenjj]  = = = = = = = =  fgasOng; fpowerOng; fcoolDowng; fcoolDowng; fgasO g fpowerO g fwarmUpg fwarmUpg  4.1 Definition  In our current model, macro-events are obtained by considering sequential, alternative, parallel, or iterated occurrences of elementary events, or any combination of these constructions.
The second class consists of the properties sparking and safetyVOpen.
While the former simply requires that power be supplied when the lighter button is pressed, the latter has a more complex behavior.
Consider rst how to terminate the property safetyVOpen: if the pilot light is o, releasing the safety button is suAcient to close the valve; however if this is not the case, the only way to shut the safety valve is by extinguishing the pilot light, which is achieved, as we will see, by interrupting the supply of gas.
We have the following formalization: [sparkingjjpoweri [safetyVOpenjji hsparkingjj] hsafetyVOpenjjpilotOn] hsafetyVOpenjjpilotO]  = = = = =  Denition 4.1 (Macro-Events) Given a set of events E and a temporal domain T = (T; ; +; 0), macro-events, denoted with m possibly subscripted, are expressions dened by the following grammar: m  fprLighterg fprDisableg frelLigher; powerO g fgasO g frelDisableg  j j j j  e m1 ;D d m2 m1 + m2 m1 m2 m  jj  (Basic event) (Sequence with delay (Alternative) (Parallelism) (Iteration)  where d; D 2 T and d  D. Let macro-events over T .
MT  d  to D)  be the set of the  This denition formalizes the core of the notion of process studied at length in [10, 22], which in turn extends the limited notion of macro-events (essentially delayed sequencing) presented in [12].
The constructors we included in this language are based on the path expression operators of [3] and on the process calculi operators found in [15, 21].
Observe that a number of useful constructs are easily expressible with the language in Denition 4.1.
In particular sequencing with arbitrary delay (;), immediate sequencing (;;), non-empty iteration (+ ) and xed-length iteration (n ) are dened as follows in [22]:  The remaining properties pilotOn and pilotO, which records whether the pilot light is on or o, depends on constraints such as the availability of power and gas.
However, they are initiated and terminated respectively by the simultaneous occurrence of two events (prDisabled and prLighter).
Similar remarks apply to the property burning.
The extensions to EC we devised so far are insuAcient to specify these situations.
4  ::=  Event Calculus with Macro-Events  As we just saw, even when events with durations are available, EC does not lend itself to easily expressing situations where properties are initiated or terminated not by single events, but by the occurrence of multiple  m1 ; m2 m+ m1 ;; m2 mn  52  = = = =  m1 ;1 0 m2 m; m m1 ;00 m2 m; : : : ; m  (n times)  me(e; [t1 ; t2 ]; s; l) i (e; s) 2 T ^ l = A (e) ^ [s; s + l]  [t1 ; t2 ] me(m1 ;D m ; [ t ; t ] ; s; l ) i 9t01 ; t02 ; l1 ; l2 2 T: me(m1 ; [t1 ; t01 ]; s; l1 ) ^ me(m2 ; [t02 ; t2 ]; s2 ; l2 ) d 2 1 2 s + l1  t01  t02  s2 ^ s + l1 + d  s2  s + l1 + D l = s2 + l2 s me(m1 + m2 ; [t1 ; t2 ]; s; l) i me(m1 ; [t1 ; t2 ]; s; l) _ me(m2 ; [t1 ; t2 ]; s; l) me(m1 jj m2 ; [t1 ; t2 ]; s; l) i 9s1 ; l1 ; s2 ; l2 2 T: me(m1 ; [t1 ; t2 ]; s1 ; l1 ) ^ me(m2 ; [t1 ; t2 ]; s2 ; l2 ) s = min(s1 ; s2 ) ^ l = max(s1 + l1 ; s2 + l2 ) s me(m ; [t1 ; t2 ]; s; l) i 9l1 ; s2 ; l2 2 T: (s = t1 ^ l = 0) _ (me(m; [t1 ; t]; s; l1 ) ^ me(m ; [t; t2 ]; s2 ; l2 ) ^ s + l1  t  s2 ^ l = s2 + l2 s)  ^ ^ ^  Figure 3.
Definition of me  Before giving the exact semantics of macro-events, we update our formalization of ECTD of Section 3.2 to accomodate these entities.
The changes to the denition of ECTD -structure turn out to be very modest.
4.2 Monitoring and Evaluation  me(m; [t1 ; t2 ]; s; l), which veries whether a macroevent m has occurred over the interval [t1 ; t2 ], and, if this is the case, computes its starting point s and duration l. These two arguments are necessary to correctly process the bounds of a delayed sequence of events.
Given a MECTD -structure H = (E; P; M; [jji; hjj]; T; A) and a time structure T on H, this predicate is recursively dened in Figure 3, where we have promoted  to denote the sub-interval relation over T .
Observe that me denes the semantics of the macroevent constructors presented in Denition 4.1.
In the base case of the recursion, i.e.
if m is an event e, we verify if an ocurrence of e has been recorded in the time-structure T .
If so, we check that it takes place over a subinterval of [t1 ; t2 ].
In the case of a sequence, we must make sure that the endpoint of the rst component and the starting time of the second are within the acceptable delay.
Clearly, they must take place over sequentially disjoint intervals.
In order to verify that an alternative macro-event has occurred, we look for the occurrence of either component.
Parallel macroevents must have both occurred over the same interval.
Observe that we do not require that the two branches mention distinct events; indeed m jj m is equivalent to m. Finally, iterated macro-events are essentially reduced to (possibly empty) sequences.
Notice that we do not force any form of maximality: the empty iteration is always satised; its starting point is made to coincide with the beginning of the test interval, and it always has null duration.
We check whether a given macro-event has occurred over some interval by using the above denition, while abstracting from the starting time and duration.
The occurrence of a macro-event is not explicitely recorded, but must be determined on the basis of its denition and of the time-structure at hand.
In order to do so, we dene the auxiliary predicate  Denition 4.3 (Monitoring Macro-Events) Let H be a MECTD-structure and T be a timestructure over H. We say that a macro-event m has occurred over an interval [t1 ; t2 ], written  Denition 4.2 (MECTD-structure) A structure for the Macro-Event Calculus (MECTD-structure for short) is a septuple H = (E; P; M; [jji; hjj]; T; A) which differs from the denition of ECTD-structure only by the following points:       M   MT  is a set of macro-events over T .
The codomain of [jji and hjj] are redened to be 2M : indeed [jji : P  2P  2P !
2M and hjj] : P  2P  2P !
2M .
This implies that properties can be started and ended by generic macro-events, not just plain events.
We assume that the temporal domain (T; ) has a maximum element 1, and that (T; +; 0) is a group, with the inverse operation of +.
Observe that we did not propagate the change to the duration function: only basic events are explicitly given a duration by means of A .
The duration of occurrences of macro-events will instead be computed on the basis of their structure and of the participating basic events.
We do not modify our notion of time-structure T : only elementary events are recorded.
Occurrences of macro-events will instead be inferred.
53  check(m; [t1 ; t2 ]), i  drops below the thermostatic threshold, gas will start burning.
The second handles the case where the room is cold at the moment in which the pilot light is ignited.
This property has an equally interesting termination behavior: it can always be ended by cutting the gas supply, but if the pilot light is on it is suAcient that the room temperature warms up above the threshold set through the thermostat.
This concludes the specication of the properties of interest.
It is worth observing that the overall specication of the gas heater could be considerably shortened by admitting the notions of auto-initiation and autotermination [10] to our calculus.
An auto-initiated property (here burning is a good candidate) is explicitly initiated not by the occurrence of events, but as soon as the validity periods of one or more other properties start overlapping (here pilotOn and cold).
Autotermination is dened in a dual way.
We plan to include these constructs in a forthcoming version of the macro-event calculus.
9s; l 2 T: me(m; [t1 ; t2 ]; s; l) is valid.
The starting point and duration of macro-events are useful in order to compute MVIs, and ultimately to check whether a given property holds at a certain point in time.
Therefore, me oers a way to update the predicate happens from Denition 3.5 to operate over generic macro-events.
Denition 4.4 (MECTD-model) Let H be a MECTD-structure and T be a timestructure over H. We modify Denition 3.5 by overriding the denition of happens given there with the following clause: happens(m; t; d) i me(m; [0; 1]; t; d) This denition provides an elegant specication to the core of the system presented in [10].
We limited ourselves to treating the situation where macro-events can initiate or terminate properties.
This is suAcient for many applications, and involves much simpler definitions than the general case.
We did not include here the possibility of a macro-event that cancels eects caused by some of its components (be it an elementary event, or a macro-event).
A complete specication can be found in [22], and will be included in an extended version of this paper.
An implementation of these specications can be found in Section 5.3.
5  We will now give a Prolog [25] implementation of the calculi we have presented so far and an encoding of our case study.
We assume the reader is familiar with this logic programming language.
5.1 EC with Explicit Time  We represent the initiation and termination maps, [ji and hj] of an ECTD -structure by means of the Prolog predicates init and term, respectively.
We use Prolog 's lists to represent the preconditions of the effect of an event.
We adopt the integers as the temporal domain T .
The precedence relation  is then mapped to <.
Each pair (e; t) in a time-structure is represented as the fact happens(peq ,ptq ), where peq and ptq encode e and t respectively.
For aesthetic reasons, we represent an interval [t1 ; t2 ] with the two-element list [pt1 q ,pt2 q ].
The contents of Denition 3.3 are then transcribed as follows in Prolog, where we have kept the predicate name unchanged.
4.3 Example a Part II  Macro-events are a convenient tool to complete the specication of the example in Section 2.
Continuing from Section 3.3, we can now express the validity of properties pilotOn, pilotO, and burning: [pilotOnjjpower; gasi [pilotO jji [pilotO jjpilotOni [burningjjpilotOni [burningjjcold; power; gasi hpilotOnjj] hpilotO jjpower; gas] hburningjjpilotOn] hburningjj]  = = = = = = = = =  Implementation  fprLighter jj prDisableg fstartg fgasO g fcoolDowng fprLighter jj prDisableg fgasO g fprLighter jj prDisableg fwarmUpg fgasO g  holdsAt(P, T) :mvi(P, [T1,T2]), T1 =< T, T < T2.
mvi(P, [T1,T2]) :initiate(P, T1), terminate(P, T2), T1 < T2, \+ broken(P, [T1,T2]).
Property burning has two initiation clauses.
The rst applies when the pilot light is lit: if the temperature 54  happens(E, P),  mHoldsAt([],_).
mHoldsAt([P|C], T) :holdsAt(P, T), mHoldsAt(C, T).
lasts(E, D).
mHoldsDuring([], _).
mHoldsDuring([P|C], I) :holdsDuring(P, I), mHoldsDuring(C, I).
initiate(P, T) :init(E, P, C), happens(E, T), mHoldsAt(C, T).
holdsDuring(P, [T1,T2]) :mvi(P, [T0,T3]), T0 =< T1, T2 =< T3.
terminate(P,T) :term(E, P, C), happens(E, T), mHoldsAt(C, T).
Again, this implementation can be proved sound and complete with respect to Denition 3.4.  broken(P, [T1,T2]) :(initiate(P, T) ; terminate(P, T)), T1 < T, T < T2.
We encode the process constructions m1 ;D d m2 , m1 + m2 , m1 jj m2 and m by means of the Prolog terms seq(pm1q ,pm2 q ,pdq ,pDq ), alt(pm1q ,pm2 q ), par(pm1q ,pm2 q ), and it(pmq ), respectively.
Lacking a better abstraction, we used 100000 for 1.
We implement the predicate me and Denitions 4.34.4 by replacing the clause for happens/3 above with the following code.
The convoluted denition for subinterval is due to the fact that it is used both for checking that an interval is contained in another interval, but also to set either endpoints of the former.
5.3 Macro-Event Calculus  Whenever the syntactic conditions [8] mentioned in Section 3.1 are met, this program allows not only verifying the validity of a property at a given time point t, but also computing all the properties that hold at t. Using the technique in [9], it is possible to prove the soundness and completeness of this program with respect to Denition 3.3.
5.2 EC with Explicit Time and Event Duration  We add one argument to init and term to the previous representation to encode the constraints that distinguish [jji and hjj] in an ECTD -structure.
Moreover, we rely on Prolog 's arithmetic to emulate the operation now available in the temporal domain.
Finally, we rely on the predicate lasts to model the duration function A .
The behavior of ECTD is captured by replacing the clauses for initiate and terminate with the following denitions, and adding the accessory predicates happens, mHoldsDuring, and holdsDuring.
me(E, I, S, L) :happens(E, S), lasts(E, L), T is S + L, subinterval([S,T], I).
initiate(P,TD) :init(E, P, C, K), happens(E, T, D), TD is T + D, mHoldsAt(C, T), mHoldsDuring(K, [T,TD]).
me(alt(M1,M2), I, S, L) :(me(M1, I, S, L) ; me(M2, I, S, L)).
me(seq(M1,M2,Min,Max), [T1,T2], S, L) :me(M1, [T1,T11], S, L1), me(M2, [T22,T2], S2, L2), S + L1 =< T11, T11 =< T22, T22 =< S2, Min =< (S2 - S - L1), (S2 - S - L1) =< Max, L is S2 + L2 - S.  me(par(M1,M2), I, S, L) :me(M1, I, S1, L1), me(M2, I, S2, L2), S is min(S1,S2), L is max(S1+L1,S2+L2) - S.  terminate(P, TD) :term(E, P, C, K), happens(E, T, D), TD is T + D, mHoldsAt(C, T), mHoldsDuring(K, [T,TD]).
me(it(M), [T1,T2], S, L) :me(M, [T1,T], S, L1), me(it(M), [T,T2], S2, L2), S+L1 =< T, T =< S2, L is S2 + L2 - S.  happens(E, P, D) :-  55  me(it(_), [T,T], T, 0) :- !.
me(it(_), [T,_], T, 0).
check(M, I) :me(M, I, _, _).
term(powerOff,  power,  [], []).
init(coolDown, term(warmUp,  cold, cold,  [], []).
[], []).
init(coolDown, term(warmUp,  thermoVOpen, [], []).
thermoVOpen, [], []).
happens(M, T, D) :me(M, [0,100000], T, D).
init(prLighter, sparking, term(relLighter, sparking, term(powerOff, sparking,  subinterval([B1,E1], [B2,E2]) :((var(B2), B1 = B2, !)
; B2 =< B1), ((var(E2), E1 = E2, !)
; E1 =< E2).
init(prDisable, safetyVOpen, [], []).
term(gasOff, safetyVOpen, [], [pilotOn]).
term(relDisable, safetyVOpen, [], [pilotOff]).
Showing the soundness and completeness of this implementation with respect to the specications given in Section 4.2 is complicated by the non-logical implementation of subinterval.
However, once this predicate has been processed in isolation, standard techniques from [9] can be successfully applied.
init(coolDown, burning, [], init(par(prLighter,prDisable), burning, [], term(warmUp, burning, [], term(gasOff, burning, [],  We will now complete the treatment of the gasheater example by displaying the clauses that encode the associated MECTD -structure and time structure.
The latter is immediately rendered by the following facts: 0).
0).
1).
2).
3).
3).
5).
happens(relDisable, happens(warmUp, happens(coolDown, happens(warmUp, happens(powerOff, happens(gasOff, happens(coolDown,  This completes the formalization of the gas heater.
We can now use it to extract information that is implicit in the example.
The following query retrieves the maximum validity intervals of all the properties that appear in this case study.
6).
8).
10).
11).
18).
19).
25).
?- mvi(M, I).
lasts(E, 0).
summarizes all the relevant information about event duration.
The initiation and termination maps relative to the gas heater problem are given by the following code, where we have grouped these facts by the property being initiated or terminated: gas, gas,  [], []).
[], []).
init(powerOn,  power,  [], []).
[cold,power,gas]).
[pilotOn]).
[]).
init(start, pilotOff, [], []).
init(gasOff, pilotOff, [], [pilotOn]).
term(par(prLighter,prDisable), pilotOff, [], [power,gas]).
where we have kept the name of the events (and below properties) as in the body of this paper.
In our example, all events are instantaneous.
Therefore, the clause  init(gasOn, term(gasOff,  [pilotOn]).
init(par(prLighter,prDisable), pilotOn, [], [power,gas]).
term(gasOff, pilotOn, [], []).
5.4 Example a Part III  happens(start, happens(coolDown, happens(gasOn, happens(powerOn, happens(prDisable, happens(prLighter, happens(relLighter,  [], [power]).
[], []).
[], []).
56  M = gas  I = [1, 19]  ;  M = power  I = [2, 18]  ;  M = cold M = cold  I = [0, 8] I = [10, 11]  ; ;  M = thermoVOpen M = thermoVOpen  I = [0, 8] I = [10, 11]  ; ;  M = sparking  I = [3, 5]  M = safetyVOpen  I = [3, 19]  ;  M = burning M = burning  I = [10, 11] I = [3, 8]  ; ;  M = pilotOn  I = [3, 19]  ;  ;  M = pilotOff  I = [0, 3]  establishing the expressiveness and complexity of variants of the Macro-Event Calculus and at systematically comparing them with other formalisms for discrete process modeling proposed in the literature.
;  No  We can also inquire whether a given macro-event has occurred, when it started, and how long it lasted.
For example, if we want to know if it ever happened that the room rst got cold and then warm within 20 time units, the following query will provide three answers:  References  [1] J. Abrial.
Steam-boiler control specication problem.
In Proc.
of the Dagstuhl Seminar on Methods for Semantics and Specications, Dagstuhl, Germany, 1995.
[2] J. F. Allen and G. Ferguson.
Actions and events in interval temporal logic.
Journal of Logic and Computation, 4(5):531{580, 1994.
[3] S. Andler.
Predicate path expressions.
In Proceedings of the 6th ACM Symposium on Principles of Programming Languages, 1979.
[4] C. Baral and M. Gelfond.
Representing concurrent actions in extended logic programming.
In Proc.
of 13th International Joint Conference on Articial Intelligence | IJCAI, pages 866{871, Chambery, France, 1993.
Morgan Kaufmann.
[5] C. Baral and M. Gelfond.
Reasoning about eects of concurrent actions.
Journal of Logic Programming, 31(1{3):85{117, 1997.
[6] P. Belegrinos and M. George.
A model of events and processes.
In Proc.
IJCAI'91, Sydney, Australia, 1991.
[7] I. Cervesato, L. Chittaro, and A. Montanari.
A modal calculus of partially ordered events in a logic programming framework.
In L. Sterling, editor, Proceedings of the Twelfth International Conference on Logic Programming | ICLP'95, pages 299{313, Kanagawa, Japan, 13{16 June 1995.
MIT Press.
[8] I. Cervesato, M. Franceschet, and A. Montanari.
A guided through some extensions of the event calculus.
Computational Intelligence, 16(2):307{347, May 2000.
[9] I. Cervesato and A. Montanari.
A general modal framework for the event calculus and its skeptical and credulous variants.
Journal of Logic Programming, 38(2):111{164, Feb. 1999.
[10] L. Chittaro and A. Montanari.
Reasoning about discrete processes in a logic programming framework.
In D. Sacca, editor, Proceedings of the Eight Conference on Logic Programming | GULP'93, pages 407{421, Gizzieria Lido, Italy, 1993.
Mediterranean Press.
[11] L. Chittaro and A. Montanari.
EAcient temporal reasoning in the cached event calculus.
Computational Intelligence, 12(3):359{382, 1996.
[12] C. Evans.
The macro-event calculus: Representing temporal granularity.
In Proceedings of the Pacic Rim International Conference on Articial Intelligence | PRICAI'90, Nagoya, Japan, 1990.
IOS Press.
[13] M. Gelfond, V. Lifschitz, and A. Rabinov.
What are the limitations of the situation calculus?
In R. Boyer, editor, Automated Reasoning: Essays in Honor of Woody Bledsoe.
Kluwer, 1991.
?- happens(seq(coolDown,warmUp,0,20), T,D ).
T = 0 T = 0 T = 10  D = 8 D = 11 D = 1  ; ; ;  No  Observe that the rst interval embeds the second.
This is acceptable since, dierently from the evaluation of MVI -goals, queries about macro-events do not involve maximality checks.
Finally, we show a use of the predicate check, which veries whether a given macro-event has occurred in a given interval.
?- check(seq(coolDown,warmUp,1,5), [3,19]).
Yes 6  Conclusions and Future Work  In this paper, we presented a preliminary attempt at dening a Calculus of Macro-Events that extends Kowalski and Sergot's Event Calculus with primitives (process constructors) for modeling discrete processes.
In particular, we showed how to infer the occurrence of macro-events from the occurrence of their atomic components (monitoring) as well as how to derive the maximal validity intervals of properties initiated and/or terminated by a given set of macro-events (evaluation).
Furthermore, the expressive power of the Macro-Event Calculus has been demonstrated through the encoding of a simple real-world example (a more complex example, namely the formalization of the Dagstuhl steamboiler control specication problem [1], has been implemented using a superset of our proposal in [22]).
We are investigating ways to extend this work to naturally capture ner process constructors, in particular denitions, non-occurrence, and exclusive alternatives.
This would allow, for example, a complete specication of the gas heater problem.
We are also interested in formalizing synergetic eects and interference [22].
Finally, we are currently working at properly 57  [14] J. Gustafsson and L. Karlsson.
Reasoning about actions in a multi-agent environment.
Linkoping Electronic Articles in Computer and Information Science, 1997. http://www.ep.liu.se/ea/cis/1997/014.
[15] C. Hoare.
Communicating Sequencial Processes.
Prentice-Hall, 1985.
[16] R. Kowalski and M. Sergot.
A logic-based calculus of events.
New Generation Computing, 4:67{95, 1986.
[17] Y. Lesperance, H. J. Levesque, F. Lin, D. Marcu, R. Reiter, and R. B. Scherl.
A logical approach to high-level robot programming | a progress report.
In Control of the Physical World by Intelligent Systems, WorkNotes of the 1994 AAAI Fall Symposium, 1994.
[18] F. Lin and Y. Shoham.
Concurrent actions in the situation calculus.
In Proc.
of the 10th National Conference of the American Association for Articial Intelligence | AAAI, pages 590{695.
AAAI Press/MIT Press, 1992.
[19] S. Lin and T. Dean.
Localized temporal reasoning using subgoals and abstract events.
Computational Intelligence, 12(3):423{449, 1996.
[20] R. Miller and M. Shanahan.
Narratives in the situation calculus.
Journal of Logic and Computation, 4(5):513{ 530, 1994.
[21] R. Milner.
A Calculus of Communicating Systems.
Springer-Verlag, 1980.
[22] M. Mizzaro.
La modellazione di sistemi complessi nel calcolo degli eventi: Analisi di un caso di studio (in italian).
Tesi di Laurea in Scienze dell'Informazione, Universita di Udine, 1997.
Under the supervision of A. Montanari.
[23] J. Pinto.
Concurrent actions and interacting eects.
In Proc.
of the 6th International Conference on Principles of Knowledge Representation and Reasoning | KR'98, pages 292{303.
Morgan Kauman, 1998.
[24] M. Shanahan.
Solving the Frame Problem.
The MIT Press, 1997.
[25] L. Sterling and E. Shapiro.
The Art of Prolog: Advanced Programming Techniques.
MIT Press, 1994.
[26] C. Yi.
Reasoning about concurrent actions with features and uents.
In Proceedings of the Third International Workshop on Temporal Representation and Reasoning | TIME'96, pages 6{13, Key West, FL, 1996.
58
Symbolic Representation of User-defined Time Granularities Claudio Bettini Roberto De Sibi DSI, University of Milan via Comelico 39, 20135 Milan, Italy bettini@dsi.unimi.it, rdesibi@it.oracle.com  Abstract In the recent literature on time representation, an effort has been made to characterize the notion of time granularity and the relationships between granularities, in order to have a common framework for their specification, and to allow the interoperability of systems adopting different time granularities.
This paper considers the mathematical characterization of finite and periodical time granularities, and it identifies a user-friendly symbolic formalism which captures exactly that class of granularities.
This is achieved by a formal analysis of the expressiveness of well-known symbolic representation formalisms.
1.
Introduction There is a wide agreement in the AI and database community on the requirement for a data/knowledge representation system of supporting standard as well as user-defined time granularities.
Examples of standard time granularities are days, weeks, months, while user defined granularities may include businessweeks, trading-days, working-shifts, school-terms, with these granularities having different definitions in different application contexts.
The work in [3, 4] represents an effort to formally characterize the notion of time granularity and the relationships between granularities, in order to have a common framework for their specification and to allow the interoperability of systems adopting different time granularities.
The formal definition, however, is essentially a mathematical characterization of the granules, and it is not suitable for presentation and manipulation by the common user.
The goal of this paper is identifying an intuitive formalism which can capture a significant class of granularities within the formal framework and which is closed for this class with respect to the operations it  allows.
This class can be intuitively described as containing all finite granularities, as well as all periodical ones.
Instead of inventing yet another symbolic formalism for this purpose, in this work we consider some existing proposals, analyzing their expressiveness with respect to our goal.
A symbolic formalism, based on collections of temporal intervals, was proposed in [11] to represent temporal expressions occurring in natural language and used in several application domains like appointment scheduling and time management.
This formalism has been adopted with some extensions by many researchers in the AI [9, 15, 6] and Database area [8, 5].
From the deductive database community, a second influential proposal is the slice formalism introduced in [14], and adopted, among others, in [2].
None of these formalisms and extensions seems to have the expressive power we are seeking, despite some of the proposals include features that go beyond what is needed in our framework.
For example, [6] provides a powerful formalism to represent calendars and time repetition, including existential and universal quantification, which supports the representation of uncertainty, a feature not considered in our framework.
Moreover, some calendar expressions in [6] go beyond the specification of granularities, as defined in [4, 3] and in this paper, allowing the representation of overlapping granules of time.
The formalism can represent recurring events in the form of non-convex intervals, but it does not seem to be able to represent what in the following we call gap-granularities, where gaps may not only occur between one granule and the next, but also within granules.
A business-month seen as an indivisible time granule defined as the union of all business-days within a month is an example.
Relevant work on non-convex intervals and repetition includes [10, 13], but the emphasis in these works is more on reasoning with qualitative relations than on calendar expression representation.
In addi-  tion to the research cited above, significant work on time granularity includes [16, 12, 7].
The contribution of this paper is twofold: on one side we give results on the expressiveness of the formalisms proposed in [11] and [14] which we identify as the two basic approaches to symbolic representation, while, on the other side, we propose an extension to one of these formalisms that allows to capture exactly the class of finite and infinite periodical granularities we defined in [3].
In the next section we introduce the formal notion of time granularity.
In Section 3 we briefly describe the collection and slice symbolic representation formalisms, and we evaluate their expressiveness and formal properties.
In Section 4, we propose an extension to the collection formalism to capture gapgranularities, and we conclude the paper in Section 5.
Appendix A summarizes the syntax of the symbolic formalism, and Appendix B contains the proofs of the results in the paper.
2.
Characterization of time granularities In this section we introduce the mathematical characterization of time granularities as proposed in [4] and further refined and summarized in [3].
Granularities are defined with respect to an underlying time domain, which can be formally characterized simply as a set whose elements arefiordered by a relation ship.
For example, integers , natural numbers  	   , rational , and real numbers are all possible choices for the time domain.
Definition 1 A granularity is a mapping  from the integers (the index set) to subsets of the time domain  such that: (1) if  and    and   are nonempty, then each element of   is less than all ele and ments of   , and (2) if  and       are non-empty, then   is non-empty.
The first condition states that granules in a granularity do not overlap and that their index order is the same as their time domain order.
The second condition states that the subset of the index set that maps to non-empty subsets of the time domain is contiguous.
While the time domain can be discrete, dense, or continuous, a granularity defines a countable set of granules, each one identified by an integer.
The index set can thereby provide an aencodinga of the granularity in a computer.
The definition covers standard granularities like Days, Months, Weeks and Years, bounded granu-  larities like Years-since-2000, granularities with non-contiguous granules like Business-Days, and gap-granularities, i.e., granularities with non-convex intervals as granules like Business-Months.
As an example of the encoding, Years-since-2000  can be defined as a mapping  , with  mapped to the subset of thetime domain corresponding to the  year 2000,    to the one corresponding to the "!$#  year 2001, and so on, with   for " .
Independently from the integer encoding, there may be a atextual representationa of each non-empty granule, termed its label, that is used for input and output.
This representation is generally a string that is more descriptive than the granuleas index (e.g.,aAugust 1997a, a1/2/2000a, etc.)
.
Among the many relationships between time granularities (see [4]), the following defines an essential concept for this paper.
Definition 2 A granularity spect to a granularity  if %  is periodical with re-    there exists a (possibly infi1.
For each '& )!
( of the integers such that %  nite) subset  *,+.-0/   ; 32  54  2.
There exist 1 & , where 1 is less than the number of non-empty granules of % , such that for "!$* +.-0/  8!
all , if and 5& %    % 671 "!
2; # *;+.-0/  < then % 9:1 .
The first condition states that any non-empty granule %  is the union of some granules of  ;  %  for instance, assume is the union of the gran =?>@A  =6B3C@C@C =6DE ules    .
The periodicity property (condition 2 in the definition) ensures that the 1;F G  granule after %  = >  , i.e., , if non-empty, is % H1 2;3  = B 2;A@CC@C@  = D 2;      the union of  .
This results in a periodic apatterna of the composition of 1 granules of % in terms of granules of  .
The pattern repeats along2 the time domain2 by ashiftinga each granule of % by granules of  .
is also called the aperioda of % .
The condition on 1 enforces that at least one granule of % is a periodic repetition of another granule.
A granularity % which is periodical with respect to  C@CisC. specified by: (i) the 1 sets of indexes of  > (JI (LKNM describing the non-empty granules of 2 % within one period; (ii) the value of ; (iii) the indexes of first and last non-empty@granules in % , if their CC@C. > value is not infinite.
Then, if ( I  OP3@CC@(C. KNM are the sets  of indexes of  describing % , re% 1RQ spectively, then the description of an arbitrary granule    *  -0/   2     is given by1  1   .
fi	  Many common granularities are in this kind of relationship, for example, Years is periodical with respect to both Days and Months.
Business-Months is periodical with respect to Business-Days, which in turn is periodical with respect to Days.
Most practical problems seem to require only a granularity system containing a set of time granularities which are all periodical with respect to a basic granularity.
Usually Days, Hours, Seconds or Microseconds take this role, depending on the accuracy required in each application context.
In this paper, for simplicity, we assume there is a fixed basic granularity covering the whole time domain.
%    Definition 3 We say that a granularity  is periodical if it is periodical with respect to the basic granularity.
In Figure 1 we represent the whole set of granularities, according to Definition 1, partitioned in two main subsets: those having all granules with contiguous values (NO - GAP) and those admitting granules with non-contiguous values (GAP).
The inner circle identifies finite and periodical granularities: finite granularities are divided (dash line) into finite irregular and finite periodical2 while infinite periodical granularities are divided into those having a first non-empty granule and no last granule (INFINITE - R), those having a last non-empty granule and no first granule (INFINITE - L), and those infinite on both sides (INFINITE).
This classification will be useful when considering the expressive power of symbolic formalisms.
3.
Two approaches to symbolic representation In this section we first remind the syntax and semantics of collection and slice formalisms, and then analyze their expressiveness with respect to the class of periodical granularities.
3.1.
Collections and slices The temporal intervals collection formalism was proposed in [11].
A collection is a structured set of intervals where the order of the collection gives a measure of the structure depth: an order 1 collection is 1 This formula is correct provided that no granule of  is empty, but it can be easily adapted to the case with finite index for first and last non-empty granules.
2 Despite this formal distinction, finite granularities will be treated uniformly in the results.
GAP  NO-GAP FINITE  INFINITE-R INFINITE-L INFINITE  Figure 1.
A classification of time granularities.    )
an ordered list of intervals, and an order  ( collection is an ordered list of collections having order  Q .
Each interval denotes a set of contiguous moments of time.
For example, the collection of Months, where each month is represented as the collection of days in that month, is a collection of order 2.
In order to provide a user-friendly representation of collections, the authors introduce two classes of operators on collections and the notion of calendar, as a primitive collection.
A calendar is defined as an order 1 collection formed by an infinite number of meeting3 intervals which may start from a specific one.
The two classes of operators are called dicing and slicing.
A dicing operator allows to further divide each interval within a collection into another collection.
For example, Weeks:during:January1998 divides the interval corresponding to January1998 into the intervals corresponding to the weeks that are fully contained in that month.
Other dicing operators are allowed, adopting a subset of Allenas interval relations [1].
Slicing operators provide means of selecting intervals from collections.
For example, [1,-1]/Weeks:during:January1998 selects the first and last week from those identified by the dicing operator above.
In general, slicing can be done using a list of integers, as well as with the keyword the, which identifies the single interval of the collection (if it is single), and the keyword any, which gives nondeterministically one of the intervals.
Collection expressions can be arbitrarily composed using these two classes of operators starting from calendars, which are explicitly specified either by a periodic set of intervals, or as a grouping of intervals from previously defined 3 Interval    meets interval    if  starts when    finishes.
calendars.
The slice formalism was introduced in [14] as an alternative to the collection formalism in order to have an underlying evaluation procedure for the symbolic expressions.
It is based on the notions of calendar and slice.
Similarly to the collection formalism, calendars are periodic infinite sets of consecutive intervals, but there is no first nor last interval.
Intervals in a calendar are indexed by consecutive integers.
Once a basic calendar is given in terms of the time domain, other calendars can be defined dynamically from exist=fi L 	  > C@C@C  ing ones by the construct    which generates a new calendar with  intervals in > each period, the first one obtained grouping gran	 	   ules of calendar , starting , the second  B from grouping the successive granules, and so on,	with  >C@CC@  > treated 	 asB a 	 circular list.
A calendar is 	 B > a	 B subcalendar of ( ) if each interval of 	fiis> exactly covered by finite number of intervals of . Weeks, Days,  Months are calendars with 8 Days Months, Days Weeks, Weeks Months.
A slice is a symbolic expression built from calendars and denoting a (finite or infinite) set of not necessarily C	 >  consecutive intervals.
It has the form  where the sum identifies the starting points of the inter	 vals and  their duration.
Each is a symbol denot ing a calendar and is either a set of natural numbers  >C	fi> or the keyword all.
If the sum is simply 	 > , it denotes the starting points of the intervals of whose  > > of all inindex belongs to , or the starting points C	 M  > !
=fi  >   tervals if .
If !
"!$# % the sum is   C	  with it denotes the starting points   #   	 of the> -th interval of following each point in M >   C	  .
For example,   !
% !
% the sum all.Years + 2,4 .Months + 1 .Days denotes the set of points corresponding to the beginning of the first day  of February andC 	(April of each ' 	)' year.
The duration has the form & where 	('  	 is a symbol denoting a calendar such that 	*'  , and & is the number of successive intervals of specifying the !
% duration.
Hence, the slice all.Years + 2,4 .Months !
% + 1 .Days  2.Days denotes a set of intervals corresponding to the first 2 days of February and April of each year.
3.2.
Expressiveness and relationships Both collections and slices essentially characterize periodic sets.
Similarly to granularities, even in these formalisms there is the notion of a basic calendar, which defines the finest time units in the domain.
Without loss of generality, in the following of  the 	 paper we assume that this basic calendar (denoted by ) is the basic granularity we 	 mentioned in Seccan be associated tion 2.
A period, in terms of with each slice expression ( as well as with any collection expression + .
Intuitively, the period indicates 	 the number of instants of after which the same pattern of intervals denoted by the expression is repeating; each interval in a period can be obtained by a constant shift the corresponding interval in another pe	fi>of @CC@C@ 	 riod.
If are the calendars appearing in the  expression, then the is the least common 2 #$, 	 	,period  2 #$, 	 multi	, ple of   .
Technically,   , is   + >   fi =  # 	 , 	         defined as   /.
 , where /.
 =fi  #  	  	, is a list of integers, each one de	 noting the duration an interval in terms 	 !1  /.
 of =fi # 	 	,  of    of 32 &      ,0 ,        re3  2 &  returns the 45 element of the  , and   turns=fithe number of elements in the  .
For example, /.
   #  Years/Days !768:9<;689; 8:9<;6=8:9:9<> ,4 2 #$, "!
@?
9  and, hence,   Years/Days .
We now consider the expressiveness of slice expressions with respect to the formal notion of granularity introduced in Section 2.
If all the intervals denoted by a slice ( are disjoint, we call ( a disjoint slice.
We also say that a granularity  is equivalent to a slice ( , if each granule of  is formed by	 the union of a set of granules of the basic granularity ( ) and this set is represented by one of the intervals denoted by the slice; moreover, each of the intervals must describe one of these sets.
Theorem 1 Given a disjoint slice ( , there exists a nogap finite granularity, or a no-gap infinite periodical granularity  equivalent to ( .
!
>    C	  C  	)'   & Technically, if ( is an in   > !
=< finite slice ( !BA >  > ), we have an algorithm to derive DC@C@CC.A    DC% >  % Q the intervals    % Q ,    where % is the length in terms of the basic calen	 	*' dar corresponding to & granules of , starting at  .
These intervals are the ones denoted by ( within a slice period.
Then, a periodical granularity can  !
2 !
2  #$, 1   be defined by taking , , where 2  #$, 	 <!
 is> the slice period in terms of , and   	 L  !
C@C@C *FEHG 4I G M J  EKG  .
It is shown that for each   is equivalent to ( .
When ( is finite, the same algorithm can be easily adapted to derive all the intervals ( denotes.
Then, the equivalent granularity is simply defined explicitly mapping each granule to one of these intervals.
Disjointness ensures that the result of this mapping is indeed a granularity.
4 Ignoring  exceptions to leap years.
!
%  Example 1 Let ( =all.Weeks + 2,3# .Days .
  be  12.Hours be an infinite slice and % 2  #$, 9  is hours the basic calendar.
The slice (the number of hours in a week) and in the period containing !
Hours(1) A ;6=8:9$CA ? the 90O$C% slice denotes the set of intervals .
The periodical granularity  , equivalent to ( , is defined by tak!fi 1 ing (the number of * intervals in a Lperiod), J B	 2 !
2  #$,  !
  J   , Hours and  0"!$* J  I L  J  Hours .
  The following example shows that if a slice is non-disjoint, then there is no equivalent granularity.
!
%  Example 2 Let ( =all.Weeks + 2,3 .Days  3.Days.
According to the slice semantics, this expression denotes all intervals spanning from Tuesday to Thursday and all intervals from Wednesday through Friday.
By Definition 1, no pair of granules of the same granularity can overlap.
Hence, no granularity can be found which is equivalent to ( .
 To understand the expressiveness of the slice formalism with respect to granularities, we still need to check if any granularity in the identified classes is representable by a disjoint slice.
Theorem 2 Given a no-gap finite granularity or a no-gap infinite periodical granularity, there exists an equivalent slice.
The theorem states that any finite (periodical or not) granularity can be represented by a slice, and that the same holds for periodical granularities which are unbounded on both sides.
INFINITE - R and INFINITE L granularities cannot be represented by a slice, since the only way to denote an infinite set of intervals with  > !
=fi a slice is to have , and there is no way within the slice formalism to impose a minimum or a maximum on that set.5 From the above results we can conclude that disjoint slices can represent exactly the set of granularities identified in Figure 2, while non-disjoint ones do not represent granularities at all.
Unfortunately, it seems that there is no way to enforce disjointness by simple syntax restrictions.
We now consider the collection formalism.
GAP     NO-GAP                                                              FINITE                                      INFINITE-R INFINITE-L                                              INFINITE                                                          Disjoint slice expressions  Figure 2.
The subset of the granularities captured by the slice formalism  any two intervals equal or disjoint.
  and .
contained in +  are either  Proposition 1 follows from the semantics of the operators, and from the fact that each calendar contains only disjoint intervals.
Similarly to slices, we say that a granularity  is equivalent to a collection of  is formed by the union of the + , if each granule 	 represented by one of the intervals in granules of the collection; moreover, each interval in the collection describes the composition of one of the granules of  .
Theorem 3 Given a collection expression, there exists an equivalent no-gap periodical or finite non periodical granularity.
Similarly to Theorem 1, we developed an algorithm to parse the expression, to derive its period, the intervals it denotes within the period6 , and lower/upper bounds if present.
Once the intervals are derived, we have all the data that is needed to define the granularity  , since it will have the same period, the intervals within the period define the corresponding granules, and the lower/upper bounds are used to impose a starting/ending non-empty granule.
Proposition 1 Any collection + resulting from the application of a dicing or slicing operator is such that  Example 3 Consider !
% + = 1/Mondays:during:Years.
 .2000 .
This collection expression identifies an order 1  5 Note however, that the addition of a reference interval (bound) to each slice, as used in [2], provides an easy extension to capture all no-gap periodical granularities.
6 The intervals may be structured in a collection of order higher than 1, but this is irrelevant with respect to the time granules that the expression denotes.
collection that contains all first Mondays of each year starting since Monday, January 1st 2001.
We assume Days is the basic calendar with Days(1) = Saturday, Jan 1st 2000.
We first have to compute the expression period.
Since Mondays is defined as 1/Days:during:Weeks with the periods  of Days and Weeks equal to and respec    !
tively,  is the period computed for Mondays.
Similarly, since the period for Years @?
9  with respect to the basic calendar is (4 years in Days), the whole expression period is computed  @?
9  !
O  as  (28 years in Days).
2 !
O Then, is defined as having period ,  !
 1 (the number of granules in each period), ,!
8:9  E,!
8    Days Days  (1/1/2001),  !
O  (7/1/2002), ...,  Days (3/1/2028), !
'#  and   for   .
To obtain these intervals the algorithm first restricts years to those after 2000, then it represents all Mondays within those years, and in the end it extracts the intervals corresponding to the first Monday.
 We also have the counterpart of Theorem 3.
Theorem 4 Given a no-gap periodical or finite nonperiodical granularity, there exists an equivalent collection expression.
GAP   NO-GAP    fi fi fi fi      fi fi fi fi   FINITE      fi fi fi fi      fi fi fi fi   fi fi fi fi  INFINITE-R         fi fi fi fi      fi fi fi fi  INFINITE-L     fi fi fi fi      fi fi fi fi  INFINITE     fi fi fi fi      fi fi fi     fi fi fi   collection expressions  Figure 3.
The subset of the granularities captured by the collection formalism Note that in this case, all granularities in the right side of the inner circle of Figure 3 are captured.
We can conclude that slices and collections have incomparable expressiveness, since slices can represent sets of overlapping intervals, and collections can represent INFINITE - R and INFINITE - L periodical granularities.
From the above results, it is clearly possible to translate from one formalism to the other, when considering expressions denoting FINITE or INFINITE granularities, but it seems to be difficult to devise general rules to translate at the symbolic level, preserving the intuitiveness of the expression.
Indeed, despite the + operator in slices may be intuitively interpreted as equivalent to :during: in collections, they actually have a different semantics.
The collection formalism has been extended with some additional operators in [8].
In particular, control statements if-then-else and while are introduced to facilitate the representation of certain sets of intervals, as for example, the fourth Saturday of April if not an holiday, and the previous business-day otherwise.
Unfortunately, the syntax allows the user to define collections which contain overlapping intervals7 .
This implies that there are collection expressions in the extended formalism for which there does not exist an equivalent granularity.
4.
An extension proposal Both the collection and slice formalisms as well as their known extensions cannot represent gap granularities.
Indeed, this requires a non-convex interval representation for each granule which is formed by non-contiguous instants.
For example, they cannot represent Business-Months, where each granule is defined as the set of Business-Days within a month, and it is perceived as an indivisible unit.
We propose an extension to the collection formalism in order to capture the whole set of periodical granularities.
We introduce the notion of primitive collection, which includes calendars as defined in the collection formalism as well as order 1 collections of non-convex intervals, where each of the intervals represents a gran2 	 ule.
A primitive collection can be specified by 2 	 !
=fi L 	  2 	     @ I , where is a synchronization point with respect to an existing calendar 	 2 	  I , is the period expressed in terms of I , and is the set of non-convex intervals8 identifying the posi2 	 tion of granules within a period.
The synchro of 2 	 nization point says that will start at the same 	   instant as I .
7 For example, consider an expression representing a semester following the last day of the month, if it is a Sunday, otherwise the week following that day.
Considering 31/5/1998 and 30/6/1998, both the semester starting 1/6/1998 and the week starting 1/7/1998 will be denoted, with the first properly containing the second.
8 Each   is the non-convex interval representing the  -th granule.
Example 4 Suppose a company has 2 weekly working shifts for !
its employees: % and shift1= !
Monday, Wednesday, Saturday % shift2= Tuesday, Thursday, Friday .
It may be useful to consider these as two periodic granularities, where each shift is treated as a single time granule within a week.
If Thursday 1/1/1998 is taken as Days(1), shift1 = =fi ;   D!B6A EDCA 8 =8$CA 9 =9$C >%     Days .
Indeed, the synchronization point is 5, since the first granule of shift1 following Days(1) starts 	 on Monday January 5th 2 1998 which is 5 days later.
I is Days, is 7 days and is composed L > !
the6KA period  DCA 8 =8$CA 9 9 C> by which identifies the single granule within the period, formed by the first, third, and sixth day, starting from 5/1/1998, and repeating every 7D!Bdays.
= =fi 9  6A EDCSimilarly, A 8 =8$CA ?
?
C shift2 >%  Days denotes    the first, third and fourth day, starting from 6/1/1998, and repeating every 7 days.
 The user can specify collection expressions by arbitrarily applying dicing and slicing operators starting from primitive collections.
Since operators now apply to non-convex intervals, we need to revise  .
be non convex intheir definition.
Let and  !
6KA =?> >C@CC@C. A =  C> .
!
tervals, , and 6KA  > , > C@with CC@C@ A  ,,< C> !
$ !
L P = >    ( ; moreover, let! L C@C@C =  L %  >   > 4!
and ( , > C@CC     :",   % be the sets of values rep resented by and .
respectively.
Dicing operators are based on the following binary relations on non-convex intervals:9    fi         	            , .
 iff (  32 .
(    DH .
8!
  iff ( (   =  H  .
= >fi!
>@ iffH and   .
iff > !
    =?> ,     .
iff   > .
    iff  C operator  !
A > dicing $ @CC@C@= D %  C    #   ,             Example 5 Consider the collection expression Weeks:  :2/shift1:during:1998/Years where shift1 was defined in Example 4.
This expression denotes all weeks following the end of the second work-shift !PCC@C@6A EC C 8:of9<; 1998.
C > C@CC % Years is the order 1 collection A , C C and, 89;$C for simplicity, we assume the interval corresponds to year 1998.
Then,6KA  the slicing 1998/Years C C 89;$C> returns the interval , and the dicing shift1:during:1998/Years returns the finite collection of order all the work-shifts !
6KA ;6 ; 1C composed A  C A O by O$C > 6KA 8;;6=8;:;$C during 1998: , , , ..., , A 8<; 8; C A 8:9EO 89EO$C >% , .
The selection of the second of those the non-convex inter6KA    work-shifts  CA  ?
 ?
:C Areturns   C > val .
the dicing 6KA     CA  ?
 ?
:C A  Finally,  C> Weeks:>: generates the order 1  collection !Bof after 6A  all    8$the C >3 6Kweeks A  9 8EO that C>A@CC@start C% January -th, i.e., .
 We state a formal property of the proposed extension.
Theorem 5 The extended collection formalism can represent all and only the granularities which are either periodical or finite non-periodical.
To support this result, the algorithm used in the proof of Theorem 3 has been extended to consider nonconvex intervals.
The granularities captured by the proposed extension are shown in Figure 4.    takes an order 1 collection as its left operand and an interval .
as its right operand, and it returns an order 1 !"!
 !
 !
C@CC  $collection  .
% + for some   and .
If !
$ !
 $ .
3<!
the strict form is used, then +  !R CC@C    .
% for some , i.e., only the por  and  tion of which is contained in .
is part of the resulting    collection.
When the right operand is a collection, instead of a single interval, the same procedure is applied for each of its intervals, resulting in a collection of one order higher.
A slicing operator  + replaces each order 1 collection contained in + with!
the  -th non> CC@C % convex interval in that collection, while    +  replaces it with the collection made of the subset of intervals position in the collection is specified by !
> C@CC whose %   .
  9 This set of relations is similar to the one chosen in [11] for convex intervals.
We consider it only as a good basic set which allows the representation of most common granularities while having a simple implementation.
It can be extended to a richer set considering, for example, the taxonomy of relations given in [10].
5.
Conclusions In this paper we have considered a recently proposed theoretical framework for time granularities and we have analyzed two of the most influential proposals for calendar symbolic representation.
On one side, we have shown that the theoretical framework is general enough to capture all the sets of disjoint intervals representable by those formalisms.
On the other side we have shown exactly which subclass of granularities can be represented by each formalism.
From this                            GAP  NO-GAP  FINITE  INFINITE-R INFINITE-L  cation to Temporal Reasoning, Annals of Mathematics and Artificial Intelligence, 22(1,2):29a58, Baltzer Science Publ., 1998.
A preliminary version appeared in Proc.
of TIME96.
[5] F. Casati, B. Pernici, G. Pozzi, G. Sanchez, J. Vonk, Conceptual workflow model, in book Database support for workflow management: the WIDE project, chap.3, Kluwer, in press.
INFINITE  fififi fififi fififi fififi  extended collection expressions  Figure 4.
The subset of granularities captured by the proposed extension  analysis, we have proposed an extension of the collection formalism which captures a well-defined and large class of granularities, providing a good coverage of granularities that may be found in database and temporal reasoning applications.
We are currently working at the definition and implementation of set operations, performed at the symbolic level, among extended collection expressions.
This problem has interesting applications (see e.g., [2]) but it is not addressed in [11] and derivative work for collections, and only briefly investigated in [14] for slices.
References [1] James F. Allen, Maintaining Knowledge about Temporal Intervals, Communications of the ACM, 26(11):832a843, 1983.
[2] Elisa Bertino, Claudio Bettini, Elena Ferrari and Pierangela Samarati, An Access Control Model Supporting Periodicity Constraints and Temporal Reasoning, ACM Transactions on Database Systems, 23(3), 1998.
[3] C. Bettini, C.E.
Dyreson, W.S.
Evans, R.T. Snodgrass, X.S.
Wang, A glossary of time granularity concepts, in book: Temporal Databases: Research and Practice, O. Etzion, S. Jajodia, S. Sripada (Eds.
), LNCS State-of-the-art Survey 1399, pp.
406-413, Springer, 1998.
[4] C. Bettini, X. Wang, and S. Jajodia, A General Framework for Time Granularity and its Appli-  [6] D. Cukierman, J. Delgrande, Expressing Time Intervals and Repetition within a Formalization of Calendars, Computational Intelligence, 14(4):563a597, 1998.
[7] I.A.Goralwalla, Y.Leontiev, M.T.Ozsu, D.Szafron, C.Combi, Temporal Granularity for Unanchored Temporal Data, in Proc.
of CIKM, pp.
414-423, ACM Press, 1998.
[8] R. Chandra, A. Segev, and M. Stonebraker, Implementing calendars and temporal rules in next generation databases, in Proc.
of ICDE, pp.
264a 273, 1994.
[9] J. Koomen, Reasoning about recurrence, Int.
Journal of Intelligent Systems, 6:461a496, 1991.
[10] P. Ladkin, Time representation: a taxonomy of interval relations, in Proc.
of AAAI Int.
Conf., pp.
354a359, 1986.
[11] B. Leban, D. Mcdonald, and D. Foster, A representation for collections of temporal intervals, in Proc.
of AAAI, pp.
367a371, 1986.
[12] A. Montanari.
Metric and Layered Temporal Logic for Time Granularity, ILLC Dissertation Series 1996-02, Institute for Logic, Language and Computation, University of Amsterdam, 1996.
[13] R. Morris, W. Shoaff, and L. Khatib, Domainindependent temporal reasoning with recurring events, Computational Intelligence, 12(3):450a 477, 1996.
[14] M. Niezette and J. Stevenne, An efficient symbolic representation of periodic time, in Proc.
of CIKM, Baltimore, pp.
161a168, 1992.
[15] P. Terenziani, Reasoning About Periodic Events, in Proc.
of 2nd International Workshop on Temporal Representation and Reasoning (TIME), Melbourne Beach, Florida, 1995.
[16] The TSQL2 Temporal Query Language, R. T. Snodgrass ed., Kluwer Academic Pub., 1995.
A  Syntax  In the following, we summarize the syntax of the sym	 bolic formalism.
Note that is the basic calendar in the considered system, and text variables (e.g., day, month, ...) are often used in symbolic expres	 sions to denote the calendars (PC) generated from .
E  :==  6  >      interval-list PC > B + .rel.=fi+  =sel/E 	  2 	  :==  =  H $,  .
 3 :==  2 H     PC rel                +  >  :rel: +      D H      B    interval-list :== interval interval,interval-list interval :== [integer,integer] sel :== integer [integer .
.
.
integer] !
% integer-list integer-list :== integer integer,integer-list    !
'   L  for begin !L   !
to     2    #$,fi  for every element in .
.
.
do    B  do  do for every element in  begin , 	 !
O ;	   !
for each ( to Q ) do begin              !
             ,   , LN receives as input two calThe function 	 	 > > > endars ( and 4 ) and two integer numbers (  and # 4 # 4 ) and it computes the index of the -th interval 	 > of 4 which	 comes after the starting point of the  th interval of  .
fi   fi  returns the index   of	 the granule The	 function B > of which contains the   -th granule of if one exists, and indefinite otherwise.
Each point  identified by the algorithm is the beC	*' ginning of an interval with duration & 	 .
However, in order to derive intervals in terms 	 of we need to  is contranslate this duration in terms 	 ' of .
If tained in the  -th granule of , then the new duration % is equal to  Q 	  where  is the maximum  index of a granule of contained in the  "& 	(' .
CThis allows to derive the intervals th of> !BA  granule C@CC.A     C % >   > $% Q $% Q denoted    by the slice ( in one period.
The intervals in other peA 2  #$,     32 @  #$,     riods are simply given by   C :  2 #$,  %  Q where	   ,   is the  slice period in terms of , and is an integer.
Then, by > taking a periodical granularity  can be defined !
2 !
2 #$,  !
L  * E G 4I G M J  EHG 1  , !
' @CC@C.  , and   for each   .
It is easily seen that  is indeed a granularity, since the disjointness of the slice guar> antees that granules of  do not overlap.
4 By definition  !
 * E G I G M  EHG J    J of periodicalgranularity, if C@C@C@ > 8!
# for some  EHG  EHG 4 > I G M and    1 , then "!* E G 4I G M 	 2;  J J EHG  9:1   .
 Suppose now, by contradiction, that there exists within an interval denoted by ( but is not the in	 dex of a granule of contained in any granule of A2      32        %   Q  C , for  , i.e., &   !
@CC@C. % 	   but some integer  , and   & is not included in   for any  .
Then, since ( is periodical,  2    A     DC    %  Q , and by constructionof Q & "!	  CC@CF	       %  Q  granularity  ,       H!
2  .
Then, by the periodicity of  ,        Proofs    !
end; end; end;  Proof of TheoremC 1.
 C	 ' 	 >  & and be the basic calLet ( be     > !
=fi endar.
We first consider an infinite slice ( ), and we illustrate an algorithm to derive the points !
 >C@C@C  % , where each  represents the starting 	  point of the  -th interval denoted by ( in terms of .
The algorithm works as follows: As the first step  C	 we add a  a as the last element in the sum describing the slice ( .
(This is a technical trick to obtain the 	 starting points in terms of .)
The main loop consid	 > ers each granule of within a slice period, starting 	 >  from .
This avoids checking an infinite number of granules.
Then, we consider each one of the possible combinations	 of the offsets, and we compute the index in terms of of the single granule denoted by the corresponding slice.
C	 2  #$, >  ; INPUT:  !
  > @CC@C. ,%   OUTPUT:  ;  METHOD:   >    ,   , L 	   # 4  	 4 > !
if (  indefinite) then begin , 	 !
  ; break; end; end; , !
O if (  	 ) then begin A C !
    B                  	        C@CC  2             	    Q .
Hence is     contradicting contained in  the hypothesis.
!6E@C@CCA %    	  s.t.
t   & We can that   & A 2  (  conclude 3   2 (    D  fi C       %  Q      .
On the other side, suppose there exists 	    s.t.
is contained in , for some integer , but      & A 2    C   32     % for any integer and  Q      1 .
Since  	 is periodical, 2    there must exist an   is contained in    integer   such that where   1 .
By construction of granularity  , if "!
  <C@CC 	      %  Q , then the interval  A     DC   %  Q is one of those denoted by ( .
Hence,  2    by   belongs to an interval denoted ( .
However, 2  since ( is periodical with period , also must be in one of these contradicts  intervals.
!6E@CC@CAthis  %  	 the  hypothesis.
 i    &  s.t.
t $  Hence,  A 2 (  2  ( DC      :%  Q .
&   When ( is finite, the same algorithm can be easily adapted to derive all the intervals ( denotes.
Then, the equivalent granularity is simply defined explicitly mapping each granule to one of these intervals.
Disjointness of the slice ( ensures that the result of this mapping is indeed a granularity.
In the finite case the equivalence between  and ( is trivial.
  %      Proof of Theorem 2.
Let  be a no-gap infinite periodical granularity.
By Definition 3,  is periodical wrt the basic granularity 	 which we identify with the basic 2 	 calendar .
Then, has a period in terms of , 1 granules within  each period, and an explicit description of these C@CC@ > (LI ( KNM granules within a period by sets of 	 indexes of .
Since the granularity has noA = gaps,  C each set !
canO C@be represented by	 =< an interval   CC  with   .
Let be the calendar 	1 Q generated from including these intervals as well as the largest intervals that can be added to these ones order to cover the whole time line.
Formally, 	 =fi in !
=fi  =  	  =   = >  C@CC.  3=  D 2       =6D  Q    I  >    = D   4  >  D Q  I  Q  Q  I  E@CC@C.  KNM  >  Q  Q  =  I  KNM  >  Q    , where the calendar constructor was defined in Section 3.1.
In this case = 2 I is used as a synchronization point, and is=fi the  period of  .
(If one of the values in the   @ 	 =fi O expression evaluates to it is ignored.)
Let be a calendar such that interval=<starts where a 	 =fi each !
= =	 3= >  I granule of starts:    Q = C@C@C = D = D C@C@C@ = 2 = > >  4 I Q I  Q KNM .
We now show =fi that to the C 	 =fi   is  equivalent C	 =fi slice expression .
First, if !
=6 CC@C  	  A=  C , then must be   =fi C	 =fi  EC 	 =fi .
By construction, 	 =fi an interval in = contains an interval starting on .
Indeed, this is trivial   I      =fi  Q    KNM  Q                if we  1 , otherwise, by the periodicity of      know that there exists containing as first granule 	  = 2     !R     1  with    MOD 1  .
Then, by = 2   construction,   1 is the starting point of an 	 =< 	 =fi interval2 in= and, since and  have the same period , must also be a starting point of 	 =fi 	 =fian  interval A=  C of .
Moreover, by construction of , is 	 =< 	 =< also one of the intervals in , since contains all intervals denoted by  plus the newly inserted 	 =< ones.
Then, according to the 	 the slice semantics, gives =fi the starting points and the duration, implying that A=  C is among the intervals denoted by the slice.
        It is easily seen, by the construction of 	 fi =   	 < =   and   , that the vice-versa also holds, i.e., for each in-  terval [r,s] by the slice, there exists  such that "!
denoted   7C@CC  	  .
  	 =fi 	 If=fithe  granularity is finite (periodical or not), and are generated explicitly listing all the intervals.
The equivalence proof is analogous.
  Proof of Proposition 1.
The proposition follows from the semantics of the opC C erators.
Indeed, the expression + $ + , apart from possibly structuring the intervals into a collection of higher order, only selects some of the intervals in + .
Hence, for each interval in + the collection we derive is a subset of that in + .
Since + has only disjoint intervals, its subsets are also disjoint.
However, there may be some interval .
in + satisfying relation C $  C with more than one interval in + .
This leads to the multiple presence of .
in the resulting collection.
Strict operators (  ) select common subintervals of intervals in + and + , and there is no way of generating overlapping intervals if there are none in + and + .
The other type of operators (slicing) simply select some intervals from a collection + based on their index.
Since we assumed that collection expression are built from calendars recursively applying only slicing and dicing operators, and no calendar has overlapping intervals, we can conclude that the derived collections only contain equal or disjoint intervals.
              Proof of Theorem 3.
The proof is based on an algorithm to compute the intervals denoted by a collection expression within its period if it is infinite, or all the intervals if it is finite.
Clearly, the algorithm is trivial when the collection is explicitly given as a set of intervals; Otherwise, for each dicing and slicing operator allowed in the expression, the algorithm has to generate the intervals resulting from its application.
The algorithm represents each infinite collection of order 1 by a finite set of intervals  within one of the periods, by the period value, and possibly by lower and upper bounds identified by values =fiL    and  .
The choice of the beginning of the period is induced either by the calendar definition (if the order-1 collection is a defined calendar) or by the specific operations that have been applied.
The algorithm evaluates the collection expression from right to left.
When the algorithm applies dicing to a collection, its order is increased by 1, while if it applies dicing to an interval it becomes a collection of order 1.
On the other side, if the algorithm applies slicing (with a single slicing value) to a collection, its order is decreased by 1, resulting in a single interval if the collection had order 1.
If the algorithm applies a  a or a  a dicing to a finite collection expression it returns a lower or upper bounded collection expression, while for other operators a finite collection is returned.
If the algorithm generates a bounded collection expression,=<L it stores the minimum value    or the maximum  bounding the collection.
At the end of the algorithm, if we have 2 !
.
 ,    , , then the collection expression is finite, otherwise the collection is periodic.=fiIfL   is defined, then it is lower bounded and if  is defined, it is upper bounded.
The algorithm to calculate the intervals identified by a collection expression uses a set of procedures implementing each slicing and dicing operator.
Here, for the sake of brevity, we only illustrateC # the  =case  EC of the procedure!
that implements the < C # fi@ =@3EC  operator.
Let + be the collection expression identifying the intervals of overlapping    those of , where and are collection expressions themselves.
thatC the intervals denoted by  are A >, >CAssume @C@CC@ A  =, .
Then, for each interval A  =, C !
C C@C   , we execute the following with  code, where  denotes the granularity identifying  	 the same granules (intervals)  = fi as the collection , is the basic calendar, and  	 fi is a function returning the indexes of granules of (an interval) forming the = -th granule of   .
A  , C INPUT: !BA  >  ;> C@CC@C. A    C %   OUTPUT: ; # allA  the  =, C intervals of which overlap METHOD: !
' ;  fi  + = !
fi ;  A  +   + C !
= + fi  fi , ;,  + while   do begin !
   + =         !
  A+   + C   +      !
M  >  =   +       fi  fi fi  fi  end; Details of the algorithm are reported in the extended version of this paper.
Once we have derived the intervals denoted by a collection expression, we still need to find an equivalent granularity.
We first consider an arbitrary collection expression + denoting an infinite number of intervals.
Suppose that, by the global algorithm above, !BA >  described  C% we obtain the set of intervals > CC@CC.A identified by2 + within one of   the collectionas =<periods, the period and, if they exL .
Then, we can define the periodiist,    and  2 cal granularity  as having period equal to that of the collection, 1 equal  to!
 	(the number of intervals   C@CC 	  in !
the period), and for   CC@C   .
If the collection is bounded =fiby a miniL one  in terms mum 	 value   and/or a maximum !
#  of , then we impose   	 for each   that would contain granules=fiL of with indexes lower than .
This can be checked by the   or greater than  formula given in Section 2 to derive granules of periodical granularities.
If + is finite not periodical, we can similarly apply the algorithm, this time deriving all the intervals denoted by + .
Then, each interval defines explicitly a granule of  .
It is easily seen that, if the algorithm computing the intervals denoted by + is correct, the above construction leads to a granularity equivalent to + .
While this is trivial in the finite non-periodical case, the periodical case must be considered in detail.
Each granule is the union of a set of 	 granules of specified by one of the intervals denoted by + , since they have been explicitly defined this way within one period, and the period length has been taken equal for + and  .
We show that for each interval A  C there is a corresponding granule.
Suppose is an interval denoted by + , and there is no  such that "!
 C@CC 	  .
+ must denote more   A  C intervals than those contained in a period, and must not have been in the period considered by the algorithm, otherwise   would have been explicitly A D0 deD@C fined that way.
Then, there must be an interval among the 1 intervals !returned byD the algorithm,   2 !
  2 and D an integer such that and ,   2 where is the period.
Then, by the property of  be	 5!
ing periodical 	 wrt  2   , we know  that for each   ,   * -0/   1  unless   has been  fi   !   explicitlyHdefined as empty.
Consider 1 ; .
Ei!* 	  2      H!
# -	     ther   , or .
      The first would contradict the assumption that there  !
  C@CC 	  is no  such that   .
The second, by the construction of  , can only be true if    , as given by the formula above, would contain  	  =fiL    with 7   or      .
Both cases lead  to a contradiction, since , and we assumed A  C to be one of the intervals denoted by + , that is =fiL      .
Hence, we can conclude that  is equivalent to + .
  Proof of Theorem 4.
Let  be the granularity considered in the theorem.
Following the first part of proof 	 the =< 	 =fiof Theorem 2, we and .
If  is finite construct the calendars non-periodical or INFINITE periodical, 	 =<C it  =  isHEeasily C 	 =fi seen that the collection expression C =  HEC is equivalent to  , where A LL <CCis  =  defined HECA C C on generic intervals as follows: if L !
 and .
A slightly more elaborate C H C construction makes use of the   relation C  =  HEC defined in [11] instead of .
FINITE, INFINITE R, and INFINITE - L periodical granularities require an additional term in their equivalent collection expression.
If the definition  !
A of  C specifies a first     , then we define non-empty granule 	 =<C  =  HEC 	 =fi AC C 	 + as   Q !
 A .
 Similarly, C for aC filast non-empty granule   , using C  	    .
If the definition of   specifies !
A  aC first and a =filast non-empty granule       L  !
A  C   + and , then we define as  2 	 C =  HEC 2 	  , A C C C 	 .
  32   .
               Proof of Theorem 5.
Introducing non-convex intervals as granules, Theorems 3 and 4 are extended with analogous proof techniques to show that the gap counterpart of all the periodical granularities is captured by the extended formalism.
Regarding the extension of Theorem 3, the only relevant part that needs to be modified is the algorithm.
In particular, the procedures implementing dicing operators must follow the new semantics when applied to non-convex intervals.
Details of the algorithm are reported in the extended version of this paper.
Depending on the expression being periodical or not, all the non-convex intervals or only those in a period are derived, each one defining a granule of the gap granularity  .
Then  is easily shown to be equivalent to the extended collection expression.
Let us consider now the extension of Theorem 4.
The finite non-periodical case is trivial, hence we consider here the periodcal case.
Let  be a gap granularity 2 with a period , 1 granules@C(with gaps) within each C@C. > (LI (LKNM period described by sets of indexes of 	 the basic granularity .
Each set can!
beOfirepresented C@C@C  by a non-convex interval  with   .
1 Q If  has no first nor last 2 granule, then we can de	 fine a primitive collection equivalent to  .
Let      6KA =    CC@C@C@A =    C> 2 	 above be .
Then, is I =fi I  = =	 32     I given by   @ , where contains I L C@C@C@L > 1 non-convex intervals I KNM , where each inL A = + = I    = I  C terval in!
 O is given by ! Q for I  I Q I  CC@C O CC@C  each    and   1 Q .
Essentially, we just transformed the granularity description in the equivalent notation for primitive collections.
Similarly to Theorem 4 FINITE, INFINITE - R, and INFINITE - L periodical gap-granularities are accommodated by using C C C C , the operators a  a, a  a, and a .
  32 a.
  
Temporal Query Languages Expressive Power: TL vs. T- WHILE Nicole Bidoit, Matthieu Objois Laboratoire de Recherche en Informatique L.R.I.
UMR 8623 CNRS Batiment 490 - UniversitAS Paris Sud 91405 Orsay Cedex - FRANCE {bidoit,objois}@lri.fr  Abstract We investigate the expressive power of implicit temporal query languages.
The initial motivation was redZning the results of [1] and enrich them with comparison to TL [17].
Thus, we address two classes of temporal query languages: TL-like languages based on TL and T- WHILE-like languages based on WHILE.
We provide a two-level hierarchy (w.r.t.
expressive power) for these temporal query languages.
One of the contributions solves an open problem: the relative expressivity of TL and T- FIXPOINT [1, 11].
1  Introduction  In this paper we focus our attention on temporal databases and temporal query languages.
Two different representations of time are classically considered: the implicit framework dedZnes a temporal database as a dZnite sequence of instances; in the explicit framework, tuples in relations are timestamped.
It is now well-known that as far as representing time is concerned, the implicit and explicit representations of time are equally expressive.
However, as far as temporal query languages are concerned, the choice between implicit and explicit frameworks leads to different languages w.r.t.
expressive power.
With implicit time, the linear temporal logic TL and its extensions are the basic formalisms underlying query languages [8, 7].
When time is explicitly represented, queries are specidZed using the language TS - FO (or extensions) which is simply the relational calculus FO with timestamps.
As pointed out in [7], temporal logic and their extensions are especially attractive as query languages for temporal databases because of their simplicity and computational advantages.
Indeed, because the references to time are hidden, queries are formulated in an abstract, representationindependent way.
As far as computational complexity is concerned, [15] shows that the satisdZability problem for propositional TL is PSPACE-complete (whereas the same  problem for dZrst-order theory is non elementary [13]).
The work of [1] (see also [5]) gives new insight to the relative expressive power of TL and TS - FO as query languages: surprisingly enough1, dZrst order TL is strictly less expressive than TS - FO.
Subsequent research work [1, 11, 4] investigate query languages more expressive than TL.
More recently, [16] has proved (based on the techniques of [5]) that TS - FO is strictly more expressive than any dZxed-dimensional dZrstorder temporal logic.
This paper focuses on the implicit languages introduced in [1] where the authors investigate a classical way to extend FO in order to build implicit temporal languages.
These languages rely on traditional imperative mecanisms namely assignements, iterations (while statements) and temporal moves.
These languages further denoted T- WHILE-like languages are also extensions of WHILE introduced in [6] to capture recursive queries in the static framework.
In this paper, we address another class of temporal query languages called TL-like languages.
These languages can be viewed as extensions of TL [17, 11] or alternatively as extensions of FO with dZxpoint formulas [14, 6, 9, 10, 2].
[1] provides an expressive power hierarchy of temporal languages including TL, TS - FO, T- WHILE-like languages and their explicit counterparts.
Surprisingly enough [1, 11] do not address TL-like languages in their expressive power comparative study.
The work presented in this paper aims at contributing to the following open problem pointed out in [7]: aFixpoint temporal logic TL has been extensively used in program veridZcation, although only in the propositional case.
The dZrst order version of TL remains to be studied.
In particular, its relationship to T- WHILE (...) needs to be elucidated.a One of the main contributions of our paper is to enrich the expressive power hierarchy of [1] showing that the dZrst order indZationary extension of TL is strictly less expressive than T- FIXPOINT 2 which is roughly the indZationary TWHILE -like language with some non-indZationary features.
1 This 2 In  result stands in contrast with the propositional case.
the paper, this language is renamed T- WHILE 	.
Proceedings of the 12th International Symposium on Temporal Representation and Reasoning (TIMEa05) 1530-1311/05 $20.00 AS 2005 IEEE  We also show that T- FIXPOINT is equivalent to the non indZationary T- WHILE which entails that T- FIXPOINT is not PTIME as claimed in [1].
The expressive power results presented in this paper shed new light on the impact of the features of the two classes of languages.
In the static case, each result holding for a WHILE -like language also holds for its FO dZxpoint counterpart.
Surprisingly enough, this symmetry is broken in the temporal framework: for instance, the indZationary TWHILE -like language with some non-indZationary features (already mentionned above as T- FIXPOINT) is strictly more expressive than its TL-like counterpart.
Another interesting impact of our results is that they highlight the frontier between querying static databases and querying temporal databases.
More precisely, part of our results relies on the assumption that the temporal instances under consideration are composed of at least two states as opposed to static ones composed of a single state.
The paper is organized as follows.
Section 2 is devoted to preliminaries and section 3 to the presentation of the implicit temporal languages whose comparative expressive power are investigated in section 4.
We conclude the paper in section 5 by discussing our results and addressing some open problems.
2  Preliminaries  We assume the reader familiar with both dZrst-order logic and temporal logic TL and with the usual dedZnitions of relation schema, database schema and instances.
We denote the arity of a relation schema  by  .
In the whole paper, we consider the database schemas  A"  fififi  and  A" fififi  and we assume a unique domain .
An implicit temporal instance over , is a dZnite sequence 	A"  fififi 	 of (dZnite) instances over .
For each    fi, 	 is called the state of at time point  .
The instance of the relation schema  at time point   is denoted by  fi.
The active domain of , denoted  , is the union of the active domains of the states 	 .
The well known aunsafe querya problem is solved here by restricting variables to range over the active domain of the input temporal database.
In the paper,  	A"  fififi 	 is a temporal instance of size  over ,  represents an unspecidZed tuple of variables whose arity is clear from the context, and  is a valuation of  ranging over  .
Let   A"  fififi  be a temporal instance of size  over  .
Merging the the database schema where instances and  leads to the instance denoted  over the , dedZned as A"  fififi  where for all database schema    fi: 1) for all   fi,  fi   	 fi  and 2) for all   fi,  fi    fi .
whose We denote A any temporal instance over states are all empty.
FO          fi  fi  fi    fi        3  Temporal query languages  In this section, we revisit the specidZcation of some of the languages introduced in [17, 1, 11] in order to better identify their features, and we also introduce new languages.
We proceed by introducing two classes of implicit temporal query languages, each one sharing the same paradigm: TL -like languages presented in section 3.1 are both extensions of TL and of FO dZxpoint and T- WHILE-like languages presented in section 3.2 are temporal extensions of WHILE.
Languages in a class differ by some features (e.g.
indZationism vs. non indZationism) leading to alternative query languages.
We now proceed to the presentation of the languages.
The main expressivity results are developed and discussed in section 4.
3.1  TL -like  languages  All query languages presented in this section are based on the TL language, introduced in the propositional case by Vardi in [17] and extended to the dZrst order case by Herr in [11].
These languages can also be seen as temporal extensions of the static query language FO dZxpoint, which has a long history: it has dZrst been considered over indZnite stuctures in [14], and afterward studied in the dZnite case (which is relevant to the database context) in [6, 9, 10, 12].
Syntax The syntax of TL-like languages over the is obtained by the formation rules for database schema FO , together with the following additional rules: if  is a formula then    and  are formulas.
suppose that  .
We call auxiliary schema any schema in .
We dedZne a (simultaneous) inductive operator for the auxiliary schemas.
Let A"  fififi  be formulas 3 .
Each formula fi has as over the database schema many free variables as the arity of  .
For all   fi, the -expression A"  fififi  fiAVA"  fififi  fi  can be used like a relation schema to build a formula A"  fififi  fiAVA"  fififi  fi .
The schemas A"  fififi  (resp.
AVA"  fififi  ) are called the checked (resp.
unchecked) auxiliary schemas of the -expression.
Note that -expressions can be nested.
Whether an auxilliary schema is checked or unchecked has an impact on the semantics of the language (see below).
Intuitively, the former (resp.
the latter) enforces the instance of the auxilliary schema to be used (resp.
ignored) when computing the semantics of a -expression.
          fi  E XAMPLE 3.1 Let  (resp.
A" , Az ) be a binary relation schema in (resp. )
and consider the formulas 4 : AA" AAz       fi 	        A"    	  ,      A"     	     Az  .
A"   A and A"   A can appear in the formula Afi for all   fi .
4 fi (resp.
 ) stands for fi  (resp.
fi ).
3 Both  Proceedings of the 12th International Symposium on Temporal Representation and Reasoning (TIMEa05) 1530-1311/05 $20.00 AS 2005 IEEE  The formula   fi     fi   is referred to as  fi   in the rest of the section.
Intuitively,  fi   computes a aback-and-forth temporal patha along the temporal instance, beginning with the value 	 (in the dZrst state).
Semantics The satisfaction of a formula   over at time point  fifi  given a valuation  , denoted fi fi      is dedZned as follows:  if   is obtained by a dZrst-order rule, fi fi      is dedZned as usual  if   is , fi fi      iff   fi and fi   fifi      if   is , fi fi      iff    and fi   fifi      if   is    fi fi      fi fi    , we dZrst need some preliminaries: let  A be the sequence of temporal instances of size  over fi dedZned by:   is the empty temporal instance    is the temporal instance dedZned for each time point  fifi  and each auxiliary schema  for  fifi  by:         fi fi     fi  a valuation.
Intuitively,     is the answer of the query   evaluated over   at time point  .
Next, the sequence fi   A is the one obtained from  A by projection over the checked auxiliary schemas 5  fi fi  .
Suppose that fi   A reaches a repetition , fi .
Intuand consider the least   such that fi     itively, the unchecked auxiliary relations are ahiddena when checking for a repetition.
Thus when   is   fi fi      fi fi    , we have fi fi      iff     , that is iff the tuple   belongs to the relation over  of the temporal instance  at time point .
If fi   A never reaches a repetition, we say that the semantics of both the -expression and the formula it appears in are undedZned.
E XAMPLE 3.2 The table below illustrates the above semantics using the  formula of example 3.1.
It gives a temporal instance for  and the sequence  A .
t  A A A A Afi A A A A    1    fi	   fi	         fi	    fi	    fi	    fi	    fi	    fi	   fi	   fi	   fi	       2  fi 	 fi fi	      fi	 fi  fi	 fi  fi	 fi   fi	 fi fi	  fi	 fi fi	  fi	 fi fi	         3  fifi	   fi	  fi	  fi	  fi	  fi	          fi	  fi	  fi	  fi	      The above computation reaches a repetition at iteration 6, because   .
Thus the evaluation of the instance over auxiliary schema  (resp.
 ) is   (resp.
 ); 5 We say that  	 Azareaches a repetitiona iff there exists 	  A" .
a sequence   Az  such that 	    hence the evaluation of  over the above temporal instance is    	fi fi 	fi fi 	fi .
Let   be obtained from  by declaring  unchecked:   fi          fi  .
The computation of   A is the same for both  and  , but such is not the fi case for the sequence   A .
Indeed, fi   A reaches a  A only contains the inrepetition at iteration 3, since fi  stance over the auxiliary schema  , and    	 .
Thus the evaluation of   over the above temporal instance is    fi fi .
Query languages We dedZne queries based on the formulas dedZned above exactly like for TL.
A query  is specidZed by a formula  and the answer of  over is obtained by evaluating  at time point 1:         fi fifi    fi  a valuation  Next, we use the notion of indZationary formulas.
A formula  fi is indZationary if, considering the sequence  A dedZned above,          for all  fifi .
Hence an indZationary formula is intuitively a acumulativea one.
We now dedZne four TL-like query languages based on the previous syntax and semantics.
1.
TL denotes the language without unchecked auxiliary schemas and where all formulas of -expressions are indZationary.
2.
TL denotes the language without unchecked auxiliary schemas and with arbitrary formulas.
3.
TL  denotes the language where auxiliary schemas can be checked or unchecked, and where all formulas of -expressions adedZninga some checked relations are indZationary.
4.
TL denotes the language where auxiliary schemas can be checked or unchecked, and with arbitrary formulas.
Note that, in all four languages, -expressions must contain obviously at least one auxiliary schema and moreover at least one auxiliary schema must be checked.
Repetition vs. limit The semantics of -expressions is dedZned using the repetition of the checked sequence fi   A .
Another classical way to dedZne such a seman tics is by using the limit of the sequence.
These two definitions lead to different semantics for TL  and TL , and they lead to equivalent query languages for TL and  TL.
Indeed, TL  repetition and TL limit (resp.
TL repetition and TL limit) are probably not equivalent query languages.
Hence, we beleive that only TL and TL would deserve to be called AT dZxpoint query languages At.
It is easy to show that in the static case, the repetition and limit dedZnitions lead to equivalent query languages for both the indZationary and non indZationary FOdZxpoint.
Proceedings of the 12th International Symposium on Temporal Representation and Reasoning (TIMEa05) 1530-1311/05 $20.00 AS 2005 IEEE  Partial query languages There exists formulas belonging to TL and TL	 whose semantics is undedZned, but every TL AV and TL AV 	 formula admits a dedZned semantics.
The facts that on the one hand TL and TL	 are partial languages and on the other hand TLAV and TLAV 	 are total languages generalize the static case: recall that non indZationary FO dZxpoint is a partial query language and indZationary FO dZxpoint is a total query language.
3.2  T- WHILE -like  languages  All query languages presented in this section are based on the T- WHILE language introduced in [1].
T- WHILE is a temporal extension of the well-known WHILE query language for static databases [6] Syntax We assume w.l.o.g.
that the database schema includes (implicitly) two 0-ary schemas fi and fi 6 .
A program over is specidZed by a sequence of declarations followed by a sequence of instructions.
Declarations enable to specify some new relation schemas for programs, i.e.
schemas not in , called auxiliary schemas.
An auxiliary schema can be declared: shared or private, checked or unchecked.
Instances of auxiliary shared schemas are identical in all states of the temporal instance.
Intuitively, such a relation is common to all states of the database.
Instances of auxiliary private schemas can be different in the states of the temporal instance.
Declaring an auxiliary schema checked or unchecked has an impact on the semantics of the language.
Intuitively, the former (resp.
the latter) enforces the instance of the auxiliary schema to be used (resp.
ignored) when computing the semantics of a while statement.
From now on, we suppose that is the set of all auxiliary schemas.
The possible instructions of a program are: temporal moves left and right, assignment 	 fi  	, where  and   is a FO formula over with free variables 	.
iterator while  do fifi end, where  is a closed FO formula and fifi a sequence of instructions.
Clearly, while iterators can be nested.
             fi      E XAMPLE 3.3 Consider the following programs A" (over fi ) and Az (over fi  ) whose semantics are explained later on:        	  A": unchecked shared  ; while  fi do { right ;   fi    } end.
6  fi (resp.
fi) is a boolean whose value is fi only in the dZrst  (resp.
last) state of any temporal instance.
Az: checked shared  ;  	  fi 	   	 fi  ; while  fi do { right ;  	   fi   	       } end ; while   fi do { left ;  	   fi  	        } end.
Semantics The evaluation of a program over  is obtained considering a current time point, which is simply one of the time points of the instance .
A condZguration of a program is composed of: the current time point, the instances over all checked auxiliary schemas of in all states.
Note that when the evaluation starts, the current time point is the dZrst one and the instances of auxiliary schemas are empty in all states 7 .
left (resp.
right) decreases (resp.
increases) the current time point by 1.
If the current time point is the dZrst (resp.
last) one of the instance, then left (resp.
right) has no effect.
	 fi  	 changes the value of the instance over .
If  is private then  only changes in the current state (i.e.
the state at current time point); if  is shared then  changes in all states of the temporal instance.
When  achangesa, its new value is set to the answer of the query  	 evaluated at the current time point.
while  do fifi end executes all instructions of fifi until either  becomes false, or until the sequence of condZgurations observed after each execution of fifi reaches a repetition.
When both the current time point and the checked relations repeat, the computation terminates.
Recall that unchecked instances of auxiliary schemas do not belong to condZgurations, and thus are not taken into account when checking for a repetition.
If none of the conditions above is ever fuldZlled (i.e.
 never becomes false and the sequence of condZgurations never reaches a repetition), we say that the semantics of both the while statement and the program it belongs to are undedZned.
            E XAMPLE 3.4 We consider the programs of example 3.3.
For A" , the auxiliary schema  is boolean (i.e.
of arity 0).
When evaluated, the program executes   fi   in all states of the temporal instance, from left to right, starting from the dZrst state.
Thus the instance over  , whose initial value is  , returns the parity of the size of the temporal instance.
Note that there is no checked auxiliary schema in this program.
For Az , it can be shown that when the computation is dZnished the instance over  is the same as the evaluation of formula  from example 3.2.
       7 Thus, the initial value of boolean auxiliary schemas is 	  in all states.
In this paper, we take advantage of this fact by never initializing booleans with 	 .
Proceedings of the 12th International Symposium on Temporal Representation and Reasoning (TIMEa05) 1530-1311/05 $20.00 AS 2005 IEEE  Query languages We dedZne queries based on the programs dedZned above exactly like for the WHILE language, i.e.
by giving both a program and an auxiliary schema .
The answer of a query on  is the instance over at time point 1 output by the evaluation of over .
Note that can be either checked or unchecked in .
Next, we use the notion of indZationary assignment.
An assignment     is indZationary iff it adds all tuples which satisfy  to .
An indZationary assignment     is intuitively a acumulativea one and is equivalent to      .
We now dedZne four query languages based on the previous syntax and semantics.
1.
T- WHILE denotes the language without unchecked auxiliary schema and where all assignments are indZationary.
2.
T- WHILE denotes the language without unchecked auxiliary schema and with arbitrary assignments.
3.
T- WHILE 	 denotes the language where auxiliary schemas can be unchecked or checked, and where all assignments whose left-hand-side is a checked schema are indZationary.
4.
T- WHILE	 denotes the language where auxiliary schemas can be unchecked or checked, and with arbitrary assignments.
Note that one can specify programs of T- WHILE 	 and T- WHILE 	 with no checked auxiliary schemas.
Indeed one may write T- WHILE-like programs with no auxiliary schema at all.
This is the case of the following program which ends up setting the current time point to the last time point fi of the temporal instance: while  do right end.
About language names For the sake of our investigation and presentation, we choose different names for some of the languages introduced in [1].
The names given by us to the languages identify the features which are the key of the expressive power results provided in this paper.
Thus:  the language T- FIXPOINT of [1] is T- WHILE	 here,  the language T- WHILE of [1] is the same here,  the language called T- WHILE here was briedZy referred to as the apurely indZationary restrictiona of T- WHILE in [1].
Repetition vs. limit The semantics of the while statement is dedZned using the repetition of the sequence of condZgurations.
Another classical way to dedZne such a semantics is by using the limit of the sequence.
These two dedZnitions lead to different semantics for the languages T- WHILE 	 and TWHILE 	 , and they lead to equivalent query languages for T- WHILE  and T- WHILE .
Indeed, T- WHILE  	 firepetition limit (resp.
T WHILE repetition and fi fi and T- WHILE 	 	 T- WHILE 	 filimit) are probably not equivalent query languages.
Hence, we beleive that only T- WHILE and TWHILE would deserve to be called AT dZxpoint query languages At.
It is easy to show that in the static case, the repetition and limit dedZnitions lead to equivalent query languages for both WHILE and WHILE  .
Partial query languages There exists programs, belonging to T- WHILE , T- WHILE, T- WHILE 	 and T- WHILE	 whose semantics is undedZned.
Below we provide a program belonging to all four languages whose semantics is undedZned when the size of the temporal instance is greater than 3. right ; while  do left ; while 	  do  right ; right fi end ; end.
As opposed to TL-like languages, the fact that all TWHILE -like languages are partial query languages (even TWHILE  ) stands in contrast with the static case: recall that WHILE  is a total query language.
4  Expressive power results  This section begins with an introductory discussion.
Note that all TL-like and T- WHILE-like languages discussed in this section are strictly more expressive than TL.
T- WHILE 	 vs. T- WHILE In [1], the authors show that the equivalence T- WHILE 	  T- WHILE is aimprobablea.
Their proof proceeds to suppose that T- WHILE 	  T- WHILE.
is equivalent to T- WHILE Then in particular, T- WHILE 	 on temporal databases consisting of a single state, and hence WHILE  WHILE (because it is shown in [1] that auncheckeda schemas do not increase the expressive power of WHILE ).
However WHILE  WHILE is proved equivalent to PTIME  PSPACE in [3], which is unlikely.
The fact that in this paper we show T- WHILE 	  TWHILE should not be interpreted as a contradiction.
Indeed our proof considers temporal instances containing at least two states, whereas as pointed out above, the result in [1] is obtained by restricting temporal instances to static ones.
These two results (the unlikelyness of T- WHILE 	  T- WHILE for single state temporal instances and the proof of T- WHILE 	  T- WHILE for temporal instances of size at least 2) highlight an essential difference between static and temporal querying.
Data complexity of T- WHILE 	 As mentioned above, we show in this paper that T- WHILE 	  T- WHILE over really temporal instances.
This entails that T- WHILE 	 is PSPACE which disproves the claim of [1] that T- FIXPOINT is PTIME.
Intuitively, this is due to the non indZationary behavior of the current time point in T- WHILE-like languages.
New expressive power results The expressive power results that are now presented lead to a two-level hierarchy, as shows the dZgure below.
Inside each level, we display  Proceedings of the 12th International Symposium on Temporal Representation and Reasoning (TIMEa05) 1530-1311/05 $20.00 AS 2005 IEEE  equivalent query languages (the dZgure indicates the way the equivalences are proved by displaying the inclusions we establish).
In order to obtain the hierarchy, we dZrst focus on the lower level, then we show the separation between the two levels, and dZnally turn to the higher one.
For the sake of space, the proofs are sketched and sometimes even not presented.
ahigher levela T- WHILE	    T- WHILE	  T- WHILE  (T-FIXPOINT)  TL	  TL  TL      TL	  To prove this result, we consider propositional temporal instances (i.e.
instances having only relations of arity 0).
In this context, we show that TL  L INTIME and TL expresses the NP-complete problem SAT.
The complexity measure we use here is the size of the temporal instance 8 .
As a matter of fact, we have TL  L INSPACE: the only space needed for the computation of -expressions is the one to store the auxiliary relations 9 .
We then have TL  L INTIME, because TL is an indZationary language: during the aevaluationa of a -expression, each auxiliary relation in each state of the temporal instance, can either stay empty or become full and stay full thereafter.
Finally, it is rather straightforward to encode the wellknown problem SAT (satisdZability of a propositional formula) in TL, and SAT is NP-complete.
The encoding works by considering a formula in conjonctive normal form ; it encodes each clause of the formula in a state of the temporal instance, and computes the potential models of the formula by using as many auxiliary schemas as there are variables in the formula.
Higher level of the hierarchy L EMMA 4.4  TL	  TL  alower levela  Lower level of the hierarchy Obviously from the dedZnitions, TL TL  	 .
Moreover: L EMMA 4.1  TL  	  TL   This result shows that unchecked auxiliary relations, though not indZationary, do not increase the expressive power of the indZationary language TL .
The proof is done by means of a translation of TL 	 -expressions into equivalent TL formulas.
The key argument is that, due to the indZationary nature of the computation, formulas belonging to -expressions of TL 	 are only evaluated as long as tuples are inserted in the checked auxiliary relations.
Hence, the contents of the unchecked relations can be simulated by versioning their tuples with the tuples inserted in the checked relations since the previous iteration.
The versioning is done using cartesian product.
The aolda versions are cumulated in a separate relation, so that the process is fully indZationary, as needed for the query language TL .
This technique is sketched in [1] to show that unchecked relations do not increase the expressive power of the static query language WHILE .
T HEOREM 4.2  TL     TL  	  Relation between the two levels of the hierarchy T HEOREM 4.3  TL   TL  This result shows that unchecked auxiliary relations do not increase the expressive power of TL.
The proof structure is roughly the same as the one for TL TL , 	 though more simple.
Indeed, here we do not have to deal with aindZationary vs. not indZationarya formulas, hence the versioning technique is unnecessary.
L EMMA 4.5  TL  T- WHILE  	  This result is shown only over temporal instances whose size is at least 2.
The proof is done by means of a translation of TL formulas into T- WHILE 	 programs.
This translation makes use of the fact that the size of the temporal instances is at least 2 as highlighted below.
We dZrst introduce some macros for T- WHILE 	 .
They use special auxiliary schemas called macro schemas: 1) all macro schemas are boolean and unchecked, 2) when writting a program, it is assumed that each time a macro is used new names are given to its macro schemas.
Let  fi be a closed FO formula.
 A conditional assignment if  fi then      is dedZned by      fi fi      fi fi     A conditional temporal move if  fi	   left right) is dedZned by:  fi then fi	  (where  8 Note that the complexity measure for query languages is usually the number of tuples rather than the number of states of the instance.
9 In the propositional context, each relation is either empty (i.e fi) or afulla (i.e.
, when its content is the empty tuple).
Proceedings of the 12th International Symposium on Temporal Representation and Reasoning (TIMEa05) 1530-1311/05 $20.00 AS 2005 IEEE     fi ; while 	   do { 	  ;     } end where  is a shared macro schema.
 The macro goto  with   a boolean schema is dedZned by: while fi do left end ; while   do right end ; Intuitively, goto  sets the current time point to the leftmost time point where   is true.
 The macro FlipdZop is dedZned by: if fi then    fi ; if fi then right ; if    then left ;     ; where  is a shared macro schema.
Intuitively, FlipdZop changes the current time point as long as the temporal instance contains at least two states.
 The macro scan 	 fi, where fi is a sequence of instructions is dedZned by: fi   fi ; while fi do left end ; while 	 do { fi ; right } end ; goto fi  ; fi    ; where fi is a private macro schema.
Intuitively scan 	 fi executes fi in all states of the temporal instance from left to right starting from the dZrst state, as long as 	 is true.
The initial instruction of scan marks the current state (using fi ) in order to be able ago back therea using goto.
Hence, scan preserves the current time point.
 A global assignment     is dedZned by: scan fi     ; Intuitively, a global assignement enforces A to behave like a shared auxiliary schema for the assignment    .
 The macro Repetition    is dedZned by:     fi ; scan          fi   ; where   is a shared macro schema.
Intuitively, Repetition    checks that the instances over X and Y are the same in all states.
Sketch of proof: We now proceed to the translation.
Let   be a TL formula ; we inductively give a program  and an auxiliary schema  such that for all instance    A"    and for all   fi , the evaluation over  at time at time point point  of  is equal to the instance over  produced by the evaluation of  .
In order to build  , we inductively build an intermediate program  ; 	  (where  are declarations and 	  are instructions) and a FO formula  .
The program  is given by:   ; checked private 	  ;    .
   ;  where    denotes the number of free variables of a .
 If  is obtained through an FO formation rule, then  is essentially  itself, and  and 	  are straightforward.
 If      , let:    fi ; unchecked shared !
; unchecked private "    ;      "   	   	fi ;  "    ; /* resets  in case of nesting */ scan  fi  ; /* sets  */ where fi is: { right ; !
  fi  ; left ; "   !
 }  Intuitively, it is anot possiblea in T- WHILEAV  to grab, at the current time point, anything from the next state or any other state.
Thus in order to translate    we use !
whose purpose is to move fi  from the next state to the current state.
Hence !
needs to be shared.
It also needs to be unchecked because we compute    in all states and this computation is not indZationary.
 the case     fi#   is similar to the previous one and is omitted here.
 If     fi	 fi  $   10 , where $ is an auxiliary schema of 	 , let:    fi ; unchecked shared  	  $  ; unchecked private $ 	   $    	   fi  ;      $   	    fi   fi ; $    ; while $   do FlipdZop ;  $     $  ; 	fi ;  /* marks the current state */ /* resets 	 to  /* enforces current time point to change */ /* sets 	  to 	 in all states */  $  fi  ; Repetition $ $   ; fi if  	  then $    fi ; /* sets  end ; goto fi  fi       in case of nesting */  to 	  iff     */  ;   ; $     ;  Intuitively, since 	 is not necessarily an indZationary formula, it is translated with the unchecked auxiliary schema $ .
This implies that the evaluation of the while block proceeds with condZgurations whose single component is the current time point.
Recall that the evaluation of while loops stops either because their condition is false or because of condZguration repetition.
Here our goal is to force the evaluation of the while block to stop when a$ reaches a repetitiona.
Because $ is unchecked, repetition check over $ is encoded in the while block.
Morerover FlipdZop enforces 10 Here we consider w.l.o.g.
expressions.
Proceedings of the 12th International Symposium on Temporal Representation and Reasoning (TIMEa05) 1530-1311/05 $20.00 AS 2005 IEEE  TL  restricted to single-formula  -  two consecutive condZgurations to differ by changing the current time point and thus the evaluation of while block only stops when its condition becomes false.
Note that FlipdZop works properly only if the temporal instance size is at least 2.
Of course, if the semantics of the -expression    fi  is undedZned, so is the semantics of the above program.
 Obviously T- WHILE   L EMMA 4.6 T- WHILE  T- WHILE  .
Moreover:  T- WHILE  This result shows that unchecked relations do not increase the expressive power of T- WHILE.
Sketch of proof: Let  be a while statement of a TWHILE  program  , such that  contains two auxiliary schemas fi and  , where fi (resp  ) is declared checked (resp.
unchecked).
We suppose that both fi and  appear as left-hand-sides of assignments within  .
Now let  Az be obtained from the program  by declaring  checked.
The evaluation of  in  Az is necessarily at least as long as its evaluation in  : if fi ,  and the current time point repeat, then in particular fi and current time point repeat.
But the evaluation of  in  Az can be longer, and may not terminate (e.g.
if  never reaches a repetition).
Hence a way to have equivalent evaluations of  in both  and  Az is to observe when both fi and the current time point reach a repetition, and enforce that the condition of  becomes 	 at that moment.
 L EMMA 4.7 T- WHILE   TL  Sketch of proof: This proof is decomposed into two parts: 1) partial unnesting of while statements 2) translation of partially unnested T- WHILE programs into  TL formulas.
Both parts are rather technical and intricate, hence for the sake of space we only sketch them below.
We need to unnest while statements because of a fundamental difference between T- WHILE and  TL : in the former, an instance of an auxiliary schema fi can be assigned by more than one instruction, and hence its contents can change at various nesting levels.
This is anot possiblea in  TL because an instance of fi is only dedZned once by a single formula  .
Partial unnesting of T- WHILE programs addresses this issue.
Now, let us try to outline the main ideas behind partial unnesting.
Recall that WHILE programs can be fully unnested.
Indeed, consider the following WHILE program: while A" do fiA"  fi 	A"  ; while Az do fiAz  fi 	Az  end ; end.
This program is equivalent to the following unnested one:  fiAzfi  fi fiAz  ; while A" do if  fiAzfi  fi fiAz  then fiA"  fi 	A"  ; fiAzfi  fi fiAz  ; if Az then fiAz  fi 	Az  ; end.
For WHILE programs, the main idea behind unnesting is to unfold nested while statements by means of aconditionala assignments which are assignments themselves, thus the process can be iterated until obtaining fully unnested programs.
The above technique can be applied to unnest T- WHILE programs: assignments and temporal moves in nested while statements are transformed into conditional assignments and conditional temporal moves.
Contrary to the case of WHILE, the process does not lead to fully unnested T- WHILE programs for the following reason: a conditional temporal move is a (simple) while statement 11 .
Furthermore, to unnest T- WHILE programs, we need to propagate the storage instruction fiAzfi  fi fiAz  on all states.
This leads to a global assignment, which is also a while statement.
To sum up, the partial unnesting of a T- WHILE program leads to a program whose while statements contain only conditional temporal moves, conditional assignments, conditional global assignments, and the conditional variants of the macros Repetition and Time.
Once a T- WHILE program is partially unnested, translating its conditional instructions listed above turns out to be technical but rather simple.
The main problem raised by the translation of a T- WHILE program is to deal with time: a major component of the condZguration used in dedZning the semantics of T- WHILE-like languages is the current time point which introduces some kind of explicit control of time in TWHILE ; no such notion exists for   TL -like languages.
The translation manages this difdZculty as follows: to each instruction we associate a formula aiming at marking its current time point.
Such a formula has to be declared checked in order to take time into account the way T- WHILE does.
 T HEOREM 4.8  The languages  TL,  TL , T- WHILE, TWHILE  , T- WHILE AV  are equivalent.
We would like to emphasize once again that this theorem is valid under the two-states instances assumption only.
The result does not hold for single state instances (see section 4).
5  Discussion  The dZrst contribution of this paper is to enrich the expressive power hierarchy of [1] with results for  TL-like languages.
In particular, our two-level hierarchy provides the proof of the claim of [11] that the dZrst order extension 11 We  do not succeed in unnesting conditional temporal moves.
Proceedings of the 12th International Symposium on Temporal Representation and Reasoning (TIMEa05) 1530-1311/05 $20.00 AS 2005 IEEE  of TL (denoted TL in our framework) is strictly less expressive than T- FIXPOINT (denoted T- WHILE 	 in this paper).
Because T- FIXPOINT is equivalent to T- WHILE over really temporal instances, T- FIXPOINT is PSPACE.
The second important contribution of this paper is to highlight the impact of unchecked auxiliary schemas on TWHILE -like and TL -like languages.
Allowing one to use unchecked auxiliary schemas has no impact on the expressive power of non indZationary languages of both classes and on the expressive power of the indZationary language TL .
In the static case, the indZationary (resp.
non indZationary) WHILE language is equivalent to the indZationary (resp.
non indZationary) FO dZxpoint language.
The paper shows that this symmetry is broken in the temporal framework: TL 	 T- WHILE  	.
We do not provide any insight w.r.t.
the expressive power of T- WHILE .
However, we have strong reasons (although not yet proofs) to beleive that: C ONJECTURE 5.1 T- WHILE  T- WHILE  As in [1] (see section 4), we can use a complexity argument to prove that in case of static instances T- WHILE T- WHILE probably holds because in that case T- WHILE  reduces to WHILE , T- WHILE reduces to WHILE and WHILE WHILE iff PTIME  PSPACE .
This argument relying on static instances does not completely satisfy us because it does not entail that the result holds for really temporal instances (as our result about T- WHILE 	 and T- WHILE shows).
Another direction of investigation is to show that T- WHILE  and T- WHILE are separated by some query.
A candidate is the query denoted twin which checks wether there exists two identical states in the temporal instance.
To conclude this discussion, let us comment on the specidZc treatment of time in T- WHILE-like languages.
Somehow, time is not fully implicit in these languages because of the dedZnition of condZguration.
Moreover, the temporal moves left and right alone are too apoora to simulate the  and fi modalities of TL-like languages.
Roughly, (see for instance the translation of  in the proof of lemma 4.5), during the evaluation of a T- WHILE-like program, the computations made at some time point are only aware of the state at that time point and of the shared auxiliary relations.
Thus, without shared auxiliary schemas, moving to the right does not help to compute queries dependending on data of both the current and next states because the move allows one to access the next state data and meanwhile prevents from accessing data in the current state.
It is then clear that such queries need to be supported by shared and non indZationary auxiliary schemas.
References [1] S. Abiteboul, L. Herr and J.
Van den Bussche.
Temporal Connectives versus Explicit Timestamps in Temporal Query  Languages.
In Journal of Computer and System Science, 58(1):54a68, 1999.
[2] S. Abiteboul and V. Vianu.
Datalog extensions for database queries and updates.
In Journal of Computer and System Science, 43:62a124, 1991.
[3] S. Abiteboul and V. Vianu.
Generic computation and its complexity.
In Proc.
ACM SIGACT Symp.
on the Theory Of Computing, pages 209a219, 1991.
[4] N. Bidoit and S. De Amo.
Implicit temporal query languages: towards completeness, In FST&TCS, Chennai, India, LNCS Vol.
1738, 1999, pages 245a257.
[5] N. Bidoit, S. De Amo, and L. SegoudZn.
Order independent temporal properties.
In Journal of Logic and Computation, 14(2):277a298, 2004.
[6] A. K. Chandra and D. Harel.
Structure and comlexity of relational queries.
In Journal of Computer and System Science, 25(1):99a128, 1982.
[7] J. Chomicki and D. Toman.
Temporal Logic in Information Systems.
In Logics for databases and information systems, Kluwer Academic Publishers, chapter 3, pages 31a70, 1998.
[8] E. A. Emerson.
Temporal and Modal Logic, In Handbook of Theoretical Computer Science, Volume B: Formal Models and Semantics, Jan van Leeuwen, Ed., Elsevier Science Publishers (1990) 995a1072.
[9] Y. Gurevich.
Toward a logic tailored for computational complexity.
In Computation and Proof Theory, pages 175a216, M. M. Ritcher et al.
editor, Springer Verlag, LNM 1104, 1984.
[10] Y. Gurevich and S. Shelah.
Fixed-point extensions of dZrstorder logic.
In Annals of Pure and Applied Logic, 32:265a 280,1986.
[11] L. Herr.
Langages de RequASte pour les Bases de DonnASes Temporelles.
Ph.D thesis, UniversitAS Paris Sud, 1997.
[12] D. Leivant, Inductive dedZnitions over dZnite structures.
In Information and Computation, 89:95a108, 1990.
[13] A.R.
Meyer.
Weak monadic second order theory of successor is not elementary recursive.
In Proceedings Logic Colloquium, Lecture Notes in Mathematics, Vol.
453, pp.
132a 154, Springer-Verlag, 1975.
[14] Y. N. Moschovakis.
Elementary Induction on Abstract Structures, North Holland, Amsterdam, 1974.
[15] A. P. Sistla and E. M. Clarke.
The Complexity of Propositional Linear Temporal Logics.
In Journal of the ACM, 32(3):733a749, 1985.
[16] D. Toman.
On Incompleteness of Multi-dimensional Firstorder Temporal Logics.
In TIME: 99a106, 2003.
[17] M. Y. Vardi.
A temporal dZxpoint calculus.
In Proceedings 5th ACM Symposium on Principles of Programming Languages, pages 250a259, 1988.
Proceedings of the 12th International Symposium on Temporal Representation and Reasoning (TIMEa05) 1530-1311/05 $20.00 AS 2005 IEEE
Axiomatizations for Temporal Epistemic Logic with Perfect Recall over Linear Time Szabolcs Mikulas School of Computer Science and Information Systems Birkbeck College London, U.K. szabolcs@dcs.bbk.ac.uk  Abstract--This paper presents various semantic interpretations for logics of knowledge and time with prefect recall.
We allow both past and future operators and examine the interpretation of different linear flows of time.
In particular, we present temporal epistemic logics for each of the following flows of time: arbitrary linear orders; the integers; the rationals; the reals; and for uniform flows of time.
(By uniform flows of time, we mean that time is an arbitrary linear order that is common knowledge to all agents).
We propose axiomatizations for all logics except the last case, for which we show that no finite axiomatization can be found.
The axiomatizations are shown to be sound and complete in the case of arbitrary linear orders and the rationals.
Keywords: epistemic logic, temporal logic, complete axiomatizations I. I NTRODUCTION Logics of time and logics of knowledge have, independently, found many applications in reasoning about computational systems.
Temporal logics are applied to reason about program correctness [12], and logics of knowledge are applied to reason about information in distributed systems [4].
Furthermore, the combination of the two are well-studied and applied for reasoning about security protocols [9].
The application of temporal epistemic logic to specifying and verifying computational systems have generally relied on discrete flows of time [10], [5], and interactions between logics of knowledge and non-discrete flows of time have not received a great deal of attention in the literature.
Independent of epistemic logic, there has been a detailed study of temporal logic over various flows of time [14], [1].
In [6] some combinations between epistemic logic and non-discrete temporal logics were examined.
This work presented the complexity of a number of temporal epistemic logics and derived axiomatizations for some simple cases not involving perfect recall.
Here we seek to further this vein of research by providing axiomatizations for temporal epistemic logics with perfect recall given flows of time defined over different linear orders including: discrete linear orders (the integers), dense linear orders (the rationals) and continuous linear orders (the reals).
As the foundation of these results we present an axiomatization of temporal epistemic logic over arbitrary linear orders (so that the agents may not know the flow of time).
We show that strengthening this class so that the agents always know the flow of time  Mark Reynolds and Tim French School of Computer Science and Software Engineering University of Western Australia Perth, Australia {mark,tim}@csse.uwa.edu.au  results in a logic that cannot be finitely axiomatized.
For the interactions between knowledge and time we assume perfect recall (or no forgetting [10]), so that once an agent knows something about the set of possible flows of time, they always know it.
Our extensions to non-discrete flows of time are not only motivated by aesthetic arguments: There are numerous applications where we are required to not only reason about a computation, but also it's interaction with the environment.
The study of hybrid systems allows models of discrete computation (automata) to interact with continuous systems modeled by differential equations [3].
There is an emerging application for formal models of dense and continuous systems.
Complete theories for temporal epistemic logics over non-discrete time may complement such applications.
II.
T HE LANGUAGE Let the natural number n be fixed.
We will investigate the following logic T ELn .
The language consists of the propositional connectives, the epistemic modalities Ki (for i [?]
n) and the temporal operators F and P. The set Fml of formulas is built up from a set P of atomic propositions in the usual manner.
Semantics is given as follows.
Let S be a set of (local) states and L be a set of linear flows of time (L, <).
A run r over L is a function r : L - n S for some (L, <) [?]
L, i.e., for a given linear flow (L, <), it associates an n-tuple of states with every time point in L. For runs r, r0 over L, we will write r(l) ~i r0 (l0 ) (for i [?]
n) iff r(l)i = r0 (l0 )i .
A valuation v is function v : P - P(n S) associating to every atomic proposition in P a set of n-tuples of states.
A model M consists of a set R of runs over L and a valuation.
Truth of a formula is evaluated at a point l of a run r. The nonpropositional cases are as follows: for i [?]
n, 0 0 0 * (M, r, l) |= Ki ph if and only if (M, r , l ) |= ph for all r 0 0 0 and l such that r(l) ~i r (l ) 0 0 * (M, r, l) |= Fph if and only if (M, r, l ) |= ph for some l 0 such that l < l 0 0 * (M, r, l) |= Pph if and only if (M, r, l ) |= ph for some l 0 such that l < l We will use the usual abbreviations: Li ph for !Ki !ph, Gph for !F!ph, and Hph for !P!ph.
For a temporal operator T, let  T0 denote its "weak" version (defined by <= instead of <), e.g., P0 ph is defined as Pph [?]
ph.
Perfect recall The logic of perfect recall is defined by the following semantical condition: for all i [?]
n, if r(l) ~i r0 (l0 ) and k < l, then there is k 0 <= l0 with r(k) ~i r0 (k 0 ).
Let T ELpr n denote the logic T ELn satisfying the additional condition of perfect recall.
This simple definition for perfect recall is very intuitive in the case where all agents knowledge is synchronized (that is, for all runs r, for all distinct points x and y, r(x) 6~i r(y), see [4]).
However, here we do not restrict the models to be synchronized and this can lead to some unintuitive models.
For example, consider the model consisting of a single run over the integers, and a single agent such that for all x, y [?]
Z, r(x) ~ r(y) if and only if x and y have the same parity.
Such an agent is unable to distinguish between the past, present and future, which seems contrary to notion of perfect recall.
From the perspective of logic we are mostly interested in the properties that we are able to express using formulae, rather than anomalous classes of models.
Some interesting examples of such properties are: 1) An agent cannot distinguish between a single point in one run and an infinite set of points (an interval) in another run.
Only in such a case would the following formula be satisfied: Ki x [?]
HKi !x [?]
Li (x [?]
Px) Here the agent knows x is true, and previously knew x had never been true.
However, the agent is unsure whether this is the first instance which x has been true.
By the perfect recall axiom, in all related runs, all past moments where x was true must be indistinguishable to the agent from the current state.
2) An agent does not know whether time has an end.
Li GF> [?]
Li FG[?]
Here, an agent considers two runs possible: one with a final point and one without.
3) An agent may not recognize a final point in time: G[?]
[?]
Li GF> This is perhaps a little more challenging.
Should an agent recognize when there is no next moment of time?
This may vary with context, but from an applications point of view we could imagine an agent monitoring traffic on a communications channel, where the agent is unable to distinguish between no data being transmitted (but the channel silently persisting) the channel closing (a final point in time).
Flows of Time All logics we examine satisfy the perfect recall constraint and we define different logics by varying the flows of time (L, <).
Recall a linear flow of time (L, <) is defined such that L is any non-empty set, and < is a transitive, irreflexive and anti-symmetric relation such that for all x, y [?]
L, either x = y, x < y or y < x.
Particularly we consider: 1) General Linear Flows: T ELpr n refers to the case where each run may be any linear flow of time.
2) Uniform Linear Flows: UT ELpr n refers to the logic where every run in the model must be over the same flow of time.
However, there are no restrictions on what that linear flow of time may be.
3) The Integers: ZT ELpr n refers to the logic where every run in the a model must be over the integers, Z.
4) The Rationals: QT ELpr n refers to the logic where every run in the a model must be over the rational numbers, Q.
5) The Reals: RT ELpr n refers to the logic where every run in the a model must be over the integers, R. III.
C OMPLETENESS We will give a finite Hilbert-style axiomatization for T ELpr n .
THEOREM 1: T ELpr n is finitely axiomatizable.
Proof: The axioms are as follows.
Besides the axioms for propositional logic, we have the * axioms for epistemic logic (stating that every Ki is an S5 modality) * axioms for linear temporal logic (for strict F and P) (see for example [8], Chapter 6).
0 0 * Ki ph - G Ki P Ki ph (perfect recall axiom, PR).
PR says that if agent i knows ph, then agent i will always know that once (s)he knew ph.
Rules of inference are Modus Ponens and Universal Generalization.
We refer to the axiom system as TELpr n .
First let us note that the above calculus is sound w.r.t.
T ELpr n .
We just check the validity of PR.
Assume that (M, r, k) |= Ki ph and that k < l. We have to show that (M, r, l) |= Ki P0 Ki ph.
So let r0 and l0 be such that r(l) ~i r0 (l0 ).
We need (M, r0 , l0 ) |= P0 Ki ph.
By the perfect recall condition, there is k 0 <= l0 with r(k) ~i r0 (k 0 ).
Hence we have (M, r0 , k 0 ) |= Ki ph, whence (M, r0 , l0 ) |= P0 Ki ph follows.
To show completeness, assume that kh is a consistent formula.
We will construct a model satisfying kh in a step-by-step manner by "curing defects".
By a partial model M we mean a subset M of N x Q together with a function f M associating a maximal consistent set of formulas (mcs, for short) with every element of M , and with n equivalence relations ~M (i [?]
n) i such that 1) (x, y) ~M (x0 , y 0 ) implies that f M (x, y) and i f M (x0 , y 0 ) contain the same Ki ph formulas, 2) (x, y), (x, y 0 ) [?]
M , y < y 0 and Gph [?]
f M (x, y) imply ph [?]
f M (x, y 0 ), 3) the mirror image of 2 for H.  We will define four types of defect of a partial model M. Let (x, y), (x0 , y 0 ) [?]
M and ph be a formula (in the language of kh) such that ph [?]
f M (x, y).
Future defect: ph has the form Fps, and there is no (x, y 0 ) [?]
M such that y < y 0 and ph [?]
f M (x, y 0 ).
Past defect: the mirror image for P. Epistemic defect: ph has the form Li ps, and there is no (x0 , y 0 ) [?]
M 0 0 M 0 0 (x , y ).
such that (x, y) ~M i (x , y ) and ph [?]
f Perfect recall defect: 0 0 (x, y) ~M i (x , y ) and z < y such that (x, z) [?]
M 0 0 but there is no z 0 <= y 0 such that (x, z) ~M i (x , z ).
Given a countable partial model, there are at most countably many defects.
For a structure X x Y , d is a potential defect if there is a partial model on some M [?]
X x Y such that d is a defect.
Again, it is not difficult to see that if X and Y are countable, then there are at most countably many potential defects.
Thus we can assume that the potential defects of NxQ are enumerated: D = (d0 , d1 , .
.
.
).
Let s be a fair scheduling function of all potential defects of N x Q.
That is, s : o - D such that, for every k and d [?]
D, there is l > k such that s(l) = d. We will define the required model for kh by induction.
Assume the following induction hypothesis: Induction hypothesis: In each step of the construction, we have defined a finite partial model based on a substructure of N x Q.
Base step 0: Let X be an arbitrary mcs such that kh [?]
X.
The partial model M0 has universe {(0, 0)}, f M0 (0, 0) = X and, for every i, ~iM0 = {((0, 0), (0, 0))}.
Inductive step k + 1: Let the finite partial model defined so far be Mk based on the structure Mk [?]
Nk x Qk , where Nk [?]
N and Qk [?]
Q.
Let s(k) = d. If d is not a defect of Mk , then we define Mk+1 = Mk .
If d is a defect of Mk , then we consider the following cases according to the type of the defect.
Future defect: We have (x, y) [?]
Mk such that Fps [?]
f Mk (x, y) = / f Mk (x, y 0 ).
Y and, for all y 0 > y, ps [?]
By linear temporal logic we can create a witness.
First assume that for all y 0 > y, Fps [?]
f Mk (x, y 0 ).
Let z be the largest number such that (x, z) [?]
Mk , z 0 be z + 1 and Y 0 be a mcs containing {ph : Gph [?]
f Mk (x, z)} [?]
{ps}.
Otherwise let z be the greatest rational number w.r.t.
the following conditions: * *  (x, z) [?]
Mk , Fps [?]
f Mk (x, z).
Let Z = {ph : Gph [?]
f Mk (x, z)}, Z 0 = {r : Hr [?]
f Mk (x, y 0 ) for some y 0 > z} and Y 0 be a mcs containing Z [?]
Z 0 [?]
{ps}.
We let z 0 be a rational number greater than z but smaller than any of {u : (x, u) [?]
Mk , z < u}.
Ki P0 Ki km  Ki km , G0 Ki P0 Ki km [?]
Y  (x, y)  (x0 , y 0 )  r P0 Ki km , psm [?]
Yl  *  (x0 , yl-1 )  l-1 xm  (x0 , yr )  r xm  (x0 , z 0 )  phrm  (x0 , yr-1 )  r-1 xm  (x0 , y0 )  0 xm  (x, z) Q *  Q Q Q Q Q Q  *  Fig.
1.
Curing PR-defect  In both cases the new partial model Mk+1 is defined by expanding Mk as Mk+1 = Mk [?]
{(x, z 0 )}, f Mk+1 (x, z 0 ) = M k Y 0 and ~i k+1 =~M [?
]{((x, z 0 ), (x, z 0 ))} for i [?]
n. i Using temporal reasoning, it is easy to check that Mk+1 is a partial model.
Past defect: This case is completely analogous to the previous one, and we omit the details.
Epistemic defect: Li ps [?]
f Mk (x, y) = Y , and there is no (x0 , y 0 ) [?]
Mk k such that (x, y) ~M (x0 , y 0 ) and ps [?]
f Mk (x0 , y 0 ).
i We use epistemic logic to create a witness.
By epistemic (in fact, normal modal logic) reasoning, the set Y 0 = {Ki ph : Ki ph [?]
Y } [?]
{ps} is consistent.
Let z be the smallest element of N such that z [?]
/ Nk .
We define Mk+1 as the following expansion of Mk : let Mk+1 = Mk [?]
{(z, y)}, label M k (z, y) with a mcs Z [?]
Y 0 and let ~i k+1 [?
]~M be the i smallest equivalence relation containing ((x, y), (z, y)), while M k ~j k+1 =~M [?
]{((z, y), (z, y))} for j 6= i. j Since Ki is an S5 modality, Mk+1 is a partial model.
Perfect recall defect: k We have (x, y) ~M (x0 , y 0 ), and z < y such that i (x, z) [?]
Mk , but there is no z 0 <= y 0 such that k (x, z) ~M (x0 , z 0 ).
i We use the perfect recall axiom and temporal reasoning to create a witness, see Figure 1.
We have to find a mcs Y 0 such that Y 0 and Y = f Mk (x, z) agree on Ki -formulas, and place Y 0 in the finite linear order in which (x0 , y 0 ) occurs.
Let us consider this linear order (x0 , y0 ) < * * * < (x0 , yj ); then y 0 = yl for some l <= j.
Let us denote f Mk (x0 , yp ) by Yp (0 <= p <= l).
Our aim is to show that one of the following sets of formulas is consistent.
Ph0 : {ph : Hph [?]
Yl } [?]
{ph : Hph [?]
Yl-1 } [?]
* * * [?]
{ph : Hph [?]
Y1 } [?]
{ph : Hph [?]
Y0 } [?]
{Ki ph : Ki ph [?]
Y }  Ph1 :  {ph : Hph [?]
Yl } [?]
{ph : Hph [?]
Yl-1 } [?]
* * * [?]
{ph : Hph [?]
Y1 } [?]
{Ki ph : Ki ph [?]
Y } [?]
{ph : Gph [?]
Y0 }  k+2  ... Phl :  {ph : Hph [?]
Yl } [?]
{Ki ph : Ki ph [?]
Y } [?]
{ph : Gph [?]
Yl-1 } [?]
* * * [?]
{ph : Hph [?]
Y1 } [?]
{ph : Hph [?]
Y0 } Let (Ki ph0 , .
.
.
, Ki phm , .
.
. )
be an enumeration of all the forV mulas of form Ki ph in Y , and define km = {Ki php : p <= m}.
Clearly Ki km - km , whence Ki km [?]
Y .
Thus, by the perfect recall axiom, G0 Ki P0 Ki km [?]
Y .
Since we have a partial model, we get P0 Ki km [?]
Yl .
By epistemic reasoning, P0 km [?]
Yl .
By our assumption km [?]
/ Yl for a large enough m, whence Pkm [?]
Yl .
For every p <= l, let us V enumerate (php0 , php1 , .
.
. )
the formulas p in Yp , and define xm = {phpq : q <= m}.
Similarly we define p gm for G-formulas in Yp and khpm for H-formulas in Yp .
For every m consider the formula psm : Pkm [?]
l xm  [?]
l-1 P(xm  [?]
l-2 P(xm  [?]
*** [?]
1 P(xm  [?]
0 Pxm ) .
.
.
)).
Since Mk is a partial model, psm [?]
Yl , i.e., psm is consistent.
By our assumption that none of the Yp is the right witness, for p a big enough m, !km [?]
Yp for every 0 <= p <= l, i.e., km [?]
xm is inconsistent.
Then, by temporal reasoning, at least one of p the following formulas psm (0 <= p <= l) is consistent.
0 l l-1 1 0 psm : xm [?]
P(xm [?]
P(* * * [?]
P(xm [?]
P(xm [?]
Pkm )) .
.
. ))
0 1 l-1 l 1 )) .
.
. ))
psm : xm [?]
P(xm [?]
P(* * * [?]
P(xm [?]
P(km [?]
Pxm ... 0 1 l-1 l l ) .
.
. ))
[?]
Pxm [?]
* * * [?]
P(xm [?]
P(km [?]
P(xm : xm psm As m grows, for at least one fixed p, say r, infinitely many p r p 0 formulas psm are consistent.
Observe that psm 0 - psm for m > r m, whence psm is consistent for every m. Next consider the following formulas phpm .
0 1 ph0m : khlm [?]
khl-1 m [?]
* * * [?]
khm [?]
khm [?]
km 0 1 [?]
k [?]
* * * [?]
kh ph1m : khlm [?]
khl-1 m [?]
gm m m ... l-1 1 0 phlm : khlm [?]
km [?]
gm [?]
* * * [?]
gm [?]
gm p p If phm is inconsistent, then so is the formula psm 0 for a 0 r big enough m (by temporal reasoning).
Thus phm must be consistent for all m. It follows that Phr is consistent.
Then there is a mcs Z containing Phr .
This is the required witness, since we can insert a point (x0 , z 0 ) into the linear order (x0 , y0 ) < * * * < (x0 , yl ) below (x0 , yr ) (and above (x0 , yr-1 ) if this exists).
(Again we use the density of rational numbers here.)
Then the partial model Mk+1 is defined by adding (x0 , z 0 ) to Mk , labelling it with Z, and taking a minimal extension M M k k of ~M so that (x, z) ~i k+1 (x0 , z 0 ) (again ~j k+1 =~M i j 0 0 0 0 [?
]{((x , z ), (x , z ))} for j 6= i).
Limit step: We take the union M of Mk (k [?]
N).
Clearly M is a partial model.
Furthermore, the fair scheduling policy guarantees that it does not contain any defect (once a defect has been cured it cannot reoccur).
Thus M is a partial model of kh without defects.
From M we can define a model N for kh as follows.
Let us replace every element m = (x, y) of M by an n-tuple h(m) = (m0 , m1 , .
.
.
, mn-1 ) such that h(m)i = h(m0 )i iff m ~i m0 in M. We define h(m) < h(m0 ) iff  k+1  k+1  k+1  k  k  k  k  k-1  k-1  k-1  k-1  1  1  1  1  0  0  0  0  Fk+1  Fk  Fig.
2.
Frames Fk and Fk+1  m < m0 .
The valuation v is determined by the labels in M: h(m) [?]
v(p) iff p [?]
f M (m).
It is routine to check that N indeed satisfies kh.
REMARK 1: The reader might wonder why we are not using the canonical model to satisfy kh.
By Sahlqvist's Correspondence and Completeness Theorems (see [2, Theorem 3.54 and Theorem 4.42]) the canonical model satisfies the frame conditions corresponding to our axioms (< is a transitive, non-branching relation, ~i s are equivalence relations and the confluence property of perfect recall holds) and kh.
But it is not obvious to us how to turn the canonical model to a model based on tuples of states and runs.
Furthermore, our hope is that modifications of the step-by-step construction above might work for special cases (like the reals).
IV.
T HE UNIFORM CASE We start with showing that UT ELn is not finitely axiomatizable in general.
This is in contrast to pure temporal logic where the logic of linear flows is finitely axiomatizable.
Later we will see that in specific cases (such as the rationals) uniformity can be achieved.
We prove the theorem below by exploiting the relationship between modal logic and algebraic languages.
See [2] (Chapter 5) for a good general overview of this relationship.
THEOREM 2: The uniform version of temporal epistemic logic UT ELn is not finitely axiomatizable, even for a single agent (n = 1).
Proof: We will prove the theorem by showing that the equational theory of the complex algebras of the frames of UT ELn is not finitely axiomatizable in first-order logic.
For every positive integer k, we let Lk = (k, <), i.e., the linear flow of time with length k - 1.
We define the frame Fk consisting of two disjoint runs, one on Lk+1 and one on Lk+2 .
Thus we can identify these runs with the corresponding flows of time.
The relation ~i is defined as (k + 1 ] k + 2) x (k + 1 ] k + 2) where ] denotes disjoint union.
That is, there are two flows of time, with length k and k + 1, respectively, and the universal relation ~i to interpret the ith agent's knowledge.
Sometimes we will denote the runs Lk+1 and Lk+2 of Fk by Fk0 and Fk1 , respectively.
(See Figure 2).
Let Cm(F) denote the (full) complex algebra of the frame F. Hence Cm(Fk ) has universe P(k +1]k +2) and operations [?
], r and o[?]
for every modal/temporal connective [?
]: o[?]
(X) = {y [?]
k + 1 ] k + 2 : yR[?]
x for some x [?]
X} where R[?]
is the accessibility relation defining [?].
Let V denote the variety generated by all Cm(F) where F is a frame of UT ELn .
Recall from basic modal logic that the validities of UT ELn and the valid equations of V correspond to each other in the following sense: F |= ph - ps == Cm(F) |= ph = ps and thus UT ELn |= ph - ps == V |= ph = ps.
To prove that V is not finitely axiomatizable in first-order logic it is enough to show that its complement is not closed under ultraproducts.
This is done in the following two lemmas.
LEMMA 1: For every k, Cm(Fk ) is not in V. Proof: Let phk be the formula expressing that time is k long: H(H[?]
- Fk (> [?]
G[?]))
and consider psk = phk - Kphk .
Note that psk is valid formula of UT ELn , since all runs in a frame have the same length.
On the other hand, Fk 6|= psk .
Thus Cm(Fk ) 6|= psk = >, whence Cm(Fk ) is not in V. LEMMA 2: Any non-principal ultraproduct A of Cm(Fk ) is in V. Proof: Let F be the ultraproduct of the frames Fk over a non-principal ultrafilter U, and denote its complex algebra Cm(F) by B.
Then the lemma follows by 1) B is in V, 2) A can be embedded into B.
For 1 observe that F has two isomorphic (uncountable) linear flows of time, i.e., it is a frame of the logic.
Indeed, one can define the isomorphism by "shifting" (an equivalence class of) a sequence by one to the right.
In more detail, let x be an element of the ultraproduct of frames and x be such that x = x/U and {i : xi [?]
Fi1 } [?]
U, i.e., x is an element of the ultraproduct such that it is defined by a sequence of elements 0 from the runs Fi1 .
Since Fi1 = Fi+1 = (i + 2, <), for a large 0 set (i.e., in U) of indices i, we have xi [?]
Fi+1 as well.
Let y = (0, x0 , x1 , .
.
. )
and y = y/U.
We define the isomorphism i by i(x) = y.
It is easy to check that i is surjective and injective, and that it preserves the ordering <.
For 2: This is a standard result.
One can define the embedding as follows: given an equivalence class of a sequence X of subsets Xi , map it to the set of equivalence classes of the sequences determined by the product of the Xi s. In more detail, let Q X be an arbitrary element of A, and Xi be such that X = Xi /U.
Define the isomorphism kh by kh(X) = {x/U : xi [?]
Xi }.
It is routine to check that kh is indeed an isomorphism.
COROLLARY 1: UT ELpr n is not finitely axiomatizable.
Proof: Note that F satisfies the perfect recall condition, since ~i is the universal relation.
REMARK 2: Since the frame F that we defined in the proof of Lemma 2 is discrete, we have that UT ELn and UT ELpr n over discrete flows of time are not finitely axiomatizable.
We have shown that temporal epistemic logic with perfect recall cannot be axiomatized with these assumptions alone.
However, if we also specify the particular flow of time, then axiomatizations may be feasible.
The axiomatization of T ELpr n and proof presented in Section III is a basic approach to knowledge and time.
Effectively we have shown for an arbitrary temporal logic, and an arbitrary epistemic logic, the axiom PR is sufficient to capture the concept of perfect recall.
However, practical reasoning about knowledge and time will often use the assumptions that 1) There is one consistent flow of time; 2) this consistent flow of time is common knowledge to all agents.
Below we extend the above axiomatization to apply to such specified uniform flows of time.
V. T HE R ATIONALS We now suppose the T ELpr n is interpreted solely over flows of time that are isomorphic to the rational numbers.
To axiomatize such a logic it is sufficient to add to the axioms above the no-end-point axiom (NEP) and the density axiom (Dense): NEP Dense  P> [?]
F> Fp - FFp.
pr We let QTELpr n be the system TELn augmented with NEP and Dense.
THEOREM 3: The system QTELpr n is sound and complete for temporal epistemic logic with perfect recall over rational flows of time, QT ELpr n .
Proof: Soundness is easy to check and is left to the reader.
To prove completeness we show that the construction above can be extended to include and cure density defects.
That is, we apply the construction for general linear flows of time (above) with a new density defect in the schedule and suppose each mcs is consistent with respect to all substitution instances of the axioms NEP and Dense.
(Note that the end points will not require a new defect since an F> defect will be cured as a future defect).
In more detail, we call (x, y, y 0 ) a density defect if y < y 0 , f Mk (x, y) = Ps, f Mk (x, y 0 ) = Ph, and for all z with y < z < y 0 , f Mk (x, z) is undefined.
We include all density defects to the schedule of defects waiting to be cured.
Given a density defect as above, we have for all a [?]
Ph, Fa [?]
Ps.
By the density axiom it follows that FFa [?]
Ps.
Therefore, for all a [?]
Ph and b [?]
Ps, b [?]
FFa is consistent.
Hence Pb [?]
Fa is consistent by temporal reasoning.
It follows that there is a mcs L such that for all b [?]
Ps, Pb [?]
L and for all a [?]
Ph, Fa [?]
L. If such a L could not be found, there must be some finite a [?]
Ph and b [?]
Ps such that Fa - !Pb contradicting the reasoning above.
Thus we can define Mk+1 = Mk [?]
{(x, z)} and let M k [?
]{((x, z), (x, z))} for f Mk+1 (x, z) = L and ~i k+1 =~M i i [?]
n, curing the defect.
VI.
T HE I NTEGERS Previously temporal epistemic logic with perfect recall, next and until, but without past operators has been axiomatized over integer flows of time [9], [10].
Also, temporal epistemic logic with perfect recall, next, until and since has been axiomatized over flows of time isomorphic to the natural numbers [11].
For completeness we examine the temporal epistemic logic with perfect recall using only the tense operators (so that next and previous operators are not available).
Note that the axiomatizations in [9], [10] make use of the operators until and next which are known to be expressively complete for the natural numbers, whereas F and P are not expressively complete).
Furthermore, the axiomatization relies on the relationship between the next, until and knowledge operators to capture the perfect recall property.
For these reasons the axiomatization of ZTELpr n does not follow trivially from the works mentioned above.
An axiomatization for temporal epistemic logic over the integers can be defined by adding the axioms NEP (see above), and: ZF  (G(Gp - p)) - (FGp - Gp) discrete future  ZP  (H(Hp - p)) - PHp - Hp) discrete past  These axioms are known to be sufficient to axiomatize the tense operators, F and P, over the integers (see for example pr [1]).
We define ZTELpr n to be the axiom system TELn augmented with the axioms NEP, ZF and ZP.
LEMMA 3: The system ZTELpr n is sound for temporal epistemic logics with perfect recall over integer flows of time.
Proof: (Sketch) The soundness of NEP, ZF and ZP is well known (e.g.
[1]).
The soundness of the other axioms follows from their soundness for general linear flows of time.
We conjecture that the system is also complete, but the proof has thus far been elusive.
The approach taken has been to to show that every formula that is consistent with ZTELpr n has an integer model.
We know that every formula consistent pr with ZTELpr n is consistent with TELn (the logic of general linear flows), so we are able to apply Theorem 1 to build a model that satisfies both the consistent formula, as well as every substitution instance of the axioms NEP, ZF and ZP.
Having built this model, we attempt to transform it into an integer model without introducing any new defects, using the techniques of [13].
While the strategy is promising, there remain some anomalous cases which are difficult to address.
CONJECTURE 1: The system ZTELpr n is complete for temporal epistemic logics with perfect recall over integer flows of time.
VII.
T HE R EALS We now present an axiomatization for logics of knowledge with perfect recall over real flows of time.
For notational  convenience, we only present the single agent case, but the multi-agent case may be treated similarly.
As with the previous results, we simply augment the axiom systems for general flows of time with sufficient axioms to ensure that all the legitimate flows of time will be isomorphic to the real line.
We show that, as with linear temporal logic, it is sufficient to augment the axioms for rational flows of time (i.e.
density axioms and no end-points) with an axiom for Dedekind completeness.
In terms of topology of the line, these axioms ensure that every convergent sequence of points converges to some accumulation point.
The Dedekind axiom is: D  FHp - (Gp [?]
F(Hp [?]
!p) [?]
F(Hp [?]
p [?]
GP!p)) (1)  pr and we let RTELpr n be the axiom system QTELn augmented with the axiom D. (Note that as we are extending QTELpr n , density and no end-points are already given).
Note that the inverted version of D, below, can be inferred using D and the other temporal axioms presented.
PGp - (Hp [?]
P(Gp [?]
!p) [?]
P(Gp [?]
p [?]
HF!p).
(2)  LEMMA 4: The system RTELpr n is sound for temporal epistemic logics with perfect recall over real flows of time.
The proof is relatively straightforward and is left to the reader.
CONJECTURE 2: The system RTELpr n is complete for temporal epistemic logics with perfect recall over real flows of time.
As with the integer case, we attempt to show every formula that is consistent with RTELpr n has a model over real-flows of time via a rational-flowed model for that formula.
Given a rational-flowed model of the consistent formula we attempt to convert that model into a real-flowed model without introducing any additional defects.
It was hoped that we could extend the knowledge relations and the valuation of atomic propositions to include the irrational points, as can be done in the purely temporal case (see for example [7]).
However, in the presence of epistemic operators with perfect recall this is not straightforward, and it appears that this technique needs to be further extended.
VIII.
C ONCLUSION Here we have presented a number of axiomatizations and related results for temporal epistemic logics with perfect recall and varying flows of time.
We have shown the axiomatizations pr for T ELpr n (where flows of time are arbitrary) and QT ELn (where the flows of time are isomorphic to the rational numbers) are sound and complete.
We have also shown that the logic UT EL1 (where the flow of time is arbitrary, but known to all agents) no finite axiomatization can be given.
pr In the remaining cases, ZT ELpr n (the integers) and RT ELn (the reals) work is ongoing.
We propose sound axiomatizations here, and are working towards completeness proofs.
R EFERENCES [1] J. van Benthem.
The Logic of Time.
Kluwer Academic Publishers, 1983.
[2] Patrick Blackburn, Maarteb de Rijke, and Yde Venema.
Modal Logic.
Cambridge University Press, 2001.
[3] J. M. Davoren and Anil Nerode.
Logics for hybrid systems.
In Proceedings of the IEEE.
Springer-Verlag, 1993.
[4] R. Fagin, J. Halpern, Y. Moses, and M. Vardi.
Reasoning about knowledge.
MIT Press, 1995.
[5] T. French, R. van der Men, and M. Reynolds.
Axioms for logics of knowledge and past time: synchrony and unique initial states.
Advances in Modal Logic, 5:53-72, 2005.
[6] D. Gabbay, A. Kurucz, F. Wolter, and M. Zakharayashev.
Many Dimensional Modal Logics: Theory and Applications.
Elsevier, 2003.
[7] Dov Gabbay and Ian Hodkinson.
An axiomatization of until and since over the real numbers.
Journal of Logic and Computation, 1:229-260, 1990.
[8] R. Goldblatt.
Logics of Time and Computation.
Center for the Study of Language and Information, 1987.
[9] J. Halpern, R. van der Meyden, and M. Vardi.
Complete axiomatizations for reasoning about knowledge and time.
SIAM Journal on Computing, pages 674-703, 2004.
[10] R. van der Meyden.
Axioms for knowledge and time in distributed systems with perfect recall.
In Logic in Computer Science, pages 448- 457, 1994.
[11] R. van der Meyden, 2002.
Manuscript.
[12] A. Pnueli.
The temporal logic of programs.
In Proceedings of the Eighteenth Symposium on Foundations of Computer Science, pages 46- 57, 1977.
[13] Mark Reynolds.
Axiomatizing until and since over integer time.
In D. Gabbay and H.-J.
Ohlbach, editors, Lecture Note in A.I., volume 827, pages 117-132, 1994.
[14] Mark Reynolds.
The complexity of temporal logic over the reals.
Research Report cs.LO/9910012, arXiv, October 1999.
titative temporal information 7].
Moreover, we are also applying LaTeR to model-based diagnosis of dynamic systems.
In both cases, LaTeR high-level language provide a useful interface for obtaining a loosely coupled integration, and LaTeR's e	cient treatment of queries (and updates) provides crucial advantages.
A discussion on such applications can be found in 5].
A prototype of LaTeR has been implemented in C on Sun workstations, under the UNIX operating system.
References  1] J. Allen.
Maintaining knowledge about temporal intervals.
Communications of the ACM, 26:832{ 843, 1983.
2] J. Allen.
Time and time again: the many ways to represent time.
Int.
J.
Intelligent Systems, 6(4):341{355, 1991.
3] R. Arthur and J. Stillman.
Temporal reasoning for planning and scheduling.
Technical report, AI Lab, General Elettric Research Center, 1992.
4] V. Brusoni, L. Console, B. Pernici, and P. Terenziani.
LaTeR: a general purpose manager of temporal information.
In Methodologies for Intelligent Systems 8, pages 255{264.
Lecture Notes in Computer Science 869, Springer Verlag, 1994.
5] V. Brusoni, L. Console, B. Pernici, and P. Terenziani.
Dealing with time in knowledge based systems: a loosely coupled approach.
In Proc.
FLAIRS '95, Melbourne, FL, 1995.
6] V. Brusoni, L. Console, and P. Terenziani.
On the computational complexity of querying bounds on dierences constraints.
Articial Intelligence (to appear), 1995.
7] L. Console, B. Pernici, and P. Terenziani.
Towards the development of a general temporal manager for temporal databases: a layered and modular approach.
In Proc.
of the Int.
Work.
on an Infrastructure for Temporal Databases, Arlington, Texas, 1993.
8] E. Davis.
Constraint propagation with interval labels.
Articial Intelligence, 32:281{331, 1987.
9] T. Dean and D. McDermott.
Temporal data base management.
Articial Intelligence, 32:1{ 56, 1987.
10] R. Dechter, I. Meiri, and J. Pearl.
Temporal constraint networks.
Articial Intelligence, 49:61{ 95, 1991.
11] A. Gerevini and L. Schubert.
E	cient temporal reasoning through timegraphs.
In Proc.
13th IJCAI, pages 648{654, Chambery, 1993.
12] H. Kautz and P. Ladkin.
Integrating metric and qualitative temporal reasoning.
In Proc.
AAAI 91, pages 241{246, 1991.
13] J. Koomen.
The TIMELOGIC temporal reasoning system.
Technical Report 231, Computer Science Department, University of Rochester, Rochester, NY, March 1989.
14] L. McKenzie and R. Snodgrass.
Evaluation of relational algebras incorporating the time dimension in databases.
ACM Computing Surveys, 23(4):501{543, 1991.
15] I. Meiri.
Combining qualitative and quantitative constraints in temporal reasoning.
In Proc.
AAAI 91, pages 260{267, 1991.
16] R. Snodgrass, editor.
Proc.
of the Int.
Work.
on an infrastructure for Temporal Databases.
1993.
17] A. Tansell, R. Snodgrass, J. Cliord, S. Gadia, and A. Segev.
Temporal Databases: Theory, design and implementation.
Benjamin Cummings, 1993.
18] P. VanBeek.
Approximation algorithms for temporal reasoning.
In Proc.
11th IJCAI, pages 1291{1297, 1989.
19] P. VanBeek.
Temporal query processing with indenite information.
Articial Intelligence in Medicine, 3:325{339, 1991.
20] M. Vilain.
A system for reasoning about time.
In Proc.
AAAI 82, pages 197{201, 1982.
21] M. Vilain and H. Kautz.
Constraint propagation algorithms for temporal reasoning.
In Proc.
AAAI 86, pages 377{382, 1986.
22] M. Vilain, H. Kautz, and P. VanBeek.
Constraint propagation algorithms for temporal reasoning: a revised report.
In D.S.
Weld and J. de Kleer, editors, Readings in Qualitative Reasoning about physical systems, pages 373{381.
Morgan Kaufmann, 1989.
23] Ed Yampratoom and J. Allen.
Performance of temporal reasoning systems.
SIGART Bulletin, pages 26{29, 1993.  long less than 100 and more than 75% for sequences long from 100 to around 200.
A more detailed evaluation of the results can be found in 6].
5 Comparisons with Related Work  Dierent criteria can be considered in order to compare the temporal managers developed in the articial intelligence literature.
A rst important criteria concerns completeness.
In LaTeR, as in many articial intelligence approaches, we choose to retain completeness.
since it seems important to us in order to provide users and applications with uncontestable and reliable results.
This rises a trade-o between expressive power and computational complexity of complete temporal reasoning.
As e.g.
in Timegraph 11] and in Tachyon 3] we chose to limit the expressive power in order to retain tractability.
In particular, the expressive power of LaTeR is comparable to that of Tachyon, which deals with temporal constraints that can be mapped onto conjunctions of bounds on differences, too.
In 23], Allen distinguishes between two dierent class of temporal managers: (i) managers that use a constraint satisfaction technique at assertion time, building an all-to-all graph with the constraints between each pair of temporal entities in the knowledge base (ii) managers that build partial graph structures, which need further processing at query time.
For instance, Allen classied TimeLogic 13], MATS 12] and Tachyon 3] as systems of the rst type, and Timegraph 11] and TMM 9] as systems of the second type.
In 23], Allen, considering only atomic queries (i.e., queries for extracting the constraints between two entities in the graph, or yes/no queries without conjunction) pointed out that the approaches computing the all-to-all graph are more e	cient than those computing only partial graphs when dealing with queries.
In fact, in these approaches, queries can be answered in constant time, by reading the values from the graph, while in the approaches in (ii) some further reasoning may be needed.
On the other hand, the approaches in (i) are less e	cient when dealing with assertions (updates), since the whole all-to-all graph has to be computed after each assertion.
LaTeR is a system computing the all-to-all graph (which is the minimal network in the case of LaTeR) that reconciles the advantages of both types of approaches, thanks to its e	cient treatment of complex queries and of assertions as hypothetical queries.
This  result has been obtained via the treatment of complex types of queries.
As shown in van Beek's work 19], as soon as one considers non-atomic queries (even only conjunctions of yes/no queries), two problems arise: on the one hand, the distinction between queries about necessity and queries about consistency is needed on the other hand, constraint propagation may be required.
Van Beek's work has two major limitations with respect to the work in this paper: (i) it deals with qualitative information only and (ii) it performs constraint propagation on the whole network (global propagation) both for queries about necessity and queries about consistency (notice, however, that Van Beek allows the use of all logical connectives in the query language, although answering queries becomes exponential).
On the other hand, we showed that propagation is needed only for queries about consistency (and hypothetical queries, which are not considered in 19]) and, even in such a case, local propagation is su	cient.
Thus, LaTeR retains the e	cient query processing typical of approaches computing the all-to-all graph also in case complex queries.
Furthermore, since in LaTeR assertions followed by queries can be simulated by hypothetical queries (which are answered by local temporal reasoning), LaTeR does not have to recompute the whole all-to-all graph at each assertion, so that also assertions are managed e	ciently.
Besides providing the computational advantages above, LaTeR treatment of dierent (and complex) types of queries seems to us a main feature of the system in itself, since queries (and assertions) constitute the main way of interacting with temporal managers.
Thus, we believe that the expressive query language (and manipulation language) constitutes an advantage of LaTeR with respect to the other systems in the literature.
For instance, high-level interface languages are widely used in the temporal databases community.
However, most of the approaches to temporal databases only deal with time stamps associated with information and do not consider temporal relations between entities (see, e.g., 14]), so that temporal constraint propagation is not needed.
6 Conclusions  In the paper we showed how queries on an heterogeneous temporal knowledge base can be answered e	ciently, independently of the dimension of the knowledge base Currently, LaTeR is being loosely coupled with Oracle, in order to extend relational databases to deal also with (possibly imprecise) qualitative and quan-  the consistency of the knowledge base and thus consistency must be checked after each update and before answering the queries following the update itself.
Since answering queries in an inconsistent knowledge base is meaningless, the consistency check must be performed anyway.
Moreover, the minimal network of the updated knowledge base can be produced by the same algorithms that check consistency.
This means that the presence of updates does not aect the e	ciency of our approach: consistency has to be checked anyway but this produces the minimal network and queries can be answered e	ciently given the minimal network (see the previous section).
Our approach, on the other hand, suggests an e	cient way for dealing with a class of updates, specifically updates that add new constraints (which are the most common in many applications, see the discussion in 5]).
In fact, in such a case one can answer the queries following an update as hypothetical ones.
More specically, a query Q following an update U can be answered as the hypothetical query: Q if U which only involves local propagation.
If a query Q follows a sequence of updates U1  : : : Uh , this can be simulated as the query Q if U1  : : :Uh .
The advantage of such an approach is that during a session of interleaved queries and updates all the operations can be performed with local propagation and the actual update of the knowledge base (which can be very costly) can be delayed with respect to the query process (e.g., performed once and o-line at the end of the session).
Dealing with updates as hypothetical queries can provide signicant computational advantages.
However, when the sequence of updates and queries becomes very long and the updates involve signicant parts of the knowledge base, such advantages may be lost.
A detailed evaluation of the such computational advantages and trade-os can be found in 6] where we compare: the case where the minimal network is recomputed after each update (and then queries are answered with local propagation as discussed in the previous section) the case where queries are dealt with as hypothetical ones.
The evaluation is performed by taking into account three dierent parameters: The length of the sequences (\k" in (7)) The average dimension of updates/queries (we assume that updates and queries have the same  average dimension), i.e., the average ratio between the dimension of queries/updates and the dimension of the knowledge base How extensive the updates are, that is: how many entities involved in the i ; th update were not involved in the previous ones.
At one extreme, all the updates may involve the same set of variables (i.e., the same part of the knowledge base is repeatedly changed) at the other extreme, each update may involve a part of the knowledge base that was not involved by any previous update and thus the updates in the sequence tend to involve larger and larger parts of the knowledge base as the length of the sequence increases (in general, both extreme cases are unlikely taking this as a parameter allows us to consider all possibilities).
Two dierent evaluations are then performed: First of all we evaluated the break-even point between the two approaches that is: the maximum length of the sequence for which dealing with updates as hypothetical queries provides advantages, given the average dimension and extension of the updates or, conversely, which is the maximum dimension for the updates for which there are advantages, given the length of the sequence.
For example, it turned out that for a sequence of 40 updates and queries in which one half of the variables involved in each update was not involved by previous ones (so that the updates tend to extend to signicant parts of the knowledge base), dealing with updates as hypothetical queries provides advantages when the average dimension of each update/query is less that 7% of the knowledge base.
Conversely, when the average dimension of each query/update is 1% of the knowledge base, the approach is advantageous when the length of the sequence is less than 320.
From our experience in the practical application of LaTeR (see 5]), these dimensions are realistic in the sense that it is common that the dimension of updates/queries is around 1% of the dimension of the knowledge base and in any case never more than 5%.
We evaluated how big the computational advantage is.
For example, when the average dimension of update/queries is 1% of the knowledge base (and one half of the variables involved in each update were not involved in previous ones), the advantage is around 90% if the sequence is  Each one of the constraints in (6), taken in isolation, is consistent with (5), but the conjunction in (6) is inconsistent with (5).
Thus constraint propagation is needed in order to check whether a set of constraints is consistent with a given knowledge base.
However, we proved that global propagation of the constraints in the query to the whole knowledge base is not needed for computing the answer.
In fact, since the minimal network is available and since the goal is not to update the whole knowledge base but just to answer the query, local propagation is su	cient (local propagation concerns only the variables in the query).
More formally, we proved the following theorem (the proof can be found in 6]):  Theorem 1 Let S be a set of variables, K:B: a set  of bounds of dierences on such variables and NS the consistent minimal network computed by the (complete) propagation algorithm.
Let us consider a query MAY (Q) on K:B: (where Q is a conjunction of atomic tests) referring to a set G  S of variables (i.e., all the constraints in the query involve only variables in G).
Let NS be the minimal network obtained by propagating the constraints in Q to NS (i.e., to all the variables - global propagation in S ) and NG the minimal network obtained by propagating the constraints in Q to NG , where NG is the restriction of NS to the variables in G (local propagation) then NS is consistent if and only if NG is consistent.
0  0  0  0  The theorem guarantees that in order to answer MAY queries of the form: MAY (C1 AND C2 : : : AND Cn) it is su	cient to propagate the constraints Ci in the query to the part of the minimal network whose nodes are the variables in the query (i.e., occurring in C1 C2 : : : Cn ).
Therefore conjunctive MAY queries can be answered in a time that is cubic in the number of variables in the query and that is independent of the dimension of the knowledge base.
3.2.3 Hypothetical Queries.
Hypothetical queries are queries of the form Q if C where Q is a query of one of the types discussed in the previous subsections and C is a conjunction of temporal constraints, expressed in LaTeR's high-level language.
For example, given the knowledge base in gure 1, the following queries could be asked:  HowLong John work If Mary work Lasting 4h 50min?
Answer : 5h MUST ( Tom work During Mary work) If Mary work Lasting 4h 50min?
Answer : Y es In principle, an hypothetical query should be answered in 3 steps: (i) adding the constraints C to the temporal knowledge base (ii) computing the minimal network N for the new knowledge base (iii) answering the query Q given N .
However, we proved the following theorem (see 6] for more details): Theorem 2 Given S, NS , G, NG , Q, NS and NG as in Theorem 1, then for each pair of variables hX Y i in G, the maximal admissibility range for X ; Y provided by NG (minimal network computed with local 0  0  0  0  0  propagation) is the same as the maximal admissibility range for X ; Y provided by NS (minimal network computed with global propagation).
0  In other words, as regards the variables in G, local propagation to the part of the minimal network concerning the variables in G produces the same results as global propagation to the whole minimal network.
This means that, for any query Q If C, it is su	cient to proceed as follows: perform local propagation of the constraints in C to the part of the minimal network involving the variables in C fi Q Answer Q as discussed in the previous subsections.
The theorem guarantees that this procedure provides the same result that would be obtained by propagating the constraints in C to the whole knowledge base before answering the query Q.
Thus, also hypothetical queries are answered in LaTeR in a time which is independent of the dimension of the knowledge base (more specically, in a time that is cubic in the number of variables in C fi Q).
4 Dealing with updates  In the practical applications of temporal reasoning queries are interleaved with updates.
In other words, a typical session with a temporal knowledge server could have the form of a sequence: U1  Q1 U2 Q2 : : : Uk  Qk (7) of alternated updates (Ui ) and queries (Qi ).
An update corresponds to the the addition or removal of some temporal assertion.
Each update may aect  MAY ( Tom work During Mary work AND start(John work) After 1620) which involves checking that the conjunction of the two assertions is consistent with the knowledge base.
Given the example in gure 1 the answer to such a query is negative.
Queries about consistency/necessity are mapped into conjunctions of atomic tests each one of which is a check on the distance between two time points (and thus the dierence between two variables in the minimal network).
The mapping is the same used for translating assertions into bounds on dierences sketched in section 2.
For example, the conjunction of atomic tests corresponding to the queries (1) and (2) above are respectively: MUST( 0 < STW ; SMW AND 0 < EMW ; ET W) MAY ( 0 < ST W ; SMW AND 0 < EMW ; ET W ) (STW and ET W are as above SMW and EMW are the starting and ending points of \Mary work").
In other words, high level queries about consistency (necessity) are answered by checking that a conjunction of bounds on dierences (atomic tests) is consistent (follows necessarily) from the constraints in the knowledge base.
Given the minimal network, atomic tests can be performed as local checks on such a network.
However, dierent checks are performed in case of MUST and MAY queries as a result the computational complexity of the cases is dierent, as it will be discussed in the two following subsections.
3.2.1 MUST queries  Let us consider a query about necessity of the form MUST(C1 AND C2 : : : AND C ), where each C is an atomic test of the form c  X ; Y  d .
We distinguish two cases:  (n = 1), i.e., the query involves only one atomic test and thus has the form: MUST(c  X ; Y  d) Let afi b] be the maximal admissibility range for the dierence X ; Y (read from the minimal network).
The query is satised i all the values for X ; Y which satisfy the constraints are in cfi d], that is: MUST (c  X ; Y  d) , cfi d]  afi b] (3) Intuitively, since the maximal admissibility range afi b] includes all the values for X ; Y satisfying the constraints, then any interval cfi d] such that cfi d]  afi b] includes all the values for n  i  i  i  i  i  X ; Y satisfying all the constraints.
A query involving one constraint can thus be answered in constant time with a simple lookup in the minimal network and a containment check.
 (n > 1), i.e., the query involves a conjunction of atomic tests and has the form MUST (C1 AND C2 : : : AND C ).
In this case each one of the C can be checked independently of the others since the following property holds: MUST(C1 AND C2 : : : AND C ) , MUST(C1 ) AND : : :AND MUST(C ) n  i  n  n  Thus a query about necessity can be answered in time linear in the number of constraints (and thus in the number of variables) in the query.
3.2.2 MAY queries  Let us consider a query about possibility, i.e., of the form MAY (C1 AND C2 : : : AND C ), where each C is an atomic test of the form c  X ;Y  d .
This case is more complex than the one of MUST queries since the MAY operator does not distribute over a conjunction.
The base case, however, is similar, in the sense that when n = 1, the answer to a query of the form: MAY (c  X ; Y  d) can be provided with a local check on the minimal network.
Let afi b] be the maximaladmissibility range for the dierence X ; Y (read from the minimal network).
The query is satised i there is (at least) a value p 2 cfi d] for X ; Y which satises all the constraints, that is: MAY (c  X ; Y  d) , cfi d] \ afi b] 6= 	 (4) Intuitively, since the maximal admissibility range afi b] includes only values for X ; Y satisfying the constraints in the knowledge base, then any interval cfi d] intersecting afi b] contains at least one value for X ; Y satisfying all the constraints.
The case where the consistency of a conjunction of constraints has to be checked is more complex since atomic tests are not independent of each other.
For instance, consider the knowledge base formed by the following constraints: f0  X ; Z  30fi 5  Z ; W  25fi 10  Y ; X  20fi 15  Y ; Z  30g (5) and the query: MAY (15  Y ; Z  20 AND 15  X ; Z  20) (6) n  i  i  i  i  i  where STW and ET W (SJW and EJW) are the variables associated with the starting and ending points of \Tom work" (\John work") respectively.
Given a knowledge base of temporal information (expressed as bounds on dierences), its consistency must be checked before answering queries (or performing updates), since query processing is not interesting in an inconsistent knowledge base.
LaTeR checks the consistency of a set of bounds on dierences constraints using the complete algorithm discussed in 10], whose complexity is O(N 3), where N is the number of variables.
This algorithm produces the minimal network of the set of constraints, i.e., a compact representation of all the solutions.
More specically, for each pair hXfi Y i of variables, the minimal network provides the maximal admissibility range afi b] for the dierence X ; Y .
In other words afi b] is the set of all and only the values for X ; Y consistent with the knowledge base.
LaTeR keeps track of such a network since, as we shall discuss in the following section, this provides interesting computational advantages during query processing.
3 Ecient Query Answering in LaTeR  At least three dierent types of high-level queries are important for querying a temporal knowledge base: queries for extracting some piece of information from the knowledge base (e.g., the duration of an event or the relation between two events), queries for checking whether a set of temporal constraints is consistent with or follows necessarily from the knowledge base and hypothetical queries.
LaTeR provides a high-level language for expressing all these types of queries.
Queries in the high-level language are then translated into the corresponding low-level queries on bounds on dierences constraints, which are answered eciently, in a time that is independent of the dimension of the knowledge base.
Let us consider the types of queries listed above one at a time.
3.1 Queries for extracting temporal information.
Dierent high-level primitives are provided: When, HowLong, Delay and Relation, which give as answer respectively (1) the temporal location of temporal entities (points or intervals), (2) the duration of time intervals, (3) the delay between two time points and (4) the temporal relations between two temporal entities.
These queries can be answered by a simple lookup in the minimal network.
For example, given the knowledge base in gure 1, the following query could be asked:  HowLong John work?
Answer : 4hfi 50min ; 5h This query can be answered by simply reading in the minimal network the maximal admissibility range of the dierence between the variables corresponding to the end and start of \John work".
As a further example, the following query could be asked: Relation Mary workfi John work Answer : start(Mary work) After start(John work) end(Mary work) non strict Before end(John work) Also in such a case the answer can be read directly from the minimal network (and is then translated in the output format above).
Notice that the answer corresponds to the following relation in Allen's interval algebra: Mary work (During OR Finishes) John work  3.2 Queries about consistency/necessity.
A second important type of query is that of Yes/No queries for asking whether a set (conjunction) of constraints is true in the given knowledge base.
Since in LaTeR temporal information may be imprecise, it is necessary to distinguish whether some conclusion must necessarily hold (i.e., it is entailed by the knowledge base) or whether it may hold (i.e., it is consistent with the knowledge base).
This distinction is similar, e.g., to the one in 19].
Therefore, modal operators must be introduced in the query language in order to distinguish between queries asking whether a set of constraints is possible (consistent) given the knowledge base or whether it follows from the knowledge base.
In LaTeR queries about necessity/consistency are expressed by prexing the MUST or MAY operator to the primitives of the high level manipulation language.
For instance, given the knowledge base in gure 1, one could ask: MUST (T om work During Mary work) (1) MAY (T om work During Mary work) (2) (1) corresponds to asking whether the relation Tom work During Mary work is entailed by the knowledge base (2) asks whether it is consistent with the knowledge base.
Given the knowledge base in gure 1, the answer to (1) is negative while the answer to (2) is positive.
Conjunction is also provided, so that one can ask for the necessity/consistency of a conjunction of temporal constraints.
For example, one could ask the following query:  the approaches that maintain the minimal network and those that perform reasoning at query time 23] discussing how our approach strongly supports the former alternative (since we deal eciently with complex queries and with a class of updates).
2 Representing time in LaTeR  LaTeR is a general purpose manager of temporal  information conceived as a \knowledge server" that can be loosely-coupled with dierent Articial Intelligence and database applications 4, 5].
We believe that a knowledge server must have a predictable behavior.
This has at least two main consequences: (i) from the inferential point of view, complete temporal reasoning must be performed (ii) from the computational point of view, reasoning must be performed in polynomial time.
Moreover, a friendly interface language for interacting with the system must be available in particular, a powerful query language must be provided and query processing must be performed very eciently.
LaTeR is a two-level architecture: the higher level provides the manipulation and query interface language (to which we shall return in the following) the lower level is based on the use of a constraint framework.
LaTeR assumes that time is linear, totally ordered, continuous and metric.
Time points are the basic entities an interval I is dened as a convex set of time points with a starting and an ending point, denoted respectively as start(I) and end(I) (with start(I) < end(I)).
The distance between time points is the basic primitive in our approach and is dened as follows: Given two time points P1 and P2, the assertion distance(P1,P2,afi b]) is true i the distance between P1 and P2 is between a and b, where afi b 2 Rfi a  b:1 The notion of distance is isomorphic to the notion of dierence between reals.
Thus, standard and well-known constraint propagation techniques (see 8] or frameworks such as tcsp and stp 10]) can be used to implement such a notion: the variables correspond to the time points and each assertion distance(P1,P2,afi b]) can be represented as a bound on the dierence between the variables X1 and X2 corresponding to P1 and P2, i.e., as a linear inequality of the form: a  X2 ; X1  b In order to achieve the goal of tractable complete reasoning we limited the expressive power to deal 1 We consider also the case where one of the extremes and b or both of them are not included, i.e.
the range is partially or completely open.
a  only with conjunctions of bounds of dierences, in which complete constraint propagation is performed in O(N 3 ) (where N is the number of variables).
The expressive power of LaTeR's lower level is thus the one of stp 10].
LaTeR provides a high-level interface language for manipulating and querying a temporal knowledge base.
Each assertion in such a language is translated (in constant time) into a set of lower-level constraints (bounds on dierences).
Given the restrictions above on the lower level, we have some restrictions on the expressive power of the interface language.
In particular, the following types of information can be expressed: precise or imprecise location of time points and intervals, precise or imprecise duration of time intervals, precise or imprecise delay between time points, qualitative relations between points, intervals or points and intervals, limiting to the continuous pointisable relations 22] (as discussed in 19] this is not too restrictive in practice since many commonly used relations are indeed continuous pointisable).
Figure 1 provides examples of assertions in LaTeR's high level language (see 4] for a denition of the language).
John work Since 1400 ; 1430 Until 1800 ; 1900 start(Mary work) 10 ; 40 min After start(John work) Mary work Lasting AtLeast 4 hfi 40 min end(Mary work) non strict Before end(John work) Tom work Since 1415 Until 1830 Tom work During John work Figure 1: A simple knowledge base.
For example, the rst assertion localizes (in an imprecise way) the interval of time corresponding to \John work" the second denes a delay between the starting point of \Mary work" and the starting point of \John work" the third denes the duration of \Mary work".
The non strict operator can be used in conjunction with the precedence (and containment) relations to express that the relation itself is not strict (in the example the meaning is that the end of \Mary work" is before or equal the end of \John work").
As an example of the translation of high-level assertions into bounds on dierences, the last assertion in gure 1 is translated into bounds on dierences as follows: (0 < STW ; SJW ) ^ (0 < EJW ; ET W)  Ecient query answering in LaTeR  V. Brusoni and L. Console and P. Terenziani Dip.
Informatica, Universitfia di Torino, Corso Svizzera 185, 10149 Torino, Italy E-mail: fbrusoni,lconsole,terenzg@di.unito.it  Abstract  In the paper we address the problem of answering queries eciently in heterogeneous temporal knowledge bases (in which qualitative and quantitative pieces of information are amalgamated).
In particular, we rst outline a powerful high-level language for querying a temporal knowledge base.
We then show that, in our language, if the minimal network computed during consistency checking is maintained, then queries can be answered eciently in time that depends only on the dimension of the query and is independent of the dimension of the knowledge base.
Finally, we discuss how our approach can deal eciently also with updates and, specically, with sequences of interleaved updates of the knowledge base and queries.
1 Introduction  A lot of attention has been paid in the Articial Intelligence community to the problem of dealing with time 2, 22].
In particular, most articial intelligence approaches focus on reasoning issues 1, 15, 18, 20, 21].
On the other hand, the problems of (i) designing a high level language for manipulating and querying temporal knowledge bases and (ii) answering (complex) queries eciently have been often disregarded.
Some of these problems have been faced in the database community 14, 16, 17] where, however, reasoning and complexity issues received only a limited attention.
The aim of this paper is to reconcile these two complementary tendencies in a general-purpose manager of temporal information: LaTeR (Layered Temporal Reasoner).
In LaTeR heterogeneous temporal information (that is, qualitative and quantitative information) is amalgamated in a principled way startThis work was partially supported by CNR under grant no.
94.01878.CT07.
ing from the notion of distance between time points.
LaTeR, moreover, provides a high-level language for manipulating temporal information the expressive power of the language has been limited in such a way that complete constraint propagation can be performed in polynomial time (section 2 sketches those aspects of LaTeR that are relevant in this paper, see 4, 5] for more details).
The paper denes a powerful query language including modal operators for asking whether a set of assertions follows necessarily from a knowledge base or it is only possibly true and supporting yes/no queries, queries for extracting temporal information and hypothetical queries.
We believe that having a powerful language for querying temporal knowledge bases is fundamental for the practical applicability of managers of temporal information.
The main goal of the paper is to propose an approach for answering queries eciently in a temporal knowledge base (section 3).
Notice that the problem is interesting only in case the knowledge base is consistent since answering queries such as those mentioned above in an inconsistent knowledge base is banal.
We show that in our language, if we maintain the propagated knowledge base (\minimal network") obtained as a result of checking consistency of the knowledge base, then the complexity of answering queries is independent of the dimension of the knowledge base and depends only on the dimension of the query, where the dimension of a knowledge base (query) corresponds to the number of temporal entities involved in the knowledge base (query).
A critical aspect when the minimal network is maintained is that of updating such a network each time the knowledge base is updated (since, in principle, the whole network has to be recomputed after every update).
In the paper we discuss how updates to the temporal knowledge base and interleaved sequences of updates and queries can be dealt with efciently in our approach (section 4).
In section 5 we compare our approach to related ones.
In particular, we consider the trade-o between
A trace semantics for Positive Core XPath Pieter Hartel, Univ.
of Twente, http://www.cs.utwente.nl/Epieter  Abstracta We provide a novel trace semantics for positive core XPath that exposes all intermediate nodes visited by the query engine.
This enables a detailed analysis of all information relevant to the query.
We give two examples of such analyses in the form of access control policies.
We translate positive core XPath into Linear Temporal Logic, showing that branching structures can be linearised effectively.
The translation is proved correct.
We use the SPIN model checker in a proof of concept implementation to resolve the queries, and to perform the access control.
The performance of the implementation is shown to be competitive.
I. I NTRODUCTION Many approaches towards Access control on XML data use XPath (directly or indirectly) both for the queries and for access control (e.g.
[5]).
We are interested in combining a more flexible, logical approach [1] to access control, with the standard XPath based querying.
An XPath (version 1.0 [10]) query is normally resolved by giving the answer set.
This hides intermediate nodes visited by the query engine, which might contain sensitive information.
We intend to expose this information so that it can be analysed, for example from the point of view of access control.
a) Example 1: Consider the family tree of Fig.
1 with query1 asking for all family members with following siblings: query1 = descendant :: "*"[following sibling :: "*"] The answer set (i.e.
Cain and Abel) does not reveal (1) the name of some of the following siblings (i.e.
Seth), (2) that one of the members of the answer set is in fact a following sibling himself (i.e.
Abel), and (3) the multiplicity of the answers (Cain is included for two reasons).
So the answer set hides information that is available to the query engine.
This information may be sensitive, and we are interested in making this information available for analysis.
We achieve this by resolving a query not to the answer set but to the entire trace from the root produced by the query engine.
For the example above there are three traces: results1 = {[Root, Adam, Cain, (Abel, Seth), Cain], [Root, Adam, Cain, (Abel), Cain], [Root, Adam, Abel, (Seth), Abel]} Some tags, like Abel and Seth in the first trace, are shown in parentheses to indicate that they are the result of exploring the predicate [following sibling :: "*"] of query1 .
Other tags, such as Cain, are shown twice in the first trace because they have been visited twice: the first time while moving right from Adam to Cain and the second time returning from Seth to Cain.
We can now use the information contained in a trace for analysis purposes, such as access control.
We give two examples.
<Adam> <Cain> <Enoch/> </Cain> <Abel/> <Seth> <Enosh/> </Seth> </Adam>  0: Root H  p  c   3: Enoch  c  p  c   5 1: Adam H a  p  t 2: Cain l H  p  c r l  ,   4: Abel m  c p r  ,  l  !
5: H Seth p  c   6: Enosh  Fig.
1.
Sample family tree in XML format (left) and in navigational format (right).
The edge labels are: c for children, p for the parent, and l and r for the immediate sibling to the left and right respectively.
Firstly, suppose that (if only for historical reasons) the node tagged Cain should not be included in a trace that contains Abel also.
Furthermore, we should like to be free to choose whether to access Cain first, or whether to access Abel first.
This corresponds to (the object specification of) a Chinese wall policy [6], where Cain and Abel are in the same conflict of interest class.
XPath is not powerful enough to formulate such a general policy because we do not know a-priori which axes to navigate to travel between members of a conflict of interest class.
All we could hope to do is to formulate a specific policy for each query.
To solve this problem we use Linear Temporal Logic (LTL, see Appendix A) to express the policy as follows (for generality extending the conflict of interest class to all children of Adam): Chinese wall = 2( Cain a AZ 3( Abel a" Seth)) The formula Chinese wall states that we should always (operator 2) have that as soon as we encounter Cain, then we must not eventually (operator 3) encounter either Abel or Seth.
This corresponds to the mandatory aspect of the Chinese wall policy.
The formula Chinese wall does not insist that Cain is ever encountered, which corresponds to the discretionary aspect of the Chinese wall policy.
Secondly, using an idea of de Alfaro [12], suppose that every trace to a confidential node Cain must pass through an access control node Adam, thus blocking access via Abel.
This can be formalised intuitively in LTL with past operators (an equivalent LTL expression with only future operators exists but it is less intuitive [32]): Access control = 2( Cain a 3a1 Adam) The formula Access control states that any access of Cain is due to some earlier access of Adam.
The two examples above only mention the object specification of an access control policy; we have tacitly assumed that  security views using efficient query optimization techniques.
Our approach to combining a query with an access control object specification is an example of case (3): the model checker ensures that only relevant parts of the state space are explored.
Luo et al [23] also perform query rewriting, but consider forward axes only.
Murata et al [28] use static analysis techniques to optimise query processing.
Bertino and Ferrari [4] present a versatile system for authoring XML based access control policies.
Both the subject and the object are represented by XPath expressions.
The policies themselves are again XML documents.
Milau and Suciu [27] use XQuery (and thus also XPath) to state access control policies.
Fundulaki and Marx [14] use XPath to represent the object specification of an access control policy, which as we have shown is less powerful than using LTL for the same purpose.
Fu et al [13] use SPIN to model check XPath queries but their approach is radically different from ours in the sense that both the XML data and the query are part of the model.
Fu et al use LTL formulae to specify liveness properties of the model, where we use LTL for the queries.
Fu et al do not present performance data.
the subject making the query is identified and authenticated, that the authorisation is positive only, and that the privilege is assumed to be areada.
Extension to more aspects of access control policies is future work.
Having motivated using LTL to express access control policies, a natural target for expressing a query is also LTL, so that we can combine them simply with a logical aSS operator, using the same formalism and implementation for both querying and access control.
Therefore, the focus of the paper is on the semantics of positive core XPath because this can be translated efficiently into LTL.
The main contributions are (1) a novel trace semantics for positive core XPath, (2) the translation of positive core XPath into LTL, (3) the correctness proof of the translation with respect to the trace semantics, and (4) a proof of concept implementation of the system.
The next section discusses related work.
Sec.
III motivates the positive core XPath subset.
Sec.
IV formalises undecorated XML trees.
Sec.
V defines the Kripke structure that forms the link between the formalised XML tree representation and the semantics of LTL.
Sec.
VI defines the embedding of positive core XPath into LTL via a translation algorithm.
Sec.
VII provides a natural semantics for positive core XPath.
Sec.
VIII presents the implementation of the positive core XPath engine using the SPIN model checker [20], and compares the performance of the implementation to that of state-of-the-art XPath query engines.
The last Sec.
concludes and gives ideas for future work.
Appendix B gives a correctness proof of the positive core XPath translation with respect to the natural semantics.
III.
P OSITIVE CORE XPATH Full XPath is impractical to use as a tool for investigating the fundamental relation between query and access control.
Several subsets have been defined, such as Core XPath [16], Simple XPath [2], and Navigational XPath [26].
We adopt a similar approach in that we omit expressions and focus on location paths and predicates.
Contrary to some of the work cited earlier, we do support most (11 of the 13) axes, omitting attribute and namespace only.
We omit negations for reasons to be explained later.
Our subset is essentially positive core XPath, which is core XPath [16] without negations.
The abstract syntax of positive core XPath is:  II.
R ELATED WORK Our work has similarities with the work of Afanasiev et al [2], who translate the downwards fragment of XPath into the existential fragment of Computation Tree Logic (CTL) [11], using the nuSMV model checker as the query engine.
The differences include: (1) we are interested in trace semantics, whereas Afanasiev et al work with the standard semantics for answer sets; (2) our method of model building is orders of magnitude more efficient, and (3) we support all navigational axes, not just the downward axes.
The efficiency of our method is mainly due to the judicious use of Embedded C code support provided by the SPIN model checker.
Since SPIN supports LTL (and not CTL), we represent XPath queries using LTL, rather than CTL.
While using the latter is more intuitive, we believe that our LTL rendering of XPath is still relatively simple.
Benedikt et al [3], and Marx [25], [24] study the expressive power of various fragments of XPath, including positive core XPath by embedding in various logics.
XML based access control offers three fundamental choices [23].
Should the XML data be filtered according to the access control policy: (1) before a query is applied, (2) after the query is applied, or (3) should the query be rewritten?
Security views are an example of case (1).
However, security views are expensive to compute and to maintain, which is why Fan et al [8] propose a method of avoiding to build  X aA X || X | / X | X / X | X[Q] | A :: L QaAX A aA self | child | descendant | descendant or self | parent | ancestor | ancestor or self | preceding sibling | following sibling | preceding | following The node test in a step is restricted to a name test (i.e.
kind tests are not supported, which is consistent with the use of undecorated XML data).
We use location paths X by way of predicates Q.
A. Disjunction A typical answer contains several results.
Hence we should expect the trace semantics of a query to be a set of traces.
The semantics of the || operator applied to two queries is therefore the union of the traces returned for each query separately.
2  b) Example 2: Consider query2 below, which in the standard semantics yields an answer set consisting of Cain and Seth: query2 = descendant :: "*" [child :: Enoch a" child :: Enosh]  Therefore we can dispense with the aSS operator also, as repeated use of predicates can achieve the desired effect [3, Proposition 2].
Using the fact that propositions with a", aSS and AZ can always be written in conjunctive normal form [22], we can also remove all nested conjunctions and disjunctions.
The standard semantics for XPath prescribes that the result of the predicate should be a Boolean.
In our interpretation we take an empty trace to mean false and a non-empty trace to represent true [33].
However, we should also like to preserve the traces resulting from the predicate, because all visited nodes must be kept for further analysis.
This leads to the idea that the result should consist of two traces, both with an initial segment corresponding to descendant :: "*".
Then the traces differ: one contains the trace corresponding to the left hand side of the a" operator, and the other takes care of the right hand side.
In both cases a common trailing segment follows.
The initial and trailing segment are effectively copied and concatenated to each intermediate segment, yielding the following result:  C. Negation Negation is a problem because it is unclear what trace to return for a negated predicate.
Assume first that predicates are in conjunctive normal form, and that all occurrences of aSS and a" have been removed as described above.
Then there are only atomic propositions and negated atomic propositions left.
d) Example 4: Consider query4 , which in the standard semantics returns the singleton answer set {Root}: query4 = descendant or self :: "*"[AZ parent :: "*"] The question now is: which traces(s) to return for the predicate?
(a) Should it be all possible traces that do not satisfy the predicate?
This would be infinitely many with the 11 axes of XPath!
(b) Or should the trace be empty?
This would jeopardise our ability to analyse the trace properly: Consider our family tree again with a query asking for brothers not involved in fratricide.
Should the query return {Seth}?
Or should it return an empty answer set because it violates the Chinese wall policy?
(c) The most likely possibility is to label segments of the trace that correspond to negated steps, so that these can be distinguished from positive steps in the analysis.
This, however, we leave as future work and for now omit negation.
Note that often in policy specifications the same approach is taken: what is not explicitly allowed is forbidden, hence negative steps are not always necessary [19].
However, see Sec.
VIII for an experiment with alternative (a) in SPIN.
This concludes the motivation of the positive core XPath subset.
results2 = {[Root, Adam, Cain, (Enoch), Cain], [Root, Adam, Seth, (Enosh), Seth]} With this acopyinga semantics in mind the a" and || operators are identified, thus obviating the need for a separate a" operator [3, Proposition 2] and so that query2 is interpreted as: query20 = descendant :: "*" [child :: Enoch || child :: Enosh] B.
Conjunction The trace semantics of a location path with a predicate is the concatenation of the trace of the location path and the trace of the predicate.
c) Example 3: Consider query3 , which in the standard semantics returns the singleton answer set {Adam}:  IV.
XML DATA REPRESENTATION  query3 = descendant :: "*" [child :: Cain aSS child :: Abel]  XPath queries operate on an appropriate representation of the data that we assume to be bulk loaded; dealing with updates and inserts is beyond the scope of the paper.
To provide efficient support for the 11 axes in queries, a representation of the XML data is needed that is slightly more sophisticated than a tree.
Fig.
1 shows the navigational representation that we adopt.
The four types of edges shown are p for parent, c for child, r for immediate following sibling and l for immediate preceding sibling.
All other axes (except self) are supported by traversing more than one edge.
The nodes of the graph are represented by a given set N, which in the case of our running example is:  The resulting trace should contain an initial segment corresponding to the location path descendant :: "*".
However for the predicate to succeed we must be sure that there is at least one non-empty trace corresponding to the left hand side of the aSS operator as well as a non-empty trace corresponding to the right hand side.
Both non-empty traces must be returned as part of the full trace, which we achieve by concatenating the results.
We have arbitrarily chosen to concatenate the right hand side trace onto the left hand side trace; interleaving or reordering would also be possible but this is subject to further work.
In all cases a trailing segment will follow.
The result then becomes:  N aA Root | Adam | Cain | Enoch | Abel | Seth | Enosh  result3 = [Root, Adam, (Cain), (Abel), Adam] The edges are represented by four functions, one for each type of edge (i.e.
upd , downd , leftd , and rightd ).
In addition we need a function to return to the root (rootd ), as well as a function to stay put (hered ).
We show only the definition of downd , the remaining functions are similar.
This, however, is exactly the trace that would be returned by the following query: query30 = descendant :: "*" [child :: Cain][child :: Abel] 3  O  O  O  We now have all ingredients to show the Kripke structure of our running example D below.
Here the function D(n, d, s) defines the set of all possible successor states.
For example the successor state of (n, Push, s) consists of the set of states (n, d0 , n : s), where d0 ranges over all possible 9 directions in D, and where n : s represents the current stack extended with the current node.
Summarising, the interpretation of state (n, d, s) is: we are now at node n going in the direction d, with current stack s.  n, Up n,< Push gg3 n, Stop n, Start kXXX n, Here O XXXXX b g XXXXX ggggg X ! }
ggggg o / n PP WW / n, Right n, Left o PPPWWWWWW mmm m m W P' WWWW+ vmm  n, Pop0 n, Popm n, Down0 n, Downk     Fig.
2.
State n showing the nine possible directions for reaching a successor state.
MIa aA ({Ia}, Iaa{Ia}, Iaa{L}) D :: M S D = ({(n, d, s) | naN aSS daD aSS saN}, D, It) where D(n, Start, s) = {(rootd n, d0 , s) | d0 aD} D(n, Here, s) = {(n, d0 , s) | d0 aD} D(n, Up, s) = {(n0 , d0 , s) | n0 aupd n aSS d0 aD} D(n, Down, s) = {(n0 , d0 , s) | n0 adownd n aSS d0 aD} D(n, Left, s) = {(n0 , d0 , s) | n0 aleftd n aSS d0 aD} D(n, Right, s) = {(n0 , d0 , s) | n0 arightd n aSS d0 aD} D(n, Push, s) = {(n, d0 , n : s) | d0 aD} D(n, Pop, n0 : s) = {(n0 , d0 , s) | d0 aD} D(n, Pop, []) = {} D(n, Stop, s) = {} It(n, d, s) = {n, d}  downd :: Na{N} downd (Root) = {Adam} downd (Adam) = {Cain, Abel, Seth} downd (Cain) = {Enoch} downd (Seth) = {Enosh} = {} downd We follow the approach of the work cited at the beginning of Sec.
III to focus purely on the tags of XML data, omitting all other information, so that this concludes the presentation of our representation of an undecorated XML tree.
V. K RIPKE STRUCTURE  We have tacitly assumed here that all nodes in the tree have a unique tag.
If this is not the case, the Kripke structure must be extended with a unique identifier for each node.
We will ensure that this is the case in the high performance SPIN models.
This concludes the presentation of the Kripke structure so that we can turn our attention to the translation of positive core XPath into LTL.
Before we can give the translation of XPath into LTL we must develop a Kripke structure for the resulting logic.
The structure is based on the definition of two sets, N, given earlier to represent the nodes, and D to represent the directions corresponding to the axes (Here for self, Up for parent, Down for child, Left for immediate preceding sibling, and Right for immediate following sibling) as well as a further four directions (Start, Stop, Push, and Pop) to be discussed shortly.
VI.
T RANSLATION OF POSITIVE CORE XPATH INTO LTL  D aA Start | Here | Up | Down | Left | Right | Push | Pop | Stop  The function Tx below translates an XPath query into an LTL formula.
The function takes a query as its first argument, and an LTL formula D which represents what should happen after we have dealt with the query.
Consider for example the first clause of Tx .
Since D represents what happens after xp1 ||xp2 , D must happen after xp1 as well as xp2 .
This corresponds to the acopyinga semantics alluded to in the introduction.
Consider also the second clause, which states that for an absolute query /xp we go from the current node in the Start direction, leading to the node Root in the next (X) step.
Then we continue with xp, ultimately followed by D. The remaining clauses are intended to be self explanatory.
Fig.
2 shows all nodes in the Kripke structure that correspond to a single node of an XML tree.
This representation is quadratic in the number of nodes of the original XML tree, which is clearly inefficient.
We will come back to this issue in Sec.
VIII, but we need to make the situation worse first by considering how to deal with predicates.
Referring back to the introduction, we saw that predicate yields a trace segment that returns to the starting node of the segment, to linearise a finite branching structure.
To support this we need a stack of nodes in the Kripke structure.
The states of the Kripke structure are defined by the triple S below, where N represents the stack (i.e.
a list of nodes):  Tx :: XaTaT Tx [[xp1 || xp2 ]]D = Tx [[xp1 ]]D a" Tx [[xp2 ]]D Tx [[/ xp]]D = Start aSS X(Root aSS Tx [[xp]]D) Tx [[xp1 / xp2 ]]D = Tx [[xp1 ]](Tx [[xp2 ]]D) Tx [[xp1 [xp2 ]]]D = Tx [[xp1 ]](Push aSS X(Tx [[xp2 ]](Pop aSS X D))) Tx [[a :: l]]D = Ta [[a]]( l aSS D)  S aA (N, D, N) N aA [N] Given an XML tree with n nodes, and a query with predicates nested to a depth of d, the state space in the worst case grows as n(d+1) .
In practice the state space remains small as we shall see later (Sec.
VIII).
4  The function Ta below follows the same pattern as Tx .
The first argument is an axis and the second argument D corresponds to the query that must be matched after the current axis has been matched.
For example the first clause states that the proposition Here must be true in the current state, and that D must hold in the next state.
Also note the difference between descendant and descendant or self.
In the former we check first that a move in the direction Down can be made, optionally followed by a further sequence of moves in the Down direction until finally a state is found in which D is true.
In the latter case we accept either a move to the current node (direction Here) or the moves implied by the axis descendant.
The cases for the remaining axes are expected to be self explanatory.
The trace result6 is a model for the LTL formula ltl6 with respect to the given Kripke structure: result6 |= ltl6 2  Ta :: AaTaT Ta [[self]]D = Here aSS X D Ta [[child]]D = Down aSS X D Ta [[parent]]D = Up aSS X D Ta [[descendant]]D = Down aSS X(Down U D) Ta [[ancestor]]D = Up aSS X(Up U D) Ta [[descendant or self]]D = Ta [[self]]D a" Ta [[descendant]]D Ta [[ancestor or self]]D = Ta [[self]]D a" Ta [[ancestor]]D Ta [[following sibling]]D = Right aSS X(Right U D) Ta [[preceding sibling]]D = Left aSS X(Left U D) Ta [[following]]D = Up U(Right aSS X(Right U(Down U D))) Ta [[preceding]]D = Up U(Left aSS X(Left U(Down U D))) A.
Examples of the translation We present some examples of the translation.
e) Example 5: Query5 delivers the traces from the current context node to a child with tag Adam.
There is one such trace from the Root.
query5 = child :: Adam ltl5 = Tx [[query5 ]] Stop = Down aSS X( Adam aSS Stop) result5 = [(Root, Down, []), (Adam, Stop, [])]  ltl1  g) Example 7: Query7 cannot be matched because the Root is not a proper descendant of itself.
query7 =descendant :: Root ltl7 =Tx [[query7 ]] Stop = Down aSS X( Down U( Root aSS Stop)) results7 ={} h) Example 1 revisited: We now revisit query1 to demonstrate how predicates are translated.
= Tx [[query1 ]] Stop = Down aSS X( Down U( Push aSS X( Right aSS X( Right U( Pop aSS X Stop))))) results1 = {[(Root, Down, []), (Adam, Down, []), (Cain, Push, []), (Cain, Right, [Cain]), (Abel, Right, [Cain]), (Seth, Pop, [Cain]), (Cain, Stop, [])], [(Root, Down, []), (Adam, Down, []), (Cain, Push, []), (Cain, Right, [Cain]), (Abel, Pop, [Cain]), (Cain, Stop, [])], [(Root, Down, []), (Adam, Down, []), (Abel, Push, []), (Abel, Right, [Abel]), (Seth, Pop, [Abel]), (Abel, Stop, [])]} As expected, all traces of the set results1 are models for the LTL V formula ltl1 with respect to the given Kripke structure, i.e.
: {r |= ltl1 | raresults1 } 2 This concludes the translation of positive core XPath into LTL.
VII.
NATURAL SEMANTICS FOR XPATH  The LTL translation ltl5 and the trace result5 satisfy: result5 |= ltl5 2 f) Example 6: The longer query6 delivers the traces from the current context node to a descendant with tag Adam, then to a child Seth, then to a preceding sibling Abel.
There is one such trace from the Root.
query6 =descendant :: Adam / child :: Seth / preceding sibling :: Abel / preceding sibling :: Cain ltl6 =Tx [[query6 ]] Stop = Down aSS X( Down U( Adam aSS Down aSS X( Seth aSS Left aSS X( Left U( Abel aSS Left aSS X( Left U( Cain aSS Stop))))))) result6 =[(Root, Down, []), (Adam, Down, []), (Seth, Left, []), (Abel, Left, []), (Cain, Stop, [])]  Borrowing ideas from Wadleras work [34], the semantics of positive core XPath below defines a relation between a trace and a query on the left hand side and a trace on the right hand side.
The trace on the left hand side is the end point of the current trace, from which the current (context) node can be found.
Consider for example the rule [abs] for absolute queries.
The endpoint of the current trace is (x, aL), where x is the current context node, and the direction in which to go is yet unknown (aL).
The premise of the rule asserts that the relative query xp started at the Root yields a trace xs0 , where the direction taken from the Root will be known (i.e.
6= aL).
The right hand side of the conclusion prepends the state (x, Start) to xs0 , to account for the fact that now we know in which direction to proceed from the original, initial node x.
We hope that the remaining clauses are self explanatory.
(As usual we omit explicit coercions, for example using the : operator for the concatenation of traces and traces, traces and elements etc.).
5  go :: Da(Na{N})a(Pa{P}) go d f(x, aL) = {(x, d) : (y, aL) | yaf x} herep :: Pa{P} herep = go Here hered downp , upp :: Pa{P} downp = go Down downd upp = go Up upd leftp , rightp :: Pa{P} leftp = go Left leftd rightp = go Right rightd  aA [(N, D)] :: hP, XiaP h(x, aL), xp1 i a xs0 1 [bar ] h(x, aL), xp1 || xp2 i a xs0  P a  [bar ]  h(x, aL), xp2 i a xs0 h(x, aL), xp1 || xp2 i a xs0  [abs]  h(Root, aL), xpi a xs0 h(x, aL), / xpi a (x, Start) : xs0  [slash]  h(x, aL), xp1 i a xs0 : (x0 , aL), h(x0 , aL), xp2 i a xs00 h(x, aL), xp1 / xp2 i a xs0 : xs00  2  [pred]  [step]  The function horizontalp is used by the axes preceding and following to discover trace segments corresponding to the nodes that precede the current node in XML document order.
horizontalp :: (Pa{P})a(Pa{P}) horizontalp fp = hhc a"p ((upp +p ) aSSp hhc) where h = fp +p hhc = h a"p (h aSSp (downp +p ))  h(x, aL), xp1 i a xs0 : (x0 , aL), h(x0 , aL), xp2 i a xs00 : (x00 , aL) h(x, aL), xp1 [xp2 ]i a (xs0 : (x0 , Push) : xs00 : (x00 , Pop) : (x0 , aL) xs0 : (x0 , aL)aPa [[a]](x, aL) h(x, aL), a :: li a xs0 : (x0 , aL), if l = "*"a"l = x0  Finally we need three operators (+p , aSSp , and a"p ) to glue trace segments together.
+p :: (Pa{P})a(Pa{P}) r +p = r a"p (r aSSp r +p ) aSSp , a"p :: (Pa{P})a(Pa{P})a(Pa{P}) (r aSSp q)(x, aL) = {ys : zs | ys : (y, aL)ar(x, aL)aSS zsaq(y, aL)} (r a"p q)(x, aL) = r(x, aL) aS q(x, aL)  The semantic function Px below provides a convenient interface to the natural semantics.
Px :: Xa(Pa{P}) Px [[xp]][(x, aL)] = {xs0 : (x0 , Stop) | xs0 : (x0 , aL)a h(x, aL), xpi a}  This concludes the presentation of the Natural semantics of positive core XPath.
The rule [step] relies on the function Pa below to deal with the 11 axes of XPath.
VIII.
SPIN ENGINE We now present two ways of representing the Kripke structure as an explicit state model for SPIN to show that in practical cases, the state space does not grow as in the worst case.
Pa :: Aa(Pa{P}) Pa [[self]] = herep Pa [[child]] = downp Pa [[parent]] = upp Pa [[descendant]] = downp +p Pa [[ancestor]] = upp +p Pa [[descendant or self]] = Pa [[self]] a"p Pa [[descendant]] Pa [[ancestor or self]] = Pa [[self]] a"p Pa [[ancestor]] Pa [[following sibling]] = rightp +p Pa [[preceding sibling]] = leftp +p Pa [[following]] = horizontalp rightp Pa [[preceding]] = horizontalp leftp  A.
Pure Promela Model The Promela model below is an optimised representation of the Kripke structure of Sec.
V. The state consists of an mtype declaration introducing the nodes and directions, and three variables tag, dir, and stack representing the current tag, direction of travel, and stack.
mtype={ Root, Adam, Cain, Enoch, Abel, Seth, Enosh, Start, Here, Up, Down, Left, Right, Push, Pop, Stop }; mtype tag=Root; mtype dir=Down; byte stack=0;  The function Pa in turn relies on a number of functions below to calculate the possible traces from the current node (again found in the endpoint of the current trace) in the direction indicated by the axis.
For example downp with a current node x yields a set of segments [(x, Down), (y, aL)] where y ranges over all children of node x, as defined by the function downd of Sec.
IV.
The result of downd is empty if node x has no children.
The XML tree is built using a series of macros node.
.
..
The first parameter is the node number as shown in Fig.
1, the second the tag and the remaining parameters are the node numbers of the parent, children, and the nodes immediately to the left and the right.
6  Code has fewer control states (106 versus 493 of the Pure Promela model) but it has more data states.
init{ nodeR(0,Root,1); node3(1,Adam,0,2,4,5); node1r(2,Cain,1,3,4); node0(3,Enoch,2); nodel0r(4,Abel,1,2,5); nodel1(5,Seth,1,4,6); node0(6,Enosh,5); end: skip }  short tag ; byte dir ; int stack ; c_state "Nodeptr ptr" "Global" short nr ;  The init process below consists of the initialization where the C code which parses the XML tree is called.
This is followed by a do statement with a non-deterministic choice for each of the nine directions, except for the DOWN direction, which has more cases to support nodes with many children efficiently.
We show the cases for the direction Up and one of the cases for Down, the remaining cases are similar.
We do not give the definitions of the macros as these are largely repetitive.
Instead we show the expansion of the node with tag Adam.
Starting at label s1, where 1 is the node number of Adam, there is a non-deterministic choice leading to all possible successor states of s1.
Promela does not offer a acomputed gotoa, so this has to be simulated for popping the stack.
Promela models must be finite.
Therefore, we limit the stack depth to 2, supporting a nesting level of 2 for predicates.
(Using qualifier flattening [29] a nesting depth of 1 would be sufficient).
s1: :: :: :: :: :: :: :: :: :: :: :: fi  init { ... Initialisation calling XML parser ... do :: d_step{ c_expr{ now.ptr->parent != NULL } -> c_code{ now.tag = now.ptr->tag ; now.dir = Up ; now.ptr = now.ptr->parent ; } } :: d_step{ c_expr{ now.ptr->sz > 0 } -> c_code { now.tag = now.ptr->sym ; now.dir = Down ; now.ptr = now.ptr->child[0] ; } } ... Other cases ... }  if d_step{ tag=Adam; dir=Start }; goto s0 d_step{ tag=Adam; dir=Here }; goto s1 d_step{ tag=Adam; dir=Up }; goto s0 d_step{ tag=Adam; dir=Down }; goto s2 d_step{ tag=Adam; dir=Down }; goto s4 d_step{ tag=Adam; dir=Down }; goto s5 d_step{ tag=Adam; dir=Push; stack=(stack<<4)|1 }; goto s1 d_step{ (stack&15)==0 -> tag=Adam; dir=Pop; stack=(stack>>4) }; goto s0 ... d_step{ (stack&15)==6 -> tag=Adam; dir=Pop; stack=(stack>>4) }; goto s6 d_step{ tag=Adam; dir=Stop }; goto end ;  With the Kripke structure in place all that remains is to add the never claim generated by SPIN for the LTL formula that represents the query.
The never claim specifies undesirable behaviour and SPIN will try to find a counter example.
Therefore every counter example represents a match of the query, showing the details of the trace as required.
B. Promela model with Embedded C code Promela provides facilities to embed C code in the model [21].
We use this facility to separate parsing an XML file, and building an in-memory data structure in C on the one hand from the query processing with SPIN on the other hand.
We use the eXpat library to parse the XML data [9].
The in-memory data structure follows the navigational format as shown in Fig.
1 and in the Kripke structure.
For each node in the tree we malloc() a node with the appropriate number of children using the following C type definition:  C. Performance We discuss the performance of the pure Promela model first, and then compare the performance of the Promela model to that of state of the art XPath query engines.
All our performance figures apply to a Sun SPARC Ultra-Enterprise Server running SunOS 5.8. i) Pure Promela: The number of control states defined by our running example D from Sec.
V is 493.
The number of data states defined by tag, dir and stack is at least 7 A 9 A 72 = 3087.
Multiplied by the number of control states, this yields over 1.5 M states.
However a small percentage of these states is explored, as is shown in the second row of Table I.
The columns correspond to the seven example queries discussed earlier in the paper.
The first row shows the query number, the second shows the number of states stored to find at least one trace.
(SPIN does not guarantee to find all traces.
typedef struct node* Nodeptr ; typedef struct node { int tag ; int sz ; /* Number of children */ Nodeptr parent, left, right ; Nodeptr child[sz] ; } Node ;  The state of the Embedded C Promela model consists of five variables, where tag, dir, and stack are as in the pure Promela model.
The added variable ptr points at the Node to which we are moving, and nr is used to index the appropriate child.
The Promela model with Embedded C 7  SPIN version pure embedded C  1 114 36  2 189 91  query1...7 3 4 5 186 36 18 46 7 5  6 102 58  7 52 41  10000000  TABLE I SPIN  1000000  PERFORMANCE FOR THE FAMILY TREE EXAMPLE IN TERMS OF THE NUMBER OF STATES STORED .
100000  10000  For query2 SPIN returns all (2) traces, but in the case of query1 only one of the three traces is found.)
The presence of data for query4 in Table I is due to the fact that we translate the negated predicate into a negated LTL formula thus:  1000  100 0.01  Tx [[xp1 [AZ xp2 ]]]D = Tx [[xp1 ]](AZ(Push aSS X(Tx [[xp2 ]](Pop aSS X D))))  0.02  0.05  0.1  0.2  0.5  1  Fig.
3.
MacMill query times (AlSec) as a function of the XMark f parameter.
This means that SPIN will try to discover infinitely many counter examples, which for the purpose of this experiment has been capped at 100. j) Promela with Embedded C Code: The Promela model with embedded C code performs better than the pure Promela model, because only the work relevant for the query processing is exposed to the model checker, the rest is hidden in the C code.
The number of states explored is shown in the second row of Table I.
The runtime of the query processing is not interesting since the XML tree corresponding to D (Sec.
V) is tiny.
k) Comparison with XML Task Force and MacMill: The third experiment repeats and extends the experiments of Afanasiev et al [2], using the standard XMark XML benchmark as the data base [31], with MacMill [7] and the XML taskforce query engine [17].
The six queries of Afanasiev et al are as follows:  10000000  1000000  100000  10000  1000  100 0.01  Fig.
4.  xmark1 = / child :: site / child :: regions / child :: africa / child :: item / child :: description / child :: parlist / child :: listitem / child :: text xmark2 = / descendant :: item / child :: description / child :: parlist / child :: listitem / child :: text xmark3 = / descendant :: item / descendant :: text xmark4 = descendant :: open auction[child :: bidder] xmark5 = descendant :: item[child :: payment] [child :: location] xmark6 = descendant :: item[descendant :: payment]  0.02  0.05  0.1  0.2  0.5  1  SPIN query times (AlSec) as a function of the XMark f parameter.
Using the scaling parameter settings f = 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0, the XML files generated by XMark range in size from 1.11MB to 111MB.
For each of the 9 XPath queries and 7 XML files, we report (a) the total time taken to read and parse the XML file and to execute the query, and (b) the time to process just the query itself.
Each measurement is an average of ten experiments, with a standard deviation of 8% or less.
The query processing speed (defined as the size of the XML file divided by the time necessary to parse the XML input and to execute the query) is independent of the 9 XMark queries and the 7 data base sizes.
The processing speed of MacMill is best with an average and standard deviation of 4885 Aa 2% KB/s, for the TaskForce engine we found 2302Aa6% KB/s, and for SPIN 2173 Aa 3% KB/s.
Overall we conclude that the SPIN implementation is competitive, which, going by the processing speed is MacMill : TaskForce : SPIN = 2.3 : 1.1 : 1.0 Fig.
3 reports the pure query processing times for MacMill, and Fig.
4 for the SPIN implementation.
Each graph shows  The extension consists of the three queries below which focus on antagonist axes.
These examples originate from Grust et al [18].
xmark7 = descendant :: open auction / descendant :: description xmark8 = descendant :: age / ancestor :: person xmark9 = descendant :: open auction / child :: privacy / preceding sibling :: bidder  8  the query time as a function of the XMark f parameter.
With MacMill, the time increases with the f parameter because MacMill returns all answers, whereas SPIN stops after reporting the first answer.
For queries 4, 7, 8 and 9 SPIN performs worse than for the other queries because these queries look for tags such as open auction and age, which occur at the end of the XML tree (document order), whereas the fast queries look for tags such as item, which occur at the beginning of the tree.
Overall SPIN is faster.
The SPIN implementation could be improved by fine tuning.
On the other hand the range of XPath queries supported by the SPIN implementation is more limited than that of the MacMill and the XML task force engines.
For example, the MacMill and the TaskForce engines report the entire answer set, including decorations, whereas the SPIN implementation is not guaranteed to report the entire answer set, and then only undecorated.
The fact that SPIN does not report all answers is not a problem with the main results of the paper, i.e.
the trace semantics and its embedding in LTL.
It should also be pointed out that because we supply the translated query and the access control policy together, the answer that the SPIN implementation does produce is guaranteed to satisfy the access control policy.
Being unable to provide all answers is a problem of the implementation with SPIN.
To solve this problem with the current implementation of SPIN might require changes to the model, the translated query, and possibly the SPIN engine itself.
We leave this a future work.
On the other hand how often have you followed up all the hits that Google offers for a particular query?
Google even provides an aIam feeling luckya mode, which gives just one hit.
Therefore, there may be cases where not providing the entire answer set is acceptable.
l) Exponential time query complexity: Gottlob et al [17] report how naAaEve query processors suffer from exponential runtimes for relatively simple queries.
The XML data base of their example is:  effectively with the use of a stack.
The translation is proved correct with respect to the trace semantics.
The trace semantics provides opportunities for analysis.
We give two examples showing that enforcing access control policies amounts to model checking the conjunction of the policy and the (LTL translation of) the query.
Finally the SPIN model checker has been used as an efficient query engine, by providing it with a representation of an XML file and a never claim corresponding to the query translated into LTL.
The performance of the SPIN implementation is comparable to that of the W3C XPath Taskforce Query engine.
Our SPIN implementation represents a successful experiment in creative laziness in the sense that we use existing tools (SPIN and the eXpat parser) for a new purpose (query processing) [30].
The necessary glue consists of a small Promela model and some C code (400 lines) that enable the model checker to traverse the XML tree, and a small compiler from XPath expressions into LTL (17 lines of Haskell).
By comparison MacMill is 7Kloc.
Our SPIN implementation has some undesirable features.
In particular it stops after reporting one trace, and each new query must be compiled.
This makes the current implementation unsuitable for practical use.
A way forward would be to build, a query engine based on based on state of the art model checking technology.
Instead of developing a tool from scratch one would use building blocks from a modular model checker and build an efficient special purpose tool with relative ease.
This opens up a spectrum of possibilities ranging from a complete implementation from scratch (MacMill), via a partial implementation using existing model checker modules (future work) to our implementation with minimal glue.
Future work includes: aV  <A> <B/> <B/> </A> n times  z }| { The queries are //a/b/ parent :: a/b.
We have repeated the experiment to ensure that the SPIN implementation shows indeed linear behaviour, which is the case.
However, to our surprise the SPIN compiler from LTL formula to never claims shows exponential runtimes, and so does the alternative compiler ltl2ba [15].
It is future work to investigate how to generate never claims directly from XPath queries.
This would avoid exponential compilation times because we could exploit the regular structure of our LTL formulae.
aV  aV  IX.
C ONCLUSIONS AND F UTURE WORK We define a novel trace semantics for positive core XPath that supports location paths, predicates, and 11 out of 13 axes.
Expressions and negation are not currently supported.
We show that positive core XPath can be translated into LTL.
The translation is based on the idea that a branching structure as induced by location paths with predicates can be linearised 9  Study the interaction of a wider class of security policies with our embedding of XPath queries into LTL, and extend the approach to embrace not only the object part of access control policies.
It would also be of interest to investigate how XPath symmetries can continue to be exploited for query optimisation without undesirable interactions with the policy.
Incorporate negation into the framework, as well as expressions and the two axes that we have omitted (namespace and attribute).
Particularly the support of link edges for id and idref should not pose technical problems and should give rise to more interesting access control applications.
Investigate the use of LTL formulae to cut down the search space by (1) compiling the query into more coarse grained filters, which can then be combined efficiently with the actual query, or (2) short circuiting the LTL compilation of the query expression using projection techniques, (3) using path equivalences to simplify the XPath expressions, or (4) adding further edges to shortcut recursive searches.
ACKNOWLEDGEMENTS  [19] J. Y. Halpern and V. Weissman.
Using First-Order logic to reason about policies.
In 16th IEEE Computer Security Foundations Workshop (CSFW), pages 187a201, Pacific Grove, California, Jun 2003.
IEEE Computer Society Press, Los Alamitos, California.
[20] G. J. Holzmann.
The SPIN Model Checker: Primer and Reference manual.
Pearson Education Inc, Boston Massachusetts, 2004.
[21] G. J. Holzmann and R. Joshi.
Model-Driven software verification.
In S. Graf and L. Mounier, editors, 11th Int.
SPIN Workshop: Model Checking Software, volume LNCS 2989, pages 76a91, Barcelona, Spain, Apr 2004.
Springer-Verlag Heidelberg.
[22] M. Huth and M. Ryan.
Logic in Computer Science.
Cambridge University Press, UK, 2004.
[23] B. Luo, D. Lee, W.-C. Lee, and P. Liu.
QFilter: Fine-Grained RunTime XML access control via NFA-based query rewriting.
In 13th Conf.
on Information and Knowledge Management (CIKM), pages 543a552, Washington D. C., Nov 2004.
ACM Press, New York.
[24] M. Marx.
Conditional XPath, the first order complete XPath dialect.
In 23rd Principles of Database Systems (PODS), pages 13a22, Paris, France, Jun 2004.
ACM Press, New York.
[25] M. Marx.
XPath with conditional axis relations.
In E. Bertino, S. Christodoulakis, D. Plexousakis, V. Christophides, M. Koubarakis, K. BoEhm, and E. Ferrari, editors, 9th Int.
Conf.
on Extending Database Technology (EDBT), volume LNCS 2992, pages 477a494, Heraklion, Crete, Greece, Mar 2004.
Springer-Verlag, Berlin.
[26] M. Marx and M. de Rijke.
Semantic characterizations of navigational XPath.
Technical report, Univ.
of Amsterdam, 2004.
[27] G. Miklau and D. Suciu.
Controlling access to published data using cryptography.
In J. C. Freytag, P. C. Lockemann, S. Abiteboul, M. J. Carey, P. G. Selinger, and A. Heuer, editors, 29th Int.
Conf.
on Very Large Data Bases (VLDB), pages 898a909, Berlin, Germany, Sep 2003.
Morgan Kaufmann.
[28] M. Murata, A. Tozawa, M. Kudo, and S. Hada.
XML access control using static analysis.
In 10th ACM conference on Computer and communication security, pages 73a84, Washington D. C., 2003.
ACM Press, new York.
[29] D. Olteanu, H. Meuss, T. Furche, and F. Bry.
XPath: Looking forward.
In A.
B. Chaudhri, R. Unland, C. Djeraba, and W. Lindner, editors, XML-Based Data Management and Multimedia Engineering (EDBT), volume LNCS 2490, pages 109a127, Prague, Czech Republic, Mar 2002.
Springer-Verlag, Heidelberg.
[30] T. C. Ruys.
Optimal scheduling using branch and bound with SPIN 4.0.
In T. Ball and S. K. Rajamani, editors, 10th Int.
SPIN Workshop on Model Checking Software, volume LNCS 2648, pages 1a17, Portland, Oregon, May 2003.
Springer-Verlag, Berlin.
[31] A. R. Schmidt, F. Waas, M. L. Kersten, M. J. Carey, I. Manolescu, and R. Busse.
XMark: A benchmark for XML data management.
In 28th Int.
Conf.
on Very Large Data Bases (VLDB), pages 974a985, Hong Kong, Aug 2002.
VLDB Endowment Inc. [32] Ph.
Schnoebelen.
The complexity of temporal logic model checking.
In Ph.
Balbiani, N.-Y.
Suzuki, F. Wolter, and M. Zakharyaschev, editors, Selected Papers from the 4th Workshop on Advances in Modal Logics (AiMLa02), pages 393a436, Toulouse, France, Sep 2002.
Kingas College Publication, London.
[33] P. L. Wadler.
How to replace failure by a list of successes, a method for exception handling, backtracking and pattern matching in lazy functional languages.
In J.-P. Jouannaud, editor, 2nd Functional programming languages and computer architecture (FPCA), volume LNCS 201, pages 113a128, Nancy, France, Sep 1985.
Springer-Verlag, Berlin.
[34] P. L. Wadler.
Two semantics for XPath.
Technical note, Dept.
of Comp.
Sci, Univ.
of Edinburgh, Jan 2000.
Loredana Afanasiev and Massimo Franceschet provided their CTL benchmark and commented on the approach.
Sandro Etalle suggested using multi-lateral security as a motivating example.
Gerard Holzmann and Theo Ruys answered many SPIN questions.
Theo Ruys and Maurice van Keulen commented on a draft of the paper.
Christoph Koch provided MacMill.
R EFERENCES [1] M. Abadi.
Logic in access control.
In 18th Annual IEEE Symp.
on Logic in Computer ScienceC (LICS), pages 228a233, Ottawa, Canada, Jun 2003.
IEEE Computer Society Press, Los Alamitos, California.
[2] L. Afanasiev, M. Franceschet, M. Marx, and M. de Rijke.
CTL model checking for processing simple XPath queries.
In 11th Int.
Symp.
on Temporal Representation and Reasoning (TIME), pages 117a124, Tatihou, France, Jul 2004.
IEEE Computer Society Press, Los Alamitos, California.
[3] M. Benedikt, W. Fan, and G. M. Kuper.
Structural properties of XPath fragments.
In D. Calvanese, M. Lenzerini, and R. Motwani, editors, 9th International Conference on Database Theory (ICDT), volume LNCS 2572, pages 79a95, Siena, Italy, Jan 2003.
Springer-Verlag, Berlin.
[4] E. Bertino and S. Castano.
Securing XML documents with Author-X.
IEEE Internet Computing, 5(3):21a31, May 2001.
[5] E. Bertino, S. Castano, E. Ferrari, and M. Mesiti.
Specifying and enforcing access control policies for XML document sources.
World Wide Web, 3(3):139a151, 2000.
[6] D. F. C. Brewer and M. J. Nash.
The chinese wall security policy.
In 10th IEEE Symposium on Security and Privacy (S&P), pages 1a3, Oakland, California, May 1989.
IEEE Computer Society, Washington, DC.
[7] P. Buneman, M. Grohe, and Ch.
Koch.
Path queries on compressed XML.
In J. C. Freytag, P. C. Lockemann, S. Abiteboul, M. J. Carey, P. G. Selinger, and A. Heuer, editors, 29th Int.
Conf.
on Very Large Data Bases (VLDB), pages 141a152, Berlin, Germany, Sep 2003.
Morgan Kaufmann.
[8] C.-Y.
Chan and M. Garofalakis.
Secure XML querying with security views.
In G. Weikum, A. Christian KoEnig, and S. DeAloch, editors, SIGMOD Int.
Conf.
on Management of Data, pages 587a598, Paris, France, Jun 2004.
ACM Press, New York.
[9] J. Clark.
Expat XML Parser.
Open Software Technology Group, Fremont, California, Jul 2004.
[10] J. Clark and S. DeRose (eds.).
XML Path Language (XPath Version 1.0).
W3C, Nov 1999.
[11] E. M. Clarke, O. Grumberg, and D. A. Peled.
Model Checking.
The MIT Press, Cmabridge, Massachusetts, 1999.
[12] L. de Alfaro.
Model checking the world wide web.
In G. Berry, H. Comon, and A. Finkel, editors, 13th Int.
Conf.
on Computer Aided Verification (CAV), volume LNCS 2102, pages 337a349, Paris, France, Jul 2001.
Springer-Verlag, Berlin.
[13] X. Fu, T. Bultan, and J. Su.
Analysis of interacting BPEL web services.
In 13th conf.
on World Wide Web, pages 621a630, New York, NY, USA, 2004.
ACM Press, New York.
[14] I. Fundulaki and M. Marx.
Specifying access control policies for XML documents.
In 9th ACM Symp.
on access control models and technologies, pages 61a69, IBM, Yorktown Heights, USA, Jun 2004.
ACM Press, New York.
[15] P. Gastin and D. Oddoux.
Fast LTL to BuEchi automata translation.
In G. Berry, H. Comon, and A. Finkel, editors, 13th Int.
Conf.
on Computer Aided Verification (CAV), volume LNCS 2102, pages 53a65, Paris, France, Jul 2001.
Springer-verlag, Berlin.
[16] G. Gottlob, C. Koch, and R. Pichler.
XPath processing in a nutshell.
SIGMOD Rec., 32(2):21a27, Jun 2003.
[17] G. Gottlob, Ch.
Koch, and R. Pichler.
Efficient algorithms for processing XPath queries.
In 28th Int.
Conf.
on Very Large Data Bases (VLDB), pages 95a106, Hong Kong, China, Aug 2002.
VLDB Endowment Inc. [18] T. Grust, M. Van Keulen, and J. Teubner.
Accelerating XPath evaluation in any RDBMS.
ACM Trans.
Database Syst., 29(1):91a131, Mar 2004.
10  p) Lemma II: Given a function fp a {herep , upp , downp , leftp , rightp }, then for every function gp a {fp , fp +p , horizontalp fp }, every node x a N, and trace xs a P such that: xs a gp (x, aL)  The appendices are included for the convenience of the reviewers, they will not be part of the final paper.
A PPENDIX A a L INEAR T EMPORAL L OGIC We summarise the syntax and semantics of LTL here to make the paper self contained.
m) Syntax.
: We use the fragment of temporal logic below, with proposition symbols L drawn from the sets N and D.  there is a direction d a D, a (possibly empty) trace xs0 a P, and a node x0 a N such that: xs = (x, d) : xs0 : (x0 , aL) Proof: by induction on the length of the trace xs.
2 q) Lemma III: For every trace ys, zs a P, node y, y0 a N, direction d, d0 a D, axis a a A, and LTL formula D we have that if:  TaAT | F | L | AZ T | T aSS T | T a" T | T a T | XT|2T|3T|TUT n) Semantics.
: The semantics for finite traces is:  (y, d) : ys : (y0 , aL) a Pa [[a]](y, aL)  |= :: [Ia]aTaB [] |= = False xs |= T = True xs |= F = False (x : xs) |= l = l a It x xs |= AZ D = AZ(xs |= D) xs |= D aSS D = xs |= DaSSxs |= D xs |= D a" D = xs |= Da"xs |= D xs |= D a D = xs |= (AZ D) a" D (x : xs) |= X D = xs |= D xs |= 2 D = xs |= AZ 3(AZ D) xs |= 3 D = xs |= T U D xs |= D U D = True, if xs |= D = xs |= X(D U D), if xs |= D = False, otherwise  and: (y0 , d0 ) : zs |= D then: (y, d) : ys : (y0 , d0 ) : zs |= Ta [[a]]D Proof: by case analysis on the structure of a.
2 r) Lemma IV: For every node y, y0 a N, query xp a X, direction d, d0 a D, trace ys, zs a P, and LTL formula D we have that if: h(y, aL), xpi a (y, d) : ys : (y0 , aL) and: (y0 , d0 ) : zs |= D  Here the It function is defined in the Kripke structure (See Sec.
V).
then: (y, d) : ys : (y0 , d0 ) : zs |= Tx [[xp]]D  A PPENDIX B a C ORRECTNESS To prove the correctness of the translation Tx with respect to the semantics Px we must have that for every query xp, trace ys and context node x holds:  Proof: by induction on the shape of the derivation tree for .
.
.
a .
.
.. s) Case bar1 :: Assume that the conclusion holds because the premise holds.
Then by Lemma I we have:  ys a Px [[xp]][(x, Start)] implies ys |= Tx [[xp]] Stop  h(x, aL), xp1 i a (x, d) : xs0 : (x0 , aL) a (Assume that (x0 , d0 ) : zs |= D and apply the induction hypothesis)  The proof follows directly from lemma I, the definition of Px and Lemma IV.
o) Lemma I: For every node x a N, query xp a X, and trace xs a P such that:  (x, d) : xs0 : (x0 , d0 ) : zs |= Tx [[xp1 ]]D a (Equation for a" of |=)  h(x, aL), xpi a xs  (x, d) : xs0 : (x0 , d0 ) : zs |= Tx [[xp1 ]]D a" Tx [[xp2 ]]D  there is a direction d a D, a (possibly empty) trace xs0 a P, and a node x0 a N such that:  a (Equation for || of Tx ) (x, d) : xs0 : (x0 , d0 ) : zs |= Tx [[xp1 || xp2 ]]D  xs = (x, d) : xs0 : (x0 , aL)  2 t) Case bar2 :: The proof of this case is analogous to that of case bar1 .
Proof: by induction on the shape of the derivation tree for .
.
.
a .
.
.. 2 11  y) Lemma V: For every node y a N, direction d a D, trace xs a P, query xp1 , xp2 a X, and LTL formula D we have that if: (y, d) : xs |= Tx [[xp1 ]](Tx [[xp2 ]]D)  u) Case abs:: Assume that the conclusion holds because the premise holds.
Then by Lemma I we have that: h(Root, aL), xpi a (Root, d) : xs0 : (x0 , aL) a (Assume that (x0 , d0 ) : zs |= D and apply the induction hypothesis)  then there exists a node y0 a N, a direction d0 a D, and traces ys, zs a P such that:  (Root, d) : xs0 : (x0 , d0 ) : zs |= Tx [[xp]]D  xs = ys : (y0 , d0 ) : zs and:  a (Equations for aSS and atom of |=)  (y0 , d0 ) : zs |= Tx [[xp2 ]]D  (Root, d) : xs0 : (x0 , d0 ) : zs |= Root aSS Tx [[xp]]D  Proof: by induction on the length of xs.
2  a (Equations for X and atom of |=) (x, Start) : (Root, d) : xs0 : (x0 , d0 ) : zs |= Start aSS X( Root aSS Tx [[xp]]D) a (Equation for (unary) / of Tx ) (x, Start) : (Root, d) : xs0 : (x0 , d0 ) : zs |= Tx [[/ xp]]D 2 v) Case slash:: We omit the proof for this case, as it is a simplified version of the proof for case pred below.
w) Case pred:: Firstly, assume that: (x0 , d00 ) : zs |= D a (Equations for X and atom of |=) (x00 , Pop) : (x0 , d00 ) : zs |= Pop aSS X D Secondly, assume that the conclusion holds because the two premises hold.
Then applying Lemma I to the second premise yields: h(x0 , aL), xp2 i a (x0 , d0 ) : xs00 : (x00 , aL) a (First assumption and induction hypothesis applied to second premise) (x0 , d0 ) : xs00 : (x00 , Pop) : (x0 , d00 ) : zs |= Tx [[xp2 ]]( Pop aSS X D) a (Equations for X and atom of |=) (x0 , Push) : (x0 , d0 ) : xs00 : (x00 , Pop) : (x0 , d00 ) : zs |= Push aSS X(Tx [[xp2 ]]( Pop aSS X D)) a (Induction hypothesis applied to the first premise) (x, d) : xs0 : (x0 , Push) : (x0 , d0 ) : xs00 : (x00 , Pop) : (x0 , d00 ) : zs |= Tx [[xp1 ]](Push aSS X(Tx [[xp2 ]]( Pop aSS X D))) a (Equation for .
.
.
[.
.
.]
of Tx ) (x, d) : xs0 : (x0 , Push) : (x0 , d0 ) : xs00 : (x00 , Pop) : (x0 , d00 ) : zs |= Tx [[xp1 [xp2 ]]]D 2 x) Case step:: The proof follows immediately from Lemmas II and III.
12
This is an author-produced version of an article in the Annals of Mathematics and Artificial Intelligence (Springer), DOI 10.1007/s10472-013-9356-8 .
The final publication is available at link.springer.com .
Compositional reasoning using intervals and time reversal Ben Moszkowski  Received: 19 April 2012 / Final revision received: 26 April 2013 / Accepted: 29 April 2013 / Available online: 5 June 2013  Abstract Interval Temporal Logic (ITL) is an established formalism for reasoning  about time periods.
We investigate some simple kinds of ITL formulas which have application to compositional reasoning and furthermore are closed under conjunction and the conventional temporal operator known both as "box" and "always".
Such closures help us modularly construct formulas from simple building blocks in a way which preserves useful compositional properties.
The most important class considered here is called the 2-to-1 formulas.
They offer an attractive framework for analysing sequential composition in ITL and provide the formal basis for most of the subsequent presentation.
A key contribution of this work concerns a useful and apparently new and quite elementary mathematical theorem that 2-to-1 formulas are closed under "box".
We also use a natural form of time symmetry with 2-to-1 formulas.
This extends known facts about such formulas by looking at them in reverse.
An important example of this involves showing that 2-to-1 formulas are also closed under a variant of "box" for prefix subintervals rather than suffix ones.
We then apply the compositional formulas obtained with time symmetry to analyse concurrent behaviour involving mutual exclusion in both Peterson's algorithm and a new and more abstract one.
At present, our study of mutual exclusion mainly serves as a kind of experimental "proof of concept" and research tool to develop and illustrate some of the logical framework's promising features.
We also discuss how time symmetry sometimes assists in reducing reasoning in ITL to conventional linear-time temporal logic.
Keywords Interval Temporal Logic * compositional reasoning * formal verification * time reversal * symmetry * mutual exclusion * Peterson's algorithm  Software Technology Research Laboratory De Montfort University Leicester, UK E-mail: benm@dmu.ac.uk  2  Ben Moszkowski  1 Introduction  Intervals and discrete linear state sequences offer a compellingly natural and flexible way to model computational processes involving hardware or software.
Interval Temporal Logic (ITL) [47] is an established formalism for reasoning about such phenomena.
It has operators for sequentially combining formulas.
For example, if A and B are formulas, so are A[?
]B ("chop" ) and A[?]
("chop-star" ).
These are somewhat analogous to the concatenation and Kleene star operators for regular languages and expressions.
ITL can express some imperative programming constructs (e.g., while-loops) and has executable subsets [47].
We first summarise the primary contributions of this presentation and then discuss them in more detail: - Several classes of compositional ITL formulas which all share the important  - - - -  property that they are closed under conjunction and the conventional temporal operator "always" (2).
Various syntactic and semantic applications of time symmetry to such formulas.
Some useful techniques for compositionally manipulating a number of suitable sequential and parallel combinations of the formulas with others.
A detailed application of these ideas to mutual exclusion, including the analysis of a novel abstract algorithm as well as Peterson's well-known concrete one [64].
All of results are accompanied by rigorous, detailed mathematical theorems, lemmas and associated proofs, which are moreover themselves a quite indispensable part in the development of the framework.
Our main contribution concerns a novel categorisation and mathematical analysis of various simple classes of compositional formulas in Propositional ITL (PITL) [54, 55, 57] which are closed under conjunction and the conventional temporal operator "always" (2).
The main class we consider consists of what we call 2-to-1 formulas (which are formally defined in Sect.
4).
Briefly, a PITL formula A is defined to be 2-to-1 if the implication (A; A) [?]
A if valid, where ";" is a second variant of chop.
So if two portions of a system both ensure such a formula's behaviour, then their sequential composition is guaranteed to as well.
The 2-to-1 formulas play a quite central role in almost all of the techniques presented here.
For example, we can show that for the propositional variables p and q , the conventional temporal logic formula p [?]
3q ("if p is true in the initial state, then q is true in some state ") is 2-to-1.
Our new closure theorem immediately guarantees that the liveness formula 2(p [?]
3q ) ("whenever p is true, q is true then or later ") is 2-to-1 as well.
Such a formula is suitable for forward analysis from a state satisfying p to one satisfying q .
The many compositional properties we identify and rigorously prove clearly show that further systematic research about 2-to-1 formulas and other such classes of formulas closed under conjunction and 2, including the relationship between them, is compelling required.
We also propose here a second significant research contribution which exploits the symmetry of finite linear time to transform 2-to-1 formulas for forward analysis such as 2(p [?]
3q ) into others for backward analysis from a state to its predecessors (as described in Sects.
5 and 6).
This involves a two-stage approach.
In the first stage, our mathematical framework takes some suitable 2-to-1 formulas and views them in reverse in finite time to obtain more formulas which are 2-to-1 in finite time.
In the second stage, these formulas are then shown to even be 2-to-1 in  Compositional reasoning using intervals and time reversal  3  infinite time.
The process of transforming formulas demonstrates the significance of both syntactic and semantic forms of time symmetry.
The relationship between our use of time symmetry and some relevant earlier work using it is primarily discussed later in Sect.
16.1.
We postpone a comparison until then in order that readers will have a better understanding of our framework.
The approach here based on 2-to-1 formulas and time symmetry further develops our ITL-based compositional techniques described in [48-51] since, for example, it helps to systematically obtain additional properties for sequential composition.
Moreover, a number of results about 2-to-1 formulas and time symmetry are also applicable to the first-order version of ITL used in our earlier work, but we do not delve into this further.
We will consider a variety of relevant properties and other related categories of PITL formulas for compositional reasoning about sequential and parallel behaviour.
The main techniques here can be summarised as Introduction, Sequential combining, Extension leftward or rightward, Parallel combining and Iteration (see Sect.
4.1).
This is abbreviated with the shorthand ISEPI.
The 2-to-1 formulas and time symmetry are then applied (in Sects.
11-13) to showing by means of backward analysis the correctness of a new high-level abstract algorithm for mutual exclusion as well as the much studied one of Peterson [64].
It is first of all quite important to emphasise that the study of mutual exclusion led us in the first place to the 2-to-1 formulas and time symmetry.
However, at present, our study of mutual exclusion mainly serves as a kind of experimental "proof of concept".
It has significantly influenced the development of virtually all aspects of the presentation here and moreover helps to illustrate some of the logical framework's promising features.
Nevertheless, we do not claim that it is sufficiently  mature for practical deployment.
Readers may indeed experience some difficulties with the intuition behind some formulas.
Therefore, the material on mutual exclusion must be regarded, at least at present, as being primarily a powerful research tool for the intriguing compositional framework's evolving theory rather than a distinct and independent application on its own.
As such, it is for the moment indispensable for understanding the work.
Our presentation also shows (in Sect.
14) how time symmetry can assist in reducing satisfiability of suitable 2-to-1 formulas and some other PITL formulas to finite-time satisfiability of formulas in lower-level point-based temporal logic.
This might help provide a way to extend the scope of some algorithms, implemented software tools and mathematical techniques for conventional temporal logic to eventually include suitable subsets of PITL involving 2-to-1 formulas as well.
The proofs given about PITL formulas are semantically based and so do not use a formal axiom system.
However, an analysis could in principle include deductions in our complete axiom system for PITL with finite time [54] (see also Bowman and Thompson [10]) and our newer one with infinite time [57].
Readers new to interval-based reasoning will find the approach quite different from those using point-based temporal logics.
This applies even to our use of a conventional temporal logic formula such as 2(p [?]
3q ), when, for example, we explain why  it is 2-to-1 or use time symmetry on it.
In fact, we believe that even readers having experience with intervals will find our presentation quite novel.
They should however keep in mind that time symmetry can be rather subtle.
It requires an investment of patience and effort to be understood.
4  Ben Moszkowski  For the particular benefit of readers unfamiliar with ITL, we now briefly mention some recent publications by others which reflect current topics where ITL is being applied.
They arguably contribute to making a case for the study of ITL's mathematical foundations, which naturally include such issues as compositionality and time symmetry.
The KIV interactive theorem prover [68] has for a number of years included a slightly extended version of ITL for interactive theorem proving via symbolic execution both by itself (e.g., for concurrent algorithms and lock-free techniques [6, 7]) and also as a backend notation which supports Statecharts [78] and UML [2].
The concluding remarks of [7] note the following advantages of ITL: Our ITL variant supports classic temporal logic operators as well as program operators.
The interactive verifier KIV allows us to directly verify parallel programs in a rich programming language using the intuitive proof principle of symbolic execution.
An additional translation to a special normal form (as e.g.
in TLA [Temporal Logic of Actions [38]]) using explicit program counters is not necessary.
The Duration Calculus (DC) of Zhou, Hoare and Ravn [85] extends ITL to real-time.
Zhou and Hansen [84] give a comprehensive presentation of various aspects of DC and its application.
They include a large bibliography of literature on DC.
Olderog and Dierks' recent textbook [59] uses DC as the formal logic in a framework for seamless design flow from specification to verified implementation.
This approach also includes timed automata and automata for programmable logic controllers (PLC-automata).
Duan and his group have been investigating the theory and application of Projection Temporal Logic, an ITL extension with operators for temporal granularities and framing [13-17] (our later Sect.
13.1 gives an explanation of framing).
Some of their recent work on applications such as the specification and verification of asynchronous communication is described in [44] and [83].
Our presentation is a revised and greatly extended version of the earlier one by us in [56] that readers might benefit from because of its much briefer and more superficial format.
The focus here differs from that in [56] by concentrating more on the general compositional issues.
This is because we have subsequently come to realise that the theory of 2-to-1 formulas and related classes is much more central than its application to time symmetry, which is nevertheless quite intriguing.
As a consequence, we have now added various definitions, explanations and other material.
The topics we consider have many interesting aspects of relevance to compositional reasoning with and without time symmetry.
More recently, in [58] we present in a concise manner new techniques for systematically and incrementally elucidating connections between 2-to-1 formulas and some associated compositional classes.
These are a direct outcome of the research described here and can serve as a quick introduction to the mathematics of such classes.
However, the compositional ISEPI techniques, time symmetry and applications to mutual exclusion are not discussed in [58].
Here is our presentation's structure: - Section 2 overviews PITL.
- Section 3 presents some important point-based subsets of PITL used later on  in our analysis.
Compositional reasoning using intervals and time reversal  5  - Section 4 introduces 2-to-1 formulas, presents various kinds of them and proves that they are closed under conjunction, the temporal operator 2 ("always") as well as the time-wise symmetric operator 2f , which concerns finite prefix  -  -  -  -  - - -  subintervals instead of suffix subintervals.
As we discuss there, 2-to-1 formulas can be used for reasoning about various safety and liveness properties.
A categorisation is given of general compositional ISEPI techniques for Introduction, Sequential combining, Extension leftward or rightward, Parallel combining and Iteration.
Section 5 starts our discussion about the application to 2-to-1 formulas of both time symmetry and reductions from infinite time to finite time.
It therefore shows how to relate some of the time reversed formulas to other semantically comparable ones expressed in a version of conventional Propositional LinearTime Temporal Logic (PTL) with past time because this is much better known than PITL.
Section 6 uses time symmetry to obtain a versatile class of 2-to-1 formulas for backward analysis from other 2-to-1 formulas for forward analysis.
Such formulas are extensively used in all subsequent sections.
Sections 7-10 primarily concern versions of the various ISEPI techniques suitable for compositionally combining 2-to-1 formulas for backward analysis: - Section 7 provides ways to compositionally introduce such 2-to-1 formulas.
- Section 8 deals with a compositional technique for sequentially extending the scope of the 2-to-1 formulas for backward analysis from an interval's prefix subinterval to the entire interval.
- Section 9 concerns the parallel combining of such 2-to-1 formulas.
- Section 10 presents techniques for compositional reasoning involving the sequential iteration of these 2-to-1 formulas.
Sections 11-13 concern mutual exclusion: - Section 11 looks at an abstract mutual exclusion algorithm.
The section applies to the algorithm the results from the previous sections concerning 2-to-1 formulas for backward analysis and ISEPI techniques for sequential and parallel composition of such formulas.
- Section 12 considers in more detail an individual process in the abstract mutual exclusion algorithm and also its relation to a process in Peterson's algorithm.
- Section 13 analyses Peterson's algorithm.
This is done by formally relating it to the abstract algorithm.
Section 14 examines further reductions using time symmetry to transform some PITL formulas into ones in conventional point-based temporal logic.
Section 15 discusses various pertinent issues.
Section 16 surveys related work.
Our view is that the separation of the underlying mathematics from the subsequent application to mutual exclusion helps make the foundational theoretical aspects of the framework clearer.
One could indeed even take this a stage further and argue that in principle the more purely theoretical material on compositionality in Sects.
2-10 could be studied somewhat independently of its application to mutual exclusion in Sects.
11-13.
However, in practice, our investigation of the abstract theory and its application to mutual exclusion have been done simultaneously with much cross-fertilisation involving experimentation and trial and error.
6  Ben Moszkowski  As a result, the theory and application of 2-to-1 formulas for backward analysis seem quite interrelated.
Indeed, it appears virtually impossible for us to have developed either of them in isolation.
We therefore believe they are best understood and appreciated when studied together in a way which offers a more complete picture of the approach in its current form.
The evolution of this work is moreover inextricably connected with the rigorous construction of many theorems, lemmas and associated proofs.
This also seems to be an inseparable and quite invaluable and essential part of the exploration process.
What we present here gives a picture of the current state of the approach.
It continues to progress as we gain more knowledge about the remarkable and extensive mathematical terrain of 2-to-1 formulas for forward and backward analysis.
2 Propositional Interval Temporal Logic  We now describe the version of (quantifier-free) PITL used here.
More on ITL and PITL can be found in [47, 54, 55] (see also Kroger and Merz [37], Fisher [19] and the ITL web pages [32]).
Below is the syntax of PITL formulas in BNF, where p is any propositional variable: A ::= true | p | !A | A [?]
A | skip | A[?
]A | A[?]
.
(1) The last two constructs are called chop and chop-star, respectively.
The boolean operators false , A [?]
B , A [?]
B (implies ) and A [?]
B (equivalence ) are defined as usual.
We refer to A[?]
B as strong chop and likewise refer to A[?]
as strong chopstar.
Weak versions are discussed shortly when we present some derived operators.
Time within PITL is modelled by discrete, linear intervals.
An interval s is any finite or o -sequence of one or more states s0 , s1 , .
.
.
(which are not necessarily distinct from one another).
Each si maps every propositional variable p to true or false .
This mapping is denoted as si (p).
Let S denote the set of all states.
An interval s has interval length |s| >= 0, which, if s is finite, is the number of s 's states minus 1 and otherwise o .
So if s is finite, it has states s0 , .
.
.
, s|s| .
If the same state occurs twice in s , it is counted twice for determining s 's interval length.
Let S + denote the set of finite intervals and S o denote the set of infinite ones.
The (standard) version of PITL used here with state-based propositional variables is called local PITL.
A subinterval of s is any interval which is a contiguous subsequence of s 's states.
This includes s itself.
The notation s |= A, defined shortly by induction on A's syntax, denotes that interval s satisfies formula A.
Moreover, A is valid, denoted |= A, if all intervals satisfy it.
Below are the semantics of the first five PITL constructs in (1): - - - - -  True: s |= true trivially holds for any s .
A variable p: s |= p iff s0 (p)=true (initially p).
Negation: s |= !A iff s |6 = A. Disjunction: s |= A [?]
B iff s |= A or s |= B .
Skip: s |= skip iff s has exactly two states (i.e., |s| = 1).
Note that an interval s satisfies skip even if s 's two states are identical to each other.
For natural numbers i, j with 0 <= i <= j <= |s|, let si:j be the finite subinterval si .
.
.
sj (i.e., j - i + 1 states).
Define si| to be s 's suffix subinterval from state si .
Compositional reasoning using intervals and time reversal  7  Below are semantics for strong chop and chop-star: - A[?]
B : s |= A[?]
B iff for some natural number i: 0 <= i <= |s|, both s0:i |= A and si| |= B .
Note that in the case where |s| = o , we actually have i < |s|.
- A[?]
: s |= A[?]
iff one of the following holds: (1) The interval s has only one state.
(2) s is finite and either itself satisfies A or can be split into a finite number of (finite-length) subintervals which share end-states (like chop) and all satisfy A.
(3) |s| = o and s can be split into o finite-length intervals sharing end-states (like chop) and each satisfying A.
Case (3) is called chop-omega and denoted as Ao .
We depict below the behaviour of variable p in a sample 5-state interval s and denote true and false by t and f. p  s0 t  s1 f  s2 t  s3 f  s4 t  This interval satisfies the following formulas: p  skip [?]
!p  p [?]
(true [?]
!p)  (p [?]
(skip [?]
skip ))[?]
.
For instance, the formula skip [?]
!p is true because s0 s1 satisfies skip and s1 .
.
.
s4 satisfies !p since s1 (p) = false .
The fourth formula is true because the interval s 's three-state subintervals s0 s1 s2 and s2 s3 s4 both satisfy p [?]
(skip [?]
skip ).
The interval s does not satisfy the formulas below: !p  skip [?]
p  true [?]
(!p [?]
!
(true [?]
p)).
Table 1 shows useful derived PITL operators, including empty for one-state intervals and the weak chop construct A; B which can ignore B in an infinite interval satisfying A.
We derive here ITL's conventional weak chop-star A* from the strong version, although the two are interderivable.
In an infinite interval, strong chop-star requires an infinite number of iterations each of finite length, whereas weak chop-star also permits a finite number of iterations with the last having infinite length.
The strong variants of chop and chop-star are taken as primitives here to simplify some of the reasoning about time symmetry.
However, we extensively use the weak versions for reasoning about possibly nonterminating parts of programs.
We discuss in Sect.
15.4 the reason for our use of the term "empty" to describe one-state intervals even though in language theory it refers the unique empty word with no letters at all.
Let w and w' denote state formulas without any temporal operators.
Table 2 contains several sample PITL formulas which are valid.
For example, the formula (w [?]
A) [?]
(empty [?]
w); A can be understood as stating that an interval satisfies state formula w and PITL formula A iff the first state of the interval satisfies w and the interval satisfies A.
Here the first state is equally regarded as f f being a one-state interval in its own right.
The valid equivalence (2f 2A ) [?]
2A uses f f 2A to test the formula A in finite subintervals and uses 2f 2A to test A in these  8 A  3A 2A more empty finite inf fin A stable A f 3A f 2A A;B i 3A i 2A A* A+ Ao A-B A <~ B  Ben Moszkowski  = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b  skip [?
]A true [?
]A !3!A  true !more 3empty !finite 2(empty [?]
A) 2(more [?]
(A [?]
A)) A[?]
true f !3!A (A[?
]B ) [?]
(inf [?]
A) A;true i !3!A  [?]
A [?]
A[?]
[?]
(inf [?]
A) A;A* inf [?]
A[?]
finite [?]
((fin A) [?]
B ) A - B [?]
(stable A; skip )  Next Eventually Henceforth (Always) More than one state Only one state Finite interval Infinite interval Weak test of final state Stability Some initial finite subinterval All initial finite subintervals Weak chop Some initial subinterval (even infinite) All initial subintervals (even infinite) Conventional weak chop-star One or more iterations Chop-omega Temporal assignment Padded temporal assignment  Table 1 Some useful derived PITL operators  (finite  [?]
(w [?]
A); B [?]
w w  [?]
skip [?]
f 2A )[?
]A  A [?]
(empty  [?]
[?]
(A; B )  A [?]
(empty ; A)  w); A  finite [?]
(A* [?]
A[?]
)  f (A [?]
B ) [?]
(2A f f 2 [?]
2B ) f f (2f 2A ) [?]
2A  inf [?]
2more  f f (22A ) [?]
(22A )  A* [?]
(empty  [?]
A+ )  f f f (3A f f 3A [?]
3B [?]
3 [?]
3B )  Table 2 Sample valid PITL formulas  finite subintervals' own finite subintervals.
The equivalence is valid because the set of finite subintervals contained within an interval's finite subintervals is exactly the same as the set of the interval's own finite subintervals.
This is related by time symmetry to the equivalence (22A) [?]
2A which concerns suffix subintervals and is f f f found even in conventional temporal logic.
The last formula (3A [?]
3B ) [?]
3f (3A [?]
f 3B ) states that two formulas A and B are each true in finite prefix subintervals f f of an interval exactly if the conjunction 3A [?]
3B is true in some finite prefix subinterval (e.g., the larger of the two satisfying A and B , respectively).
Compositional reasoning using intervals and time reversal  9  3 Some important point-based subsets of PITL  We now present some point-based subsets of PITL which have been found useful in our previous work [55, 57] and come in handy later.
3.1 Subset of PITL with only skip, next and diamond Let PTL denote the subset of PITL formulas in conventional Propositional Linear-Time Temporal Logic with just the (derived) temporal operators  and 3 in Table 1.
We use X , X ' and Y for PTL formulas.
Various useful compositional safety and liveness properties can be expressed in PTL.
For instance, we already presented the sample PTL formulas p [?]
3q and 2(p [?]
3q ) in the introduction in Sect.
1.
Note that the PITL primitive construct skip can be derived in PTL as  !
true (the same as  empty ), so we can regard PTL as containing it as well.
We will use PTL here since it can express various compositional formulas.
Furthermore, it has much lower computational complexity and better tool support than full PITL, which has nonelementary complexity (a theorem by Kozen described in our own joint work with Halpern [45] (reproduced in [54])).
Therefore, Sect.
14 describes some potential transformations using time symmetry from PITL to a slightly enhanced version of PTL with an until operator defined shortly in Sect.
3.3.
3.2 Subset of PTL with just unnested next operators We extensively use an important subset of PTL involving the operator  :  Definition 3.1 (Next Logic) The set of PTL formulas in which the only primitive temporal operator is  is called Next Logic (NL).
The subset of NL in which no   is nested within another    is denoted as NL1 .
For example, the NL formula p [?]
q is in NL1 , but the NL formula p [?]
(q [?]
p) is not.
The variable T denotes formulas in NL1 .
All state formulas (e.g., p [?]
!q ) are in NL1 because they contain no temporal operators.
The important derived PITL constructs more and empty already defined in Table 1 are also in NL1 .
Unlike state formulas, more and empty can detect whether or not an interval has just one state.
However, the primitive construct skip , which tests for exactly two states, cannot be expressed in NL1 because it requires one  within another:  !
true (the same as  empty ).
In order to further illustrate the nature of NL1 expressiveness, we list below some more properties which NL1 formulas cannot express, together with PTL formulas not in NL1 which do capture these properties: Property Corresponding formula not in NL1 The formula p [?]
!q is true in the third state  (p [?]
!q )   true (same as  more ) The interval has more than two states  !
true (same as  empty ) The interval has exactly two states  10  Ben Moszkowski  The NL1 formulas play a significant role in the theory of PITL.
We therefore strongly encourage readers seriously interested getting a better understanding of ITL to study our presentation in [55], where we systematically describe some natural applications of NL1 to relating point-based and interval-based temporal logic.
Our new complete axiom system for PITL with infinite time [57] likewise makes extensive use of NL1 formulas and therefore shows how they can be profitably employed.
3.3 PTL with until operator Our presentation also makes use of a PTL variant called here PTLu .
It has a somewhat restricted strong version of the standard temporal operator until which is derivable in PITL: T until A  = b  (skip  [?]
T )[?]
[?]
A.
Only NL1 formulas are permitted on our restricted until 's left side, as the definition indicates by the use of T .
This is because if the left operand is not in NL1 , the restricted until will not work properly.
For example, the formula (  p) until q actually reduces to false .
Now PTLu is more expressive than PTL (e.g., see [37]), but reducible to it using auxiliary variables to mimic until .
For example, when considering the satisfiability of the formula p [?]
(p until q ) [?]
!
(p until q ), we can transform it into the formula below with an extra auxiliary variable r : p  [?]
r [?]
!r  [?]
2 r [?]
q  [?]
(p [?]
r )    [?]
2(r [?]
3q ).
We make extensive use of PTLu in our recent axiomatic completeness proof for PITL with infinite time [57].
Section 15.3 later shows an alternative way to derive until without chop-star.
Section 5 (particularly regarding formula (13)) and Sect.
14 include reductions from PITL to PTLu .
We use PTLu because formulas in it can be more expressive than in PTL.
Nevertheless, they can be readily transformed to PTL, although, as we noted already, this necessitates the introduction of some auxiliary variables.
When compared with PITL, the differences between PTL and PTLu are certainly quite small.
4 2-to-1 formulas  We now discuss in more detail the 2-to-1 formulas, which were already briefly considered in Sect.
1.
These are the main class of formulas closed under conjunction and the temporal operator 2 in our presentation here.
Such closure properties help to modularly construct formulas from simple building blocks in a way guaranteed to ensure that the results preserve some useful compositional properties.
Many of the properties of 2-to-1 formulas which we consider concern sequential composition.
However, parallel composition of formulas is not neglected either.
In this section we focus on formally defining 2-to-1 formulas and studying some of their fundamental theoretical properties.
The presentation mostly looks at the relatively abstract mathematics of compositional reasoning rather than any  Compositional reasoning using intervals and time reversal  11  particular application of the formulas.
We believe that the issues explored at such a level of abstraction are themselves an important contribution to work in the area.
Nevertheless, this material provides a solid and practical basis when the formulas are extensively used later on with time symmetry in Sects.
5 and 6 and in our experimental analysis of mutual exclusion in Sects.
11-13.
Definition 4.1 (2-to-1 Formulas) Any PITL formula A for which the implication (A; A) [?]
A is valid is called a 2-to-1 formula.
For example, the state formulas true and p are 2-to-1.
In the first case, true is trivially true for any interval, so the implication (true ; true ) [?]
true is valid.
Here is a proof for the case of p: Proof (p is 2-to-1) Suppose that an interval s satisfies p; p. We can readily show from the PITL semantics of weak chop (see Table 1) that s has a prefix subinterval s ' (perhaps s itself) which satisfies p. Hence, the first state of s ' also satisfies p. Now the first states of s and s ' are identical, so s itself satisfies p and consequently also the implication (p; p) [?]
p. Therefore, every interval satisfies this implication, so it is indeed valid.
Consequently, p is 2-to-1.
[?]
[?]
In contrast to true and p, the formula skip is not 2-to-1 and perhaps even the simplest example of this.
Observe that the chop formula skip ; skip is satisfied solely by intervals with precisely 3 states, whereas skip checks that an interval has exactly 2 states.
Hence, the implication (skip ; skip ) [?]
skip is not satisfied by 3-state intervals and is consequently not valid.
As the next Lemma 4.2 shows, it does not actually matter whether we use weak or strong chop to define 2-to-1 formulas: Lemma 4.2 The following is an alternative way to characterise 2-to-1 formulas using strong chop instead of weak chop: |=  (A[?
]A)  [?]
A.
Proof The chain of equivalences below demonstrates that the two implications (A;A) [?]
A and (A[?
]A) [?]
A are in fact semantically indistinguishable:  (A;A) [?]
A [?]
(A[?
]A) [?]
(inf  [?]
(A[?
]A) [?]
A [?]
(A[?
]A) [?]
A [?]
(A[?
]A) [?]
A.
[?]
[?]
[?]
 A) [?]
A  (inf [?]
A) [?]
A true  The first equivalence simply re-expresses A;A using the definition of weak chop in Table 1 in Sect.
2.
The third equivalence holds since the implication (inf [?]
A) [?]
A is valid for any PITL formula A and can therefore be replaced by the formula true .
Therefore the two characterisations of 2-to-1 formulas are semantically equivalent.
[?]
[?]
We prefer to define 2-to-1 formulas using weak chop because it better copes with nontermination in applications (such as for mutual exclusion in Sects.
11-13).
Hence, for consistency we will mostly stick with this convention in our presentation  12  Ben Moszkowski  here.
Nevertheless, the variant with strong chop can sometimes help to slightly shorten proofs.
Local PITL is decidable (see our earlier work with Halpern [45] (reproduced in [54])), but it has nonelementary complexity (a theorem by Kozen presented there).
Therefore, the PITL subset consisting of 2-to-1 formulas is likewise decidable.
Our recent axiomatic completeness proof for PITL with infinite time [57] ensures that a corresponding PITL theorem can be deduced for any 2-to-1 formula.
We can generalise the previous 2-to-1 example p to be any state formula w. Any NL1 formula T (see Definition 3.1 in Sect.
3.2) is 2-to-1.
Furthermore, any i i formulas 3C and 3C are 2-to-1, where 3C (defined in Table 1) tests that C is true in some prefix subinterval, possibly the interval itself even if it is infinite.
The i cases for NL1 formulas and 3C can subsume the case for a state formula w because 1 i it is in NL and additionally semantically equivalent to the PITL formula 3w .
Let us now consider one lemma dealing with all these cases and another concerning the conjunction of 2-to-1 formulas: Lemma 4.3 All of the following are 2-to-1 formulas: 1.
2.
3.
4.
5.
Any state formula w. Any NL1 formula T .
Any PITL formula of the form 3C.
i Any PITL formula of the form 3C.
i Any PITL formula of the form w [?]
3B, for any state formula w and PITL formula B.
Proof We examine each of these separately: - A state formula w: If an interval s satisfies the formula w; w then the semantics of PITL ensures that s 's first state must satisfy w. Hence, s does as well.
As  we already noted, this case can alternatively be subsumed by either the next i one for NL1 formulas or the later one for 3C .
1 - An NL formula T : Let s be an interval satisfying T ; T and let s ' be the subinterval satisfying the left instance of T .
We use case analysis to show that the interval s indeed satisfies T as well.
- If s ' has only one state, then s itself must satisfy the right-hand instance of T .
- Otherwise, both s and s ' have two or more states.
An NL1 formula cannot test beyond the second state and distinguish between the intervals s and s ' .
Consequently, if s ' satisfies T , so must s .
- A PITL formula of the form 3C : This follows from the next chain of valid implications involving the definition of weak chop in Table 1: (3C ); 3C  [?]
(3C )[?]
3C    [?]
(inf  [?]
3C )  [?]
(33C ) [?]
3C  [?]
3C.
For the purposes of comparison, here is a somewhat shorter valid chain of implications using Lemma 4.2's alternative characterisation of 2-to-1 formulas based on strong chop (i.e., |= (A[?
]A) [?]
A): (3C )[?]
3C  [?]
33C  [?]
3C.
Compositional reasoning using intervals and time reversal  13  i i i - A PITL formula of the form 3C : Suppose an interval s satisfies (3C ); 3C .
Then ' some prefix subinterval s of s (perhaps s itself) satisfies the left instance of i i 3C and furthermore s satisfies 3i 3C .
Moreover, some prefix subinterval s '' ' ' of s (perhaps s itself) satisfies the subformula C .
Now s '' is also a prefix i subinterval of s , so consequently s satisfies 3C .
Here is a corresponding chain of valid implications: i i (3C ); 3C  [?]
i (3C ); true  [?]
i 3C i 3  [?]
i 3C.
i - A PITL formula of the form w [?]
3B : The equivalence chain below invokes the i i case for 3C to also handle any formula w [?]
3B :  i (w [?]
B ) i (!w ) [?]
B i (!w ) [?]
3B i 3 [?]
3 [?]
3 i i [?]
(!w) [?]
3B [?]
w [?]
3B.
[?]
[?]
Lemma 4.4 For any 2-to-1 formulas A and B, the conjunction A [?]
B is a 2-to-1 formula as well.
That is, if |= (A; A) [?]
A and |= (B ; B ) [?]
B, then also |= (A [?]
B ); (A [?]
B ) [?]
(A [?]
B ).
Proof Here is a simple semantic proof with four steps: 1 |= A; A [?]
A 2 |= B ; B [?]
B 3 |= (A [?]
B ); (A [?]
B ) [?]
(A; A) [?]
(B ; B ) 4 |= (A [?]
B ); (A [?]
B ) [?]
A [?]
B  Assumption Assumption PITL 1-3, Prop.
The mention of "PITL" in Step 3 refers to some routine semantic reasoning about intervals which we do not further justify here.
However, we provide detailed deductions for valid properties of this kind in our recent axiomatic completeness proof for PITL with infinite time [57].
We can summarise the proof as a chain of valid implications: (A [?]
B ); (A [?]
B )  [?]
(A; A) [?]
(B ; B )  [?]
[?]
[?]
A [?]
B.
The next theorem about 2-to-1 formulas appears to us to be an important, yet previously unknown elementary mathematical property about compositionality: Theorem 4.5 If A is 2-to-1, so is 2A.
That is, from the valid implication A follows the next one: |= ((2A); 2A) [?]
2A.
|=  (A; A) [?]
Proof Our goal is to prove the validity of the implication below for any 2-to-1 formula A: |= (2A); 2A [?]
2A.
(2)  The proof of validity is a little simpler if we use Lemma 4.2's alternative characterisation of 2-to-1 formulas involving strong chop (i.e., |= (A[?
]A) [?]
A) to establish the validity of the next semantically equivalent implication: |=  (2A)[?]
2A  [?]
2A.
(3)  Let s be an interval satisfying (2A)[?]
2A.
We now show that s also satisfies 2A.
The semantics of strong chop ensures that there exists at least one pair of subintervals s ' and s '' of s which share a state, combine to make s and both satisfy the subformula 2A.
Here is a diagrammatic representation of this: s  z  }| { (2A)[?
]2A .
| {z }| {z } s'  s''  14  Ben Moszkowski  From the semantics of 2 we have that every suffix subinterval of s ' and s '' (including s ' and s '' themselves) satisfies the subformula A.
Let us now consider an arbitrary suffix subinterval s ''' of the overall interval s .
We want to show that it satisfies A and hence s satisfies 2A.
There are two subcases: - s ''' consists of a suffix of s ' followed by all of s '' (perhaps even s itself ): Now the suffix subinterval of s ' and the subinterval s '' both satisfy A.
Therefore, s ''' satisfies the formula A[?]A.
The assumption that A is 2-to-1 and Lemma 4.2 then yield that s ''' likewise satisfies A.
- s ''' is a suffix of s '' (perhaps even s '' itself ): Hence, s ''' immediately satisfies the 2-to-1 formula A.
Therefore, s satisfies 2A.
Consequently, implication (3) is valid, as is (2), so 2A is indeed 2-to-1.
Observe that we can alternatively express this reasoning about the interval s and the formula (2A)[?]
2A by means of a chain of valid implications starting with (2A)[?]
2A and ending with 2A: (2A)[?]
2A [?]
2 A [?]
(A[?
]A)    [?]
2 A[?
]A    [?]
2A.
(4) [?]
[?]
Lemma 4.6 For any NL1 formula T and PITL formulas B and C, the following are 2-to-1 formulas: 2T  23C  i 23C  i 2(w [?]
3B ).
Proof This readily follows from Lemma 4.3 about some simple kinds of 2-to-1 formulas together with Theorem 4.5.
[?]
[?]
Recall that T subsumes w, so 2T likewise subsumes 2w.
i The 2-to-1 formulas of the form 2(w [?]
3B ) can express some standard temporal liveness properties.
For example, the PTL formula 3q is semantically equivalent i to 33q , so consequently the conventional PTL formula 2(p [?]
3q ) is in fact 2-to-1.
Indeed, its subformula p [?]
3q is also 2-to-1 because the semantic equivalence of i i 3q and 33q ensures that the implication can be expressed as p [?]
33q .
Let us now discuss why the following three frequently occurring formulas (all defined in Table 1) are 2-to-1: finite  fin w  inf ,  where w is any state formula.
The first one finite is 2-to-1 because it denotes 3empty , which is 2-to-1 by Lemma 4.3.
The formula fin w denotes 2(empty [?]
w).
The subformula empty [?]
w is in NL1 , so 2(empty [?]
w) and fin w are 2-to-1 by Lemma 4.6.
It then follows from this that inf , which denotes !finite , is also 2-to-1 since it is semantically equivalent to fin false .
Alternatively, inf is 2-to-1 because it can be expressed as 2more .
Now more is in NL1 , so 2more is likewise 2-to-1 by Lemma 4.6.
Compositional reasoning using intervals and time reversal  15  4.1 Introduction, combining and extension of 2-to-1 formulas Our interest here is in compositionally proving the validity of implications of following form: w  [?]
Sys  [?]
A  [?]
fin w' ,  where w is a state formula about the initial state, Sys expresses some abstract or concrete system's behaviour in PITL, A is a 2-to-1 formula and w' is a state formula about the final state if the system terminates.
Now we can build Sys by starting with various simple formulas corresponding to individual concrete or abstract program steps.
These are then combined in different ways, such as sequentially (e.g., using chop) or in parallel (using logical-and).
For example, Sys could be the sequential composition Sys ' ; Sys '' of two parts Sys ' and Sys '' .
Suppose we have already proved the validity of the following two implications for Sys ' and Sys '' , respectively: |=  w  |=  w''  [?]
Sys ' [?]
Sys ''  [?]
A [?]
[?]
A  fin w'' [?]
fin w' .
The validity of the previous implication for Sys then follows from the validity of these, in part because the two instances of the 2-to-1 formula A can be combined into a single one.
Our mutual exclusion examples discussed later in Sects.
11-13 involve two processes running in parallel, with each containing several sequential parts.
We first employ a technique for showing that some of the system's individual steps imply 2-to-1 formulas.
We regard this as a way to introduce 2-to-1 formulas.
These can then be combined together (e.g., sequentially or in parallel) or extended using some of the other techniques to obtain 2-to-1 formulas about bigger portions of the overall system.
Eventually we show that the entire system with its initial condition implies a 2-to-1 formula.
Let us now discuss four general kinds of techniques to help compositionally reason about 2-to-1 formulas.
Each is associated with one or two valid generic implications concerning such formulas.
We later present some specific suitable implications when we look at the four techniques individually in greater detail.
However, these implications are not meant to be exhaustive.
Below is a list of the main categories we consider: |= A' [?]
A Introduction of a 2-to-1 formula A Sequential combining of two copies of a 2-to-1 formula A |= (A; A) [?]
A |= (A' ; A) [?]
A Extension of a 2-to-1 formula A leftward or rightward |= (A; A' ) [?]
A |= (A [?]
A' ) [?]
A'' .
Parallel combining of two 2-to-1 formulas A and A' The shorthand ISEP can be used as an abbreviation for the four parts Introduction, Sequential combining, Extension leftward or rightward and Parallel combining.
The later Sects.
6-9 cover in detail ISEP techniques for a class of 2-to-1 formulas for backward analysis.
The basic theory of ISEP techniques can even be formalised in PITL with just chop and skip and so without chop-star.
The theory therefore seems fairly elementary from a mathematical standpoint.
Section 10 adds a further technique for Iteration of 2-to-1 formulas for backward analysis.
The abbreviation ISEPI enlarges ISEP to include this as well.
The ISEPI techniques are later applied to mutual exclusion in Sects.
11-13.
16  Ben Moszkowski  We now illustrate how the first three ISEP techniques can be used together to combine several sequential formulas in order to obtain from them a single 2to-1 formula.
Let Sys be a hypothetical system with four sequential parts somehow or another expressed in PITL as the formulas Sys 1 , .
.
.
, Sys 4 .
We have Sys itself denote the sequential composition of Sys 1 , .
.
.
, Sys 4 : = b  Sys  Sys 1 ; Sys 2 ; Sys 3 ; Sys 4 .
Now further assume that Sys 1 , .
.
.
, Sys 4 have the associated valid implications below, which also include five state formulas w1 , .
.
.
, w5 to serve as pre- and postconditions: |=  w1 w2 w3 w4  |= |= |=  Sys 1 Sys 2 Sys 3 Sys 4  [?]
[?]
[?]
[?]
[?]
[?]
[?]
[?]
2!p [?]
fin w2 (finite [?]
fin p) 32q [?]
fin w4 (finite [?]
fin q )  [?]
fin w3  [?]
fin w5 .
(5)  Our goal here is to compositionally prove that the four valid implications in (5) together ensure that Sys implies the 2-to-1 liveness formula 2(p [?]
3q ) as expressed by the next valid implication: |=  w1  [?]
Sys  2(p [?]
3q )  [?]
[?]
fin w5 .
(6)  It happens that all the subformulas 2!p, (finite [?]
fin p), 32q and (finite [?]
fin q ) in (5) are in fact themselves 2-to-1.
However, this point is not essential here since our sole aim is to show the validity of implication (6) relating Sys with the 2-to-1 formula 2(p [?]
3q ).
Below is a more detailed discussion which explains and motivates each ISEP technique and relates the first three of them to our example: - ISEP Introduction of a 2-to-1 formula: Here we show that some formula A' concerning a system step implies the desired 2-to-1 formula A: A'  |=  [?]
A.
In our example (5), ISEP Introduction concerns three subformulas 2!p, 32q and finite [?]
fin q for which we can formalise some valid PTL implications: |=  2!p [?]
2(p [?]
3q )  |=  32q [?]
2(p [?]
3q )  |=  (finite  [?]
fin q ) [?]
2(p [?]
3q ).
These ways for ISEP Introduction of the 2-to-1 formula 2(p [?]
3q ) provide a means to obtain from three of the four valid implications in (5) the valid implications below for Sys 1 , Sys 3 and Sys 4 , respectively: |= |= |=  w1 w3 w4  [?]
[?]
[?]
Sys 1 Sys 3 Sys 4  [?]
[?]
[?]
2(p [?]
3q ) 2(p [?]
3q ) 2(p [?]
3q )  [?]
[?]
[?]
fin w2 fin w4 fin w5 .
(7)  Incidentally, the justification for obtaining 2(p [?]
3q ) from finite [?]
fin q can be subsumed by the case for 32q owing to the next chain of valid implications: finite  [?]
fin q  [?]
32q  [?]
2(p [?]
3q ).
The valid implications such as |= (2!p) [?]
2(p [?]
3q ) for ISEP Introduction of a 2-to-1-formula are quite important since they can provide a way to start a compositional analysis involving this formula.
Compositional reasoning using intervals and time reversal  17  It is straightforward to check that if we have a valid implication |= A' [?]
A for ISEP Introduction, then the ones below can also be used for ISEP Introduction : 2A' [?]
2A  |=  For example, from  ' f f 2A [?]
2A.
|=  !p [?]
(p [?]
3q ) follows |= 2!p [?]
2(p [?]
3q ).
- ISEP Sequential combining of two instances of a 2-to-1 formula: Here we take two sequential instances of a 2-to-1 formula A and merge them together: |=  |=  A; A  [?]
A.
This with the particular 2-to-1 formula 2(p [?]
3q ) together provide a way to reduce the two valid implications in (7) for Sys 3 and Sys 4 to the next valid one concerning their sequential composition Sys 3 ; Sys 4 : |=  w3  [?]
(Sys 3 ; Sys 4 )  2(p [?]
3q )  [?]
[?]
fin w5 .
(8)  Theorems and lemmas about closures provide ways to obtain an instance of an ISEP technique for Sequential combining from a simpler variant of itself.
For example, Theorem 4.5 ensures that |= (A; A) [?]
A yields |= ((2A); 2A) [?]
2A.
- ISEP Extension of a 2-to-1 formula leftward or rightward : The previous ISEP technique of Sequentially combining two instances of a 2-to-1 formula A such as 2(p [?]
3q ) seems quite attractive.
Unfortunately, it is not always the case that two adjacent subintervals both satisfy such a 2-to-1 formula A so that the overall interval automatically also does.
However, if one of the subintervals satisfies A, then we can try to simplify the sequential compositions A' ; A and A; A' involving A and some other suitable formula A' .
The next two valid implications show the two possible ways to perform the ISEP technique of Extending leftward or rightward by merging A and A' together into A: |=  A' ; A  [?]
A  |=  A; A'  [?]
A.
Of course, the implications do not work for arbitrary A' , but we shortly consider some actual practical instances.
Observe that the previous ISEP technique of Sequential combining of a 2-to-1 formula with itself (i.e., |= (A; A) [?]
A) is in fact just a special case of ISEP Extending leftward or rightward, where A' is identical to the 2-to-1 formula A.
It seems that sequential extension can be highly dependent on the nature of A' .
The next valid implication illustrates the first case |= (A' ; A) [?]
A: |=  (finite  [?]
fin p); 2(p [?]
3q )  [?]
2(p [?]
3q ).
(9)  Here we take A to be the 2-to-1 formula 2(p [?]
3q ) and extend it leftward by the formula finite [?]
fin p which plays the role of A' .
Implication (9) is valid because the instance of p in the left operand of the chop ensures that p is also initially true in the right operand's subinterval.
Therefore, the right-hand subinterval moreover satisfies 3q , so the prefix subintervals of the overall interval which start before the right-hand subinterval and contain it likewise satisfy 3q , and hence also the 2-to-1 formula p [?]
3q .
We can then use valid implication (9) to obtain from the implication for Sys 2 in (5) and the later one for Sys 3 ; Sys 4 in (8) the next valid implication for Sys 2 ; Sys 3 ; Sys 4 : |=  w2  [?]
(Sys 2 ; Sys 3 ; Sys 4 )  [?]
2(p [?]
3q )  [?]
fin w5 .
(10)  18  Ben Moszkowski  Once again using the fact that 2(p [?]
3q ) is 2-to-1, we sequentially combine its two instances in the earlier implication for Sys 1 in (5) and the other implication (10) for Sys 2 ; Sys 3 ; Sys 4 to arrive at our overall goal, the validity of implication (6) for Sys .
Sect.
8 consider ways to obtain an instance of an ISEP technique for Extending leftward or rightward from a simpler variant of itself (e.g., see Theorems 8.1 and 8.7).
Here is a chain of valid implications summarising of all of the ISEP transformations which we have so far applied on the sequential composition of the original subformulas 2!p, (finite [?]
fin p), 32q and (finite [?]
fin q ) in (5): (2!p); (finite | {z } Sys 1  [?]
[?]
fin p); (32q ); (finite [?]
fin q ) | {z } | {z } Sys 3  2(p [?]
3q ); (finite  [?]
Sys 4  fin p); 2(p [?]
3q ); 2(p [?]
3q ) | {z } Sys 3 and Sys 4  [?]
2(p [?]
3q ); (finite |  [?]
fin p); 2(p [?]
3q ) {z }  Sys 2 and Sys 3 ;Sys 4  [?]
Sequential combining  Extending leftward  2(p [?]
3q ); 2(p [?]
3q ) {z } | Sys 1 and Sys 2 ;Sys 3 ;Sys 4  [?]
Introduction  Sequential combining  2(p [?]
3q ).
Underbraces indicate the subformulas reduced to the 2-to-1 formula 2(p [?]
3q ) in each step and also give the associated parts of Sys .
Instead of the first step's reductions of each of the pair of 2-to-1 formulas 32q and finite [?]
fin q to 2(p [?]
3q ), we can alternatively use ISEP Introduction to reduce finite [?]
fin q to 32q , and then invoke ISEP Sequential combining on (32q ); 32q to obtain 32q .
We follow that by a second application of ISEP Introduction to arrive at our goal 2(p [?]
3q ).
Here is a chain of valid implications summarising this: (32q ); (finite |  [?]
[?]
{z  fin q ) }  Sys 4  (32q ); 32q |  {z  }  Sys 3 and Sys 4  [?]
32q | {z }  Sys 3 ;Sys 4  [?]
2(p [?]
3q )  Introduction  Sequential combining Introduction  We now consider the last of the four ISEP techniques, namely ISEP Parallel combining of two suitable 2-to-1 formulas.
Consider a hypothetical system Sys ' constructed as the conjunction Sys '1 [?]
Sys '2 of two parts Sys '1 and Sys '2 , both somehow expressed in PITL.
Suppose we have the following valid implications for Sys '1 and Sys '2 : |= |=  w1' ,1 w2' ,1  [?]
[?]
Sys '1 Sys '2  [?]
[?]
A [?]
fin w1' ,2 A' [?]
fin w2' ,2 ,  Compositional reasoning using intervals and time reversal  19  Description of ISEPI technique Introduction (simple version): |= A' [?]
A Introduction (with relaxed assumption): |= A' [?]
A Sequential combining of 2-to-1 formula: |= (A; A) [?]
A Extend a 2-to-1 formula rightward : |= (A; A' ) [?]
A Parallel combining of 2-to-1 formulas: |= (A [?]
A' ) [?]
A'' Iteration of +-to-1 formula: |= A+ [?]
A Iteration of "almost" *-to-1 formula: |= w [?]
A* [?]
A  Basis (19) (20) Def.
4.1 (25) (30) (34) (40)  Use (62) (63) (64), (75) (65) (50), (66) (35) (54)  Table 3 Examples of ISEPI-based compositional reasoning about 2-to-1 formulas  where the state formulas w1' ,1 , .
.
.
, w2' ,2 serve as pre- and post-conditions.
ISEP Parallel combining provides a way to obtain a similar implication concerning Sys ' from these two.
Here is the most straightforward such implication which is valid: |=  (w1' ,1  [?]
w2' ,1 )  [?]
(Sys '1  [?]
Sys '2 )  [?]
A [?]
A'  [?]
fin (w1' ,2  [?]
w2' ,2 ).
However, we are particularly interested in cases where A and A' are 2-to-1 formulas and moreover their conjunction A [?]
A' implies some formula A'' which is noticeably simpler than the conjunction: A [?]
A'  |=  [?]
A'' .
Here is a valid PTL formula illustrating the ISEP technique of Parallel combining : |=  2(p [?]
p)  [?]
2(q [?]
!p)  [?]
2!
(p [?]
q ).
(11)  The following is another PTL example of ISEP Parallel combining : |=  2(p [?]
32p)  [?]
2(q [?]
32!p)  [?]
2!
(p [?]
q ).
ISEP Parallel combining finds application in Sects.
11-13 when we want to merge together the 2-to-1 formulas obtained for each of two parallel processes concerning mutual exclusion.
Observe that from |= (A [?]
A' ) [?]
A'' readily follows |= ((2A) [?]
(2A' )) [?]
2A'' .
This semantic inference rule can be used to prove the validity of the two implications just given concerning 2!
(p [?]
q ) from simpler ones about !
(p [?]
q ).
Later Sects.
6-9 will consider the ISEP techniques of Introduction, Sequential combining, Extension and Parallel combining on a class of formulas which are suitable for backward analysis.
For the convenience of readers, Table 3 provides an index to various additional instances of the implications subsequently mentioned for the various ISEP techniques.
This includes two extra entries for combining Iterations of a +-to-1 formula and an "almost" *-to-1 formula, which we describe later on in Sect.
10, so in fact all the ISEPI techniques are represented in Table 3.
Remark 4.7 It is interesting to note that in our applications of the ISEPI techniques considered above and later on, the concrete instances of all the formulas A, A' and A'' found in the implications are always 2-to-1 formulas.
For example, all three 2-subformulas in implication (11), which involves ISEPI Parallel combining (i.e., |= (A [?]
A' ) [?]
A'' ), are 2-to-1 by Lemma 4.6 because in each of them, the operand  20  Ben Moszkowski  of 2 is in NL1 .
In fact, the sole exception to formulas being 2-to-1 is just the statement of Theorem 8.7 in Sect.
8.2 for extending a 2-to-1 formula A to the right: |= (A; A' ) [?]
A.
However, even there the generic formula for A' is in a class called 1-to-2f formulas (see Definition 8.2 in Sect.
8.1) which, like the class of 2-to1-formulas, is closed under conjunction and the temporal operator 2 (as stated in Sect.
8.1 in Lemma 8.4 and Theorem 8.5).
In our application of Theorem 8.7 in Sect.
12, the concrete instance of A' is in fact both 1-to-2f and 2-to-1.
4.2 2-to-1 formulas involving finite prefix subintervals The earlier Theorem 4.5 shows that the class of 2-to-1 formulas is closed under the operator 2, which concerns suffix subintervals.
It is natural to ask whether time symmetry can help extend the result to prefix subintervals and the associated operator 2f .
In this section we demonstrate that this is indeed the case.
The result is needed when we later consider in Sect.
6 a class of 2-to-1 formulas suitable for backward analysis.
These 2-to-1 formulas play a central role in practically all of the subsequent sections, including Sects.
11-13 on mutual exclusion.
f Theorem 4.8 If A itself is 2-to-1 for finite time, so is 2A for all intervals, including | = f f f infinite ones.
More precisely, if finite [?]
(A; A) [?]
A, then |= (2A ); 2A [?]
2A.
Proof The proof is largely based on applying time symmetry to the earlier proof for Theorem 4.5, which concerns 2 and suffix subintervals instead of 2f and prefix subintervals.
The earlier chain of valid implications (4) in Theorem 4.5 can be f adapted for use with 2A in place of 2A: [?]
f f f A [?]
(A A) (2A )[?]
2A [?]
2    f A [?]
A [?]
2    f [?]
2A.
(12) [?]
[?]
The following is a simple corollary of Theorem 4.8: f Corollary 4.9 If a formula 2A is 2-to-1 for finite time, it is itself likewise 2-to-1 for  |= finite [?]
((2A f f f all intervals, including infinite ones.
More precisely, if ); 2A ) [?]
2A,  f f f then |= (2A ); 2A [?]
2A.
f f f f Proof We start with (2A ); 2A .
Now the PITL formulas 2A and 2f 2A are semanti-  cally equivalent since they both inspect exactly the finite prefix subintervals.
The f assumption that 2A is 2-to-1 for finite time together with Theorem 4.8 ensures f f f that 22A is 2-to-1 for all intervals.
Hence, so is the equivalent formula 2A .
[?]
[?]
i Remark 4.10 It is not hard to adapt the results in this section to deal with 2A , f which is the weak version of 2A defined in Table 1.
We omit the details here.
f A formula 2A can in principle be 2-to-1 even if A itself is not 2-to-1.
The f f formula 2skip is a (not especially useful) example.
This is because 2skip is semantically equivalent to the 2-to-1 formula false , but the operand skip is not 2-to-1  by our earlier discussion near the beginning of this Sect.
4.
At present we are not aware of any such formulas with some practical benefits.
Compositional reasoning using intervals and time reversal  21  5 Time reversal and reflection  In this section we consider two complementary ways to exploit time symmetry.
The first is syntactic and the second is semantic.
One way to extend known facts and techniques is by interpreting them in reverse.
For example, as we discussed in Sect.
1, the 2-to-1 PTL formula p [?]
3q can be viewed as a forward analysis from a state in which p is true to one in which q is true.
For backward analysis, we in essence reverse our perspective by means of the formula (fin p) [?]
3q ("if p is true in the final state, then q is true in some state ").
This implication considers the behaviour of p in a finite interval's last state rather than the first one.
In the two sample implications, the subformula 3q has the same semantic meaning in both the forward or reversed perspectives.
The reversed way of reasoning can with care provide a basis for performing backward analysis from a situation in a state to some activities which lead up to it.
For instance, an analysis of a system fault could investigate various plausible anomalies which must precede it.
The next statement is also an example: "If I am wearing shoes, then they must have been previously placed on my feet".
We will look at some simple and natural syntactic transformations on formulas which involve time symmetry and are referred to here as time reversal.
These transformations are in general limited to finite intervals, so we employ a twostage approach to also obtain results for infinite time.
For example, we can prove validity of suitable formulas for infinite time after using time reversal to establish their validity for finite time.
The current section includes some compositional uses of the two-stage process on the class of 2-to-1 formulas already introduced in Sect.
4.
Various 2-to-1 formulas are then later applied to doing backward analysis of mutual exclusion in Sects.
11-13.
For any PITL formula A, define the temporal reversal Ar by induction on A's syntax to act like A in reverse: true r = b true  pr = b fin p  skip r = b skip  (!A)r = b !
(Ar )  (A[?]
B )r = b B r [?]
Ar  (A[?
]B )r = b A r [?
]B r  (A[?]
)r = b (Ar )[?]
.
For instance, more r (the same as (skip [?]
true )r ) reduces to true [?]
skip , which is semantically equivalent to more in finite intervals (although not in infinite ones).
f Similarly, (2A )r reduces to 2(Ar ).
For a finite interval s , let s r denote the interval s|s| .
.
.
s0 which temporally reverses s .
Observe that any such s equals the twice reversed interval s rr .
Here are some simple lemmas concerning time reversed intervals and formulas: Lemma 5.1 For any finite interval s and PITL formula A, the following are equivalent statements:  (a) s |= A (b) s r |= Ar .
Proof We do induction on formula A's syntax.
[?]
[?]
Lemma 5.2 Any PITL formula A is semantically equivalent to Arr in all finite intervals.
This can be expressed by the valid implication below: |=  finite [?]
(A [?]
Arr ).
22  Ben Moszkowski  Proof We use Lemma 5.1 together with the equivalence of s and s rr to show that A and Arr have the same truth values for every finite interval s : s  |=  A  iff  sr  |=  Ar  iff  s rr  |=  Arr  iff  s  |=  Arr .
[?]
[?]
Lemma 5.3 For any PITL formula A, the following statements are equivalent:  (a) (b)  |= |=  finite [?]
A finite [?]
Ar .
Proof The formula finite [?]
A is valid iff all finite intervals satisfy A.
Let (S + )r denote the set of reversed finite intervals.
This in fact equals S + .
Time reversal of the intervals creates a 1-to-1 mapping between S + and itself.
Furthermore, Lemma 5.1 ensures that each finite interval s satisfies A iff the finite interval s r satisfies Ar .
Hence, (a) and (b) are indeed equivalent statements.
[?]
[?]
Note that PITL with just finite time, like some other temporal logics such as quantified PTL, expresses the regular languages with words having one or more letters (as we discuss in [54]).
The set of regular languages for any (finite) alphabet is closed under word reversal.
This explains semantically why reversal cannot increase PITL's expressiveness.
The next semantic concept provides a further application of time symmetry: Definition 5.4 (Reflections) A PITL formula A reflects another PITL formula B if |= finite [?]
(A [?]
B r ).
We call A a reflection of B .
For example, the state formula w [?]
w' reflects fin (w [?]
w' ).
The 2-to-1 PTL formula 3w reflects itself and so can be said to be self-reflecting.
It is important to keep in mind that time reversal and reflection both involve time symmetry, but time reversal is a syntactic concept, whereas reflection is a semantic one.
In practice, we often employ both techniques together.
We now consider some other examples of reflection in order for readers to gain fluency with the concept in the context of PITL.
This will help when we later look in Sect.
6 at some properties of reflections of 2-to-1 formulas.
The formula (fin p) [?]
3q reflects the formula p [?]
3q .
They indeed exhibit symmetrical behaviour in finite intervals.
The first formula (fin p) [?]
3q ensures that if p is true in the final state, then some state has q true.
The second formula p [?]
3q ensures that if p is true in the initial state, then some state has q true.
It follows that the next formula reflects the 2-to-1 PTL formula 2(p [?]
3q ):  f (fin p) [?]
3q .
2  (13)  It is not hard to see how p is reflected to be fin p. Similarly, 2 becomes 2f .
We later show in Sect.
6 that formula (13) is likewise 2-to-1.
This formula ensures that whenever p is true in an interval state, then q is either true in that same state or some earlier one.
Recall from Sect.
3.3 the version of PTL called PTLu and having a strong until operator.
The PTLu formula below has the same semantics as the PITL formula (13), although we do not claim that this is obvious: 2!p  [?]
  (!p) until q .
The left conjunct 2!p deals with intervals where p is never true.
In such intervals, q does not need to be true either, so we can ignore its behaviour.
The right disjunct  Compositional reasoning using intervals and time reversal  23  (!p) until q rather opaquely ensures that if, on the other hand, q is somewhere true, then p will stay false until the first time q is true.
This suffices to guarantee that the first instance of p cannot precede the first instance of q in the interval.
Let us now look at some trickier examples of reflection involving 2-to-1 formulas and the operators skip and .
The formula 3(skip [?]
q ) reflects the 2-to-1 formula NL1 formula  q .
Let us consider why this is so.
For any finite interval, the formula  q ensures that the interval has at least two states with q true in the second state.
The formula 3(skip [?]
q ) likewise ensures that the interval has at least two states with q true in the penultimate state (i.e., the one which is next to last).
Consequently, any finite interval s indeed satisfies one of the formulas 3(skip [?]
q ) and  q iff the interval's reversal s r satisfies the other.
The next formula reflects the 2-to-1 formula 2(p [?]
q ) and by the presentation in Sect.
6 is likewise 2-to-1: f (fin p) [?]
3(skip 2  [?]
 q) .
(14)  The only tricky part of the reflection here is when we time-wise reverse the effect of  q by reflecting it using 3(skip [?]
q ) as discussed above.
Consider what kind of finite intervals are satisfied by formula (14).
First of all, a finite interval satisfies the subformula (fin p) [?]
3(skip [?]
q ) in (14) iff the propositional variable p is false in the interval's last state or the interval has at least two states and the propositional variable q is true in the interval's penultimate state.
So if p ends up in the last state being true, then the interval has two or more states and the last one is immediately preceded by another with q true.
The effect of the subformula (fin p) [?]
3(skip [?]
q ) is therefore to make the overall formula (14) test that within each finite prefix subinterval of an interval, if p is true in the final state, then the subinterval has at least two states and q is true in the one just before the final state.
This is identical to testing that any state in the overall interval with p true is immediately preceded by another state with q true.
The PTL formula below has the same semantics as PITL formula (14): !p  [?]
2 (more  [?]
 !q ) [?]
!p .
We now demonstrate that every formula has a reflection: Lemma 5.5 For any PITL formula A, the formula Ar is a reflection of A.
In fact, the formulas A and Ar reflect each other.
Proof Lemma 5.2 ensures for any PITL formula A the valid implication |= finite [?]
A [?]
Arr ).
Therefore, by Definition 5.4 about reflections, the formula A is a reflection of Ar .
In addition, we have the trivially valid implication |= finite [?]
Ar [?]
Ar ).
From this and Definition 5.4 about reflections, the formula Ar is a reflection of A. Consequently, the formulas A and Ar indeed reflect each other.
[?]
[?]
It also follows from our discussion that A reflects B iff B reflects A.
Reflecting can sometimes aid in avoiding redundant finite-time proofs in two directions.
Instead, we try to do a proof in one time direction and then with care reflect the result to apply the other way around.
For example, later on in Sect.
6 we reflect some syntactic classes of 2-to-1 formulas to obtain further classes of 2-to-1 formulas.
Sect.
14 discusses how reflection can help reduce reasoning involving 2f to simpler PTL-based reasoning.
24  Ben Moszkowski  Here is another example of reflecting based on the previously mentioned chains of implications (4) and (12), which concern the closure of 2-to-1 formulas under 2 and 2f , respectively:  [?]
2 A [?]
A [?]
2A r r[?]
r  r r r f A [?]
(A f A [?]
A f [?]
2 A ) [?]
2 [?]
2A .
(2A)[?]
2A [?]
2 A [?]
(A[?
]A) r [?
]f r f (2A ) 2A    Remark 5.6 We can alternatively define Ar to be a primitive operator in a variant  of PITL called PITLr .
However, it seems at present simpler to work in conventional PITL.
5.1 PTL with past time In our later application of time symmetry to compositional reasoning with 2-to-1 formulas, we sometimes compare PITL formulas to others in a version of PTL with past time, denoted here as PTL- .
It is not a subset of conventional PITL because that does not have past time.
Our experience is that even readers with previous experience with ITL will find the unfamiliar processes of viewing formulas in reverse and interval-based backward analysis somewhat challenging.
Consequently, it seems beneficial to compare PITL formulas obtained using time symmetry with semantically quite similar formulas in a more widely known formalism such as PTL- .
Time is modelled in PTL- as being linear and discrete (like for PITL and PTL) but having a bounded past.
The syntax of PTL is modified to include the - - X (read previous X ) and 3X two additional primitive operators  (read once X ).
| = The semantics of a PTL formula X is now expressed as (s, k) X , where k is any natural number not exceeding |s|.
The purpose of k is to indicate the present - are as follows: - and 3 state.
For example, the semantics of  - X iff k > 0 and (s, k - 1) |= X (s, k) |=  - (s, k) |= 3X iff for some j : 0 <= j <= k, (s, j ) |= X.
Consider the sample formula below: p  - !p [?]
[?]
3q  - [?]
3r.
This is satisfied by any pair (s, k) with k >= 1 where p is true in the state sk , false in the previous one sk-1 , q is true in the state sk or after it, and r is true in the state sk or before it.
The derived PTL- operator first is defined as follows to test for the first state of an interval: - true.
first = b !
We later use the operator first to help us relate formulas in PITL with others in PTL- .
For example, the following two examples in PTL- and PTL, respectively, are satisfied by the same intervals: first  [?]
(p  -q) [?]
more  [?]
  ( p) [?]
q .
More precisely, for any interval s , the pair (s, 0) satisfies the left-hand PTL- formula iff s satisfies the right-hand PTL formula.
The PTL- formula expresses  Compositional reasoning using intervals and time reversal  25  that there are at least two states and the first one, which is the present state, has no past.
Furthermore, if p is true in the second state, q is true in its predecessor, the first state.
The second formula is in PTL and expresses that the interval has at least two states (with no past), and if p is true in the second one, then q is true in the first.
So both formulas concern the same kind of behaviour.
A PTL- formula X is defined to be satisfiable iff (s, k) |= X holds for some pair (s, k) with k <= |s|.
The formula X is valid iff (s, k) |= X holds for every pair (s, k) with k <= |s|.
Duan [13, 14] and Bowman et al.
[9] present versions of ITL with past-time constructs (see also Gomez and Bowman [22]).
So in principle, PTL- can be regarded as a subset of PITL with past time.
6 2-to-1 formulas for backward analysis  Recall Theorem 4.8 in Sect.
4.2 which establishes that if a PITL formula A is 2-to-1 f for finite intervals, then the PITL formula 2A is 2-to-1 for all intervals, including even infinite ones.
Let us now consider a significant class of such 2f -formulas which are shown to be 2-to-1 with the help of time symmetry.
They offer a natural compositional framework  for backward analysis.
The previously mentioned PITL formula 2f (fin p) [?]
3q is an example.  The PITL formula 2f (fin w) [?]
3B is a generalisation of 2f (fin p) [?]
3q and tests that in any finite prefix interval where w ends true, it is preceded by B .
The subformula B therefore represents some activity observable (non-strictly) prior to any state where w is true.
Such formulas provide a way to do backward analysis when we want to reason about what must have preceded a state with w true.
They will be extensively investigated and applied in our presentation.
Below is an informal graphical representation of a 10-state interval containing some finite 8-state prefix subinterval which satisfies (fin w) [?]
3B and ends with w true: w  States: B 3B   The role which the 2-to-1 formula 2f (fin w) [?]
3B plays here is similar to the one for formulas in the past-time variant PTL- of PTL (see Sect.
5.1) having the form 2(w [?]
X ), where the only temporal operators in the PTL- formula X are past-time ones.
Perhaps the most important result we need  is the following one about a key f property of the PITL formula 2 (fin w) [?]
3B : Theorem 6.1 For any state formula w and PITL formula B, the following formula is 2-to-1:  f (fin w ) [?]
3B .
2 (15) r i Proof The operand (fin w) [?]
3B can be reflected to obtain the formula w [?]
3B , which is 2-to-1 by our previous Lemma 4.3.
Hence, the formula (fin w) [?]
3B is itself  26  Ben Moszkowski  f 2-to-1  for finite intervals.
It follows from this and Theorem 4.8 that 2 (fin w) [?]
3B is 2-to-1 for all intervals.
There is alsoan alternative proofinvolving the reflection of 2.
We can reflect r i f 2 (fin w) [?]
3B to be 2 w [?]
3B  .
By Lemma 4.6, this 2-formula is 2-to-1.
Hence, the formula 2f (fin w) [?]
3B is 2-to-1 for finite intervals.
By Corollary 4.9, this formula is 2-to-1 for all intervals, including infinite ones.
[?]
[?]
  Let us now consider the next instance of 2f (fin w) [?]
3B : f (fin p) [?]
3(skip 2  [?]
 q) .
(16)  We already mentioned formula (16) as (14) when previously defining and explaining the concept of reflecting formulas.
It is true for intervals when each state with p true is immediately preceded by one with q true.
This is because the formula ensures that any finite prefix subinterval ending with p true in the subinterval's last state has q equal true in the subinterval's penultimate state.
So any state with p true must be immediately preceded by one with q true.
We explained when previously discussing the earlier instance of (16) as formula (14) that it is a reflection of the 2-to-1 PTL formula 2(p [?]
q ).
Let us now relate formula (16) to one in PTL- , the version of PTL with past time previously discussed in Sect.
5.1.
We believe that this will help readers better familiarise themselves with our approach.
Formula (16) is comparable to the next - for examining the previous PTL- formula with the standard past-time operator  state: first  [?]
- q ).
2(p [?]
(17)  By "comparable", we mean here that an interval s satisfies the first formula (16) iff the pair (s, 0) satisfies the second formula (17).
Our use of the PTL- derived - q ) only construct first in formula (17) ensures that the second subformula 2(p [?]
considers intervals with no past.
This is in order to conform to the time model for PITL which, unlike PTL- , lacks past time.
It can be useful to consider the simple case where the interval s has just one state.
Observe that s satisfies the first formula (16) iff p is false in that state.
Similarly, the pair (s, 0) satisfies the PTL- formula (17) iff p is false in s 's single state.
If s has exactly two states, then either p is false in both of them or else the initial state has p false and q true and the second one has p true.
Pnueli [66] and Lichtenstein, Pnueli and Zuck [41] give early accounts about how to formalise safety properties for mutual exclusion using past-time formulas of the form 2(w [?]
X ), where the temporal formula X only concerns past states and perhaps the current state, but not future ones.
We later look at such approaches in more detail in Sect.
16.2.
Here is another example of a 2f -formula which is an instance of (15) and hence 2-to-1 by Theorem 6.1:  f (fin p) [?]
3!p .
2  (18)  - of 3: This is analogous to the next PTL- formula with the past-time variant 3  first  [?]
- 2(p [?]
3!p ).
Compositional reasoning using intervals and time reversal  Introduction Sequential combining Extension rightward Parallel combining Iteration  A' [?]
A |= (A; A) [?]
A |= (A; A') [?]
A |= (A1 [?]
A2 ) [?]
A' |= A+ [?]
A, |= (w [?]
A* ) [?]
A |=  27  Sect.
Sect.
Sect.
Sect.
Sect.
7 6 8 9 10  Table 4 ISEPI compositional techniques for a 2-to-1 formula for backward analysis  7 ISEPI introduction of 2-to-1 formulas for backward analysis  Recall the ISEPI techniques previously described in Sect.
4.1.
A large part of this Sect.
7 and the subsequent Sects.
8-10 concerns the ISEPI techniques for 2-to-1 formulas for backward analysis, including iteration of such formulas.
The 2-to-1 formulas and their ISEPI techniques will also be extensively used for backward analysis when we formally study mutual exclusion in Sects.
11-13.
Table 4 gives a summary of our presentation of ISEPI techniques in the previous, current and next sections concerning 2-to-1 formulas for backward analysis.
In this section we consider the ISEPI technique of Introduction for use with backward analysis.
It can provide a way for a 2-to-1 formula A to be implied from another one A' (i.e., |= A' [?]
A).
We now discuss two simple valid implications to  do the ISEPI technique of f Introduction with the 2-to-1 formula 2 (fin w) [?]
3B .
As we already mentioned in Sect.
4.1, such implications can be quite important since they provide a way to start a compositional analysis involving the 2-to-1 formula.
Therefore, readers should make sure that they understand the material here.
Instances of the implications are later used in our analysis of mutual exclusion in Sects.
11-13.
The first valid implication for ISEPI Introduction considered here concerns situations where the state formula w is everywhere false.
The implication provides a way to introduce from a quite simple 2-to-1 formula 2!w in PTL the much more  complicated 2-to-1 PITL formula 2f (fin w) [?]
3B : |=  2!w  [?]
 f (fin w ) [?]
3B .
2  (19)  Proof (Validity of (19)) This follows from the fact that if in an interval the state formula w is always false, then the PTL formula fin w is false in every finite subinterval.
Hence, for such an interval the implication (fin w) [?]
3B is trivially true in all finite subintervals.
It also follows that the details of B are irrelevant.
[?]
[?]
We can alternatively show the validity of implication (19) by observing that the formula 2!w is equivalent to 2f fin !w (i.e., |= 2!w [?]
2f fin !w).
Now 2f fin !w is f semantically equivalent to 2!
fin w and in addition, simple propositional reasoning ensures that !
fin w implies (fin w) [?]
3B in each finite prefix subinterval.
The next valid implication is an example of (19) and its simple form of ISEPI Introduction : |=  2!p  [?]
 f (fin p) [?]
3q .
2  This can be interpreted as stating that if p is always false, then every state with p true is (non-strictly) preceded by a state with q true.
28  Ben Moszkowski  Below is a variant of (19) for ISEPI Introduction which relaxes the requirement in finite intervals that w is everywhere false.
Instead, w only has to be false in all states except for perhaps the last one: |=  2(more [?]
!w)  [?]
(inf  [?]
3B )  [?]
 f (fin w ) [?]
3B .
2  (20)  Observe that if the subformula B is in PTL, so is the antecedent of (20).
Proof (Validity of (20)) We consider the two cases for finite and infinite intervals  separately.
The case for infinite ones is easier, so we look at it first.
- For any infinite interval s , the formula more is true for each of s 's suffix subintervals (including s itself).
As a result, the formula 2(more [?]
!w) is semantically equivalent to 2!w.
Therefore, the previous valid implication (19) ensures that s also satisfies 2f (fin w) [?]
3B .
- On the other hand, suppose s is a finite interval which satisfies the antecedent of (20).
Hence, s satisfies 2(more [?]
!w), so in each proper prefix subinterval s ' of s , the PTL formula fin w is false.
This in turn ensures that (fin w) [?]
3B is true in all such s ' .
In addition, s itself satisfies 3B , so it likewise satisfies the implication (fin w) [?]
3B .
Hence, each prefix subinterval of s , including s  f itself, satisfies (fin w) [?]
3B .
Therefore, s also satisfies 2 (fin w) [?]
3B .
[?]
[?]
Below is a simple valid instance of (20) and the relaxed form of ISEPI Introduction : |=  2(more [?]
!p)  [?]
(inf  [?]
3q )  [?]
 f (fin p) [?]
3q .
2  In the later Sects.
11-13 on mutual exclusion, the first simpler variant (19) of ISEPI Introduction will be used when a process is not in its critical section.
The second relaxed version (20) finds application for the process step in which a request is made to enter the critical section.
8 ISEPI extension of 2-to-1 formulas rightward for backward analysis  We have so far presented the 2-to-1 formulas for backward analysis and looked at associated ISEPI techniques for Introduction and Sequential combining.
Here is an example of the ISEPI technique for Extending rightward already discussed in Sect.
4.1: |= (!p [?]
2q ); 2q [?]
!p [?]
2q.
(21) Below is a proof using a chain of valid implications showing that the 2-to-1 formula !p [?]
2q is extended rightward by the 2-to-1 formula 2q : (!p [?]
2q ); 2q  [?]
!p [?]
(2q ; 2q )  [?]
!p [?]
2q.
We later use implication (21) in Sect.
8.2 when we illustrate how to incrementally obtain another variant of the ISEPI technique for Extending rightward a 2-to-1 formula.
The earlier Theorem 4.8 in Sect.
4.2 concerns 2-to-1 formulas being closed under 2f .
The next Theorem 8.1, which naturally generalises Theorem 4.8, provides an incremental way to adapt the ISEPI technique of Extending rightward a formula ' f f A using another one A' to Extending rightward the formula 2A using 2A .
Compositional reasoning using intervals and time reversal  29  Theorem 8.1 For any PITL formulas A and A' , we have the semantic inference rule below:  ' |= f f f finite [?]
(A; A') [?]
A = |= (2A ); 2A [?]
2A.
(22) Proof The reasoning in Theorem 4.8's proof can be readily adapted for application to (22) by simply using two formulas A and A' instead of just one.
For example,  here is a chain of valid implications which generalises the earlier one (12): ' [?]
' f f f A [?]
(A A ) (2A )[?]
2A [?]
2    f A [?]
A [?]
2    f [?]
2A.
[?]
[?]
Theorem 8.1 is later used in Sect.
8.2 in Theorem 8.7's proof.
Incidentally, a symmetric variant of Theorem 8.1 to generalise Theorem 4.5 using 2 instead of ' f is possible (i.e., |= (A ; A) [?]
A 2 = |= ((2A' ); 2A) [?]
2A).
This can facilitate adapting the ISEPI technique of Extending leftward a formula A using another one A' to Extending leftward the formula 2A using 2A' .
8.1 A class of formulas for use with ISEPI extending rightward There are various classes of formulas which, like the 2-to-1 formulas, are closed under conjunction and 2.
Our presentation now considers one for use in the next Sect.
8.2 with the  ISEPI technique for Extending rightward the 2-to-1 formula f 2 (fin w) [?]
3B for backward analysis.
This material finds later application in our analysis of mutual exclusion in Sects.
11-13 when we merge some sequential steps of a process together.
f formulas) Any PITL formula A for which the implication Definition 8.2 (1-to-2 f f formula.
A [?]
2A is valid is called a 1-to-2 f formula therefore has the property that if it is true in an interval, then A 1-to-2 it is also true in all the interval's finite prefix subintervals.
The 1-to-2f formulas include all state formulas and 2f -formulas as well as the formula finite because the next three implications are all valid:  |=  f w [?]
2w  |=  f f (2B ) [?]
2f 2B  |=  f finite [?]
2finite.
f since any one-state interval falsifies the imThe NL1 formula more is not 1-to-2 f plication more [?]
2more .
However, we have the next lemma for a general syntactic class of NL1 formulas involving more :  Lemma 8.3 For any NL1 formula T , the NL1 implication more [?]
T is a 1-to-2f formula.
Proof We consider two cases for intervals with just one state and with more than  one state.
In each case, we show that the intervals indeed satisfy the following implication: (more [?]
T ) [?]
2f (more [?]
T ).
(23) If an interval s has just one state, then the PTL formula more is false, so the interval satisfies more [?]
T .
Furthermore, in a one-state interval, any PITL formula f A is semantically equivalent to 2A .
Therefore, the interval s satisfies the PITL formula 2f (more [?]
T ) and hence also implication (23).
30  Ben Moszkowski  Now consider an interval s which has more than one state and satisfies the NL1 implication more [?]
T .
It follows that the interval also satisfies more and therefore the NL1 formula T as well.
The formula T , like any NL1 formula, can only test at most the first two states of an interval, so all of s 's finite prefix subintervals with two or more states also satisfy T .
It follows that every finite prefix subinterval of s , including the initial one-state one, satisfies the implication more [?]
T .
Therefore, f -formula 2 f (more [?]
T ) and hence also implication (23).
s itself satisfies the 2 [?]
[?]
Lemma 8.4 The 1-to-2f formulas are closed under conjunction.
That is, if f f (A [?]
B ).
and |= B [?]
2B, then also |= (A [?]
B ) [?]
2 Proof Here is a simple semantic proof with four steps: f 1 |= A [?]
2A f 2 |= B [?]
2B | = f f f (A [?]
B ) 3 2A [?]
2B [?]
2 4 |= A [?]
B [?]
2f (A [?]
B )  |=  f A [?]
2A  Assumption Assumption PITL 1-3, Prop.
[?]
[?]
f formulas is somewhat analogous to the earlier Theorem 8.5 below for 1-to-2 Theorem 4.5 concerning closure under 2 for 2-to-1 formulas: f Theorem 8.5 If A is 1-to-2, so is 2A.
That is, from the valid implication follows the next one: |= f 2A [?]
22A.
Proof Here is a short semantic proof: f 1 |= A [?]
2A f 2 |= 2A [?]
22A | = f f 3 22A [?]
22A f 4 |= 2A [?]
22A  |=  f A [?]
2A  Assumption 1, PTL PITL 2, 3, Prop.
[?]
[?]
Now for any NL1 formula T , the implication more [?]
T is also in NL1 .
So we already have by Lemma 4.6 that the PTL formula 2(more [?]
T ) is 2-to-1.
It follows f .
This includes from Lemma 8.3 and Theorem 8.5 that 2(more [?]
T ) is also 1-to-2 the PTL formula stable p defined in Table 1.
The operator stable frequently occurs in applications of ITL, so it is convenient that a formula such as stable p is both f .
The PTL formulas w , 2w and finite are likewise 2-to-1 and 2-to-1 and 1-to-2 1-to-2f .
The formula 2q already mentioned in the sample valid implication (21) is an example.
We shortly make use of the formula 2q being 1-to-2f .
Remark 8.6 Another simple example of formulas which are closed under conjunction and 2 is the set of 1-to-2 formulas for any A for which A [?]
2A is valid.
f ones.
We do not These are to a degree time-wise symmetric versions of the 1-to-2 further discuss here the theory of the 1-to-2 formulas but briefly encounter them  later in Sect.
13.3 (when we analyse formula (92)).
8.2 Incremental version of ISEPI technique to extend a 2-to-1 formula rightward f formulas just presented in Sect.
8.1.
We now provide an application of the 1-to-2 The main result here is Theorem 8.7, which provides a way to do the ISEPI  Compositional reasoning using intervals and time reversal  31    technique of Extending rightward the 2-to-1 formula 2f (fin w) [?]
3B for backward analysis.
Theorem 8.7 concerns a semantic inference rule for ensuring that if B is Extended rightward by a suitable PITL formula C , then the 2-to-1 formula f (fin w ) [?]
3B , which contains B as a subformula, is itself Extended rightward by 2 the conjunction w [?]
C .
Theorem 8.7 Let B be a PITL formula, C be a 1-to-2f formula and w be a state formula.
Then the following semantic inference rule is sound: |=  (B ; C ) [?]
B  |=  =    ' ' f f (2B ); (w [?]
C ) [?]
2B ,  (24)   ' f f (fin w ) [?]
3B .
where 2B is simply the 2-to-1 formula 2  The second implication in the semantic inference identical to the following  rule is ' f f one which does not abbreviate 2 (fin w) [?]
3B as 2B :   f (fin w ) [?]
3B 2      ; (w [?]
C ) [?]
2f (fin w) [?]
3B .
(25)  Here is now the proof of Theorem 8.7: Proof (Theorem 8.7) If we have the valid implication |= (B ; C ) [?]
B , then the following instance of the same ISEPI technique for Extending rightward is also  valid:    (fin w) [?]
3B ; (w [?]
C )  |=    (fin w) [?]
3B .
[?]
(26)  Here is a chain of valid implications to justify this from its sole required assumption |= (B ; C ) [?]
B :   (fin w) [?]
3B ; (w [?]
C ) [?]
(3B ); C  [?]
[?]
  (fin w) [?]
3B  3(B ; C )  [?]
3B    [?]
 fin w ; C    (fin w) [?]
3B .
[?]
(27)  From implication (26) and Theorem 8.1 then follows the validity of the next implication that is a variation of (26): |=    2 (fin w) [?]
3B f    ; 2f (w [?]
C )  [?]
 f (fin w ) [?]
3B .
2  (28)  f The formula 2f (w [?]
C ) can be re-expressed as w [?]
2C .
This and our assumption that C is 1-to-2f permit us to obtain the next chain of valid implications:  w  [?]
C  [?]
w  [?]
f 2C  [?]
f (w 2  [?]
C ).
Consequently, the next implication is valid: |=    f (fin w ) [?]
3B 2 ; (w [?]
C )  [?]
  f (fin w ) [?]
3B 2    ; 2f (w [?]
C ).
This together with the earlier one (28) ensures the validity of implication (25) and therefore the desired soundness of semantic inference rule (24).
[?]
[?]
32  Ben Moszkowski  Like the semantic inference rule in the previous Theorem 8.1, semantic inference rule (24) provides an incremental way to obtain an instance of an ISEPI technique from a simpler variant of it.
We illustrate the use of Theorem 8.7 by taking as examples of w, B and C the propositional variable p, and the two PTL formulas !p [?]
2q and 2q , respectively.
Here is the associate instance of |= (B ; C ) [?]
B : |=  (!p [?]
2q ); 2q  [?]
!p [?]
2q.
This was already presented as implication (21) at the beginning of this section to provide a simple example of the ISEPI technique of Extending rightward.
It was furthermore shown there to be valid.
The formula 2q is 1-to-2f (and 2-to-1).
Implication (21) can therefore serve as the first implication required by Theorem 8.7's semantic inference rule (24) to obtain the sample instance below of implication (25) for the ISEPI technique of Extending rightward the 2-to-1 formula f (fin p) [?]
3(!p [?]
2q ) : 2 |=    f (fin p) [?]
3(!p [?]
2q ) 2      ; (p [?]
2q ) [?]
2f (fin p) [?]
3(!p [?]
2q ) .
The particular instance of C we later use in our analysis in Sect.
13 of mutual exclusion for Peterson's algorithm is the conjunction (70) of two formulas each of form 2(more [?]
T ), where T is in NL1 .
Such formulas are conveniently both 2-to-1 and 1-to-2f , as we already noted above in Sect.
8.1.
The PTL formulas w, 2w and f (and additionally 2-to-1), so they can likewise be included in finite are also 1-to-2 such conjunctions which serve as instances of C .
9 ISEPI parallel combining of 2-to-1 formulas for backward analysis  The next Lemma 9.1 involves an instance of the ISEPI technique already discussed in Sect.
4.1 for the Parallel combining of two suitable 2-to-1 formulas.
It will be needed later on to obtain Corollary 11.1 in Sect.
11.2.3.
That lemma concerns mutual exclusion and gives a way to establish that two processes operating in parallel are not simultaneously in their critical sections.
We consider  here the f f conjunction of two 2-to-1 2-formulas each of the form 2 (fin w) [?]
3B : Lemma 9.1 For any state formulas w and w' and PITL formulas B and B ' , suppose the following implication is valid: |=  3B  [?]
3B '  [?]
(29)  inf .
Then the next implication for use with ISEPI Parallel combining and backward analysis is also valid: |=  f (fin w ) [?]
3B 2    [?]
' ' f (fin w ) [?]
3B 2    [?]
2!
(w  [?]
w ' ).
(30)  The assumption (29) states that the formulas B and B ' cannot both occur in suffix subintervals of any finite interval.
Compositional reasoning using intervals and time reversal  33  Before proving Lemma 9.1, we discuss some illustrative examples of implications (29) and (30), respectively: |= |=  3(skip  [?]
q)  [?]
3(skip  f (fin p) [?]
3(skip 2  [?]
[?]
q)    !q ) [?]
[?]
inf '  f (fin p ) [?]
3(skip 2  [?]
!q )    (31) [?]
'  2!
(p [?]
p ).
(32)  The first implication (31) expresses that a finite interval with two or more states cannot have the propositional variable q being both true and false in the penultimate state (i.e., the one next to last).
Observe that the antecedent of implication (31) is actually unsatisfiable, so the implication is vacuously true.
The second implication (32) involves backward analysis to specify that any state with p true is immediately preceded by one with q true, and similarly each state with p' true is immediately preceded one with q false.
This implication is comparable to the valid PTL- formula below: -q) 2(p [?]
|=  - !q ) 2(p' [?]
[?]
[?]
2!
(p [?]
p' ).
By "comparable", we mean here that an interval s satisfies the PITL implication (32) iff the pair (s, 0) satisfies the PTL- formula.
Proof (Lemma 9.1) Here is a proof in steps which assumes the validity of (29): - The next implication is valid by the assumed validity of (29) together with  propositional reasoning: |=  !inf  (fin w) [?]
3B  [?]
  [?]
(fin w' ) [?]
3B '    [?]
(!
fin w) [?]
(!
fin w' ).
This contains the subformula !inf in the antecedent and so concerns behaviour in finite intervals.
- We then have the following chain of valid implications involving PTL-based reasoning about the operator fin : (!
fin w) [?]
[?]
(!
fin w' ) [?]
(fin !w) [?]
(fin !w' ) fin (!w [?]
!w' ) [?]
fin !
(w [?]
w' ).
The valid implication below, which is suitable for ISEPI Parallel combining, subsequently results from combining the previous one and this chain: |=  !inf  [?]
(fin w) [?]
3B    [?]
(fin w' ) [?]
3B '    [?]
fin !
(w  [?]
w ' ).
- The following implication for ISEPI Parallel combining, which is about finite  prefix subintervals, is consequently valid: |=  f 2!inf  [?]
f (fin w ) [?]
3B 2    [?]
' ' f (fin w ) [?]
3B 2    [?]
f fin !
(w 2  [?]
w ' ).
This is because for any PITL formulas A1 , .
.
.
, An and A' , if the implication ' f f f (A1 [?]
* * * [?]
An ) [?]
A' is valid, so is (2A (and indeed n ) [?]
2A 1 ) [?]
* * * [?]
(2A  also (2A1 ) [?]
* * * [?]
(2An ) [?]
2A' ).
f - The subformula 2!inf is trivially true because !inf is semantically equivalent to finite and therefore true in all finite intervals.
Furthermore, the subformula ' ' f fin !
(w [?]
w ) and the PTL formula 2!
(w [?]
w ) are semantically equivalent.
2 This is because for any interval s , the set of the final states of s 's finite prefix subintervals (which 2f fin !
(w [?]
w' ) examines) and the set of s 's states (which 2!
(w [?]
w' ) examines) are identical.
Hence, the previous valid implication is semantically equivalent to our goal (30), which is therefore also valid.
[?]
[?]
34  Ben Moszkowski  10 ISEPI iteration of 2-to-1 formulas for backward analysis  We now define some natural variants of 2-to-1 formulas which involve the iterative constructs chop-star and  chop-plus instead of chop.
It turns out that the 2-to-1 formula 2f (fin w) [?]
3B for backward analysis has special connections with such variants.
The associated ISEPI technique for Iteration finds application in our analysis of mutual exclusion in Sects.
11-13 when we consider multiple requests by a process to a shared resource.
Definition 10.1 (*-to-1 formulas) Any PITL formula A for which the implication A* [?]
A is valid is called a *-to-1 formula.
Definition 10.2 (+-to-1 formulas) Any PITL formula A for which the implication A+ [?]
A is valid is called a +-to-1 formula.
Here is a brief summary of the three classes of formulas we have defined for sequential composition: 2-to-1 formulas *-to-1 formulas +-to-1 formulas  (A; A) [?]
A  |=  A* [?]
A |= A+ [?]
A.
|=  The three categories are all closed under conjunction and 2 (e.g., see Lemmas 10.4 and 10.5 below for +-to-1 formulas).
It is not hard to see that any formula A which is +-to-1 is 2-to-1: Lemma 10.3 Every +-to-1 formula is also 2-to-1.
Proof Observe that for any PITL formula A, we have that A; A implies A+ : |= (A; A) [?]
A+ .
Now if A is +-to-1, then A+ in turn implies A.
Hence, by transitivity, the formula A; A implies A as well, so the formula A is indeed 2-to-1.
Here is a corresponding chain of two valid implications: A; A  [?]
A+  [?]
A.
[?]
[?]
Likewise, any formula which is *-to-1 is also 2-to-1 and +-to-1 as well because of the valid PITL implications |= (A; A) [?]
A* and |= A+ [?]
A* , which respectively yield the following two chains of valid implications: A; A A+  [?]
[?]
A* A*  [?]
[?]
A A.
However, the three categories are by no means identical.
Below are sample formulas which illustrate this point: finite p empty  2-to-1 X X X  *-to-1  +-to-1  X  X X  Compositional reasoning using intervals and time reversal  35  The reason not every 2-to-1 formula is also +-to-1 is because of the situation in infinite time.
Consider the 2-to-1 formula finite .
Any infinite interval satisfies finite + but not finite .
The same reasoning holds if we replace finite + by finite * , so the formula finite is therefore also not *-to-1.
All 2-to-1 2f -formulas are also +-to-1 as is later shown in Theorem 10.7.
Fur f f thermore, the 2-formulas of the form 2 (fin w) [?]
3B , which we already considered for backward analysis, are subsequently shown in a meaningful formal sense to be nearly members of the class of *-to-1 formulas.
Certain instances of such formulas can then be profitably used in our analysis of mutual exclusion when we want to compositionally analyse the behaviour of a process making multiple requests to a shared resource.
Let us now discuss further properties of 2-to-1 and +-to-1 formulas.
We make some use of *-to-1 formulas as well and later mention in Sect.
15.5 their connection with our earlier work on compositionality in ITL.
Lemma 10.4 For any +-to-1 formulas A and B, their conjunction A [?]
B is +-to-1 as well.
That is, if |= A+ [?]
A and |= B + [?]
B, then also |= (A [?]
B )+ [?]
(A [?]
B ).
Proof The formula (A [?]
B )+ implies both A+ and B + and consequently also their conjunction A+ [?]
B + .
Our assumption that A and B are both +-to-1 then guarantees that this implies A [?]
B .
Here is a corresponding chain of valid implications:  (A [?]
B )+  [?]
A+  [?]
B+  Lemma 10.5 If A is +-to-1, so is 2A.
That is, if 2A.
[?]
|=  [?]
[?]
A [?]
B. A+ [?]
A, then also  |=  (2A)+ [?]
Proof Let s be an interval which satisfies (2A)+ .
Our proof will check that each suffix subinterval of s , including s itself, satisfies A+ and hence also A.
Therefore, s satisfies 2A.
There are two cases to consider which depend on whether the  number of iterations is finite or infinite: - The chop-plus involves a finite number of sequential iterations of 2A: It follows that s satisfies the PITL formula (2A)[?][?]
2A containing strong versions of chop and chop-star.
Each of s 's suffix subintervals can then be shown to satisfy A+ and so also A, since A is +-to-1.
Hence, s satisfies 2A.
- The chop-plus involves o sequential iterations of 2A (so the interval is infinite): We can readily check that each suffix subinterval s ' of s satisfies Ao and hence also A+ .
Therefore, s ' satisfies A itself because A is +-to-1.
Consequently, s satisfies 2A.
[?]
[?]
The next theorem is the converse of Lemma 10.3, but necessarily restricted to finite intervals for reasons given shortly: Lemma 10.6 If A is 2-to-1, then it is +-to-1 for finite time, that is, the implication below is valid: finite [?]
(A+ [?]
A).
(33) Proof Let s be a finite interval satisfying A+ .
We want to show that s satisfies A as well.
Now for some natural number k >= 1, s satisfies k instances of A sequentially combined with k - 1 chops between then.
For example, if k is 3, then s satisfies A; A; A.
Note that in finite intervals, strong and weak chop have the same  36  Ben Moszkowski  semantics.
We do induction on the number of chops in the formula A; .
.
.
; A and employ the assumption that A is 2-to-1 to demonstrate that s satisfies A itself.
Therefore, A is indeed +-to-1 for finite-time intervals and hence implication (33) is valid.
[?]
[?]
We already pointed out above that the formula finite is an example of a 2to-1 formula which is not +-to-1 in infinite time.
This explains Lemma 10.6's requirement about finite time.
However, the next Theorem 10.7 demonstrates that all 2f -formulas which are 2-to-1 formulas are also +-to-1 even for infinite time.
Such formulas are moreover later used in our analysis of mutual exclusion in Sects.
11-13 for multiple requests by a process to a shared resource.
f Theorem 10.7 Any 2-to-1 formula 2B is also +-to-1 for all intervals, including infinite ones: |= f f (2B )+ [?]
2B.
(34)  The proof of Theorem 10.7 is given shortly.
Note that in contrast to a 2-to-1 2f -formula, a 2-to-1 2-formula, which looks at suffix subintervals rather than the prefix ones examined by 2f , is not necessarily +-to-1.
We can take the formula 2finite to serve as an example of this.
It is semantically equivalent to finite , which we already pointed out is not +-to-1 in infinite intervals.
We use Theorem 10.7 to provide an ISEPI technique for Iteration with chopplus.
This has the form |= A+ [?]
A.
The theorem ensures that the implication is indeed valid if we take A to be the 2-to-1 formula 2f (fin w) [?]
3B for backward analysis: |=    + f (fin w ) [?]
3B 2   f (fin w ) [?]
3B .
2  [?]
(35)  Before proving Theorem 10.7, we present a lemma which concerns a semantic inference rule used in the proof: Lemma 10.8 For any PITL formulas C and C ' , the next semantic inference rule is sound: ' ' |= (finite [?]
C + ) [?]
2C f f (36) = |= C [?]
C [?]
[?]
2C . '
f Proof We start by assuming the validity of the implication (finite [?]
C + ) [?]
2C .
Our goal is to show from this that any interval s which satisfies C [?]
C [?]
also satisfies ' f 2C .
Let s ' be any finite prefix subinterval of s (including s itself if it is finite).
Our ' f proof will show that any such s ' satisfies C ' and hence s itself satisfies 2C .
Now [?]
[?]
' '' s satisfies C C , so s is contained in a finite prefix subinterval s which likewise satisfies C [?]
C [?]
and so also both C + and finite [?]
C + .
Hence by the assumption, ' f s '' also satisfies 2C , so its prefix subinterval s ' satisfies C ' .
It follows that all of the finite prefix subintervals of s indeed satisfy C ' , and therefore s itself satisfies ' f 2C .
[?]
[?]
We now supply Theorem 10.7's proof: Proof (Theorem 10.7) Case for finite time: Lemma 10.6 ensures that (33) is valid  for finite time for any 2-to-1 formula, so the next instance of (33), which is moreover a variant of (34), is valid as well: finite  [?]
f (2B )+    [?]
f 2B.
Compositional reasoning using intervals and time reversal  37  Case for infinite time: Our goal here is to show the validity of the next implica-  tion: inf  [?]
f (2B )+  f Let A denote 2B .
We re-express inf  |=  inf  [?]
A+  [?]
[?]
  [?]
f 2B.
A+ :  (inf  [?]
A[?]
)  A[?]
[?]
(inf  [?]
The subformula A[?]
[?]
(inf [?]
A) is re-expressible as inf proof can be divided into two parts: (A[?]
[?
]A)  |=  inf  |=  inf [?]
A[?]
[?]
[?]
[?]
[?]
[?]
 A) .
(A[?]
[?
]A), so our semantic  A  A.
(37) (38)  Subcase for (37): We already have A+ [?]
A valid for finite time.
Therefore, the chain of implications below is valid since A is 2-to-1 and A[?]
occurs in the finite  left of  [?]
: A[?]
[?
]A  [?]
[?]
(empty [?]
A+ )[?
]A [?]
(empty [?
]A) [?]
(A+ [?
]A) A [?]
(A[?
]A) [?]
A [?]
(A; A) [?]
A [?]
A [?]
A.
Hence, formula (37) is valid.
f -formula 2B f Subcase for (38): Recall that A denotes here the 2 .
Furthermore, f f f the PITL equivalence 2B [?]
22B is valid (much like the valid PTL equivalence |= 2p [?]
22p).
Hence, we have |= A [?]
2A f .
Lemma 10.8 permits us to take an instance of the sound semantic inference rule (36) with C and C ' both A: |=  (finite  [?]
f A+ ) [?]
2A  f We then replace each 2A by A using  |=  (finite  [?]
|=  A+ ) [?]
A  =  |=  f A[?]A[?]
[?]
2A.
f A [?]
2A :  =  |=  A[?]A[?]
[?]
A.
  f f We already have the validity of finite [?]
(2B )+ [?]
2B from the case for finite f time.
This is re-expressed using A instead of 2B to obtain |= (finite [?]
A+ ) [?]
A.
The semantic inference rule then yields that the implication A[?]A[?]
[?]
A is also valid.
In infinite time, A[?]
and A[?]
A[?]
are semantically equivalent, so our goal (38) is valid.
The combination of (37) and (38) ensures (34) is valid for infinite intervals.
Our proof's two cases for finite and infinite intervals then yield (34)'s validity for all intervals.
[?]
[?]
Remark 10.9 Let us briefly note without proof some interesting facts not needed i here.
Recall from Lemma 4.3 that any formula w, T or 3C is 2-to-1.
They are in fact also +-to-1 even for infinite time.
Also, if B is 2-to-1, so are the two formulas w [?]
(B [?]
fin  w) and 2 w [?]
(B [?]
fin w) .
Reflection helps ensure that f f -formula is +-to-1 even for infinite time.
2 (fin w) [?]
(B [?]
w) is as well.
This 2 Furthermore, if the formula B * [?]
B is valid (i.e., B is *-to-1), then so is C * [?]
C , where C is any of these three formulas.
38  Ben Moszkowski  10.1 Zero or more sequential iterations of a 2-to-1 formula When we later compositionally analyse how a process can make multiple accesses to a shared resource, it is natural to include the case where no accesses are performed.
So it would be convenient in such circumstances to use for backward analysis some *-to-1 formulas introduced in Definition 10.1 at the beginning of this  Sect.
10.
Now every 2f -formula 2f (fin w) [?]
3B has already been shown to be 2to-1 (Theorem 6.1) and therefore also +-to-1 (Theorem 10.7 and implication (35)).
 However, we now show that 2f (fin w) [?]
3B is unfortunately not necessarily *to-1.
Nevertheless, we offer a workaround which is nearly *-to-1 and quite suitable for using as an ISEPI technique for Iteration when we look at mutual exclusion in Sects.
11-13.
Let us now present two lemmas  concerning the relationship between instances of the 2f -formula 2f (fin w) [?]
3B for backward analysis and the class of *-to-1 formulas:  f (fin w ) [?]
3B Lemma 10.10 Not every formula 2 is *-to-1.
f -formula and an interval which satisfies the weak chopProof We exhibit such a 2 star of the formula but not the formula itself.
Consider the previous 2f -formula (16),  which is reproduced below: f (fin p) [?]
3(skip 2  [?]
 q) .
Let s be a one-state interval with the propositional variable p true.
We show that s falsifies the next implication:   f (fin p) [?]
3(skip 2  [?]
q)  *  [?]
f (fin p) [?]
3(skip 2  [?]
 q) .
(39)  Now s , like every one-state interval, trivially satisfies any weak chop-star formula A* .
Therefore, s satisfies (39)'s antecedent.
In a one-state interval, the consequent of implication (39) reduces to the PTL formula p [?]
(skip [?]
q ).
However, the interval s , which sets p to true, cannot satisfy the subformula skip since that requires at least two states to be present.
[?]
[?]
The earlier sample 2f -formula (18) is also not *-to-1.
This is because in a one-state  interval, 2f (fin p) [?]
3!p simplifies to p [?]
!p, which is falsified if the interval sets p to true .
 The next lemma shows how instances of the 2f -formula 2f (fin w) [?]
3B can always be regarded as "almost" *-to-1 if we require w to initially equal false :  f (fin w ) [?]
3B , the next implication is valid: Lemma 10.11 For any formula 2 |=  !w  [?]
  f (fin w ) [?]
3B 2  *  [?]
 f (fin w ) [?]
3B .
2  (40)   f (fin w ) [?]
3B .
We already used Theorem 10.7 to show that Proof Let C denote 2 C is +-to-1 (i.e., see the earlier valid implication (35)).
Therefore, we have the  following: |=  C+  [?]
C.  (41)  Compositional reasoning using intervals and time reversal  39  This is the ISEPI technique of Iteration for chop-plus, as we previously mentioned with regard to (35).
We can also show the following: |=  empty  [?]
!w  [?]
(42)  C.  f This is because in a one-state interval any formula 2A is semantically identical to its operand A.
In particular, C is identical to (fin w) [?]
3B , which in a one-state interval further simplifies to w [?]
B .
So a one-state interval which satisfies !w also satisfies C .
Let us now look at merging the two implications (41) and (42) into a single one from which we can later on obtain that C is "almost" *-to-1:  |=  C+  [?]
(empty  [?]
!w)  [?]
(43)  C.  Simple propositional reasoning ensures that the conjunction !w implies the antecedent C + [?]
(empty [?]
!w) in (43): |=  !w  [?]
(empty  [?]
C+)  [?]
C+  [?]
(empty  [?]
[?]
(empty  [?]
C+)  !w).
We can then combine this and (43) using further straightforward propositional reasoning: |= !w [?]
(empty [?]
C + ) [?]
C. The PITL equivalence A* [?]
(empty [?]
A+ ) is valid for any formula A.
We use it to simplify empty [?]
C + to obtain the next valid implication: |=  !w  [?]
C*  [?]
C.  This is in fact identical to our goal (40).
[?]
[?]
For the convenience of readers, Table 5 lists the ISEPI techniques presented in Sects.
6-10 specifically for use with the 2-to-1 formulas for backward analysis.
The table can be used for reference when we apply the techniques to mutual exclusion in the next three Sects.
11-13.
11 Analysis of an abstract mutual exclusion algorithm  In this section and the next two, we consider how to apply compositional backward analysis and the previously introduced 2-to-1 formulas of the form 2f ((fin w) [?]
3B ) to mutual exclusion and Peterson's algorithm [64].
These have provided us with a rich and stimulating initial testing ground for developing and experimenting with our ideas about backward analysis in ITL.
They together also serve as a proofof-concept of the approach and at least at present are a rather inseparable part of our exploration of time symmetry.
We certainly do not claim that our research has reached a stage where it is ready to be deployed in practical problems.
Figure 1 shows a version of Peterson's algorithm.
One reason for looking at it is because it is a quite elegant and popular example of mutual exclusion and seems to serve as a kind of benchmark for formal techniques.
We will have much more to say about Peterson's algorithm in Sect.
13 where we formalise in PITL a version of it with two concrete processes P0 and P1 .
However, our analysis of mutual exclusion initially mostly focuses on a more abstract and higher-level  40  Ben Moszkowski  Introduction (first variant): Formula (19) in Sect.
7: |=   f (fin w ) [?]
3B .
2  [?]
2!w  Introduction (second variant): Formula (20) in Sect.
7: 2(more [?]
!w)  |=  [?]
(inf  [?]
3B )   f (fin w ) [?]
3B .
2  [?]
Sequential combining: (See Theorem 6.1)   f (fin w ) [?]
3B 2    ; 2f (fin w) [?]
3B     f (fin w ) [?]
3B .
2  [?]
Extending rightward: Formula (25) in Sect.
8.2:   |=  f (fin w ) [?]
3B 2      ; (w [?]
C ) [?]
2f (fin w) [?]
3B ,  where C is a 1-to-2f formula and extends B rightward (i.e., |= (B ; C ) [?]
B ).
Parallel combining: Formula (30) in Sect.
9: f (fin w ) [?]
3B 2  |=  where  |=  (3B  [?]
  [?]
' ' f (fin w ) [?]
3B 2    [?]
2!
(w  [?]
w ' ),  3B ' ) [?]
inf .
Iteration (version for +-to-1 formula): Formula (35) in Sect.
10: |=    + f (fin w ) [?]
3B 2  [?]
 f (fin w ) [?]
3B .
2  Iteration (version for "almost" *-to-1 formula): Formula (40) in Sect.
10.1: |=  !w  [?]
  f (fin w ) [?]
3B 2  *  [?]
 f (fin w ) [?]
3B .
2  Table 5 Summary of ISEPI techniques for 2-to-1 formulas for backward analysis  algorithm with two abstract processes Q0 and Q1 .
It contains shared aspects of several algorithms and proofs.
This is in part because our study of compositionality in Peterson's algorithm has helped us see benefits of applying time symmetry to formalising in PITL some abstract issues arising in mutual exclusion.
In the future we would of course like to gain more experience by considering other applications and also a range of modelling assumptions, but our research has not yet progressed to this stage.
Let us now review the notion of mutual exclusion.
It is one way to ensure that multiple processes safely access a shared resource.
Examples of it include cash machines accessing a single bank account and processes utilising a shared printer.
Compositional reasoning using intervals and time reversal  Process P0 noop 0 ; flag 0 := 1; turn := 1; await (flag 1 = 0 [?]
turn = 0); noop 0 ; (critical section) flag 0 := 0;  41  Process P1 noop 1 ; flag 1 := 1; turn := 0; await (flag 0 = 0 [?]
turn = 1); noop 1 ; (critical section) flag 1 := 0;  1.
1.
2.
2.
3.
3.
4.
4.
5.
5.
6.
6.
7. noop 0 7. noop 1 Initially flag 0 = flag 1 = 0.
The starting value of turn is unimportant.
Each statement noop i denotes a "no-operation" which does no assignments.
Fig.
1 A version of Peterson's algorithm with two concrete processes P0 and P1  Here is the general structure of a single access by one abstract process: (a) (b) (c) (d) (e)  Noncritical section; Request exclusive right to resource; Critical section with exclusive access; Release exclusive right to resource; Noncritical section.
(44)  Taubenfeld's textbook [76] gives English-language proofs of mutual exclusion for various algorithms, starting with Peterson's (as is indeed often the case in textbooks).
Unlike mutual exclusion proofs in conventional point-based temporal logic such as those by Pnueli [66] and Kroger and Merz [37], ours does not use a comprehensive set of labels for all relevant program steps.
This reflects the quite different nature of point- and interval-based approaches, which can be respectively referred to as endogenous and exogenous.
We have more to say about this later on in Sect.
15.2.
The abstract processes Q0 and Q1 and the associated analysis capture some general features of mutual exclusion which apply to many concrete algorithms, not just Peterson's.
A major benefit of the abstract framework is that it involves a fairly direct application of the ISEPI techniques for 2-to-1 formulas for backward analysis presented earlier in Sects.
6-10.
At first glance, it might seem easier to formalise something specific and tangible such as the processes P0 and P1 in Peterson's algorithm than to formalise the more abstract processes Q0 and Q1 .
However, before we can reason about the concrete Peterson processes P0 and P1 in PITL, their individual statements need to be expressed as PITL formulas.
This requires some further explanation and justification about the modelling assumptions used for concurrency and so is deferred until later in Sect.
13.
Furthermore, any analysis dealing just with the concrete processes P0 and P1 in Peterson's algorithm is of course much more limited than an analogous one about a higher-level framework for abstract processes Q0 and Q1 which can be adapted to many algorithms, including Peterson's.
In distinct contrast to the situation with modelling and reasoning about Peterson's algorithm in PITL, our ISEPI techniques for 2-to-1 formulas presented in Sects.
6-10 can be almost immediately used to provide a fairly concise and high-level analysis of the abstract processes Q0 and Q1 .
42  Ben Moszkowski  Nevertheless, it can be confusing to work just with the abstract algorithm without having any motivation provided by a concrete one.
Therefore, Fig.
1 shows our processes P0 and P1 for Peterson's algorithm.
We will only discuss certain aspects of them here which help with understanding the abstract processes Q0 and Q1 and the associated correctness formulas.
The concrete processes P0 and P1 in Fig.
1 together have three program variables flag 0 , flag 1 and turn with values in {0, 1}.
To stay propositional, let 0, 1 and = stand for false , true and [?
], respectively.
Both flag 0 and flag 1 are initialised to 0, but turn 's starting value is unimportant.
The statements noop 0 and noop 1 are simply "no-operations" or "no-ops" during which time the processes do not assign their respective variables values.
Real processes would likely examine and modify other variables besides flag i and turn , but we ignore them here.
The PITL semantics of noop i and other statements in each concrete process Pi are given in Sect.
13.
We do not need to know their details in our analysis of the abstract processes Q0 and Q1 .
11.1 Model with the abstract processes Q0 and Q1 The abstract processes Q0 and Q1 are modelled as executing together with iniV tialisation, and expressed as i[?
]{0,1} (init i [?]
Qi ).
Here init i is some state formula for initialising Qi 's variables.
The analysis assumes that each process Qi has an auxiliary boolean variable cs i true exactly when Qi is in its critical section and that init i sets cs i to false: |=  init i  [?]
(45)  !cs i .
Section 13 gives a concrete instance Pi' in (71) for each abstract process Qi .
We use Pi' to serve as a variant of Peterson's algorithm with additional formulas describing the behaviour of cs i .
In Sect.
13 we likewise define in (76) the concrete version pinit i of init i to be the state formula flag i = 0 [?]
!cs i .
As is already noted in Fig.
1, the initial value of the shared writable variable turn is not important.
For our analysis of the abstract algorithm, we need for each init i that the implication (45) is valid.
This is certainly the case with the concrete formula flag i = 0 [?]
!cs i , as the next valid implication demonstrates: flag i = 0  |=  [?]
!cs i  [?]
!cs i .
Our goal here is to have two general Abstract Assumptions which together with the valid implication (45) (i.e., |= init i [?]
!cs i ) suffice to ensure that the abstract processes Q0 and Q1 are never simultaneously in their critical sections (line (c) in (44)).
The Abstract Assumptions will be shortly introduced in Table 6 in the next Sect.
11.2.
We now briefly summarise our goal concerning mutual exclusion.
Our aim is to prove !
(cs 0 [?]
cs 1 ) is always true, as stated in the next implication: |=  ^  (init i  [?]
Qi )  [?]
2!
(cs 0  [?]
cs 1 ).
i[?
]{0,1}  The Abstract Assumptions will indeed be shown in Sect.
11.2 to be sufficient to guarantee the validity of this.
Compositional reasoning using intervals and time reversal  43  First Abstract Assumption for each i [?]
{0, 1} |=  init i  [?]
Qi  [?]
AbsSafe i  [?]
fin init i ,  (46)  where AbsSafe i is defined as 2f ((fin cs i ) [?]
3Di ) for some Di .
Second Abstract Assumption |=  3D0  [?]
3D1  [?]
inf  (47)  Table 6 The first and second abstract assumptions  Our abstract analysis can also be generalised to multiple accesses to the shared resource by processes as is discussed later in Sect.
11.3.
Properties besides mutual exclusion such as freedom from deadlock are not considered here, but they could be shown with our compositional techniques for liveness [50,51] using 2-to-1 formulas for forward analysis such as 2(p [?]
3q ).
11.2 Basic mutual exclusion for the abstract processes We now introduce the two Abstract Assumptions (46) and (47).
They concern mutual exclusion for the abstract processes Q0 and Q1 and describe some temporal behaviour.
Table 6 shows these Abstract Assumptions, which are individually referred to as the First Assumption (46) and the Second Assumption (47).
The following 2-to-1 formula AbsSafe i is used in the First Assumption (46) to describe a safety property for Qi : AbsSafe i  = b  f ((fin cs ) [?]
3D ).
2 i i  (48)  This formula AbsSafe i is an instance of the 2-to-1 formula 2f ((fin w) [?]
3B ) introduced in Sect.
6 for compositional backward analysis.
Our experience is that many readers have trouble grasping the intuition behind the Abstract Assumptions (46) and (47) and in particular the role of the abstract formulas Di and AbsSafe i .
We therefore first discuss concrete instances of Di and AbsSafe i and only then give a general explanation of the Abstract Assumptions.
One aim here is to preview some aspects of our analysis of Peterson's algorithm in Sect.
13.
11.2.1 Justification of the first abstract assumption  As already noted above in Sect.
11.1, we later on define a variant process Pi' of each process Pi in (71) in Sect.
13 (more precisely, in Sect.
13.2).
The purpose of Pi' is to ensure that the auxiliary variable cs i is indeed true exactly when the process Pi is in its critical section.
The later definition of process Pi' in (71) is reproduced below for the convenience of readers in order to assist in our explanation of Di and AbsSafe i :  Pi' = b Pi 's lines 1-3 [?]
stable csi ; Pi 's line 4 [?]
cs i <~ true ; Pi 's line 5 [?]
cs i <~ false ; Pi 's lines 6-7 [?]
stable cs i .
44  Ben Moszkowski  Recall the derived constructs stable and padded temporal assignment (<~) given in Table 1.
The actual concrete instance of Di defined and used in Sect.
13.3 for our analysis of Peterson's algorithm is the formula Ei given below (see (73)): Ei  = b  flag i = 1  [?]
turn = 1 - i  [?]
3(flag 1-i = 0  [?]
turn = i)  [?]
nochange i .
Here nochange i is defined later in Sect.
13.1 as part of the PITL semantics of the statements in Peterson's algorithm.
The formula nochange i specifies that the process Pi is not assigning any values to flag i and turn .
Further details about nochange i are not needed for the moment.
Let us consider a concrete formula PeteSafe i which summarises some key behaviour observable when process Pi' is its critical section.
We later formally define PeteSafe i as (74) in Sect.
13.3 to be a concrete version of AbsSafe i with the concrete instance Ei of the abstract formula Di .
The definition is reproduced below for the convenience of readers: PeteSafe i  = b  f ((fin cs ) [?]
3E ).
2 i i  We now informally show that if the first state of an interval s satisfies the initialisation formula flag i = 0 [?]
!cs i for process Pi' and the interval s itself satisfies the formula Pi' , then each finite prefix subinterval s ' of s satisfies the next concrete formula, and hence s satisfies PeteSafe i : (fin cs i )  [?]
3Ei .
Here is the main goal expressed as an implication which the overall interval s satisfies:  f (fin cs ) [?]
3E flag i = 0 [?]
!cs i [?]
Pi' [?]
2 i i .
Suppose the finite prefix subinterval s ' of s satisfies fin cs i .
It follows that cs i is true in the last state of s ' and hence process Pi' is in its critical section during this state.
Inspection of our definitions of Pi' and Pi reveal that whenever the variable cs i is true, this is preceded by the execution of the following sequence of statements in Pi : flag i := 1; turn := 1 - i; await (flag 1-i = 0  [?]
turn = i).
In the state immediately after the first two of these statements, the formula flag i = 1 [?]
turn = 1 -i is true.
That state in fact marks the start of some suffix subinterval s '' in s ' which satisfies Ei .
This is because when the process manages to enter its critical section following the execution of the await statement, the test flag 1-1 = 0 [?]
turn = i must have succeeded.
So the state with cs i = 1 will be preceded by a state with flag 1-1 = 0 [?]
turn = i.
That state is itself preceded by a state with flag i = 1 [?]
turn = 1 -i.
Furthermore, from the moment that flag i = 1 [?]
turn = 1 -i is true in that state until when the process leaves its critical section, the process does not assign either flag i or turn , so the formula nochange i holds.
Therefore, the suffix subinterval s '' of s ' (itself a finite prefix of s ) satisfies the following formulas: flag i = 1 [?]
turn = 1 - i  3(flag 1-1 = 0 [?]
turn = i)  nochange i .
Compositional reasoning using intervals and time reversal  45  Consequently, s '' satisfies Ei , which is simply the conjunction of these, and s ' itself satisfies 3Ei and hence also (fin cs i ) [?]
3E  i .
It follows that the overall inf terval s satisfies the formula 2 (fin cs i ) [?]
3Ei , which is the same as PeteSafe i .
Therefore, the following concrete instance of the First Assumption (46) for the concrete instance Pi' of the abstract process Qi is valid: |=  flag i = 0  [?]
!cs i  [?]
Pi'  f (fin cs ) [?]
3E 2 i i  [?]
  [?]
fin (flag i = 0  [?]
!cs i ).
(49) The formula 2f (fin cs i ) [?]
3Ei is identical to the concrete version PeteSafe i of AbsSafe i .
In (49) we also mention the concrete formula fin (flag i = 0 [?]
!cs i ) about the last state if the process Pi' terminates.
This formula, which is a concrete version of the formula fin init i in the First Assumption (46), is easy to check from the behaviour of Pi' .
In our analysis of Peterson's algorithm in Sect.
13, we let the concrete instance pinit i of the abstract initialisation formula init i denote this conjunction flag i = 0 [?]
!cs i (as already noted in Sect.
11.1).
The First Assumption (46) is an abstracted version of implication (49), where we leave the fine points of Qi , Di and the 2-to-1 formula AbsSafe i largely unspecified.
The formula fin init i in the First Assumption later helps to iterate Qi using Q* i in Sect.
11.3 when we compositionally reason about multiple requests to access a shared resource.
This concludes our motivation for the First Assumption (46).
  11.2.2 Justification of the second abstract assumption  We now turn to motivating the Second Assumption (47) in Table 6.
Let us consider the undesirable situation where both processes P0' and P1' in Peterson's algorithm somehow end up simultaneously in their critical sections.
Suppose the processes are running in an interval s .
Therefore, the concrete instance (49) of the First Assumption (46) ensures that s satisfies PeteSafe 0 and PeteSafe 1 .
Furthermore, the failure of mutual exclusion means that s has a state satisfying cs 0 [?]
cs 1 .
Let s ' denote the finite prefix subinterval of s ending with that state.
It follows that s ' satisfies fin (cs 0 [?]
cs 1 ).
The combination of the fact that s satisfies PeteSafe 0 and PeteSafe 1 together with the definition of PeteSafe i moreover ensures that s ' must satisfy the concrete implications (fin cs 0 ) [?]
3E0 and (fin cs 1 ) [?]
3E1 .
Here is a summary of this: s'  |=  fin (cs 0  [?]
s'  cs 1 )  |=  (fin cs 0 ) [?]
3E0  s'  |=  (fin cs 1 ) [?]
3E1 .
Hence, s ' also satisfies the two formula 3E0 and 3E1 .
Consequently, if we can somehow prove that in fact the conjunction (3E0 ) [?]
(3E1 ) is not satisfied by any finite interval (such as s ' ), it follows from a proof by contradiction that s ' does not exist.
Instead, all finite prefix subintervals of s satisfy fin !
(cs 0 [?]
cs 1 ), so s itself satisfies 2f fin !
(cs 0 [?]
cs 1 ), which is semantically equivalent to 2!
(cs 0 [?]
cs 1 ).
This demonstrates that mutual exclusion is achieved.
Indeed, we later show in Sect.
13.3 (Lemma 13.9) the validity of following implication about 3E0 and 3E1 not being simultaneously satisfiable in a finite interval: |=  3E0  [?]
3E1  [?]
inf .
The Second Assumption (47) in Table 6 is simply a much more abstract version of this implication which can capture behaviour in many mutual exclusion algorithms.
This concludes our motivation for the Second Assumption.
46  Ben Moszkowski  11.2.3 Proof of mutual exclusion from the two abstract assumptions  Lemma 11.2 shortly establishes that the First and Second Assumptions together suffice to ensure mutual exclusion for the abstract processes Q0 and Q1 as stated in the following implication: ^  |=  (init i  [?]
Qi )  2!
(cs 0  [?]
[?]
cs 1 ).
i[?
]{0,1}  Let us also point out that we already encountered in Lemma 9.1 the implication (29) which serves as an assumption and is moreover exactly like the Second Assumption (47).
Now Lemma 9.1 concerns the ISEPI technique for Parallel combining (i.e., |= (A1 [?]
A2 ) [?]
A' ) of two suitable 2-to-1 formulas such as AbsSafe 0 and AbsSafe 1 which are intended for backward analysis.
Recall that Lemma 9.1 states that if the implication (3B [?]
3B ' ) [?]
inf is valid, then so is the implication below (which reproduces formula (30) in Lemma 9.1's statement): |=  f (fin w ) [?]
3B 2    [?]
' ' f (fin w ) [?]
3B 2    2!
(w  [?]
[?]
w ' ).
The Second Assumption (47) |= (3D0 [?]
3D1 ) [?]
inf for abstract processes Q0 and Q1 is indeed just a straightforward instance of the assumption |= (3B [?]
3B ' ) [?]
inf .
Therefore, a version of Lemma 9.1 can be specialised to deal with AbsSafe 0 , AbsSafe 1 , cs 0 and cs 1 : Corollary 11.1 Suppose the Second Assumption (47) in Table 6 holds.
Then it ensures that the abstract safety formulas imply mutual exclusion as expressed by the valid implication below which combines two instances of the 2-to-1 formula AbsSafe i in parallel: |=  AbsSafe 0  [?]
AbsSafe 1  [?]
2!
(cs 0  [?]
cs 1 ).
(50)  Corollary 11.1 helps in a simple proof of the next Lemma 11.2 concerning mutual exclusion for the abstract processes Q0 and Q1 : Lemma 11.2 The First Assumption (46) and Second Assumption (47) together ensure that two abstract processes can achieve mutual exclusion as formalised in the valid implication below: ^  |=  (init i  [?]
Qi )  [?]
2!
(cs 0  [?]
cs 1 ).
(51)  i[?
]{0,1}  Proof The Second Assumption (47) and Corollary 11.1 together ensure the valid  implication (50).
We then use simple propositional reasoning to combine this implication with the First Assumption (46) to obtain the desired valid implication (51), thus ensuring mutual exclusion for the abstract processes Q0 and Q1 .
Below is a chain of valid implications which capture the main reasoning: V  i[?
]{0,1} (init i [?]
Qi )  [?]
AbsSafe 0  [?]
AbsSafe 1  [?]
2!
(cs 0  [?]
cs 1 ).
[?]
[?]
Compositional reasoning using intervals and time reversal  47  11.3 Multiple accesses by a process to a shared resource Let us now consider a lemma showing that the two Abstract Assumptions (46) and (47) ensure validity of an implication which describes mutual exclusion for multiple accesses to the critical sections expressed with weak chop-star: Lemma 11.3 If Assumptions (46) and (47) in Table 6 hold, then the next formula concerning multiple requests for a shared resource is valid: ^ |= (init i [?]
Q* [?]
2!
(cs 0 [?]
cs 1 ).
(52) i ) i[?
]{0,1}  Proof Recall the First Assumption (46): |=  init i  Qi  [?]
[?]
AbsSafe i  [?]
fin init i .
This has the form |= (w [?]
A) [?]
(B [?]
fin w).
Our earlier work on compositional reasoning (e.g., [48-51]) discusses semantic inference rules for various combinations of such formulas.
Here is a version of one from [48] which is quite suitable for our purposes here: |=  (w [?]
A) [?]
(B  [?]
fin w)  =  |=  (w [?]
A* ) [?]
(B *  [?]
fin w).
For example, we can prove this for finite time by doing induction on interval length.
The rule yields from the First Assumption (46) a valid generalisation to multiple exclusive accesses by one process: |=  init i  [?]
Q* i  [?]
(AbsSafe i )*  [?]
fin init i .
(53)  The formula AbsSafe i , which has the form 2f ((fin w) [?]
3B ), is "almost" *-to-1 by Lemma 10.11 in Sect.
10.1.
This is formalised by a valid implication for the ISEPI technique of Iteration (see also the valid implication (40) and Table 5 in Sect.
10.1): |= !cs i [?]
(AbsSafe i )* [?]
AbsSafe i .
(54) As noted earlier, we assume that init i implies !cs i (see implication (45)), so we can replace !cs i in (54) by init i : |=  init i  [?]
(AbsSafe i )*  [?]
AbsSafe i .
(55)  Propositional reasoning then permits us to combine implications (53) and (55) into the following one: |=  init i  [?]
Q* i  [?]
AbsSafe i  [?]
fin init i .
(56)  The Second Assumption (47) together with Corollary 11.1 about the conjunction AbsSafe 0 [?]
AbsSafe 1 and some further propositional reasoning then yields our goal (52).
Below is a chain of valid implications to capture the main reasoning: V  i[?
]{0,1} (init i [?]
[?]
Qi ) AbsSafe 0  V * [?]
i[?
]{0,1} !cs i [?]
(AbsSafe i ) [?]
AbsSafe 1 [?]
2!
(cs 0 [?]
cs 1 ).
[?]
[?]
48  Ben Moszkowski  |=  init i  |=  Di'  |=  Di ; Di'  [?]
(2!cs i ); Ri ; (cs i  [?]
Qi  [?]
Di' ); 2!cs i  ' f 2D i  [?]
[?]
  [?]
fin init i  (58) (59) (60)  Di  where Ri is defined as follows: = b  Ri  2(more [?]
!cs i )  [?]
(inf  [?]
3Di ).
(61)  Table 7 The third abstract assumption  12 Correctness of an individual abstract process  Recall that the First Assumption (46) in Table 6 in Sect.
11.2 states that a single abstract process Qi ensures that the 2-to-1 formula AbsSafe i is true: |=  init i  [?]
Qi  [?]
AbsSafe i  [?]
fin init i .
The rather abstract First Assumption gives no details about how Qi achieves this.
We shortly define what we call the Third Abstract Assumption or more briefly the Third Assumption.
This contains some sufficient conditions concerning the overall structure of the sequential behaviour of Qi .
Later in this section's Lemma 12.1, these conditions are shown to indeed ensure the validity of the First Assumption for a single abstract process.
Subsequently in Sect.
13.3 we prove that an individual process in Peterson's algorithm fulfils the Third Assumption (see Theorem 13.4).
Therefore, Lemma 12.1 guarantees that it fulfils the First Assumption as well.
For the convenience of readers, we reproduce below our earlier informal outline of an abstract process previously given as (44) at the start of Sect.
11: (a) (b) (c) (d) (e)  Noncritical section; Request exclusive right to resource; Critical section with exclusive access; Release exclusive right to resource; Noncritical section.
(57)  Table 7 shows some formulas (58)-(61) which collectively make up the Third Abstract Assumption (also referred to as the Third Assumption) about the behaviour of abstract process Qi 's steps.
As we already noted, the Third Assumption is meant to precisely model the informal description in (57).
It is important to observe that we have fashioned the individual formulas in the Third Assumption in Table 7 so that they are readily suitable for use with the compositional ISEPI techniques already presented in Sects.
6-10 for backward analysis with 2-to-1 formulas.
Table 8 lists several ISEPI techniques for the 2-to-1 formula AbsSafe i which are all instances of the ones in the previous Table 5 in Sect.
10.1.
The various formulas found in the Third Assumption in Table 7 find application with most of the entries for ISEPI techniques in Table 8.
Let us now consider each of the Third Assumption's parts individually: - Third Assumption's implication (58): |=  init i  [?]
Qi  [?]
(2!cs i ); Ri ; (cs i  [?]
Di' ); 2!cs i    [?]
fin init i .
Compositional reasoning using intervals and time reversal  49  Introduction (first variant): Instance of formula (19) in Sect.
7: |=  [?]
2!cs i  (62)  AbsSafe i .
Introduction (second variant): Instance of formula (20) in Sect.
7: 2(more [?]
!cs i )  |=  [?]
(inf  [?]
3Di )  [?]
AbsSafe i .
(63)  Sequential combining: (See Theorem 6.1) |=  AbsSafe i ; AbsSafe i  [?]
AbsSafe i .
(64)  Extending rightward: Instance of formula (25) in Sect.
8.2: |=  AbsSafe i ; (cs i  [?]
Di' )  [?]
AbsSafe i ,  (65)  ' f formula and extends D rightward (i.e., |= (D ; D ) [?]
where Di' is a 1-to-2 i i i Di ).
Parallel combining: Instance of formula (30) in Sect.
9: |=  where  |=  (3D0  [?]
AbsSafe 0  [?]
AbsSafe 1  2!
(cs 0  [?]
[?]
cs 1 ),  (66)  3D1 ) [?]
inf .
Iteration (version for +-to-1 formula): Instance of formula (35) in Sect.
10: |=  (AbsSafe i )+  [?]
AbsSafe i .
Iteration (version for "almost" *-to-1 formula): Instance (54) in Sect.
13.3 of formula (40) in Sect.
10.1: |=  !cs i  [?]
(AbsSafe i )*  [?]
AbsSafe i .
Table 8 Summary of ISEPI techniques with the 2-to-1 formula AbsSafe i  This primarily ensures that process Qi achieves four sequential phases corresponding to lines (a), (b), (c) and the pair of lines (d)-(e) of the abstract process in (57).
A feature of the Third Assumption's first implication (58) is that it is abstract and compositional enough to not need a detailed labelling of individual program steps in Qi .
We instead sequentially compose them using the chop operator in PITL.
Implication (58) asserts that when Qi operates with the state formula init i initially true, then the steps are sequentially performed and also init i is true in the last state if there is one.
Here are the lines in the abstract process in (57) and the corresponding individual steps: (a) 2!cs i  (b)  (c)  Ri  cs i [?]
Di'  (d)-(e) 2!cs i .
The first and fourth of the abstract steps are both the formula 2!cs i .
This concerns a period of time when Qi is not in its critical section and does not even try to enter it.
The second formula Ri is for when Qi has succeeded or  50  Ben Moszkowski  failed to enter the critical section in line (b) in the abstract process in (57).
The formula Ri 's definition (61) is discussed shortly.
- Third Assumption's implications (59) and (60): |=  Di'  [?]
' f 2D i  Di ; Di'  |=  [?]
Di .
These together restrict the formula Di' to being a 1-to-2f formula (Definition 8.2 in Sect.
8.1) and also ensure that it extends Di rightward.
We require the two assumptions so that we can invoke Theorem 8.7 (found in Sect.
8.2) for the ISEPI technique of Extending rightward a 2-to-1 formula for backward analysis using a 1-to-2f formula.
Table 8 includes an instance (65) of the ISEPI technique for Extending rightward the 2-to-1 formula AbsSafe i .
This is obtained using Theorem 8.7.
We illustrate Di and Di' with a simple contrived example.
If Di is the formula pi [?]
3p'i [?]
stable pi , then Di' could be the formula stable pi , which is in fact both f and 2-to-1 (as discussed in Sect.
8.1 after Theorem 8.5).
The chain of 1-to-2 valid implications below shows the ISEPI technique of Extending rightward the formula pi [?]
3p'i [?]
stable pi (which serves as Di ) using the formula stable pi : (pi  [?]
[?]
3p'i [?]
stable pi ); stable pi pi [?]
3p'i [?]
stable pi .
[?]
pi  [?]
3p'i  [?]
(stable pi ; stable pi )  These sample Di and Di' are only for illustrative purposes since they are unlikely to properly ensure mutual exclusion.
- Third Assumption's definition of Ri in (61): Ri  = b  2(more [?]
!cs i )  [?]
(inf  [?]
3Di ).
This concerns the step in line (b) of (57) when the process awaits entry into its critical section.
The definition captures the idea that the process waits infinitely long in vain to enter the critical section or succeeds with the formula Di true in some suffix of the interval associated with Ri .
We can immediately use this formula as an instance of the earlier valid implication (20) in Sect.
7 for the ISEPI technique of Introduction of AbsSafe i .
Implication (63) in Table 8 corresponds to this.
The next Lemma 12.1 formally states that the conditions in the Third Assumption suffice to imply the First Assumption: Lemma 12.1 For any formulas init i , Qi , Di , Di' , if all three implications (58)-(60) in the Third Assumption are valid, then so is the First Assumption (46).
Proof We first prove the validity of the next formula (67) and make use of the valid implications (62)-(65) in Table 8 concerning ISEPI techniques for AbsSafe i : |=  1 2 3 4 5 6 7 8 9  |=  (2!cs i ); Ri ; (cs i  [?]
Di' ); 2!cs i  [?]
AbsSafe i .
2!cs i [?]
AbsSafe i 2(more [?]
!cs i ) [?]
(inf [?]
3Di ) [?]
AbsSafe i |= Ri [?]
AbsSafe i |= AbsSafe ; (cs i [?]
D' ) [?]
AbsSafe i i i |= Ri ; (cs i [?]
D' ) [?]
AbsSafe i i |= (2!cs i ); Ri ; (cs i [?]
D' ); 2!cs i [?]
AbsSafe i ; AbsSafe i ; AbsSafe i i |= AbsSafe ; AbsSafe [?]
AbsSafe i i i |= AbsSafe ; AbsSafe ; AbsSafe [?]
AbsSafe i i i i |= (2!cs i ); Ri ; (cs i [?]
D' ); 2!cs i [?]
AbsSafe i i |=  (67) (62) [ISEPI] (63) [ISEPI] 2, Def.
of R (65) [ISEPI] 3, 4, PITL 1, 5, PITL (64) [ISEPI] 7, PITL 6, 8, Prop.
Compositional reasoning using intervals and time reversal  51  This can be summarised as a chain of valid implications clearly showing our application to AbsSafe i of the ISEPI techniques discussed in Sects.
6-8 for Introducing, Sequential combining and Extending rightward such 2-to-1 formulas for backward analysis.
We underline the parts of formulas which get reduced to AbsSafe i : Di' ); 2!cs i [?]
AbsSafe i ; Ri ; (cs i [?]
Di' ); AbsSafe i AbsSafe i ; AbsSafe i ; (cs i [?]
Di' ); AbsSafe i AbsSafe i ; AbsSafe i ; AbsSafe i [?]
AbsSafe i .
(2!cs i ); Ri ; (cs i [?]
[?]
[?]
It then follows from implication (58) in the Third Assumption together with implication (67) that the Third Assumption indeed suffices to ensure the First Assumption (46)'s validity.
[?]
[?]
Suppose we instead let the abstract process Qi itself be defined to be the following: (2!cs i ); Ri ; (cs i [?]
Di' ); 2!cs i .
Then the Third Assumption's first formula (58) can be simplified as shown below: |=  init i  [?]
Qi  [?]
fin init i .
13 Analysis of Peterson's algorithm  Before showing mutual exclusion for Peterson's algorithm (given earlier in Fig.
1 in Sect.
11), we capture the processes' behaviour in PITL.
Varying concurrency assumptions can be made.
We discuss one possible way which illustrates some compositional techniques and time symmetry, and furthermore serves as an initial proof-of-concept.
This follows the practice of Pnueli [66], Barringer, Kuiper and Pnueli [5] and many other researchers over the years who have used Peterson's algorithm as a sort of canonical benchmark for studying mutual exclusion.
Peterson's algorithm also serves this purpose in the recent textbooks by Aceto et al.
[1], Taubenfeld [76], Herlihy and Shavit [29] and Kroger and Merz [37].
As we already noted at the beginning of Sect.
11, Taubenfeld's discussion about mutual exclusion using Peterson's algorithm [76] has a special significance for our approach because it helped inspire us to see the potential of time symmetry.
13.1 Expressing Peterson's algorithm in PITL One of the main issues with modelling Peterson's algorithm in temporal logic involves the semantics of assignment statements.
In imperative programming languages, when one variable is assigned, the values of others normally do not change.
On the other hand, in temporal logic, a formula which only mentions the dynamic behaviour of some variables gives absolutely no indication about what happens with other variables  not occurring in the formula.
For example, the PTL formula skip [?]
( p) [?]
!p can be regarded as setting the next value of the propositional variable p to the negation of its current value.
This tells us nothing about the behaviour of q and other propositional variables.
Such a phenomenon is an instance of the frame problem given prominence by McCarthy and Hayes [43] (see also  52  Ben Moszkowski  Shanahan [72]).
If we want variables to remain unchanged during an interval, some explicit formula or semantic mechanism for this must be in place.
The simplest solution is to add a formula such as stable q (defined in Table 1 in Sect.
2) for each relevant variable.
Hale [24] initiated the study of framing variables in ITL.
Duan has also investigated this issue [13-17].
f can The presentation here shows one way formulas which are 2-to-1 and 1-to-2 be used to handle framing issues.
Our illustrative formulation in temporal logic of each process Pi in Peterson's algorithm needs to ensure that the variable flag i is always being framed or assigned using :=.
We handle framing for flag i by modelling process Pi as having exclusive write access to this variable.
Therefore, the definitions of statements for the process Pi which do not change flag i (i.e., all statements except the ones of the form flag i := j ) can simply include the formula stable flag i .
On the other hand, in Peterson's algorithm the two processes must have shared write access to the variable turn .
This significantly complicates framing turn .
A process cannot simply frame turn by asserting stable turn because this would prevent the other process from changing the variable's value.
However, observe that process P0 only uses := to assign the variable turn the value 1, and similarly P1 only uses := to assign turn the value 0.
So if for example turn = 0, then only process P0 is interested in possibly changing it to 1.
We can therefore adopt the convention that if a process Pi wants to frame turn , the process only needs to do so between the pairs of adjacent states with turn = i in the first one.
When turn = 1 - i, the other process has the responsibility for framing.
The temporal formula frameturn i defined below formalises this approach in our modelling of process Pi : frameturn i  = b  2 (more  [?]
 turn = i) [?]
turn = i .
(68)  Process Pi just has to include frameturn i instead of stable turn in the statements which do not change turn (i.e., all of the statements except turn := 1 - i).
This is an acceptable solution to framing in Peterson's algorithm.
For example, no logical inconsistency occurs if say process P0 is assigning turn the value 1 while at the same time process P1 is partially framing turn to prevent it from changing whenever it already equals 1.
Also, if both processes simultaneously frame turn , then the combination of activities is logically equivalent to stable turn as expressed by the following valid PTL formula: |=  stable turn  [?]
frameturn 0  [?]
frameturn 1 .
(69)  We now define another PITL formula nochange i .
It is used as a part of statements in Peterson's algorithm which frame both flag i and turn .
The formula nochange i describes any finite or infinite period when P0 changes neither the variable flag i nor the variable turn : nochange i  = b  stable flag i  [?]
frameturn i .
(70)  We later need in Sect.
13.3 (in Lemma 13.5) the next Lemma 13.1 concerning nochange 0 : f Lemma 13.1 The formulas frameturn i and nochange i are 2-to-1 and 1-to-2.
Compositional reasoning using intervals and time reversal  noop 0 flag 0 := c  53  = b = b  nochange 0 [?]
finite (stable flag 0 [?]
skip ) [?]
fin (flag 0 = c) [?]
frameturn 0 turn := 1 = b (frameturn 0 [?]
skip ) [?]
fin (turn = 1) [?]
stable flag 0 await (flag 1 = 0 [?]
turn = 0) = b  finite [?]
3(flag 1 = 0 [?]
turn = 0) [?]
nochange 0 .
Table 9 Semantics of individual statements in process P0 in Peterson's algorithm  Proof We consider each of the formulas individually: - frameturn i : Observe that frameturn i (defined in (68)) has the form 2T , where T is in NL1 .
Therefore, Lemma 4.6 ensures that frameturn i is 2-to-1.
Furthermore, frameturn i can be re-expressed to have the form 2(more [?]
T ' ), where T '  is also in NL1 : |=  frameturn i  [?]
 2 more [?]
(turn = i [?]
turn = i) .
Recall from Sect.
8.1 that all such 2-formulas (e.g., stable flag i ) are 1-to-2f (see the earlier Lemma 8.3 and Theorem 8.5).
- nochange i : This is defined in (70) to be the conjunction of stable flag i and f .
In addition, the 1-to-2 f forframeturn i .
Each of these is both 2-to-1 and 1-to-2 mulas, like the 2-to-1 formulas, are closed under conjunction (see Lemmas 4.4 f .
and 8.4).
Hence, nochange i is 2-to-1 and 1-to-2 [?]
[?]
An alternative and perhaps more general and intuitive approach to framing turn in each Pi can employ interleaving controlled by an additional auxiliary variable.
This variable determines which process has write access to turn .
Therefore, a process has write access and is responsible for framing exactly at such times.
That process can then either choose to assign a value to turn or frame it.
We would like in future work to look at such an approach and formally compare it with the one used here.
13.2 Semantics of one process in Peterson's algorithm Table 9 shows the semantics of the individual statements of the concrete process P0 in Peterson's algorithm.
Note that flag 0 := c is also definable as (flag 0 <~ c) [?]
frameturn 0 [?]
finite using the padded temporal assignment operator <~ (defined in Table 1 in Sect.
2).
We define await to terminate iff the wait condition is eventually true.
Termination might not be immediate.
Process P1 has analogous definitions.
Remark 13.2 Some readers will wonder why the definition of flag 0 := c requires a skip subformula.
This is needed so that the variable flag 0 remains stable except perhaps in the very last state when it might change.
If we omit the skip , then the variable flag 0 will always be stable and unable to change value.
This is because the definition of stable flag 0 in Table 1 in Sect.
2 specifies that the variable's value remains unchanged between all adjacent pairs of states.
Hence, for any propositional variable p, the formula stable p is semantically equivalent to the conjunction  54  Ben Moszkowski  (2!p) [?]
(2p).
For example, the following formula concerning a change from 0 to 1 is unsatisfiable in finite intervals: !p  stable p  [?]
[?]
fin p.  One can however dispense with the skip in flag 0 := c by replacing stable flag 0 [?]
skip with the formula finite [?]
padded flag 0 containing the derived PITL construct padded .
The formula padded A is defined to keep the formula A stable except for perhaps in the last state: = b  padded A  2(more [?]
!A)  [?]
2(more [?]
A).
However, the formulas stable A and padded A are semantically equivalent in infinite intervals.
Our analysis uses a version of Pi called Pi' (previewed in Sect.
11.2.1) with the auxiliary boolean variable cs i to track Pi 's critical section: Pi'  = b  Pi 's Pi 's Pi 's Pi 's    lines 1-3 [?]
stable csi ; line 4 [?]
cs i <~ true ; line 5 [?]
cs i <~ false ; lines 6-7 [?]
stable cs i .
(71)  We ultimately prove the validity of the next implication which formalises mutual exclusion for Peterson's algorithm: |=  ^  (pinit i  [?]
Pi' )  [?]
2!
(cs 0  [?]
cs 1 ).
(72)  i[?
]{0,1}  This is a concrete instance of the earlier formula (51) for abstract mutual exclusion.
The state formula pinit i for initialisation denotes the conjunction flag i = 0 [?]
!cs i and is later formally defined as formula (76), but it was already previewed in Sect.
11.1.
13.3 Mutual exclusion for Peterson's algorithm based on the abstract one We now consider how the properties we showed for an abstract model of a single process can be employed to reason about Peterson's algorithm.
This will save us from having to do a detailed analysis specifically for Peterson's algorithm.
Such an analysis would require us to first use ISEPI techniques to prove that various parts of process Pi' with suitable pre-conditions each imply some 2-to-1 formula or a related one and then to combine them to get a 2-to-1 formula for Pi' .
Instead, we only need to show something weaker about Peterson's algorithm.
This then ensures that a concrete instance of the 2-to-1 formula AbsSafe i for backward analysis holds as well.
Furthermore, various other properties of the abstract algorithm automatically apply to Peterson's algorithm (e.g., formula (52) in Lemma 11.3 for multiple accesses).
Our main goal here is the validity of concrete instances for Peterson's algorithm of the Second Assumption (47) and the Third Assumption (implications (58)-(60) in Table 7 in Sect.
12).
As we previously noted, the Third Assumption is meant to precisely model the informal description in (57) of an abstract process.
Compositional reasoning using intervals and time reversal  55  Let PeteSafe i denote a concrete instance of AbsSafe i .
We already discussed PeteSafe i in a preliminary manner in Sects.
11.2.1 and 11.2.2 in order to motivate our use of the abstract processes Qi , the associated 2-to-1 formula AbsSafe i and the two associated Abstract Assumptions (46) and (47) in Table 6.
The concrete instance of Di used in PeteSafe i is the following conjunction, which we denote as Ei : Ei  = b  flag i = 1  [?]
turn = 1 - i  [?]
3(flag 1-i = 0  [?]
turn = i)  [?]
nochange i .
(73) Recall that the formula nochange i previously defined in (70) specifies that Pi does not alter either of the variables flag i and turn .
The formula Ei is based on the values of flag i and turn after process Pi 's lines 2-3 together with the behaviour of lines 4-5.
This was already overviewed in Sect.
11.2.1 but is now summarised again.
The relevant phase of process operation concerns requesting entry into the critical section, at which time flag i = 1 and turn = 1 - i, and then either succeeding with flag 1-i = 0 or turn = i or alternatively forever waiting in vain.
The formula Ei contains the subformula 3(flag 1-i = 0 [?]
turn = i) and so deals with the case when the request is successful.
Lemma 13.3 The formula Ei is 2-to-1.
Proof This follows from Ei being the conjunction of formulas which are themselves 2-to-1 (using Lemmas 4.3 and 13.1 and then Lemma 4.4).
[?]
[?]
Lemma 13.3 is invoked later on in Lemma 13.6's proof.
We can in principle use noop i (defined in Table 9) instead of nochange i in Ei 's definition.
However, the analysis with nochange i is slightly simpler because it omits the subformula finite in noop i .
The use of the formula Ei as a concrete instance of Di results in the concrete instance of AbsSafe i which we denote as PeteSafe i : PeteSafe i  = b  f ((fin cs ) [?]
3E ).
2 i i  (74)  Another possibility for Ei in PeteSafe i is the weak chop of Pi 's lines 2-5 in Sect.
11's Fig.
1.
We can denote this portion of Pi as Pi,2-5 .
The formula PeteSafe i is 2-to-1 because it is a concrete instance of AbsSafe i , and we therefore have the next instance of the valid implication (64) in Table 8 in Sect.
12: |= PeteSafe i ; PeteSafe i [?]
PeteSafe i (75) In addition to formulas PeteSafe i and Ei , we also define pinit i , ptest i and ptest 'i to each be a state formula as described below: pinit i ptest i ptest 'i  = b  flag i = 0  [?]
!cs i  flag i = 1  [?]
turn = 1 - i  (See line 3 of Pi in Fig.
1)  (77)  = b  flag 1-i = 0  turn = i  (See line 4 of Pi in Fig.
1).
(78)  = b  [?]
(76)  Therefore, the following equivalence relating the formula Ei (defined in (73)) with ptest i , ptest 'i , and nochange i is valid: |=  Ei  [?]
ptest i  [?]
3ptest 'i  [?]
nochange i .
(79)  56  Ben Moszkowski  ' Part P0,- ' P0,1-3 ' P0,4 ' P0,5 ' P0,6-7  Pre-condition pre 0,-  Post-condition post 0,-  flag 0 = 0 [?]
!cs 0 flag 0 = 1 [?]
turn = 1 [?]
!cs 0 flag 0 = 1 [?]
cs 0 flag 0 = 1 [?]
!cs 0  flag 0 = 1 [?]
turn = 1 [?]
!cs 0 flag 0 = 1 [?]
cs 0 flag 0 = 1 [?]
!cs 0 flag 0 = 0 [?]
!cs 0 .
Table 10 Pre- and post-conditions for parts of P0'  We need concrete instances of the formulas init i (found in the First Assumption (46) in Table 6 and Third Assumption in Table 7) and Di' (found in the Third Assumption in Table 7) which are suitable for Peterson's algorithm.
Let us take init i to be pinit i and Di' to be nochange i .
Here is a summary of the various abstract formulas and corresponding concrete instances: Abstract formula: Concrete formula:  init i pinit i  Di Ei  Di' nochange i  AbsSafe i PeteSafe i .
Alternatively, Di can be Pi,2-5 (the weak chop of Pi 's lines 2-5) or even Pi,2-4 and Di' can be noop i .
In addition, we use a concrete version Si of the formula Ri defined in the Third Assumption in Table 7.
The two formulas are given below to facilitate comparison: Abstract formula Ri : 2(more [?]
!cs i )  [?]
(inf  [?]
3Di )  Concrete formula Si : 2(more [?]
!cs i )  [?]
(inf  [?]
3Ei )  We now turn to showing that Peterson's algorithm indeed obeys the requirements imposed on the abstract algorithm to guarantee mutual exclusion.
Our presentation first considers the Third Assumption in Table 7, which by Lemma 12.1 implies the First Assumption (46), and then deals with the Second Assumption (47).
These suffice to show that all of the mutual exclusion properties we established for the abstract algorithm also apply to Peterson's concrete one.
Owing to symmetry, our analysis only needs to consider process P0 in Peterson's algorithm.
For clarity, we typeset in boldface references to the version P0' , which was defined in (71) in Sect.
13.2 and includes the behaviour of cs 0 .
Let us first re-express the formula P0' as the semantically equivalent formula ' ' ' ' P0,1-3 ; P0,4 ; P0,5 ; P0,6-7 and look at the behaviour of the four primary sequen' ' ' ' tial parts P0,1-3 , P0,4 , P0,5 and P0,6-7 .
Our analysis shows that each of these combined with a suitable pre-condition implies a corresponding part in the formula given below: (2!cs i ); Si ; (cs i [?]
nochange i ); 2!cs i .
This is a concrete instance of a subformula of the Third Assumption's first formula (58).
Every primary sequential part with its pre-condition furthermore also implies the associated post-condition in the final state when there is one.
State formulas for the pre- and post-conditions for the parts of P0' are fairly straightforward.
Table 10 shows one possible approach.
For example, pre 0,4 refers ' to the pre-condition flag 0 = 1 [?]
turn = 1 [?]
!cs 0 for part P0,4 .
Observe that normally the post-condition for each part is actually the pre-condition of the next  Compositional reasoning using intervals and time reversal  57  ' one.
In the case of P0,6-7 , which is the last part, the post-condition can be taken ' to be the pre-condition pre 0,1-3 for P0,1-3 .
We therefore only need to refer to the formulas for pre-conditions and do not actually need separate names for the post-conditions.
Note that pre 0,1-3 is identical to pinit 0 (i.e., both denote flag 0 = 0 [?]
!cs 0 ; see (76)).
Theorem 13.4 The concrete instances of all of the Third Assumption's three implications (58)-(60) for our version of Peterson's algorithm are valid.
The proof is deferred until after we first state and prove Lemmas 13.5-13.7 for the concrete instances of the Third Assumption's three implications (58)-(60) for Peterson's algorithm.
Since the proof of Lemma 13.7 for the first one (58) is the most complicated, we save it for last.
Lemma 13.5 (Validity of instance of Third Assumption's implication (59)) ' f The next concrete instance of the abstract algorithm's implication Di' [?]
2D i is valid: |=  f nochange i [?]
2nochange i.
Proof This follows immediately from the earlier Lemma 13.1 in Sect.
13.1, thus ensuring that nochange i is 1-to-2f .
[?]
[?]
Lemma 13.6 (Validity of instance of Third Assumption's implication (60)) The next concrete instance of the abstract algorithm's implication (Di ; Di' ) [?]
Di is valid: |=  Ei ; nochange i  [?]
(80)  Ei .
Proof Recall from Lemma 13.3 that Ei is 2-to-1.
The proof of the validity of (80) involves a routine use of the ISEPI technique of Extending rightward such a 2to-1 formula.
We employ the equivalence (79) to express Ei as the conjunction ptest i [?]
3ptest 'i [?]
nochange i .
Here is chain of valid implications which make use of the fact that nochange i is 2-to-1 as well (Lemma 13.1): Ei ; nochange i [?]
ptest i [?]
3ptest 'i [?]
nochange i ); nochange i [?]
ptest i [?]
3ptest 'i [?]
(nochange i ; nochange i ) [?]
ptest i [?]
3ptest 'i [?]
nochange i [?]
Ei .
Hence, Ei can indeed be Extended rightward with nochange i , and so implication (80) is in fact valid.
[?]
[?]
Lemma 13.7 (Validity of instance of Third Assumption's implication (58)) For each process Pi' , the following concrete instance of the Third Assumption's implication (58) is valid: |=  pinit i  [?]
Pi'  [?]
(2!cs i ); Si ; (cs i  [?]
nochange i ); 2!cs i    [?]
fin pinit i .
(81)  58  Ben Moszkowski  Proof We will only deal with P0' , but the proof easily generalises to P1' .
State  formulas used for the lines' pre- and post-conditions are found in the previously ' presented Table 10.
Each of the four main parts of P0' in (71), that is P0,1-3 , ' ' ' P0,4 , P0,5 and P0,6-7 , contributes one of the chop operands in implication (81)'s subformula (2!cs 0 ); S0 ; nochange i ; 2!cs 0 .
Here are the associated implications, which are shortly proven to be valid: ' P0,1-3  |=  pre 0,1-3  [?]
|=  pre 0,4  [?]
' P0,4  [?]
S0  [?]
fin pre 0,5  |=  pre 0,5  [?]
' P0,5  [?]
cs 0  [?]
nochange i  |=  pre 0,6-7  [?]
[?]
' P0,6-7  [?]
2!cs 0  2!cs 0  [?]
[?]
(82)  fin pre 0,4  (83) [?]
(84)  fin pre 0,6-7  (85)  fin pre 0,1-3 .
We structure the rest of Lemma 13.7's proof as four steps.
Let us first look at a summary of them: ' ' - Step 1, case for P0,1-3 and P0,6-7 : We show the validity of the associated  implications (82) and (85). '
- Step 2, case for P0,4 : We show the validity of the associated implication (83). '
- Step 3, case for P0,5 : We show the validity of the associated implication (84). '
- Step 4, case for P0 : We show the validity of the associated implication (81). '
' Step 1, case for P0,1-3 and P0,6-7 to show the validity of implications (82) and (85): Validity readily follows from the associated pre-conditions which set cs 0 to equal false together with the temporal formulas in the following ' ' parts of our definition (71) of P0' corresponding to P0,1-3 and P0,6-7 : ' P0,1-3 : Pi 's lines 1-3  [?]
stable cs i    ' P0,6-7 : Pi 's lines 6-7  [?]
 stable cs i .
Below are versions of the two implications (82) and (85) with the various formulas replaced by their definitions for easier checking: |=  flag 0 = 0 [?]
!cs 0 [?]
(noop 0 ; flag 0 := 1; turn := 1) [?]
stable cs i [?]
2!cs 0 [?]
fin (flag 0 = 1 [?]
turn = 1 [?]
!cs 0 )  |=  flag 0 = 1 [?]
!cs 0 [?]
(flag 0 := 0; noop 0 ) [?]
stable cs i [?]
2!cs 0 [?]
fin (flag 0 = 0 [?]
!cs 0 ).  '
Step 2, case for P0,4 to show the validity of implication (83): Recall that the Third Assumption's definition of Ri in Table 7 is the conjunction 2(more [?]
!cs i ) [?]
(inf [?]
3Di ), so Si is the concrete instance 2(more [?]
!cs i ) [?]
(inf [?]
3Ei ).
Here is an expanded version of implication (83): |=  flag 0 = 1 [?]
turn = 1 [?]
!cs 0 [?]
await (flag 1 = 0 [?]
turn = 0) [?]
cs 0 <~ true [?]
2(more [?]
!cs 0 ) [?]
(inf [?]
3E0 ) [?]
fin (flag 0 = 1 [?]
cs 0 ).
(86) From !cs 0 and cs 0 <~ true readily follows 2(more [?]
!cs 0 ).
The main remaining portion of the proof of implication (86)'s validity involves showing that (86)'s antecedent implies the consequent's subformula inf [?]
3E0 .
Recall from (73) that Ei has the following definition: Ei  = b  flag i = 1  [?]
turn = 1 - i  [?]
3(flag 1-i = 0  [?]
turn = i)  [?]
nochange i .
Compositional reasoning using intervals and time reversal  59  ' Below is a proof first showing that the pre-condition pre 0,4 and P0,4 together imply inf [?]
E0 , from which readily follows that they imply inf [?]
3E0 .
For conciseness in the proof, we use ptest 0 and ptest '0 to denote the state formulas flag 0 = 1 [?]
turn = 1 and flag 1 = 0 [?]
turn = 0, as previously defined in (77) and (78), respectively.
1  |=  2 3  |=  4  |=  5  |=  6 7 8  |=  |=  |= |=  await (ptest '0 ) [?]
(finite [?]
3ptest '0 ) [?]
nochange 0 (finite [?]
3ptest '0 ) [?]
inf [?]
3ptest '0 ptest 0 [?]
await (ptest '0 ) [?]
ptest 0 [?]
(inf [?]
3ptest '0 ) [?]
nochange 0 ptest 0 [?]
(inf [?]
3ptest '0 ) [?]
nochange 0  [?]
inf [?]
ptest 0 [?]
3ptest '0 [?]
nochange 0 ptest 0 [?]
await (ptest '0 ) [?]
inf [?]
(ptest 0 [?]
3ptest '0 [?]
nochange 0 ) ptest 0 [?]
await (ptest '0 ) [?]
inf [?]
E0 inf [?]
E0 [?]
inf [?]
3E0 ptest 0 [?]
await (ptest '0 ) [?]
inf [?]
3E0  Def.
of await PTL 1, 2, Prop.
Prop.
3, 4, Prop.
5, Def.
of E0 PTL 6, 7, Prop.  '
Step 3, case for P0,5 to show the validity of implication (84): This is fairly straightforward from the definitions of noop 0 and cs 0 <~ false .
Here is a version of ' implication (84) with pre 0,5 , P0,5 and pre 0,6-7 replaced by their definitions: |=  flag 0 = 1 [?]
cs 0 [?]
noop 0 [?]
cs 0 <~ false [?]
cs 0 [?]
nochange 0 [?]
fin (flag 0 = 1  [?]
!cs 0 ).
Step 4, case for P0' to show the validity of implication (81): The formulas pinit 0 and pre 0,1-3 are identical (i.e., both denote flag 0 = 0 [?]
!cs 0 ; see (76)).
Consequently, the initial process state ensures that pre 0,1-3 is true.
The four valid ' ' ' ' implications (82)-(85) for P0,1-3 , P0,4 , P0,5 and P0,6-7 can then be sequentially combined to obtain the validity of the implication (81) for P0' .
[?]
[?]
Recall that Theorem 13.4 states that concrete instances of the Third Assumption's three implications (58)-(60) (shown in Table 7 in Sect.
12) for our version of Peterson's algorithm are valid.
The proof of Theorem 13.4 now readily follows: Proof (Theorem 13.4) The previous Lemmas 13.5-13.7 together establish that the concrete instances of all three implications (58)-(60) are indeed valid.
[?]
[?]
We now use Theorem 13.4 to show that Peterson's algorithm has suitable instances of the First Assumption: Lemma 13.8 The next concrete instance of the First Assumption (46) for each process Pi' in our version of Peterson's algorithm is valid: |=  pinit i  [?]
Pi'  [?]
PeteSafe i  [?]
fin pinit i .
(87)  Proof Lemma 12.1 yields from the Third Assumption the First Assumption (46).
In addition, Theorem 13.4 demonstrates that each process Pi' in Peterson's algo-  rithm fulfils an associated concrete instance of the Third Assumption.
The combination of these then guarantees that each Pi' also fulfils the associated concrete instance (87) of the First Assumption.
[?]
[?]
60  Ben Moszkowski  We also need the next Lemma 13.9 concerning the Second Assumption (47): Lemma 13.9 The following concrete instance of the Second Assumption (47) for Peterson's algorithm is valid: |=  3E0  [?]
3E1  [?]
(88)  inf .
Recall that the Second Assumption (47) was needed in Sect.
11.2.3 to show mutual exclusion exclusion for the abstract algorithm.
Consequently, implication (88), as an instance of the Second Assumption, encapsulates in a concise way a key aspect of Peterson's algorithm, and it focuses on a central mechanism used to ensure mutual exclusion.
Therefore, the proof of the validity of implication (88) has a special significance in the understanding of how Peterson's algorithm works.
Proof (Lemma 13.9) Our proof of the validity of implication (88) actually shows the stronger result that 3E0 [?]
3E1 is unsatisfiable.
Simple temporal reasoning  allows us to establish this by a case analysis which demonstrates that each of the following two formulas is unsatisfiable: E0  [?]
3E1  E1  (89)  3E0 .
[?]
This is because if 3E0 [?]
3E1 were to be satisfiable, then there would be some suffix subinterval satisfying one of the two formulas E0 [?]
3E1 or E1 [?]
3E0 in (89).
Owing to the symmetry involved, we only need to consider the first of these here.
Let us use the valid equivalence (79) to re-express E0 [?]
3E1 in terms of ptest i , ptest 'i , and nochange i : ptest 0  [?]
3ptest '0  [?]
nochange 0  [?]
3 ptest 1  [?]
3ptest '1  [?]
 nochange 1 .
(90)  Our analysis can ignore the conjunct 3ptest '0 .
When we replace the remaining state formulas ptest 0 , ptest 1 and ptest 'i by their respective definitions given in (77) and (78), the slightly shortened version of formula (90) without 3ptest '0 becomes the following: flag 0 = 1 [?]
turn = 1 [?]
nochange 0 [?]
3 flag 1 = 1 [?]
turn = 0 [?]
3(flag 0 = 0  [?]
 turn = 1) [?]
nochange 1 .
(91)  The variable flag 0 always equals 1 because of the effect of stable flag 0 in the definition of nochange 0 (see (70)).
Therefore, the subformula flag 0 = 0 [?]
turn = 1 in (91) can be reduced to turn = 1, which we underline below: flag 0 = 1 [?]
turn = 1 [?]
nochange 0  [?]
3 flag 1 = 1 [?]
turn = 0 [?]
3(turn = 1) [?]
nochange 1 .
(92)  Observe that the formulas nochange 0 and nochange 1 , as defined in (70), are both conjunctions of 2-formulas.
It follows that if nochange 0 and nochange 1 are true for an interval, then they are also true for all suffix subintervals (i.e., they are 1-to-2 formulas): |=  nochange i  [?]
2nochange i .
Now consider the suffix subinterval starting with the state where the subformula flag 1 = 1 [?]
turn = 0 in (92) is true.
In that subinterval, the formulas nochange 0 and nochange 1 are both true, so the two formulas frameturn 0 and frameturn 1 in  Compositional reasoning using intervals and time reversal  61  their definitions are as well.
Therefore, the variable turn must remain stable from then on (as we previously noted with valid equivalence (69) in Sect.
13.1).
This behaviour concerning nochange 0 , nochange 1 and turn is expressed by the following valid PTL formula: |=  nochange 0  [?]
nochange 1  [?]
stable turn.
However, the eventual stability of turn in formula (92) is contradicted by the conjunction turn = 0 [?]
3(turn = 1) found within the outer 3-formula.
Consequently, the formula (92) itself is in fact unsatisfiable.
It then follows from this that formula (91) is likewise unsatisfiable and so is the previous formula E0 [?]
3E1 in (89).
Symmetry ensures that the formula E1 [?]
3E0 is unsatisfiable as well.
This all demonstrates that the conjunction 3E0 [?]
3E1 is unsatisfiable and so implies anything, including the formula inf .
Consequently, implication (88) is indeed valid.
[?]
[?]
At this stage, we have obtained for Peterson's algorithm concrete instances of the Third Assumption (Theorem 13.4) , then the First Assumption (Lemma 13.8), and finally the Second Assumption (Lemma 13.9).
Consequently, from the concrete instances (87) and (88) of the First Assumption (46) and Second Assumption (47), respectively, all of the mutual exclusion properties discussed for abstract processes in Sect.
11 can be carried over to Peterson's algorithm.
14 Use of time symmetry to reduce some PITL formulas to PTL  We now briefly discuss at an exploratory level how our new techniques of time symmetry and reflections introduced in Sect.
5 can provide a theoretical basis for transforming some PITL formulas to the computationally more tractable formalism PTL.
The main contribution of this section is to show how to combine some ideas from our earlier work in [55] with the new concept of reflection and reasoning about prefix subintervals.
In particular, we will first reduce a PITL safety property concerning backward analysis to a PTL formula involving suffix subintervals together with the temporal operators 2 and until .
This kind of reduction can then also be done on a formula about a system and an associated safety property.
One potential benefit is that we can extend the application of some existing decision procedures and tool-support for PTL to handle suitable PITL formulas as well.
Some computational aspects of PTL are surveyed by Kroger and Merz [37] and Fisher [19], who also provide further references to the significant literature on the subject.
In [55] we discuss an implemented decision procedure for PTL with both finite and infinite time which has connections with the theory of PITL.
We only mention this because some of the techniques presented in [55] are later on adapted when we reduce PITL formulas to PTL.
A key observation here is that a reflection of a formula with 2f -subformulas contains 2-subformulas.
The later can be easier to reduce to PTL because 2 is itself a PTL construct, whereas 2f is not.
Recall that PTLu is the version of PTL with strong until defined earlier in Sect.
3.3.
Let us use the formula PeteSafe 0 defined in (74) to illustrate obtaining from a 2f -formula a reflection in PTLu .
In order to derive a PTLu formula which reflects PeteSafe 0 , our reduction to PTLu  62  Ben Moszkowski  first obtains the next interval-oriented way to re-express PeteSafe 0 :  f (fin cs ) [?]
3 (empty [?]
ptest ); nochange ; 2 0 0 0   ' (empty [?]
ptest 0 ); nochange 0 .
(93)  Here the PTL formula E0 (defined in (73)) has been replaced by a PITL formula with weak chops which is semantically equivalent to E0 in finite intervals.
We now reflect (93):  r ' f nochange ; (empty [?]
ptest ); 2 cs 0 [?]
3 0 0  (94) nochange r0 ; (empty [?]
ptest 0 ) We then re-express the 3f -subformula in PTLu : Y until ptest '0  [?]
  (Y until ptest 0 ) ,  (95)  where Y acts like nochange r0 on pairs of adjacent states: ( flag 0 ) = flag 0  [?]
  ( turn = 0) [?]
turn = 0 .
Finally, we can take the PTLu reflection of PeteSafe 0 to be (94) with the 3f subformula replaced by (95).
Note the reduction to PTLu of the reflection of a f -formula containing chop-star might require auxiliary variables.
This is because 2 PITL with chop-star (which can express regular and omega-regular languages [77]) is much more expressive than PTLu .
However, this is not always an issue as our example demonstrates.
See Kroger and Merz [37] for a discussion of the operator until and the expressiveness of temporal logics containing it.
It would appear that reductions from PITL to PTLu could be automated for a range of syntactic classes of formulas.
To further illustrate the potential of reflection, let us now consider how to check ' f the validity of a formula (w [?]
Sys ) [?]
(2A [?]
fin w ), for some system Sys expressed in PITL.
For finite-time analysis, this has the reflection ((fin w) [?]
Sys r ) [?]
(2Ar [?]
w' ).
We can reduce Sys r to some PTL formula X with auxiliary variables and test f finite-time satisfiability of (fin w) [?]
X [?]
!
(X ' [?]
w' ), where X ' is a reflection of 2A u expressed in PTL as described above for the example PeteSafe 0 .
For infinite time, we can first reduce Sys to a PTL formula with auxiliary variables or an omega automaton [37,77].
As we show in [55], these can be represented in PTL by a low-level transition configuration of the form below: 2T  [?]
init  [?]
2 3+ L,  (96)  where T is an NL1 formula, init is a state formula, 3+ L abbreviates  3L (strict 3), and L is a finite conjunction of implications each of the form w [?]
3w' .
As shown in [55], the transition configuration has ultimately periodic models and is equivalent to the next formula in infinite time: (X ''  [?]
init )[?]
X ''  [?]
L [?]
(V - V )  [?]
,  (97)  where X '' denotes 2(more [?]
T ) and V - V is the conjunction of temporal assignments v - v for each variable v in the transition configuration.
Note that in [55], the chop-omega operator (Ao ) is used instead of strong chop-star (A[?]
).
However, the two operators have identical semantics in infinite intervals.
Testing for  Compositional reasoning using intervals and time reversal  63  ' f infinite-time validity of (w [?]
Sys ) [?]
(2A [?]
fin w ) is reducible to checking infinitef time unsatisfiability of Sys [?]
w [?]
!2A .
Here fin w' is trivially true for infinite time and ignored.
We then replace Sys by (97) to obtain the formula below:  (X ''  init )[?]
X ''  [?]
[?]
L [?]
(V - V )  [?]
[?]
w  [?]
f !2A.
This is equivalent to a variant with w in the chop's left side: (X ''  [?]
init  [?]
w)[?]
X ''  [?]
L [?]
( V - V)  [?]
[?]
f !2A.
(98)  The next semantic inference rule (related to (36)) reduces testing unsatisfiability for (98) to finite-time unsatisfiability: |= finite [?]
f (B1 [?]
B2[?]
) [?]
2B 3  =  |=  f (B1 [?]
B2[?]
) [?]
2B 3,  where the Bi s can be any formulas.
More precisely, it follows from this that if f the conjunction (B1 [?]
B2[?]
) [?]
!2B 3 is unsatisfiable, then it is unsatisfiable in finite time.
Observe that (98) has this form.
In order to do the testing for finite time, we can first reflect (98) and reduce it to a PTL formula with more auxiliary variables.
f For example, if 2A is the formula PeteSafe 0 we reflected above and reduced to u PTL , then this PTLu reflection can be used.
The transition configuration (96) is only meant for analysing infinite-time behaviour.
However, a simplified transition configuration of the form shown below can ' f analogously be used for checking finite-time validity of (w [?]
Sys ) [?]
(2A [?]
fin w ): 2T  [?]
init  [?]
finite.
We would like to see these rather experimental ideas implemented and also to have this approach compared with others, such as one based on a reduction of the ' f f implication (w [?]
Sys ) [?]
(2A [?]
fin w ) to a suitable formula with A instead of 2A .
Various formulas in our analysis of Peterson's algorithm could be used as an initial test.
15 Discussion  We now touch upon a number of topics with relevance to our framework based on ITL, 2-to-1 formulas and time symmetry.
15.1 Summary of formulas closed under conjunction and box For the convenience of readers, Table 11 lists the classes of formulas closed under conjunction and 2 which we have looked at.
It also mentions where they are described and some of their uses with suitable formulas.
However, note that the "almost" *-to-1 formulas are not a proper class.
We plan in future work to discuss some other classes of formulas which are closed under conjunction and 2 and have potential applications.
One example is i the 1-to-2i formulas, that is, any formula A for which the implication |= A [?]
2A is valid.
Recall that the operator 2i (defined in Table 1 in Sect.
2) examines all prefix subintervals.
In contrast, the operator 2f only examines prefix subintervals having  64  Ben Moszkowski  Class of formulas 2-to-1 Formulas *-to-1 Formulas +-to-1 Formulas ("Almost" *-to-1 Formulas f Formulas 1-to-2 1-to-2 Formulas  Where defined Def.
4.1 Def.
10.1 Def.
10.2 Lemma 10.11 Def.
8.2 Remark 8.6  Some uses ISEPI Sequential combining ISEPI Iteration ISEPI Iteration ISEPI Iteration ) ISEPI Extend rightward Import formula into 3  Table 11 Various classes of formulas closed under conjunction and box  finite length.
We are studying whether the operator 2i and its associated class of f formulas in practice.
For 1-to-2i formulas can be used instead of 2f and 1-to-2 example, 2f ((fin w) [?]
3B ) is semantically equivalent to the 2i -formula 2i ((sfin w) [?]
3B ).
Here sfin w is a strong version of fin derivable as 3(empty [?]
w) and also expressible as finite [?]
fin w. Our general experience is that weak interval operators can sometimes be more convenient in applications involving compositionally.
More evidence one way or the other still needs to be collected.
We presented in previous sections various results which relate some of the classes.
For example, Theorem 10.7 gives a sufficient condition for a 2-to-1 formula to also be +-to-1.
Similarly, Lemma 10.11 concerns 2-to-1 formulas which are "almost" *-to-1.
It seems worthwhile to further investigate interrelationships between various classes.
We report some new results in [58].
15.2 Exogenous and endogenous frameworks Our compositional way of reasoning about concurrency in ITL using 2-to-1 formulas contrasts with the better known and much more widely used one based on point-based temporal logic that Pnueli [65] and others have quite successfully advocated.
In particular, the point-based approach does not represent or reason about a program directly in the logic but requires it to be first translated into a state-transition system with many labels (as was also done earlier by Floyd [20]).
Temporal logic is used to reason about these.
Pnueli already in his first publication about temporal logic over thirty five years ago describes this as being endogenous [65]: Another point that is worth mentioning is that the [Endogenous] approach taken here can be classified together with Floyd's [20], .
.
.
.
By that [the term Endogenous] we mean that we immerse ourselves in a single program which we regard as the universe, and concentrate on possible developments within that universe.
Characteristic of this approach is the first phase which translates the programming features into general rules of behavior which we later logically analyze.
An ITL-based analysis, on the other hand, is much closer to what Pnueli [65] refers to as being exogenous when he compares the two categories: These [proponents of Exogenous systems such as Hoare [30]] suggest a uniform formalism which deals in formulas whose constituents are both logical assertions and program segments, and can express very rich relations  Compositional reasoning using intervals and time reversal  65  between programs and assertions.
We will be the first to admit the many advantages of Exogenous systems over Endogenous systems.
These include among others: a.
The uniform formalism is more elegant and universal, richer in expressibility, no need for the two-phase process of Endogenous systems.
b. Endogenous systems live within a single program.
There is no way to compare two programs such as proving equivalence or inclusion.
c. Endogenous systems assume the program to be rigidly given, Exogenous systems provide tools and guidance for constructing a correct system rather than just analyse an existent one.
Against these advantages Endogenous system can offer the following single line of defense: When the going is tough, and we are interested in proving a single intricate and difficult program, we do not care about generality, uniformity or equivalence.
It is then advantageous to work with a fixed context rather than carry a varying context with each statement.
Under these conditions, Endogenous systems attempt to equip the prover with the strongest possible tools to formalize his intuitive thinking and ease his way to a rigorous proof.
We do not believe that this is the place for a detailed, meaningful assessment of the merits of the (endogenous) point-based and (exogenous) interval-based temporal frameworks, particularly since ours is certainly much more experimental and less applied.
It seems appropriate to quote below the related discussion by Harel et al.
in their comparison of Dynamic Logic (DL) [27,28] with point-based temporal logic since the succinctly expressed points concerning compositionality equally apply here: There are two main approaches to modal logics of programs: the exogenous approach, exemplified by Dynamic Logic and its precursor Hoare Logic [30], and the endogenous approach, exemplified by Temporal Logic and its precursor, the invariant assertions method of Floyd [20].
A logic is exogenous if its programs are explicit in the language.
Syntactically, a Dynamic Logic program is a well-formed expression built inductively from primitive programs using a small set of program operators.
Semantically, a program is interpreted as its input/output relation.
The relation denoted by a compound program is determined by the relations denoted by its parts.
This aspect of compositionality allows analysis by structural induction.
The importance of compositionality is discussed by van Emde Boas [80].
In Temporal Logic, the program is fixed and is considered part of the structure over which the logic is interpreted.
The current location in the program during execution is stored in a special variable for that purpose, called the program counter, and is part of the state along with the values of the program variables.
Instead of program operators, there are temporal operators that describe how the program variables, including the program counter, change with time.
Thus Temporal Logic sacrifices compositionality for a less restricted formalism.
Readers should be able to readily discern that the explanation of Harel et al.
gives the impression that temporal logic as a whole is somehow intrinsically limited to being endogenous.
The authors do not mention research exploring exogenous uses  66  Ben Moszkowski  of temporal logics to reason about imperative program behaviour.
However several earlier publications on this subject by us and others were already available at the time (e.g., [13, 16, 23, 24, 47-51]).
Most of these appeared significantly before the summary appeared.
Unlike Dynamic Logic, this ITL-based work does not have separate notations for programs and formulas.
Our range of new and fundamental mathematical results about 2-to-1 formulas and time symmetry are a direct continuation of the research on the exogenous use of ITL to express imperative programming constructs.
This is a topic we have been pursuing since the 80s.
More recent work by Duan et al.
[17] and the KIV theorem prover group [7] concerns exogenous uses of variants of ITL for concurrent algorithms.
15.3 2-to-1 formulas and the assumption of discrete time Our central Theorem 4.5 states that 2-to-1 formulas are closed under the temporal operator 2.
Observe that the proof there requires that time is linear but does not at all depend on it being discrete.
The theorem is even applicable to a restricted version of PITL consisting of conventional propositional logic with the sole addition of the temporal operator weak chop.
Now 2 is the only other temporal operator needed to formalise basic 2-closure of 2-to-1 formulas.
It is not hard to derive 2 from weak chop (as described in our earlier publications [48-51]): inf = b true ; false  finite = b !inf  3A = b finite ; A  So Theorem 4.5 seems quite basic in the theory of temporal logic.
2A = b !3!A.
We can alternatively take strong chop as a primitive to obtain 2-to-1 formulas using Lemma 4.2's second characterisation of them (i.e., |= (A[?
]A) [?]
A).
The PTL temporal operator 2 is then derivable as shown in Table 1 in Sect.
2 in order to formalise 2-closure of 2-to-1 formulas.
Our comments here about Theorem 4.5 not requiring discrete time also apply to the analogous Lemma 10.5 concerning the closure of +-to-1 formulas under 2.
If we take skip and either weak or strong chop as the two temporal primitives, then the derived operator until defined in Sect.
3.3 (and used to express 2-to1 formulas in Sects.
5 and 14) is expressible without chop-star by means of the following semantic equivalence: |=  T until A  [?]
finite  [?]
 2(more [?]
T ) ; A  This uses the PTL subformula 2(more [?]
T ) instead of the PITL subformula (skip [?]
T )[?]
in the original definition of T until A in Sect.
3.3.
The two subformulas are semantically equivalent because of the valid PITL equivalence below (we formally state and prove this in [55, Theorem 5.4]): |=  2(more [?]
T )  [?]
(skip  [?]
T )[?]
.
Note that our technique for defining until as a derived operator using skip assumes discrete time.
However, the second definition of until can be made to work without discrete time if the left operand is limited to being a state formula (e.g., p until 2q ).
We can simply take either more or empty to be a primitive operator.
Alternatively, if chop-star is taken to be a primitive, then we first derive empty from chop-star as false [?]
and then derive more (which is normally defined using skip ) using !empty .
Compositional reasoning using intervals and time reversal  67  15.4 Empty intervals We have for about thirty years used the adjective "empty" to describe one-state intervals in ITL.
Some readers will surely find this convention a bit puzzling because, in contrast, the empty word in regular languages has no letters at all.
Let us now examine the choice of terminology.
This also helps explain the behaviour of chop-star with one-state intervals, which is a further source of confusion.
In language theory, the empty word is the unique word with no letters at all.
However, since the time of our early work on ITL [45-47], we have alway let the derived construct empty (defined in Table 1 in Sect.
2) denote the test for one-state intervals, which are also known in ITL as empty intervals.
In fact, intervals in ITL and PTL with finite time always have at least one state, so there is normally no ambiguity about the meaning of the word "empty" in these logics.
Another reason why the term "empty" seems reasonable is because one-state intervals in fact play a role in ITL quite similar to empty words in regular languages and the standard finite-state automata associated with them.
For example, some of our proofs of axiomatic completeness for versions of ITL [52, 53, 57] use such automata to encode ITL formulas, but the operation of the automata is modified so that they always examine at least one letter.
Such a letter represents both an individual state and a one-state interval.
The appropriateness of using "empty" for one-state intervals can also be clearly seen by means of a comparison of Kleene star with chop-star's semantics on finite intervals: - Standard definition of Kleene star on a regular language L: Define L0 to be the singleton set {o} containing just the empty word o.
For each k >= 0, inductively define Lk+1 to be the set of finite words {ab : a [?]
L, b [?]
Lk }, where ab is the usual string concatenation of words a and b .
The language L* S is defined to be the infinitary union of these sets: k>=0 Lk .
- Semantics of PITL's chop-star for finite intervals: For any PITL formula A, we can analogously define A0 to be the formula empty and for each k >= 0, the formula Ak+1 to be A[?
]Ak .
For each k >= 0, let Sk denote the set of finite intervals which satisfy the formula Ak and let S ' denote the set of finite intervals which satisfy the chop-star formula A[?]
(as defined in Sect.
2).
Then the two S sets k>=0 S k and S ' can be shown to be equal.
Observe that the set obtained from the application of Kleene star to a regular language, even the empty one {} with no words in it, always contains the empty word o.
This is because the language L0 equals {o} for every L, so L* also includes o as an element.
Similarly, if one applies chop-star to a PITL formula A, the result A[?]
is satisfied by all one-state (empty) intervals, even if A itself is unsatisfiable.
The formula false [?]
therefore provides a natural alternative way to express empty using just the boolean formula false combined with the temporal operator chopstar.
Duan [13-15, 17] and Bowman and Thompson [10] follow our convention of using empty , although Duan recently abbreviates it as e [18, 83].
We should point out that some other naming conventions for the formula empty nevertheless also exist.
For example, Paech uses the construct L0 [61].
This follows a convention found in some earlier work by others on process logics [26, 62].
The formula l = 0 is favoured in the Duration Calculus [59, 84, 85], where the special construct l  68  Ben Moszkowski  equals interval length.
The formula l = 0 can be abbreviated as [?][?]
[?][?].
The KIV group use the construct last to specify one-state intervals [7].
15.5 Star-to-1 formulas and chop-star fixpoints The *-to-1 formulas defined in the beginning of Sect.
10 (i.e., |= A* [?]
A in Definition 10.1) are identical to the ones called chop-star fixpoints in our earlier work on compositional reasoning in ITL [48-51].
A chop-star fixpoint is any formula A for which the equivalence A [?]
A* is valid.
Now for any PITL formula A, the implication A [?]
A* is valid.
Hence, A is *-to-1 iff the equivalence A [?]
A* is valid.
We present an analysis of *-to-1 formulas which relates them to other classes of formulas in recent work [58] that further explores the theory of 2-to-1 formulas.
16 Related work  We now consider relevant research by others and limit our coverage to the categories below: - Mirror images - Early proposals for using temporal logic with past time to reason about con-  currency - Interval-based approaches for analysing mutual exclusion  More information about other formal ways to analyse mutual exclusion, including extensive bibliographies, can be found in the various recent textbooks we already cited at the beginning of Sect.
13 when justifying our choice of Peterson's algorithm to illustrate time symmetry.
16.1 Mirror images Time reversal and reflections are related to mirror images (see Prior [67]) used with temporal logics to obtain a rule for past-time operators from an analogous one for future-time operators by means of time symmetry.
Analyses of conventional temporal logics for computer science typically cannot directly exploit mirror images because the time models are intentionally asymmetric with an infinite future and either no past or a bounded one.
That has severely limited the application of mirror images.
Nevertheless, Furia and Spoletini [21] and Reynolds [69] have recently applied mirror images to symmetric time models (e.g., bounded past and future).
This demonstrates ongoing interest in mirror images and associated techniques.
16.2 Early applications of temporal logic with past time to concurrency Our presentation already mentioned in Sect.
6 the work by Pnueli [66] and Lichtenstein, Pnueli and Zuck [41] in the mid 80s which formalises safety properties using temporal formulas of the form 2(w [?]
X ), where the only temporal operators in X are past-time ones such as those described in Sect.
5.1.
Therefore, X can just  Compositional reasoning using intervals and time reversal  69  concern past states and the current state, but not future ones.
Only Pnueli [66] specifically discusses mutual exclusion and Peterson's algorithm.
One motivation for using past time is to assist in doing backward analysis about what must have preceded certain events.
We pointed in Sect.
6 out that the formula 2(w [?]
X ) bears a certain resemblance to our class of 2-to-1 PITL formulas having the form f (fin w ) [?]
3B 2 and showed how some instances can be formally related in a semantic sense.
One example of this given in Sect.
6 concerns the PITL formula (16) and PTL- formula (17) which we reproduce below for the convenience of readers: f (fin p) [?]
3(skip 2  [?]
q)    first  [?]
- q ).
2(p [?]
Pnueli's main justification given for past time is that point-based temporal logic without past-time constructs imposes a more global view of the system behaviour.
In contrast, past-time constructs help to modularly specify and analyse the behaviour of an individual process.
Interestingly, around the same time as Pnueli, both Barringer and Kuiper [3, 4] and Koymans, Vytopil and de Roever [36] similarly suggest the use of pasttime constructs for reasoning about concurrency.
They do not discuss ones of the form 2(w [?]
X ).
However, Barringer, Kuiper and Pnueli in the slightly later joint paper [5] mention a couple of formulas of this kind and also examine mutual exclusion and Peterson's algorithm.
The straightforward definitions of satisfiability and validity we use for PTL- in Sect.
5.1 correspond to the so-called floating framework of PTL with past time.
However, Manna and Pnueli propose another approach called the anchored framework [42] (also discussed by Lichtenstein and Pnueli in [40]) which they argue is superior.
In this framework, satisfiability and validity only examine pairs of the form (s, 0).
There exist ways to go between the two conventions, but we will not delve into this here and instead simply assume the more traditional floating interpretation.
16.3 Other interval-based analyses of mutual exclusion We now mention some interval-based work involving algorithms for mutual exclusion and the related topic of lock-free data structures.
The only previously published analysis of Peterson's algorithm in some ITL variant seems to be the one by Pei and Xu [63] which uses the Discrete Time Duration Calculus [25,84].
Verification is performed using model checking with the popular SPIN tool [31] and is global rather than modular in the sense of Pnueli [66] (as we briefly discussed in Sect.
16.2).
Projection Temporal Logic is an ITL extension with operators for temporal granularities and framing [13-15, 17].
Duan [13, 14] expresses Dekker's mutual exclusion algorithm (first published by Dijkstra in [12]) in Projection Temporal Logic but without any formal analysis.
Yang, Duan and Ma [82] have applied Projection Temporal Logic to the analysis of an mutual exclusion example involving a counter and described earlier by Biere et al.
[8].
The interactive theorem prover PVS [60] provides tool support for a global proof in Pnueli's sense involving the combined behaviour of two concurrent processes.
Consequently, no compositional properties involving the correctness of the individual processes are given.
Duan, Zhang and  70  Ben Moszkowski  Koutny [18] investigate axiomatic completeness for propositional Projection Temporal Logic and illustrate their framework by summarising the global analysis of another mutual exclusion example.
In principle, Projection Temporal Logic supports past-time constructs, so a modular analysis, at least in Pnueli's sense, seems feasible.
However, the case studies of mutual exclusion in [18, 82] are formalised in - ("previa version of the logic where the only past-time construct, the operator  ous"), seems intended solely for framing variables.
Our techniques involving time symmetry and compositional formulas which are closed under conjunction and the temporal operator 2 might also be applicable to analysis involving Projection Temporal Logic because it supports basic ITL operators.
The KIV interactive theorem prover group [7,79] has combined a variant of ITL with the rely-guarantee paradigm [34, 35] of Jones to verify lock-free algorithms.
However, they have not yet looked at mutual exclusion.
Moreover, the lack of much published literature on applying the quite established rely-guarantee approach to mutual exclusion (e.g., Stark [74] and see also the related work of Stolen [75] and Collette [11]) suggests that the framework is not particularly well suited for it.
The textbook by de Roever et al.
[70] presents a rely-guarantee example involving mutual exclusion and is a comprehensive source of information about compositional reasoning based on rely-guarantee conditions as well as other similar work.
We take this opportunity to also mention a class of ITL formulas which Siewe et al.
[73] and Janicke et al.
[33] use for describing access control policies.
Such formulas have the form given below:  f (3B ) [?]
fin w .
2  (99)   Their syntax makes them similar to the 2-to-1 formula 2f (fin w) [?]
3B we first discussed in Sect.
6.
However, the variant (99) is not necessarily 2-to-1.
For example, consider any three-state interval s .
It has exactly two two-state subintervals.
Each of these subintervals trivially satisfies the next 2f -formula:    f 2 3(skip [?]
skip ) [?]
fin false .
(100)  This is because a one- or two-state interval does not satisfy the implication's left operand 3(skip [?]
skip ), which is only true for intervals with three or more states.
Hence, the implication's right operand fin false is ignored.
Now let A denote the f -formula (100).
Our reasoning so far about the subintervals ensures that the 2 three-state interval s satisfies the chop formula A; A.
Nevertheless, s fails to satisfy A because the left subformula 3(skip [?]
skip ) of the implication in A is true in s , but the right subformula fin false is not.
Hence, s does not satisfy the implication (A; A) [?]
A, so the formula (100) is not 2-to-1.
Conclusions and Further Work We believe that our results about interval-based compositional reasoning using 2to-1 formulas and time symmetry are promising.
The various compositional classes of formulas described here which are closed under conjunction and the temporal operator 2 seem quite intriguing owing to their simple mathematical features and natural connections with PTL.
The approach therefore appears worthy of further study.
Moreover, perhaps the application of the compositional classes and time  Compositional reasoning using intervals and time reversal  71  symmetry can even somewhat narrow the currently perceived wide practical gap between PITL and PTL and help increase combined use of the two formalisms.
Possible connections could also be explored involving temporal logics with the same expressiveness as PITL but lower computational complexity such as Regular Linear Temporal Logic proposed by Leucker and Sanchez [39, 71].
Incidentally, PITL itself contains a natural, equally expressive sublogic of this sort called Fusion Logic [54, 55], which has some tool support.
Ideally, we would also like to see a comparative analysis encompassing a number of suitable benchmark applications, range of formalisms and models of concurrency such as interleaving and true concurrency.
It furthermore seems appropriate to evaluate the tradeoffs between analyses involving concrete algorithms and more abstract ones.
The compositional details required in our analysis of Peterson's algorithm certainly suggest to us that abstraction can be quite beneficial.
We have clearly focused our attention on modular techniques here, but the nature of both global and modular ones deserves further investigation.
Our future research plans include using 2-to-1 formulas and time symmetry in a calculus of sequential and parallel composition based on Hoare triples having assertions expressed in ITL.
Implementations of decision procedures for PITL using time symmetry and reductions to point-based temporal logic are also envisioned.
We end our discussion here by noting that we believe that the basic mathematical concepts described here enrich the body of knowledge about intervals and temporal logics, no matter what the ultimate practical implications might be.
They include some elementary and exciting properties about compositionality and time symmetry which turned out with the hindsight of several decades to be quite elusive and so until now were completely overlooked and unexplored.
It also seems remarkable that most of them only involve the subset of PITL with just the temporal operators chop and skip , but not chop-star.
Perhaps similar treasures still remain hidden, waiting to be discovered.
We believe that the further systematic and scientific study and application of the compositional techniques we already presented in our earlier work [48-51], together with our interval-oriented analysis of conventional point-based linear time temporal [55] and new completeness proof for PITL with infinite time [57], could help in the exploration.
This view is supported by the fact that the material in these publications played a crucial part in leading us to uncovering the results we have described here.
Acknowledgements We would like to thank Antonio Cau, Amin El-kustaban, Helge Janicke, Maciej Koutny, Sven Schewe, Xiaoxiao Yang and anonymous referees for comments.
Shirley Craig's outstanding library services deserve special mention.
References 1.
Aceto, L., Ingolfsdottir, A., Larsen, K.G., Srba, J.: Reactive Systems: Modelling, Specification and Verification.
Cambridge University Press (2007) 2.
Balser, M., Baumler, S., Knapp, A., Reif, W., Thums, A.: Interactive verification of UML state machines.
In: J. Davies, W. Schulte, M. Barnett (eds.)
Proc.
6th International Conference on Formal Engineering Methods (ICFEM 2004), LNCS, vol.
3308, pp.
434- 448.
Springer-Verlag (2004) 3.
Barringer, H., Kuiper, R.: Hierarchical development of concurrent systems in a temporal logic framework.
In: S.D.
Brookes, A.W.
Roscoe, G. Winskel (eds.)
Seminar on Concurrency, LNCS, vol.
197, pp.
35-61.
Springer-Verlag (1985)  72  Ben Moszkowski  4.
Barringer, H., Kuiper, R.: Towards the hierarchical, temporal logic, specification of concurrent systems.
In: B. Denvir, W. Harwood, M. Jackson, M. Wray (eds.)
The Analysis of Concurrent Systems, LNCS, vol.
207, pp.
157-183.
Springer-Verlag (1985) 5.
Barringer, H., Kuiper, R., Pnueli, A.: A really abstract concurrent model and its temporal logic.
In: Proc.
13th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages (POPL'86), pp.
173-183.
ACM (1986) 6.
Baumler, S., Balser, M., Nafz, F., Reif, W., Schellhorn, G.: Interactive verification of concurrent systems using symbolic execution.
AI Communications 23(2-3), 285-307 (2010) 7.
Baumler, S., Schellhorn, G., Tofan, B., Reif, W.: Proving linearizability with temporal logic.
Formal Aspects of Computing 23(1), 91-112 (2011) 8.
Biere, A., Cimatti, A., Clarke, E.M., Strichman, O., Zhu, Y.: Bounded model checking.
Advances in Computers 58, 117-148 (2003) 9.
Bowman, H., Cameron, H., King, P., Thompson, S.: Mexitl: Multimedia in Executable Interval Temporal Logic.
Formal Methods in Systems Design 22(1), 5-38 (2003) 10.
Bowman, H., Thompson, S.J.
: A decision procedure and complete axiomatization of finite Interval Temporal Logic with projection.
Journal of Logic and Computation 13(2), 195- 239 (2003) 11.
Collette, P.: Composition of assumption-commitment specifications in a UNITY style.
Science of Computer Programming 23(2-3), 107-125 (1994) 12.
Dijkstra, E.W.
: Cooperating sequential processes.
In: F. Genuys (ed.)
Programming Languages: NATO Advanced Study Institute, pp.
43-112.
Academic Press (1968) 13.
Duan, Z.: An extended interval temporal logic and a framing technique for temporal logic programming.
Ph.D. thesis, Dept.
of Computing Science, University of Newcastle Upon Tyne (1996).
Technical report 556, later published as [14] 14.
Duan, Z.: Temporal Logic and Temporal Logic Programming.
Science Press, Beijing, China (2005).
Published version of [13] 15.
Duan, Z., Koutny, M.: A framed temporal logic programming language.
Journal of Computer Science and Technology 19(3), 341-351 (2004) 16.
Duan, Z., Koutny, M., Holt, C.: Projection in temporal logic programming.
In: F. Pfenning (ed.)
Proc.
of Logic Programming and Automated Reasoning (LPAR '94), LNCS, vol.
822, pp.
333-344.
Springer-Verlag, Berlin (1994) 17.
Duan, Z., Yang, X., Koutny, M.: Framed temporal logic programming.
Science of Computer Programming 70(1), 31-61 (2008) 18.
Duan, Z., Zhang, N., Koutny, M.: A complete axiomatization of propositional projection temporal logic.
Theor.
Comp.
Sci.
(2012).
DOI 10.1016/j.tcs.2012.01.026 19.
Fisher, M.: An Introduction to Practical Formal Methods Using Temporal Logic.
John Wiley & Sons (2011) 20.
Floyd, R.W.
: Assigning meanings to programs.
In: J.T.
Schwartz (ed.)
Proc.
AMS Symp.
on Applied Mathematics 19, pp.
19-32.
American Mathematical Society, Providence, Rhode Island, USA (1967) 21.
Furia, C.A., Spoletini, P.: Tomorrow and all our yesterdays: MTL satisfiability over the integers.
In: J.S.
Fitzgerald, A.E.
Haxthausen, H. Yenigun (eds.)
5th International Colloquium on Theoretical Aspects of Computing (ICTAC 2008), LNCS, vol.
5160, pp.
126-140.
Springer-Verlag (2008) 22.
Gomez, R., Bowman, H.: PITL2MONA: Implementing a decision procedure for propositional Interval Temporal Logic.
Journal of Applied Non-Classical Logics 14(1-2), 105-148 (2004).
Special issue on Interval Temporal Logics and Duration Calculi.
V. Goranko and A. Montanari, guest editors 23.
Hale, R.: Temporal logic programming.
In: A. Galton (ed.)
Temporal Logics and Their Applications, pp.
91-119.
Academic Press, London (1987) 24.
Hale, R.W.S.
: Programming in temporal logic.
Ph.D. thesis, Computer Laboratory, Cambridge University, Cambridge, England (1988).
Appeared in 1989 as Technical report 173 25.
Hansen, M.R., Zhou Chaochen: Duration calculus: Logical foundations.
Formal Aspects of Computing 9(3), 283-330 (1997) 26.
Harel, D., Kozen, D., Parikh, R.: Process Logic: Expressiveness, decidability, completeness.
Journal of Computer and System Sciences 25(2), 144-170 (1982) 27.
Harel, D., Kozen, D., Tiuryn, J.: Dynamic Logic.
MIT Press, Cambridge, Mass.
(2000) 28.
Harel, D., Kozen, D., Tiuryn, J.: Dynamic Logic.
In: D. Gabbay, F. Guenthner (eds.)
Handbook of Philosophical Logic, vol.
4, 2nd edn., pp.
99-217.
Kluwer Academic Publishers, Dordrecht (2002)  Compositional reasoning using intervals and time reversal  73  29.
Herlihy, M., Shavit, N.: The Art of Multiprocessor Programming.
Morgan Kaufmann Publishers Inc., San Francisco, CA, USA (2008) 30.
Hoare, C.A.R.
: An axiomatic basis for computer programming.
Communications of the ACM 12(10), 576-580,583 (1969) 31.
Holzmann, G.: The SPIN Model Checker: Primer and Reference Manual.
Addison-Wesley Professional (2003) 32.
Interval Temporal Logic web pages.
http://www.tech.dmu.ac.uk/STRL/ITL/ 33.
Janicke, H., Cau, A., Siewe, F., Zedan, H., Jones, K.: A compositional event & time-based policy model.
In: Proceedings of POLICY2006, London, Ontario, Canada, pp.
173-182.
IEEE Computer Society Press (2006) 34.
Jones, C.B.
: Specification and design of (parallel) programs.
In: R.E.A.
Mason (ed.)
Proc.
IFIP Congress '83, pp.
321-332.
North Holland Publishing Co., Amsterdam (1983) 35.
Jones, C.B.
: Tentative steps toward a development method for interfering programs.
ACM Transactions on Programming Languages and Systems 5(4), 596-619 (1983) 36.
Koymans, R., Vytopil, J., de Roever, W.P.
: Real-time programming and asynchronous message passing.
In: Proceedings of the Second Annual ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing (PODC'83), pp.
187-197 (1983) 37.
Kroger, F., Merz, S.: Temporal Logic and State Systems.
Texts in Theoretical Computer Science (An EATCS Series).
Springer-Verlag (2008) 38.
Lamport, L.: Specifying Systems: The TLA+ Language and Tools for Hardware and Software Engineers.
Addison-Wesley Professional (2002) 39.
Leucker, M., Sanchez, C.: Regular Linear Temporal Logic.
In: C.B.
Jones, Z. Liu, J. Woodcock (eds.)
Proc.
4th International Colloquium on Theoretical Aspects of Computing (ICTAC'07), Macau, China, LNCS, vol.
4711, pp.
291-305.
Springer-Verlag (2007) 40.
Lichtenstein, O., Pnueli, A.: Propositional temporal logics: Decidability and completeness.
Logic Journal of the IGPL 8(1), 55-85 (2000) 41.
Lichtenstein, O., Pnueli, A., Zuck, L.: The glory of the past.
In: R. Parikh, et al.
(eds.)
Logics of Programs, LNCS, vol.
193, pp.
196-218.
Springer-Verlag, Berlin (1985) 42.
Manna, Z., Pnueli, A.: The anchored version of the temporal framework.
In: J.W.D.
Bakker, W.P.
de Roever, G. Rozenberg (eds.)
Linear Time, Branching Time, and Partial Order in Logics and Models for Concurrency (REX Workshop 1988), LNCS, vol.
354, pp.
201-284.
Springer-Verlag (1989) 43.
McCarthy, J., Hayes, P.J.
: Some philosophical problems from the standpoint of artificial intelligence.
In: D. Michie, B. Meltzer (eds.)
Machine Intelligence 4, pp.
463-502.
Edinburgh University Press, Edinburgh (1969).
Reprinted in [81, 431-450] 44.
Mo, D., Wang, X., Duan, Z.: Asynchronous communication in MSVL.
In: S. Qin, Z. Qiu (eds.)
13th Int'l Conf.
on Formal Engineering Methods (ICFEM 2011), LNCS, vol.
6991, pp.
82-97.
Springer-Verlag (2011) 45.
Moszkowski, B.: Reasoning about digital circuits.
Ph.D. thesis, Department of Computer Science, Stanford University (1983).
Technical report STAN-CS-83-970 46.
Moszkowski, B.: A temporal logic for multilevel reasoning about hardware.
Computer 18, 10-19 (1985) 47.
Moszkowski, B.: Executing Temporal Logic Programs.
Cambridge University Press, Cambridge, England (1986) 48.
Moszkowski, B.: Some very compositional temporal properties.
In: E.R.
Olderog (ed.)
Programming Concepts, Methods and Calculi (PROCOMET'94), IFIP Transactions, vol.
A-56, pp.
307-326.
IFIP, Elsevier Science B.V. (North-Holland) (1994) 49.
Moszkowski, B.: Compositional reasoning about projected and infinite time.
In: Proc.
1st IEEE Int'l Conf.
on Engineering of Complex Computer Systems (ICECCS'95), pp.
238-245.
IEEE Computer Society Press (1995) 50.
Moszkowski, B.: Using temporal fixpoints to compositionally reason about liveness.
In: He Jifeng, J. Cooke, P. Wallis (eds.)
BCS-FACS 7th Refinement Workshop, electronic Workshops in Computing.
BCS-FACS, Springer-Verlag and British Computer Society, London (1996) 51.
Moszkowski, B.: Compositional reasoning using Interval Temporal Logic and Tempura.
In: W.P.
de Roever, H. Langmaack, A. Pnueli (eds.)
Compositionality: The Significant Difference, LNCS, vol.
1536, pp.
439-464.
Springer-Verlag, Berlin (1998) 52.
Moszkowski, B.: An automata-theoretic completeness proof for Interval Temporal Logic (extended abstract).
In: U. Montanari, J. Rolim, E. Welzl (eds.)
Proc.
27th Int'l.
Colloquium on Automata, Languages and Programming (ICALP 2000), LNCS, vol.
1853, pp.
223-234.
Springer-Verlag, Geneva, Switzerland (2000)  74  Ben Moszkowski  53.
Moszkowski, B.: A complete axiomatization of Interval Temporal Logic with infinite time (extended abstract).
In: Proc.
15th Ann.
IEEE Symp.
on Logic in Computer Science (LICS 2000), pp.
242-251.
IEEE Computer Society Press (2000) 54.
Moszkowski, B.: A hierarchical completeness proof for Propositional Interval Temporal Logic with finite time.
Journal of Applied Non-Classical Logics 14(1-2), 55-104 (2004).
Special issue on Interval Temporal Logics and Duration Calculi.
V. Goranko and A. Montanari, guest editors.
55.
Moszkowski, B.: Using temporal logic to analyse temporal logic: A hierarchical approach based on intervals.
Journal of Logic and Computation 17(2), 333-409 (2007) 56.
Moszkowski, B.: Compositional reasoning using intervals and time reversal.
In: 18th Int'l Symp.
on Temporal Representation and Reasoning (TIME 2011), pp.
107-114.
IEEE Computer Society (2011) 57.
Moszkowski, B.: A complete axiom system for propositional Interval Temporal Logic with infinite time.
Logical Methods in Computer Science 8(3:10), 1-56 (2012) 58.
Moszkowski, B.: Interconnections between classes of sequentially compositional temporal formulas.
Inf.
Process.
Lett.
113(9), 350-353 (2013) 59.
Olderog, E.R., Dierks, H.: Real-Time Systems: Formal Specification and Automatic Verification.
Cambridge University Press, Cambridge, England (2008) 60.
Owre, S., Shankar, N.: A brief overview of PVS.
In: O.A.
Mohamed, C. Munoz, S. Tahar (eds.)
21st International Conference on Theorem Proving in Higher Order Logics (TPHOLs 2008), LNCS, vol.
5170, pp.
22-27.
Springer-Verlag (2008) 61.
Paech, B.: Gentzen-systems for propositional temporal logics.
In: E. Borger, H.K.
Buning, M.M.
Richter (eds.)
Proceedings of the 2nd Workshop on Computer Science Logic (CSL'88), LNCS, vol.
385, pp.
240-253.
Springer-Verlag (1989) 62.
Parikh, R., Chandra, A.K., Halpern, J.Y., Meyer, A.R.
: Equations between regular terms and an application to process logic.
SIAM Journal on Computing 14(4), 935-942 (1985) 63.
Pei Yu, Xu Qiwen: Checking interval based properties for reactive systems.
In: B. Steffen, G. Levi (eds.)
Verification, Model Checking, and Abstract Interpretation, LNCS, vol.
2937, pp.
51-75.
Springer-Verlag (2004) 64.
Peterson, G.L.
: Myths about the mutual exclusion problem.
Inf.
Process.
Lett.
12(3), 115-116 (1981) 65.
Pnueli, A.: The temporal logic of programs.
In: Proc.
18th Ann.
IEEE Symp.
on the Foundation of Computer Science (FOCS), pp.
46-57.
IEEE Computer Society Press (1977) 66.
Pnueli, A.: In transition from global to modular temporal reasoning about programs.
In: K.R.
Apt (ed.)
Logics and Models of Concurrent Systems, NATO ASI Series F, vol.
13, pp.
123-144.
Springer-Verlag (1985) 67.
Prior, A.: Past, Present and Future.
Oxford Univ.
Press, London (1967) 68.
Reif, W., Schellhorn, G., Stenzel, K., Balser, M.: Structured specifications and interactive proofs with KIV.
In: W. Bibel, P.H.
Schmitt (eds.)
Automated Deduction - A Basis for Applications, Volume II: Systems and Implementation Techniques, pp.
13-39.
Kluwer Academic Publishers, Dordrecht (1998) 69.
Reynolds, M.: A tableau for Until and Since over linear time.
In: 18th Int'l Symp.
on Temporal Representation and Reasoning (TIME 2011), pp.
41-48.
IEEE Computer Society (2011) 70. de Roever, W.P., de Boer, F., Hanneman, U., Hooman, J., Lakhnech, Y., Poel, M., Zwiers, J.: Concurrency Verification: Introduction to Compositional and Noncompositional Methods.
No.
54 in Cambridge Tracts in Theoretical Computer Science.
Cambridge University Press (2001) 71.
Sanchez, C., Leucker, M.: Regular Linear Temporal Logic with past.
In: 11th Int'l Conf.
on Verification, Model Checking, and Abstract Interpretation (VMCAI 2010), LNCS, vol.
5944, pp.
295-311.
Springer-Verlag (2010) 72.
Shanahan, M.: Solving the Frame Problem: A Mathematical Investigation of the Common Sense Law of Inertia.
MIT Press (1997) 73.
Siewe, F., Cau, A., Zedan, H.: A compositional framework for access control policies enforcement.
In: M. Backes, D. Basin, M. Waidner (eds.)
ACM Workshop on Formal Methods in Security Engineering (FMSE'03), pp.
32-42.
ACM Press, Washington, DC (2003) 74.
Stark, E.W.
: A proof technique for rely/guarantee properties.
In: Proceedings of the 5th Conference on Foundations of Software Technology and Theoretical Computer Science (FSTTCS 1985), LNCS, vol.
206, pp.
369-391.
Springer-Verlag (1985) 75.
Stolen, K.: A method for the development of totally correct shared-state parallel programs.
In: CONCUR 1991, LNCS, vol.
527, pp.
510-525.
Springer-Verlag (1991)  Compositional reasoning using intervals and time reversal  75  76.
Taubenfeld, G.: Synchronization Algorithms and Concurrent Programming.
Pearson/Prentice Hall (2006) 77.
Thomas, W.: Automata on infinite objects.
In: J. van Leeuwen (ed.)
Handbook of Theoretical Computer Science, vol.
B: Formal Models and Semantics, chap.
4, pp.
133-191.
Elsevier/MIT Press, Amsterdam (1990) 78.
Thums, A., Schellhorn, G., Ortmeier, F., Reif, W.: Interactive verification of Statecharts.
In: H. Ehrig, W. Damm, J. Desel, M. Grosse-Rhode, W. Reif, E. Schnieder, E. Westkamper (eds.)
SoftSpez Final Report, LNCS, vol.
3147, pp.
355-373.
Springer-Verlag (2004) 79.
Tofan, B., Baumler, S., Schellhorn, G., Reif, W.: Temporal logic verification of lockfreedom.
In: Proc.
MPC 2010, Springer LNCS 6120, pp.
377-396 (2010) 80. van Emde Boas, P.: The connection between Modal Logic and Algorithmic Logic.
In: 7th Symposium on Mathematical Foundations of Computer Science (MFCS 1978), lncs, vol.
64, pp.
1-15. springer (1978) 81.
Webber, L., Nilsson, N.J.
(eds.
): Readings in Artificial Intelligence.
Tioga Publishing Co., Palo Alto, California (1981) 82.
Yang, X., Duan, Z., Ma, Q.: Axiomatic semantics of projection temporal logic programs.
Mathematical Structures in Computer Science 20(5), 865-914 (2010) 83.
Zhang, N., Duan, Z., Tian, C.: A cylinder computation model for many-core parallel computing.
Theor.
Comp.
Sci.
(2012).
DOI 10.1016/j.tcs.2012.02.011 84.
Zhou Chaochen, Hansen, M.R.
: Duration Calculus: A Formal Approach to Real-Time Systems.
Monographs in Theoretical Computer Science (An EATCS series).
SpringerVerlag (2004) 85.
Zhou Chaochen, Hoare, C.A.R., Ravn, A.P.
: A calculus of durations.
Inf.
Process.
Lett.
40(5), 269-276 (1991)  This author-produced version was formatted on 26 July 2013.
Efficient Aggregation over Moving Objects* Peter Revesz Yi Chen Computer Science and Engineering Department University of Nebraska-Lincoln Lincoln, NE68588, USA {revesz,ychen}@cse.unl.edu Abstract We study two types of aggregation queries over a set S of moving point objects.
The first asks to count the number of points in S that are dominated by a query point Q at a given time t. The second asks to find the maximum number of points in S that are dominated by a query point at any time.
These queries have several applications in the area of Geographic Information Systems and spatiotemporal databases.
For the first query and any fixed [?
]dimension d, we give two different solutions, one using O( N ) time and O(N ) space and another using O(log N ) time and O(N 2 ) space, where N is the number of moving points.
When each of the points in S is moving piecewise linearly along the the same line and the total number [?]
of pieces is O(N ), then we can do the count query in O( N ) time and O(N ) space.
For the second query, when all objects move along the xaxis we give a solution that uses O(log N ) time and O(N 2 ) space in the worst case.
Our solutions introduce novel search structures that can have other applications.
1.
Introduction Aggregation operators are frequently used in database queries.
The efficiency of database queries with aggregate operators is well understood and studied in the context of traditional relational data.
However, aggregation operators are also important for more complex data that cannot be represented in relational databases.
Example 1.1 Suppose that a large company has a number of manufacturing plants P1 , P2 , P3 , .
.
.. Each plant produces four different products X1 , X2 , X3 and X4 .
The profit at each plant for each product changes over time as shown in Table 1.1.
* This research was supported in part by NSF grant EIA-0091530 and a Gallup Research Professorship.
Table 1.
Profits for various plant and product combinations.
Id 1 2 3 4 5 .. .
X1 t + 2t + 10 t3 - 8t - 10 t2 - 50 t4 - 16 t3 + 81 .. .
2  X2 80 10t 3t 7t 4t .. .
X3 t + 30 t2 - 2t 5t - 10 5t2 3 t - 21 .. .
X4 5t - 10 t3 - 3t + 4 t - 10 t - 30 t + 10 .. .
T t t t t t .. .
The company has the opportunity to buy a new plant Q where profits are rising rapidly.
The board of directors would approve the buy only if five years from now Q will be more profitable for each product than 10 of the current plants.
In this case, the input relations P (Id, X1 , X2 , X3 , X4 , T ) and Q(X1 , X2 , X3 , X4 , T ) form a constraint database [10, 12, 16].
Therefore, we can find out how many plants are less profitable in 2007 by the following SQL query: select count(Id) from P, Q where P.X1 < Q.X1 and P.X2 < Q.X2 and P.X3 < Q.X3 and P.X4 < Q.X4 and P.T = 2007 and Q.T = 2007; Suppose that the company has a long-term plan to eliminate all products except X1.
Therefore, the board of directors gives an approval for the purchase plan subject to the following extra condition: Q should have the potential to some day be more profitable on product X1 than 20 of their current plants.
We can find out the maximum number  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  of plants that will be less profitable than Q by the following SQL query: select count(Id) from P, Q where P.X1 < Q.X1 and P.T = Q.T group by T having count(Id) >= all (select count(Id) from P, Q where P.X1 < Q.X1 and P.T group by T);  taking pictures of the ground, which is represented as the rectangular area in Figure 1.
Given a time instance, find out how many cars will be covered in the picture at that time.
= Q.T  While Example 1.1 can be extended to any higher dimension, many practical aggregation queries use only 1, 2 or 3-dimensional moving objects.
Example 1.2 Consider a set of ships moving on the surface of the ocean.
The locations of these ships are known by an enemy submarine which moves secretly underwater at constant depth.
If the submarine fires, it calls attention to itself.
Hence the submarine wants to wait until the maximum number of ships are within its firing range (which is some rectangle with the submarine in the center) before firing at as many ships as possible.
Let us assume that we have the relations Ship(Id, X, Y, T ) and Range(X, Y, T ), which describe the ships and the firing range of the submarine, respectively.
A ship is in the firing range at a time instance if its (X, Y ) location is equal to a point in the Range at the same time instance.
Hence the above can be expressed by the following SQL query using a maximum aggregation operator.
select max(ship-count)) from (select count(Id) as ship-count from Ship, Range where Ship.X = Range.X and Ship.Y = Range.Y and Ship.T = Range.T group-by T); There are many alternatives to express in SQL the same query.
For example, the above SQL query could be also expressed in by another SQL query that has a structure similar to the second SQL query in Exercise 1.1.
It is a practical problem to recognize that these different SQL structures both express max-count queries, which we will define below.
In this paper, we do not deal with the parsing problem.
Example 1.3 We show in Figure 1 three cars driving along three path, which can be represented by piecewise linear constraints.
We assume each car travels at a constant speed in each line segment.
Assume a plane flying in the air keeps  Figure 1.
Aggregations on piecewise linearly moving points  Examples 1.1 and 1.2 are both cases of a group of frequently occurring aggregation problems where the input data can be visualized as a set S of N number of kdimensional moving points.
In Example 1.1 each point represents one plant and the value of the ith dimension represents the profit of the i-th product at that plant.
In Example 1.2 each point represents one ship in 2-dimensions using latitude and longitude.
In Example 1.3, the speed and direction of the cars change as they enter new line segments, but the movement can still be represented by piecewise linear constraints.
We say that point Pi dominates point Pj if and only if Pi has a larger value than Pj has for each dimension.
Then the queries in Examples 1.1 and 1.2 can be generalized as follows: Count: Given a moving point Q and a time instance t, find the number of points in S dominated by Q at time t. Max-Count: Given a moving point Q, find the maximum number of points in S that Q can dominate at any time.
In this paper we focus only on the above two aggregation queries, because several other more complex aggregation queries can be reduced to them or can be solved similarly.
For example: Range-Count: Given a time instance t and two moving points Q1 and Q2 , find the number of points in S located in the hyper-rectangle defined by Q1 and Q2 .
(This reduces to a sequence of count queries.)
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  Max-Time: Given a moving point Q and time instance t, find out whether Q dominates at time t the maximum possible number of points in S. (This reduces to testing whether the results of a count and a max-count query are the same.)
Sum: Assign a value to each moving point.
Then given a moving point Q and time instance t, find the sum of the values of the points in S dominated by Q at time t. (This requires only minor changes in the index structures that we develop for count queries.)
Aggregation queries can be evaluated in O(N log N ) time and O(N ) space (see Appendix).
However, this performance is not acceptable in applications where the input data is large and the query evaluation speed is critical, like in Example 1.2.
The goal of this paper is to develop novel indexing structures that can greatly speed up count and maxcount aggregate query evaluation.
There are some indexing structures for moving objects [1, 2, 5, 11, 17].
One may use these indices to answer the count and the range-count query by first finding the set of points S  [?]
S dominated by a new point Q or being within a hyper-rectangle defined by Q1 and Q2 , and then counting the number of points in S  .
However, the counting may require linear time in the size of S  .
Our goal is to find the count in logarithmic time.
Further, these indices cannot be used to answer the max-count and max-time queries.
As shown by Zhang et al.
[20], if we have a static set of points, then the range-count problem can be solved by generalizing some earlier work on dominance by Bentley [3].
Zhang and Tsotras [19] also considered the max-count aggregation problem for static sets of points in S. However, these methods are not easily generalizable to moving points, which is our focus in this paper.
Lazaridis and Mehrotra [13] , Choi and Chung [6] and Tao et al.
[18] study the approximation of aggregate queries for spatial and spatiotemporal data.
In contrast to them, our algorithm produce precise answers without a significant loss in performance.
This paper is organized as follows.
In Section 2, we review some basic concepts, including partition trees for indexing moving objects proposed by Agarwal et al.
[1].
In Section 3, we consider two different methods for answering count aggregation queries.
The first method extends partition trees, to partition aggregation trees.
The second method uses a novel data structure called dominance-time graphs.
Dominance-Time graphs are faster than partition aggregation trees and they can also be used when the position of moving points are represented by polynomial functions of time.
In Section 4 we consider max-count aggregation queries.
Finally, in Section 5 we discuss some open problems.
Our main results are summarized in Table 2,  Table 2.
Computational complexity of aggregation on moving objects.
Query Count Count Max  I/O [?]
N log N log N  S  D  Function  Method  N N2 N2  d d 1  linear polynomial linear  PA tree DT graph Dome subdiv  where D means dimensions and S means space requirements.
2.
Basic Concepts We review two basic concepts.
Duality [8] allows mapping k-dimensional moving points into 2k-dimensional static points.
Partition Trees proposed by Agarwal et al.
[1] are search trees for moving points.
2.1 Duality Suppose the positions of the moving points in each dimension can be represented by linear functions of time of the form f (t) = a*t+b, which is a line in the plane.
We may represent this line as a point (a, b) in its dual plane.
Similarly, a point (c, d) can be represented as a line g(t) = c*t+d in its dual plane.
Suppose line l and point P have dual point L and dual line p respectively.
Then, l is below P if and only if L is below p .
Lemma 2.1 Let P = aP *t+bP and Q = aQ *t+bQ be two moving points in one dimensional space, and P  (aP , bP ) and Q (aQ , bQ ) be their corresponding points in the dual plane.
Suppose P overtakes Q or vice versa at time instance t, then bP - bQ t=- aP - aQ Let Slope(P  Q ) denote the slope of the line P  Q .
Then we have t = -Slope(P  Q ), that is, t is equal to the negative value of the slope of the line P  Q .
Hence, given a time instance t, the problem of finding how many points are dominated by Q reduces to the problem of finding how many points are below l, where l is a line crossing Q in the dual plane with the slope -t. Definition 2.1 Let S be a set of N points and l be a line in the plane.
We define the function CountBelow(l) as follows.
If l is a vertical line with r1 points on the left and r2 points on the right, then CountBelow(l) = max(r1 , r2 ).
Otherwise, if r number of points are below l, then CountBelow(l) = r.  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  [?]
we can answer the count query in O( N ) time.
Then we describe a more novel data structure, called an dominancetime graph, that needs only logarithmic time.
l1  3.1 Partition Aggregation Trees l2  (A)  (B)  Figure 2.
Rank of a line.
Note that Definition 2.1 is logical, because if l is a vertical line, then we can always tilt it slightly left or right to get another line that has the same value of CountBelow as we defined.
Example 2.1 Figure 2 shows a set of points and two lines l1 and l2 .
There are four points below l1 , hence CountBelow(l1 ) = 4.
There are five points to the left and one point to the right of l2 , which is a vertical line.
Hence CountBelow(l2 ) = 5.
2.2 Partition Trees Given a set S of N points in two dimensional space, we represent a simplicial partition of S as P = {(S1 , [?
]1 ), (S2 , [?
]2 ), ..., (Sm , [?
]m )}, where Si 's are mutually disjoint subsets of S whose union is S, and [?
]i is a triangle that contains all points of Si .
For a given parameter r, 1 <= r < N , we say this simplicial partition is balanced if each subset Si contains between N/r and 2N/r points.
Figure 3(A) shows an example of balanced simplicial partition for 35 points with r = 6.
The crossing number of a simplicial partition is the maximum number of triangles crossed by a single line.
The following is known about crossing numbers: Theorem 2.1 (Matousek[14]) Let S be a set of N points in the plane, and let 1 < r <= N/2 be a given parameter.
For some constant a (independent of r), there exists a balanced simplicial partition P of size r, such that any line crosses at most cr1/2 triangles of P for a constant c. If r <= N a for some suitable a < 1, P can be constructed in O(N log r) time.
Using Theorem 2.1, it is possible to recursively partition a set of points in the plane.
This gives a partition tree.
3.
Count Aggregation Queries In this section, we first make a simple extension of partition trees, described in Section 3.1.
With the modification,  Definition 3.1 Let S be a set of N points in k dimensional space and T be a multi-level partition tree for S. Let vi be an internal node in T , which stores a triangle [?
]i .
We attach a new value Ai to node vi , such that Ai is the number of points in Si .
We call the new tree structure Partition Aggregation Tree (PA Tree).
Theorem 3.1 PA Tree is a linear [?]
size data structure that answers the count query in O( N ) I/Os.
Example 3.1 Figure 3(B) shows a partition tree with four top level triangles A, B, C and D. The query line q crosses two top level triangles A and B.
There are three second level triangles A4, B2 and B3 that are crossed by q.
Figure 3(C) shows the structure of the PA-tree.
For simplicity, we only show for each node the triangle name and the count of the points contained in that triangle.
To find CountBelow(q), we start from the root of the PA-tree, load all top level triangles into memory and compare them to the query line q.
Since both triangles C and D are below the line, we add the precomputed value to the result CountBelow(q) = 12 + 17 = 29.
For the triangles A and B, we traverse their children recursively.
In this case, triangle B4 is below q, then we have CountBelow(q) = CountBelow(q) + CountIn(B4) = 29 + 4 = 33, where CountIn(B4) is the number of points in the subset associated with B4.
When we reach the leaf nodes of the PA-tree, we compare each point in the node with q, and add the number of points below q.
There is one point in triangle B3 that is below q.
Finally, the answer to the aggregation problem is 34.
In Figure 3(C), we indicate using double sided rectangles those nodes that are accessed by this algorithm.
In Example 1.3, the movement of a car can be represented by piecewise linear functions.
When the direction or speed changes, we may consider the car to be replaced by a new car with different direction or speed.
We have the following theorem for the piecewise linearly moving points in one dimensional space: Theorem 3.2 Let S be a set of piecewise linearly moving points with N number of pieces in one dimensional space.[?
]The dominance-sum problem of S can be answered in O( N ) I/Os with O(N ) space.
The above talks about one dimensional space.
That may occur when each car is going on a straight highway, but each car may slow down in certain intervals due to road construction or heavy traffic, and they change direction only if they  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  A  B  A2  B1  A3  B2 q  A1 B3 B4  A4  C C1 C2  D1 D2  D  C3 (A) D3 D4 (B)  S  A1 5  A  16  A2 4  A3 4  B  A4 3  61  16  C  B1 B2 B3 B4 4 4 4 4  12  C1 C2 C3 4 4 4  D 17  D1 D2 D3 D4 4 4 4 5  (C)  Figure 3.
A partition aggregation tree.
make U-turns.
It is an open problem to find a similarly efficient solution for two or higher dimensional space.
true for time instance t that is within any of the open intervals.
Note that any real number and -[?]
and +[?]
are allowed as interval endpoints.
3.2 Dominance-Time Graph Partition aggregation trees are limited because they only work when the points are moving linearly.
In this section we introduce dominance-time graphs, a novel index data structure that can handle polynomial functions of time.
Definition 3.2 For two k-dimensional moving points P = (f1 , ..., fk ) and Q = (g1 , ..., gk ), we say P dominates Q at time t, denoted as dom(P,Q,t), if and only if fi (t) > gi (t) for 1 <= i <= k. If P does not dominate Q at time t, then we write ndom(P,Q,t).
Definition 3.3 Let S be a set of N moving points in k dimensional space.
The dominance-time graph G(V, E) for S is a directed labeled graph, such that for each point in S, there exists a corresponding vertex in V , and there is an edge in G from P to Q labeled by the set of disjoint intervals {(a1 , b1 ), ..., (am , bm )}, if and only if dom(P, Q, t) is  Example 3.2 Suppose that we are given the following set of two dimensional moving points: P1 = (t + 10, t - 5) P2 = (2t, 2t - 10) P3 = (3t + 5, 3t - 15) P4 = (4t - 5, 0) The dominance-time graph of these moving points is shown in Figure 3.
Note that for any time instance t [?]
(5, 10) the condition dom(P3 , P4 , t) is true.
Hence the edge from P3 to P4 is labeled {(5, 10)}.
The labels on the other edges can be found similarly.
Definition 3.4 Let P and Q be two moving points and t0 and t be two time instances such that t0 < t. We say that  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  P2  , 5)  )  8  (-  , -5)  8  (5,+  , 2.5)  8  (-  (-  8  P1  )  8  (10, +  (2.5, 5) )  8  (5, +  P3  P4 (5, 10) Figure 4.
A dominance-time graph.
between t0 and t an increment event happens to P with respect to Q if ndom(P, Q, t0 ) and dom(P, Q, t).
Similarly, we say that between t0 and t a decrement event happens to P with respect to Q if dom(P, Q, t0 ) and ndom(P, Q, t).
Definition 3.5 Let Rank(P,t) be the number of points that are dominated by P at time t. Lemma 3.1 An increment event happens to P with respect to Q if and only if there is an outgoing edge from P that has a label in which no interval contains t0 and some interval contains t. Similarly, a decrement event happens to P with respect to Q if and only if there is an outgoing edge from P that has a label in which some interval contains t0 and no interval contains t. Lemma 3.2 Let t0 and t be two time instances such that t0 < t. Let P be any vertex in a dominance-time graph.
Let m (and n) be the number of increment (and decrement) events that happen to P with respect to different other vertices between t0 and t. Then the following is true: Rank(P, t) = Rank(P, t0 ) + m - n Example 3.3 Table 3 shows the rank of each point of Example 3.2 at time instances t = -8 and t = 12.
Note that dom(P2 , P3 , -8) and ndom(P2 , P3 , 12) are both true.
Hence, an increment event happened to P2 between time t = -8 and t = 12.
Similarly, ndom(P2 , P1 , -8) and dom(P2 , P1 , 12) are also both true.Hence a decrement event happens to P2 between the same times.
Thus, according to Lemma 3.2, we have Rank(P2 , 12) = Rank(P2 , -8) + 1 - 1 = 1  Table 3.
Location and rank of points at times t = -8 and t = 12.
Point P1 P2 P3 P4  Location t = -8 (2, -13) (-16, -26) (-19, -39) (-37, 0)  Rank t = -8 2 1 0 0  Location t = 12 (22, 7) (24, 14) (41, 21) (43, 0)  Rank t = 12 0 1 2 0  3.3 Time and Space Analysis In this section we describe the basic structure of dominance-time trees and show how to use them to answer count aggregation queries in O(log mN ) I/Os, where N is the number of moving points and m is the maximum degree of the polynomial functions used to represent the position of the points.
A dominance-time tree for point P is a B-tree to index the consecutive time intervals: (-[?
], t1 ), (t1 , t2 ), .
.
.
, (ti , ti+1 ), .
.
.
, (tn , +[?])
such that during each interval (ti , ti+1 ), the rank of P remains unchanged.
The rank of P during these intervals and the ti endpoints of these intervals can be precomputed and stored in the B-tree.
Therefore, the B-tree can find the rank of P for any time instance in (-[?
], +[?]).
Let S be a set of N moving points.
For any point P in S, we may compute (precisely for polynomials up to degree 5  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  Theorem 3.3 Let S be a set of N moving points in kdimensional space.
Let m be a fixed constant and assume that the position of each moving point in each dimension is represented by a polynomial function of degree at most m. Given a point P in S and a time instance t, the DominanceTime Tree for each P [?]
S requires O(N ) space.
Hence the count aggregation problem can be done in O(logB N ) I/Os using a total of O(N 2 ) space.
e1 e2 e3 e4 5  9  18 22  30 35  t  Figure 5.
A time line.
and approximately for higher degree polynomials) a set of n time instances ti (1 <= i <= n) such that during each interval (ti-1 , ti ) the rank of P remains unchanged.
Example 3.4 Suppose in a dominance-time graph, there are four outgoing edges, e1 , e2 , e3 and e4 for a point P .
They are labeled as the following respectively: e1 : (5, 18), (22, 35) e2 : (9, 30) e3 : (0, 9), (22, +[?])
e4 : (0, 22) Figure 5 shows the intervals contained in the labels with thick line segments.
In this case, the B-tree contains the time instances 0, 5, 9, 18, 22, 30, 35 and the following time intervals: (-[?
], 0),(0, 5),(5, 9),(9, 18),(18, 22), (22, 30),(30, 35), (35, +[?])
Definition 3.6 Suppose G is a dominance-time graph for a set of moving points and P is a vertex in G. A DominanceTime Tree TP is a data structure based on a B-tree, which indexes all end points of time intervals contained in the labels of outgoing edges from P .
The leaf node of the dominance-time tree contains a list of consecutive time instances, t1 , t2 , ..., tb , and b + 1 data fields v1 , v2 , ..., vb+1 where b is chosen according to the size of the disk pages.
For each field vi for 1 <= i <= b we store the precomputed rank of P during the interval (ti-1 , ti ).
Given a time instance t, the rank of P can be found by searching the dominance-time tree until we find the leaf node with the interval that contains t. Now we can prove the following.
The preprocessing of the dominance-time tree structure involves computation of polynomial functions.
However, for a moving point which is represented by a polynomial function, it is not difficult to use piecewise linear functions to approximately represent its trajectory.
Using this approximation method, the number of time intervals when the rank of a particular point remain unchanged will remain unchanged.
4.
Max-Count Aggregation Queries Our max-count aggregation algorithm uses a novel data structure built on the concept of domes, which we introduce here as a new type of spatial partition of the dual plane of a set of one-dimensional moving points.
We start this section with a few definitions.
Definition 4.1 Let S be any set of points in the plane.
For any new point Q, we define MaxBelow(Q) to be the maximum number of points below any line that passes through Q.
Definition 4.2 Let S be any set of points in the plane.
Let L be the set of lines that cross at least two points in S or cross at least one point in S and are vertical.
For 0 <= i <= N , we define Li = {l [?]
L|CountBelow(l) + CountOn(l) >= i}, where CountOn(l) is the number of points in S crossed by line l. Definition 4.3 For any line l, let Below(l) be the halfplane below l, or if it is a vertical line, then the half-plane on that side of the line that contains more points.
Let Below(Lk ) be the intersection of the half-planes associated with the lines in Lk .
Let k-dome, denoted as dk , be the boundary of the region Below(Lk ).
The intuition is that any point above dk has a line through it with at least k points below.
Definition 4.4 Layer(k)= {Q|Q [?]
Below(Lk+1 ) and Q [?]
Below(Lk )}.
Example 4.1 We show in Figure 6 a set of seven points.
In this case, L7 is composed of the dotted lines (i.e., the lines crossing P2 P3 , P3 P4 ,P4 P5 and the two vertical lines  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  crossing P2 and P5 ), while L6 is composed of the union of the dotted and dashed lines (i.e, the lines crossing P2 P7 , P3 P5 , P4 P6 , P4 P7 and the two vertical lines crossing P3 and P6 ).
The two thick polygonal lines in the figure are d7 and d6 , respectively, and Layer(6) is the area between them.
Now we prove some properties of the above concepts.
Lemma 4.1 For any i and j such that i <= j, the following hold.
(1) Li [?]
Lj .
(2) Below(Li ) [?]
Below(Lj ).
(3) no point of dome di is above any point of dome dj .
Lemma 4.2 Layer(k) consists of those points that are strictly outside dk and on or inside dk+1 .
Lemma 4.3 Each point belongs to only one layer.
We can now show the following characterization of layers.
Theorem 4.1 Q [?]
Layer(m) - M axBelow(Q) = m. Theorem 4.1 implies that the layers partition the plane in such a way that there is a one-to-one correspondence between any element of the partition and the M axBelow value of the points in that element.
We can use this theorem to build a data structure for efficiently identifying which element of the partition a new point is located in, using the following well-known result from computational geometry.
Theorem 4.2 [15] Point location in an N-vertex planar subdivision can be effected in O(log N ) time using O(N ) storage, given O(N log N ) preprocessing time.
Lemma 4.4 Any dome dk has O(N ) edges.
Lemma 4.5 Let S be any set of N points in the plane and Q a query point.
Then we can find in O(log N ) time using an O(N 2 ) space data structure M axBelow(Q) = m. Lemma 4.6 Let S be a set of N points and Q a query point moving along the x axis.
Let S  and Q be the duals of S and Q, respectively.
Then the following hold.
(1) For any time instance t the moving point Q dominates CountBelow(l) number of points in S, where line l crosses Q and has slope -t. (2) The maximum number of points that Q dominates is M axBelow(Q ).
Finally, we have the following theorem.
Theorem 4.3 The Max-Count aggregation query can be answered using an O(N 2 ) size data structure in O(log N ) query time and O(N 2 log N ) preprocessing time.
The above considers only objects that exist at all times.
Suppose that objects only exist between times t1 and t2 .
That means that only lines passing Q and having slopes be(t ,t ) tween -t2 and -t1 are interesting solutions.
Let Li 1 2 be the modification of Li that allows only lines that have slopes between -t2 and -t1 and cross two or more points or cross only one point and have slopes exactly -t2 or -t1 .
With this modification, we can correspondingly modify the definition of layers.
Then Theorems 4.1 and 4.3 still hold.
5.
Further Work There are several interesting open problems.
We list below a few of these.
1.
Are there count or max-count aggregation algorithms that are more efficient in time or space than our algorithms, or can a tight lower bound be proven for these aggregation problems?
2.
Can the count aggregation algorithm for piecewise linear moving points in one dimension be [?]
extended to higher dimensions while keeping the O( N ) time and O(N 2 ) space in the worst case?
3.
Can the max-count aggregation algorithm in one dimension be extended to higher dimensions while keeping the O(log N ) time and O(N 2 log N ) space in the worst case?
4.
How can we make the data structures dynamic, that is, allow efficient deletions and additions of new moving points?
We have partial solution to this problem when only insertions are considered.
5.
What is the average case of the count and max-count algorithms?
6.
Can the algorithms be improved by considering approximations?
As described in Section 1, approximations for the count aggregation query were considered in the work of [13, 6, 18].
However, there are no approximation algorithms for the max-count aggregation problem.
7.
Moving objects can be represented not only by moving points but also by parametric rectangles [4], by geometric transformation objects [7, 9], or by some other constraint representation [12, 16].
These constraint representations are more general because they also represent the changing (growing, shrinking) shape of the objects over time.
It is possible to consider count and max-count aggregation queries on these more general moving objects.
Is it possible to solve these queries within the same time complexity?
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  P4 P3 P5  P7 P2 P6  P1 layer(6)  d6  d7  Figure 6.
Layer(6) for seven points.
We are also interested in implementations of these algorithms and testing them on real data, for example, aviation data sets, and truck delivery data sets.
References  International Converence on Management of Data, pages 57-64, 2000.
[6] Y.-J.
Choi and C.-W. Chung.
Selectivity estimation for spatio-temporal queries to moving object s. In SIGMOD, 2002.
[1] P. K. Agarwal, L. Arge, and J. Erickson.
Indexing moving points.
In Symposium on Principles of Database Systems, pages 175-186, 2000.
[7] J. Chomicki and P. Revesz.
A geometric framework for specifying spatiotemporal objects.
In Proc.
International Workshop on Time Representation and Reasoning, pages 41-6, 1999.
[2] J. Basch, L. J. Guibas, and J. Hershberger.
Data structures for mobile data.
In SODA: ACM-SIAM Symposium on Discrete Algorithms (A Conference on Theoretical and Experimental Analysis of Discrete Algorithms), 1997.
[8] M. de Berg, M. van Kreveld, M. Overmars, and O. Schwarzkopf.
Computational Geometry: Algorithms and Applications.
Springer Verlag, Berlin, 1997.
[3] J. L. Bentley.
Multidimensional divide-and-conquer.
Communications of the ACM, 23(4), 1980.
[9] S. Haesevoets and B. Kuijpers.
Closure properties of classes of spatio-temporal objects under Boolean set operations.
In Proc.
International Workshop on Time Representation and Reasoning, pages 79-86, 2000.
[4] M. Cai, D. Keshwani, and P. Revesz.
Parametric rectangles: A model for querying and animating spatiotemporal databases.
In Proc.
7th International Conference on Extending Database Technology, volume 1777, pages 430-44.
Springer-Verlag, 2000.
[5] M. Cai and P. Revesz.
Parametric r-tree: An index structure for moving objects.
In Proc.
10th COMAD  [10] P. C. Kanellakis, G. M. Kuper, and P. Z. Revesz.
Constraint query languages.
Journal of Computer and System Sciences, 51(1):26-52, 1995.
[11] G. Kollios, D. Gunopulos, and V. J. Tsotras.
On indexing mobile objects.
In ACM Symp.
on Principles of Database Systems, pages 261-272, 1999.
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  [12] G. Kuper, L. Libkin, and J. Paredaens.
Constraint Databases.
Springer Verlag, 2000.
[13] L. Lazaridis and S. Mehrotra.
Progressive approximate aggregate queries with a multi-resolution t ree structure.
In SIGMOD, 2001.
[14] J. Matousek.
Efficient partition trees.
Discrete Comput.
Geom., 8:315-334, 1992.
[15] F. P. Preparata and M. I. Shamos.
Computational Geometry: An Introduction.
Springer Verlag, New York, 1985.
[16] P. Revesz.
Introduction to Constraint Databases.
Springer Verlag, 2002.
[17] S. Saltenis, C. S. Jensen, S. T. Leutenegger, and M. A. Lopez.
Indexing the positions of continuously moving objects.
In SIGMOD Conference, pages 331-342, 2000.
[18] Y. Tao, J.
Sun, and D. Papadias.
Selectivity estimation for predictive spatio-temporal queries.
In ICDE, 2003.
[19] D. Zhang and V. J. Tsotras.
Improving min/max aggregation over spatial objects.
In ACM-GIS, pages 88-93, 2001.
[20] D. Zhang, V. J. Tsotras, and D. Gunopulos.
Efficient aggregation over objects with extent.
In Symposium on Principles of Database Systems, pages 121-132, 2002.
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE
"TellMe": A Novel Protocol and Location Prediction Scheme exploiting the "One For All" Framework for Location Management Amal ElNahas Faculty of Media Engineering and Techn.
German University in Cairo, Egypt Amal.ElNahas@guc.edu.eg  Omar H. Karam Faculty of Computer and Information Ain Shams University, Cairo, Egypt Ohkaram@asunet.shams.edu.eg  Ahmad Hamad Faculty of Computer and Information Ain Shams University, Cairo, Egypt Amhamad13@yahoo.com  Ingy Ramzy Faculty of Computer and Information Ain Shams University, Cairo, Egypt IngyRamzy@usa.com  Abstract  For a moving object equipped with a positioning device reporting its position to a location server, a location model defines what information to be stored at the server and exchanged between the object and the server.
Equivalently an update policy defines when an object should send a location update to optimize the bandwidth /precision tradeoff.
The proposal cited in this paper is based on the "One For All" (OFA) location management framework which exploits the fact that several objects tend to move in clusters (ex: fighters and soldiers in a battlefield), hence experience the same conditions and varying bandwidth availability which finally converges them to similar deviation and update behavior [3].
Accordingly, OFA considers mobility groups, where objects with similar movement pattern are grouped into clusters and a cluster leader (the one) sends updates on behalf of other objects within its group (the all), which significantly reduces the total number of updates while maintaining acceptable precision.
In This paper we extend the OFA through the "TellMe" protocol and prediction scheme, which decentralizes the task of mobility groups' formation, hence diminishing the incurred overheads.
Furthermore, it significantly improves the ongoing location prediction process at the moving objects and the location server by relying on what the group leader has communicated regarding the adopted path.
The outline of this paper is as follows.
In section 2 we survey existing location models and policies.
The motivation behind the proposal is introduced in section 3.
Section 4 elaborates the proposed "TellMe" protocol and prediction scheme which is evaluated through  Motivated by the surge of location prediction challenges and inspired by the mutual advancements in knowledge discovery and location management, this paper opts for exploiting the merits of the "One For All" (OFA) framework for moving objects databases(MOD) to address the crucial challenge of extrapolating objects' positions and balance the mainstay "bandwidth-precision" tradeoff.
The proposed "TellMe" protocol and prediction scheme utilizes the OFA mobility groups to provide individual objects with accurate foreknowledge of their motion paths.
Equivalently it diminishes the OFA's overheads of groups' formation.
Simulation results have proven that the "TellMe" supercedes the cost savings of the OFA's current protocol and prediction scheme.
1.
Introduction The emergence of location based services has created a surge of research activities that address the various challenges of situational awareness exposed by the specially featured location information stored and maintained in Moving Objects Databases (MOD).
Applications of situational awareness enabled through MOD extend to both the civilian and military fields [1][2].
MOD challenges involve devising efficient location models and update policies to cope with the dynamic nature of location information and improve the scarce bandwidth utilization yet maintain acceptable data precision.
Proceedings of the 12th International Symposium on Temporal Representation and Reasoning (TIME'05) 1530-1311/05 $20.00 (c) 2005 IEEE  experimentation presented in section 5.
Finally section 6 concludes the proposed work.
2.
Related Work The Moving-Objects Spatio-Temporal model (MOST) introduced in [4] exposes the concept of dynamic attributes which change as a function of time without being explicitly updated.
An explicit update is required to control the deviation between the actual and predicted object's position.
In [5] an information cost model is proposed that quantifies the tradeoff between the deviation and communication costs as an information cost, upon which two proposed update policies were based.
The MOST extension in [1] and [2] introduced the concept of deviation threshold (uncertainty) which resulted in extending the information cost model in [5] to account for the uncertainty.
The cost model derived three dead reckoning policies: Speed (SDR), Adaptive (ADR) and Disconnection Detection (DTDR) Dead Reckoning.
These policies employed the deviation threshold (uncertainty) to determine when an object should update its location information.
The Trajectory Location Management cited in [6], [7], [8] and [9] utilizes prior knowledge related to object's routes and destinations to reduce the overall costs.
Finally, the "One For All" (OFA) framework introduced in [3] extended the MOST model to handle objects' states and upkeep clusters/groups identifying data.
OFA equivalently extended the policy by having the group leader send updates on behalf of the group members.
OFA designates an analogy to the trajectory location management with no prior knowledge of objects' trips.
3.
Motivation The "One For All" (OFA) framework designates a new perspective for location management that considers mobility groups to enhance individual moving objects' behavior.
The OFA consolidates research efforts in knowledge discovery and location management by having the location server deploy a dynamic clustering algorithm that attempts to attach single moving objects to their nearest centroid/leader satisfying a clustering criteria.
During the objects' trip, the centroid broadcasts its location information and each follower updates its database position accordingly.
Nonetheless, the followers continuously compute their deviation as the difference between the predicted position (at the location server) and its' real position.
A follower detaches from the cluster, by  sending an update message whenever its deviation exceeds its computed threshold [3].
Though demonstrated significant cost savings regarding the number of updates, the OFA's novelty mandates numerous challenges.
Nonetheless, its trend is entailed by the nature of MOD applications either in the military or civilian fields.
Moreover it promises synergistic advances in query processing, particularly for the nearest neighbor and reverse nearest neighbor queries, a tribute to its pre-defined groupings [10].
3.1.
Groups/Clusters Formation Overheads The OFA employs two types of control messages sent from the location server to the moving objects for the sake of clusters/groups formation, namely: "Attach_Follower" and "Detach_Follower" messages [3].
These are utilized by the location server to acknowledge a group centroid/leader of an incepted follower, or simply to detach a follower.
Upon cluster formation the location server sends two "Attach_Follower" messages, one to the anticipated centroid, and the other to the moving object itself.
Depending on the object's state, the server may send a "Detach_Follower" request to detach an object from a previous centroid.
Accordingly a moving object joining a group penalizes the server an extra cost of three update control messages two for attachment and the one for detachment.
Although the OFA entails significant cost savings, its merits could be severely challenged in case of frequent detachments introduced by hesitant moving objects or simply enforced by the underlying environment.
Moreover, the OFA protocol of clusters/groups formation may result in bad groupings, since upon clustering, it utilizes prediction data to evaluate the clustering criteria.
The need for prediction stems from the fact that the location server attempts to cluster an object upon the reception of its location update, meanwhile, to elect the best centroid/leader relative to this object, the server has to quantify the relative distance and speed extracted from the position attribute of the anticipated leader, which may be outdated at the time of clustering.
This mandates employing prediction to extrapolate the leader's position, hence evaluate the clustering criteria at the time of group formation.
Intuitively, bad formations multiply the chances of detachments and penalize the overall information cost.
Accordingly, the clustering overheads should be practically diminished to enable the OFA to cope with the various movement patterns and achieve the anticipated cost savings.
Proceedings of the 12th International Symposium on Temporal Representation and Reasoning (TIME'05) 1530-1311/05 $20.00 (c) 2005 IEEE  3.2.
Managing highly dynamic objects The OFA underlying dead reckoning update policy employs a linear prediction function for extrapolating moving objects positions during their update-to-update interval.
This prediction method and other mathematical methods fail to provide accurate foreknowledge of objects' motion.
The challenge is an extension of prediction challenges in general that focus on how to communicate future experience.
Unfortunately, for the OFA, inaccurate prediction could highly penalize the overall cost.
Consider a centroid C and a follower F, where C attached F at a relative distance of dr and a relative speed of vr.
Assume t1 time units after the attachment, C decelerated to a stop in a traffic jam causing it to send a location update reporting its zero speed vc and position (xc, yc).
Relative to C's reported position, F's database speed would be vr.
Shortly after C, F decelerates to a stop in the same traffic jam.
The centroid C trapped in the jam for t2 time units would cause F to detach, since its predicted position, t2 time units after C's last update, reports a position that overshoots its real position and speed resulting in a fast deviation as a function of elapsed time.
Accordingly if F could induce this relative distance or speed change it would have maintained its group membership since it is evident that F and C exhibited the same movement behavior but at different times.
4.
"TellMe": The Protocol and Prediction Scheme The proposed protocol and prediction scheme, namely: "TellMe", exposes the full potential of the OFA framework through an enhanced de-centralized group formation protocol as well as an efficient prediction scheme.
The novelty of the "TellMe" lies in grouping objects that tend to exhibit the same behavior but at different time points.
This core concept enables the OFA to handle moving objects with high dynamicity, by assuming a semi-identical shifted behavior relative to the group leader.
This resolves the possibly high rate of change of the relative distances and speeds.
And better suits the motion of moving objects in a cluster/group, by assuming a consummate leader incepting the environmental incidents and communicating it to its followers who are expected to undergo the same experience afterwards.
Accordingly the leader acts as a traffic reporter, generating anticipated best routes of motion for its followers based on real life experience.
The strength of the "TellMe" is particularly exploited by applications demanding the province of recommended routes/motion paths for a particular service member, which mandates identifying the current traffic patterns and continuously updating the member with up-to-date recommended motion paths.
In this regard the "TellMe" realizes the full portfolio of benefits offered through the trajectory location management but with no prior knowledge and no overheads (for example: a traffic server, route composer,...etc).
Moreover it excels to provide a practical real-life experience rather than a proposed computed route.
It is worth mentioning that the centroid/leader communicates its experience to its followers, accordingly it is mandated that it precedes the followers in behavior.
Hence we shall denote the centroid as a "leader" and its cluster as a "group" to better communicate the logical concept.
4.1.
"TellMe": The Protocol The "TellMe" protocol addresses the OFA challenge of group formation overheads as well as the crucial aspect of the groupings' quality.
It adopts a decentralized approach for composing the mobility groups instead of relying on the location server to exploit these groups (centralized approach).
The merit is realized by enabling moving objects to broadcast its' location information which are captured by other objects (anticipated leaders) that are contending to win appropriate followers by advertising their motion plans.
The location server considers a leader offer of membership a winner if the object replies back the offer through an attachment approval.
Other objects receiving the approval, discard the sender as a potential candidate for group membership.
Noticeably, the objects demand to be acknowledged of other objects' states or motion paths ("TellMe") which decentralizes the group formation task and diminishes the grouping overheads.
Moreover, the groups are formed based on real-time data, rather than extrapolated prediction data which reduces the chances of bad formations.
The MOST model underlying the OFA has been extended to enable the "TellMe" protocol.
Firstly, a moving object should maintain a local trace of its positions and update thresholds to be able to communicate its motion plan to win followers.
Definition1.
An Object Position Trace OPT(O) of a moving object O at time t, defines a polyline in three dimensional space (X,Y, Time) represented as a sequence of points (xi,yi,i), (xi+1, yi+1, i+1),...,(xt,yt,t).
Proceedings of the 12th International Symposium on Temporal Representation and Reasoning (TIME'05) 1530-1311/05 $20.00 (c) 2005 IEEE  Denote the object's tracked history length by j, j:1l+, then i:(t-j)lt Definition2.
An Object Threshold Trace OTT(O) of a moving object O at time t is represented as a sequence of values :Kti , Kti+1 ,..,Ktq-1.
Threshold Kti, i:0lq-1,where q is the number of update points over [t-j,t[ and j is the object's tracked history length where j:1l+.
Secondly, the moving object should continuously evaluate other objects as followers.
Accepted objects are anticipated group candidates that are maintained in a Candidates List which is dynamically updated.
Definition3.
The Candidates List CL(O) of a moving object O at time t, represents anticipated followers as a sequence of pair values: (Oj , uj), ( Oj+1 , uj+1),.., ( On-1, un-1), where Oj , j:0ln-1,denotes the anticipated follower (Candidate) object id, and n, the number of O's potential candidates at t. uj,,j:0ln-1 denotes the attachment time shift of anticipated follower (Candidate) j from object O(leader).
Thirdly, the moving object should be able to formulate four types of location update messages, namely: Standard Update, Leader Advertisement, Member Advertisement and Leader Update Messages.
Initially, the moving object should utilize the Standard Update to communicate its position to the server and other objects (through message broadcast).
Definition4.
A Standard Update Message MS(O) of a moving object O, broadcasted to the location server and other moving objects, reports its position attribute (xt,,yt), speed (vt) and Threshold (Kt) at update time t. Afterwards, the object begins receiving Leader Advertisement messages, constituting membership proposals.
The message encompasses a trajectory proposal (extracted from the sender's position trace), and a proposed time shift through which the sender acknowledge the server of the shift parameter to enable the prediction scheme.
It is worth noting that a Leader Advertisement message received by a follower constitutes an update rather than a membership proposal, since it is already a follower to the sender.
Definition5.
The Followers List FL(O) of a moving object O at time t, represents actual current followers of O as a sequence of pair values: ( Oi , ui), ( Oi+1 , ui+1),.., ( On-1,, un-1), where Oi,i:0ln-1,denotes the follower object id, ui,,i:0ln-1 denotes the attachment time shift of follower i from the leader O and n denotes the number of O followers at t. Definition6.
A Leader Advertisement Update Message MLA(O) of a moving object O at t, Multicast to the location server and objects in CL(O) and FL(O), reports its speed vt, trajectory TRt(O)L OPT[t-u,t], Thresholds THt(O)LOTT[t-u,t], and Candidates CDt(O)L  CLt, where the time shift u >= ui, i:0ln-1,ui: -l+  and n is the number of candidate and follower objects.
If the values included in the leader's advertisement message prove to be better than linear prediction, a non-follower object sends a Member Advertisement message, communicating its leader id and its position attribute, the message constitutes an approval to the leader's proposal of membership.
The Leader, hearing the message, attaches it to its Followers List, while other objects remove it from their Candidates Lists.
Definition7.
A Member Advertisement Update Message MMA(O) of a moving object O, broadcasted to the location server and other moving objects, reports its position attribute (xt,,yt), speed (vt), Threshold (Kt) and Leader id LIDt(O) at update time t. Finally, the moving object receives Leader Update Messages containing a new trajectory, and utilizes the provided trajectory to significantly decrease the deviation cost by building a local prediction trajectory that it utilizes for prediction instead of the mathematical linear prediction method.
Since the group formation involves no initial overheads, objects' detachments do not constitute a threat to the overall information cost, and an object can simply detach by sending a Standard Update Message.
Definition8.
An Object Prediction Trajectory OPrT(O) of a moving object O at time t, defines a polyline in three dimensional space (X,Y, Time) represented as a sequence of points: (xi, yi, i), (xi+1, yi+1, i+1) ,..., (xr, yr,r).
where i:(t+1)lr.
Denote O's leader's last update message time by t', then rL t'+u where u is O's attachment time shift.
And each (xi, yi) designates O's leader's position at time (i-u).
Definition9.
A Leader Update Message MLU(O) of a moving object O, Multicast to the location server and follower objects in FL(O), reports its speed vt,, trajectory TRt(O)L OPT[t-a,t] and threshold (Kt(O)) .
let ui denote the time shift of follower i relative to its leader O, and n denotes the total number of O followers, while T denotes the length of O's last update-to-update interval.
Thus a could be expressed as follows.
a=  {  T  u>T and u> ui , i :0ln-1 (1)  u u<T and u> ui , i :0ln-1 Table 1. lists the main/basic routine running at each moving object.
A leader could receive Leader advertisement messages, as indicated in lines 7 through 17.
These messages advertise leaders' positions and thresholds up to the maximum potential candidate shift from the leader's update time.
Proceedings of the 12th International Symposium on Temporal Representation and Reasoning (TIME'05) 1530-1311/05 $20.00 (c) 2005 IEEE  Table 1.
Moving Object Basic Routine 1 Begin 2 while (true) 3 Begin 4 Sender=Receive(UpdateMess) 5 If(MyState not "Follower") 6 Begin 7 If (UpdateMess type is Lead_Adver) 8 Begin 9 Cand=UpdateMess.
CDt 10 If(Cand Contains(Me)) 11 Begin 12 Accepted=Eval_Proposal 13 (Cand[Me].
u, UpdateMess.
TRt) 14 If(Accepted) 15 Accept_Proposal(UpdateMess,Cand) 16 End 17 End 18 Else 19 Begin 20 My_Cand_List=CLt 21 If(UpdateMess type is Member_Adver) 22 If(My_Cand_List contains Sender)  23 Remove(Sender)  24 Else //standard message received  25 Begin 26 If(My_Cand_List not contain(Sender)) 27 Begin  28 Potential_Cand=Eval_Member  29 (UpdateMess.Position)  30 If(Potential_Cand)  31 Add_To_Cand_List(Sender, u)  32 End  33 Else 34 Update_Shift  ( 35 My_Cand_List[Sender])  36 End  37 End  38 End 39 Else //I'm a follower  40 Begin 41 Update_Prediction_Trajectory  42 (UpdateMess.Trajectory, CurrentTime)  43 End  44 Send=Compute_Dev_And_Act  45 (Curr_state, Pred_state)  46 End   47   End  Table 2 lists the leader advertisement evaluation routine through which an object evaluates a leader  proposal by comparing the deviation of the linearly predicted position ("PredState") and the proposed position ("Trajectory[Shift]").
As listed in Table 3, once a Leader is approved by an object, the object state is altered to be "follower" and it sends a Member Advertisement message declaring the proposal acceptance, then it caches the leader's trajectory ("Cash_My_Trajectory") and clears its candidates list.
Denote the current time by t and proposed shift by u, then the object copies the trajectory's data from t down through to t-u.
As indicated in lines 20 through 23 in Table 1, if the object received a member advertisement message, it should remove this member from its candidates list, as it designates a declaration of fellowship to another object/leader.
This action excludes this candidate from the object's next leader advertisement message.
An object receiving a standard update message [Lines 24 through 36 of Table1] should evaluate the sender as a potential candidate.
Table 2.
Eval_Proposal Routine 1 Eval_Proposal (Shift, Trajectory) 2 Begin 3 Dev=Check_Dev(CurrState,PredState) 4 Dev1=Check_Dev(CurrState,Trajectory 5 [Shift]) 6 If(Dev1<Dev) 7 return true 8 Else 9 return false 10 End Table 3.
Accept_Proposal Routine 1 Accept_Proposal(UpdateMess, Cand) 2 Begin 3 SendUpdate("Member_Adver") 4 Cash_My_Trajectory(Cand[Me].Shift, 5 UpdateMess.
TRt) 6 Clear_Candidates_List() 7 MyState="Follower" 8 End Table 4 lists the membership evaluation routine, where the object looks up the sender's position in its local position trace.
This method guarantees that the sender's behavior follows the leader, since it is subset of the leader's past experience stored in its Position Trace.
An object receiving a standard update from a candidate in the candidates list, should update the shift  Proceedings of the 12th International Symposium on Temporal Representation and Reasoning (TIME'05) 1530-1311/05 $20.00 (c) 2005 IEEE  as indicated in Lines 33 through 35 of table 1.
On the other hand, a follower should update its local trajectory, upon the inception of a leader update message as indicated in lines 40 through 43 in table 1.
Finally, tables 5 and 6 detail the ongoing deviation computation routine that identifies the type of message to send if the deviation threshold was exceeded.
Table 4.
Eval_Member Routine 1 Eval_Member (SenderPos) 2 Begin 3 MinDev=Value //user defined  4 Flag=false  Trace (OPT) 5 for each time ti in Position 6 Begin  7 Dev=Check_Dev(SenderPos,OPT[t i])  8 If(Dev<MinDev)  9 Begin 10 MinDev=Dev   11 Flag=true  i 12 u =CurrentTime-t 13 End  14 End  15 Return Flag 16 End  Table 5.
SendUpdate Routine 1 SendUpdate(Type) 2 Begin 3 If( Type is "Leader_Update") 4 Send(LeaderUpdateMessage) 5 Else 6 If( Type is "Member_Adver") 7 Send(MemberAdvertisement) 8 Else 9 If(Type is "Lead_Adver) 10 Begin 11 Send(LeaderAdvertisement) 12 Clear_Candidates_List() 13 End 14 Else 15 Begin 16 Send(StandardUpdateMessage) 17 MyState= "Leader" 18 End 19 End  Table 6.
Compute_Dev_And_Act Routine 1 Compute_Dev_And_Act(CurrState, PredState) 2 Begin 3 Send=Check_Dev(CurrState,PredState); 4 If(Send) 5 Begin 6 If( Cand_List is not empty) 7 SendUpdate("Lead_Adver")  8 Else  9 If(Followers_List is not empty)  10 SendUpdate("Leader_Update") 11 Else  12 SendUpdate("Standard")  13 End  14 End Moving Object States: Noticeably, three main moving objects' states, namely: Single, Leader and Follower, derive the messages exchange scenarios of the proposed "TellMe" protocol.
As depicted in the state transition diagram of figure 1, a single object O alters its state from "Single" to "Leader" upon the reception of a Member Advertisement update message (MMA(Oi)) from a "Single" moving object Oi confirming the acceptance of O's membership proposal.
Alternatively, an empty Followers List (FL(O)) at a leader O, causes state alteration to "Single".
An object O switches to a "Follower" state upon sending a Member Advertisement update message (MMA(Oi)), whereas sending a Standard Update messages switches a "Follower" object to the "Single" state.
Figure 1.
Moving Object States  Proceedings of the 12th International Symposium on Temporal Representation and Reasoning (TIME'05) 1530-1311/05 $20.00 (c) 2005 IEEE  The protocol diagram of figure 2 demonstrates the messages exchange scenarios of the "TellMe" protocol.
The diagram denotes single and leader objects as "Free Objects", while the location server and follower objects are represented as separate entities.
moving Object O, with Prediction Trajectory OPrT [t1,t2], its predicted position at t is as follows.
P(O)=  {  OPrT(t)+I  t [?]
[t1,t2]  P(O)+vtg  t [?]
[t1,t2], g=t-t2  (2)  I denotes the attachment deviation error and g denotes the time elapsed since updating the position by OPrT(t2).
The "TellMe" prediction scheme utilizes the local prediction trajectory for prediction, but if the object consumed all the prediction trajectory entries ([t1,t2]) before being updated by new entries, the object should utilize linear or other mathematical prediction function to extrapolate the position after t2.
It is evident that as the attachment time shift u increases the prediction trajectory entries increase, which improves the ongoing prediction process.
Meanwhile, increased u entails more chances of environmental changes which deviates the follower from its leader.
Nonetheless, u does not affect the length of the leader update message, since it is bounded to the length of the leader's update-to-update interval, whereas it has an insignificant effect on the initial Leader Advertisement message since it is sent only once.
5.
Performance Evaluation Figure 2.
The TellMe protocol  4.2.
"TellMe": The Prediction Scheme Consider a moving object O, moving at speed vt and is at position (xt,yt) at time t. Assume that O's current deviation threshold is Kt.
Accordingly, O should send a location update if and only if its current deviation exceeds Kt as dictated by the Adaptive Dead Reckoning Policy (ADR) ([1], [2]).
O's current deviation is computed by differencing O's current position (xt,yt) and its predicted position (xp(t),yp(t)), which is obtained using a linear predictor that utilizes the current speed vt and time elapsed T since the last update to estimate the traveled distance (vt*T) since the last update time (t-T) ([1], [2]).
Other nonlinear prediction functions can be employed for the OFA to provide better prediction.
The "TellMe" prediction scheme utilizes the model introduced by the "TellMe" protocol.
It makes use of the moving object's local Prediction Trajectories to accurately predict the current object location.
For a  Dr. Brinkhoff's Framework for generating NetworkBased moving objects has been utilized for generating the required spatiotemporal data [11], [12].
The framework generates objects moving on road networks and exposes them to the influence of external events, and time-scheduled traffic.
Moreover it considers important aspects of movement in a network including the maximum speed and the maximum capacity of the connections resulting in a data that combines the features of both real and statistically generated data.
A model-policy simulator was built on top of the framework to process the generated data and evaluate the various location update policies and models.
The model-policy simulator has been utilized to compare the proposed protocol and prediction scheme, namely: "TellMe", and the "One For All"(OFA) as well as the Adaptive Dead Reckoning Policy (ADR).
The policies are evaluated based on the average total information cost of the objects' trips.
The cost is expressed in terms of the total update, deviation and uncertainty costs introduced in [1] and [2].
It should be noted that in order to test the OFA or the proposed "TellMe" protocol, the objects should have tendency to move in  Proceedings of the 12th International Symposium on Temporal Representation and Reasoning (TIME'05) 1530-1311/05 $20.00 (c) 2005 IEEE  groups, accordingly the generator behavior has been modified to expose the required group mobility patterns.
Figures 3 through 6 demonstrate the results of two different experiments.
Note that the experiments did not include the OFA's control messages' cost, although it penalizes the OFA's information cost as a result of numerous attachments and de-attachments.
The reason is that it was required to compare the policies based on the cost savings realized for the moving objects during their trips, and being an overhead of the location server rather than the objects, the control messages are not included in the figures 3 through 6.
Adr  TellMe  OFA  Total Information Cost  60000 50000 40000 30000 20000 10000 0 1  3  5  7  9  11 13 15 17 19 21 23 25 27 29 Message Cost  Figure 3.
Total Information Cost versus the Message Update Cost of Experiment 1 Adr  14000  TellMe  OFA  TellMe  OFA  40000  Total Information Cost  Total Deviation Cost  Adr  45000  12000  35000 30000  10000 8000  25000 20000  6000  15000 10000  4000 2000 0  5000 0  1  3  5  7  9  11 13 15 17 19 21 23 25 27 29 Message Cost  Figure 4.
Total Deviation Cost versus the Message Update Cost of Experiment 1 Adr  18000  TellMe  14000 12000 10000 8000 6000 4000 2000 0 1  3  5  7  9  1  3  5  7  9 11 13 15 17 19 21 23 25 27 29 M essage Cost  Figure 6.
Total Information Cost versus the Message Update Cost Of Experiment 2  OFA  16000 Total Update Cost  The first experiment considered five objects for 400 time units using an uncertainty relative cost of 0.75, measuring the total information cost, against different message update relative costs.
The relativity is to the deviation cost which is considered 1.
The generator's speed divisor was set to 50 to generate objects that are neither too fast, nor too slow.
Simulation results proved that the "TellMe" protocol and prediction scheme reduces the total information cost and its components beyond the cost savings introduced by the OFA.
It should be noted that the OFA experienced several detachments as a result of its adopted protocol and prediction scheme, and that accounts for its cost increase versus the ADR.
In fact in the absence of detachments and with objects that exhibits higher identicality, the OFA recorded significant cost savings [3].
Though subjected to the same conditions, the "TellMe" survived the challenge and recorded a cost decrease of 40% in contrast to the OFA that resulted in a maximum of 18% decrease in the total information cost.
Figure 4 demonstrates that the "TellMe" decreased the deviation cost by 66% while the OFA decreased it by a maximum of 27%.
Concerning the update cost, and as demonstrated in figure 5, the "TellMe" decreased the cost by 80% while the OFA decreased it by a maximum of 32%.
11 13 15 17 19 21 23 25 27 29 Message Cost  Figure 5.
Total Update Cost versus the Message Update Cost of Experiment 1  The second experiment considered the same settings but the speed divisor was set to 250 to generate slow moving objects.
The generated objects exhibited movement patterns with a slow rate of change resulting in a small number of updates as a result of the slow deviation patterns.
As evident in figure 6 the "TellMe" approached the OFA cost savings, since the linear prediction was then used to extrapolate the object's positions due to the lack of entries in the Prediction Trajectory as a result of the seldom location updates.
Moreover the OFA exposed better results as a result of the decreased objects dynamicality.
Finally, it should  Proceedings of the 12th International Symposium on Temporal Representation and Reasoning (TIME'05) 1530-1311/05 $20.00 (c) 2005 IEEE  be noted that the cost savings are a function to a great extent in the variance of the objects' behavior within the groups.
6.
Conclusions and Future Work Moving Objects Databases (MOD) possesses several challenges, particularly for modeling and updating location information.
In this paper we have proposed a protocol and prediction scheme as an extension to the "One For All"(OFA) framework introduced in [3].
The OFA considered mobility groups to realize significant cost savings per individual moving object, nonetheless the novelty of the OFA framework exposed numerous challenges.
The proposed "TellMe" protocol and prediction scheme opts for addressing the challenges of cluster formation and managing objects of high dynamicality.
The proposal migrated the group formation protocol to a de-centralized approach through an enhanced dialogue among the objects.
Moreover, it enabled the group leader to communicate its past experience, to enhance the prediction of its followers, so the leader is "Telling" the objects what happened which significantly reduces the overall costs as proven by experimentation.
New challenges are added up by the introduction of the "TellMe" protocol and prediction scheme.
One of the main challenges is to enable moving objects within a group to develop a sense of ongoing membership evaluation versus the individual behavior.
The concept is vital to discover false memberships that may escalate the information cost and result in a definite detachment.
Even if an object exhibited a true fellowship to its leader, it should be allowed to switch to the singular mode as needed to maneuver the environmental challenges that may give a false indication of its fellowship.
Although the "TellMe" diminishes the control messages' overheads that penalized the group formation process, the mode switching should not restart the group membership evaluations, instead it should be seamless and transparent to the main three tiers involved, namely: the object, the location server and the leader.
The constraint is crucial to enable the moving object to switch its mode at will and whenever needed with no considerations or overheads.
Moreover, moving objects with hesitant movement patterns should not be considered in leaders' advertisement messages, accordingly a crucial challenge is to discover these hesitant objects and exclude them from the advertisements.
Another challenge is quantifying the maximum optimum attachment time shift that exploits the protocol and scheme's strength, since a small shift, entails less prediction points, and a big one, threats the  objects' motion similarity.
Furthermore, quantifying the optimum length of the positions and thresholds traces maintained at the objects exposes tangible benefits to the flexibility and practicability of the whole framework.
7.
References [1] O. Wolfson, P. Sistla, S. Chamerlain, Y. Yesha, "Updating and Querying Databases that Track Mobile Units", Distributed and Parallel Databases, 7(3), 1999, pp.
257387.
[2] O. Wolfson, B. Xu, S. Chamberlain, L. Jiang, "Moving Object Databases: Issues and Solutions", Proc.
10th International Conf.
on Scientific and Statistical Database Management, Capri, Italy, 1998, pp.
111-122.
[3] O. Karam, A.Nahas, A.Hamad, I.Ramzy, "One For All: A new perspective for Modeling and Updating Location Information in Moving Objects Databases", The 2nd International Conference on Communication Systems and Networks, Spain, Sept 2003, pp.
175-180.
[4] P. Sistla, O. Wolfson, S. Chamberlain, S. Dao, "Modeling and Querying Moving Objects", Proc.
13th International Conf.
on Data Engineering, Birmingham, UK, 1997, pp.
422-432.
[5] O. Wolfson, S. Chamberlain, S.Dao, L. Jiang, G. Mendez, "Cost and Imprecision in Modeling the Position of Moving Objects", Proc.
14th International Conf.
on Data Engineering, Orlando, FL, 1998, pp.
588-596.
[6] O. Wolfson, "Moving Objects Information Management: The Database", Proc.
5th Workshop on Next Generation Information Technologies and Systems, Caesarea, Israel, 2002, pp.
75-89.
[7] G. Trajcevski, O. Wolfson, F. Zhang, S. Chamberlain, "The Geometry of Uncertainty in Moving Objects Databases", Proc.
8th International Conf.
on Extending Database Technology, Berlin, Germany, 2002, pp.
233-250.
[8] G. Trajcevski, O. Wolfson, C. H. Lin, F. Zhang, N. Rishe, "Managing Uncertain Trajectories of Moving Objects With DOMINO", Proc.
4th International Conf.
on Enterprise Information Systems, Spain, 2002, pp.
217-224 [9] G. Trajcevski, O. Wolson, B. Xu, P. Nelson, "Real-Time Traffic Updates in Moving Objects Databases", Proc.
13th IEEE International Workshop on Database and Expert Systems Applications, 2002, pp.1529-4188/02.
[10] Ritmantas Benetis, Christian S. Jensen, Gytis Karciauskas, Simonas Saltenis, "Nearest Neighbour and Reverse Nearest Neighbour Queries for Moving Objects", TimeCenter Technical Report, 2001.
[11] T. Brinkhoff, "A Framework for generating networkbased moving objects", Geoinformatics, 6(2), 2002, pp.
153180.
[12] T. Brinkhoff, "Generating Network-Based Moving Objects", Proc.
12th International Conf.
on Scientific and Statistical Database Management, Berlin, Germany, 2000, pp.
253-255  Proceedings of the 12th International Symposium on Temporal Representation and Reasoning (TIME'05) 1530-1311/05 $20.00 (c) 2005 IEEE
Decidability of the Logics of the Reflexive Sub-interval and Super-interval Relations over Finite Linear Orders Angelo Montanari Dept.
of Mathematics and Computer Science University of Udine, Italy angelo.montanari@uniud.it  Ian Pratt-Hartmann School of Computer Science Manchester University, UK ipratt@cs.man.ac.uk  AbstractaAn interval temporal logic is a propositional, multimodal logic interpreted over interval structures of partial orders.
The semantics of each modal operator are given in the standard way with respect to one of the natural accessibility relations defined on such interval structures.
In this paper, we consider the modal operators based on the (reflexive) subinterval relation and the (reflexive) super-interval relation.
We show that the satisfiability problems for the interval temporal logics featuring either or both of these modalities, interpreted over interval structures of finite linear orders, are all PS PACEcomplete.
These results fill a gap in the known complexity results for interval temporal logics.
Keywords-interval temporal logic; decidability; computational complexity  I. I NTRODUCTION Interval temporal logics provide a natural framework for representing and reasoning about temporal properties in many areas of computer science, including formal specification and verification of reactive systems, temporal databases, knowledge representation, and natural language semantics [1].
For many years, the discouragingly high computational complexity of these logics impeded their systematic investigation.
(As an example, Halpern and Shohamas modal logic of time interval HS [2] and Venemaas CDT logic [3] are highly undecidable.)
Recently, however, the discovery of expressive decidable fragments of HS has generated renewed interest in this subject.
The most significant of these fragments are the logics of temporal neighbourhood [4] and the logics of sub-interval and super-interval structures [5].
Decidability of temporal neighbourhood logics over various classes of linear orders can be proved by reducing their satisfiability problems to that of the two-variable fragment of first-order logic over the same classes of linear orders [6].
In fact, neighbourhood temporal logic turns out to be a maximal decidable fragment of HS, when interpreted over any class of linear orders that contains at least one linear order with an infinitely ascending/descending sequence [4], [7], [8].
In this paper, we focus on interval logics of subinterval and super-interval structures.
There are three natural definitions of the sub-interval  Pietro Sala Dept.
of Mathematics and Computer Science University of Verona, Italy pietro.sala@univr.it  relation [5]: reflexive  (the current interval is a sub-interval of itself), proper  (sub-intervals share at most one endpoint with the current interval), and strict  AV (both endpoints of the sub-intervals are strictly inside the current interval).
And interval temporal logics based on all of these versions of the sub-interval relationainterpreted over the class of dense linear ordersahave been studied in the literature.
Thus, the logic D of reflexive sub-intervals is studied by van Benthem in [9], where it is proved to be equivalent to the standard modal logic S4.
In [10], Shapirovsky and Shehtman establish a connection between the logic of strict sub-intervals DAV and the logic of Minkowski space-time.
They provide a sound and complete axiomatic system for DAV ; moreover, they prove PS PACE-completeness of DAV by means of a suitable filtration technique [10], [11].
An optimal tableau system for DAV is given by Bresolin et al.
in [5].
Finally, the logic D is treated extensively in [5].
The authors prove decidability and PS PACE-completeness of D , and provide an optimal tableau system.
All of the above results concern logics interpreted over dense linear orders; and the question naturally arises as to what happens if we consider classes of discrete orders instead.
In this paper, we go some way to answering this question.
Specifically, we study the satisfiability problem of interval temporal logics based on the reflexive sub-interval and super-interval relations interpreted over finite linear orders.
The paper is organized as follows.
In Section II, we give the syntax and semantics of three propositional modal languages: LD , featuring the modal operator [D], LDE , featuring the modal operator [DE], and LD,DE , featuring both operators.
Informally, we read [D]D as aD is true at all subintervals of the current intervala, and [DE]D as aD is true at all super-intervals of the current intervala.
We interpret all three languages over the class of finite linear orders.
In Section III, we prove PS PACE-hardness of the satisfiability problem for LD and LDE , by reduction from the satisfiability problem for quantified Boolean formulas.
In Section IV, we prove that the satisfiability problem for LD is in PS PACE.
In Section V, we generalize this latter result to LDDE .
It can easily be  1, m  m, m i, j  so @ Rsq , q p1 , p s	 @ 1 ?
?
s q2 , q p2 , p s  j, j  [i, j] i, i i  j Figure 2.
A relational frame satisfying (1)a(10).
1, 1 Figure 1.
Geometrical depiction of intervals.
shown that the proposed techniques cannot be directly lifted to the strict and proper cases.
Moreover, the undecidability of various extensions of the logics of sub-interval and superinterval structures has been recently proved [4], [7], [12].
II.
P RELIMINARIES Fix a set P of proposition letters, and denote its power set by P(P).
The language LD is defined to be the smallest set of expressions satisfying: (i) P a LD ; (ii) if D, D a LD , then D aSS D, AZD, [D]D a LD .
The language LDE is defined similarly, but with [DE] in place of [D]; the language LDDE contains both modalities.
In the following, we employ the usual abbreviations D a" D (for AZ(AZD aSS AZD)), DD (for AZ[D]AZD), and DED (for AZ[DE]AZD).
We denote by 	D	 the total number of symbols occurring in D. For the purposes of this paper, an interval is a set of the form {h a N | i  h  j}, where i, j are positive integers with i  j; we write [i, j] to denote this interval.
Note that intervals are finite and non-empty, but may be singletons.
If I is an interval, we write Sub(I) to denote the set of intervals J such that J a I.
A structure is a pair A = (IA , AVA ), where IA = [1, m] for some m fi 1, and AVA is a function from Sub(IA ) to P(P).
We refer to IA as the domain of A, m as the size of A, and AVA as the interpretation function of A.
We define a truth-relation for LDDE -formulas (hence for LD -formulas and LDE -formulas), relative to structures A and intervals I a IA , as follows: (i) for p a P, A |=I p if and only if p a I A ; (ii) the usual rules for aSS and AZ; (iii) A |=I [D]D if and only if, for all intervals J a I, A |=J D; (iv) A |=I [DE]D if and only if, for all intervals J a Sub(IA ) such that J a I, A |=J D. If IS is a set of formulas, we write A |=I IS if A |=I D for all D a IS.
If A |=I D for some I a Sub(IA ), we say that A is a model of D. If D has a model, then D is satisfiable; if AZD has no model, then D is valid.
We denote the set of valid LD -formulas by D. If I = [1, m], it is useful to depict Sub(I) as the upper left-hand half of an m A m integer grid, as shown in Fig.
1,  where each interval [i, j] a Sub(I) corresponds to the cell in the grid having co-ordinates i, j.
In this representation, all intervals sharing a common end-point j lie in the jth row (counting from the bottom upwards), all intervals sharing a common start-point i lie in the ith column (counting leftto-right), and all singleton intervals [i, i] lie on the diagonal section of the boundary.
Motivated by this representation, if A is a structure of size m, and 1  j  m, then we refer to the set of intervals {[i, j] | for some i (1  i  j)} as the jth row of A, and to the interval [i, j] as the ith cell in that row.
We refer to the mth row of A as the top row.
Evidently, we may identify LD with the language of propositional modal logic, where [D] replaces the modal operator ; and it is natural to ask how D is related, under this identification, to the modal logics of various familiar frame-classes.
Thus, for example, it is easy to show that every satisfiable LD -formula (using the semantics for LD given above) has a relational (Kripke) model over a finite, reflexive, transitive tree.
Since the logic S4Grz is known to be sound and complete for the class of finite, reflexive, transitive trees [13, p. 101], we have S4Grz a D. However, a little thought shows that this inclusion is strict.
For consider the following set of LD -formulas featuring the proposition letters o, p, q, p1 , p2 , q1 and q2 : o  (1)  [D](o a" p1 a" p2 a" q1 a" q2 ) [D](o a D(p1 aSS AZo) aSS D(q1 aSS AZo))  (2) (3)  [D](p1 a D(p2 aSS AZp1 )) [D](q1 a D(q2 aSS AZq1 ))  (4) (5)  [D](p1 a" p2 a p) [D](q1 a" q2 a q) [D]AZ(p aSS q)  (6) (7) (8)  [D](p a [D]p) [D](q a [D]q).
(9) (10)  The corresponding -formulas are easily seen to be satisfied at the root of the reflexive, transitive tree with two depth-3 branches illustrated in Fig.
2 (where only the true proposition letters are indicated); hence these formulas are S4Grzconsistent.
Now suppose (1)a(10) are true at an interval I in some structure A (under LD -semantics).
From (3)a(5), every sub-  interval of I satisfying o has length at least 3, and every sub-interval of I satisfying p1 or q1 , length at least 2.
Hence, by (2), all unit-length sub-intervals of I satisfy either p2 or q2 , and by (6)a(8), no such interval satisfies both.
By (1), (3)a(7) and (9)a(10), there exists at least one unit-length subinterval of I satisfying p2 and similarly for q2 .
Therefore, we can find adjacent unit-length intervals, say, Jp satisfying p2 and Jq , satisfying q2 .
Now consider the length-2 interval J which includes Jp and Jq .
Since J is of length less than 3, J cannot satisfy o; since Jp satisfies p by (6), J cannot satisfy q1 or q2 by (7)a(8) and (10); likewise, J cannot satisfy p1 or p2 by (6) and (8)a(9).
This falsifies (2).
q0  I, = Q1 p1 .
.
.
Qn pn D,  (11)  where D is a formula of propositional logic and, for all i (1  i  n), Qi is either a or a.
Given a truth-value assignment to its free variables, I, is assigned a truth-value in the obvious way; in particular, if I, is closed (has no free variables), it is either true or false simpliciter.
The problem of determining the truth-value of closed quantified Boolean formulas was shown to be PS PACE-complete in [14].
Theorem 1.
The satisfiability problems for LD -formulas and LDE -formulas over finite linear orders are both PS PACEhard.
Proof: By (logarithmic space) reduction from the satisfiability problem for quantified Boolean formulas.
Let the quantified Boolean formula I,, of the form (11), be given, where D is a formula of propositional logic featuring only the proposition letters p1 , .
.
.
, pn .
We construct an LD formula I,E, and show that I,E is satisfiable over finite linear orders if and only if I, is true.
For all i (0  i  n), define I,i to be the formula Qi+1 pi+1 .
.
.
Qn pn D, with free variables p1 , .
.
.
, pi .
Thus, I,0 = I, and I,n = D. The LD -formula I,E will feature the proposition letters p1 , .
.
.
, pn together with additional proposition letters q0 , .
.
.
, qn .
We define I,E to be the conjunction of the following set of formulas: q0 (1  j  i < k  n) [D](qi aSS pj a [D](qk a pj )) [D](qi aSS AZpj a [D](qk a AZpj )) (1  j  i < k  n) (0  i < n) [D](qi a fiDqi+1 ) (0  i < n, Qi+1 = a) [D](qi a fiD(qi+1 aSS pi+1 ) [D](qi a fiD(qi+1 aSS AZpi+1 ) (0  i < n, Qi+1 = a)  (12) (13) (14) (15) (16) (17)  q3 , p 3  q2 ,AZp2  q4 ,AZp4 q3 ,AZp3 q4 , p 4  q1 ,AZp1 q2 , p 2  q3 , p 3  q3 ,AZp3 q4 , p 4  p3 /0  AZp1 , p2 , AZp3  a  p1 /1  AZp1  AZp1 , p2  p4 /1  Figure 3.  p1 /0  p2 /1  q4 ,AZp4  III.
L OWER B OUND The task of this section is to establish the PS PACEhardness of the satisfiability problems for D and D over finite linear orders.
The argument is straightforward, and proceeds by reduction from the satisfiability problem for quantified Boolean formulas, the canonical PS PACE-hard problem.
Recall, in this context, that a quantified Boolean formula is an expression of the form  q1 , p 1  p3 /1  AZp1 , p2 , p3  p4 /0  p1  p3 /0  p2 /0 p1 , AZp2  p1 , AZp2 , AZp3  p4 /1  p3 /1  p1 , AZp2 , p3  p4 /0  AZp1 , p2 ,  AZp1 , p2 ,  p1 , AZp2 ,  p1 , AZp2 ,  AZp3 , p4  p3 , AZp4  AZp3 , p4  p3 , AZp4  A tree-model and its embedding into a grid structure.
[D](qn a D)  (18)  It is routine to check that the construction of I,E requires only space bounded by a logarithmic function of 	I,	.
Suppose A |=I0 I,E.
We think of the truth of a proposition letter qi (0  i < n) at an interval as indicating that the quantifier Qi+1 is being aconsidereda.
Formula (12) starts the process by considering Q1 .
Now, suppose A |=I qi , where 0  i < n and I a I0 .
By Formulas (13) and (14), every sub-interval of I satisfying qk (k > i) must agree with I on the truth-values assigned to p1 , .
.
.
, pi ; thus these proposition letters may be regarded as fixed for all such intervals.
If Qi+1 = a, then, by Formulas (15), I has a sub-interval J satisfying qi+1 (at which pi+1 must be either true or false); on the other hand, if Qi+1 = a, then, by Formulas (16) and (17), I has two sub-intervals satisfying qi+1 : one in which pi+1 is true, and one in which pi+1 is false.
Finally, Formula (18) ensures that, if I a I0 and A |=I qn , then D is true at I.
Now, any I a I0 such that A |=I qi defines a natural truth-value assignment I"I with domain {p1 , .
.
.
, pi } obtained by taking I"(pj ) =  just in case A |=I pj , for all j (1  j  i).
Using a (backwards) inductive argument, we claim that, if I a I0 and A |=I qi , then I"I |= I,i .
For i = n, the claim is guaranteed by Formula (18).
Assuming the result holds for i (0 < i  n) Formulas (13), (14), (16) and (17) guarantee that it holds for i a 1 if Qi = a, and Formulas (13)a(15) do the same if Qi = a.
Formula (12) then guarantees that I, is true.
Conversely, suppose I, is true.
We define a tree tr(I,) as follows.
Each node I" of tr(I,) will be a truth-value assignment with domain {p1 , .
.
.
, pi } for some i (0  i  n), with the property that I" |= I,i ; we call i the level of I".
We take the root node I"0 of tr(I,) to be the empty assignment (i.e., the unique assignment with level 0).
Since I, = I,0 is a true formula with no free variables, we have I"0 |= I,0 .
Suppose now that I" is a node of tr(I,), with level i  (0  i < n), such that I" |= I,i .
If I,i = api+1 I,i+1 , let I"  be an assignment with domain {p1 , .
.
.
, pi+1 } such that I" a I"  , and I"  |= I,i+1 ; we then take I"  to be the sole daughter of I" in tr(I,).
If, on the other hand, I,i = api+1 I,i+1 , let I"  = I" aS {pi+1 a }, I"  = I" aS {pi+1 a aL}, and take I"  and I"  to be the daughters of I" in tr(I,); evidently, I"  |= I,i+1 and I"  |= I,i+1 .
This completes the definition of tr(I,).
Note that, in particular, if I" is a node with level n, then I" |= I,n ; i.e., I" |= D. A straightforward induction on n shows that, setting m = 2n+1 , we may define an embedding f : tr(I,) a Sub([1, m]) in such a way that f (I") a f (I"  ) if and only if I" is a descendant of I"  in tr(I,).
This is illustrated by the lefthand diagram in Fig.
3, where a tree-model for the QBF formula I, = ap1 ap2 ap3 ap4 (p1 a" p2 ) aSS (AZp1 a" AZp2 ) aSS (p3 a" p4 ) aSS (AZp3 a" AZp4 ) is given, together with its embedding into a grid structure.
To show that I,E is satisfiable, define A with domain [1, m] as follows.
For all i (0  i  n), we set A |=I qi if and only if I = f (I") for some node I" of tr(I,) with level i.
Further, for each node I" with level i, and each j (1  j  i), we set A |=f (I") pj if and only if I"(pj ) = .
The truth-values of the other proposition letters may be assigned arbitrarily.
It is then obvious that A |=f (I"0 ) I,E.
An example of this construction is shown in the right-hand diagram of Fig.
3.
By replacing every occurrence of [D] with [DE], we similarly establish the PS PACE-hardness of LDE .
IV.
U PPER B OUND The task of this section is to establish the membership in PS PACE of the satisfiability problem for LD -formulas over the class of finite linear orders.
Fix an LD -formula D. Let IS be the set of all sub-formulas of D (including D itself), and let IS be the set of all formulas D or AZD, where D a IS .
A 1-type is a subset Ia a IS satisfying the properties: (T1) for all formulas AZD a IS, either D a Ia or AZD a Ia; (T2) Ia is propositionally consistent; (T3) if [D]D a Ia, then D a Ia.
Let A be a structure and I a Sub(IA ).
Evidently, there is a unique 1-type Ia such that A |=I Ia; we say that A realizes Ia at I.
For the purposes of this paper, a vector is a finite, nonempty sequence of positive integers.
The following notation will be used.
If wE = (w1 , .
.
.
, ws ) is a vector of length s, we write |wE| = s, and if m is a positive integer, we write mwE for the vector (mw1 , .
.
.
, mws ).
If vE = (v1 , .
.
.
, vs ) is a vector of the same length, we write wE  vE (equivalently: vE fi wE) if wi  vi for all i (1  i  s).
Less conventionally, we take corresponding upper-case letters to denote the successive partial sums of vectors, thus: Wi = w1 + AV AV AV + wi , for all i (0  i  s).
Note that, for any vector wE, W0 = 0.
Let A be a structure of size m, I1 , .
.
.
, Is , a selection of intervals in IA occurring left-to right across a single  wi  w1  Ia1 Figure 4.
AVAVAV  Ia1  AVAVAV  Iai  AVAVAV  ws  Iai  AVAVAV  Ias  AVAVAV  Ias  Geometrical depiction of the realization of a profile by a row.
row in the grid-representation of Sub(IA ), and Ia1 , .
.
.
, Ias the respective 1-types realized in A by these intervals.
In addition, suppose that the final cell in that row realizes Ias .
Evidently: (C1) if 1  h < h  s and [D]D a Iah , then [D]D a Iah ; (C2) if DD a Ias , then D a Ias .
Accordingly, we say that a configuration is a finite, nonempty sequence IaE = (Ia1 , .
.
.
, Ias ) of 1-types satisfying conditions (C1) and (C2).
Again, we denote the length of this sequence by |IaE|.
A profile is a pair IaE, wE, where IaE is a configuration of length s fi 1, and wE = (w1 , .
.
.
, ws ) a vector, also of length s; we say that the length of this profile is s. For 1  j  m, we say that the jth row of A realizes the profile (Ia1 , .
.
.
, Ias ), (w1 , .
.
.
, ws ) if Ws = j and, for all h (1  h  s) and all i (Wha1 < i  Wh ), A |=[i,j] Iah .
Pictorially, this means that the successive 1-types Ia1 , .
.
.
, Ias are realized horizontally in uniform blocks of length w1 , .
.
.
, ws , respectively, and that these blocks cover the entire row, as shown in Fig.
4.
For all j (1  j  m), the jth row of A obviously realizes at least one profile; however, since there is no requirement that neighbouring Iai are distinct, this profile will not in general be unique.
The following lemma shows that we may, without loss of generality, confine attention to ashorta profiles.
Lemma 2.
If an LD -formula D has a model, then it has a model over the same domain in which every row realizes a profile of length at most 	D	 + 1.
Proof: Fix a model A of D, of size m. If D a IS, a witness for D is an interval I a Sub(IA ) such that A |=I D. We call an interval I = [i, j] a Sub(IA ) special if there exists D a IS such that A |=I D and, for all k (i < k  j) A |=[k,j] Dathat is, if there is some formula of IS for which I is the right-most witness in its row.
Since every singleton interval, [j, j], is special, it follows that, for any I = [i, j] a Sub(IA ), there is a least k (i  k  j) such that K = [k, j] is special.
We refer to K as the first special interval to the right of I. Evidently, K = I if and only if I is itself special.
Moreover, since |IS|  2	D	, and the right-most interval of any row is the right-most witness for at least |IS|/2 of the formulas of IS in that row, it follows that the number of special intervals in any row is at most 	D	 + 1.
Now define the structure B, over the same domain as A, by setting, for all I a Sub(IA ), I B = K A , where K is the first special interval to the right of I.
We claim that, for all I a Sub(IA ) and all D a IS:     Vh Vh a1  v1 v2 .
.
.
vta1  -  -  vt  -  6 I,h    Uh Figure 5.
Iah -  Neighbouring rows of a structure  Vt a Us  I,1 I,2 .
.
.
I,ta1 ?  (S1) for all h (1  h  s) and all h (1  h  t), if [D]D a I,h and Uh > Vh a1 , then [D]D a Iah ; (S2) for all h (1  h  t), if DD a I,h , either there exists h (h  h  t) such that D a I,h , or there exists h (1  h  s) such that Uh fi Vh and DD a Iah ; (S3) Vta1  Us < Vt .
The following two lemmas motivate this definition.
Lemma 3.
Let A be a structure of size m, let 1  j < m, and let IaE, uE, I,E, vE be profiles realized by the jth and  -  A  B |=I D if and only if A |=K D, where K is the first special interval to the right of I.
We prove the claim by structural induction.
If D = p is a proposition letter, the claim is immediate by the construction of B; furthermore, the Boolean cases are trivial.
So suppose D = [D]I, (whence AZD, AZI, a IS).
Pick any I a Sub(IA ), and let K be the first special interval to the right of I.
Suppose first that B |=I D. Then B |=J I, for some J a I.
Let K  be the first special interval to the right of J, so that, by inductive hypothesis, A |=K  I,.
Since K  a J a I, we have A |=I AZ[D]I,; so let I  be the right-most witness for AZ[D]I, in the same row as I.
By definition, I  is special, and hence must lie (non-strictly) to the right of K; thus, I  a K. But then A |=K AZ[D]I,, i.e., A |=K D. Conversely, suppose A |=K D. Then A |=J AZI, for some J a K. So now let J  be the right-most witness for AZI, in the same row as J.
By definition, J  is special, and thus (trivially) the first special interval to the right of J  , whence, by inductive hypothesis, B |=J  AZI,.
But J  a J a K a I, so that B |=I D. This completes the induction, and establishes the claim.
To conclude the proof, suppose A |=I D, and suppose, without loss of generality, that I is the right-most witness for D in its row.
Thus, I is special, and, furthermore, is (trivially) the first special interval to the right of I. Constructing B as above, BI |= D. Finally, the claim shows that every row in B realizes a profile of length no greater than the number of special intervals in that row.
But we have already noted that this number is bounded by 	D	 + 1.
Now suppose IaE = (Ia1 , .
.
.
, Ias ) and I,E = (I,1 , .
.
.
, I,t ) are configurations.
A solution for IaE, I,E is a pair uE, vE, where uE is a vector of length s and vE a vector of length t, satisfying the following conditions:  Us  I,t  Figure 6.
The structure B of Lemma 4.
(j + 1)th rows of A, respectively.
Then uE, vE is a solution for IaE, I,E.
Proof: Numbering the cells of the jth row from left to right (starting with 1), we observe that, for all h (1  h  |IaE|), the cells in positions Uha1 + 1 to Uh all satisfy Iah ; similar remarks apply to row j +1.
Conditions (S1)a(S2) are then evident by inspection of Fig.
5, which depicts the two rows of A in question.
For condition (S3), note that Us = j, Vt = j + 1 and, by assumption, vt > 0.
Lemma 4.
Let IaE and I,E be configurations.
Suppose that uE, vE is a solution for IaE, I,E, and that A is a structure whose top row realizes the profile IaE, uE.
Then there exists a structure B whose top row realizes the profile I,E, vE.
Proof: Write I,E = (I,1 , .
.
.
, I,t ), and let s = |IaE|.
The structure B shown in Fig.
6 has the required properties.
A label I,h in a region indicates that all the intervals in that region make true exactly the proposition letters I,h aSP.
Condition (S3) guarantees that the dimensions of the rectangles labelled I,1 , .
.
.
, I,ta1 are such that the atrapeziuma labelled I,t exists.
In particular, each of the new rows (above the top row of A) contains a final block of cells in the region labelled I,t .
(This is important, because (I,1 , .
.
.
, I,h ) need not be a configuration for h < t.) Using Conditions (C1)a(C2) and (S1)a(S2), a routine structural induction establishes that, for all h (1  h  t), and any interval I lying in the region labelled I,h , B |=I I,h .
Thus, Lemma 3 states that configurations belonging to profiles realized by successive rows in a structure always have a solution, while Lemma 4 provides a partial converse: given a solution uE, vE for IaE, I,E, we can extend any structure whose top row realizes the profile IaE, uE to one whose top row realizes the profile I,E, vE (notice that reflexivity of the sub-interval relation plays an essential role in Lemma  4.3).
Indeed, the next lemma allows us to create, ex nihilo, a structure whose top-row realizes any profile of length 1.
Lemma 5.
Let Ia be a 1-type such that DD a Ia implies D a Ia, and let m be a positive integer.
Then there exists a structure A whose top row realizes the profile (Ia), (m).
Proof: Set IA = [1, m], and I A = Ia aS P for all I a Sub(IA ).
The following sequence of lemmas shows that, if IaE, I,E has one solution, then it has many.
Lemma 6.
Let IaE and I,E be configurations, and m a positive integer, and suppose that uE, vE is a solution for IaE, I,E.
Then muE, mvE is also a solution for IaE, I,E.
Proof: Satisfaction of inequalities x1 + AV AV AV + xh  y1 + AV AV AV + yk and x1 + AV AV AV + xh < y1 + AV AV AV + yk is preserved under multiplication by m. Lemma 7.
Let IaE and I,E be configurations, and suppose that uE, vE is a solution for IaE, I,E.
If uE is a vector satisfying uE  uE , then there exists a vector vE  satisfying vE  vE  such that uE , vE   is a solution for IaE, I,E.
Proof: It suffices to prove the lemma where uE = (u1 , .
.
.
, us ), vE = (v1 , .
.
.
, vt ), and uE = (u1 , .
.
.
, uia1 , ui + 1, ui+1 , .
.
.
, us ).
Since Vt > Us , pick the least j (1  j  t) such that Vj > Uia1 , and set vE  = (v1 , .
.
.
, vja1 , vj + 1, vj+1 , .
.
.
, vt ).
It is then obvious that, for all i (0  i  s) and all j (0  j  t), Ui > Vj if and only if Ui > Vj .
Lemma 8.
Let IaE and I,E be configurations, with |IaE| = s  m and |I,E| = t  n, such that IaE, I,E has a solution.
Then IaE, I,E has a solution uE, vE satisfying uE  (n, .
.
.
, n).
Indeed, it has a solution uE, vE satisfying uE  (n, .
.
.
, n) and vE  (m + 1, .
.
.
, m + 1).
Proof: Let uE, vE be a solution with uE = (u1 , .
.
.
, us ) and vE = (v1 , .
.
.
, vt ).
For the first statement of the lemma, suppose ui > n for some i (1  i  s).
We claim that there exists j (1  j  t) such that, writing uE = (u1 , .
.
.
, uia1 , ui a 1, ui+1 , .
.
.
, us ) and vE  = (v1 , .
.
.
, vja1 , vj a1, vj+1 , .
.
.
, vt ), uE , vE   is also a solution.
By repeated applications of this claim, we obtain the desired solution.
The claim is obvious if n = 1; so assume n > 1 (whence ui fi 3).
Now, since Vt > Us fi Usa1 + 1, let j  be the smallest positive integer such that Vj  > Uia1 + 1.
If vj  > 1, then, since ui fi 3, setting j = j  gives the required vE  .
So suppose vj  = 1, in which case, we also have j  fi 2 and indeed Vj  = Uia1 + 2.
Since, in addition, ui fi n + 1, there exists j (j  < j  t) such that vj > 1 and Vja1  Ui a 2.
Again, it is easy to check that this value of j gives the required vE  .
The second statement of the lemma follows by an almost identical argument.
Lemma 9.
Let IaE and I,E be configurations whose length  is bounded by a polynomial function of 	D	.
Then we can determine, using space likewise bounded by a polynomial function of 	D	, whether IaE, I,E has a solution.
Proof: Try all pairs of vectors uE, vE satisfying the bounds given in Lemma 8.
Lemma 10.
Let IaE and I,E be configurations, with |I,E|  n, such that IaE, I,E has a solution; and let m be a positive integer.
For any vector uE satisfying uE fi (mn, .
.
.
, mn), there exists a vector vE satisfying vE fi (m, .
.
.
, m), such that uE, vE is a solution for IaE, I,E.
Proof: By Lemma 8, let uE , vE   be a solution satisfying uE  (n, .
.
.
n).
By Lemma 6, muE , mvE   is a solution, with muE  uE, and (trivially) mvE  fi (m, .
.
.
, m).
By Lemma 7, there exists a vector vE with vE fi mvE  such that uE, vE is a solution.
Define the directed graph GD = (C, E) as follows:   C  = {IaE | IaE is a configuration s.t.
|IaE|  	D	 + 1}  E  = {IaE, I,E a C 2 | IaE, I,E has a solution}.
Since the number of 1-types is at most 2fiDfi , |C|  2n , where n = 	D	(	D	 + 1).
The main lemma of this section states necessary and sufficient conditions for D to be satisfiable.
Lemma 11.
Let D be an LD -formula, and let the graph GD = (C, E) be constructed as above.
Then D is satisfiable if and only if there is a path in GD from some configuration IaE of length 1 to some configuration I,E containing a 1-type I, such that D a I,.
Proof: Suppose first that A is a model of D. By Lemma 2, we may assume that each row of A realizes some profile (IaE, uE) of length at most 	D	 + 1, so that IaE a C. The first row necessarily realizes a profile of length 1; by Lemma 3, the configurations corresponding to successive rows are joined by an edge in E; and, since A is a model of D, some row of A realizes a profile in which D occurs somewhere.
That is: there is a path in GD from some configuration IaE of length 1 to some configuration I,E containing a 1-type I, such that D a I,.
Conversely, suppose there is a path IaE1 , .
.
.
, IaEz in GD where: (i) IaE1 = (Ia) is a configuration of length 1; (ii) IaEi , IaEi+1  is an edge of E for all i (1  i < z); and (iii) IaEz contains a 1-type I, such that D a I,.
Let n = 	D	 + 1.
By definition, each configuration in C is of length at most n. We show by induction that, for all j (1  j  z), there exists a structure Aj whose top row realizes a profile of the form IaEj , uE, with uE fi (nzaj , .
.
.
, nzaj ).
It follows that Az is a model of D, proving the lemma.
Case j = 1: By Lemma 5, there exists a structure whose top row realizes the profile (Ia), (nza1 ).
Case 1 < j  z: Suppose Aja1 has been defined, and that the top row of Aja1 realizes the profile IaEja1 , uEja1 , with  uEja1 fi (nzaj+1 , .
.
.
, nzaj+1 ).
Since (IaEja1 , IaEj ) a E, the pair IaEja1 , IaEj , has a solution, and hence, by Lemma 10, a solution uEja1 , uEj  with uEj fi (nzaj , .
.
.
, nzaj ).
By Lemma 4, there exists a model Aj whose top row realizes the profile IaEj , uEj .
This completes the induction, and the proof.
Theorem 12.
The satisfiability problem for LD is in PS PACE.
Proof: Denote 	D	(	D	 + 1) by n, and consider the following nondeterministic procedure 1. set N = 0; 2. guess a configuration I,E of length 1; 3. until N = 2n or I,E contains a 1-type I, such that D a I, do: 4. set IaE = I,E and increment N ; 5. guess a configuration I,E s.t.
|I,E|  	D	 + 1; 6. if IaE, I,E has no solution, then fail; 7. end until 8. if N = 2n then fail; 9. succeed.
By Lemma 9, the test in Line 6 requires only space bounded by a polynomial function of 	D	; further, the counter N requires only n + 1 bits.
Evidently, the procedure has a successful run if and only if there exists a path in GD from some configuration IaE of length 1 to some configuration I,E containing a 1-type I, such that D a I,.
The result then follows by Lemma 11 and Savitchas theorem.
V. A DDING THE O PERATOR [DE] In this section, we show how to generalize Theorem 12 to the satisfiability problem for LDDE over finite linear orders (LDE is just a special case).
First, we modify the definitions of configuration and profile by adding to conditions (C1)a (C2) the further condition: (C3) if 1  h < h  s and [DE]D a Iah , [DE]D a Iah .
Next, we prove an analogue of Lemma 2.
Lemma 13.
If an LDDE -formula D has a model, then it has a model over the same domain in which every row realizes a profile of length at most 4	D	 + 3.
Proof: Fix a model A of D. If D a IS, a witness for D is an interval I a Sub(IA ) such that A |=I D. We call an interval I = [i, j] a Sub(IA ) right (resp., left) special if there exists D a IS such that A |=I D and, for all k (i < k  j) (resp., (1  k < i)) A |=[k,j] Dathat is, if there is some formula of IS for which I is the right-most (resp., left-most) witness in its row.
A special interval is a right special or left special interval.
Evidently, for any I = [i, j] a Sub(IA ), there is a least k (i  k  j) (resp., greatest k  (1  k  < i)) such that Kr = [k, j] (resp., Kl = [k  , j]) is special.
We refer to Kr (resp., Kl ) as the closest-to-the-right (resp., closestto-the-left) special intervals for I.
Finally, since |IS|  2	D	,  and the right-most (resp., left-most) interval of any row is the right-most (resp., left-most) witness for at least |IS|/2 of the formulas of IS in that row, it follows that the number of special intervals in any row is at most 2	D	 + 2.
Let f : Sub(IA ) a Sub(IA ) be a function that satisfies the following properties: (F1) for every special interval I, f (I) = I; (F2) for every non-special interval I, f (I) = I  , where I  is a non-special interval such that the closest-to-the-left and closest-to-the-right special intervals for I and I  are the same; (F3) for every pair of non-special intervals I, I  , if the closest-to-the-left and closest-to-the-right special intervals for I and I  are the same, then f (I) = f (I  ).
Observe that, for any D = [D]I, a IS and any I a Sub(IA ), A |=I D if and only if A |=f (I) D; similarly for D = [DE]I,.
Let B be a structure with the same domain as A (that is, Sub(IB ) = Sub(IA )) such that I B = f (I)A .
We claim that, for every I a Sub(IA ) and every D a IS: B |=I D if and only if A |=f (I) D. We prove the claim by structural induction.
If D = p is a proposition letter, the claim is immediate by the construction of B. Boolean cases are trivial.
So suppose D = [D]I, (whence AZD, AZI, a IS).
Pick I a Sub(IA ), and let f (I) = I  .
For the left-to-right implication, suppose that A |=I  AZD.
Let K be the right-most witness for AZD in the same row as I.
Since K is special and non-strictly to the right of I  , we have K a I.
Now, there exists an interval KE such that KE a K, A |=KE AZI,, and KE is the right-most witness for AZI, in its row.
Since KE is special, f (KE) = KE, and thus, by inductive hypothesis, B |=KE AZI,, whence B |=I AZD.
Conversely, suppose B |=I AZD.
Then, there exists J a I such that B |=J AZI,, whence A |=f (J) AZI,, by inductive hypothesis.
Let K be the right-most witness for AZI, in the same row as J.
Since K is special and non-strictly to the right of f (J), we have K a J, whence A |=I AZ[D]I,.
But we have already observed that A |=I [D]I, if and only if A |=f (I) [D]I,.
Hence, A |=f (I) AZD as required.
The case D = [DE]I, is completely symmetric.
Now, suppose A |=I D. We may assume without loss of generality that I is special, whence I = f (I), so that, constructing B as above, B |=I D. To conclude the proof, it suffices to observe that every row in B realizes a profile that features one 1-type for every special interval in that row (at most 2	D	+2) plus at most one 1-type for every maximal consecutive sequence of non-special intervals in that row (at most 2	D	 + 1).
Next, we modify the definition of a solution for a pair of configurations IaE, I,E, given in Section IV, by adding to conditions (S1)a(S3) the further conditions (S4) for all h (1  h  s) and all h (1  h  t), if [DE]D a Iah and Uh > Vh a1 , then [DE]D a I,h ;  (S5) for all h (1  h  s), if DED a Iah , either there exists h (1  h  h) such that D a Iah , or there exists h (1  h  t) such that Vh a1  Uha1 and DED a I,h .
We construct the graph GD exactly as described in Section IV, except that we employ the revised definitions of configuration and solution, and set C = {IaE | IaE is a configuration s.t.
|IaE|  4	D	 + 3}.
In place of Lemma 11, we then have: Lemma 14.
Let D be an LDDE -formula, and let the graph GD = (C, E) be constructed as above.
Then D is satisfiable over finite linear orders if and only if there is a path in GD from some configuration Ia of length 1 to some configuration I,E containing a 1-type I, such that D a I,, and a path in GD from I,E to some configuration (Il1 , .
.
.
, Ilk ) such that, for every j (1  j  k) and every DED a Ilj , there exists j  (1  j   j) with D a Ilj  .
The following theorem is the counterpart of Theorem 12 for LDDE , and may be proved similarly.
Theorem 15.
The satisfiability problem for LDDE is in PS PACE.
VI.
C ONCLUSIONS In this paper, we have shown that the satisfiability problems for the interval logics of the reflexive sub-interval and super-interval relations interpreted over finite linear orders are PS PACE-complete.
The authors are currently investigating the problem of establishing whether decidability, and complexity, of LDDE are preserved if we replace finite linear orders by the natural numbers or the integers.
The problem is of interest, because, as is easily shown, DE can be used to write formulas satisfiable over infinite discrete linear orders, but not over finite linear orders.
As we have already pointed out, some basic steps of our proof do not work if we replace reflexive sub-interval and super-interval relations by strict or proper ones.
To the best of our knowledge, the satisfiability problems for interval logics of the strict (respectively, proper) sub-interval and super-interval relations, interpreted over finite linear orders, are still open.
ACKNOWLEDGMENTS This paper resulted from a visit of the first and third authors to Manchester University in January 2010, funded by the ESF Research Networking Programme GAMES (Games for Design and Verification).
The same authors acknowledge the support of the Italian PRIN and GNCS projects Innovative and multi-disciplinary approaches for constraint and preference reasoning and Logics, automata, and games for the formal verification of complex systems.
The second author acknowledges the support of the EPSRC, grant reference EP/E035248.
R EFERENCES [1] V. Goranko, A. Montanari, and G. Sciavicco, aA road map of interval temporal logics and duration calculi,a Journal of Applied Non-Classical Logics, vol.
14, no.
1a2, pp.
9a54, 2004.
[2] J. Halpern and Y. Shoham, aA propositional modal logic of time intervals,a Journal of the ACM, vol.
38, pp.
279a292, 1991.
[3] Y. Venema, aA modal logic for chopping intervals,a Journal of Logic and Computation, vol.
1, no.
4, pp.
453a476, 1991.
[4] D. Bresolin, V. Goranko, A. Montanari, and G. Sciavicco, aPropositional interval neighborhood logics: expressiveness, decidability, and undecidable extensions,a Annals of Pure and Applied Logic, vol.
161, no.
3, pp.
289a304, 2009.
[5] D. Bresolin, V. Goranko, A. Montanari, and P. Sala, aTableaux for logics of subinterval structures over dense orderings,a Journal of Logic and Computation, vol.
20, no.
1, pp.
133a 166, 2010.
[6] M. Otto, aTwo variable first-order logic over ordered domains,a Journal of Symbolic Logic, vol.
66, no.
2, pp.
685a 702, 2001.
[7] D. Bresolin, D. Della Monica, V. Goranko, A. Montanari, and G. Sciavicco, aDecidable and undecidable fragments of Halpern and Shohamas interval temporal logic: towards a complete classification,a in Proc.
of the 15th International Conference on Logic for Programming, Artificial Intelligence, and Reasoning, ser.
LNCS, vol.
5330.
Springer, 2008, pp.
590a604.
[8] A. Montanari, G. Puppis, and P. Sala, aMaximal decidable fragments of Halpern and Shohamas modal logic of intervals,a in Proc.
of the 37th International Colloquium on Automata, Languages and Programming, 2010.
[9] J. van Benthem, The Logic of Time: A Model-Theoretic Investigation into the Varieties of Temporal Ontology and Temporal Discourse, Second Edition.
Kluver, 1991.
[10] I. Shapirovsky and V. Shehtman, aChronological future modality in Minkowski spacetime,a in Advances in Modal Logic.
London: Kingas College Publications, 2003, vol.
4, pp.
437a459.
[11] I. Shapirovsky, aOn PSPACE-decidability in Transitive Modal Logic,a in Advances in Modal Logic.
London: Kingas College Publications, 2005, vol.
5, pp.
269a287.
[12] E. Kieronski, J. Marcinkowski, and J. Michaliszyn, aB and D are enough to make the Halpern-Shoham logic undecidable,a in Proc.
of the 37th International Colloquium on Automata, Languages and Programming, 2010.
[13] K. Segerberg, aAn essay in classical modal logic,a Filosofiska Studier, vol.
13, 1971.
[14] L. J. Stockmeyer and A. Meyer, aWord problems requiring exponential time,a in Proc.
of the 5th ACM Symposium on the Theory of Computing, 1973, pp.
1a9.
Probabilistic Reasoning about Uncertain Relations between Temporal Points Vladimir Ryabov Department of Computer Science and Information Systems, University of Jyvaskyla, 40351 Jyvaskyla, Finland v l a d a y t k o .jyu.3  Seppo Puuronen Department of Computer Science and Information Systems, University of Jyvaskyla, 40351 Jyvaskyla, Finland sepiaytko.jyu.3 about these relations.
A temporal interval can be represented as a pair of points denoting the start and the end of the interval, and then, the proposed representation can also be applied with intervals, as it is shown in [14].
In many situations there is a need to deal with temporal relations, which are, in reality, not always certain.
Uncertainty, which is one kind of imperfect information according to Parsons' classification [ l 11, arises from the lack of information about the state of the world.
This lack of information makes it impossible to determine if certain statements about the world are true or false.
We are able only to estimate the tendency of the statement to be true or false using, for example, some numerical measure of degree to which one may be sure [ l l ] .
Uncertainty can stem from a number of sources, such as many sources of information, granularity mismatch, input errors, or even data can be deliberately made uncertain for reasons of security [SI.
Various approaches to the problem of handling imperfect information are mentioned in the bibliography on uncertainty management by Dyreson [5], and in the survey by Parsons and Hunter [121.
The topic of handling uncertainty in temporal knowledge was underlined as a newly emerging and growing subarea of temporal representation and reasoning in the recent survey by Chittaro and Montanari [2].
However, during the past decades a number of approaches to this problem were proposed.
The notion of indefinite temporal relation, which is a disjunction of the basic relations, was used by van Beek and Cohen [19].
That representation did not include any numerical measures for uncertainty, mostly concentrating on the reasoning algorithms and the constraint satisfaction problem.
In the research area of integrating time and probability, there are a number of approaches introducing into temporal contexts mathematical formalisms such as Bayesian networks, for example, [13], [7], and Markov processes, for example, [17].
In this paper we use a probabilistic approach to deal with uncertain temporal relations, although, there are several other means of handling uncertainty.
For example, Dubois and Prade [3] applied possibility theory to process fuzzy temporal knowledge.
Abstract A wide range of AI applications should manage time varying information.
Many published research articles in the area of temporal representation and reasoning assume that temporal data is precise and certain, even though in reality this assumption is often false.
However, in many real applications temporal information is imperfect and there is a need to find some way of handling it.
An uncertain relation between two temporal points is represented as a vector with three probability values denoting the probabilities of the three basic relations: "c" (before), "=" (at the same time), and '5"(after).
The reasoning mechanism includes inversion, composition, addition, and negation operations.
We propose formulas to calculate the probability values within the uncertainty vectors representing the resulting relations of the reasoning operations.
We also consider an example of using the proposed representation and reasoning mechanism.
1.
Introduction In a wide range of AI research fields there is a need for representation and reasoning about temporal information.
Temporal formalisms are applied in natural language understanding, planning, keeping medical records, i.e.
in all the areas, where the time course of events plays an important role.
Many temporal representation and reasoning approaches assume that precise and certain temporal information is available, and they give little or no support for situations in which we are dealing with imperfect temporal information.
However, in many real applications temporal information is imperfect and there is a need to find some way of handling it.
Temporal points and intervals are the main ontological primitives used by temporal formalisms.
In this paper we use temporal points as ontological primitives.
We propose a probabilistic approach to represent uncertain relations between temporal points and a mechanism for reasoning  0-7695-1107-4/01 %10.000 2001 IEEE  35  A totally certain relation (TCR) between two temporal points a and b is a relation represented by the uncertainty  Another growing subarea of temporal representation and reasoning is the area of temporal databases, where there is also a need to handle uncertainty.
Dyreson and Snodgrass [6] proposed to support valid-time indeterminacy in temporal databases by a probabilistic approach, concentrating on the temporal query language for uncertain temporal information.
In this paper we propose an approach to represent the uncertain temporal relation between two points as a vector with three probability values denoting the probabilities of the three basic relations ( "<" - before, "=" - at the same time, and ">" - after), that can hold between these points.
A reasoning mechanism includes inversion, composition, addition, and negation operations.
In the definitions of the reasoning operations we provide formulas for calculating the probability values for the uncertainty vectors representing the resulting relation.
The structure of the paper is the following.
In the next section we introduce the main concepts used in the paper.
In Section 3 we present four operations of the reasoning mechanism.
In Section 4 we consider an example of representation and reasoning with uncertain temporal relations, and in Section 5 we discuss about the approach used in this paper.
And, finally, in Section 6 we make conclusions and point out some directions for further research.
vector (e<,e',e>)a,b , where e:,b =1,  Two uncertain temporal relations r1represented by the vector (~e:,eT,eT) and r2 represented by the vector  (e; ,ei,e:i) are equal if e: = e;', ef = e;, and e; = e;.
Othenvisc, the relations rl and r2 are unequal.
For example, two uncertain relations (0.7,0.3,0),b and (0.2,0.8,Cl),b between points a and b are unequal, although they are equal ("I") at the symbolic level of representation.
In many application domains, even when we know nothing about the relation between any two temporal points, the basic temporal relations between these points are not equally probable.
Let us denote the domain probability values of the basic relations as e;, e:, and  e;.
These probability values represent the probabilities of the basic relations between two points in the situation, when we know nothing about the relation between these points in the given domain area.
The sum of these probability values is equal to 1.
We suggest, that the domain probability values are defined for a particular temporal context or application, otherwise, the basic relations will be considered equally probable, which often introduces some imprecision to the description of the situation.
We will use the domain probability values in the definition of the composition and negation operations.
2.
Main concepts In this section we define the basic concepts used  .
throughout the paper.
3.
Reasoning about uncertain relations  The three basic relations that can hold between two temporal points are: "<" (before), "=" (at the same time), and ">" (after).
These are certain temporal relations between points.
Possible disjunctions of the certain or y), "2" or 'y relations, and or ",,), ?
( <" or "=" or ''>"), are called the uncertain relations between temporal points.
We propose to extend this representation by providing the probabilities of the basic relations within the uncertain relation.
- Definition 1.
Let an uncertain relation between two temporal points a and b be represented by a vector C6<77  '6  99  ("&?
66>??
),  In this section we define four operations for reasoning with uncertain relations between temporal points.
Our reasoning mechanism includes inversion, composition, addition, and negation operations, which we consider correspondingly in the following four subs&tions.
(,6<?7  66  3.1.
Invlersion The operation of inversion (-) derives the relation rba between two temporal points b and a, when the relation r,b between a and b is known, and rb,a = Fa,b as presented  (e<,e=,e>)al,, where the value .e:,b is the probability of a<b, the value  =1, or e,',b=l.
is the probability of a=b, and the  in Figure 1.  value e,'.b is the probability of a>b.
The sum of these  + e:,b =1, probability values is equal to 1, i.e.
e:,b + since they represent the probabilities of all the basic relations.
For example, in the situation, when the temporal relation between the points a and b is "<" or "=", and the probability of "4'is 0.7, and the probability of "=" is 0.3, the uncertainty vector is (0.7,0.3, 0)lqb.
G+=  -  rb,a= ra,b rab  Figure 1.
Operation of inversion The relations "4' and ">" are mutually inverted, and an inversion of "=" is the relation "=".
To obtain the  36  probability needs to be divided between the probabilities uncertainty vector for an inverted uncertain temporal relation between two points, we only need to exchange the of all three basic relations, according to the domain values of the probabilities of the relations "<" and 'Y',as probability values e ; , e ; , and e;.
in the following definition.
Definition 3.
Let the relations r+b, rho and r,, Definition 2.
Let the relations ra,band rb, be defined between the temporal points a, b, and c be represented by by the vectors (e<,e',e>)a,b and (e<,e=,e>)b,a the uncertainty vectors (e',e=,e') a,b ' (ec,e',e>)b,c, and '  correspondingly, and elr =  rb,= is,,b.
e,<, = eib,  Then,  = e:,beb<,c + e:,bei,c  From the above definition it is easy to derive the property of double inversion, according to which an uncertain temporal relation rqb is equal to the double inversion of this relation, i.e.
r&b = Fa,b.
c,;'  -  n  \ay Figure 2.
Operation of composition  uncertainty vectors for the relations r q b and rb,e There exist 9 combinations of the values of the relations r+b and rb,c(in each combination the first value is the probability of the relation between the points a and b, and the second value between the points b and c): 1) "<" and "4';2) "<" and 3) and ~ ~4)> ~ and ~ ; 5) and , 6) and 7) and 8) and 9) "=?7  6'<,*;  9  and *  In many situations, we need to deal with more than one possible uncertain relation between two temporal points.
This happens when the information about a relation, for instance, is collected from a number of information sources or experts.
For example, according to the first expert we might know that the relation between points a and b is "I" and is represented by the uncertainty vector (0.6,0.4,0),b.
At the same time, the second expert suggests that the relation between these points is "2"with the vector (0,0.8,0.2),b.
In this situation, it can be helpful to combine these two uncertain temporal relations into a single uncertain relation r,b between the points a and b.
To be able to do this, we propose the binary operation of addition illustrated in Figure 3.  e:,, between the temporal points a and c, with the known  ">99  + ez,be;,ce6  7  3.3.
Addition  The goal of the composition operation is to find out the probabilities of the three basic relations e:,, , e:,, , and  &&<??
+ e:,be6,ce;  + e:,be;,ce;  easily proved using the formulas in Definitions 2 and 3.  b @ rb,c  6,>99;  a,b b,c  + e;.beb<,c + e:,beB,ce;  defined above, is equal to 1, which can be easily proved by transforming and simplifying their sum.
The operation of composition is obviously noncommutative (i.e., r+b @ rb,c # r b C @ r+b), associative (i.e., rqb@ (rb,c@ rqd)= (rab8 rbC)8 rqd)for symbolic representation of relations, non-associative for particular probability values within the uncertainty vectors, and TCR "=" is the identity for composition (i.e., "="@rb,c = r b C and rqb@'="= r,b).
Moreover, an inversion of the composition of two uncertain temporal relations rqband rbc is equal to the composition of their inversions (i.e.
-(raa 8 rb,c)= Fb,, 0 ).
The latter property can be  The operation of composition (8) derives the relation rqCbetween the temporal points a and c, when there exist the relation r,b between the points a and b and the relation rb,cbetween the points b and c, as presented in Figure 2.
&&=73;  = e = e=  = ea>,be6,c + ea>,beb=,c + e:,beb>,c + e:,be6,ce; + e:,be;,ce; The sum of the probability values e:,c, e:,c, and  3.2.
Composition  6'=99  @ rbc is:  (e<,e=,e>)a,c correspondingly.
Then rqC=r+b  ,and e;,.
= e:,b .
66<3*; 6'>99  ?.=))  46=79;  U=)).
U>??
and '5".In the combinations 1, 2, and 4 the basic relation "<" is supported.
In the combination 5 the basic relation "=" is supported.
In the combinations 6, 8, and 9 the basic relation '5'' is supported.
And, finally, in the combinations 3 and 7 all the three basic relations are supported.
In each combination the probability of the particular value of the relation is derived as a multiplication of the correspondent probability values from the uncertainty vectors.
For example, the probability obtained in the combination 1 is a multiplication of e:,b  (e),  n  n rl,b I /  and e:,c supporting e:,, .
In the combinations 3 and 7, the  Figure 3.
Operation of addition  37  Operation of addition is similar to the operation of intersection defined, for example, in the interval algebra of Allen [l], in the point algebra of Vilain and Kautz [20], and further in the extensions of that algebras by van Beek [IS].
We extend the operation of intersection, which uses the symbolic representation of temporal relations, with probabilistic measures of uncertainty, and provide the formulas to calculate the probability values within the resulting vector.
Definition 4.
Let the uncertain relations rl,b, r2+b, r,b between the temporal points a and b be represented by the uncertainty vectors (e;,e,  (e;,ei ,e;),,,  relation r&b is definitely not "c".In this case, r q b can still be or (<=)) G6>y9  The unary operation of negation was also included in the algebra by Ladkin and Maddux [9], [lo] (called operation of complement there).
We extend their definition with our representation of uncertain relations using uncertainty vectors and provide the formulas for calculating the probability values within the resulting vector.
We suppose, that the probability of a particular basic temporal relation which is impossible between two points needs to be divided between the probabilities of the other two basic temporal relations in the resulting uncertainty vector.
'This division between these two contributed relations should be made according to the domain probability values.
Definition 5.
Let the temporal relations r + b and r l , b  and  (e<,e=,e>),,, respectively.
Then ra,b= rla,b8 r2,,, is: .
s<  -  s=  S>  eZb = - , e;,b = -,and e:,, = - ,where S=s<+s=+s', S S-S  be represented by the uncertainty vectors (e<,,=,e>) a,b  and (e:,., ,e;),,, correspondingly.Then r  The operation of addition is commutative (i.e., rl,b0r2,b=r2,b8r1,b), idempotent (i.e., rlpb8rl,b=rl,b), and TCR is an annihilator for addition (i.e., rl,b8TCR=TCR8rl,b=TCR).
Addition is associative (is., (rl,b8r2,b)8r3,b'rl,b8(r2,b8~3,b)) at the symbolic level of representation, and nonassociative for particular probability values within the uncertainty vectors.
Moreover, an inversion of the addition of two uncertain temporal relations rl,b and r2,b is equal to the addition of their inversions (i.e.,  , b = q  is:  For example, the probability value e: is decomposed  eie: e;.
: - which and e6 +e; e; +e; contribute to the values e, and e; correspondingly.
In a similar way we divide the probabilities et and e ; .
A negation of an inversion of an uncertain temporal relation lis equal to the inversion of the negation of this relation (i.e., Fa,b=-- (rab)), which can be easily proved.
into the probabilities  -(rlab@ r2a,b)= (Fla,b 8 f2,,b) ).
The latter property can be proved using formulas in Definitions 2 and 4.
Combining a number of relations into one, we irrevocably lose the information about the added relations.
This means, that after deriving the uncertainty vector as a result of the addition it is in most cases impossible to know which relations were added.
This can be crucial in the systems where it is important to know the ancestors of the derived relation.
One possible solution to this problem is to keep the history of added relations, although, in many complex reasoning systems this will appear to be too expensive and, hence, unreasonable.
~  In the next section we consider an example of represenlation of uncertain temporal relations and the use of different operations to reason with them.
4.
Example Let U:; consider three temporal points a, b, and c, and uncertain relations between them.
Let the domain probabilities of the basic relations be: e;=0.3, e i 9 .
1 ,  3.4.
Negation There &e situations, when we do not have the information about the relations which are possible between two temporal points, but we might know the relations that are impossible.
The operation of negation (-) derives the possible uncertain temporal relation r,b between points a and b, when it is known that the uncertain temporal relation rlqb between the points a and b is impossible.
For example, it might be known that the  and e g a .
6 .
Let the information about the relation between the points a and b be obtained from two information sources.
According to the first information source, the probabilities of the basic relations "Q', "=", and '5''between a and b are 0.7, 0.1, and 0.2 correspondingly.
The second information source suggests that the probabilities of the basic relations between these points are: 0.45, 0.4, and 0.15.
And, finally, it is known  38  that the relation between the points b and c is definitely not '5".
Let us find the probabilities of the basic relations between the points c and a.
The given information is: rla,b=(0.7,0.1,0.2), r2,,b=(0.4570.470.
15), and rlbF=(O,O, l), as it is illustrated at Figure 4a.
"imprecision", "calculus", "consistency", "assessment", and "computation".
Moreover, to be able to model accurately uncertain temporal relations, a formalism needs to model "alternative relations" (we suppose that only one of the basic relations actually holds between two points), "dependent values" (for example, the values of the endpoints of two temporal intervals are dependent; to model the relation between these intervals we need to model these dependencies, as shown in [14]), and, finally, we need to have clear and explicit rules for combining the uncertainty measures.
Based on criteria above, we suggest that the probabilistic approach is better suited to model the uncertain temporal relations.
At the same time, fuzzy set theory can be successfully used to model another aspects of temporal knowledge, for example, generalized events [3].
We also suppose that Dempster-Shafer theory of evidence can also be applied for dealing with uncertain temporal relations, since it uses the notion of probability.
It is different from our approach in a way that we do not consider probabilities of all possible subsets of the basic relations.
Although, in some particular applications it can be useful, and we suppose that our approach can be extended to do this.
n  r2,b=(0.45,0.4,0.15)  Figure 4.
Initial relations between a, b, and c n  Figure 5.
Derived temporal relations  6.
Conclusions The relation r, can be derived as an inversion of the relation rqc,which is a composition of the relations r,b and as it is illustrated at Figure 5.
The relation r,b can be found as an addition of the relations rl,b and r2,b, and the relation rb,eis a negation of the relation &,c: rW=  -  ('a,b  C3rb,c)="((r1a,b  r2a,b) @  In this paper we proposed a probabilistic approach to represent uncertain relations between temporal points and a mechanism for reasoning with these relations.
The uncertain relation between two points is represented by the uncertainty vector with three probabilities of the basic relations (''e'', "=", and ">").
The reasoning mechanism includes inversion, composition, addition, and negation operations.
Further research is needed to analyze the applicability of the proposed mechanism in different applications of temporal formalisms.
As another direction for further research we consider the development of the means to measure the degree of certainty of a particular temporal relation and study the change of this degree while performing different reasoning operations.
c) *  Applying the formulas of Definitions 3,4,5, and 6: rw="(((0.7,0.1,0.2) 0 (0.45,0.4,0.15)) B  o )=  "((0.623,0.182,0.195) C3 (0.75,0.25,0)) = = "(0.803,0.06,0.137) = (0.137,0.06,0.803) .
The derived uncertainty vector includes the probabilities of the basic relations between the temporal points c and a: e:,, = 0.137, e:,, = 0.06, and e:,, = 0.803.
7.
Acknowledgements 5.
Discussion We are grateful to anonymous reviewers for useful comments and constructive criticism.
This work was partly funded by COMAS graduate school at the University of Jyvaskyla.
Probabilistic approach, which was applied in this paper to handling uncertain temporal relations, is actually one of the numerical formalisms for dealing with uncertainty.
The other widely used techniques are possibility theory [4], Dempster-Shafer's theory of evidence [ 151, certainty factors [16], etc.
The selection of the probabilistic approach was made based on the criteria for the evaluation of uncertainty management techniques proposed in [2 11.
These include: "interpretation",  8.
References V I J.F.
Allen, "Maintaining Knowledge about Temporal  Intervals", Communications of the ACM, Vo1.26, 1983,  pp.832-843.
39  S. Parsons and A.
Hunter, "A Review of Uncertainty  L. Chittaro and A. Montanari, "Temporal Representation and Reasoning in Artificial Intelligence: Issues and Approaches", Annals of Mathematics and ArtGcial Intelligence, Vo1.28,2000, pp.47-106.
D. Dubois and H. Prade, "Processing Fuzzy Temporal Knowledge", IEEE Transactions on Systems, Man, and Cybemetics, Vol.
19(4), 1989, pp.729-744.
D. Dubois and H. Prade, Possibility Theory: an Approach to the Computerized Processing of Uncertainty, Plenum Press, New York, 1998.
C.E.
Dyreson, "A Bibliography on Uncertainty Management in Information Systems", in Uncertainty  Handling Formalisms", in Applications of Uncertainty Fomuzlisms, A.
Hunter and S .
Parsons (eds.
), Lecture Notes in Artificial Intelligence, Vo1.1455, Springer, 1998, pp.8-37.
J. Pearl, "Probabilistic Reasoning in Intelligent Systems", 2-d Edition, Morgan Kaufmann, 1992.
V. P.yabov, "Uncertain Relations between Indeterminate Temlporal Intervals", Proceedings of the IO-th International Conference on Management of Data, R. Agrawal, K. Ramamritham, and T. Vijayaraman (eds.
), Tata McGraw Hill Publishing Company Limited, New Delhi, India, 2000, pp.87-95.
G. S hafer, A Mathematical Theory of Evidence, Princeton University Press, 1976.
E. Shortliffe, Computer-Based Medical Consultations: MYCIN, Elsevier,New York, 1976.
A.
'rawfik and E. Neufeld, "Irrelevance in Uncertain Temporal Reasoning", Proceedings of the 3-d  Management- in Infomation Systems: From Needs to Solutions, Kluwer Academic Publishers, 1997,  pp.415458.
C.E.
Dyreson and R.T. Snodgrass, "Supporting ValidTime Indeterminacy", ACM Transactions on Database Systems, Vo1.23(1), 1998,pp.1-57.
P. Haddawy, "Representing Plans under Uncertainty: a Logic of Time, Chance, and Action", Lecture Notes in Computer Science, Vo1.770, Springer, 1994.
S .
Kwan, F. Olken, and D. Roten, "Uncertain, Incomplete, and Inconsistent Data in Scientific and Statistical Databases", in Second Workshop on Uncertainty  Intemational Workshop on Temporal Representation and Reasoning (TIME'96), IEEE Computer Society, 1996,  pp.182-187.
P. van Beek, "Exact and Approximate Reasoning about Qualitative Temporal Relations", Ph.D. thesis, Department of Computer Science, University of Waterloo, Canada, 1990.
P. van Beek and R. Cohen, "Exact and Approximate Reasoning about Temporal Relations", Computational Intelligence, Vo1.6, 1990, pp.132-144.
N. Vilain and H. Kautz, "Constraint Propagation Algorithms for Temporal Reasoning", Proceedings of the  Management and Information Systems: from Needs to Solutions, Catalina, USA, 1993.
P.B.
Ladkin and R. Maddux, "On Binary Constraint Networks", Technical Report, KES.U.88.8, Kestrel Institute,Palo Alto, California, 1988. .
[lo] P.B.
Ladkin and R. Maddux, 'The Algebra of Binary Constraint Networks", Technical Report, KES.U.88.9, Kestrel Institute, Palo Alto, California, 1988.
[11] S .
Parsons, "Current Approaches to Handling Imperfect Information in Data and Knowledge Bases", IEEE Transactions on Knowledge and Data Engineering, V01.8(3), 1996,pp.353-372.
.
.
5 t h National Conference of the American Association for Arti]?cial Intelligence, Morgan Kaufmann, 1986, pp.377-  382.
P. VJaley, "Measures of Uncertainty in Expert Systems", ArtQZcial Intelligence, Vo1.83, 1996, pp.1-58.
40
F2), F2-F1 and F1\F2.
The Qualitative Relations Manager is simply our implementation of Allen's operations of inversion, intersection and composition of Interval Algebra relations.
Finally, the third module implements our operations of inversion (not described in the paper), check-intersection and composition.
Given an I-Time de nition, one could ask the temporal location of the instances of the I-Time in a given Frame-Time (e.g., one could want in output the list of all Mondays between 1-1-90 and 1-6-90).
This operation is managed by the Unfolding Manager.
Besides these queries, TeMP also deals with queries concerning the KB of periodic events speci cations.
For example, one can ask which are the temporal relations between two periodic events ev1* and ev2*, in a given Frame-Time F. In such a case, TeMP gives as output the list of temporal speci cations of the form F'\F ev1* EACH C R ev2* in the KB of periodic events where F' is a Frame-Time intersecting F. TeMP has been implemented in Quintus Prolog and runs on Sun workstations, under Unix.
8 Conclusions  The temporal framework sketched in this paper constitutes an integration of part of the works developed by the two mainstreams of research about periodic events in AI and TDB, extending current approaches to deal with both user-de ned I-Times and qualitative temporal relations concerning periodic events.
Although we believe that our framework is signi cantly more powerful and expressive than the other approaches dealing with periodic events in the AI and TDB literature, many other aspects have to be taken into account in order to obtain a comprehensive approach to periodic events.
In particular, we are currently investigating the possibility of extending our framework for dealing with quanti ers such as "only", "sometimes" etc.
in Morris et al., 93] and with partial relations between I-Times.
This work was partially supported by the Italian CNR, project "Ambienti e strumenti per la gestione di informazioni temporali".
References  Allen, 83] J.F.
Allen: "Maintaining Knowledge about Temporal Intervals", Comm ACM 26(11), 832843 (1983).
Allen, 91] J. Allen: "Time and Time again: the Many Ways to Represent Time", International Journal of Intelligent Systems 6(4), 341-355 (1991).
Baudinet et al., 93] M. Baudinet, J. Chomicki, P. Wolper: "Temporal Deductive Databases", in A. Tansell, R. Snodgrass, J. Cliord, S. Gadia, A. Segev  (eds): Temporal Databases: Theory, Design, and Implementation, Benjamin-Cummings (1993).
Chandra and Segev, 93] R. Chandra, A. Segev: "Managing Temporal Financial Data in an Extensible Database", Proc.
19th International Conference on Very Large Databases (1993).
Chomicki and Imielinsky, 88] J. Chomicki, T. Imielinsky: "Temporal Deductive Databases and In nite Objects", Proc.
ACM Symposium on Principles of Database Systems, 61-73 (1988).
Kabanza et al., 90] F. Kabanza, J.-M. Stevenne, P. Wolper: "Handling In nite Temporal Data", Proc.
ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, 392-403 (1990).
Ladkin, 86a] P. Ladkin: "Primitive and Units for Time Speci cation", Proc.
AAAI'86, 354- 359 (1986).
Ladkin, 86b] P. Ladkin: "Time Representation: A Taxonomy of Interval Relations", Proc.
AAAI'86, 360-366 (1986).
Leban et al., 86] B. Leban, D.D.
McDonald, D.R.
Forster: "A representation for collections of temporal intervals", Proc.
AAAI'86, 367-371 (1986).
Ligozat 91] G. Ligozat: "On Generalized Interval Calculi", Proc.
AAAI'91, 234-240 (1991).
Morris et al., 93] R.A. Morris, W.D.
Shoa, L. Khatib: "Path Consistency in a Network of Nonconvex Intervals", Proc.
IJCAI'93, 655-660 (1993).
Niezette and Stevenne, 92] M. Niezette, J.-M. Stevenne: "An Ecient Symbolic Representation of Periodic Time", Proc.
First International Conference on Information and Knowledge Management (1992).
Poesio, 88] M. Poesio: "Toward a Hybrid Representation of Time", Proc.
ECAI'88, 247-252 (1988).
Soo and Snodgrass, 92] M. Soo, R. Snodgrass: "Mixed Calendar Query Language Support for Temporal Constraints", Tech.
Rep. TempIS No.29, University of Arizona (1992).
Soo, 93] M. Soo: "Multiple Calendar Support for Conventional Database Management Systems", Proc.
Int.
Workshop on an Infrastructure for Temporal Databases, (1993).
Stonebraker, 90] M. R. Stonebraker: Chapter 7: Extensibility.
Readings in Database Systems, M.R.
Stonebraker (editor), Morgan Kaufman (1990).
Terenziani, 95] P. Terenziani: "Integrating calendar-dates and qualitative temporal constraints in the treatment of periodic events", Tech.
Report 12-95, Dipartimento di Informatica, Universita' di Torino (1995).
Van Eynde, 87] F. Van Eynde: "Iteration, Habituality and Verb Form Semantics", Proc.
3rd Conf.
of European Chapter of the Association for Computational Linguistics, 270-277 (1987).
Property 2.
Our operation of check-intersection does not loose information (this is obvious for composition, which only adds new pieces of information).
Property 1 has been proved by showing that, for each case in the de nitions of check-intersection and composition, the logical description of the "antecedent" part of the de nition (including also the axioms formalising the relation between the I-Times being considered) implies the logical description of the "consequent" part.
For example, we proved the correctness of our de nition of composition in the case where the relation between the two I-Times being composed is 2 by proving that ev1* EACH C1* R1 ev2* ^ ev1* EACH C2* R2 ev3* ^ C1* 2 C2* !
ev2* EACH C1* R ev3* where R is the composition of R1 and R2 in Allen's Interval Algebra and each one of the speci cations has been replaced by the logical axioms describing its meaning.
Property 2 has been proved by showing that, for each case in the de nition of check intersection, the "consequent" part of the de nition, plus the axioms formalising the relation between the I-Times being considered imply the "antecedent" of each de nition.
For instance, in the case where the relation between the two I-Times being composed is temporal equality, we proved that ev1* EACH C1* R ev2* ^ C1* =T C2* !
ev1* EACH C1* R1 ev2* ^ ev1* EACH C2* R2 ev2* (where R is the intersection of R1 and R2, in Allen's Interval Algebra).
It is important to notice that Properties 1 and 2 grant that our operations of check-intersection and composition can be regarded as a compilation of a set of logical inferences that could also be performed (in a less ecient way), e.g, by a theorem prover for the  rst order logic.
Moreover, as a consequence of these properties, Corollary 1 holds.
Corollary 1.
PCforPE is correct and does not loose information (in fact, PCforPE is a pathconsistency algorithm repeatedly applying checkintersection and composition).
The temporal logic and the proofs are not reported in this paper for the sake of brevity, and are presented in Terenziani, 95].
7 The TeMP system  The TeMP system (Temporal Manager of Periodic events) has been realised on the basis of the approach described in this paper.
TeMP is a general purpose  temporal manager dealing with user-de ned calendric de nitions (I-Times) and temporal speci cations about periodic events.
The architecture of TeMP is shown in  gure 2: boxes denote modules and ovals represent data.
Interface Module Manipulation  I-Times definitions KB  Unfolding Manager  Query  Periodic Events KB  I-Times Relations KB  Heuristic Rules Manager  PCforPE  Frame Time Manager  Qualitative Relations Manager  Inversion Check-intersection Composition  Figure 2: Architecture of the TeMP system The Interface Module manages the interaction with the user, allowing the insertion/deletion of ITimes de nitions and temporal speci cations about periodic events (using the formalism described in section 2), as well as queries.
Insertions and deletions build up the knowledge base (KB) of Periodic Events speci cations and the KB of I-Times de nitions.
The Heuristic Rules Manager is an auxiliary module which operates on the de nitions of I-Times provided by the user and gives as output the relation holding between each pair of I-Times, on the basis of a set of heuristic rules (see section 4).
The basic reasoning module is PCforPE, which is based on the algorithm shown in section 5.
PCforPE takes in input the KB of temporal speci cations about periodic events and the KB of temporal relations between I-Times, and check the consistency of the temporal speci cations and infers new speci cations.
In order to operate, the PCforPE module takes advantage of three dierent modules, which operate on the dierent components of our temporal speci cations (i.e., <Frame>, <Qual-Rel> and <ITime> in the syntax (S)).
The Frame Time Manager takes in input two Frame-Times F1 and F2 and provides as output three (possibly empty) sets of FrameTimes: F1-F2 (i.e., the dierence between F1 and   C1 2 C2 (the inverse holds if C2 2 C1)  The result of the composition is ev2* Q C1 R ev3*.
Notice that C1 is the I-Time for the new speci cation between ev2* and ev3*.
For example, the composition of (s11) and (s12) is (s13).
 C1 2inc C2 (the inverse holds if C2 2inc C1) No new speci cation in our formalism can be inferred (the motivation is analogous to that discussed when dealing with ).
For example: a* EACH Christmas* (BEFORE,MEETS) b* @ a* EACH Months* (AFTER) c* {> NO NEW TEMPORAL SPECIFICATION  C1  C2 (the same holds if C2  C1) An inconsistency is reported, since it cannot be the case that ev1* happens exactly once both in C1 and in C2.
For example, consider again the composition of (s12) and (s14).
 C1 ] C2 (C1 and C2 are not comparable) No new qualitative relation can be inferred.
For example: a* EACH Mondays* (BEFORE,MEETS) b* @ a* EACH Tuesdays* (AFTER) c* {> NO NEW TEMPORAL SPECIFICATION  5.3 Reasoning Process  We developed an extension of Allen's path consistency algorithm Allen, 83] in order to reason with a knowledge base of temporal speci cations in our formalism.
Since path-consistency is not complete for Allen's Interval Algebra, a-fortiori it is not complete for our temporal speci cations (which include the Interval Algebra for specifying the qualitative constraints): as in many AI approaches dealing with the Interval Algebra (see, e.g., the survey in Allen, 91]), we chose to loose the completeness of the reasoning process in order to retain tractability.
Dierently from Allen, 83], we must consider multiple temporal speci cations relating the same pair of events.
We denote as Sa (ev1*, ev2*) a temporal speci cation relating ev1* and ev2* "S" is indexed in order to distinguish among dierent speci cations relating ev1* and ev2*.
Figure 1 sketches our path consistency algorithm for periodic events (PCforPE).
In PCforPE, Paths(Sa (ev1*, ev2*)) contains, for each event evh* (ev1* 6= evh* 6= ev2*), all the speci cations relating ev1* and evh* -i.e., Si (ev1*, evh*), for all i and for all event evh*-).
STACK is a stack containing all the new speci cations.
Before the execution of PCforPE, all the input speci cations are pushed onto STACK.
Both check-intersection and composition may generate new speci cations, which are pushed onto the STACK, in order to propagate  FORALL speci cation Sa (ev1*, ev2*) in STACK DO POP Sa (ev1*, ev2*) from STACK FORALL speci cation Si (ev1*, ev2*) DO X <{ Sa (ev1*, ev2*) \ Si (ev1*, ev2*) PUSH the new speci cations in X (if any) onto STACK OD FORALL speci cation Si (ev1*, evh*) in Paths(Sa (ev1*, ev2*)) DO X <{ Sa (ev1*, ev2*) @ Si (ev1*, evh*) PUSH the new speci cations in X (if any) onto STACK OD OD Figure 1: PCforPE algorithm the new constraints they convey.
The algorithm stops when the stack is empty or when an inconsistency is reported (by check-intersection or composition).
Check-intersection and composition do not generate either new bounds (dierent from the upper and lower bounds of the input Frame-Times) nor new ITimes.
Let H and K be the number of bounds and I-Times introduced by the user in the input speci cations.
Thus, at most O(H 	 K ) speci cations may hold between the same pair of periodic events.
Thus, PCforPE considers considers O(H 	 K 	 N 2) dierent temporal speci cations (where N is the number of events in the knowledge base).
Each of them can be pushed onto STACK at most 13 times (due to the fact that an ambiguous qualitative relation is at most a disjunction of 13 basic Interval relations, so that it can be reduced at most 13 times Allen, 83]).
Whenever a speci cation S is pushed onto the stack, PCforPE performs at most O(H 	 K ) check-intersections and O(H 	K 	N ) (the cardinality of Paths(S )) compositions.
Thus, PCforPE operates in polynomial time, performing O(H 2 	 K 2 	 N 2 ) check-intersections and O(H 2 	 K 2 	 N 3 ) compositions.
6 Temporal logic and properties of the approach We provided a logical formalization for the dierent components of our temporal approach and, besides the others, for (i) the basic notions of correlation and association, (ii) the temporal speci cations in our formalism, (iii) the relations between I-Times (e.g., ).
On the basis of our  rst order temporal logic for periodic events we proved the following properties: Property 1.
Our operations of check-intersection and composition are correct  (where Q is the quanti er EACH, C1 and C2 are I-Times and R1 and R2 are qualitative relations in Allen's Interval Algebra) in the intersection d1', d1"] \ d2', d2"] of the Frame-Times is de ned by cases (in the following, R indicates the intersection between the Interval Algebra relations R1 and R2):  C1 =T C2 If R is the null relation, an inconsistency is reported.
Otherwise, the result of checkintersection is ev1* Q C1 R ev2*.
For example: a* EACH Days* (BEFORE,MEETS) b* \ a* EACH Days* (BEFORE,OVERLAPS) b* {> a* EACH Days* (BEFORE) b*  C1  C2 (the inverse holds if C2  C1) If R is empty, an inconsistency is reported.
Otherwise, the result of check-intersection is ev1* Q C1 R ev2* and ev1* Q C2 R ev2* (the intersection of the qualitative temporal relations is selected in the output speci cation).
E.g., a* EACH Mondays* (BEFORE) b* \ a* EACH Weeks* (BEFORE,MEETS) b* {> a* EACH Mondays* (BEFORE) b*, a* EACH Weeks* (BEFORE) b* Notice that both speci cations must be provided in output, since they conjunctively convey the information that a* and b* occur exactly once each Weeks*, and, more speci cally, in the Mondays* part of Weeks*.
 C1 2 C2 (the inverse holds if C2 2 C1) If R is empty, an inconsistency is reported.
Otherwise, the result of check-intersection is ev1* Q C1 R ev2* and ev1* Q C2 R2 ev2*.
In fact, the qualitative relations holding in the restricted ITime must be forced to be compatible those holding in general.
For example, since Mondays* 2 Days*, the result of check-intersection on (s8) and (s7), in the overlapping part of the Frame-Times (i.e., in 1-6-91, 1-1-92]) is the pair of speci cations (s10) and (s11) above.
 C1 2inc C2 (the inverse holds if C1 2inc C1) If R is empty, an inconsistency is reported.
Otherwise, the result is ev1* Q C1 R ev2* and ev1* Q C2 R2 ev2*.
For example: a* EACH Christmas* (AFTER,STARTS) b* \ a* EACH Months* (AFTER,MEETS) b* {> a* EACH Christmas* (AFTER) b*, a* EACH Months* (AFTER,MEETS) b*  C1  C2 (the same holds if C2  C1) In such a case an inconsistency is reported.
For example:  a* EACH Days* (BEFORE,MEETS) b* \ a* EACH Months* (BEFORE) b* gives an inconsistency, since a* cannot happen exactly once a day and exactly once a month.
 C1 ] C2 The temporal speci cations provide two dierent constraints between the same pair of periodic events, holding at "incomparable" I-times.
In such a case no new qualitative constraint can be inferred, and the input constraints are left unchanged (with the implicit meaning that both of them must hold in the intersection -if any- of the instances of the two I-Times).
For example: a* EACH Mondays* (MEETS) b* \ a* EACH Tuesdays* (AFTER) b* {> INPUT SPECIFICATIONS UNCHANGED  5.2 Composition  Composition (@) applies to two speci cations of periodic events as in (s16) (s16) d1', d1"] ev1* Q C1 R1 ev2* @ d2', d2"] ev1* Q C2 R2 ev3* No new information can be inferred as regards the non-intersecting parts of the two Frame-Times.
In the time interval d1', d1"] \ d2', d2"] (if any), the result of composition depends on which relation holds between the I-Times C1 and C2 (in the following, R represents the composition, in Allen's Interval Algebra, of the inverse of R1 with R2).
 C1 =T C2 The composition of the two speci cations is ev2* Q C1 R ev3*.
For example: a* EACH Days* (BEFORE,MEETS) b* @ a* EACH Days* (AFTER) c* {> b* EACH Days* (AFTER) c*  C1  C2 (the inverse holds if C2  C1) In such a case, no new speci cation in our speci cation formalism can be inferred.
For instance, a* EACH Mondays* (BEFORE, MEETS) b* @ a* EACH Weeks* (AFTER) c* {> NO NEW SPECIFICATION Notice that it would not be correct to infer either (i) b* EACH Mondays* (AFTER) c* or (ii) b* EACH Weeks* (AFTER) c*.
In fact, (i) corresponds to arbitrarily assume that c* happens each Mondays*, while (ii) corresponds to arbitrarily assume that b* happens once a week (while we only have that it happens once each Mondays*, so that it could happen also, e.g., on Tuesdays).
Thus, neither (i) nor (ii) are implied by the input speci cations.
Thus, we cannot propose a compact de nition of intersection and composition such as in Allen, 83], Morris et al., 93].
On the other hand, we have to point out a set of basic relations between I-Times (see section 4) and to propose a de nition by cases of check-intersection and composition, on the basis of the relation holding between the I-Times in the speci cations.
4 Relations between I-Times  Given two I-Times C1* and C2*, since we use the quanti er EACH ("exactly once each") in the temporal speci cations, we are interested in the cases where for each instance of C1* there is just a related instance of C2* and vice versa (bijective relation between instances of C1* and of C2*) or in the cases where for each instance of one of the two I-Times (say C1*) there is just a related instance of the other I-Time (say C2*) but not viceversa.
The treatment of the partial relations which do not cover all instances of at least one of C1* and C2* requires an extension of our formalism which is discussed in Terenziani, 95].
Moreover, the cases to be considered for temporal reasoning are those in which C1* and C2* allow one to refer to the same instances of a periodic event, i.e., those cases where there is a relation of temporal containment between the corresponding instances of C1* and C2* (C1* EQUAL C2* is a special case of temporal containment, which must be distinguished since it allows one to draw further inferences).
The disjoint relations =T , , 2 and 2inc (plus inverses) cover these cases.
Given two I-Times C1* and C2*,  C1* =T C2* (read as: C1* and C2* are temporally equal) i there is a bijection between instances of C1* and instances of C2*, and Allen's relation EQUAL holds between each pair of corresponding instances.
 C1*  C2* (C1* is more speci c than C2*) i for each instance of C1* there is exactly one instance of C2* which properly contains it and, conversely, for each instance of C2* there is exactly one instance of C1* which is properly contained in it (bijection) (e.g., Mondays*  Weeks*).
 C1* 2 C2* (C1* is a restriction of C2*) i for each instance of C1* there is an instance of C2* which is temporally equal to it, but not vice versa (e.g., Mondays* 2 Days*).
 C1* 2inc C2* (C1* is an inclusion restriction of C2*) i for each instance of C1* there is an instance of C2* which properly contains it, but not vice versa (e.g., Christmas* 2inc Months*).
Besides these relations, it is important to introduce two further relations.
 C1*  C2* (C1* is more frequent than C2*) characterises the cases where two assertions such as (i) "eventx happens exactly once each C1*" and (ii) "eventx happens exactly once each C2*" are inconsistent in a given FrameTime I.
Roughly speaking, this happens when, in any way we choose a time interval in each instance of C1* in I, at least two of these time intervals intersect the same instance of C2* (e.g., Days*  Weeks* see Terenziani, 95] for a formal de nition of  and of the relations =T , , 2 and 2inc).
 C1* ] C2* (C1* and C2* are temporally incomparable) i none of the above relations (or their inverses) hold between C1* and C2* (e.g., Mondays* ] Tuesdays*).
We devised a set of heuristic rules for determining automatically which one of the 6 relations above holds between two user-de ned I-Times.
For instance, rule (IT) states that a de nition of the form C1*   n / C2* :during: C3* implies C2*  C3*, C1*  C3* and C1* 2 C2* (e.g., from the de nition Aprils*   4 / Months* :during: Years* we have that Months*  Years* and Aprils*  Years* and Aprils* 2 Months*).
Our rules proved to be powerful enough to cover "non exceptional" cases.
However, since the user is completely free in the use of the speci cation language for I-Times, they do not cover all possible cases.
If no relation between a pair of I-Times is determined by the heuristic rules, the relation is asked to the user.
5 Check-Intersection, Composition and Reasoning Process 5.1 Check-intersection  Check-intersection (\) operates on two temporal speci cations involving the same pair of periodic events and works in two steps.
First, the intersection of the two Frame-Times is computed.
If it is empty, then no further operation must be devised, and the original speci cations are left unchanged.
Otherwise, (i) the original speci cations are left unchanged as regards the non-intersecting parts of the Frame-Times and (ii) in the intersecting part of the Frame-Times, check-intersection forces the compatibility of the qualitative temporal relations, depending on the relation between the I-Times.
More speci cally, the value of the check-intersection operation (s15) d1', d1"] ev1* Q C1 R1 ev2* \ d2', d2"] ev1* Q C2 R2 ev2*  (S) <Frame> ev1* <Quant> <I-Time> <Qual-Rel> ev2* where <Frame> is speci ed as the range of time spanning between a starting point and an ending point (e.g.
1-1-90, 1-6-94]), <Quant> is the quanti er "EACH", which stands for "exactly once each", <ITime> is speci ed as in Leban et al., 86] and <QualRel> is a (possibly ambiguous) relation in Allen's Interval Algebra.
For instance, the temporal content of Ex.1 above can be represented by (s4) (given the de nitions of I-Times in (s1-s3)): (s4) 1-1-90, 1-6-94] Sam-visits-oce-X01* EACH First-Monday-of-Aprils* (BEFORE) Sam-goes-to-his-oce* The meaning of a temporal speci cation of the form d1, d2] ev1* EACH C R ev2* is the following: for each instance C' of the I-Time C in the Frame-Time d1, d2], (i) there is one and only one instance ev1' of the periodic event ev1* and one and only one instance ev2' of ev2* associated with C' and (ii) ev1' and ev2' are correlated, and the temporal relation R holds between them.
Our approach also deals with temporal speci cations in which the I-Time is omitted.
For instance, Ex.2 can be speci ed in our formalism by (s5) (s5) (-1, +1) EACH John-works* (BEFORE) Mary-works* with the meaning that, in the Frame-Time (-1, +1), there is a one-to-one correspondence between instances of John-works* and instances of Maryworks*, and the relation BEFORE holds between the temporal extent of each correlated pair of instances.
We also deal with temporal speci cations in which only the I-time of a periodic event is speci ed.
For example Ex.5 can be represented by (s6), Ex.5 "Between 1-1-90 and 1-1-91 John run each Monday" (s6) 1-1-90, 1-1-91] John-runs* EACH Mondays* with the meaning that, between 1-1-90 and 1-1-91, there is exactly one instance of John-runs* associated with each Monday.
For the sake of brevity, in this paper we only consider temporal speci cations expressed according to the schema (S) above.
The complete description of our formalism is proposed in Terenziani, 95].
3 Intersection and Composition of temporal speci	cations  Since our temporal speci cations consider also ITimes and Frame-Times, new problems have to be faced when de ning intersection and composition.
As  regards intersection, for instance, in our approach it is no longer true that at most one speci cation may relate each pair of events.
In fact, dierent temporal speci cations may be involved at dierent FrameTimes, or even in equal or overlapping Frame-Times (consider, e.g., (s7) and (s8)).
(s7) 1-6-91, 1-1-92] mail* EACH Days* (BEFORE, MEETS) visit* (s8) 1-1-91, 1-1-92] mail* EACH Mondays* (BEFORE, AFTER) visit* However, the consistency of the temporal speci cations on the "overlapping parts" of the Frame-Times and of the I-Times must be checked.
For instance, given (s7), the AFTER relation between mail* and visit* asserted in (s8) is not possible on Mondays since 1-6-91 until 1-1-92, and must be ruled out.
Thus, we have to introduce an operation of "checkintersection" (indicated as \), which gives the intersection of two temporal speci cations (or an inconsistency) just in case the temporal speci cations overlap in a given "context" (e.g.
since 1-6-91 until 1-1-92 on Mondays* in (s7) and (s8)).
Check-intersection may give in output more than one temporal speci cation e.g., the application of check-intersection to (s7) and (s8) gives as result (s9), (s10) and (s11): (s9) 1-1-91, 1-6-91) mail* EACH Mondays* (BEFORE, AFTER) visit* (s10) 1-6-91, 1-1-92] mail* EACH Days* (BEFORE, MEETS) visit* (s11) 1-6-91, 1-1-92] mail* EACH Mondays* (BEFORE) visit* Of course, the results of check-intersection crucially depend on the relations between the I-Times of the input speci cations.
For instance, if we put the Itime Tuesdays* in (s7) instead of Days*, the AFTER relation in (s8) has no longer to be ruled out.
Analogously, also composition depends on the relation between the I-Times of the input speci cations.
For instance, the composition of (s11) above and (s12) gives as result (s13): (s12) 1-6-91, 1-1-92] mail* EACH Days* (AFTER) meeting* (s13) 1-6-91, 1-1-92] visit* EACH Mondays* (AFTER) meeting* On the other hand, the composition between (s12) and (s14) reports an inconsistency, since it is not possible that mail* happens exactly once each day and once each week.
(s14) 1-6-91, 1-1-92] mail* EACH Weeks* (AFTER) meeting*  Ex.2 "John always works before Mary" Ex.3 "Bill always works (only) during Mary's work" provides the information that John always works before Bill (see, e.g., Morris, 93]).
However, the approaches in this mainstream deal only with "contextindependent" temporal speci cations, in which no Frame-Time and no I-Time is considered (see, e.g., Ex.2 and Ex.3).
This limitation allow these approaches to propose compact de nitions of composition and intersection (see, e.g., Morris, 93]), but compromises their practical applicability.
Our goal is that of extending the approaches in the second mainstream for dealing also with the "context" (Frame-Time and I-Time) in which periodic events occur (see, e.g., Ex.1), in order to increase their expressiveness and their practical applicability to areas such as scheduling, process-control,  nancial trading, work ow and oce automation.
In section 2, we introduce our formalismfor dealing with I-Times and qualitative relations between periodic events.
In section 3, we discuss some of the main problems in the de nition of intersection and composition of temporal speci cations expressed in our formalism.
In section 4, we distinguish between six dierent types of relations between I-Times.
These relations are then used in section 5 for de ning intersection and composition.
In section 5 we also introduce a path-consistency algorithm which uses intersection and composition for performing temporal reasoning, and discuss its complexity.
In section 6, we sketch some of the properties (e.g., correctness) of our approach, and in section 7 we brie y describe the architecture of TeMP, a temporal manager of periodic events which is based on the approach described in this paper.
2 Temporal Representation of Periodic Events  We assume time to be a linear order on a domain consisting of points.
A time interval I is a convex set of points between a starting and an ending point.
As in Leban et al., 86], we de ne a collection of intervals as an ordered set of non-overlapping time intervals.
Time intervals are the temporal extents in which events take place.
Collections of time intervals represent the collection of the temporal extents upon which the dierent instances (realisations) of a periodic event take place.
The formalism we use for specifying I-Times is that in Leban et al., 86], who introduced a notation for de ning basic calendars and two types of operators on collections of intervals (dicing -e.g., ":during:" in (s1) - and slicing -e.g., "2 /" in (s1)-) for building new user-de ned collections on  the basis of the basic calendars.
For example, given the basic de nitions of Days*, Weeks* and Months*, the collection of the  rst Mondays of April can be incrementally de ned as follows: (s1) Mondays*   2 / Days* :during: Weeks* (s2) Aprils*   4 / Months* :during: Years* (s3) First-Monday-of-Aprils*   1 / Mondays* :during: Aprils* In order to deal with the qualitative temporal relations between periodic events, we adopt the qualitative relations of Allen's Interval Algebra Allen, 83].
However, since Allen's relations hold between pairs of time intervals, and periodic events happen over collections of time intervals, a way for relating pairs of time intervals belonging to dierent collections is needed.
As in Morris et al., 93], we introduce the equivalence relation of correlation between pairs of instances of periodic events, which holds as a result of some contingent relation in the world between them.
For instance, in Ex.1, correlation holds between each corresponding pair of Sam visiting the branch oce X01 and Sam going to his oce, and the temporal relation "before" holds between (the temporal extents of) each pair of correlated instances.
Since we deal with qualitative relations which hold in an I-Time, we consider also the relation of association, which relates each instance of a periodic event with the instance of the I-Time in which it occurs.
In particular, an instance e of a periodic event which occurred in a time interval i is associated with an instance p of an I-Time if and only if i is contained in p. Since the user may introduce more than one speci cation concerning the very same type of event, s/he may want to specify that these speci cations concern the same periodic event (in this case, we say that the dierent speci cations co-designate Morris et al., 93] the same periodic event) or dierent periodic events of the same type.
For example, Ex.4 introduces two dierent periodic events of the same type "John brushing his teeth".
In other words, Ex.4 involves two dierent collections of time intervals in which John brushes his teeth.
Ex.4 "Each day, John brushes his teeth after breakfast and after lunch" In our approach, dierent indexes are used to distinguish between dierent collections of the same type of event (e.g., John ; brush1 and John ; brush2 ).
Indexes will be omitted in the rest of the paper, for the sake of clarity.
2.1 Temporal formalism  In our approach, complex speci cations of periodic events can be provided, according to the syntax (S):  Reasoning about Periodic Events P. Terenziani  Dipartimento di Informatica, Universita di Torino, Corso Svizzera 185, 10149 Torino, Italy E-mail: terenz@di.unito.it  Abstract The paper describes a temporal formalism which deals with both (i) quantitative information concerning the frame of time and the user-de ned calendar-dates in which periodic events are located and (ii) the qualitative relations between periodic events.
The paper de nes the operations of intersection and composition of temporal speci cations, and describes an algorithm which takes advantage of these operations for performing temporal reasoning.
This is the kernel of TeMP, a temporal manager of periodic events.
1 Introduction Periodic events are widely studied in many research areas, such as Arti cial Intelligence (AI) and Temporal Databases (TDB).
In particular, most AI and TDB approaches provided a high-level, powerful and user-friendly formalism for representing periodic events, and (especially in AI) some form of temporal reasoning operating on them.
This work belongs to such a stream of research, and aims at providing a framework in which it is possible to deal also with very rich temporal speci cations, such as Ex.1 "Between 1-1-90 and 1-6-94, each  rst Monday of April Sam visited the branch oce X01 before going to his oce " (following VanEynde, 87], we call Frame-Time the interval which contains all the instances of the event -e.g., "From 1-1-90 to 1-6-94" in Ex.1-, I-Time the periodic time interval over which periodic events take place, which is usually expressed by some calendric expression -e.g., " rst Monday of April" in Ex.1- and e-Time the time in which the actual instance of the periodic event occurred -e.g., "before going to his of ce").
Current AI and TDB approaches do not allow one to deal with such complex speci cations.
In particular, current approaches can be roughly divided  into two mainstreams, depending on the types of temporal information they deal with.
In the  rst mainstream (carried on especially in the TDB community) most attention is devoted to the treatment of I-Times (see, for instance, Leban et al., 86], Chomicki and Imielinsky, 88], Kabanza et al., 90], Soo and Snodgras, 92], Baudinet, 93], Chandra and Segev, 93], Soo, 93]).
These approaches are based on the consideration that, in most cases, periodic events are "context-dependent", in the sense that they take place at speci c periods of time (ITimes).
Moreover, since dierent calendric systems are used for specifying I-Times (depending e.g., from cultural and social factors see e.g., Soo, 93]), most of these approaches stress the necessity of dealing with user-de ned calendars.
This is necessary, e.g., for dealing with temporal information in many areas, such as scheduling, manufacturing, process-control,  nancial trading, work ow and oce automation (see, e.g., the discussions in Stonebraker, 90], Chandra and Segev, 93] as regards  nancial trading, and consider Soo and Snodgrass, 92], Soo, 93] for a more general discussion).
However, the approaches in this mainstream do not consider the possibility of specifying the e-Time of a periodic event in term of its relative position with respect to other periodic events (as, e.g., the qualitative relation "before" in Ex.1).
As a consequence, these approaches devised only limited forms of temporal reasoning.
On the other hand, the approaches in the second mainstream (carried on especially in the AI community) focus on the treatment of quanti ers and eTime, stressing the importance of dealing with qualitative relations between periodic events (see, for instance, Ladkin, 86a, 86b], Poesio, 88], Ligozat, 91], Morris et al., 93]).
This involves, among other things, the development of some form of temporal reasoning.
Following Allen, 83], also temporal reasoning about periodic event has been usually performed using the basic operations of intersection, for checking the consistency of temporal speci cations, and composition, for inferring new speci cations for example, the composition of Ex.2 and Ex.3
Uncertain Temporal Reasoning for the Distributed Transportaion Scheduling Problem  Maroua Bouzid and Abdel-Illah Mouaddib CRIL, IUT de Lens Rue de laUniversiteE daArtois SP 16 62307 Lens-Cedex, France (bouzid,mouaddib)@cril.univ-artois.fr  Abstract Distributed Artificial Intelligence (DAI) is suitable to applications where there is no central control.
One of these applications with which we are concerned is Transportation Scheduling.
We noticed that all the approaches dedicated to this application use a weak representation of time and a simple reasoning.
Furthermore, these approaches ignore the uncertainty behavior of agents.
What we propose is an approach based on Fuzzy Temporal Characteristic Functions (FTCF) which allow a powerful representation of agent companies behaviors making us informed at each time on the degree that the agent is available.
Thinks to this representation, we develop a temporal reasoning allowing a cooperation inter and intra companies to allocate trucks and delegate orders.
1 Introduction The growing interest in the development of Distributed Artificial Intelligence (DAI) methods for large and complex applications requires new extensions to existing methods in order to increase their efficiency and expressiveness.
Indeed, DAI techniques such as Contract-Net Protocol (CNP) [4] have so far failed to provide adequate solution to cope with applications characterized by a high level of uncertainty and rapid change.
These characteristics are common to several domains such as transportation scheduling.
Algorithms used to efficiently solve static scheduling problems such as classical techniques, Opera This work has been supported by the GanymeEdeII project of the contract Plan Etat Nord-Pas-De-Calais and by the MENESR  tional research, and centralized approaches have failed to deal with open dynamic scheduling problems in the presence of uncertainty.
Some investigators have proposed approaches devoted to dynamic scheduling problems in presence of uncertainty [8, 9].
Transportation scheduling application consists of distributed transportation companies that have to carry out transportation orders which arrive dynamically.
Each company have a set of trucks at their disposal.
Each company should maximize the satisfaction of orders according to the availability of its trucks.
Several approaches have been proposed to this application such as MARS [5], TRA CONET [10] based on the task delegation method by using the CNP techniques.
The distributed AI approach is suitable to this application because of: first the complexity of a centralized scheduling algorithm and second the distributed nature of the application (companies and trucks are geographically distributed).
Indeed, most existing approaches assume that each company uses the local plans of its trucks to decide whether all orders will be satisfied or not.
When unsatisfied orders exist a negotiation protocol is fired to delegate these orders to other companies.
Existing approaches assume that each local plan (for each truck) is known in a precise way ignoring the presence of uncertainty.
However, the task of transportation is characterized by a high level of uncertainty regarding different factors such as the traffic density, the power of the truck used and so on that cannot be ignored.
Consequently, the time at which a truck arrives to destination is not usually known in a precise way, but we know with uncertainty the interval during which the truck is possible to arrive.
During this interval the availability of the  truck is weighted by uncertainty.
Consequently, during task transportation, there exist, intervals during which the truck availability is not known in a precise way but it is assumed that it can be represented by a degree of uncertainty.
Furthermore, the formal framework used for representing the availability of trucks, in the existing approach, is so weak that no powerful reasoning can be performed.
In our approach, we propose a suitable temporal representation of trucks based on Temporal Characteristic functions (TCF) [3].
A characteristic function is a (possibly partial) function describing for what instants of time a logical property holds (or does not hold) a thus it refers to the idea of characteristic function of a set.
Roughly speaking, it can be also considered as a kind of two-valued atrajectorya (or ahistorya [11]) characterizing logical behavior given by certain atemporal formulae.
Moreover, the possibility of representing uncertainty on the validity of property over time seems to follow naturally from our functional approach and leads to the Fuzzy Temporal Characteristic Function (FTCF).
FTCFs allow the representation of the uncertainty on the behavior of trucks due to traffic density and unpredictable events that prevent them from respecting their schedules.
Thus, the FTCFs allows the representation of the uncertainty that characterizes the availability of trucks during task transportation.
This approach is combined by an extended contract-net protocol to deal with negotiation-based delegation of orders when companies are not able to satisfy all orders and negotiate to delegate the remaining orders.
In this paper we outline in Section 2 a description of temporal and fuzzy temporal characteristic functions used by agents in their reasoning.
In Section 3 we present the approach and proposed solution.
Section 4 concludes our paper and describes further work in this field.
2 Temporal approach 2.1 Temporal characteristic functions In this Section, we present the idea of a temporal approach to knowledge representation based on characteristic functions [3].
Basically, a characteristic function is a function describing for what instants of time a logical property holds (or does not hold) a thus it refers to the idea of characteristic functions of a set.
A characteristic function for some set  is any function  of the form:   : T ,!
L, where T is the global domain of interest and L denotes some set of values describing to awhat degreea an element of T belongs to .
In this paper, only the sets  which have the form of fx : (x)g are considered; where  is some property.
Thus, a characteristic function is defined to be a function of the form  : T ,!
L describing to awhat degreea an element of T satisfies the property (formula) , or, for convenience, to what degree the property expressed by  is satisfied by any of the elements of T .
For further discussion let us first establish the domain (maximal time interval) of interest.
We shall consider events happening after some distinguished time 0 and before +1.
This defines the domain for all characteristic functions.
The formal definition of a characteristic function is as follows: Definition 1 Let T = [0; +1[ be the time domain of interest and let L = f0; 1g be the set of distinguished values.
Any mapping : T =) L will be called a Temporal Characteristic Function (TCF).
Any mapping : T 0 =) L will be called a weak (partial) character0 istic function, where T  T If  T  is reduced to an interval  [a; b] and L is reduced to  f1g or to f0g, we are back to the classical knowledge  representation based on convex time intervals[1].
If L is reduced to f1g or to f0g, and T remains arbitrary, we are back to the knowledge representation based on nonconvex time intervals[6, 2].
It is normally assumed that a TCF changes its value over T only a finite number of times.
Thus any TCF is an interval-stable function taking values 0 or 1 over time.
Any point of the domain where the function changes its value will be referred to as a change point, specific point or landmark.
When considering weak TCFs one may be especially interested in positive weak functions and negative weak ones.
Definition 2 A positive weak TCF is a weak TCF which takes as its value only 1 (inf fxjx = (t) for t 2 T g = 1).
A negative weak TCF is a weak TCF which takes as its value only 0 (supfxjx = (t) for t 2 T g = 0).
Definition 3 Let be a TCF (either a weak or a strong one).
A function taking 1 where takes 0 and taking 0 where takes 1 will be called a complement function to ; is undefined for all t 2 T for which is undefined.
Note that any TCF can be represented in fact as a finite union of convex intervals [6] denoting the biggest inter-  vals within which the function does not change its value.
Thus can be given by f(1 ; 1 ); : : : ; (k ; k )g provided that -values denote the beginnings of intervals for which the function has value 1 or the end of intervals for which the function has as value 0 and  -values denote the ends of respective intervals for which the function has the value 1 or the beginnings of intervals for which the function is equal to 0.
A similar representation can be applied to weak TCFs; however, the domain of the function must be given explicitly.
The problem of what value (0 or 1) is taken at the  and  -values can be solved in an arbitrary way depending on current needs.
Weak TCFs can be weaker (stronger) than some other weak functions, i.e.
there is some established partial order relation among them; by intuition, a stronger TCF provides more information than a weaker one.
Definition 4 A weak TCF 1 (defined for T1  T ) is stronger than some weak characteristic function 2 (defined for T2  T ) if and only T2  T1 and 1 (t) = 2 (t) for any t 2 T2 ; we shall write 1  2 .
The basic intuition concerning the semantics of hp; i is that some property p holds over time if the associated characteristic function takes 1 as its value, and does not hold if it takes 0 (for weak episodes, p is undetermined for undefined ).
TCFs constitute formal means for representing some properties over time; they seem to be more general and more powerful than intervals.
Basically, a single function can represent abehaviora of some property over the whole time domain T; thus, in fact, it represents the ahistorya of certain phenomenon.
Moreover, contrary to intervals, they can be easily extended to deal with multiple-valued or fuzzy logics.
2.2  Fuzzy Temporal Characteristic Functions (FTCF)  The idea of fuzzy characteristic functions consists in allowing the truth values to cover the entire range of truthvalues between true and false, i.e.
in terms of characteristic functions between 1 and 0.
Definition 7 Let T = [0; +1[ be the time domain of interest and let L = [0; 1] be the set (closed interval) of : T ,!
L will distinguished values.
Any mapping be called a Fuzzy Temporal Characteristic Function.
Any 0 mapping : T ,!
L is called a weak (or partial) FTCF, 0 Definition 5 Let 1 ; 2 ; : : : ; k be weak TCFs, such that where T  T .
for any t 2 T all the values of the functions which are defined for t are identical; such a set of functions is called Thus any FTCF takes as its values some real numbers consistent (otherwise; inconsistent).
from the closed interval [0; 1]; for simplicity we assume We define a union operation for characteristic functions.
The operation, to be denoted !
is aimed at replacing several weak functions defined on different domains by a single (weak) TCF.
that the function is asufficiently regulara.
Note that any Definition 6 Let 1 ; 2 ; : : : ; k be a consistent set of characteristic function satisfies the definition of FTCF weak TCFs.
We define the union operation !
as follows: (the converse is not necessarily true).
Similarly we can 8 if there exists i for apply Def.
4 directly to FTCF.
Further, one can redefine < i; which i is defined the concept of complement function.
!
( 1 ; 2 ; : : : ; k ) = : undefined, otherwise Definition 8 Let be a fuzzy temporal characteristic Proposition 1 Let 1 ; 2 ; : : : ; k be weak TCFs satisfy- function (either a weak or strong one).
A function such ing the above assumptions (consistency).
Then, for any that (t) = 1 , (t) will be called the complement function to ; is undefined for all t 2 T for which is i 2 f1; 2; : : :; kg we have !
( 1 ; 2 ; : : : ; k )  i .
undefined.
The idea of the knowledge representation language to be used consists in associating a propositional symbol p and Note that an arbitrary FTCF cannot be represented using a TCF for expressing explicitly when p is true, false just a set of intervals (the  and  values): the discussed or unknown over the time domain T .
Thus, the elemen- extension yields a concept significantly more general than tary objects of the language are pairs of the form hp; i. a simple interval.
However, as mentioned above, some  further definitions e.g.
the ones of strength among characteristic functions and the complement (positive and negative strong one) can be applied directly.
Further, we define the filtering operation for FTCF.
The operation aimed at determining from a FTCF defined on T and a condition C , a weak FTCF C defined on T 0  T and such that C (t) satisfies C .
More formally: Definition 9 Let be a FTCF defined on T and C a condition.
filtering( ; T; C ) is the weak fuzzy characteristic 0 function C defined on T  T and such that:  8t 2 T;  C (t) =  Example 1 Let filtering(    (t); if (t) satisfies C undefined, otherwise  be a FTCF defined on T and s 2 [0; 1]:  ; T; (t)  s) =    (t); if (t)  s undefined, otherwise  With respect to union, two cases can be distinguished.
When the set of functions are consistent (Definition 5), the union operation is reduced to the !
operation (Definition 6).
However, when the set of functions are not consistent a i.e.
they take different truth values at some moment t a one can consider three solutions.
The first one consists in strict following of the binary case.
Thus, we simply do not apply union since the information is inconsistent (this solution is simple but not very interesting).
The second solution consists of the optimistic union, i.e.
for each instant where a set of functions is defined, we take the maximum of the functions - this reflects the optimistic point of view.
In other words, when information comes from many sources, we assume that the truest is true.
The converse of this solution constitutes the pessimistic union, we take the minimal of the functions defined for the same instant.
Further, one can take some weighted mean as some intermediate solution.
3 Our approach 3.1 Architecture The architecture with which we work consists of a modified real-time specialist society [7] system which is a group of associations communicating through messagepassing mechanism.
Each association is a group of agents communicating through a blackboard.
Each association  has a controller that decides which agent to activate given a specific goal and communicates with the other controllers of other associations.
The transportation scheduling problem can be easily implemented with this architecture by modeling each company as an association and its trucks as agents.
With this architecture we distinguish between two levels of cooperation: inter-company and intracompany cooperation.
Our paper is based on establishing a formal framework using FTCFs of these two levels of cooperation.
Indeed, each truck agent is associated with a particular shipping company from which it receives orders of the form aLoad amount s of good g at location l1 and transport it to location l2 during a duration equal at most to dw before a deadline D. Each truck is assigned a specific time qualification.
The time qualification is a FTCF taking the value 1 when the truck is allocated and the value 0 when the truck is free.
The FTCF allows the representation of the uncertainty of the availability of the truck during the intervals where the availability is not known in a precise way.
The shipping company association allocates orders to its truck agents of the form mentioned above.
Among its truck agents, the controller selects those which can load s of good g at location l1 and transport it to location l2 .
Among this last, we select all trucks of which FTCFs take 0 for t < D (the deadline of the order).
After that, we determine for this truck the maximal duration needed to this task.
The duration is calcutaled according to the type of the truck, the type of good and the distance l2 , l1 .
More formally, we have: d = f (TA; g; l2 , l1 ), where TA is the type of the truck agent and g is the type of good.
This duration d includes the time required to travel from one location to another one, to load and to unload goods and to come back to the company.
Finally, the selected truck is the one with the highest utility.
This concludes the first level of cooperation.
Afterwards, the company association through its controller performs a contract-net protocol by announcing the unsatisfied orders and receiving bids conveying the contracts proposed by the other companies to satisfy these orders.
The intuition behind the conceptual framework consists in satisfying the most important orders and delegating the least important to the other companies .
Furthermore, a company tries to optimize its satisfaction to maximize its utility.
To this, it allows to allocate the smallest truck-availability interval that is begger than the required task duration.
We describe in the following these two levels of cooperation.
3.2 Formal Framework The global scenario with which we work consists of a high level of a contract-net protocol: announcing unsatisfied orders, receiving bids conveying the proposed contracts and awarding the best selected bid.
Each step of this protocol is itself a cooperation process.
Indeed, the inter-company cooperation to select the trucks best suited to satisfy the orders allows the construction of the announcement while the intra-company cooperation allows the negotiation of the best contract with the other companies to satisfy locally unsatisfied orders.
The cooperation consists of utility-based approaches where each company tries to maximize its own utility.
3.2.1 First cooperation: reasoning  inter-company temporal  Let us consider O the set of orders received by a given company where each order oi is characterized by its deadlines Di before which the order should be satisfied, a duration dei representing an estimate of the time required to satisfy the order, and the worst-case duration dw i .
These durations are determined from statistical data gathered from previous execution of the truck.
We use for the duration dei the average duration over the gathered data and the duration dw i the average duration increased with the standard deviation computed from the same gathered data.
Furthermore, we consider that each truck r has its FTCF r that indicates at any time t to what degree the truck is allocated r (t) = p. Given a set of trucks R and a set of orders O, we need to generate a service schedule of the set O.
For this, the company, that we name in the following C1 , uses an algorithm based on the following steps:       Compute expected utilities for all orders oi , UtilityC1 (oi ) = Reward(oi )- Cost(dwi ); where cost is a function depending on the duration and charges to satisfy an order while Reward is a function representing the rewarded value gained when the order is satisfied, it can for example represent the amount of money that the company is wanting to earn.
Sort the set O according to the utility of orders; Satisfy the orders one by one as follows:    a Search among the set of FTCFs of trucks those which are defined in the interval [Now, Di ] and of which values are less than a threshold s. This step is performed through a filtering operation to find these trucks.
Let Struck be the set of selected trucks.
oi e be the a For each truck j 2 Struck , let Ij;d i o i intervals such that duration( Ij;de ) > dw i (if i I = [x; y], duration(I ) = y , x) oi a Let Ik;min be the interval with the least duration for all trucks oi j 2 Struck : 8j 2 Struck Ik;min = o arg(MIN (duration(Ij;di ei ))).
The intuition behind the selection of the smallest truck-availability interval is first to maximize the utility of the truck and second to reducing the allocation of the truck to be useful for (most) tasks.
a Send to the selected truck, the order oi , its estimated and worst-case durations dei and dw i and oi .
the interval Ik;min  Let S be the set of satisfied orders and N its complementary.
3.2.2 Second cooperation: intra-company temporal reasoning When the company C1 finishes its processing to allocate trucks for orders, it starts a negotiation process to delegate the set N containing unsatisfied orders to other companies.
This process is based on an extended contract-net protocol that integrates FTCF in its processing.
This processing is based on three steps: 1.
Announcing: this step consists in broadcasting the set N to the other companies that we name in the following Ci .
2.
Bidding: this step allows the iterative analysis of each order in the set N such that:    For each oi 2 N the algorithm of satisfying the orders is used, and let I be the interval computed.
The algorithm of satisfying the orders is based on the cooperation inter-company as described bellow.
  i Send bids BidC i containing I; utilityCi (oi ))    For each order oi , construct a set of proposed bids Boi = fbC oii jCi is a company g  (I; (t) : t 2  own utility, the truck tries to allocate the interval where the possibility to be free is the highest.
Consequently, the truck agent performs a filtering operation over its FTCF in 3.
Awarding: This step allows the selection of the best Ii such that: filtering( r ; Ii ; r (t) = 0).
This operation bid from the bids proposed for each order.
This step allows to select a set S c of intervals fI c g. There are two r is as follows: possible outcomes for the set S c :        the companies Ci utilityCi (oi ) < utilityC1 (oi ).
Select  such  that  If no company is selected, the order is not satisfied because it is expensive.
Compute the utility of each selected companyas bid such that: UCi = jtangent j utilityCi (oi ), (  ), ( ) (tangent = , )  For each order oi , select the best bid with the lowest utility.
In other words, the order is delegated to the company proposing the cheepest contract: Ck = argbCi 2B (MIN (UCi )) oi oi   S c = ;: this situation means that there is no interval  during which the truck is sure that it is free.
Then, we select intervals during which the possibility that the truck is free is the highest.
Because a FTCF during these intervals is approximated with a linear feature, the most important intervals are the ones with the smallest jtangentj.
Since the tangent allows us to measure the overall uncertainty over an interval (other measures can be used such as the integral over the interval, but the tangent is sufficient to give us the required information).
Let [;  ] be this interval and r be the FTCF of the truck.
We consider two possible subcases: a  For each order oi send awards to the selected company Ck as the best contractee.
Each company Ck should update the FTCF of its truck.
This step uses a specific temporal reasoning using FTCF that we describe in the following section.
3.2.3 Allocating interval: temporal reasoning in truck agent to update FTCF From the inter- and intra-company cooperation result in the selection of the truck that will be in charge of satisfying an order.
This truck of the company Ck has, then, to take into account the received award carrying the order.
To this end, the truck should allocate the interval during which it satisfies the order.
This operation consists in updating its FTCF by changing its value for every t 2 I , where I is the interval during which it satisfies the order.
The truck r receives the order oi , its interval Ii =[a,b] during which it satisfies the order, the duration de required to satisfy the order and the duration dw as the worst-case duration for satisfying the order.
To update the FTCF, the agent have to assess several situations.
To maximize its  Tangent is positive: this means that during this interval r increases.
Then, the allocation of the interval is performed as follows: we allocate the interval [x; x + de ] such as the middle of this interval is the same as the interval [;  + dw ] (x is easily computed as x w de =d , 2 + ).
Consequently, if () = k and ( + dw ) = j , we find (Figure 1, dashed-lined is the FTCF before the update):  8t 2 [x; x + de ]; (t) = 1 8t 2 [; x]; w e (t) = dw2 ,, kde t + k(2 +ddw ,,dde ) , 2 8t 2 [x + d;  + dw ];  , 1) t+ 2( + dw ) , j (2 + dw + de ) (t) = 2( dw , de dw , de a  Tangent is negative: this means that during this interval r decreases.
Then, the allocation of interval is performed as follows: we allocate the interval [x; x + de ] such as the middle of this  a  dw d  e  1  s j  y , x  de : we allocate the interval [x; x + de ] such as r takes the value 0.
To take the worst-case duration dw , we allocate the interval [;  + dw ] in ordr to be sure that we donat meet  .
Consequently, if we suppose that () = k and ( + dw ) = j , we have (Figure 3, dashedline is the FTCF before the update):  k  0  Ia w 2Ia +d - de 2  2Ia +d 2  w  Ia +dw  I,  t  w 2 Ia +d + de 2  Figure 1: case of a positive tangent interval is the same as the interval [ ,dw ;  ] (x w de is easily computed as x =  , d + 2 ).
Consequently, if we consider that ( ) = j and ( , dw = k), we find (Figure 2, dashed-line is the FTCF before the update):  8t 2 [x; x + de ]; (t) = 1 8t 2 [ , dw ; x]; k , 1) t + 2( , dw ) , k(2 , dw , de ) (t) = 2( e d , dw de , dw e 8t 2 [x + d ;  ]; j , 1) t + 2 , j (2 , dw + de ) (t) = 2( dw , de dw , de d d  j  0 2 I, -d w 2  2I, -d w +de 2  I,  t  Figure 2: case of a negative tangent   S c 6= ;:  s  k j 0 Ia  this situation conveys the fact that there are intervals [x; y ] in the interval [a; b] during which r takes the value 0.
We select the interval [x; y ] with the highest duration, y , x = MAX[xi ;yi ]2S c (yi , xi ).
Two possible subcases are considered:  x  x+de  y Ia + dw  I,  t  Figure 3: case of y , x  de  a  k  2I,-d w -d e 2  1  w  s  I, -d w  dw de  e  1  Ia  8t 2 [x; x + de ]; (t) = 1 8t 2 [; x]; , (t) = xx(x,,kx) t + kx x, e 8t 2 [x + d ;  + dw ]; e w (t) = x + de ,1 dw ,  t , j (xx ++ dde),,(,+dwd )  y , x < de : we allocate an interval [z; z + de ]  such as its middle is the same of the middle e of [x; y ] (z easily computed as z = x+y2,d ).
In the same way, in order to take the worstcase duration into account and to avoid meeting  , we consider the interval [;  + dw ].
Consequently, suppose that ( + dw ) = j and () = k, so we have (Figure 4, the dashedlined is the FTCF before the update).
8t 2 [z; z + d]; (t) = 1 8t 2 [; z ]; e (t) = (2 ,2(xk ,, y1), de ) t + 2 2,,k(xx,+yy,+dde )  8t 2 [z + de ;  + dw ]; + 2jde (t) = 2( +(jdw,) 1) , (x + y , de ) t+ 2( + dw ) , j (x + y + de ) 2( + dw ) , (x + y , de ) d d  References  w  e  1  s k j 0 x  Ia  x+y  y  Ia + dw  2 x+y-d 2  e  society of specialist is under development.
Further work in this approach will concern the monitoring of the agent execution by taking the information gathered during execution into account and performing a re-cooperation and the update of the FTCF afterwards.
The optimality of this approach will also be studied.
I,  t  e  x+y+d 2  Figure 4: case of y , x < de For all cases, to compute the value of FTCF , we consider the linear feature of between two points (with known values of ) and then we generate the linear equation representing the feature of between these points.
In some cases, the allocation of the interval is based on constructing interval having the same middle as the one computed for the truck.
This strategy is motivated by the fact that we guarantee the allocation of the interval where the degree of the availability is highest regardless of the fact that the allocation is not necessarily the optimal one.
4 Conclusion The approach we have presented consists in using fuzzy characteristic functions to express the uncertain behavior of agents.
A simple and powerful temporal reasoning based on fuzzy temporal characteristic functions is developed resulting in a good performing approach to allocate trucks.
The representation and reasoning with fuzzy temporal characteristics contributes to the definition of a formal framework for inter- and intra-companies cooperation.
The representation and the reasoning result in a sophisticated contract-net protocol that is much more expressive and suitable to applications with high level of uncertainty.
The contract-net protocol based on the utility allows each company to maximize its own utility.
The implementation and assessment of this approach in real-time  [1] J. Allen.
Maintaining Knowledge About Temporal Intervals.
Communications of the ACM, 26(11):832a843, November 1983.
[2] M. Bouzid and P. Ladkin.
Rules for Simple Temporal Reasoning.
In Proceedings of TIME-95, International Workshop on Temporal Representaion and Reasoning, pages 73a88, 1995.
[3] M. Bouzid and A. Ligeza.
Temporal logic based on characteristic functions.
In C. R. I. Wachsmuth and W. Brauer, editors, Advances in Atrificial Intelligence, 19th Annual German Conference on Artificial Intelligence, volume 981 of Lecture Notes in Artificial Intelligence, pages 221a232.
Speinger Verlag, 1995.
[4] R. Davis and R. Smith.
Negotiation as a Metaphor for Distributed Problem Solving.
Artificial Intelligence, 20(1):63a101, 1983.
[5] K. Fischer, J. Muller, and M. Pischal.
Cooperative Transportation Scheduling: an Application Domain for DAI.
Technical Report RR-95-01, 1995.
[6] P. Ladkin.
Time Representation: A Taxonomy of Interval Relations.
In Proceedings of the 5th National Conference on AI, AAAIa86, pages 360a366.
Morgan Kaufmann, 1986.
[7] A.-I.
Mouaddib.
Progressive goal-directed reasoning for real-time systems.
Ingineering Intelligent Systems, 3(2):67a77, 1995.
[8] A.-I.
Mouaddib and S. Zilberstein.
Handling Duration Uncertainty in Meta-Level Control for Progressive Reasoning.
In IJCAI-97, pages 1201a1206, 1997.
[9] D. T. R. Morris and K. Ford.
Time and Uncertainaty in Reasoning about Order.
In Proceedings of TIME-95, International Workshop on Temporal Representaion and Reasoning, pages 129a136, 1995.
[10] T. Sandholm.
An Implementation of the Contract Net Protocol Based on Marginal Cost Calculations.
In AAAI, pages 256a262, 1993.
[11] B. Williams.
Doing time: putting qualitative reasoning on firmer ground.
In Proceedings of the 5th National Conference on AI, AAAIa86, pages 105a112, 1986.
A model to perform knowledge-based temporal abstraction over multiple signals A. Oteroa 1 , P. FeElix1 , C.V. Regueiro2 , M. RodrAaEguez1 and S. Barro1 .
1  Dpto.
de ElectroEnica e ComputacioEn Universidade de Santiago de Compostela.
Santiago de Compostela 15782, Spain.
2  Dpto.
ElectroEnica de Sistemas, Universidade de A CorunEa, CorunEa 15071, Spain.
a  Corresponding authoras email: elabra@usc.es  Abstract  This leads to the necessity of having mechanisms for the automation of abstraction processes; mechanisms that have to supply information of greater granularity and higher semantic content [12].
Thus, the decision-maker off-loads a good proportion of the data-interpretation processes, facilitating the quickest and most suitable performance possible.
The aim of the present work is to show a model, MFTP, based on knowledge representation, that makes it possible to perform abstraction over the behaviour of a physical system on the basis of a set of sampled parameters.
This model allows the automatic generation of information that is organized into a hierarchy of levels of abstraction, taking as a starting point the set of sampled parameters that represent the temporal evolution of the system.
In this paper we propose the Multivariable Fuzzy Temporal Profile model (MFTP), which enables the projection of expert knowledge on a physical system over a computable description.
This description may be used to perform automatic abstraction on a set of parameters that represent the temporal evolution of the system.
This model is based on the constraint satisfaction problem (CSP) formalism, which enables an explicit representation of the knowledge, and on fuzzy set theory, from which it inherits the ability to model the imprecision and uncertainty that are characteristic of human knowledge vagueness.
We also present an application of the MFTP model to the recognition of landmarks in mobile robotics, specifically to the detection of doors on ultrasound sensor signals from a Nomad 200 robot.
1  2  Temporal framework  We consider time as being projected on a onedimensional discrete axis D = {t0 , t1 , ..., ti , ...}.
Thus given an i belonging to the set of natural numbers N, ti represents a precise instant.
We assume that t0 represents the temporal origin, before which the existence of any fact is not relevant for the problem under consideration.
We consider a total order relation between them, in such a way that for every i a N, ti+1 a ti = at, where at is a constant.
at is the minimum step of the temporal axis.
Introduction  The proliferation of new and more sophisticated electronic measuring devices, together with improvements in communication processes, enable the development of tasks, such as supervision, in a more reliable and precise manner, by placing exhaustive and ever more complete information at the disposal of the decision-making process.
Nevertheless, the enormous quantity of data to be handled considerably increases the difficulty of this process [7], which frequently leads to problems in the assimilation of input data, and thus, lead to errors [8].
In many cases the solution involves rejecting those data that situate the decision-maker outside the domain of his/her competence.
The control of industrial processes, robotics, and patient supervision are domains that are especially affected by this problem; they require a continuous operation in which representation and reasoning about time is a fundamental key in the solution of problems.
3  Prior definitions  In this section we introduce some basic fuzzy notions, upon which the MFTP model is based.
Given as discourse universe the set of real numbers R, a fuzzy number A is a normal and convex fuzzy subset of R. A fuzzy set A with membership function AlA is normal if and only if a v a R, AlA (v) = 1.
A is said to be convex if and only if av, v  , v  a R, v  a [v, v  ], AlA (v  ) aL min AlA (v), AlA (v  )}.
128  We obtain a fuzzy number A from a flexible constraint given by a possibility distribution DA , which defines a mapping from R, to the real interval [0, 1].
Given a precise number v a R, DA (v) a [0, 1] represents the possibility of A being precisely v. By means of DA we define a fuzzy subset A of R, which contains the possible values of A, being A a disjoint subset, in the sense that its elements represent mutually excluding alternatives for A.
We introduce the concept of fuzzy increment with the aim of representing quantities, such as the difference between two numbers, fuzzy or not.
An increment D is represented by a normal and convex possibility distribution DD , defined over R. Given a pair of fuzzy numbers (A, B) the increment between A and B is given, following Zadehas extension principle [16], by D such that:  be not a major determinant isolated, may well be of interest if it appears related with other findings on other parameters which also do not seem to be definitive taken on their own.
In this sense, the temporal disposition of a set of findings plays an important role in those tasks related with the interpretation problem.
It is reasonable to expect a temporal abstraction tool to be able to handle this knowledge in the representation of a systemas evolution.
This is the most significant step carried out by the MFTP model with respect to its predecessor, the FTP model.
Both FTP and MFTP models will be the tools that enable us to structure the representation of the system information into different levels of abstraction, with the MFTP realizing the identification of those findings defined by the composition or association of other findings.
The MFTP model is based on the formalism of Constraint Satisfaction Problems (CSP) [14], and on the fuzzy set theory.
An MFTP is represented by means of a network of fuzzy constraints between a set of significant points that are defined over the evolution of the different parameters.
The ultimate aim of the model is to identify the occurrence of the pattern M over the evolution of the physical system S, automatically generating information organized in a hierarchy of levels of abstraction.
The system is characterized by a set of parameters P = {P 1 , ..., P n }.
P is obtained by means of an acquisition and sampling proj j , tj[1] ), ..., (v[m] , tj[m] ), ...}.
We cess, such that P j = {(v[1] suppose that over P we have defined an MFTP M, and by N j we denote a FTP that is described over the parameter P j .
For a clearer explanation of the model, we suppose that over each parameter only one FTP is defined.
DD (i) = max min{DA (t), DB (s)} i=tas  We define fuzzy interval by means of initial and final fuzzy numbers, and its extension, which is a fuzzy increment and which represents the difference between the endvalues of the interval.
By I(A,E,D) we denote the interval that is delimited by the values A and E, with a distance between them of D. In order for an interval to have sense, it must start before it can finish.
For this reason we assume that the support of D has to be included in the set of positive numbers; i.e., ai a R, i a$?
0, DD (i) = 0.
In this manner, although the distributions of A and E overlap, the constraint on the length of the interval will reject any assignment to A of a value equal to or bigger than E. When the discourse universe is time the concept of fuzzy number serves to represent fuzzy instant, the concept of fuzzy increment serves to represent the fuzzy temporal extension between two fuzzy instants, and lastly the fuzzy interval serves to represent fuzzy temporal intervals.
Together they will be the entities with which we will model the time in our model.
4  4.1  The FTP Model  The aim of the Fuzzy Temporal Profile (FTP) model is to represent and reason about the evolution of a profile, relative to a single physical parameter P j , which takes real values in time.
The model projects a fuzzy description of the temporal evolution of the parameter onto a fuzzy constraint network between a set of significant points.
The MFTP Model  The Multivariable Fuzzy Temporal Profile model (MFTP) enables the identification, over the temporal evolution of a set of parameters, a pattern M of special significance, described by a human expert, which consists of the appearance of a set of morphologies over each parameter and relations between them.
The MFTP model is an extension of the FTP model [4], which enables the description of a special finding as the temporal evolution of a single physical parameter.
The fact of being able to relate the occurrence of different findings amongst parameters, which is outside the scope of the FTP model, is of great importance, since in most cases the appearance of a finding over a single parameter, which may  Definition 1 We define significant point on a physical parameter P j , Xij , as the pair formed by a variable from the domain Vij and a temporal variable Tij .
A significant point Xij = < Vij , Tij > represents an unknown value Vij for the physical parameter P j at an unknown temporal instant Tij .
In the absence of constraints the variables Vij and Tij may j take any precise value v[m] and tj[m] , respectively, where j (v[m] , tj[m] ) a P j .
By Aji we denote the assignment of precise values from the evolution P j to the variables of Xij ; i.e., Aji = 129  constraints Dij1 i2 we can model linguistic descriptions such as aa slight risea, which describe changes in the magnitude of the physical parameters between significant points.
The significant point X0j =< V0j , T0j > represents the origin of times and values.
We suppose that any fact which happens before T0j is not relevant for the problem under consideration.
This significant point will allow us to convert unary descriptions over the absolute temporal instant, or magnitude, of a significant point (aA little after 5:00 ...a) into binary constraints between this significant point and X0j .
A constraint Mij1 i2 is defined by means of a normal, j convex possibility distribution AlC (Xij1 , Xij2 ) = DiM (m), 1 i2 j M where Di1 i2 (m) represents the possibility of the slope between the points Xij1 and Xij2 taking the precise value m. By definitionMij1 i1 is the universal constraint Du .
Using the constraints Mij1 i2 we model linguistic descriptions of the type athe value rose gentlya, where agentlya is translated by a low slope value.
Figure 1.
Graph corresponding to a FTP.
j j (v[m] , tj[m] ) means that Vij = v[m] and Tij = tj[m] .
A general fuzzy constraint is defined between a set of significant points, providing a computable support to soft descriptions of the form of a signal.
Definition 2 A fuzzy constraint R between a set of significant points X0j , X1j , ..., Xgj is defined by means of a fuzzy relation C = C(X0j , X1j , ..., Xgj ).
C is defined by means of a membership function AlC , which associate a degree of satisfaction of R to each assignment of precise values to the significant points X0j , X1j , ..., Xgj .
Definition 3 A Fuzzy Temporal Profile N j = {X j , Rj } is defined as a finite set of significant points X j = {X0j , X1j , ..., Xnj j } and a finite set of constraints Rj = {Rj1 , ..., Rjf j } between them.
A FTP can be represented by means of a hypergraph, in which the nodes correspond to significant points, and the arcs correspond to constraints (figure 1).
Furthermore, the FTP model enables us to restrict the evolution of a parameter P j between each pair of significant points Xij1 and Xij2 (see figure 2) by means of a membership function AlS j (Aji1 , Aji2 ) which allow us to model i1 i2 some descriptions of language, such as athe ultrasound signal raises slightly during the following 30 secondsa [5].
In the same line, an explanation of how to project on the model the use of fuzzy quantifiers, such as athroughout the last ten seconds the ultrasound signal has been higher than its mean valuea is given in [2].
In order to describe the behaviour of a parameter, a set of constraints limiting the fuzzy temporal duration, fuzzy increment and fuzzy slope between a set of significant points seems to capture a good number of features.
A constraint Lji1 i2 is defined by means of a normal, conj vex possibility distribution AlC (Xij1 , Xij2 ) = DiL1 i2 (l) where j DiL1 i2 (l) represents the possibility of the temporal duration between Tij1 and Tij2 taking the precise value l. By definition DLj (x) =0, ax = 0, and DLj (0) = 1.
In the i1 i1 i1 i1 domain of linguistic variables, these constraints may also correspond to the assignment of a linguistic description qi , from within the set L = {q1 , q2 , ..., qz } of qualitative descriptions of values from the respective discourse universe.
Using the constraints Lji1 i2 we can model temporal relations between significant points, described by means of expressions such as aapproximately a quarter of an hour latera.
In [5], FeElix et al.
propose a language that allows fuzzy constraints between significant points to be generated by applying arithmetic operators to linguistic expressions.
A constraint Dij1 i2 is defined by means of a normal, conj vex possibility distribution AlC (Xij1 , Xij2 ) = DiD1 i2 (d) where j DiD1 i2 (d) represents the possibility of the fuzzy increment between Vij1 and Vij2 taking the precise value d. By definition DDj (x) =0 ax = 0, and DLj (0) = 1.
Using the i1 i1  4.2  The MFTP Model  The MFTP model is an extension of the FTP model which allows constraints between significant points that are defined over different parameters.
We have placed special emphasis on the representation of the temporal disposition of the findings described in the pattern M of the systemas evolution.
Thus, amongst instants we represent those of convex point algebra [15], amongst intervals those defined by Allen [1], as well as relations amongst points and intervals, such as relations between the point and the beginning and ending points of the  i1 i1  130  Figure 2.
The semantics Sij1 i2 restricts the evolution of a parameter between two consecutive significant points.
In the figure only the trajectories which fall into the fuzzy course j described by AlSi1 i2 (Aji1 , Aji2 ) are allowed by the semantic constraint.
Figure 3.
The hypergraph associated to MFTP made up of three morphologies over three different signals.
interval.
Moreover, a representation based on fuzzy sets enables the model to capture the imprecision that is present in quantitative relations between temporal events, which is found in expressions such as aapproximately five minutes latera.
We previously define the constraint dimensionality of R, dim(R), as being the number of different parameters over which the variables of C are evaluated.
Trivially for all Rkj a Rj it holds that dim(Rkj ) = 1.
As has been done for the FTP model, we now go on to define an extensible set of constraints that are habitual in expertas descriptions.
is precisely d. Dij11ij22 constrains the increment in the magnitude between significant points defined over different parameters.
Using the constraints Dij11ij22 we can model linguistic descriptions of the type athe value in sensor 15 before starting to rise is approximately equal to the value in sensor 11 before starting to risea.
Definition 6 A Multivariable Fuzzy Temporal Profile M = {X M , RM } is defined as a finite set of significant points X M = {X0 , X1 , ..., Xn } and a finite set of constraints RM = {R0 , R1 , ..., Rf } between them.
Definition 4 A constraint Lji11ij22 is defined by means of a normal, convex possibility distribution AlC (Xij11 , Xij22 ) = j1 j2 1 j2 DiL1 i2 (h) with j1 = j2 , where DiLj (h) represents the 1 i2 possibility that the fuzzy increment between Tij11 and Tij22 is precisely h. Lji11ij22 constrains the temporal duration between significant points defined over different parameters.
A MFTP M can be decomposed into a set of FTPs a N M = {N 1 , N 2 , ..., N m } and a set of constraints RM = M {Rk a R : dim(Rk ) > 1}.
This decomposition groups the significant points of M into sets belonging to the same FTP: X M = {X01 , ...Xn11 , ..., X0m , ...Xnmm }, where Xij is the i significant point of N j .
We consider, with no loss in generality, that the time origin of all the FTP belonging to a MFTP is the same: T01 = T02 = ... = T0m .
A MFTP can be represented by a hypergraph in which nodes correspond to significant points, and arcs correspond to constraints.
An example of a hypergraph associated to a MFTP is shown in figure 3.
The constraints that we can find between the different parameters may be descriptive, acquired by using linguistic descriptions [5] or by means of some type of graphical tool [9], or they may come from mathematical models of the system.
Using the constraints Lji11ij22 we can model linguistic descriptions of the type aultrasound signal 15 goes back to approximately the initial value a little before ultrasound signal 11 starts to risea.
Definition 5 A constraint Dij11ij22 is defined by means of a normal, convex possibility distribution AlC (Xij11 , Xij22 ) = j1 j2 j1 j2 DiD1 i2 (d) with j1 = j2 , where DiD1 i2 (d) represents the possibility that the fuzzy increment between Vij11 and Vij22 131  Up until now, in applications carried out employing the MFTP model, constraints of a descriptive nature have been the most common and useful.
Nevertheless, in other application domains where there are mathematical models of systems, such as process control, restrictions originated from these models will play a more important role.
Thus, for example, the ideal gas encapsulated in a volume V , subjected to a pressure P and a temperature T verifies that P AV V = K AV T , where K is a constant.
This physical knowledge could be modelled by means of a set of constraints C(XiPP , XiTT , XiVV ) = ViPP AV ViVV a K AV ViTT such that T TV DC (x) = 0 a x = 0, DC (0) = 1, and LP iP iT = LiT iV = VP P T V LiV iP = 0, where XiP , XiT and XiV are significant points defined on the temporal evolution of the variables P , T and V , respectively.
5  whether they give rise to the occurrence of a global pattern.
In this case three levels of increasing abstraction are used: sampled signal, history of morphological events over each parameter, and multivariable pattern occurrences.
Following this approximation, in the first stage occurrences of each FTP N j are searched for over its corresponding evolution P j , thus obtaining a history of the occurrences of each N j .
The degree to which a set of assignments Aj = {Aj1 , ..., Aji } satisfy the set of constraints of a FTP N j is given by: j  D N (Aj ) = min j {D Rk (ARk )}  Where ARk is the projection of Aj over the set of significant points involved in Rk , and D Rk (ARk ) is the degree of j satisfaction of the constraint Rk a R for the assignment of precise values to the set of significant points of X j involved in Rk After this stage we must seek for sets of Aj which fulfil the set of constraints Ra of M , obtaining global solutions A for M .
The degree of satisfaction of the global solution can be calculated on the basis of the degrees of compatibility of each FTP N j with Aj a A by means of the following expression:  Matching  The ultimate aim of the MFTP model is to identify a pattern that is described by an expert over a set of parameters P which describe the temporal evolution of a physical system S. Given that the MFTP model is based on the formalism of constraint networks, comparing a MFTP with P is formally equivalent to resolving a CSP [13], where the domains of the variable are determined by S. A solution to a MFTP M is a set of assignments A = {A0 , A1 , ..., An },where Ai is the assignations of precise values to the significant point Xi a X M , that satisfy the set of constraints RM , with a degree greater than zero.
The conjunctive combination of the fuzzy constraints of M is a fuzzy constraint given by the min operation.
The degree of satisfaction of a solution A is given by: D M (A) =  (1)  Rk aR  j  D M (A) = min {min{D N (Aj )}, j  min  Rk aRMa  {D Rk (ARk )}}  (2) Where ARk is the projection of A over the set of significant points involved in Rk .
Performing the matching in several stages decreases the complexity of this task.
Looking for each morphology means performing the matching of the FTP; thus, in order to search for all the FTPs that make up the MFTP, we must solve m CSP, each with nj variables.
The complexity of the jmax overall task is O(en ), where njmax = max{nj }.
min {D Rk (ARk )}  Rk aRM  Where ARk is the projection of A over the set of significant points involved in Rk , and D Rk (ARk ) is the degree of satisfaction of the constraint Rk a RM for the assignment of precise values given by A.
D M (A) represents the degree of similarity between a fragment of the evolution of S with the MFTP M. We could search for the MFTP as a whole, employing two levels of abstraction in the representation: the sampled signals, and the history of occurrences of M .
Matching a MFTP as a whole may have a high computational complexity: the matching would be equivalent to solving a CSP with n variables, where n is the number of significant points which make up the MFTP.
Imitating human experts, we divide the matching into as many stages as the number of levels of abstraction given by the composition of findings.
Typically, when the experts search for the global pattern over P they initially locate the morphologies over each parameter P j , in order to check  j  After looking for occurrences of each FTP in order to complete the matching, we must look for the occurrences of each FTP that fulfil the set of constraints Ra .
The complexity of this second task is O(em ), so the order of the overall jmax complexity is O(max{en ,em }), while the complexity of m  solving a MFTP as a whole is O(en ) where n = IL ni .
i=1  The disadvantage of performing the matching with this approach is that we cannot be sure of finding the optimal solutions, as local optimal solutions do not have to be part of the optimal global solution.
It is possible to organize the information into more levels of abstraction, where each finding is built from a set of findings from a lower level; e.g., we could initially look for events that form part of a FTP, then look for the FTP over them, and finally we would look for occurrences of the whole MFTP.
132  If we wish to build up complete histories of the occurrence of events of interest at each level of abstraction, it is necessary to perform the matching in as many stages as there are levels of description in the problem.
Besides, in this way it will be possible to give detailed explanations to the human operator as to how the lower-level information has been combined in order to generate higher-level information; and it is more suitable for agent-based implementation, where each agent can take charge of matching each finding, using the results from the previous agents.
5.1  Graphs associated with both FTP and MFTP are not usually dense; furthermore experience has shown us that FTPs frequently have sequential topology (each significant point only has one constraint with its predecessor [4]), so the best algorithm, at least for a major part of real MFTP, is nFC0.
In the nFC0-type Forward Checking algorithm each time that a value is assigned to a variable arc consistency is maintained between those constraints that involve the current variable and one future variable.
In this way there will always be at least one value that is compatible with the current assignment in the domain of the variable that follows according to the assignment order.
The history of N j occurrences is used for a second matching algorithm, once again nFC0, which searches for occurrences of the FTPs that satisfy the constraints that the expert has described between significant points belonging to different parameters: i.e., those such that dim(R) > 1, where the result of this matching phase is a history of the occurrences of the pattern M. The degree of compatibility of a solution A with a MFTP M that is calculated on the basis of the degrees of compatibility of each FTP N j with Aj a A is given by the expression (2).
We must assign a degree of membership of the time instant ti to an occurrence MFTP M .
In order to be able to do this the expert must specify the temporal situation of the instant or interval of interest linked with this MFTP which could be the assignment to the earliest and latest significant points to appear in the entire MFTP, but other options are also available.
Given the significant points Xij11 and Xij22 , which the expert considers to be the end points of the interval (event if Xij11 = Xij22 ) linked with this MFTP, the degree of membership of the time instant ti to M is obtained by searching over P for the set of assignments A which maximizes the degree of satisfaction of the expression (2).
The algorithm employed for the matching  We have conceived an assignment procedure based on a search tree, as our aim is to discard futile assignments following an ordered method.
The implementation currently available of the matching algorithms based on the MFTP model divides the process into two stages.
In the first stage occurrences of each FTP N j are searched for over its corresponding evolution P j , thus obtaining a history of the occurrences of each N j .
The degree to which a set of assignments Aj = {Aj1 , ..., Aji } satisfy the set of constraints of a FTP N j is given by expression (1).
j , tj[m] ) of The degree of membership of the sample (v[m] the evolution of P j to an occurrence of the FTP N j is given by the expression: j  j , tj[m] ) = D N (v[m]  tj1  a$?
max  tj[m]  a$?
,  j  tj j n  D N (Aj )  Where tj1 , tjnj correspond to the samples assigned to the significant points X1j and Xnj j of N j , the first and the last in temporal order.
Amongst the three major types of backtracking algorithms in the bibliography, simple backtracking (BT), backjumping (BJ) and forward checking (FC) [14], it is commonly accepted that the latter behaves better.
In [3] Bessiere shows that the extension of FC algorithms to non-binary constraint problems can be done in six different ways, and gives an algorithm for each extension: nFC0, nFC1, nFC2, nFC3, nFC4 and nFC5 algorithms.
Each of these algorithms maintains a higher level of consistence between the variables which have been assigned a value and the variables which remain without value (past variables and future variables), nFC0 being the one which forces a lesser level of consistency and nFC5 the one which forces a stronger level of consistency.
We have chosen the nFC0 algorithm because, as has been shown in [3], it employs less CPU time than other nFCx algorithms (nFC1 till nFC5) in problems in which the graph is not dense.
In dense CSPs the effort made in checking consistency is compensated by the greater pruning power of the other nFCx algorithms, these beating the nFC0 algorithm.
At medium density problems all algorithms behave similar.
D M (ti ) =  j  max  j  ti 1 a$?
ti a$?
ti 2 1  {D M (A)}  2  Where tji11 and tji22 correspond to the samples assigned to the significant points Xij11 and Xij22 respectively.
6  Practical Case  The MFTP model is currently being applied in two different domains: patient supervision and mobile robotics.
Both domains may benefit from the automation of the abstraction process.
In the former, amongst the numerous applications of the MFTP model, work is currently under way to construct alarms with a better specificity-sensitivity ratio than those presently available in the ICU domain.
To improve this ratio the MFTP based alarms integrate information from several parameters, instead just from a single 133  Figure 4.
Diagram of the environment where the experiment described here was performed.
The trajectory of the Nomad 200 robot and the doors along the route (marked with numbers) can be seen.
Note the position of the ultrasound sensors used for doordetection with regard to the direction of the forward movement of the robot.
Figure 5.
On the left is a signal fragment from ultrasound sensors 11 and 15, where morphologies corresponding to a door can be seen, marked on both signals with the letter D. We show the time interval of interest in the MFTP for door detection, the one limited by te1 and tb2 .
This interval allows the positioning of the door in the spatial dimension.
On the right is the editting of the knowledge base used in the detection of the pattern shown on the left.
parameter, and the temporal evolution of them, instead of being triggered by only present data.
In the domain of mobile robotics the MFTP model can facilitate a knowledge-based interpretation from sensorial data and transformation into high-level information.
One example of this application would be the detection of landmarks, fundamental for the basic task of navigation, and more specifically for the creation and updating of an internal map of the environment, and for locating the robot on this map.
Standing out amongst landmarks of an indoor environment are doors, as, besides serving as references, they establish connections between the different places of the environment, essential information for route-planning.
Our objective is the detection of this type of landmark, using signals from ultrasound sensors.
These type of sensor supplies a signal with a high degree of noise, which rules out the application of techniques that do not tolerate imprecision well.
On the other hand, the detection of the morphology corresponding to the finding of a door on one single sensor is not sufficiently reliable as many false positives appear, which makes the integration of information coming from other sensors essential, in order to increase the specificity of detection.
The Nomad 200 robot has a ring composed by 16 ultrasound sensors, placed each 22.5 degrees.
We have chosen sensors 11 and 15 for the detection as these two sensors present a especially characteristic echo when the robot passes by a door, and is sufficient for a reliable detection.
Sensor 15 is the first to pick up the door (figure 4), its signal showing a morphology consisting of a rapid rise followed by a gentler fall to approximately the initial value (figure 5).
A little later sensor 11 picks up the door (figure 4), its signal showing a morphology which consists of a section of slow rise, followed by a sharp fall back down to approximately the initial value (figure 5).
Figure 5 shows the morphologies of the finding of a door over the sensors of the robot, and the editing of the MFTP that aims to model the finding using a Tool for the Automatic Acquisition and Recognition of Multivariable Patterns [9], which we have developed and which enables an expert to project their knowledge in a highly intuitive manner.
This echoes are produced by the doors frames.
The point in which the ultrasound 15 comes back to the initial value is the point which allows us to locate the second frame of the door (see figure 5), because in this moment the sensor is really sizing the distance from the robot to the door frame, and not a distortion caused by the frame.
In the same way the point where the ultrasound 11 starts raising sharply (see figure 5) is the point which allow us to locate the first door frame.
Therefore, the last significant point of FTP which models the echo of a door over sensor 15, and the first significant point representing of the FTP representing the echo over the ultrasound 11 are the point which will define the begin and end of the finding of interest represented by our MFTP: a door.
Figure 6 shows the result of the matching, performed and visualized with the same tool.
In the detection, 5 of the 7 existing doors were detected with a possibility of 1, and the other two with a possibility of approximately 0.7.
In some cases the morphology corresponding to a door was detected on a single signal, although the integration of both sensors 134  7  Conclusions and future work  In this paper we have presented a model, MFTP, that makes it possible to project an expertas knowledge onto a pattern of signals over a computable model, which is capable of automatically carrying out abstraction over a set of parameters that represent the temporal evolution of a physical system, by organizing information into a set of levels of increasing abstraction.
This abstracted information makes it possible to alleviate the computational load of intrinsically complex tasks such as those of interpretation or diagnosis.
On the other hand, the use of CSP formalism and the theory of fuzzy sets enables the realization of an explicit representation of knowledge, simplifying the revision of the knowledge used as well as strengthening the expertas confidence in the results obtained.
We have also presented a practical application of the MFTP model: the detection of landmarks in mobile robotics.
In the example given the high degree of reliability obtained in spite of signal noise should be noted, as well as the fact that this detection can be carried out in real time.
With regard to future projects, on a practical level, we aim to construct a module that permits the detection of further landmarks over ultrasound and laser sensor signals in mobile robots, and integrate it within an architecture able to carry out multiple tasks [10], such as navigation, mapdrawing, location, etc.
On the theoretical level, we aim to construct a general framework for temporal abstraction in which a fuzzyconstraint-based network makes it possible to integrate multiple signal abstraction techniques, not only the ones based on the MFTP model.
On the other hand we must study the problem of the consistency of the MFTP; due to the redundancy of the descriptions of the expert the MFTP may be inconsistent, that means, there can not be a solution A that is completely possible, so we should ask the expert to revise the MFTP.
It is also possible that despite the knowledge projected onto a MFTP is consistent can be further refined, and some constraints may be tightened, which will be translated into a more efficient matching process.
Figure 6.
Detection of doors, of the map shown in the figure 4, employing ultrasound signals from the Nomad 200 robot.
The numbers indicate to which of the figure 4 correspond each detection.
performed by MFTP model resulted in ruling out the finding (see sensor 11 between doors 5 and 6, for example).
In spite of the theoretically high complexity of the matching algorithms of the MFTP model, their application over problems of signal in the domains mentioned it is satisfactory.
Thus, for example, less than 30 seconds were required to perform the detection shown in figure 6 on a CeleronTM processor at 466 MHz, whilst the robot took three minutes to cover the whole trajectory, due to which it is perfectly feasible to perform this detection in real time.
On the other hand, further heuristic optimizations can be applied to the algorithms which can improve notoriously their performance; e.g., a filter based on particular values can be employed to locate the regions of the signal where a FTP may occur, avoiding scanning the whole signal.
Door-detection problem has also been tackled in [6], where an expert system (ES) is shown with this objective.
The ES attempts to simulate the modus operandi of humans when they visually locate doors on the sonar signals.
In this system a change in the morphology to be detected forces a redesigning of the ES, whilst the MFTP model requires an easy edition of constraints.
8  Acknowledgements  The authors wish to acknowledge the support from the Spanish Comisin Interministerial para la Ciencia y la Tecnologa (CICyT) under project TIC2000-0873-C02.
Another alternative for detecting doors could be to use occupancy grids [11], where door frames could be detected by using a sufficiently small cell-size.
In practice this is not feasible, since this would force us to use a very high number of cells and the storage requirements and computational complexity of the algorithms used by occupancy grids are quadratic to the number of cells.
References [1] J. F. Allen.
Towards a general theory of action and time.
Artificial Intelligence, 23:123a154, 1984.
135  [2] S. Barro, P. FeElix, P. Cariena, and A. Otero.
Extending fuzzy temporal profile model for dealing with episode quantification.
In Systematic Organization of Information in Fuzzy Systems.
NATO, 2001.
[15] M. Vilain and H. Kautz.
Constraint propagation algorithms for temporal reasoning.
In Proceedings AAAIa86, pages 377a382, 1986.
[16] L. A. Zadeh.
The concept of a linguistic variable and its application to approximate reasoning.
Information Science, 8:199a249, 1975.
[3] C. Bessiere, P. Meseguer, E.C.
Freuder, and J. Larrosa.
On forward checking for non-binary constraint satisfaction.
Artificial Intelligence, 141:205a224, 2002.
[4] P. FeElix, S. Barro, and R. MarAaEn.
Fuzzy constraint networks for signal pattern recognition.
Artificial Intelligence, to appear.
[5] P. FeElix, S. Fraga, R. MarAaEn, and S. Barro.
Linguistic representation of fuzzy temporal profiles.
International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 7:243a256, 1999.
[6] M. R. Masliah and R. W. Albrecht.
The mobile robot surrogate method for developing autonomy.
IEEE Transactions on Robotics and Automation, 14:314a 320, 1998.
[7] E. E. Milios and S. H. Nawab.
Signal abstraction in signal processing software.
IEEE Transactions on Acoustics, Speech and Signal Processing, 37:157a 180, 1989.
[8] F. A. Mora, G. Passariello, G. Carrault, and J.P.
Le Pichon.
Intelligent patient monitoring and management systems: A review.
IEEE Engineering in Medicine and Biology, 12:23a33, 1993.
[9] A. Otero.
A tool for automatic acquisition and recognition patterns on signal.
Masteras thesis, Departamento de Electronica e ComputacioEn, Universidade de Santiago de Compostela, 2002.
In Spanish.
[10] C.V. Regueiro, M. Rodrguez, J. Correa, D.L.
Moreno, R. Iglesias, and S. Barro.
A Control Architecture for Mobile Robotics Based on Specialists, volume 6, pages 337a360.
CRC Press, 2002.
[11] M. Ribo and A. Pinz.
A comparison of three uncertainty calculi for building sonar-based occupancy grids.
Robotics and Autonomous System, 35:201a209, 2001.
[12] Y. Shahar.
A framework for knowledge-based temporal abstraction.
Artificial Intelligence, 90:79a133, 1997.
[13] K. Stergiou and M. Koubarakis.
Backtraking algothms for disjunctions of temporal constraints.
Artificial Intelligence, 120:81a117, 2000.
[14] E. Tsang.
Foundations of Constraint Satisfaction.
Academic Press, 1993.
136
Temporal Resolution: Removing Irrelevant Information* Clare Dixon Department of Computing Manchester Metropolitan University Manchester M1 5GD, United Kingdom C.Dixon@doc.mmu.ac.uk  Abstract 'The generation of too m u c h information prohibits e f i c i e n t resolution proof search in classical logics.
Hence subsumption is used t o discard redundant inform a t i o n and strategies have been developed t o guide the proof search avoiding irrelevant information.
T h e extension o f the resolution method t o temporal logics, f o r example that by Fisher f o r propositional linear-time temporal logics, further magnifies this problem.
Here we provide a n algorithm t o eficiently remove irrelevant information prior t o the application of Fisher's temporal resolution rule, show it retains the completeness of the temporal resolution system and demonstrate its e f i c i e n c y o n a set of examples.
1  the number of resolution inferences that can be made and completeness can be maintained if the set of support is chosen so the conjunction of clauses n o t in the set of support is satisfiable.
Likewise, given sets of Horn Clauses, unit resolution is a strategy that is both efficient and complete.
Since the resolution principle was first suggested, extensions to more exotic logics such as temporal logics [15] have been proposed [l,5, 231.
Temporal logics have been used for the specification and verification of properties of concurrent systems, see for example [2, 12, 14, 181.
Proof methods have,also been developed for these logics based on tableau [ll, 241, automata [22, 251 and translation [17].
Resolution based methods have the advantage that they are not limited to finite state problems and, as for resolution in classical logics, strategies can be used to reduce the search space.
The complexity of such logics is a barrier to efficient proof, for example the complexity of satisfiability of PTL, the logic used in this paper, is PSPACE-complete [21].
Again, this motivates the need for strategies to guide proof search, hopefully avoiding the worst case complexity in many situations.
In this paper we examine the resolution method devised by Fisher [lo] for propositional linear-time temporal logic and we describe a way to reduce the amount of information prior to the application of the temporal resolution rule.
Fisher's temporal resolution system depends on three main parts; the translation to a normal form; classical style resolution of formulae that occur within the same state; and temporal resolution over states, of formulae such as Up (always p ) and o ~ (eventually p ~ p ) Fisher's .
method has been shown complete [19] and, as it has just one temporal resolution rule, is particularly suited to mechanization.
Unfortunately, even for small problems a large amount of information is generated, motivating the use of an algorithm t o remove information irrelevant to temporal resolution.
In the rest of the paper we describe propositional  Introduction  Resolution [20] is a decision procedure for classical propositional and first-order logic claimed to be m a chine oriented.
However, the proof of theorems may be slowed or even not finish due to the generation of too much information [28].
This information could either be irrelevant, i.e.
when a clause is produced that is not necessary for the proof or redundant, i.e.
when the information is already in the database [26].
In classical logics, to overcome such problems subsumption is used to remove redundant information while strategies have been suggested to guide resolution based proofs avoiding irrelevant information.
The set of support strategy [28] for example, restricts the application of the resolution rule so that at least one of the parent clauses must be from or derived from the set of support.
Unit resolution [27] only allows resolution inferences to take place if at least one of the parent clauses is a unit clause.
However, the imposition of such strategies may compromise the completeness of the resolution system, the ideal strategy being efficient while maintaining completeness.
For example the set of support strategy is efficient as it restricts *This work was supported partially by an EPSRC PhD Studentship and partially by EPSRC Research Grant GR/K57282  0-8186-7937-9/97 $10.00 0 1997 IEEE  4  temporal logic (PTL), SS2, and explain Fisher's resolution system in more detail, $3.
We present an algorithm to remove irrelevant information before the application of the temporal resolution rule and mention completeness and complexity issues in $4.
In $5 we give experimental results showing in most cases a reduction in the time for this part of the procedure.
Finally in '$6we present our conclusions.
2  lows (a,i) start iff i = 0; (a,i) b O A iff i > 0 and (a,i- 1) F A ; (0, i) O A iff there exists a j 2 i s.t.
( a , j ) A ; ( u l i ) O A iff for all j 2 i then ( a , j ) A; (a,i) AU B iff there exists a k 3 i s.t.
(a,k ) I= B and for all i j < k then ( a , j ) A; (a,i) A W B iff ( a l i ) A U B or (0,i) RA.
+  +  A Linear temporal logic  2.2  A normal form for PTL  Formulae in PTL can be transformed to a normal form, Separated Normal Form (SNF) [lo], which is the basis of the resolution method used in this paper.
While the translation from an arbitrary temporal formula to SNF will not be described here, we note that such a transformation preserves satisfiability and SO any contradiction generated from the formula in SNF implies a contradiction in the original formula.
Formulae in SNF are of the general form  '0'  opi 2  where each Ri is known as a rule and must be one of the following forms.
r  start E 1 Otrue  start  which only holds at the beginning of time.
Models for PTL consist of a sequence of states, representing moments in time, i.e.,  sj  V h,  (an initial U-rule)  b=l  A ka  start Here, each state, si, contains those propositions satisfied in the ith moment in time.
As formulae in PTL are interpreted at a particular moment, the satisfaction of a formula f is denoted by  v r  9  0  =+  lb  (a global 0-rule)  b=l  a=l  =+  01  (an initial 0-rule)  =+  01  (a global 0-rule)  9  O Aka a=l  Here ka, Zb, and Z are literals.
The outer ' 0' operator, that surrounds the conjunction of rules is usually omitted.
Similarly, for convenience the conjunction is dropped and we consider just the set of rules Ri.
We note a variant on SNF called merged-SNF (SNF,) [lo] used for combining rules by applying the following transformation.
I= f  (0,i)  where a is the model and i is the state index at which the temporal statement is to be interpreted.
For any well-formed formula f , model a and state index i, then either ( a , i ) f or ( ~ , i ) f. For example, a proposition symbol, 'p', is satisfied in model a and at state index i if, and only if, p is one of the propositions in state si, i.e., iff  +  The full syntax and semantics of PTL will not be presented here, but can be found in [lo].
Here we summarise the syntax and semantics of the logic used and describe the normal form required for the resolution method.
2.1 Syntax and semantics The logic used in this paper is Propositional Temporal Logic (PTL), in which we use a linear, discrete model of time with finite past and infinite future.
PTL may be viewed as a classical propositional logic augmented with both future-time and past-time temporal operators.
Future-time temporal operators include (sometime in the future), ' 0' (always in the future), '0' (in the next moment in time), ' U ' (until), ' W ' (unless or weak until), each with a corresponding past-time operator.
Since our temporal models assume a finite past, for convenience, we define an operator start in terms of the last-time operator ' 0 '  (a,i) /=p  +  <  +  OA  OB O(AAB)  +3 +  F G FAG  The right hand side of the rule generated may have to be further translated into Disjunctive Normal Form (DNF), if either F or G are disjunctive, to maintain the general SNF rule structure.
p E si.
The semantics of the temporal connectives used in the normal form or the resolution rule are defined as fol-  5  3  The resolution procedure  Here we present a review of the temporal resolution method [lo].
The clausal temporal resolution method consists of repeated applications of both 'step' and 'temporal' resolution on sets of formulae in SNF, together with various simplification steps.
The step and temporal resolution rules are applied until either a contradiction has been detected or no new resolvents can be generated.
Completeness of the resolution procedure has been shown in [19].
3.1  all but the outer level of 0-operators.
So, resolution will be between a 0-rule and a set of rules that together imply an 0-formula which will contradict the +-rule.
Thus, given a set of rules in SNF, then for every rule of the form L + 01 temporal resolution may be applied between this 0-rule and a set of global 0-rules, which taken together force 1 1 always to be satisfied.
The temporal resolution rule is given by the following  Step resolution  'Step' resolution consists of the application of standard classical resolution rule to formulae representing constraints at a particular moment in time, together with simplification rules for transferring contradictions within states to constraints on previous states.
Simplification and subsumption rules are also applied.
Pairs of initial 0-rules, or global 0-rules, may be resolved using the following (step resolution) rule where C1 and Cz are both last-time formulae or both start.
C1 + A v r La + B v l r (C, ACa) + A V B Once a contradiction within a state is found using step resolution, the following rule can be used to generate extra global constraints.
OP start Otrue  +  C  =+  $1  C  +  (AiAo)Wl'  n o=O  with side conditions for all 0  5 i 5 n t Bi  j  v I  11 n  and I-  Bi  Aj  j=O  I  where the side conditions ensure that the set of rules OAi 3 Bi together imply 0-I.
So if any of the Ai are satisfied then 1 1 will always be satisfied, i.e.,  false  =+ 1 P  +  1P  This rule states that if, by satisfying P in the last moment in time a contradiction is produced, then P must never be satisfied in any moment in time.
The new constraints therefore represent 0 l P The step resolution process terminates when either no new resolvents are derived, or false is derived in the form of one of the following rules.
start Otrue  OAo =+ Bo ... ... O A , + Bn  =+ false  +  false  3.2 Temporal resolution During temporal resolution the aim is to resolve a $-rule, C + $ 1 , where C may be either a last-time formula or start, with a set of rules that together imply 0-1,for example a set of rules that together have the effect of O A + 0-1.
However the interaction between the '0' and ' 0' operators in PTL makes the definition of such a rule non-trivial and further the translation from PTL to SNF will have removed  Such a set of rules are known as a-loop in -d.  4  Reducing the rule-set  As the application of the temporal resolution is the most expensive part of the algorithm it is on this we concentrate.
Algorithms for applying the temporal resolution rule are given and compared in [7].
We note that even for small problems, such as the list of valid temporal formulae found in [14], large sets of rules are generated making the search for loops costly.
In this section we describe an algorithm that removes rules, prior to the application of one of the loop search algorithms without affecting the completeness of the overall method.
This is known as rule reduction.
The motivation being that the time taken for rule reduction will be offset by the reduced time required to perform loop search.
IIn previous presentations of this rule two resolvents are given.
However only the resolvent given here is necessary for completeness, so for simplicity we omit the other resolvent.
4.1 Overview We begin by considering a loop in literal 1 1 and  proposition is a proposition in the scope of one negation.
For example in the rule  examine the cases when the deletion of a rule does not ,affect the loop remaining.
An example of a loop for resolution with the eventuality 01 is L O(aAc) o(aAic)  =+ j  a and d are positive propositions and b and c are negative propositions.
Let C+,C-,R+ and 8- be multi-sets of propositions defined as follows.
Throughout, we only consider global 0-rules.
a A 4  ail  formed from the rules R O(aAc)  =+ a  Oa  =+  71  o ( a Ale)  =+  U.  C+ is the multi-set of propositions that occur positively on the left hand side of the set of rules.
C- is the multi-set of propositions that occur negatively on the left hand side of the set of rules.
To get the required "looping" the literal a appears on both the left and right hand sides of rules in R and the proposition c and its negation both occur on the left hand sides of rules in R2.
Finally 1 1 , the literal in which we are looking for a loop, should occur on the right hand side of rules and may occur on the left but its negation, I , should not occur at all.
The idea behind the algorithm is that, given a set of global 0-rules we want to be able t o delete rules that we know will not form part of the loop before actually detecting the loop itself.
For example if we are resolving with 011, given the rule Ox + a and the rule-set R above, as there are no rules with lx on the left hand side or x on the right hand side we can ignore this rule for the purposes of loop search.
The deletion of the rule 0 ( a A c) + a however from rule-set R, means that no loop can be detected in the remaining rules.
The deletion of a rule crucial to the loop is avoided by noting that there is a rule in R with l e on the left hand side to correspond with the c on the left side of this rule (or a c on the right hand side).
Similarly the deletion of O a j 1 1 means that no loop can be detected from the set of rules remaining in R. To avoid this we allow 7 1 (the literal in which we are looking for a loop) to appear anywhere and again make sure that such a rule is only deleted if we can't find an a on the right hand side or -vi on the left hand side of any rule in R. 4.2 Definitions We assume that rules are in their simplest forms, i.e.
all negations are pushed through to propositions and simplified and that all step resolution possible has been carried out.
A positive proposition is a proposition not in the scope of any negations and a negative  R+ is the multi-set of propositions that occur positively on the right hand side of the set of rules.
E- is the multi-set of propositions that occur negatively on the right hand side of the set of rules.
4.3  and the set of global- 0 rules in SNF having performed all the step resolution possible is R. 1.
If 2 occurs in a rule on the right or left hand side delete the rule.
2.
Create the empty multi-sets C+,C-,%+ and %-.
3.
For each rule r E R in turn, add a proposition to the relevant multi-set for each proposition occurring in r as follows.
(a) If p is a proposition occuring positively on the left hand side of r then add p t o C+.
(b) If p is a proposition occuring negatively on  the left hand side of r then add p to 2-.
(c) If p is a proposition occuring positively on the right hand side of r then add p t o 8+.
(d) If p is a proposition occuring negatively on the right hand side of r then add p to 8-.
4.
Remove the proposition 1 from any of the multisets C+, C-,%+ or RI-.
2We could actually perform resolution on the left hand side a and thus an of two of these rules to produce a rule O a equivalent loop 00. a A 4 but in general doing this in practice produces a much larger rule-set.
+  The rule reduction algorithm  Let us assume we are trying to resolve with  +  5 .
Remove rules as follows.
7  b # %+ and b # 2- all the rules with b in a left hand side positive position are deleted and C+ becomes { a ,c}.
We cannot delete the c from %+ as there may be another rule with c in this position.
Now we must recalculate the four sets and can continue the deletions until no rules remain.
4.4 Example Assume we are resolving with L + 01 and we have the following global 0-rules.
If p is in C+ and neither C- or %+ have p as a member then delete all rules with p occurring positively on the left hand side, delete all occurrences of p from C+ and delete a copy of the other propositions occurring in these rules in their respective multi-sets.
If p is in C- and neither C+ or %- have p as a member delete all rules with p occurring negatively on the left hand side, delete all occurrences of p from C- and delete a copy of the other propositions occurring in these rules in their respective multi-sets.
If p is in %+ and p is not a member of C+ then delete all rules with p occurring positively on the right hand side, delete all occurrences of p from %+ and delete a copy of the other propositions occurring in these rules in their respective multi-sets.
6.
Repeat step 5 until no more rules can be deleted.
We delete any rules containing 1 (step 1) as we are looking for a loop in 4.
Recall that we assume that all the step resolution possible has been carried out.
The right hand side of each SNF, rule in the loop implies 1 1 so 1 may not appear on the right hand side of a rule that is combined to make this loop.
The proposition 1 is removed from the multi-sets (step 4) as we are looking for a loop in -1.
Given the rules 0 a + a , O a + 7 1 that together form the loop O a + aA-1, we don't want 1 to occur only in the multi-set 8-and thus delete the latter rule.
Further it does not matter if 7 1 happens to appear in some rules on the left hand side as we know the right hand side of each rule implies 1 anyway.
We use multi-sets and adjust these sets when a rule is deleted to avoid having to re-calculate C+, C-,R+ and %- after every deletion.
For example assume we are looking for a loop in 1 1 from the rules 7  Oa Ob  +  + +  Oa Ob  3.
4.
5.
O(CA1a)  6.  o c  Otrue O(CA-b)  + + + +  w 1w TaVIb c  *  c -4  +  Firstly we build the multi-sets C+, C-, W ,and %taking each rule in turn.
In rule 1, a occurs positively on the left hand side so we add a to the multi-set C+ and w occurs positively on the right hand side so we add w to %+.
The four multi-sets are C+ = { a } , C- = {}, %+ = {w}, and %- = {}.
In rule 2 , b occurs positively on the left hand side so we add b to the multi-set C+ and w occurs negatively on the right hand side so we add w to %-.
The four multi-sets are C+ = { a , b}, C- = {}, %+ = {w}, and %- = {w}.
We continue as above until we have processed all the rules and, having deleted any occurrences of I , we obtain the four multi-sets C+ = { a , b, c, c, c } , C- = { a , b}, %+ = {w, c, e } , and %- = {w, a , b } .
Each multi-set contains propositions of a particular type.
For example the %+ multi-set tells us that the proposition w occurs positively on the right hand side of a rule (rule 1) and that c occurs positively on the right hand side of two rules (rules 4 and 5).
When carrying out deletions, starting with C+ we see a and b are both in C- and c is in %+.
Similarly for C- a and b are both in C+.
Looking at %+ the proposition c occurs in C+ but w does not occur in C+ so we need to remove all the rules with w in the %+ position and adjust all the other multi-sets.
That is, the algorithm tells us that any rule with a positive w on the right hand side can be deleted as it cannot  If p is in %- and p is not a member of 2then delete all rules with p occurring negatively on the right hand side, delete all occurrences of p from R- and delete a copy of the other propositions occurring in these rules in their respective multi-sets.
O(cAa)  1.
2.  form part of the loop.
We see rule 1 is the only rule  containing w in a RJE+ position so remove this rule, delete w from %+ and delete an a , that occurs on the left hand side of rule 1, from C+.
The rules remaining in our set are rules 2-6 and our four multi-sets are C+ = { b , c , c , c } , C- = { a , b } , %+ = { c , c } , and 8- = {w, a, b } .
So, having deleted a rule, propositions may need to be deleted from other multi-sets so that they accurately represent the current set of rules.
Then  a 4  c.  If we construct sets rather than multi-sets, having deleted 1 from the sets, we obtain C+ = { a , b, c } and R+ = { a , c } and C- = 8- = 8.
As b E C+ but  8  the algorithm can be re-applied and more rules may be deleted.
Looking at 93- w doesn't occur in 2- so we delete any rules with w in a right hand side negative position and delete a copy of any other propositions in these rules.
Looking at the rule-set rule 2 is the only rule with iw on the right hand side so we delete this rule leaving rules 3-6, and delete a copy of b from C+.
The multi-sets of propositions become = {c, e, e}, 2- = { a , b}, R+ = {c, c}, and R- = { a , b}.
We cannot delete any more rules so the reduced rule-set becomes rules 3-6.
We may now use a loop finding algorithm to detect any loops.
In this example the loop, from merging rules 3, 4 and 6 and rules 3, 5 and 6, is  0 (CA TU) 0 (C A l b )  4.5  + +  rules matching the data shown.
The third row contains data for all examples where at least one of the timings is greater than 60 milliseconds.
The reason we have chosen to isolate these subsets is that for the former we expect rule reduction to be particularly beneficial where the examples are not trivially small, and the latter to exclude any timing inaccuracies that may occur for timings that are very small.
Subset of Data  ( C A i a A TI!
)V (C A ~b A 11) l a A d)V ( C A i b A 1 1 )  I  (C A  Reduction  > Original  I  Table 1 Summary of Results from Rule Reduction  Completeness and complexity  Completeness is shown by proving that any resolvents from the application of the temporal resolution rule can be generated by applying the rule reduction algorithm and then performing temporal resolution and some step resolution.
For more details see [6].
The complexity of the algorithm will be considered in the full paper.
5  Reduction  6 Original  The loop search used in each case is Breadth-First Search [8] however another method could be used if desired, see for example those described in [7].
The timings of the loop searches are a trade off between the overhead required to remove rules from the rule-set plus the loop finding times on a potentially smaller data set, against the time for finding the loop in the full rule-set.
The timings we have obtained suggest that rule reduction does help on larger, more difficult rule-sets.
We note it is the same two examples where the time for rule reduction and loop finding is greater than the time for loop finding alone in the subsets of the data labelled (ii) and (iii).
The times for these examples for the former are not larger than 30% more than the timings of the latter (in fact the actual figures are 14% and 29%) whereas the gain from using rule reduction on some examples is much greater.
For example in one case the time for performing loop search alone is 3; times worse than for finding the loop with rule reduction.
Results  A prototype implementation performing the temporal resolution method has been built.
The programs are written in SICStus Prolog [4] running under UNIX and timings have been carried out on a SPARCstation 1 using compiled Prolog code.
The test data is a set of valid temporal formulae taken from [14] chosen as it provides a reasonably sized collection of small problems to be proved valid.
An example of the type of formulae being shown valid (we actually show the negation is unsatisfiable) is  OW A UWZ* U(% A UWZ)V U(WZA OW).
Due to space restrictions the complete set of results is omitted here but can be found in [6], however a summary is given in Table 1.
Table 1 gives the number of eventualities where the time for rule reduction followed by loop finding on the resulting rule-set is less than or equal to, or greater than the original.
The figures in brackets are the percentages for these values.
The first row of the table gives the figures for the full data set, subsequent rows take subsets of the data.
The second row contains the data for all examples where the original rule-set contains more than 10  Conclusions and further work 6.1 Conclusions We have given an algorithm for removing rules irrelevant to the application of Fisher's temporal resolution rule and in more than 80% of the examples tried the times for detecting loops are the same or better than without using this algorithm.
If examples with trivially small numbers of rules are ignored then this figure increases to more than 90% of the examples tried.
6  9  Work Bench (LWB) 1131.
Model checking systems are also available for example the STeP system [3] which combines both model checking and deductive methods.
The algorithm is independent of the type of loop search algorithm being used so an implementation could apply the rule reduction strategy whatever loop search algorithm was chosen.
Rule reduction will be most effective in loops that are small in comparison to the size of the rule-set or involve literals that don't often appear in the rest of the rule-set.
However, if the loop consists of most of the rule-set or contains literals that appear frequently throughout the rule-set then rule reduction will not be as effective.
Note that we cannot carry out the loop search just by using the sets of propositions we have constructed as we have lost information relating to individual rules which is necessary to ensure that the conditions for a loop are satisfied.
6.2 Further work Although the rule reduction algorithm seems to help us on the small problems we have tested it would be useful to try the algorithm on larger examples.
For example it would be interesting to see whether the rule reduction algorithm helps tackle problems that are too large for the original theorem prover.
We have considered parallelising the basic temporal resolution method and several loop search algorithms in [9].
Likewise the rule reduction algorithm could also be parallelised.
Firstly the construction of the sets of propositions C+ etc.
could be carried out in parallel by splitting the set of rules into n-sets given n processors and constructing the sets of propositions for each subset.
Then the totals from each processor must be merged.
Finally, the deletion of rules could also be carried out in parallel although care must be taken t o ensure that sets of propositions are maintained correctly.
6.3 Related work Decision procedures based on resolution have been developed for linear-time temporal logics in [l,5, 231, however in many cases they are unsuitable for implementation either because they only deal with a small number of the temporal operators or because of problems with proof direction due to the large numbers of resolution rules that may be applied.
Subsumption is used and strategies developed to assist resolution theorem provers for classical logics, see for example OTTER[16].
For resolution in temporal logics although subsumption rules are generally used to keep the rule-set as compact as possible the author knows of no strategies developed to guide temporal resolution proofs.
Theorem provers for temporal logic that have been implemented tend to be tableau based, for example DP [Ill and a module that forms part of the Logic  Acknowledgements  Thanks to Howard Barringer and Michael Fisher for their guidance and encouragement during this work and to Graham Gough and Martin Peim for many helpful comments and advice.
Thanks again to Michael Fisher for all the suggestions made relating to an earlier draft of this paper.
References M. Abadi and Z.
Manna.
Nonclausal Deduction in First-Order Temporal Logic.
ACM Journal, 37(2):279-317, Apr.
1990.
H. Barringer.
Using Temporal Logic in the Compositional Specification of Concurrent Systems.
In A. P. Galton, editor, Temporal Logics and their Applications, chapter 2, pages 53-90.
Academic Press Inc. Limited, London, Dec. 1987.
N. Bjorner, A. Browne, E. Chang, M. Col&, A. Kapur, Z.
Manna, H. B. Sipma, and T. E. Uribe.
STeP The Stanford Temporal Prover Educational Release Version 1.0 User's Manual.
Computer Science Department, Stanford University, California 94305, Nov. 1995.
M. Carlsson and J.
Widen.
SICStus Prolog User's Manual.
Swedish Institute of Computer Science, Kista, Sweden, Sept. 1991.
A. Cavalli and L. Farifias del Cerro.
A Decision Method for Linear Temporal Logic.
In R. E. Shostak, editor, Proceedings of the 7th International Conference on Automated Deduction, volume 170 of Lecture Notes in Computer Science, pages 113-127.
Springer-Verlag, 1984.
C. Dixon.
Strategies f o r Temporal Resolution.
PhD thesis, Department of Computer Science, University of Manchester, 1995.
[7] C. Dixon.
Search Strategies for Resolution in Temporal Logics.
In M. A. McRobbie and J. K. Slaney, editors, Proceedings of the Thirteenth International Conference on Automated Deduction (CADE), volume 1104 of Lecture Notes in Artificial Intelligence, pages 672-687, New Brunswick, New Jersey, July/August 1996.
Springer-Verlag.
10  [8] C. Dixon.
Temporal Resolution: A Breadth-First Search Approach.
In Proceedings of TIME-96 the Third International Workshop on Temporal Representation and Reasoning, Key West, Florida, May 1996.  the Thirteenth International Conference on Automated Deduction (CADE), volume 1104 of Lecture Notes in Artificial Intelligence, pages 598612, New Brunswick, New Jersey, July/August 1996.
Springer-Verlag.
[9] C. Dixon, M. Fisher, and R. Johnson.
Parallel Temporal Resolution.
In Proceedings of TIME-95 the Second International Workshop on Temporal Representation and Reasoning, Melbourne Beach, Florida, Apr.
1995.
[18] S. Owicki and L. Lamport.
Proving Liveness Properties of Concurrent Programs.
ACM Transactions on Programming Languages and Systems, 4(3):455-495, July 1982.
[19] M. Peim.
Propositional Temporal Resolution Over Labelled Transition Systems.
Unpublished Technical Note, Department of Computer Science, University of Manchester, 1994.
[lo] M. Fisher.
A Resolution Method for Temporal Logic.
In Proceedings of the Twelfth Interna-  tional Joint Conference on Artificial Intelligence (IJCAI), Sydney, Australia, Aug. 1991.
Morgan Kaufman.
[20] J.
A. Robinson.
A Machine-Oriented Logic Based on the Resolution Principle.
ACM Journal, 12(1):23-41, Jan. 1965.
[11] G. D. Gough.
Decision Procedures for Temporal Logic.
Master's thesis, Department of Computer Science, University of Manchester, October 1984.
Also University of Manchester, Department of Computer Science, Technical Report UMCS89-10-1.
[21] A. P. Sistla and E. M. Clarke.
Complexity of Propositional Linear Temporal Logics.
ACM Journal, 32(3):733-749, July 1985.
[22] M. Y. Vardi and P. Wolper.
An AutomataTheoretic Approach to Automatic Program Verification.
In Proceedings IEEE Symposium on Logic in Computer Science, pages 332-344, Cambridge, 1986.
[12] B. T .
Hailpern.
Verifying Concurrent Processes Using Temporal Logic, volume 129 of Lecture Notes in Computer Science.
Springer-Verlag, 1982.
[23] G. Venkatesh.
A Decision Method for Temporal Logic based on Resolution.
Lecture Notes in Computer Science, 206:272-289, 1986.
[13] G. Jaeger, A. Heuerding, S. Schwendimann, F. Achermann, P. Balsiger, P. Brambilla, H. Zimmermann, M. Bianchi, K. Guggisberg, and W. Heinle.
LWB-The Logics Workbench 1.0.
[24] P. Wolper.
The Tableau Method for Temporal Logic: An overview.
Logique et Analyse, 110111:119-136, June-Sept 1985.  http://lwbwww.unibe.ch:8080/LWBinfo.html.
University of Berne, Switzerland.
[25] P. Wolper, M. Vardi, and A. P. Sistla.
Reasoning about Infinite Computation Paths.
In Proceedings of the Twentyfourth Symposium on the Foundations of Computer Science.
IEEE, 1983.
[14] Z.
Manna and A. Pnueli.
Verification of Concurrent Programs: The Temporal Framework.
In R. S. Boyer and J .
S. Moore, editors, The Correctness Problem in Computer Science, pages 215273.
Academic Press, London, 1981.
[26] L. Wos.
Automated Reasoning : 33 Basic Research Problems.
Prentice-Hall, Englewood Cliffs, New Jersey, 1988.
[15] Z.
Manna and A. Pnueli.
The Temporal Logic of Reactive and Concurrent Systems: Specification.
Springer-Verlag, New York, 1992.
[27] L. Wos, D. Carson, and G. Robinson.
The Unit Preference Strategy in Theorem Proving.
In Proceedings of AFIPS Fall Joint Computer Conference, pages 615-621.
Thompson Book Company, 1964.
[16] W. W. McCune.
OTTER 2.0 Users Guide.
Argonne National Laboratory, 9700 South Cass Avenue, Argonne, Illinois 60439-4801, March 1990.
ANL-9019.
[17] A. Nonnengart.
[28] L. Wos, G. Robinson, and D. Carson.
Efficiency and Completeness of the Set of Support Strategy in Theorem Proving.
ACM Journal, 12:536-541, Oct. 1965.
Resolution-Based Calculi for  Modal and Temporal Logics.
In M. A. McRobbie and J. K .
Slaney, editors, Proceedings of  11
(c)2006 IEEE.
Personal use of this material is permitted.
However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from the IEEE.
This material is presented to ensure timely dissemination of scholarly and technical work.
Copyright and all rights therein are retained by authors or by other copyright holders.
All persons copying this information are expected to adhere to the terms and constraints invoked by each author's copyright.
In most cases, these works may not be reposted without the explicit permission of the copyright holder.
How Would You Like to Aggregate Your Temporal Data?
Michael H. Bohlen Johann Gamper Faculty of Computer Science Free University of Bozen-Bolzano, Italy {boehlen, gamper}@inf.unibz.it  Abstract Real-world data management applications generally manage temporal data, i.e., they manage multiple states of time-varying data.
Many contributions have been made by the research community for how to better model, store, and query temporal data.
In particular, several dozen temporal data models and query languages have been proposed.
Motivated in part by the emergence of non-traditional data management applications and the increasing proliferation of temporal data, this paper puts focus on the aggregation of temporal data.
In particular, it provides a general framework of temporal aggregation concepts, and it discusses the abilities of five approaches to the design of temporal query languages with respect to temporal aggregation.
Rather than providing focused, polished results, the paper's aim is to explore the inherent support for temporal aggregation in an informal manner that may serve as a foundation for further exploration.
1  Introduction  Most applications of database technology are temporal in nature.
Examples include financial applications such as accounting, and banking; a broad range of record-keeping applications such as personnel, medical-record, and inventory management; scheduling applications such as airline, train, and hotel reservations and project management; and scientific applications such as weather monitoring.
Applications such as these rely on temporal databases, which record time-referenced data.
A database models and records information about a part of reality, termed either the modeled reality or the miniworld.
Aspects of the mini-world are represented in the database using a variety of structures or database entities-- in the relational model, tuples are used.
We will generally use the term fact for the logical statements about the miniworld that are recorded in the database.
Different temporal aspects may be associated with the  Christian S. Jensen Department of Computer Science Aalborg University, Denmark csj@cs.aau.dk  facts stored in the database [12, 29].
Most importantly, the valid time of a fact is the collected times--possibly spanning the past, present, and future--when the fact is true in the mini-world.
Valid time thus is used when capturing the time-varying states of the mini-world.
All facts have a valid time by definition.
However, the valid time of a fact may not necessarily be recorded in the database, for any of a number of reasons.
For example, the valid time may not be known, or recording it may not be relevant.
Next, the transaction time of a database entity is the time when the entity is current in the database.
Like valid time, this is an important temporal aspect.
Transaction time is the basis for supporting accountability and "traceability" requirements, which exist in many applications, e.g., financial and medical applications.
The valid and transaction time values of database entities are drawn from some appropriate time domain.
There is no single answer to how to perceive time in reality and how to represent time in a database.
For example, the time domain may or may not stretch infinitely into the past and future; and time may be perceived as discrete, dense, or continuous.
In databases, a finite, discrete, and totally ordered time domain is typically assumed, e.g., in the SQL standards.
Temporal data management can be very difficult using conventional (non-temporal) data models and query languages [24, 30, 33].
These provide little built-in support for managing such data, thus unnecessarily complicating database application development and leading to ineffective and inefficient ad-hoc solutions that must be reinvented each time a new application is developed.
As a result, data management is currently an overly involved and error-prone activity.
Temporal database research [3, 9, 10, 12, 37, 40] has produced several dozen proposals for temporal data models [31, 38] and query languages [4, 21, 22, 34, 39] that aim to remedy this situation.
This paper focuses on an increasingly important area of temporal data management, namely aggregation (e.g., [2, 6, 14, 23, 36, 41]).
Aggregation gains in prominence in step with the increasing proliferation of temporal data and diffusion of business intelligence applications.
In aggrega-  base department; the contract ID is 140, and Joe's monthly salary is 1200.  tion, an argument relation is transformed into a summary result relation.
This is traditionally done by first partitioning the argument relation into groups of tuples with identical values for one or more attributes, then applying an aggregate function, e.g., count, to each group in turn.
We advocate a framework that generalizes traditional aggregation and offers orthogonal support for two aspects of aggregation [1, 2]: a) the definition of result groups for which to report one or more aggregate values and b) the definition of aggregation groups, i.e., collections of argument tuples that are associated with the result groups and over which the aggregate functions are computed.
When aggregating temporal data, the time intervals to be associated with result tuples can depend on the actual data and are not known in advance.
Taking its outset in this framework, the paper explores the support for temporal aggregation in existing SQLbased temporal query languages that are based on tupletimestamped valid-time data models.
In particular, the paper considers five approaches to temporal query languages.
As a vehicle for exploring each approach and for illustrating aspects of its inherent support, or lack thereof, for temporal aggregation, the paper considers the formulation of four aggregation queries in an SQL-based temporal query language that is prototypical for the approach.
The findings for each approach are highlighted as observations that may serve as a basis for further study.
To cover as many concepts as possible, the paper omits formal detail and is kept relatively informal.
Section 2 introduces an example, which is used throughout the paper and motivates the need for extensions of the SQL language to support temporal aggregation.
Section 3 covers the notions of point-based and interval-based temporal data models.
Section 4 then describes the paper's framework of temporal aggregation concepts.
Section 5 proceeds to cover the different query language approaches.
Finally, Section 6 summarizes, and Section 7 offers an outlook for temporal aggregation research.
2  Argument relation (Joe, 140, DB, 1200) (Dan, 141, DB, 700)  (Dan, 150, DB , 700)  (Tim, 143, AI , 2000)  2003/01  2003/04  2003/07  2003/10  2004/01  Instantaneous count aggregation (DB, 2)  (DB, 2)  (DB, 1)  (AI , 1)  Figure 1.
Temporal Aggregation Given the prevalence of relational data management applications that manage time-varying data, one might question the need for a temporal query language.
Is the existence of these applications not proof that SQL is sufficient for writing such applications?
Put briefly, the reality is that in conventional query languages like SQL, temporal aggregation queries can be expressed, but in many cases only with great difficulty.
To illustrate the issue, consider the following aggregation query over relation E MPL that expresses the total number of contracts: select count(*) as Cnt, [min(Ts),max(Te)] as T from Empl  Here, Ts and Te denote the start and end point of a timestamp T , respectively, and min and max are well-known SQL aggregate functions.
As we are in a temporal context, we choose to return a temporal relation, even if this query might also be interpreted as a non-temporal query.
Hence, we assign the interval that lasts from the earliest start point to the latest end point of any argument tuple to the result.
The temporal generalization of this query, asking now for the time-varying count of contracts, as recorded in relation E MPL, is non-trivial to formulate.
The intended result is shown in the lower part of Figure 1.
Although possible, expressing this query in SQL is difficult.
Although the two first tuples of the DB department have the same values for the non-temporal attributes, we do not combine them into one, since they have different lineage: different sets of contracts are responsible for these two result tuples.
Reporting these two tuples instead of one yields a more informative result.
The point is that conceptually quite reasonable queries on temporal relations can be difficult to express using a query language such as SQL.
Even SQL experts would be hard pressed to express the example temporal query in SQL.
Motivating Example  As a vehicle for illustration throughout the paper, consider the employee database in Figure 1.
Relation E MPL captures work contracts with employees, recording for each contract the name of the employee who holds the contract (N ), an identifier for the contract (CID), the department to which the employee is assigned for the duration of the contract, the monthly salary for the contract period (S ), and the valid time of the contract (T ).
The upper part of Figure 1 graphically illustrates relation E MPL, which contains four tuples.
The valid time periods of the tuples are indicated by horizontal lines.
For example, the first tuple states that Joe has a contract with the data2  N CID D S Joe 140 DB 1200 *** Joe 140 DB 1200 Dan 141 DB 700 Dan 141 DB 700 Dan 141 DB 700 Dan 141 DB 700 Dan 141 DB 700 Dan 150 DB 700 Dan 150 DB 700 Dan 150 DB 700 Dan 150 DB 700 Dan 150 DB 700 Dan 150 DB 700 Dan 150 DB 700 Dan 150 DB 700 Dan 150 DB 700 Dan 150 DB 700 Tim 153 AI 1800 *** Tim 153 AI 1800  Given also the ubiquitous nature of temporal data, this indicates a strong need for temporal support beyond what SQL offers today.
We proceed to explore in more detail the meaning of the association of a time interval with a tuple.
3  Point-Based and Interval-Based Models  The data model underlying a query language specifies the data structures that the query language manipulates.
The numerous proposals for data models may be characterized according to a variety of criteria.
Within our scope of tuple-timestamped data models that capture valid time, we proceed to describe two types of data models, namely point-based and interval-based models.
The former type of model inherently associates facts with time points, while the latter inherently associates facts with intervals.
3.1  Point-Based Temporal Data Models  T 2003/01 2003/12 2003/01 2003/02 2003/03 2003/04 2003/05 2003/06 2003/07 2003/08 2003/09 2003/10 2003/11 2003/12 2004/01 2004/02 2004/03 2003/04 2003/09  (a) Relation E MPL  The perhaps most basic type of temporal data model is capable of associating a fact with a set of time instants, or points.
This association may be achieved by timestamping each tuple with a single time point.
Thus, if a fact is valid at several points in time, several so-called value-equivalent tuples (tuples that only differ in their timestamp) are used for capturing it, one for each time point.
As an illustration, part of the E MPL relation together with the result of the instantaneous temporal aggregation query [16, 23] from the previous section is shown in Figure 2, where timestamp attribute T stores valid time points at the granularity of months.
With point timestamping, syntactically different relations have different information content.
Next, timestamps are atomic values that are easy to compare and manipulate.
Assuming a totally ordered time domain, a standard set of comparison predicates, e.g., =, 6=, <, >, <=, and >=, is sufficient to conveniently compare timestamps.
Point timestamping is often considered only as a basis for the design of query languages and is not meant for physical representation.
Indeed, for all but the most trivial time domains and facts, the space needed when using the point model is prohibitive.
Point timestamps are also rarely a user-friendly format for the display of temporal relations.
Due to their simplicity, point timestamped temporal data models have been popular in theoretical studies, including constraint databases (cf., e.g., [7, 13, 25, 26, 27]).
Another approach to associating facts with time points is to timestamp tuples with sets of time points or with socalled temporal elements, which are finite unions of time intervals.
With these representations, a relation does not con-  D Cnt T DB 2 2003/01 DB 2 2003/02 DB 2 2003/03 DB 2 2003/04 DB 2 2003/05 DB 2 2003/06 DB 2 2003/07 DB 2 2003/08 DB 2 2003/09 DB 2 2003/10 DB 2 2003/11 DB 2 2003/12 DB 1 2004/01 DB 1 2004/02 DB 1 2004/02 AI 1 2003/04 AI 1 2003/05 AI 1 2003/06 AI 1 2003/07 AI 1 2003/08 AI 1 2003/09 (b) Instantaneous Aggregation  Figure 2.
Point Timestamping  tain value-equivalent tuples--all value-equivalent tuples in the corresponding point timestamped relation are combined into one tuple, with a timestamp that captures all the time points of those tuples.
Yet another approach to associating facts with time points is to timestamp tuples with intervals.
Multiple tuples are then needed if a fact is valid over a non-convex set of time points.
Figure 3(a) illustrates the approach, where the timestamp attribute T = [Ts , Te ] stores a valid time interval represented by its inclusive start and end point, respectively.
When employing intervals rather than time points as timestamps, two timestamps satisfy precisely one of the following thirteen relationships, first enumerated by James Allen: before, meets, overlaps, during, starts, finishes, and equal, in addition to the inverses of the first six of these.
Allen's pioneering work in this area has inspired designs of many of the collections of interval predicates available in temporal query languages.
While well-chosen interval predicates are more convenient to use than relationships over interval start and end points, such predicates alone turn out to not be sufficient to provide comprehensive and easyto-use support for temporal data management in general and the point-based view in particular.
As we will see in Section 5, using interval timestamps as compact representations of sets of time points has the effect of making some conceptually simple queries cumbersome 3  same values, unless an additional attribute such as a contract identifier is introduced.
For example, in Figure 3(a), attribute CID allows us to separate tuples two and three, for which all other non-timestamp attributes have the same value.
So in the interval-based models that we will use as our outset, intervals are not merely representational devices-- they carry meaning beyond denoting sets of points.
Returning to the example, two consecutive contracts with the same values are clearly different from a single contract over the whole period.
They require no additional attribute to identify the contracts, and the instance in Figure 3(b) is the appropriate representation of our result relation.
to formulate.
The notion of snapshot equivalence, which reflects a point-based view of data, establishes a correspondence between interval timestamped relations.
Consider the two instances of the result relation in Figure 3.
The relations are different, but snapshot equivalent, meaning that they contain the same snapshots.
Specifically, the relation in Figure 3(c) is a coalesced version of the relation in Figure 3(b).
In coalescing, value-equivalent tuples with adjacent or overlapping time intervals are merged.
N Joe Dan Dan Tim  CID 140 141 150 143  D DB DB DB AI  S 1200 700 700 2000  T [2003/01,2003/12] [2003/01,2003/05] [2003/06,2004/03] [2003/04,2003/10]  4  (a) Relation E MPL  D Cnt T DB 2 [2003/01,2003/05] DB 2 [2003/06,2003/12] DB 1 [2004/01,2004/03] AI 1 [2004/04,2004/09]  D Cnt T DB 2 [2003/01,2003/12] DB 1 [2004/01,2004/03] AI 1 [2004/04,2004/09]  (b) Result Relation: Instance 1  (c) Result Relation: Instance 2  We proceed to describe a general framework for temporal aggregation and then consider four examples of temporal aggregation.
4.1  Temporal Aggregation Framework  As an outset for the temporal aggregation framework, recall that Klug's (and SQL's) conventional framework for non-temporal aggregation performs aggregation on an argument relation according to two parameters [17]:  Figure 3.
Interval Timestamping  3.2  Temporal Aggregation  1. a set of attributes drawn from the argument relation, termed grouping attributes; and  Interval-Based Temporal Data Models  Interval-based temporal data models associate facts with intervals.
In such models, intervals are not just compact representations of time points.
While several different types of timestamps may be used in point-based temporal data models, it is most natural to use interval timestamps for interval-based models (although timestamps that are sets of intervals could also be considered).
To illustrate the difference between point- and intervalbased models, recall the query result displayed in the lower part of Figure 1.
This result contains more information than what is given in the point-timestamped result displayed in Figure 2(b).
Put differently, this result cannot be reconstructed from the result in Figure 2(b).
(An additional attribute, such as CID in relation E MPL, may perhaps be used for this purpose.
The next section will return to this issue.)
As yet another manifestation of the extra information captured by interval-based models, snapshot equivalent relations may have different information content in such models, as is the case in Figures 3(b) and 3(c).
The two relations are different, also from a semantic point of view.
Further, it is not appropriate to require relations in interval-based models to be coalesced.
This would imply that it is not possible to distinguish between two consecutive contracts with the  2. a set of pairs of a new attribute name and an aggregation function.
The tuples in the argument relation are partitioned according to their values for the grouping attributes.
Then for each partition, each aggregate function given in the second parameter is computed on the tuples in the partition, and the result is stored as a value of the associated attribute for each tuple in the partition.
Finally, the non-grouping attributes of the argument relation may be eliminated from the result by means of a projection using relational algebra.
We propose a temporal aggregation framework that generalizes the non-temporal one in two important respects.
Instead of partitioning the tuples in the argument relation according to their values for certain of their attributes, we introduce a separate grouping table that contains a tuple for each group to be represented in the query result.
This table generally has as attributes a subset of the attributes of the input relation, the timestamp attribute being one of them.
Additional, new attributes may also be included.
Second, we introduce a parameter that maps tuples from the input relation to tuples in the grouping table.
This mapping function may assign the same argument tuple to zero, one, or many groups.
This differs from the conventional 4  framework, where each input tuple is mapped to exactly one group.
The new framework retains the second parameter from the conventional framework.
The resulting framework generalizes the specification of result groups, it generalizes the mapping of input tuples to result groups, and it decouples the specification of result groups from the mapping of input tuples to the result groups.
An important aspect of the framework is that the values for the timestamp attribute in the tuples in the grouping relation may be either fixed or inferred from the data in the input relation.
The case of fixed intervals corresponds to how the non-timestamp attribute values are treated: they must be provided explicitly.
The case of inferred intervals is specific to the timestamp attribute.
An inferred interval is calculated as the intersection of the intervals associated with the argument tuples that contribute to the aggregate results to be associated with the group, or grouping tuple, that the inferred intervals applies to.
These inferred intervals are termed constant because there are no changes in the argument relation during these intervals.
Constant intervals are non-overlapping and maximal.
4.2  Thus, for each group, the result contains a tuple for each aggregate value and each constant interval associated with that value.
For example, the group for department DB has two aggregate values (1 and 2), and aggregate value 2 holds for two constant intervals.
Query Qfi (fixed intervals): For each department, how many contracts were in effect during each half-year?
D DB DB DB AI AI AI  The following queries together with their intended results build on the employee database.
They serve to illustrate the concepts in the aggregation framework and will also be used for illustration in the rest of the paper.
The result relations of the queries are illustrated in graphical form in Figure 4.
Query Qcum (cumulative aggregation): At each time, what is the number of contracts within the last three months?
Query Qci (constant intervals): For each department, what is the time-varying number of contracts?
Cnt 2 2 1 1  T [2003/01,2003/06] [2003/07,2003/12] [2004/01,2004/06] [2003/01,2003/06] [2003/07,2003/12] [2004/01,2004/06]  Query Qfi has the same non-temporal part as Qci , but here the query explicitly specifies fixed time intervals [2] over which to evaluate the non-temporal aggregation.
As in the previous example, the grouping table has the department and the timestamp as its attributes.
However, now the timestamp attribute values are specified explicitly.
For each department, there are three half-year intervals during which contracts are in effect.
The grouping table therefore contains six tuples.
For each of the resulting six groups, a count of contracts is computed by considering all contracts that match the department value for the group and have a timestamp that overlaps the six-month period of the group.
Example Queries  D DB DB DB AI  Cnt 3 2 1 1 1 0  Cnt 2 3 4 3 2 1  T [2003/01,2003/05] [2003/06,2003/12] [2004/01,2004/03] [2003/04,2003/09]  T [2003/01,2003/03] [2003/04,2003/05] [2003/06,2003/07] [2003/08,2003/11] [2003/12,2004/02] [2004/03,2004/05]  Query Qcum is a cumulative aggregation query, also termed a moving-window query [41, 42].
It slides along the time line, computing at each time point an aggregate that takes into consideration all tuples that were valid at some point during the past three month.
In general, the value of a cumulative aggregate at time point t is computed over all tuples whose valid intervals overlap with the interval [t-w, t], where w is the window offset.
In the result relation, tuples over consecutive time points that have the same aggregate value and identical lineage information are coalesced.
In this query, the grouping table  Query Qci is an example of an instantaneous aggregation [16, 23] that must be applied to each database state.
To compute the result at a specific time point, all tuples that are valid at that time point are considered.
The attributes of the grouping table for this query are the department attribute D and timestamp attribute T .
The table has two tuples, namely one with DB and one with AI as its D value.
These tuples have unspecified timestamps, as these are inferred as the constant intervals from the argument relation.
5  (Joe, 140, DB, 1200) (Dan, 141, DB, 700)  (Dan, 150, DB, 700)  Argument relation (Tim, 143, AI , 2000)  2003/04  2003/01  2003/07  (DB, 2)  2003/10  2004/01  (DB, 1)  (DB, 2)  Result of Qci (AI , 1)  (DB, 3) Result of Qfi  (AI , 1)  (2)  Result of Qcum  (DB, 1)  (DB, 2) (AI , 1)  (3)  (4)  (3)  (2)  (1)  (4)  Result of Qnt  Figure 4.
Temporal Aggregation Results Because the focus is on the inherent properties of the approaches, we gloss over semantic variations among temporal constants and predicates (e.g., overlaps), and we introduce additional functions (e.g., duration) into the prototypical languages as needed.
We even permit ourselves to be liberal with respect to available language constructs, to the extent that this is helpful in better representing the approaches.
We proceed to first discuss building blocks that will prove helpful in formulating temporal aggregation queries in several of the approaches covered.
Then each approach is covered in turn.
has only one attribute, the timestamp, which is inferred from the argument tuples.
The argument tuples associated with a group are all those tuples that were valid within the past three months.
Query Qnt (non-temporal aggregation): number of contracts in total?
What is the  Cnt 4 In Query Qnt , the aggregation is to be applied to the entire relation independently of any temporal information, producing one result tuple that contains the total number of contracts in the database.
Thus, the grouping table is empty.
5.1  There are a few concepts that are fundamental when expressing temporal aggregation queries and that are either not supported or barely supported in current temporal query languages.
These concepts concern the computation of the timestamps for the result tuples that depend on the data in the argument relation and possibly also on the query.
We present these concepts next and use them in the subsequent analysis of query languages.
In the following, we discuss the support for temporal aggregation inherent in different approaches to temporal query language design, using these four queries as examples.
5  General Building Blocks  Analysis of Temporal Query Languages  This section discusses the support for temporal aggregation inherent in five distinct approaches to temporal query language design.
To be specific, the section bases its discussion of each approach on a specific temporal extension to the SQL query language that is prototypical to the approach.
Computation of Constant Intervals.
Query Qci requires the computation of constant intervals, i.e., the intervals over which the sets of argument tuples do not change.
Expressing these intervals in SQL is possible, but unreasonably complicated, as illustrated by the following solution that 6  uses two views.
The function sem(T) takes as argument a chronon and returns the semester to which this chronon belongs.
For example, sem(2003/01) returns the first semester in 2003 represented as an interval: [2003/01, 2003/06].
Again, note that the non-temporal grouping attributes of the specific query have to be considered, which precludes a general solution for all queries with fixed intervals.
create view EndPoints (D,TP) as select distinct D, Ts as TP from Empl union select distinct D, Te as TP from Empl create view CI (D,T) as select a.D, [a.TP,b.TP] as T from EndPoints as a, EndPoints as b where a.D = b.D and a.TP < b.TP and not exists( select * from EndPoints as c where a.TP < c.TP < b.TP) and exists( select * from Empl as d where overlaps(d.T,[a.TP,b.TP]))  Timestamp Generation.
Another non-standard feature that facilitates the formulation of temporal statements is the availability of generative constructs: general functions that return sets of values that are then further processed.
Examples include functions that generate all time points included in an interval or all semesters covered by an interval.
Being more precise, we assume a user-defined function f that takes as input a timestamp T and returns a set of intervals, i.e., f (T ) = {I1 , .
.
.
, Im } This function can then be used in the query language and has the following semantics:  The view EndPoints(D,TP) is defined by the argument relation and determines all distinct start and end points of the argument tuples grouped by department.
These time points are the end points of the constant intervals.
The view CI(D,T) is defined over these end points and extracts those combinations of end points that form the valid constant intervals over which the result tuples are defined.
Two end points t and t' of the argument relation form a constant interval [t, t' ] if there are no end points in-between and there is an argument tuple that overlaps with the time interval [t, t' ].
Note that the computation of the constant intervals needs to take into consideration the non-temporal grouping attributes, and hence, the above SQL statement depends on the query.
Moreover, the expression is not only syntactically complicated, but also expensive to compute.
SQL(f (T )) [?]
SQL(I1 ) [?]
* * * [?]
SQL(Im ) That is, we evaluate the SQL query for each of the intervals returned by f and take the union of the result tuples.
5.2  Approach I: Abstract Data Types  The earliest and, from a language design perspective, simplest approach to improving the temporal data management capabilities of a query language is to introduce time data types and associated predicates and functions.
Observation 1 Adding a new ADT to SQL is attractive because it has limited impact on SQL and because the extension of SQL with new data types with accompanying predicates and functions is fairly well understood.
Chron Relation.
An abstract unary Chron relation has been proposed that has a single temporal attribute that stores all possible chronons (time points) of the temporal universe [39].
Such a Chron relation is helpful when expressing a broad range of queries.
For the use of this relation to be practical, implementation level solutions have to be developed that do not require the materialization of the Chron relation.
Consider query Qfi , which explicitly involves the periods during which a result tuple is expected, i.e., every semester where a tuple is valid.
Using the Chron relation, we can construct these semesters as follows:  Formulations of predicates on time-interval data types have been influenced by Allen's 13 interval relationships.
With reference to these, different sets of practical proposals for predicates have been proposed.
To illustrate this approach, we assume that the employee relation is represented by the interval-timestamped relation in Figure 3.
QSQL ci : As mentioned in Section 2, expressing a timevarying aggregation as in Qci is possible, but there exists no reasonable SQL solution.
Using the views discussed above, we can express Qci as follows: select a.D, count(*) as Cnt, a.T from CI as a, Empl as b where a.D = b.D and overlaps(a.T,b.T) group by a.D, a.T  create view FI (D,T) as select distinct D, sem(a.T) as T from Chron as a, Empl as b where overlaps(sem(a.T),b.T)  7  QSQL nt : Counting the total number of contracts in the database is straightforward:  It is evident from this example that the computation of the constant intervals is the hard part, while the computation of the aggregate function is quite straightforward and needs just a Boolean function to test the overlapping of timestamps.
select count(*) as Cnt from Empl  In summary, the availability of appropriate time data types aids only little in the formulation of temporal aggregation queries.
We identify two core problems that make temporal queries complex.
First, the calculation of the timestamps for queries with constant intervals as well as cumulative aggregates is complex.
Second, the calculation of the timestamps for Query Qfi refers to the Chron relation.
This relation cannot be materialized.
Observation 2 Instantaneous temporal queries are complex to formulate with standard SQL extended with an interval-based ADT.
QSQL : Query Qfi explicitly specifies the periods for fi which a result tuple is expected, i.e., for every semester where data are present.
We use the Chron relation and the sem function introduced above and express the query as follows:  5.3  select b.D, count(*) as Cnt, a.T from FI as a, Empl as b where a.D = b.D and overlaps(a.T,b.T) group by b.D, a.T  Approach II: Fold/Unfold  Being of fixed size, interval timestamps are very convenient when capturing the temporal aspects of information.
In some respects, the most straightforward and simplest means of capturing temporal aspects is to include an extra interval-valued time attribute in each relation.
However, one might also suspect that the difficulty in formulating temporal queries in the previous section is caused by the intervals.
SQL comes unprepared to support something (an interval) that represent something (a set of consecutive time points) that it is not.
In response to this, it has been proposed to equip SQL with the ability to normalize timestamps.
The idea is to split or merge interval timestamps so that they are aligned (identical or disjoint) and can be treated as atomic entities.
Advanced most prominently by Lorentzos and his colleagues [18, 19, 20, 21], the earliest and most radical approach is to introduce the two functions unfold and fold.
The unfold function decomposes an interval timestamped tuple into a set of point timestamped tuples, one for each point in the original interval.
The fold function "collapses" a set of point timestamped tuples into value-equivalent tuples timestamped with maximum intervals.
The only difference to Qci is in the computation of the timestamps of the result tuples.
QSQL cum : Query Qcum is similar to Query Qci in that the aggregate function is computed for each time point, and consecutive time points with the same result value and identical lineage are coalesced.
The timestamps of the result tuples can be computed from the argument tuples similar to how it was done for the constant intervals.
However, the length of the moving window has to be considered.
The timestamps of the result tuples extend beyond the timestamps of the argument tuples.
The following view computes the possible end points of the result tuples.
create view EndPoints (TP) as select distinct Ts as TP from Empl union select distinct Te+2 as TP from Empl  We must extend all end points of the argument tuples by the value 2.
Based on the end points, a view CumI can be defined that is identical to CI for constant intervals, except that for this query, there are no non-temporal grouping attributes.
With the view CumI in place, we can formulate Query Qcum as follows:  Observation 3 Extending SQL with functions fold and unfold is attractive because of its conceptual simplicity.
The idea is to use the interval-based representation of temporal information while being able to manipulate it as if the point-based representation was used, thus obtaining the representational benefits of intervals while avoiding the problems they seem to pose in query formulation.
The general pattern for queries using unfold and fold is to:  select count(*) as Cnt, a.T from CumI as a, Empl as b where overlaps([a.Ts-2,a.Te],b.T) group by a.T  As for the computation of the timestamps, we have to consider again the length of the moving window and to aggregate over all argument tuples that overlap an interval that starts two chronons before the timestamp of the result tuple.
1. explicitly construct the point-based representation by unfolding the argument relation(s); 2. compute the query on interval-free representation; and 8  are extracted.
The IXSQL predicate cp corresponds to the overlaps function and tests for common time points of the two arguments.
3. fold the result to end up with an interval-based representation.
Observation 4 Transitioning from the interval to the point representation puts a load on the database system that is exponential in the length of the intervals.
QIXSQL cum : The cumulative aggregation query follows the pattern of the previous two queries: we unfold the E MPL relation so we can work with time points, and use a join to match it with tuples within the specified window.
The fold and unfold functions have been integrated into IXSQL [18, 21], which we use for illustration.
IXSQL inherits and extends the semantics of SQL.
Thus, each SQL query is also an IXSQL query.
In the discussion below we assume the E MPL relation in Figure 3.  select count(*) as from ( select * from Empl reformat as ( select * from Empl reformat as where b.T >= a.T-2 group by T reformat as fold T  QIXSQL : Query Qci that expresses the time-varying numci ber of contracts per department can be formulated as follows: select D, count(*) as Cnt, T from ( select * from Empl reformat as unfold T ) group by D, T reformat as fold T  Cnt, a.T  unfold T ) as a,  unfold T ) as b and b.T <= a.T  Note that the two time points after the very last argument tuple (cf.
Figure 4) are missing.
This can be fixed by extending the inner SQL statement with a union statement that explicitly adds these points.
As in the case with constant intervals, the transformation into interval-timestamped result tuples by the fold operation yields the coalesced relation in Figure 3(c), which is not the intended result.
The inner query unfolds the argument relation yielding the point-based representation shown in Figure 2(a).
Then the aggregation is computed on this relation and with the fold function transformed back into a interval-stamped relation.
Note that the obtained result is different from the intended result in Figure 3(b).
The normalization step does not carry over any lineage information, and the unfold operation creates maximal intervals of snapshot equivalent tuples independently of the argument tuples that produce the result.
In particular, the first two intended result tuples are merged into a single tuple, and we get the result shown in Figure 3(c).
QIXSQL : The standard SQL solution can be used to count nt the total number of contracts: select count(*) as Cnt from Empl  In summary, a language enriched with folding and unfolding offers some support for expressing instantaneous aggregation with constant intervals as in Query Qci .
However, the final fold function, coalescing snapshot equivalent tuples of the point model into tuples over maximal intervals, might lead to wrong results.
Regarding fixed intervals, IXSQL provide no generic support.
Although the language offers a window function to generate windows of a specific size with a determined offset, it is not expressive enough to formulate sliding windows or an arbitrary number of consecutive timestamps.
The efficient evaluation of queries formulated using fold and unfold has yet to be resolved.
Unfolding has a worst case space complexity that is exponential (an m bit binary integer encodes up to 2m - 1 database states); and for the time domains available in current systems, unfolded relations are so large that storing them is impractical.
A more subtle observation is that IXSQL adopts a view on relation instances that is neither purely point-based nor interval-based.
It is not purely point-based because it is sensitive to the specific interval representation chosen for the  Observation 5 When transitioning from intervals to points any semantics associated with the intervals is lost.
QIXSQL : To express Query Qfi , we unfold the argument fi relation and determine all semesters for which data are available: select D, count(*) as Cnt, S as T from ( select distinct D, sem(T) as S from Empl reformat as unfold T ) as a, Empl as b where a.D = b.D and a.S cp b.T group by D, a.S reformat as fold T  Again, the unfold function first transforms the intervaltimestamped relation into a point-timestamped relation, from which the different pairs of departments and semesters 9  SQL/TP  Qfi : The computation of Qfi is more complicated.
We have to group the time points into semesters, and each time point of a semester must produce the same aggregate value.
data.
Thus, when different, but snapshot-equivalent, relations are used, the same query generally returns different results.
In contrast, the fold and unfold functions only preserve the information content in a relation up to that captured by the point-based view.
For example, unfolding and then folding the relation instance in Figure 3(b) yields the instance in Figure 3(c).
Finally, it may be noted that the three-step procedure for using fold and unfold is exactly a procedure and thus adds a slight procedural element to SQL, the core of which may be seen as being declarative.
5.4  select a.D, count(*) as Cnt, a.T from Empl as a, Empl as b where a.D = b.D and a.T-1 div 6 = b.T-1 div 6 group by a.D, a.T  The condition in the where clause groups the argument tuples by department and semester.
The aggregate function is computed over these groups and assigned to each time point in the semester.
For example, the result of the first semester in 2003 is as follows:  Approach III: Point Timestamps  A more radical approach to designing a temporal query language is to simply assume that temporal relations use point timestamps--fold and unfold are then not needed.
The temporal query language SQL/TP advanced by Toman takes this approach to generalizing queries on non-temporal relations to apply to temporal relations [5, 39].
The semantics of SQL/TP is defined with respect to the point-based representation, and we thus assume the E MPL relation instance in Figure 2 in the following.
The restriction to point timestamps yields a simple and unambiguous semantics that avoids many of the pitfalls that can be attributed to interval timestamps.
D DB DB DB DB DB DB AI AI AI AI AI AI  Observation 6 SQL/TP does not permit the association of information with intervals.
Cnt 3 3 3 3 3 3 1 1 1 1 1 1  T 2003/01 2003/02 2003/03 2003/04 2003/05 2003/06 2003/01 2003/02 2003/03 2003/04 2003/05 2003/06  SQL/TP  Qcum : SQL/TP does not provide any natural support for the formulation of cumulative queries, i.e., a mechanism to move a window of fixed size over the time line and to produce a result at each time point.
Hence, Query Qcum is more complex:  The strength of SQL/TP is in its generalization of queries on snapshot relations to corresponding queries on corresponding temporal relations.
The general principle is to extend the snapshot query with equality constraints on the timestamp attribute of the temporal relation, to separate different database snapshots during query evaluation.
select count(distinct CID) as Cnt, a.T from ( select distinct T from Empl ) as a, Empl as b where a.T-2 <= b.T < a.T group by a.T union select count(distinct CID), max(a.T)+1 from ( select distinct T from Empl ) as a, Empl as b where max(a.T)-1 <= b.T < max(a.T)+1 group by a.T union select count(distinct CID), max(a.T)+2 from ( select distinct T from Empl ) as a, Empl as b where max(a.T) <= b.T < max(a.T)+2 group by a.T  SQL/TP  Qci : Query Qci is straightforward to express in SQL/TP, as the argument tuples are first grouped by department and time points, upon which the aggregate function is computed.
select D, count(*) as Cnt, T from Empl group by D, T  The grouping takes care of isolating the database states from one another.
This query is restricted to finite (discrete and bounded) time domains, to avoid infinite relations and counts.
Observation 7 The semantics of SQL/TP statements is defined with respect to the point representation, which is different from the presentation of a temporal relation.
10  5.5  Note that the query is a union of three almost identical parts.
The last two parts take care of the two very last time points (cf.
Figure 4) that are not part of the timestamps of the original relation.
Approach IV: Syntactic Defaults  Along with the introduction of temporal abstract data types, what may be termed syntactic defaults have been introduced that make the formulation of common temporal queries more convenient.
The most common defaults concern access to the current state of a temporal database and for handling temporal generalizations of non-temporal queries, e.g., joins.
The most comprehensive approach based on syntactic defaults is the TSQL2 language [32, 35], which we use for exemplification.
We assume the E MPL instance in Figure 3.
In TSQL2, a default valid clause, placed after the select clause, computes the intersection of the valid times of the tuples in the argument relations mentioned in the from clause, which is then returned in the result.
For example, the timestamp of a tuple that results from joining two relations is the intersection of the timestamps of the two argument tuples that produce the tuple.
With only one relation in the from clause, this yields the original timestamps.
In order to compute an instantaneous temporal aggregation, the timestamps of overlapping argument tuples that belong to the same group must be intersected.
This computation of constant intervals cannot be expressed easily in SQL (cf.
Section 5.1).
Moreover the interaction with the default valid clause described above is not clear to the authors.
This is taken to be evidence of the complexity of a language that provides comprehensive syntactic defaults.
It also implies that the queries described in this section may not be correct.
We rely on the description of temporal aggregates by Kline et al.
[15].
SQL/TP  Qnt : Since SQL/TP counts the number of tuples in the abstract relation, it is necessary to project the time attribute and eliminate duplicates.
This yields the intended result if the tuples are distinguishable.
In our case, the contract ID ensures this.
select count(distinct CID) as Cnt from Empl  This query again requires the use of a contract identifier in order to be able to distinguish between different contracts.
The timestamps alone do not provide any information about this.
Observation 8 Aggregates in SQL/TP compute the aggregate with respect to the abstract temporal relation.
Operations such as counting the numbers of rows in a concrete representation are not possible.
In one sense, SQL/TP and SQL are opposites when it comes to the handling of temporal information.
In SQL, intervals have no special meaning--they are treated as atomic entities.
In contrast, SQL/TP effectively decomposes intervals into sets of points.
This difference becomes clear when considering aggregate queries.
In SQL, time-varying aggregation (Qci ) is poorly supported, while SQL/TP needs to resort to auxiliary attributes for "time-invariant" aggregation (Qnt ).
In several of the examples, we have used relations with contract IDs in order to be able to capture the intended information and express the desired queries.
While the reliance on contract identifiers appears to be a minor issue, it is worth noting that such identifiers do not offer a systematic approach to obtaining point-based semantics and a semantics that preserves the intervals of the argument relations.
The problem is that set operations as well as aggregation are sensitive to any additional attributes and essentially do not permit the presence of such attributes.
This issue is not germane to SQL/TP, but seems to apply equally to any approach that uses a point-based data model.
In summary, the strength of SQL/TP is its restriction to time points that ensures a simple and well-defined semantics.
As intervals are still to be used in the physical representation of the temporal information as well as when presenting the results of queries to the users, one may think of SQL/TP as a variant of IXSQL where, conceptually, queries must always apply unfold as the first operation and fold as the last.
A compilation technique has been supplied for SQL/TP that avoids this unfolding, thus offering hope that SQL/TP queries can be evaluated efficiently in practice [].
Observation 9 Well-chosen syntactic defaults yield a language that allows to succinctly formulate common temporal queries.
QTSQL2 : To formulate an instantaneous aggregation, it is ci possible to extend the group by clause with a valid clause.
In the query below, the term using instant is in fact the default and could be omitted.
We added it for clarity since valid(Empl) denotes the original timestamps and we want to group according to constant intervals, not the original timestamps.
select D, count(*) as Cnt from Empl group by D, valid(Empl) using instant  QTSQL2 : Grouping into periods is supported through the fi using clause.
In this case, we specify a grouping of 6 months (i.e., one semester).
In passing, we mention that we are uncertain whether this indeed yields January-June and July-December or whether shifted semesters might result.
11  are easily formulated in SQL on non-temporal relations are very difficult to formulate on temporal relations.
With statement modifiers, one thus formulates a temporal query by first formulating the corresponding non-temporal query (i.e., assuming that there are no timestamp attributes on the argument relations) and then applies a statement modifier to this query.
For example, to formulate a temporal join the first step is to formulate the corresponding non-temporal join.
Next, a modifier is prepended to express that temporal semantics are to be used.
The modifier ensures that the argument timestamps overlap and that the resulting timestamp is the intersection of the argument intervals.
If the enclosed query is simply a selection, the timestamps do not have to be transformed, and the only task of the modifier is to ensure that the original timestamps are returned as the timestamps of the result.
If the enclosed statement is a difference, the modifier ensures that the intervals are appropriately subtracted.
select D, count(*) as Cnt from Empl group by D, valid(Empl) using 6 month  QTSQL2 : TSQL2 provides native support for cumulative cum (moving-window) aggregates.
Specifically, the group by clause allows specification of a leading and trailing time interval for a moving window.
Hence, Qcum can be expressed as follows: select count(*) from Empl group by D, valid(Empl) leading 2 month  QTSQL2 : The default behavior of TSQL2 is to return temnt poral relations.
The snapshot keyword is used for retrieving non-temporal relations.
Thus, to retrieve the total number of contracts, we can use the following non-temporal aggregation:  Observation 11 Statement modifiers are orthogonal to the SQL language and adding them to SQL is less understood than adding a new ADT.
select snapshot count(*) as Cnt from Empl  TSQL2 is a large language with many parts and an informally specified semantics.
It provides syntactic defaults that serve as shorthands and thus simplify the formulation of temporal queries over point-based temporal databases.
The problem with syntactic defaults relates to lack of the "scalability" over language constructs.
When defining a language that uses syntactic defaults, one must explicitly specify a large number of defaults.
When extending a large and nonorthogonal language such as SQL, it becomes challenging to be comprehensive and systematic in the specification of such defaults, and to ensure that the defaults do not interact with one another in unanticipated and undesirable ways.
We therefore believe that this approach tends to yield a language where, although it may be possible to formulate common queries concisely, the language itself is complex and therefore difficult to understand and use.
Unlike the languages that consider intervals as compact representations of sets of points, the use of statement modifiers makes it possible to give more meaning to the intervals.
Thus, relations in ATSQL consist of interval timestamped tuples, and value-equivalent tuples with adjacent or overlapping intervals are permitted.
Relation E MPL as given in Figure 3 is assumed in the following.
QATSQL : Query Qci is a temporal generalization of a nonci temporal query.
Thus, it can be formulated by prepending the non-temporal SQL query by the seq vt modifier: seq vt select D, count(*) as Cnt from Empl group by D  QATSQL : By default, the seq vt clause operates at the fi lowest granularity and computes constant intervals.
This behavior can be extended by allowing the user to specify different granularities or, in the general case, fixed intervals.
Below we show an extended modifier that specifies the periods for which a result tuple is to be produced.
Observation 10 Defining a temporal language in terms of syntactic defaults is difficult since the non-temporal constructs do not offer a systematic and easy way to express the defaults.
5.6  Approach V: Semantic Defaults  seq vt for semesters(vtime(Empl)) select D, count(*) from Empl group by D  ATSQL introduces temporal statement modifiers to add temporal support to SQL [8, 4].
In contrast to syntactic defaults, statement modifiers are semantic defaults that indicate the intended semantics without specifying how to compute it.
The basic idea in statement modifiers is to offer a systematic means of constructing temporal queries from nontemporal queries, the motivation being that queries that  In this query, the semesters function is a generative function that returns all semesters that a given interval timestamps spans, e.g., semesters([2005/2, 2006/5]) = {[2005/1, 2005/6], [2005/7, 2005/12], [2006/1, 2006/6]}.
12  : Moving-window aggregation is an extension of QATSQL cum regular instantaneous aggregation.
Various syntactic constructs have been proposed for moving-window aggregation.
We use the syntax of the Oracle OLAP extensions [] to illustrate how such aggregates can be incorporated into modifiers.
may be more difficult to formulate in point-based models.
It then presents a general framework of temporal aggregation concepts.
Building on this foundation and four example aggregation queries, the paper explores the aggregation capabilities of five distinct categories of temporal query languages.
To make the coverage concrete, a prototypical query language serves as a representative for each approach.
The main findings are formulated in a number of observations.
The paper affords an informal coverage of its subject in order to cover a wide range of concepts as well as to offer a foundation for further research.
The abstract data type approach is simple, but also very limited in the support offered.
Its main strength is that adding ADTs to SQL is well understood, e.g., there exist ADTs for images, text, multimedia, etc.
The main disadvantage is that advanced and systematic support for timevarying applications seems to require solutions that cannot be offered by extending SQL with new functions and predicates.
The fold/unfold approach enables easy conversion between point and interval timestamped representation of a relation.
Using point timestamped relations makes the formulation of some queries easier, while interval timestamps are convenient for other queries, as well as for physical representation of relations and user display of query results.
The fold/unfold approach is limited by being inherently pointbased.
The main strength is the conceptual simplicity of fold/unfold.
On the downside the (syntactic) complexity of temporal queries remain fairly high and an efficient implementation of fold/unfold has yet to emerge.
The approach that solely uses point timestamps assumes that physical representation and display of relations are beyond the scope of the query language.
This leads to a clean, point-based query language.
Working solely with points greatly simplifies the formulation of instantaneous temporal queries.
A possible drawback is that the user must frequently map between intervals and points since relations are represented in their compact form whereas statements are formulated against abstract databases.
Also, some statements become system dependent.
For example a count without duplicate elimination (e.g., select count(X) from R) returns the number of tuples in the abstract relation R. This number depends on the base granularity, which may differ among systems.
Next, with syntactic defaults, typical queries may be given very short formulations.
However, it is challenging, if not impossible, to design a query language that systematically and comprehensively offers convenient syntactic defaults and that is also easy to understand.
Syntactic defaults tend to not scale well since a complex syntactic default must be specified for a large number of constructs of the original language.
The notion of statement modifiers offers what may be  seq vt with range between interval '3' months preceding and current select count(*) from Empl group by D  QATSQL : The last query must be evaluated independently nt of the timestamps of the argument tuples.
This is achieved by using a nseq vt modifier (short for "non-sequenced valid time"), which indicates that what follows should be treated as a regular SQL query.
nseq vt select count(*) as Cnt from Empl  In summary, semantic defaults offer systematic support for writing temporal queries that can be evaluated on all sets of concurrent states of the argument relations in isolation.
This language mechanism is independent of the syntactic complexity of the queries that the modifiers are applied to, which renders semantic defaults scalable across the constructs of the language being extended.
Observation 12 Statement modifiers by and large separate the temporal and non-temporal parts of a query expression.
While statement modifiers offer attractive means of formulating the example queries, it should be noted that extending a language with statement modifiers represents a much more fundamental change to the language than, e.g., extending the language with temporal abstract data types.
6  Summary  The temporal database research community has been quite prolific with respect to the design of new temporal query languages--a body of several dozen such languages exists.
Based on the observation that many of these languages can be categorized according to the approach they take to providing temporal support, this paper investigates the support for temporal aggregation inherent to five such approaches.
More specifically, the paper initially characterizes temporal query languages according to whether they are pointor interval-based, noting that certain aggregation queries 13  is assumed by existing query languages.
These examples, which go beyond the setting assumed in this paper, illustrate that existing temporal query languages may be extended to offer much better support for temporal aggregation.
Finally, the increasing prominence of business intelligence has also brought new prominence to temporal aggregation.
W. H. Inmon, known as the founder of data warehousing, mentions time variance as one of four salient characteristics of a data warehouse, and there is general consensus that a data warehouse is likely to exhibit a strong temporal orientation.
Being temporal, data warehouses are thus prime candidates to benefit from the advances in temporal aggregation.
But cross-fertilization between temporal databases and data warehousing is lacking.
In fact, some of the original impetus for a separate data model and query language for data warehouses arose from a perceived lack of temporal support in the relational model and SQL.
Few attempts have been made to exploit the advances in temporal databases in the context of data warehousing, although notable exceptions do exist.
The special dimensional data models used in data warehouses and the emphasis on supporting advanced query functionality bring novel challenges to temporal database research.
For example, few attempts have been made at integrating temporal query languages with multidimensional query languages.
termed semantic defaults: modifiers are introduced that control the semantics of any query language statements.
The strong point is the support for intervals and the systematic support for temporal queries that generalize snapshot queries.
The approach by and large decouples the temporal and non-temporal parts in a statement.
Thus, the presence of time-varying information does not change the formulation of the core query.
A drawback is that this approach is new and that there are no experiences with such extensions to SQL.
7  Outlook  In step with the increasing digitization throughout society, the increasing networking of information systems, and the ability to store increasing amounts of data, increasing volumes of time-varying data are being accumulated and made available to users.
Trends such as automatic data gathering using web-server logs in e-business applications and using sensors in a range of applications contribute to this development.
We are also witnessing an increase in analytical applications, often referred to as business intelligence applications, that extract useful information from large volumes of data, e.g., by means of aggregation.
Thus, effective support for the formulation of aggregation queries is increasingly important.
In contrast, this paper's study indicates that the support for temporal aggregation in query languages is still lacking.
In fact, the existing temporal query languages were largely designed with traditional record-keeping applications in mind, i.e., the kind of application found in banking where account-balances are kept for customers.
In banking, a (constant) account balance is valid during a time interval, and it is valid for any subset of this time interval.
New applications that do not fit this rigid template are becoming increasingly important.
This applies to applications that monitor or track continuous variables, e.g., positions of moving objects using GPS, the flow of water from a river into the sea, or temperature and humidity in different geographical locations.
These applications typically rely on sampling, so they record values that are valid only for a single point in time.
Values beyond these times must be interpolated or extrapolated.
This type of scenario is also characterized by data uncertainty--values beyond the samples are not accurate, and even the samples may not be accurate.
Next, consider an attribute that records the accumulated rainfall in a certain location over a certain time interval.
Unlike in the case of the account balance, a value of this attribute does not hold for any subset of the interval associated with it, as the accumulated rainfall in smaller interval is likely to be smaller.
This type of attribute is a good example of attributes that carry semantics that differ from what  Acknowledgments The work was partially funded by the Free University of Bolzano through the TTDBT project and the Municipality of Bozen-Bolzano through the eBZ-2015 initiative.
C. S. Jensen is also an adjunct professor in Department of Technology, Agder University College, Norway.
References [1] M. H. Bohlen, J. Gamper, and C. S. Jensen.
An Algebraic Framework for Temporal Attribute Characteristics.
Journal of Annals of Mathematics and Artificial Intelligence, 26 pages, to appear.
[2] M. H. Bohlen, J. Gamper, and C. S. Jensen.
Multidimensional Aggregation for Temporal Data.
In Proc.
EDBT, pp.
257-275, 2006.
[3] M. H. Bohlen and C. S. Jensen.
Temporal Data Model and Query Language Concepts.
Encyclopedia of Information Systems, 4: 437-453, 2003, Academic Press.
14  [4] M. H. Bohlen, C. S. Jensen, and R. T. Snodgrass.
Temporal Statement Modifiers.
ACM TODS, 25(4): 407- 456, 2000.
[18] N. A. Lorentzos.
The Interval-extended Relational Model and Its Application to Valid-time Databases.
[37, pp.
67-91].
[5] I. T. Bowman and D. Toman.
Optimizing Temporal Queries: Efficient Handling of Duplicates.
Data and Knowledge Engineering, 44(2): 143-164, 2003.
[19] N. A. Lorentzos and R. Johnson.
Extending Relational Algebra to Manipulate Temporal Data.
Information Systems, 13(3): 289-296, 1988.
[6] Y. Chen and P. Z. Revesz.
Max-Count Aggregation Estimation for Moving Points.
In Proc.
TIME, pp.
103- 108, 2004.
[20] N. A. Lorentzos and Y. Mitsopoulos.
Functional Requirements for Historical and Interval Extensions to the Relational Model.
Data and Knowledge Engineering, 17(1): 59-86, 1995.
[7] J. Chomicki and P. Z. Revesz.
Constraint-based Interoperability of Spatiotemporal Databases.
GeoInformatica, 3(3): 211-243, 1999.
[21] N. A. Lorentzos and Y. G. Mitsopoulos.
SQL Extension for Interval Data.
IEEE TKDE, 9(3): 480-499, 1997.
[8] J. Chomicki, D. Toman, and M. H. Bohlen.
Querying ATSQL Databases with Temporal Logic.
ACM TODS, 26(2): 145-178, 2001.
[22] L. E. McKenzie Jr. and R. T. Snodgrass.
Evaluation of Relational Algebras Incorporating the Time Dimension in Databases.
ACM Computing Surveys, 23(4): 501-543, 1991.
[9] J. Clifford and A. Tuzhilin (eds.).
Recent Advances in Temporal Databases: Proceedings of the International Workshop on Temporal Databases.
Workshops in Computing Series.
Springer-Verlag 1995.
[23] B.
Moon, I. F. Vega Lopez, and V. Immanuel.
Efficient Algorithms for Large-Scale Temporal Aggregation.
IEEE TKDE, 15(3): 744-759, 2003.
[10] O. Etzion, S. Jajodia, and S. Sripada (eds.).
Temporal Databases: Research and Practice.
LNCS 1399, Springer-Verlag 1998.
[24] G. Ozsoyoglu and R. T. Snodgrass.
Temporal and Real-Time Databases: A Survey.
IEEE TKDE, 7(4): 513-532, 1995.
[11] C. S. Jensen and C. E. Dyreson (eds.).
A Consensus Glossary of Temporal Database Concepts--February 1998 Version.
[10, pp.
367-405].
[25] P. Z. Revesz.
Introduction to Constraint Databases.
Springer, 2002.
[12] C. S. Jensen, and R. T. Snodgrass.
Semantics of Time-Varying Information.
Information Systems, 21(4): 311-352, 1996.
[26] P. Z. Revesz.
Efficient Rectangle Indexing Algorithms Based on Point Dominance.
In Proc.
TIME, pp.
210- 212, 2005.
[13] P. C. Kanellakis, G. M. Kuper, and P. Z. Revesz.
Constraint Query Languages.
J. Comput.
Syst.
Sci., 51(1): 26-52, 1995.
[27] P. Z. Revesz, R. Chen, P. Kanjamala, Y. Li, Y. Liu, and Y. Wang.
The MLPQ/GIS Constraint Database System.
In Proc.
SIGMOD, p. 601, 2000.
[14] N. Kline and R. T. Snodgrass.
Computing Temporal Aggregates.
In Proc.
ICDE, pp.
222-231, 1995.
[28] P. Z. Revesz and Y. Chen.
Efficient Aggregation over Moving Objects.
In Proc.
TIME, pp.
118-127, 2003.
[15] N. Kline, R. T. Snodgrass, and T. Y. C. Leung.
Aggregates.
In R. T. Snodgrass, editor, The TSQL2 Temporal Query Language, Chapter 21, pp.
395-425.
Kluwer Academic Publishers, 1995.
[29] J. F. Roddick and J. D. Patrick.
Temporal Semantics in Information Systems--a Survey.
Information Systems, 17(3): 249-267, 1992.
[30] R. T. Snodgrass (ed.).
Proceedings of the International Workshop on an Infrastructure for Temporal Databases, 1993.
[16] N. Kline and M. D. Soo.
T IME -IT: The T IME Integrated Testbed, pre-beta version 0.1 available via anonymous ftp from ftp.cs.arizona.edu, 1995.
[31] R. T. Snodgrass.
Temporal Object Oriented Databases: A Critical Comparison.
Ch.
19, pp.
386-408, of W. Kim, Modern Database Systems: The Object Model, Interoperability and Beyond.
AddisonWesley/ACM Press 1995.
[17] A. C. Klug.
Equivalence of Relational Algebra and Relational Calculus Query Languages Having Aggregate Functions.
JACM 29(3): 699-717, 1982.
15  [32] R. T Snodgrass (ed.
), I. Ahn, G. Ariav, D. Batory, J. Clifford, C. E. Dyreson, R. Elmasri, F. Grandi, C. S. Jensen, W. Kafer, N. Kline, K. Kulkarni, T. Y. Leung, N. Lorentzos, J. F. Roddick, A. Segev, M. D. Soo, and S. M. Sripada.
The TSQL2 Temporal Query Language.
Kluwer Academic Publishers 1995.
[33] R. T. Snodgrass.
Temporal Databases.
Part II of C. Zaniolo, S. Ceri, C. Faloutsos, R. T. Snodgrass, V. S. Subrahmanian, and R. Zicari.
Advanced Database Systems.
Morgan Kaufmann Publishers 1997.
[34] R. T. Snodgrass.
Developing Time-Oriented Database Applications in SQL.
Morgan Kaufmann Publishers 2000.
[35] R. T. Snodgrass, I. Ahn, G. Ariav, D. Batory, J. Clifford, C. E. Dyreson, R. Elmasri, F. Grandi, C. S. Jensen, W. Kafer, N. Kline, K. Kulkarni, T. Y. C. Leung, N. Lorentzos, J. F. Roddick, A. Segev, M. D. Soo, and S. M. Sripada.
TSQL2 Language Specification.
ACM SIGMOD Record, 23(1): 65-86, 1994.
[36] R. T. Snodgrass, S. Gomez, and E. McKenzie.
Aggregates in the Temporal Query Language TQuel.
IEEE TKDE, 5(5): 826-842, 1993.
[37] A. Tansel, J. Clifford, S. Gadia, S. Jajodia, A. Segev, and R. T. Snodgrass (eds.).
Temporal Databases: Theory, Design, and Implementation.
Benjamin/Cummings Publishers 1994.
[38] C. I. Theodoulidis and P. Loucopoulos.
The Time Dimension in Conceptual Modelling.
Information Systems, 16(3): 273-300, 1991.
[39] D. Toman.
Point-Based Temporal Extensions of SQL and Their Efficient Implementation.
[10, pp.
211- 237].
[40] Y. Wu, S. Jajodia, and X. S. Wang.
Temporal Database Bibliography Update.
[10, pp.
338-366].
[41] J. Yang and J. Widom.
Incremental Computation and Maintenance of Temporal Aggregates.
The VLDB Journal, 12(3): 262-283, 2003.
[42] D. Zhang, A. Markowetz, V. J. Tsotras, D. Gunopoulos, and B. Seeger.
Efficient Computation of Temporal Aggregates with Range Predicates.
In Proc.
PODS, pp.
237-245, 2001.
16
2013 20th International Symposium on Temporal Representation and Reasoning  Minimal Consistency Problem of Temporal Qualitative Constraint Networks Jean-FrancESSois Condotta UniversiteE Lille-Nord de France, CRIL CNRS, UMR 8188 Lens, France Email: condotta@cril.fr  Souhila Kaci UniversiteE Montpellier 2, LIRMM CNRS, UMR 5506 Montpellier, France Email: kaci@lirmm.fr  AbstractaVarious formalisms for representing and reasoning about temporal information with qualitative constraints have been studied in the past three decades.
The most known are dedZnitely the Point Algebra (PA) and the Interval Algebra (IA) proposed by Allen.
In this paper, for both calculi, we study a particular problem that we call minimal consistency problem (MinCons).
Given a temporal qualitative constraint network (TQCN) and a positive integer k, this problem consists in deciding whether or not this TQCN admits a solution using at most k distinct points on the line.
On the one hand, we prove that this problem is NP-complete for both PA and IA, in the general case.
On the other hand, we show that for TQCNs dedZned on the convex relations, MinCons is polynomial.
For these TQCNs, we give a polynomial method allowing to obtain compact scenarios.
in the case of IA).
We also consider a close decision problem which we call Minimal Consistency Problem (MinCons).
It consists, given a positive integer k and a TQCN, in deciding whether this TQCN admits a solution using at most k distinct points of the line.
Given a TQCN, to characterize a solution or a scenario among the most compact ones is very interesting and pertinent for some applications.
For example, consider a system allowing to handle temporal qualitative constraints in the context of a project management tool.
For each temporal constraint added by the user, such a system can offer some functionalities like consistency checking or visualizating possible condZgurations of the different temporal activities.
Concerning this last functionality, it is natural that the visualized solution or scenario belongs to the set of the most compact solutions or scenarios.
Another dZeld of applications where characterizing compact solutions of TQCNs can be pertinent is the intelligent systems involving preference handling.
In the context of a recommender system, for decision purposes, it is more convenient to characterize a unique complete preorder associated with a preference set [5].
In the case where preference information can be represented by TQCNs [6], [7], these distinguished models can correspond to particular compact solutions or scenarios.
I. I NTRODUCTION Representing and reasoning about temporal information is crucial in many areas of ArtidZcial Intelligence.
In this context, several qualitative constraint calculi have been proposed.
Allenas calculus [1], also called Interval Algebra, and Point Algebra [2] are certainly the most well-known qualitative temporal calculi.
These two formalisms allow to represent and to reason about possible condZgurations of temporal entities by means of Temporal Qualitative Constraint Networks (TQCNs).
Each constraint of a TQCN is dedZned by a set of base relations of the calculus representing the possible relative positions between two temporal entities.
Several fundamental problems arise with TQCNs.
The main is the consistency problem which consists in deciding whether a given TQCN admits a scenario represents consistent temporal information.
This problem has been widely studied, in particular in the context of PA and IA.
It is polynomial for PA and NP-complete for IA.
We have now efdZcient methods to solve the consistency problem of a TQCN, see for example [3].
Concerning IA, a complete complexity map of the consistency problem has been realized and now, we know all tractable subclasses of this calculus [4].
Another problem is the minimal labeling problem which consists in characterizing the feasible base relations (base relations satisdZed by at least a solution) of a TQCN.
In this paper, we introduce and study a problem which consists, given a TQCN, in dZnding one of the the most compact solutions, i.e.
a solution involving a minimal number of distinct points of the line (of distinct endpoints of intervals 1530-1311/13 $26.00 AS 2013 IEEE 1550-1311/13 DOI 10.1109/TIME.2013.11  In this work, we show that, MinCons is NP-complete for PA despite that the consistency problem is polynomial for this calculus.
Less surprisingly, MinCons is also NP-complete for IA in the general case, but it is also NP-complete for some subclasses known to be tractable concerning the consistency problem.
With that in mind, we focus on the subclasses of the convex relations of PA and IA and show that for these particular cases, MinCons is polynomial.
We dedZne, for convex TQCNs, a polynomial method allowing to extract compact scenarios corresponding to minimal elements of particular partial orders.
After necessary background in Section 2, we introduce MinCons in Section 3.
We study particular orders on scenarios in Section 4 and two aggregation operators of scenarios in Section 5.
Section 6 is devoted to MinCons restricted to the convex TQCNs.
Lastly, we conclude.
11 7  II.
P RELIMINARIES ON T EMPORAL Q UALITATIVE C ONSTRAINT N ETWORKS  pi mi  Relation Symbol Inverse  A (binary) temporal qualitative calculus [8] is based on a dZnite set B of jointly exhaustive and pairwise disjoint (JEPD) relations dedZned on a domain D, called base relations.
The set B contains the identity relation Id, and is closed under the converse operation (a1 ).
Given two elements x and y belonging to D and a base relation b a B, x b y will denote that x and y satisdZes b, i.e.
(x, y) a b.
The complete set of relations are unions of base relations.
Each relation is represented by the set of the base relations included in the corresponding union.
Hence, 2B will represent the set of relations and we get 2|B| possible relations.
Given x, y a D and r a 2B , x r y will denote that x and y satisdZes a base relation b a r, and we will say that x and y satisdZes the relation r. The set 2B is equipped with the usual set-theoretic operations (union and intersection), the converse operation, and the weak composition operation.
The converse of a relation is the union of the converses of its base relations.
The weak composition  of two base relations b and b belonging to a set of base relations B is the relation of 2B dedZned by b  b = {b : ax, y, z a D such that x b y, y b z and x b z}.
For two relations r, r a 2B , r  r is the relation of 2B  dedZned by r  r = bar,b ar b  b .
Intuitively, r  r gives the subset of possible base relations can be satisdZed by two elements x and y when x and y satisfy respectively r and r with a third element z. Renz and Ligozat [8] discuss about the difference between the weak composition and the usual relational composition.
A subclass of relations is a set A a 2B closed under converse, intersection, and weak composition.
In this paper, we are just concerned with two temporal qualitative formalisms, namely the Interval Algebra (IA) [9], [1] and the Point Algebra [10] (PA).
The domain of IA used will be the set of the intervals of the integer line, formally DIA = {(xa , x+ ) a N A N : xa < x+ }.
The set of base relations of this calculus is the set BIA = {eq, p, pi, m, mi, o, oi, s, si, d, di, f, f i}.
These thirteen binary relations represent all the orderings of the four endpoints of two intervals, see Figure 1.
Concerning PA, the domain considered will be the set of points of the integer line, i.e.
DPA = N. The set of base relations of PA, denoted by BPA , is composed by the three binary relations <, > and =, corresponding to the usual strict total orders and the identity relation on N. The set 2BPA is formed by 8 relations corresponding to the set {{<}, {>}, {=}, {<, >}, {<, =}, {> , =}, {<, >, =}, a}.
Sometimes, we will use a short hand notation for these relations: we will write <, >, =, =, a$?, aL, ?
for respectively {<}, {>}, {=}, {<, >}, {<, =}, {>, =}, {<, =, >}.
Meaning oi  X  precedes  p  pi  Y X  meets  m  mi  overlaps  o  oi  f  si Y  eq  X  di  Y  d  X  starts  s  si  during  d  di  Y  s  fi  X  o  Y X  finishes  f  fi  equals  eq  eq  Y  m  X Y  p  (a)  (b)  (a) The base relations of IA, (b) the interval lattice.
Fig.
1.  }
 {<} = {<, >, =}.
For the following dedZnitions, we assume given a set B of base relations of a temporal qualitative calculus dedZned on a domain D. A Temporal Qualitative Constraint Network (TQCN) is a pair formed by a set of variables and a set of constraints.
Each variable represents a temporal entity and each constraint represents the set of possible qualitative condZgurations between two entities.
Formally, a TQCN is dedZned as follows: DedZnition 1: A TQCN is a pair N = (V, C) where: V is a non empty dZnite set of variables; C is a mapping that associates a relation C(v, v  ) a 2B to each pair (v, v  ) of VAV.
C is such that C(v, v) a {Id} and C(v, v  ) = (C(v  , v))a1 .
Given a TQCN, N = (V, C) we have the following dedZnitions: N is said to be trivially inconsistent iff av, v  a V with C(v, v  ) = a.
An instantiation of V is a mapping D dedZned from V to the domain D. A solution D of N is an instantiation of V such that for every pair (v, v  ) of variables in V, (D(v), D(v  )) satisdZes C(v, v  ), i.e., there exists a base relation b a C(v, v  ) such that (D(v), D(v  )) a b. N is consistent iff it admits a solution.
Two TQCNs are equivalent iff they admit the same solutions.
v1  v0  v0  {eq, s}  {s}  {p, m}  {p, o, m}  {p} v2  {p}  v1  {p}  {o}  {f i}  {p}  {eq, f, f i}  v3  (a)  (b) v3  v0 0  v2  v3  v1  Example 1: For example, consider the relations of IA, {m, s} and {d, eq}.
We have the two intervals (3, 5) and (3, 20) which satisdZes {m, s} since (3, 5) s (3, 20).
Moreover, {m, s}a1 = {mi, si} and {m, s}  {d, eq} = {d, o, s, m}.
Concerning PA, by considering the two relations of 2BPA , {<, >} and {<}, we have {<, >}a1 = {<, >} and {<, >  {p}  v2 1  2  3  4  5  (c) Fig.
2.
(a) A TQCN N of IA, (b) s scenario S of N and (c) a solution of S and N .
12 8  v0a v0+ v0a = < v0+ > = v1a = < v1+ > aL v2a > aL v2+ > > v3a > > v3+ > >  v1a v1+ = < > a$?
= < > = > ?
> > > > > >  v2a v2+ < < a$?
< < < ?
< = < > = ?
< < =  v3a v3+ < < < < < < < < ?
< > = = < > =  v0a v0+ v0a = < v0+ > = v1a = < v1+ > > v2a > > v2+ > > v3a > > v3+ > >  v1a v1+ = < > < = < > = > < > > > > > >  v2a v2+ < < < < < < > < = < > = > < < =  a solution of N corresponding to S. Figure 3 represents the convex TQCN of PA corresponding to PA(N ) and the scenario of PA corresponding to PA(S).
v3a v3+ < < < < < < < < < < > = = < > =  From now, the term TQCN will implicitly refer to TQCNs of PA or TQCNs of IA.
We associate with each consistent TQCN N = (V, C) of PA a directed graph denoted by G< (N ).
This graph makes explicit the constraints of the TQCN N dedZned by the relation {<} and is dedZned in the following way : G< (N ) = (V, E) with (v, v  ) a E iff C(v, v  ) = {<}.
Note that, since N is consistent G< (N ) is acyclic.
A path ((v 0 , v 1 ), (v 1 , v 2 ), .
.
.
, (v ka1 , v k )) of G< (N ) = (V, E) will be denoted by v 0 < v 1 < v 2 < .
.
.
< v ka1 < v k .
Furthermore, we associate with each variable v a V two positive integer numbers, denoted respectively by ranka (N , v) and by rank+ (N , v), and dedZned in the following way:  Fig.
3.
The TQCN of PA N  = PA(N ) (left side), a scenario S  of N  (right side), the constraint between by two variables v and v  is given by the entry of the row (resp.
the column) corresponding to v (resp.
v  ).
A sub-TQCN N  of N , denoted by N  a N , is a TQCN (V, C  ) such that C  (v, v  ) a C(v, v  ) av, v  a V .
An atomic TQCN is a TQCN where each constraint is dedZned by exactly one base relation.
A scenario is an atomic consistent TQCN.
Given a set of variables V, the set of all scenarios dedZned on V is denoted by V.
For the particular cases of IA and PA, this set is respectively denoted by VIA and VPA .
Given a TQCN N , N  will denote the set of scenarios which are sub-TQCN of N .
We will say that the TQCN N admits the scenario S when S a N .
A TQCN N = (V, C) is -consistent iff for all v, v  , v  a V, C(v, v  ) a C(v, v  )  C(v  , v  ).
Given a TQCN N , the largest (for a) -consistent sub-TQCN of N will be denoted by (N ) and called the closure of N by weak composition.
This TQCN can be computed in polynomial time (O(|V|3 )).
In the sequel, CPA , CIA , PIA and HIA , will denote respectively the subclass of the convex relations of PA, the subclass of the convex relations of IA [11], the subclass of the pointisable relations of IA [12] and the subclass of the ORD-Horn of IA [13].
CPA corresponds to the relations of PA expected the relation {<, >}.
CIA contains 83 relations.
Ligozat introduces a lattice arranging the base relations of BIA , see Figure 1(b).
The convex relations of IA correspond to the intervals of this lattice.
A convex TQCN is a TQCN dedZned by convex relations.
Furthermore, we have CIA a PIA a HIA .
For TQCNs of PA and TQCNs of IA dedZned on HIA , computing the closure under weak composition allows to solve the consistency problem.
HIA is the maximal subclass of IA containing the singleton relations for which the consistency problem is tractable.
A constraint between two intervals dedZned by convex relations of IA can, in an equivalent manner, be expressed by constraints dedZned by convex relations of PA on the bounds of the two intervals.
In [12], is given for each convex relation of IA its translation in PA.
Given a set of interval variables V, we dedZne the set of point variables Points(V) by Points(V) =  a + vaV {v , v }.
Moreover, given a TQCN N = (V, C) of IA dedZned by relations of CIA , PA(N ) will denote the TQCN of PA dedZned on the variables Points(V) obtained by using the translation given in [12].
aV aV  ranka (N , v) is the maximal length of the paths v  < .
.
.
< v from a variable v  a V to the variable v, rank+ (N , v) is the maximal length of the paths v < .
.
.
< v  from the variable v to a variable v  a V.  We dedZne the width of the TQCN N , denoted by width(N ), by the integer number max{ranka (N , v) : v a V} + 1.
We extend this dedZnition to a convex TQCN N of IA by considering its translation into PA : width(N ) = width(PA(N )).
Given a solution D of a TQCN N = (V, C) of PA (resp.
IA), we dedZne the cardinality of D, denoted by card(D), by card(D) = |{D(v) : v a V}| (resp.
card(D) = |{x, y : v a V and D(v) = (x, y)}|).
Example 3: Figure 4(a) shows a graph G0 whose the transitive closure is the graph G< (S  ) with S  = (V, C) the scenario in Figure 3.
For each variable v a V is given the integer ranka (S  , v).
For example, we have ranka (S  , v2a ) = 2 because the longest paths of G< (S  ) having v2a as last vertex have a length of 2 (the path v0a < v0+ < v2a for example).
The width of S  , i.e.
width(S  ), is equal to 6.
Furthermore, rank+ (S  , v2a ) = 3.
The cardinality of the solution in Figure 2(c) is 6.
III.
T HE M INIMAL C ONSISTENCY P ROBLEM In this section, we introduce a decision problem concerning the TQCNs which we call the Minimal Consistency Problem (MinCons).
Intuitively, given a TQCN N and a positive integer k, this problem is to decide whether there exists or not a solution of N using at most k distinct points on the line.
More formally, this problem is dedZned in the following way: DedZnition 2: The problem MinCons : aV Given : A TQCN N = (V, C) and an integer k aL 0. aV Question : Is there a solution D of N such that card(D) a$?
k?
By noting that, given a solution D, the width of the scenario corresponding to D and the cardinality of D are equal, we can in a manner equivalent reformulate the question of MinCons by : is there a consistent scenario S of N such that width(S) a$?
k?
The restriction of MinCons to the TQCNs of PA (resp.
IA)  Example 2: Figure 2(a) represents a TQCN N = (V, C) of IA by a graph.
Note that we do not represent the constraint C(v, v  ) when C(v  , v) is already represented or v = v  .
This TQCN is dedZned on CIA and is -consistent.
In Figure 2(b) and Figure 2(c), we have respectively a scenario S of N and  13 9  0  5  0  5  v0a  v3+  v0a  v3+  v2+  v1a  5  0  v1a  v0+ 1  v2a 2  v1+  v3a  3  4  0  v0+  v1+  v3a  v2a  1  2  3  4  v2+ 5  (a) G0  (b) G1  0 v0a v1a  4  2 v1+ v0+  v3+ v3a  v2a  1  0  2  3  v2+ 4  (c) G2 Fig.
4.
Three graphs G0 , G1 and G2 corresponding to the graphs G< (S0 ), G< (S1 ), G< (S2 ) (without edges which we can obtain by transitivity), with S0 , S1 and S2 three scenarios of the TQCN N  .
The scenario S0 is the scenario S  .
D of N with card(D) a$?
3, for each v a V , there exists a unique i a {1, 2, 3} such that D(coli ) = D(v).
Moreover, for all (v, v  ) a E, D(v) = D(v  ).
We can check that G is 3 colorable iff the answer of D is yes.
aV (2) D  = N  = (V  , C  ), k  .
The integer k is dedZned by 4.
For each vertex of G and each color is introduced a variable of V  : V = V aS {col1 , col2 , col3 }.
We force col1 , col2 and col3 to be instantiated by three intervals (the colors) with same lower bounds and different upper bounds : C  (col1 , col2 ) = C  (col2 , col3 ) = {s} and C  (col2 , col1 ) = C  (col3 , col2 ) = {si}.
For all v a V , we force the interval associated with v to have the same lower bounds that the intervals associated with col1 , col2 and col3 : C  (col1 , v) = {eq, s}.
Given (v, v  ) a V A V , if (v, v  ) a E then C  (v, v  ) is dedZned by {s, si}.
In the contrary case, C  (v, v  ) is dedZned by BIA .
Furthermore, C  (v, v) = {eq} for all v a V. Other constraints of C  are dedZned by BIA .
For each solution D  of N  with card(D  ) a$?
4 and for each v a V , there exists a unique i a {1, 2, 3} such that D  (coli ) = D(v).
Moreover, for all (v, v  ) a E, D  (v) = D  (v  ).
We can show that G is 3 colorable iff the answer of D  is yes.
  will be denoted by PMinCons (resp.
IMinCons).
For example, consider the instance of IMinCons D1 = N , 6 and the instance of PMinCons D2 = N  , 3 with N and N  the TQCNs respectively described in Figure 3 and Figure 4.
The reader can check that the answer of D1 is yes whereas the answer of D2 is no.
Given a subclass A a 2BPA (resp.
A a 2BIA ), PMinCons(A) (resp.
IMinCons(A)) denotes the restriction of PMinCons (IMinCons) to the TQCNs dedZned by relations belonging to A.
Now, we are going to show that in the general case MinCons is a NP-complete problem, even for PA for which the consistency problem is polynomial [2].
By dedZning the two sets E a 2BPA and E2 a 2BIA by E = {{=}, {<, >}, BPA } and E2 = {{eq}, {s}, {si}, {s, si}, BIA }, we have the following result : Proposition 1: Let A a 2BPA and A a 2BIA two subclasses.
We have: (1) if E a A then PMinCons(A) is NP-hard ; (2) if E  a A then IMinCons(A ) is NP-hard.
Proof.
Suppose that E a A and E  a A .
We are going to dedZne a reduction from the 3-coloring problem to PMinCons(A) and a reduction from the 3-coloring problem to IMinCons(A ).
Let G = (V, E) be an undirected graph to be colored with 3 colors.
We dedZne the instance D of MinConsPA and the instance D  of MinConsIA such that the answer of D (resp.
D  ) is yes iff G is 3 colorable.
aV (1) D = N = (V, C), k.
The integer k is dedZned by 3.
To each vertex of G and each color corresponds a variable of V : V = V aS {col1 , col2 , col3 }.
We force col1 , col2 and col3 to have three different values (the colors) by dedZning C(coli , colj ) with the relation {<, >} for all i, j a {1, .
.
.
, 3} such that i = j.
For all v, v  a V such that v = v  , if (v, v  ) a E then C(v, v  ) = {<, >} (v and v  cannot have the same color).
In the contrary case, C(v, v  ) is dedZned by {<, =, >}.
Furthermore, C(v, v) = {=} for all v a V. Other constraints of C are dedZned by {<, =, >}.
For any solution  MinCons is clearly in NP since, in polynomial time, we can check whether an atomic TQCN S on V is a scenario of a TQCN N = (V, C) and check whether width(S) a$?
k with k a positive integer.
Furthermore, we have the set E  which is included by PIA a HIA .
Hence, from the previous proposition we can assert the following result : Theorem 1: PMinCons, IMinCons, IMinCons(PIA ) and IMinCons(HIA ) are NP-complete problems.
Note that MinCons is a problem that is at least as hard that the consistency problem.
Indeed, we can remark that the cardinality of a solution of a TQCN N = (V, C) is at most |V | for a TQCN of PA and at most 2|V | for a TQCN of IA.
Hence, we can solve the consistency problem of a TQCN N = (V, C) of PA (resp.
of IA) by solving PMinCons of  14 10  N , |V | (resp.
IMinCons of N , 2|V |).
From this, we can directly establish (without using Proposition 1) that IMinCons is NP-hard since the consistency problem of IA is NP-hard.
Despite Theorem 1, we will see that there exist some interesting polynomial cases for MinCons.
Indeed, in the sequel, we study the case of the convex subclasses and show that PMinCons(CPA ) and IMinCons(CIA ) are polynomial.
Proof.
The transitivity and the redZexivity follow straightly the dedZnition of (VPA , a$?a ) and (VPA , a$?+ ).
Now, consider two scenarios S = (V, C), S  = (V, C  ) a VPA such that S a$?a S  and S  a$?a S. For all v a V, ranka (S, v) = ranka (S  , v).
Hence, for all v, v  a V, C(v, v  ) = {=} iff ranka (S, v) = ranka (S, v  ) iff ranka (S  , v) = ranka (S  , v  ) iff C  (v, v  ) = {=}.
For all v, v  a V, C(v, v  ) = {<} iff ranka (S, v) < ranka (S, v  ) iff ranka (S  , v) < ranka (S  , v  ) iff C  (v, v  ) = {<}.
We can conclude that S and S  have the same constraints.
By a similar line of reasoning we can show that (VPA , a$?+ ) is antisymmetric.
  IV.
C OMPARISON OF S CENARIOS In this section, we introduce several preorder relations on scenarios of PA.
These binary relations will allow us to compare scenarios and, under certain conditions, to characterize some scenarios of a TQCN among the scenarios having a minimal width, i.e.
the scenarios corresponding to the solutions using a minimal number of points on the line.
To compare the scenarios dedZned on a set of variables V in term of width, we introduce a dZrst binary relation denoted by (VPA , a$?C ) and dedZned in the following manner:  We can show that for all scenarios S and S  , when S and S  satisfy a$?a or a$?+ , S is at least as compact as S  .
Indeed, we have the following property: Proposition 3: Let two scenarios S = (V, C) and S  = (V, C  ) belonging to VPA .
We have: if S a$?a S  or S a$?+ S  then S a$?C S  .
DedZnition 3: Given a set of point variables V, the binary relation (VPA , a$?C ) is dedZned by : aS, S  a VPA , S a$?C S  iff width(S) a$?
width(S  ).
Proof.
We know that width(S) = max{ranka (S, v) : v a V} + 1 = max{rank+ (S, v) : v a V} + 1 et width(S  ) = max{ranka (S  , v) : v a V} + 1 = max{rank+ (S  , v) : v a V}+1.
Furthermore, if S a$?a S  (resp.
S a$?+ S  ), for all v a V, ranka (S, v) a$?
ranka (S  , v) (resp.
rank+ (S, v) a$?
rank+ (S  , v)).
From all this, we can conclude that if S a$?a S  or S a$?+ S  then width(S) a$?
width(S  ).
  Clearly, (VPA , a$?C ) is a transitive and redZexive relation.
In the general case, it is not an antisymmetric relation since two distinct scenarios dedZned on a same set of variables can have same width.
For two scenarios S and S  , we will say that S is at least as compact as S  when S a$?
S  .
In the sequel, we dedZne two orders on the set of the scenarios of PA dedZned on V, respectively denoted by a$?a and a$?+ .
Intuitively, two scenarios S and S  satisfy a$?a iff for all solutions D and D  of S and S  respectively, for each variable v a V, we have the number of points on the line before D(v) for S, i.e.
|{D(v  ) : D(v  ) < D(v)}| which is less than or equal to the number of points on the line before D(v) for S  , i.e.
|{D  (v  ) : D(v  ) < D  (v)}|.
For the relation a$?+ , we compare the numbers of points on the line after D(v) rather than the points before D(v).
Formally, (VPA , a$?a ) and (VPA , a$?+ ) are dedZned in the following manner :  V. AGGREGATION O PERATORS FOR S CENARIOS In this section, we introduce two aggregation operators, denoted by  and a, allowing to combine two scenarios S and S  of PA in order to obtain a scenario both smallest than S and S  w.r.t.
the partial order a$?a (resp.
a$?+ ) for  (resp.
a).
Hence, the scenario resulting of the aggregation is at least as compact as S and S  .
These two operators will allow us to show that all consistent convex TQCN admits a unique minimal scenario w.r.t.
a$?a or a$?+ .
DedZnition 5: Let S = (V, C), S  = (V, C  ) a VPA .
DedZnition 4: Given a set of point variables V, the binary relations (VPA , a$?a ) and (VPA , a$?+ ) are dedZned by :  a aV aS, S a VPA , S a$?
S  iff av a V, ranka (S, v) a$?
a  rank (S , v) ;  + aV aS, S a VPA , S a$?
S  iff av a V, rank+ (S, v) a$?
rank+ (S  , v).
aV  aV  Example 4: For example, consider the three scenarios S0 , S1 and S2 represented by the graphs G0 , G1 and G2 in Figure 4.
We can check that S0 a$?+ S2 and S1 a$?+ S2 .
However, S0 a$?+ S1 .
Indeed, we have for example ranka (S0 , v1+ ) a$?
ranka (S1 , v1+ ) (ranka (S0 , v1+ ) = 3 and ranka (S1 , v1+ ) = 2).
S  S  is the unique scenario S  = (V, C  ) of VPA having as solution the instantiation D dedZned by D(v) = min{ranka (S, v), ranka (S  , v)} ; S a S  is the unique scenario S  = (V, C  ) of VPA having as solution the instantiation D dedZned by D(v) = (max{width(S), width(S  )} a 1) a min{rank+ (S, v), rank+ (S  , v)}.
Intuitively, from two scenarios S and S  , the operator  (resp.
a) allows to obtain a scenario by preserving, for each v a V, the shortest paths of G< (S) or G< (S  ) of the form v 0 < .
.
.
< v k with v k = v (resp.
of the form v 0 < .
.
.
< v k with v 0 = v).
Note that the operators  and a are close to the operator MAX used in [14] to aggregate preference models.
The relations a$?a and a$?+ are two partial orders, as the following proposition proves:  Proposition 2: Let V a set of point variables.
We have: (VPA , a$?a ) and (VPA , a$?+ ) are redZexive, transitive and antisymmetric.
15 11  Example 5: For example, consider the three scenarios S0 , S1 and S2 respectively represented by the graphs G0 , G1 and G2 in Figure 4.
The reader can check that S2 is S0  S2 .
elements and, with this result, we will show that MinCons is polynomial for the convex TQCNs, contrary to the general case.
Proposition 5: Let N = (V, C) be a convex TQCN of PA and two scenarios S, S  a N .
We have: S  S  a N  and S a S  a N .
Proof.
We prove the property S  S  a N .
The property S a S  a N  can be proved by a similar line of reasoning.
Let D the instantiation dedZned by D(v) = min{ranka (S, v), ranka (S  , v)} for all v a V and, let v, v  two variables belonging to V. We denote by b, b and b the base relations dedZning the constraints between v and v  of S, S  , S  S  , respectively.
Note that by dedZnition b is the base relation satisdZed by the pair (D(v), D(v  )).
We are going to show that b a C(v, v  ) : aV Case b is < and b is <.
We have < ranka (S, v  ) and ranka (S  , v) < ranka (S, v) a   rank (S , v ).
Hence, min{ranka (S, v), ranka (S  , v)} < min{ranka (S, v  ), ranka (S  , v  )}.
Consequently, we have D(v) < D(v  ).
We can conclude that b is < which belongs to C(v, v  ) since b is also <.
aV Case b is < and b is =.
We have ranka (S, v) < ranka (S, v  ) and ranka (S  , v) = ranka (S  , v  ).
Consider the three possible following cases: (a) Case ranka (S  , v) a$?
ranka (S, v).
We can show that D(v) = min{ranka (S, v), ranka (S  , v)} = ranka (S  , v) and D(v  ) = min{ranka (S, v  ), ranka (S  , v  )} = ranka (S  , v  ) = ranka (S  , v).
Hence, D(v) = D(v  ).
Consequently, b is = which belongs to C(v, v  ) since b is also =.
(b) Case ranka (S, v) < ranka (S  , v) < ranka (S, v  ).
We can show that D(v) = min{ranka (S, v), ranka (S  , v)} = ranka (S, v) and D(v  ) = min{ranka (S, v  ), ranka (S  , v  )} = ranka (S  , v  ) = ranka (S  , v).
Hence, D(v) < D(v  ).
Consequently, b is < which belongs to C(v, v  ) because b is also <.
(c) Case ranka (S, v  ) a$?
ranka (S  , v).
We can show that D(v) = min{ranka (S, v), ranka (S  , v)} = ranka (S, v) and D(v  ) = min{ranka (S, v  ), ranka (S  , v  )} = ranka (S, v  ).
Hence, D(v) < D(v  ).
Consequently, b is < which belongs to C(v, v  ) since b is also <.
aV Case b is < and b is >.
We have C(v, v  ) = {<, =, >} since {<, =, >} is the unique convex relation containing both < and >.
Consequently, b belongs to C(v, v  ).
aV Case b is = and b is =.
We have = ranka (S, v  ) and ranka (S  , v) = ranka (S, v) ranka (S  , v  ).
Hence, min{ranka (S, v), ranka (S  , v)} = min{ranka (S, v  ), ranka (S  , v  )}.
Consequently, we have D(v) = D(v  ).
We can conclude that b is = which belongs to C(v, v  ) since b is also =.
aV The other cases corresponds to previous cases by permuting  S and S  or v and v  .
As explain at the beginning of this section, the operators  and a allow to obtain a scenario at least as compact as the scenarios given as inputs and also smaller than the two scenarios w.r.t.
respectively a$?a and a$?+ : Proposition 4: Let S and S  = (V, C  ) be two scenarios belonging to VPA .
We have:  a  a  aV (1) (S  S ) a$?
S and (S  S ) a$?
S ;  +  +  aV (2) (S a S ) a$?
S and (S a S ) a$?
S ;     aV (3) (S  S ) a$?C S, (S  S ) a$?C S , (S a S ) a$?C S and   (S a S ) a$?C S .
Proof.
aV (1) Let S  = (V, C  ) be the TQCN S  S  and D the instantiation on V dedZned by D(v) = min{ranka (S, v), ranka (S  , v)}.
D uses only positive or null integers.
Consequently, we have |{v  a V : C  (v  ) = {<}}| a$?
D(v).
Hence, |{v  a V : C  (v  ) = {<}}| a$?
min{ranka (S, v), ranka (S  , v)}.
Since a  rank (S , v) = |{v  a V : C  = {<}}|, we can conclude a$?
min{ranka (S, v), ranka (S  , v)}.
that ranka (S  , v) a  Hence, rank (S , v) a$?
ranka (S, v) and ranka (S  , v) a$?
ranka (S  , v)}.
Thus, we have (S  S  ) a$?a S and (S  S  ) a$?a S  .
aV (2) The property (2) can be proved by following an approach similar to the one followed for the property (1).
aV (3) The property (3) is a consequence of the properties (1) and (2) of Proposition 3.
 Hence, both aggregation operators  and a allows us to obtain from two scenarios S and S  , a scenario at least as compact as S and S  .
Nevertheless, note that in the general case, when S and S  are scenarios of a TQCN N , the scenarios S  S  and S a S  are not necessarily scenarios of N .
To be convinced of this, consider the TQCN of PA N  = (V, C  ) dedZned by the same set of variables and the same constraints than those of the TQCN N  represented in Figure 3 except for the constraint between v1+ and v2a which is dedZned for N  by the relation {<, >}.
The scenarios S0 and S1 represented in Figure 4 are scenarios of N  .
However, the scenario S2 in the same dZgure, which corresponds to S0  S1 , is not a scenario of N  .
Indeed, the base relation satisdZed by (v1+ , v2a ) is = for this scenario whereas C  (v1+ , v2a ) is the relation {<, >}.
In spite of this, we will show in the next section that the result of the operators  and a on two scenarios of a convex TQCN is always a scenario of this TQCN.
VI.
MinCons AND THE CONVEX TQCNS In this section, we focus on the convex TQCNs, i.e.
TQCNs of PA (resp.
IA) dedZned by relations belonging to the subclass CPA (resp.
CIA ).
We will notably show that the partial orders a$?a and a$?+ restricted to the scenarios of a convex and consistent TQCN admits a unique minimal element.
We will also characterize a polynomial method to compute these minimal  From this proposition, we can establish that for every convex and consistent TQCN of PA N , (N , a$?a ) and (N , a$?+ ) have a unique minimal element :  16 12  integers D(v) and D(v  ).
Consider all possible cases and let us show that b a C(v, v  ) : a Case 1: b is the relation <.
We have k < k  .
Furthermore, there exists two paths of G< (N ) composed of distinct variables of the form v 0 < .
.
.
< v k with v k = v   and w0 < .
.
.
< wk with wk = v  .
Now, suppose that < does not belong to C(v, v  ).
We have C(v, v  ) which corresponds to one of the following relations  {{=, >}, {=}}.
Consider the variable wk a1 .
We have  k a1 ) = {>}.
As N is -consistent, we have C(v , w    C(v, wk a1 ) a (C(v, v  )C(v  , wk a1 )).
Hence, C(v, wk a1 ) is the relation {>} since {=, >}  {>} = {=}  {>} = {>}.
 Hence, w0 < .
.
.
< wk a1 < v is a path of G< (N ) with a length greater than or equal to ranka (N , v  ).
There is a contradiction.
We can conclude that < belongs to C(v, v  ).
a Case 2: b is the relation > or the relation =.
By following a line of reasoning similar to the one of the previous case, we can show that b a C(v, v  ).
aV Lastly, let us show that S is Mina$?a (N ).
Suppose that for a variable v a V, ranka (S, v) > ranka (Mina$?a (N ), v).
Let k = ranka (S, v).
For the graph G< (Mina$?a (N )), every path of the form v 0 < .
.
.
< v has a length strictly lower than k. It is not possible since G< (N ) is a subgraph of G< (Mina$?a (N )) which has a path of this form of length equal to k. Hence, for every v a V, we have ranka (S, v) a$?
ranka (Mina$?a (N ), v).
Consequently, S a$?a Mina$?a (N ).
Since Mina$?a (N ) is the unique minimal element of (N , a$?a ), S is the scenario  Mina$?a (N ).
Proposition 6: Let N = (V, C) be a consistent and convex TQCN of PA.
The partial orders (N , a$?a ) and (N , a$?+ ) admit unique minimal elements, denoted by respectively Mina$?a (N ) and Mina$?+ (N ).
Proof.
Note that N  is a non empty set since N is consistent.
Hence, (N , a$?a ) admits at least one minimal element.
Now, suppose that N admits two different scenarios S and S  such that S and S  are minimal elements of (N , a$?a ).
We will denote by S  the scenario corresponding to S  S  .
From Proposition 5, S  belongs to N .
Moreover, from Proposition 4, we have S  a$?a S and S  a$?a S  .
Since S and S  are two minimal elements of (N , a$?a ), we can assert that S  = S and S  = S  .
Consequently, S = S  .
There is a contradiction.
We can conclude that (N , a$?a ) has a unique minimal element.
With a similar line of reasoning we can prove that (N , a$?+ ) admits a unique minimal element.
 For each convex and consistent TQCN of PA N , we can also show that there is no scenario more compact than Mina$?a (N ) and Mina$?+ (N ): Proposition 7: Let N = (V, C) be a convex and consistent TQCN of PA. For every scenario S a N , we have Mina$?a (N ) a$?C S and Mina$?+ (N ) a$?C S. Proof.
Let S a N .
We denote by S  the scenario corresponding to Mina$?a (N )  S. From Proposition 5, S  is a scenario belonging to N .
Moreover, from Proposition 4, we have S  a$?a Mina$?a (N ) (a) and S  a$?a S (b).
From (a) and the dedZnition of Mina$?a (N ), we can assert that S  = Mina$?a (N ).
Consequently, from this and (b) we have Mina$?a (N ) a$?a S. From Proposition 3, we can conclude that Mina$?a (N ) a$?C S. Mina$?+ (N ) a$?C S can be proved in a similar manner.
  From this result we can establish that the scenarios Mina$?a (N ) and Mina$?+ (N ) of a convex, non trivially inconsistent and -consistent TQCN N can be computed in O(n2 ) with n the number of its variables.
Indeed, for this, in a dZrst step we compute the graph G< (N ).
Then, in a second step, we compute for each v a V, ranka (N , v) and rank+ (N , v) from the directed acyclic graph G< (N ).
In a last step, the constraints of Mina$?a (N ) and Mina$?+ (N ) are computed from the integers obtained in the previous step.
Each of these steps can be realized in O(n2 ).
For convex TQCNs, from Proposition 7 we know that Mina$?a (N ) and Mina$?+ (N ) belong to the set of scenarios which are the most compact.
From this and the previous proposition, we can establish that MinCons is a polynomial problem for the convex TQCNs:  Now, given a consistent and convex TQCN of PA N , we are going to establish a result allowing to compute the two scenarios Mina$?a (N ) and Mina$?+ (N ).
Proposition 8: Let N = (V, C) be a not trivially inconsistent, -consistent and convex TQCN of PA. We have : aV  aV  (1) Mina$?a (N ) corresponds to the scenario on V having as solution the instantiation D dedZned by: for all v a V, D(v) = ranka (N , v); (2) Mina$?+ (N ) corresponds to the scenario on V having as solution the instantiation D dedZned by: D(v) = rank+ (N , v).
Theorem 2: PMinCons(CPA ) and IMinCons(CIA ) are polynomial and can be solved in O(n3 ), with n the number of variables of the TQCN considered.
Proof.
Due to lack of space, we just give a proof for the dZrst property.
To prove the property (2), we can use a similar approach.
Note that since N is -consistent and not trivially inconsistent, N is consistent.
Let S = (V, C  ) be the scenario having as solution the instantiation D dedZned by D(v) = ranka (N , v) for all v a V. aV First, let us show that S is a scenario of N .
Let v, v  a V and b the base relation dedZning the constraint C  (v, v  ), i.e.
the base relation satisdZed by the pair (D(v), D(v  )).
Sometimes, we will denote by respectively k and k  the two  Proof.
aV Let D = (N , k) be an instance of PMinCons(CPA ).
To answer to the question associated with D we can in a dZrst step compute (in O(n3 )) the closure under weak composition of N , i.e.
the TQCN (N ).
(N ) is convex and equivalent to N .
In the case where (N ) is trivially inconsistent, we know that N is not consistent and the answer of D is no.
In the contrary case, in a second  17 13  v0a v1a  v0+  v1 v3a  v2  v0  v1+ v2+  v3+  v2a  v3  0  1  2  3  v0a v1a  v0+ v1+ v2a  v3a  v2+ v3+  (a) G3  v1  v2  v0  v3  0  1  2  3  v0a v1a  v0+ v1+  v2a v3a  v2+ v3+  (b) D1  (c) D2  v0  v0 {eq} {m}  v1  {eq}  {p} {p} v2  {p}  v1  {m}  {p} {p} v2  {p}  {f i}  {eq}  v3  v3  (d) S1  (e) S2 Fig.
5.  aV  step we compute the maximal length k  of the paths of G< ((N )) (in O(n2 ).
We know that k  corresponds to the width of the most compact scenarios of N since k  = width(Mina$?a ((N ))) = width(Mina$?+ ((N ))) (Proposition 8).
Hence, if k aL k  then the answer of D is yes, otherwise the answer is no.
Let D = (N , k) an instance of IMinCons(CIA ) with N = (V, C).
To answer to the question associated with D (or dZnd a compact scenario of N ), we can consider the equivalent problem of PMinCons(CPA ) corresponding to the instance D  = (PA(N ), k).
Remind that PA(N ) corresponds to the translation of N into TQCN N  of PA dedZned on the set of variables Points(V).
  [2] M. Vilain, H. Kautz, and P. van Beek, aConstraint propagation algorithms for temporal reasoning: a revised report,a Qualitative Reasoning about Physical Systems, pp.
372a381, 1990.
[3] J. Huang, J. J. Li, and J. Renz, aDecomposition and tractability in qualitative spatial and temporal reasoning,a Artif.
Intell., vol.
195, pp.
140a164, 2013.
[4] T. Drakengren and P. Jonsson, aA complete classidZcation of tractability in allenas algebra relative to subsets of basic relations,a Artif.
Intell., vol.
106, no.
2, pp.
205a219, 1998.
[5] C. Boutilier, aToward a logic for qualitative decision theory,a in KRa94, 1994, pp.
75a86.
[6] A. N. The and A. TsoukiaEs, aNumerical representation of pqi interval orders,a Discrete Applied Mathematics, vol.
147, no.
1, pp.
125a146, 2005.
[7] J.-F. Condotta and S. Kaci, aCompiling preference queries in qualitative constraint problems,a in Proc.
of The 26th Intern.
FLAIRS Conference (FLAIRSa13), pp.
1-6, 2013.
[8] J. Renz and G. Ligozat, aWeak composition for qualitative spatial and temporal reasoning,a in Proc.
of the 11th Intern.
Conf.
CPa05, Spain, LNCS, vol.
3709, 2005, pp.
534a548.
[9] J. F. Allen, aAn interval-based representation of temporal knowledge,a in IJCAI, 1981.
[10] M. Vilain and H. Kautz, aConstraint Propagation Algorithms for Temporal Reasoning,a in Proc.
of the Fifth National Conf.
on ArtidZcial Intelligence (AAAIa86), 1986, pp.
377a382.
[11] K. NoEkel, aTemporally distributed symptoms in technical diagnosis,a LNCS, vol.
517, pp.
1a184, 1991.
[12] P. Van Beek and R. Cohen, aExact and approximate reasoning about temporal relations,a Computational Intelligence, vol.
6, pp.
133a44, 1990.
[13] B. Nebel and H.-J.
BuErckert, aReasoning About Temporal Relations: A Maximal Tractable Subclass of Allenas Interval Algebra,a JACM, vol.
42, pp.
43a66, 1995.
[14] S. Kaci and L. W. N. van der Torre, aAlgorithms for a nonmonotonic logic of preferences,a in Proc.
of The 8th Euro.
Conf.
Symbolic and Quantitative Approaches to Reasoning with Uncertainty (ECSQARUa05), 2005, pp.
281a292.
VII.
C ONCLUSION In this paper, we introduced and studied the problem which consists in charachterizing compact solutions or scenarios of a TQCN and a related decision problem called MinCons.
We showed that these problems are NP-hard for PA and for IA.
Moreover, we proved that for the particular of the convex relations these problems are polynomial.
For this, by following an approach based on particular partial orders and aggregation operators on scenarios, we proved that we can obtain in polynomial time particular compact scenarios for the consistent convex TQCNs.
A dZnal objective of our work is to identify all tractable cases for MinCons of IA and dedZne methods to solve this problem for the general case.
R EFERENCES [1] J. F. Allen, aMaintaining knowledge about temporal intervals,a vol.
26, no.
11.
New York, NY, USA: ACM, 1983, pp.
832a843.
18 14
Temporal Implications of Database Information Accountability Kyriacos E. Pavlou and Richard T. Snodgrass Department of Computer Science University of Arizona Tucson, USA {kpavlou}{rts}@cs.arizona.edu  Abstract--Information restriction controls access and renders records immutable; information accountability requires data transparency to easily and efficiently determine when a particular use is appropriate.
Information accountability in the context of relational databases is associated with time in a surprising number of ways, as is summarized in this paper.
Notarization and validation of a database exploit the temporal semantics of a transaction-time database.
A corruption can be associated with multiple times.
Forensic analysis determines the when: bounds on the corruption time, and the where: also specified in terms of time.
These bounds are depicted in a two-dimensional corruption diagram, with both axes denoting time.
The various kinds of corruption events are defined in terms of time.
A parameter termed the regret interval has significant security and performance implications.
This paper emphasizes the deep connections between time and the definition, detection, forensic analysis, and characterized extent of a database corruption within the context of information accountability.
Keywords-information accountability; temporal semantics; transaction-time databases; forensic analysis;  The prevailing approach to achieving privacy and security for databases is information restriction.
For example, ensuring record compliance, or information compliance in general, usually entails rendering retained records immutable and controlling access to them.
We feel that the means of addressing security and compliance should be viewed as constituting a spectrum.
If one asserts that information restriction lies at one end of the spectrum then the question which inevitably arises is what lies at the other end?
In a recent article Weitzner et al.
[1] argue that access control and cryptography are not capable of protecting information privacy and that there is a true dearth of mechanisms for effectively addressing information leaks.
They propose that as an alternative information accountability "must become a primary means through which society addresses appropriate use" [1].
Information accountability assumes that information should be transparent so as to easily determine whether a particular use is appropriate under a given set of rules.
A related concept is continuous assurance technology, defined as "technology-enabled auditing which produces audit results simultaneously with, or a short period of time after, the occurrence of relevant events" [2].
This concept is crucial because it can be used to achieve a meaningful operationalization of information accountability.
Security Information Accountability Provenance  Watermarking  this article  Temporal Databases  Time  Figure 1.
Databases  The context of the present paper.
This paper studies the overlap of time, databases, and security, as shown in Figure 1.
We have encountered time in this context in many different places and under many different guises.
This suggests a deep connection between the general topics of (i) temporal databases and (ii) information accountability.
Our purpose here is to identify the many instances where time appears in the definition and implementation of information accountability and to discuss new time-security interactions we have identified recently as described in Sections VIII-X.
More generally, we hope that the many tantalizing glimpses of a fundamental connection between temporal databases and information security, as seen in this abbreviated trip through the stages of database corruption, detection, and subsequent forensic analysis, will encourage further work within this community to uncover the source(s) of this connection.
I. T HE AUDIT S YSTEM In this section we describe how to audit a database and summarize the tamper detection approach we previously proposed and implemented [3].
We give the gist of our approach, so that its temporal implications can be understood.
Table I lists the audit system execution phases, their subphases, and the actions performed during each.
The Normal Processing execution phase differentiates between the Total Chain Computation and the Tamper Detection and Partial Chain Computation subphases.
In the first subphase transactions are hashed and cumulatively  Table I AUDIT SYSTEM EXECUTION PHASES , SUBPHASES , AND ACTIONS .
Execution Phases  Subphases Total Chain Computation  Normal Processing  Tamper Detection and Partial Chain Computation  Actions - Hashing to create total chain - Notarization of total chain - Re-hashing of total chain - Validation of total chain If required by forensic algorithm: - Hashing to create partial chains - Notarization of partial chains  Corruption Region Analysis  - Running a forensic algorithm to determine where and when  Manual Analysis  - Determining who and why  Forensic Analysis  linked (by increasing transaction commit time) using a cryptographically-strong hash function, and the resulting values are digitally notarized by an external digital notarization service.
In the latter subphase hash values are recomputed and compared with those previously notarized.
It is during validation of the total chain that tampering is detected, when the just-computed hash value doesn't match the one previously notarized.
The validator provides a vital piece of information, that tampering has taken place, but doesn't offer much else.
Since the hash value is the accumulation of every transaction ever applied to the database, we don't know when the tampering occurred, or what portion of the monitored database was corrupted.
We have introduced a variety of database forensic algorithms [4], [5], [6] to provide partial answers to these questions.
Note that certain forensic analysis algorithms require the computation and notarization of one or more partial hash chains during the scan of the entire database that occurs during validation.
Details on performance, clarifications on the role of the external digital notarization service, and all the forensic analysis algorithms are beyond the scope of this paper and can be found elsewhere [5], [6].
II.
T HREAT M ODEL Time is first encountered in the underlying threat model.
We assume a Trusted Computing Base (TCB) consisting of correctly booted and functioning hardware and a correctly installed operating system and DBMS.
More precisely, we assume that the TCB is correctly functioning, the DBMS is created, maintained, and operates in a secure manner, and all network communication is performed through secure channels (such as SSL), ensuring the correctness of the internal state of the DBMS.
A tampering by an adversary ("Bob") occurs at time tc .
This tampering can take many forms.
In general, we assume that an intruder (or an insider) who gains physical access to the DBMS server will have full freedom to corrupt any database file.
III.
TAMPERING The very definition of tampering can be stated in terms of time.
Users and applications modify the database during  normal processing, and later query that data.
So how can tampering be differentiated from normal processing?
To achieve this we introduce transaction-time support to the database.
A transaction-time database records the history of its content [7].
All past states are retained and can be reconstituted from the information in the database.
This is ensured through the append-only property of a transactiontime database: modifications only add information; no information is ever deleted.
Thus the database itself can serve as an audit log.
It is this basic property that we exploit to validate the table.
Fortunately, the SQL:2011 standard and many commercial DBMSes now provide transaction-time support.
Oracle 10g added support for valid-time tables, transaction-time tables, bitemporal tables, sequenced primary keys, sequenced uniqueness, sequenced referential integrity, and sequenced selection and projection, in a manner quite similar to that proposed in SQL/Temporal.
Oracle 11g enhanced support for valid-time queries [8].
Teradata recently announced support in Teradata Database 13.10 of most of these facilities as well [9], as did IBM for DB2 10 [10].
A normal modification of a tuple can only be performed on the most recent version (that with a stop time of "until changed").
The modification changes the stop time to the current time (for deletions or updates) and inserts a new record with the current time as the start time (for insertions or updates).
A modification is considered a tampering or corruption if it (a) changes any tuple with a stop time other than "until changed", (b) inserts a tuple with a start time other than the current time and a stop time other than "until changed", (c) modifies the explicit attributes of any tuple, or (d) physically deletes any tuple.
Note that the first two conditions involve timestamps stored in the database; the second condition explicitly mentions "current time."
Specifically, any modification other than a temporal upward compatible modification [11] is considered tampering.
Section VII will examine a more refined taxonomy of corruptions.
IV.
TAMPER D ETECTION We now examine tamper detection in more detail.
Suppose that we have just detected a corruption event (or CE), which is any event that corrupts the data and compromises the database.
Table II summarizes all the time-related concepts used in this paper.
Time instants are generally denoted by a subscripted t, time intervals by a subscripted I or R. Factors are integers.
A temporal or a spatial bound occurs at a time instant, as does an event.
There exists a one-to-one correspondence between a CE and its corruption time (tc ), which is the actual time instant (in seconds) at which a CE has occurred.
Figure 2 shows that tc marks the transition from a legal database state to an illegal one.
Table II S UMMARY OF TIME - RELATED CONCEPTS .
Symbol CE  Name Corruption event  VE  Validation event  NE  Notarization event  tc tv IV tn IN V  Corruption time Validation time Validation interval Notarization time Notarization interval Validation factor  tl  Locus time  Rs  Spatial detection resolution Notarization factor Temporal detection resolution Time of first validation failure Time of most recent validation success  N Rt tFVF tRVS LTB  Lower temporal bound  UTB  Upper temporal bound  LSB  Lower spatial bound  USB  Upper spatial bound  tb tp Imax  tran  Backdating time Postdating time Transaction max-duration  IR  Regret interval  * IR IRP ts ILH  Regret interval estimate Retention interval Shred time Litigation hold interval  Iqv  Query verification interval  Definition An event that compromises the database The validation of the audit log by the notarization service The notarization of a document (hash value) by the notarization service The time instant of a CE The time instant of a VE The time between two successive VEs The time instant of a NE The time between two successive NEs The ratio IV /IN The time instant that the corruption locus data (lc ) was stored Finest interval chosen to express the spatial bounds uncertainty of a CE The ratio IN /Rs Finest interval chosen to express the temporal bounds uncertainty of a CE Time instant at which the CE is first detected The time instant of the last NE whose revalidation yielded a true result Lower bound of the temporal uncertainty of the corruption region Upper bound of the temporal uncertainty of the corruption region Lower bound of the spatial uncertainty of the corruption region Upper bound of the spatial uncertainty of the corruption region The time a timestamp was backdated to The time a timestamp was postdated to Maximum duration of a transaction Minimal time interval before an adversary can reverse a change Lower bound on the regret interval Length of the retention period The time a tuple is shredded A duration of time specified by a court of law Interval between the time a transaction reads data and the time when tampering is detected  The following discussion relates to our approach to effecting information accountability in relational databases.
A CE is detected during a validation event (or VE ) of the database by the notarization service.
A validation can be scheduled (that is, is periodic) or could be an ad hoc VE.
The time (instant) at which a VE occurs is termed the time of validation event, and is denoted by tv .
If validations are periodic, the time interval between two successive validation events is termed the validation interval, or IV .
The validator compares the hash value it computes over the data with the hash value that was previously notarized.
Tampering is indicated by a validation failure, in which the digital notarization service returns false for the particular query of a hash value and a notarization time.
What is desired is a validation success, in which the notarization service returns true, stating that everything is OK: the data has not been tampered.
A notarization event (or NE) is the notarization of a document (specifically, a hash value) by the notarization service.
As with validation, notarization can be scheduled (is periodic) or can be ad hoc.
Each NE has an associated notarization time (tn ), which is a time instant.
If notarizations are periodic, the time interval between two successive notarization events is termed the notarization interval, or IN .
The validation interval should be equal to or longer than the notarization interval, should be an integer multiple of the notarization interval, and should also be aligned with it, that is, validation should occur immediately after notariza-  tc  Transaction Processing  DBMS state (Legal)  Tampering  Figure 2.  tFVF  DBMS state (Illegal)  Validation  Tamper Detection  Forensic Analysis Algorithm  Corruption Region(s)  Tampering, Detection, and Forensic Analysis.
tion.
This is because one can only validate what one has previously notarized.
Having non-aligned notarization and validation intervals can only delay tamper detection.
Thus we speak of the validation factor V such that IV = V * IN .
As long as this constraint is respected, it is possible to change V , or both IV and IN , as desired.
This, however, will affect the size of the corruption region as emphasized in Section VI.
There are several variables associated with each corruption event.
The first is the data that has been corrupted, which we term the corruption locus data (lc ).
Forensic analysis, as discussed in Section VI, involves temporal detection : the determination of the corruption time, tc .
Forensic analysis also involves spatial detection, the determination of "where," that is, the location in the database of the data altered in a CE.
(Note that the use of the adjective "spatial" does not refer to a spatial database, but rather where in the database the corruption occurred.)
Interestingly, even the corruption locus is specified in terms of time.
Recall that each transaction is hashed.
Therefore, in the absence of other information, such as a previous dump (copy) of the database, the best a forensic analysis can do is to identify the particular transaction that stored the data that was corrupted.
Instead of trying to ascertain the corruption locus data (lc ), we will instead be concerned with the locus time (tl ), the time instant the data was originally stored.
The locus time specifically refers to the time instant when the transaction storing the corruption locus data commits.
(Here we are referring to the specific version of the data that was corrupted.
This version might be the original version inserted by the transaction, or a subsequent version created through an update operation.)
A CE can have many lc 's (and hence, many tl 's) associated with it.
Such a CE is termed multi-locus: an intruder (hardware failure, etc.)
might alter many tuples.
A CE having only one lc (such as due to an intruder hoping to remain undetected by making a single, very particular change) is termed a single-locus CE.
Now we can formally define what a corruption event is.
Definition 1.
A corruption event is a two-tuple (Tl , tc ).
The set Tl = {tl1 , tl2 , .
.
.
, tln } is the set of all locus times associated with a particular corruption event.
Each (tli , tc ) [?]
T2 , where T is a time domain.
We define Rs as the finest interval chosen to express the uncertainty of the spatial bounds of a CE.
Rs is called the spatial detection resolution.
This is chosen by the database administrator (DBA).
Similarly, the finest interval chosen by  the DBA to express the uncertainty of the temporal bounds of a CE is the temporal detection resolution, or Rt .
Several others works have studied tamper detection in databases.
An example of a WORM-based, long-term highintegrity retention technique for fine granularity business records is the transaction log on WORM (TLOW) approach for supporting long-term immutability of relational tuples [12].
TLOW stores the current database instance in ordinary storage and the transaction log on Write-Once-ReadMany (WORM) storage, while dispensing with a compliance log altogether.
The audit process uses hash values representing the data rather than the data themselves.
An audit is successful if the hash from the old database snapshot plus the hash of all the new tuples introduced in the transaction log match the hash of the current database instance.
Thus within this tamper detection framework the same notions of corruption event, auditing/validation interval, and time of first validation failure can be defined.
Another time-related concept, the query verification interval, is specific to TLOW.
Guo, Jajodia, Li, and Liu formulated a fragile watermarking scheme for database tamper detection [13], [14].
Their scheme is based on a watermark that is invisible (watermark does not distort data) and can be blindly verified (original unmarked relation is not required for verification).
During verification, the extracted watermark indicates the locations of alterations.
This approach does not utilize a temporal definition of tampering.
V. T HE C ORRUPTION D IAGRAM To explain forensic analysis within the context of our approach, we introduce the Corruption Diagram, which is a graphical representation of CE(s) in terms of the temporalspatial dimensions of a database.
Figure 3 illustrates a simple corruption event.
While this figure may appear to be complex, the reader will find that it succinctly captures all the important information regarding what is stored in the database, what is notarized, and what can be determined by the Monochromatic Forensic Analysis Algorithm--the simplest of the algorithms we have proposed--about the corruption event.
This corruption diagram shares some aspects with commonly-encountered bitemporal diagrams [15].
In a bitemporal diagram, the axes are transaction time and valid time, with rectangular polygons indicating the bitemporal extent of facts.
As we will see, the corruption diagram conveys very different information, while having the surface similarity of being a two-dimensional depiction, with time as both dimensions.
First we give the definition of a corruption diagram before describing it in detail.
Definition 2.
Let T be a time domain.
A corruption diagram is a plot in T2 having its ordinate associated with wallclock time and its abscissa associated with a partition of the database according to transaction time.
This diagram  t FVF = UTB 24 tc  First Validation Failure (FVF) VE NE124  .
CE 22  NE11 NE10  LTB 18  VE3 NE 9 NE8 NE7 VE2 NE6  When NE5  IV = 6 = 3 .
IN Rt = 6  NE4 VE NE3 1 NE2 IN = 2 Rs = 2  NE1 NE0  tRVS = LSB  Where  16 USB tl  t FVF  Figure 3.
Corruption diagram for a data-only single-locus retroactive corruption event.
depicts corruption events and is annotated with hash chains and relevant notarization and validation events.
At the end of forensic analysis, this diagram can be used to visualize the regions ([?]
T2 ) where corruption has occurred.
Let us first consider the simplest case.
During validation, we have detected a corruption event.
Though we don't know it (yet), assume that this corruption event is a single-locus CE.
Furthermore, assume that the CE just altered the data of a tuple; no timestamps were changed.
The x-axis represents when the data are stored in the database.
The database was created at time 0, and is modified by transactions whose commit time is monotonically increasing along the x-axis.
(In temporal database terminology [7], the x-axis represents the transaction time of the data.)
In the corruption diagram, time moves inexorably to the right.
The x-axis is labeled "Where."
The database grows monotonically as tuples are appended (recall that the database is append-only).
As previously explained, we designate "where" a tuple or attribute is in the database by the time of the transaction that inserted that tuple or attribute.
We delimit the days by marking each midnight, or, more accurately, the time of the last transaction to commit before midnight.
A 45-degree line is shown and is termed the action line, as all the action in the database occurs on this line.
The line terminates at the point labeled "FVF," which is the validation event at which we first became aware of tampering.
The time of first validation failure (or tFVF ) is the time at which the corruption is first detected.
(Hence the name: a corruption diagram always terminates at the VE that detected the corruption event.)
Note that tFVF is an instance of a tv , in that tFVF is a specific instance of the time of a validation event.
Also note that in every corruption diagram,  tFVF coincides with the current time.
For example, in Figure 3 the VE associated with tFVF occurs on the action line, at its terminus, and turns out to be the fourth such validation event, VE4 .
The actual corruption event is shown as a point labeled "CE," which always resides above or on the action line, and below the last VE.
If we project this point onto the x-axis, we learn "where" (in terms of the corruption locus time, tl ) the corruption event occurred.
The y-axis represents the temporal dimension (actual time-line) of the database, labeled in time instants.
Any point on the action line thus indicates a transaction committing at a particular transaction time (a coordinate on the x-axis) that happened at a clock time (the same coordinate on the y-axis).
For this reason, the two times are totally correlated and the action line is always a 45-degree line.
Projecting the CE onto the y-axis tells us when in clock time the corruption occurred, that is, the corruption time, tc .
We label the y-axis with "When."
The diagram shows that the corruption occurred on day 22 and corrupted an attribute of a tuple stored by a transaction that committed on day 16.
Notarization event NE1 hashes the transactions occurring during the first two days (here, the notarization interval, IN , is two days), linking these hash values together using linked hashing [3].
This is illustrated with the upward-rightpointing arrow with the solid black arrowhead originating at NE0 (since the linking starts with the hash value notarized by NE0 ) and terminating at NE1 .
Each transaction at commit time is hashed; here the "where" (transaction time) and "when" (wall-clock time) are synchronized; hence, hashing occurs on the diagonal.
The hash value of the transaction is linked to the previous transaction, generating a linked sequence of transactions that is associated with a hash value notarized at midnight of the second day in wall-clock time and covering all the transactions up to the last one committed before midnight (hence, NE1 resides on the action line).
NE1 sends the resulting hash value to the notarization service.
Also along the action line are points denoted with "VE."
These are validation events for which a validation occurred.
During VE1 , which occurs at midnight on the sixth day (here, the validation interval, IV , is six days), rehashes all the data in the database in transaction commit order, denoted by the long right-pointing arrow with a white arrowhead, producing a linked hash value.
In fact, the diagram shows that VE1 , VE2 , and VE3 were successful (each scanning a successively larger portion of the database, the portion that existed at the time of validation).
The diagram also shows that VE4 , immediately after NE12 , failed, as it is marked as FVF; its time tFVF is shown on both axes.
In summary, we now know that at each of the VEs up to but not including FVF succeeded.
When the validator scanned the database as of that time (tv for that VE), the hash value matched that notarized by the VE.
Then, at the last VE, the FVF, the hash value didn't match.
The corruption  event, CE, occurred before midnight of the 24th day, and corrupted some data stored sometime during those twenty four days.
VI.
F ORENSIC A NALYSIS Once the corruption has been detected, a forensic analysis algorithm, like the Monochromatic Algorithm, springs into action.
The task of this algorithm as shown in Figure 2, is to ascertain, as accurately as possible, the corruption region: the bounds on "where" and "when" of the corruption.
On validation failure we know that the corruption must lie in the upper-left triangle, delimited by the When and action axes, denoting that the corruption event occurred before tFVF and altered data stored before tFVF .
The most recent VE before FVF is VE3 and it was successful.
This implies that the corruption event has occurred in this time period.
Thus tc is somewhere within the last IV , which always bounds the "when" of the CE.
To bound the "where," the Monochromatic Algorithm can validate prior portions of the database, at times that were earlier notarized.
Revisiting and revalidating the cumulative hash chains at past notarization events will yield a sequence of validation results that start out to be true and then at some point switch to false (TT.
.
.TF.
.
.FF).
This single switch from true to false is a consequence of the cumulative nature of the total hash chain.
We term the time of the last NE whose revalidation yielded a true result (before the sequence of false results starts) the time of most recent validation success (tRVS ).
This tRVS helps bound the "where" of the CE because the corrupted tuple belongs to a transaction which committed between tRVS and the next time the database was notarized (whose validation now evaluates to false).
tRVS is marked on the Where axis of the corruption diagram in Figure 3.
Definition 3.
In light of the above observations, we define the four bounds of a CE.
* * * *  the lower temporal bound: LTB := max(tFVF -IV , tRVS ), the upper temporal bound: UTB := tFVF , the lower spatial bound: LSB := tRVS , and the upper spatial bound: USB := tRVS + IN .
These bounds define a corruption region, indicated in Figure 3 as a narrow rectangle, within which the CE must lie.
This example shows that, when utilizing the Monochromatic Algorithm, the notarization interval, here IN = 2 days, bounds the "where," and the validation interval, here IV = 6 days, bounds the "when."
Hence for this algorithm, Rs = IN and Rt = IV .
(More precisely, Rt = UTB - LTB = min(IV , tFVF - tRVS ) due to the fact that Rt can be smaller than IV for late-breaking corruption events, such as that illustrated in Figure 5.)
First Validation Failure (FVF) VE NE124  t FVF = UTB 24  When  .
.
CE postdating  tc 22  IV  NE11  t FVF = UTB 24  When  backdating  NE12 VE4 (FVF)  .
tc 22  CE IV  LTB  NE10  NE10 LTB 18  V =3 N=1 RS = 2  VE NE9 3 IN  Where  Figure 4.
18 IN NE8  22 USB t FVF tl  Postdating and backdating corruption events.
Other forensic analysis algorithms we have proposed make use of partial hash chains in addition to the total chain.
These partial chains are computed and notarized during the re-hashing and validation of the total chain.
The partial chains hash only parts of the data in the database and in certain cases are not cumulative.
Their "placement," i.e., the parts of the database they collectively cover, creates a structure over the database that allows the forensic algorithm to prune the search space efficiently and thus correctly locate multiple CEs very quickly.
It also allows the decoupling of the spatial detection resolution (Rs ) and IN .
In fact, the value of Rs can be set by the DBA to be much smaller that the value of IN .
VII.
C HARACTERIZATION OF C ORRUPTION T YPES The CE shown in Figure 3 is termed a retroactive corruption event: a CE with locus time tl appearing before the next to last validation event.
As we will see in this section, this is but one of several corruption types, characterized by various temporal relationships.
A. Data-Only Corruptions Figure 5 illustrates an introactive corruption event: a CE with a locus time tl appearing after the next to last validation event.
In this figure, the corruption event occurred on day 22, as before, but altered data on day 22 (rather than day 16 as in the diagram of Figure 3).
NE10 is the most recent validation success.
Here the corruption region is a trapezoid rather than a rectangle.
This shape is implied by the definition of LTB.
Both retroactive and introactive corruptions are types of data-only corruption events.
B. Timestamp Corruptions Data-only corruption events do not change any timestamps in the tuples.
However, there are two other kinds of corruption events that arise from timestamp corruption.
In a backdating corruption event, a timestamp is changed to indicate a previous time/date with respect to the original time in the tuple.
We term the time a timestamp was backdated to the backdating time, or tb .
It is always the case that tb < tl .
Similarly, a postdating corruption event changes a timestamp to indicate a future time/date with respect to the original commit time in the tuple, with the postdating  V =3 N=1 RS = 2  VE NE9 3  NE8 tRVS = LSB  Imax_tran NE11  tRVS = LSB  Where  22 USB t FVF tl  Figure 5.
Corruption diagram with introactive data-only CE and envelope.
time (tp ) being the time a timestamp was postdated to.
It is always the case that tl < tp .
Both types of timestamp corruption are illustrated in Figure 4.
Observe that timestamp corruption produces two corruption regions since changing a timestamp effectively changes the order in which the tuple is hashed into the total chain.
Combined with the previously introduced distinction of retroactive and introactive, these considerations induce six specific corruption event types.
(  Retroactive Introactive  )    Data-only       Backdating x     Postdating    For backdating corruption events, we ask that the forensic analysis determine, to the extent possible, "when" (tc ), "where" (tl ), and "to where" (tb ).
Similarly, for postdating corruption events, we want to determine tc , tl , and tp .
This is quite challenging given the only information we have, which is a single bit for each query on the notarization service.
We have proposed a taxonomy of many other corruption types along with a forensic analysis protocol on how to identify each type (vid.
http://www.cs.arizona.edu/projects/ tau/dragoon/taxonomy protocol.pdf).
Please note that the monitored database need not support valid time for forensic analysis to work but if it is a bitemporal database then more complex corruptions involving valid-time timestamps can arise, not yet considered in the taxonomy.
VIII.
V ERY R ECENT C ORRUPTIONS Corruption events that occur very recently present a challenge to forensic analysis.
The problem arises from the assertion that a CE cannot occur below the 45-degree line.
The argument is that it is impossible for tc < tl to occur, because that would imply that the data are corrupted before they are added to the database.
This argument holds generally, but there does exist an exception, where the CE can be below the action axis: when the CE corrupts data of a currently executing transaction.
Since a single transaction takes a finite non-zero amount of time, there is a window of opportunity between when the transaction starts and when it commits during which a corruption can occur.
In such a case we will have tc < tl and the CE will be below the 45-degree line.
Fortunately, the existence of such corruptions does not invalidate any of our previous analysis.
Only the "at-thetime-current" transactions in the most recent validation interval are susceptible to this threat.
Hence, the first change to be made is to draw a straight-line "envelope" parallel to the 45-degree action axis, whose horizontal distance from the action axis represents the maximum duration of any single transaction, denoted by Imax tran .
In this way the corruption region is augmented with a narrow slice, as shown in Figure 5, to account for this possibility.
This window of opportunity for each transaction varies since it depends on the duration of the transactions and that is the reason Imax tran was chosen as the width of the "envelope."
The only case where this will affect the result of the forensic analysis is when the CE is an introactive CE: only then can it approach and move past the 45-degree line.
Thus, in such a case the upper bound on tl will be increased by Imax tran .
Note also that this weakness has a tradeoff: an introactive CE puts a tighter lower bound on tc , meaning it is easier to find the actual time of corruption.
Observe that in Figure 5 the LTB does not coincide with VE3 but is instead raised from day 18 to day 21.
A different approach to solving the above problem is to introduce the notion of a regret interval [16].
This is a minimal time interval, IR , before any adversary can reverse the change they have made.
For example, in current legal interpretations of email compliance according to SarbanesOxley [17], this time interval is zero.
However, when monitoring bioscience lab results, we may be able to assume that after a new record is added, a week will pass--given that certain protocols require several days to complete--before any adversary is likely to "regret" its existence.
The true size of the regret interval is intrinsic to the semantics and social use of the application.
Note that the DBA has no control over it.
Furthermore, the DBA may not be able to determine its size with absolute certainty.
However, the DBA can estimate it with a (possibly) tight lower bound.
We call this the regret interval estimate and * * denote it by IR .
Observe that IR <= IR .
The existence of a nonzero regret interval estimate can be leveraged to increase throughput.
However, in order not to compromise the correctness of tamper detection and subsequent forensic analysis the DBA must ensure that notarization of hash values happens at time intervals which are smaller than the estimated regret interval.
This forces all tamperings to transpire after a notarization, something that * ensures tamper detection.
Thus we have IN < IR <= IR .
Moreover, validation events have to occur after notarizations (one cannot validate a hash value that has not already been notarized).
If we set the validation interval to be smaller than the estimated regret interval then we have * IN <= IV < IR <= IR and this enforces the absence of introactive corruption events.
IX.
S HREDDING AND L ITIGATION H OLDS Transaction-time semantics allow us to keep the history of the entire database.
This in turn enables the constant monitoring of the database state in order to detect any deviation.
Keeping the totality of data that were ever stored in the database causes complications.
First it has an adverse effect on performance since all the data have to be rehashed during validation.
The ever-increasing cost of notarizing and validating the data will at some point become prohibitive.
Moreover, companies are not wont to keep legacy data for long periods of time since such a practice poses a privacy and liability threat.
In general, old data are periodically deleted while newer data must be kept for a specific retention period according to regulations and company policy.
Therefore, a sliding time frame, the length IRP of the retention period, exists whereby records continuously become old enough to fall outside that window as time progresses.
Such records are deemed safe for physical deletion (shredding).
Shredding itself occurs at time ts , at any time after the record exits the time frame now - IRP .
This is a serious issue because shredding breaks transaction-time semantics that requires that the monitored database is append-only.
To complicate the situation even further litigation holds can be issued on the data for a duration of time, ILH , specified by courts of law.
Such a hold overrides any retention period regulations and so none of the data can be subject to shredding until the hold is lifted.
Thus it can be said that litigation holds restore transaction-time semantics.
Similar concepts have been described extensively elsewhere [18].
X. E NTERPRISE C ONSIDERATIONS Companies typically have many databases, a number of which are susceptible to tampering and thus fall under the purview of database information accountability.
It is useful for the company's Chief Security Officer (CSO) to set general corporate policies on acceptable values for those parameters in Table II.
So for example the CSO could dictate that the validation interval IV be no longer than 2 days and that the spatial detection resolution Rs be no longer than one hour, applicable to all databases being monitored.
The database administrator could then indicate, for a particular database or perhaps even particular tables, the specific values for the spatial detection resolution Rs and the regret interval * estimate IR .
These values would subsequently dictate other values, such as IN and IV .
As the values are related in ways summarized in previous sections, the CSO and the DBA both have flexibility in what to specify.
Note that it is important to record these various enterprisewide and database-specific settings.
These settings have a profound influence on the quality of the forensic analysis, and a cost model has been developed which incorporates these settings, in order to assess the forensic cost associated with each algorithm.
The effect of the settings on the forensic cost has been experimentally studied and verified [5].
Because of their importance, all settings are stored in a separate enterprise security database, itself a transactiontime database (with some bitemporal portions), located in the trusted computing base.
We need to know when each setting was specified.
Time again makes its presence known.
XI.
S UMMARY AND F UTURE W ORK As demonstrated throughout this paper, time arises in many guises: in the definition of tampering, the data that is tampered, the kinds of corruptions that can occur, the detection of tampering, the forensic analysis of tampering, and the "when" and even the "where" of the tampering determined by that analysis.
Table II lists a full two dozen of the time instants and intervals involved throughout the definition and mechanism of database information accountability.
It would be interesting to see how defining tampering in terms of pattern recognition of complex events [19] can affect detection.
Our intuition tells us that it would allow detection of tampering at a semantic level wherein modifications to the database issued through the DBMS can be identified as illegal.
In general, there is something truly fundamental going on, of which we are now seeing just the surface structure.
Determining that deep structure is our challenge to this community.
ACKNOWLEDGEMENT We gratefully acknowledge support from NSF grants IIS-0415101, IIS-0803229, IIS-0639106, and EIA-0080123.
Grants from Microsoft Corporation and from Surety, LLC also provided partial support for this work.
R EFERENCES [1] D. J. Weitzner, H. Abelson, T. Berners-Lee, J. Feigenbaum, J. Hendler, and G. J. Sussman, "Information Accountability," Communications of the ACM (CACM), vol.
51, no.
6, pp.
82-87, June 2008.
[2] M. Alles, A. Kogan, and M. Vasarhelyi, "Black Box Logging and Tertiary Monitoring of Continuous Assurance Systems," Information Systems Control Journal, vol.
1, 2003.
[3] R. T. Snodgrass, S. S. Yao, and C. Collberg, "Tamper Detection in Audit Logs," in Proceedings of the International Conference on Very Large Databases, September 2004, pp.
504-515.
[4] K. E. Pavlou and R. T. Snodgrass, "Forensic Analysis of Database Tampering," in Proceedings of the ACM SIGMOD International Conference on Management of Data, June 2006, pp.
109-120.
[5] ----, "Forensic Analysis of Database Tampering," ACM Transactions on Database Systems, vol.
33, no.
4, pp.
30:1- 30:47, November 2008.
[6] ----, "The Tiled Bitmap Forensic Analysis Algorithm," IEEE Transactions on Knowledge and Data Engineering, vol.
22, no.
4, pp.
590-601, April 2010.
[7] C. S. Jensen and C. E. Dyreson (eds), "A consensus glossary of temporal database concepts--February 1998 Version," in Temporal Databases: Research and Practice, O. Etzion, S. Jajodia, and S. Sripada, Eds.
Springer-Verlag, 1998, pp.
367- 405.
[8] Oracle Corp. (2008, Aug.) Workspace Manager Developer's Guide 11g Release 1 (11.1).
[Online].
Available: http://www.oracle.com/pls/db111/to pdf?
pathname=appdev.111/b28396.pdf [9] Teradata Corp. (2010, Oct.) Teradata Temporal.
[Online].
Available: http://www.teradata.com/database/ teradata-temporal/ [10] IBM Corp. (2010, Dec.) A Matter of Time: Temporal Data Management in DB2 for z/OS.
[Online].
Available: http://www14.software.ibm.com/webapp/iwm/ web/signup.do?lang=en US&source=sw-infomgt&S PKG= db2z-temporal-tables-wp [11] J. Bair, M. Bohlen, C. S. Jensen, and R. T. Snodgrass, "Notions of upward compatibility of temporal query languages," Business Informatics (Wirtschafts Informatik), vol.
39, no.
1, pp.
25-34, 1997.
[12] R. Hasan and M. Winslett, "Efficient Audit-based Compliance for Relational Data Retention," in Proceedings of the 6th ACM Symposium on Information, Computer and Communications Security, ser.
ASIACCS '11.
New York, NY, USA: ACM, 2011, pp.
238-248.
[13] H. Guo, Y. Li, A. Liu, and S. Jajodia, "A fragile watermarking scheme for detecting malicious modifications of database relations," Inf.
Sci., vol.
176, no.
10, pp.
1350-1378, 2006.
[14] Y. Li, H. Guo, and S. Jajodia, "Tamper Detection and Localization for Categorical Data Using Fragile Watermarks," in Proceedings of the 4th ACM Workshop on Digital Rights Management, 2004, pp.
73-82.
[15] C. S. Jensen, M. D. Soo, and R. T. Snodgrass, "Unification of Temporal Data Models," in International Conference on Data Engineering, April 1993, pp.
262-271.
[16] S. Mitra, W. W. Hsu, and M. Winslett, "Trustworthy Keyword Search for Regulatory-Compliant Record Retention," in Proceedings of the International Conference on Very Large Databases, 2006, pp.
1001-1012.
[17] Sarbanes-Oxley Act, U.S. Public Law No.
107-204, 116 Stat.
745., "The Public Company Accounting Reform and Investor Protection Act," 2002.
[18] R. Hasan and M. Winslett, "Trustworthy Vacuuming and Litigation Holds in Long-term High-integrity Records Retention," in Proceedings of the 13th International Conference on Extending Database Technology, 2010, pp.
621-632.
[19] D. C. Luckham, The Power of Events: An Introduction to Complex Event Processing in Distributed Enterprise Systems.
Boston, MA, USA: Addison-Wesley Longman Publishing Co., Inc., 2001.
Temporal Arithmetic Mixing Months and Days Feng Pan and Jerry R. Hobbs Information Sciences Institute (ISI), University of Southern California 4676 Admiralty Way, Marina del Rey, CA 90292 {pan, hobbs}@isi.edu Abstract In this paper, we present our work on creating a complete set of rules for temporal arithmetic mixing months and days based on the "history-dependent intuition".
Many examples are presented for demonstrating how the rules are used and how the computations satisfy various desired arithmetic properties, such as the subtraction, commutativity and associativity properties.
A notion of "days lost" (DL) is proposed to concisely keep track of the history of the temporal arithmetic computation and explain possible inconsistencies in terms of different desired properties.
1.
Introduction Temporal arithmetic involves the computation of adding durations to dates/times, subtracting durations from dates/times, and computing durations between date/time pairs.
Such arithmetic (sometimes is also referred as date arithmetic when only dates are of interest) is very useful in many different domains, such as artificial intelligence (Bettini et al., 2002), databases (Chandra and Segev, 1994), time series management systems (Dreyer et al., 1994), agents (Mallya et al., 2004), and Web services (Pan and Hobbs, 2004).
As long as we stay within the year-month system or the day-hour-minute-second system, temporal arithmetic is just arithmetic and requires only a few simple axioms or rules to encode.
When we mix months and days, problems arise.
For example, consider that January 31, 2006 plus 2 months equals March 31, 2006.
But if we add the months one at a time, we may get a different result: January 31, 2006 plus one month is February 28, 2006, but February 28, 2006 plus one month would seem to be March 28, 2006.
If we want to avoid results like this, we need, in some sense, to keep track of the history of the computation.
This motivating example is taken from the documentation for Java methods add() and roll() of the class Calendar1, where they explained the behavior of the methods using the above example and claimed that, "as most users will intuitively expect", the final date after adding the second month should be March 31, 2006, not March 28, 2006, and this is what you will get when you use the temporal arithmetic methods in Java.
(In what follows, we will refer to this kind of intuition as the "history-dependent intuition".)
However, different people may have different intuitions regarding how the computation should work.
Some people may believe adding one month to February 28 should always be March 28, no matter where February 28 was originally computed from.
(In what follows, we will refer to this kind of intuition as the "history-independent intuition".)
This intuition results in much simpler arithmetic rules, where when adding durations to dates, simply add each of their fields (temporal units) respectively, and handle overflow when needed.
In fact, this approach is what XML Schema is currently using in their algorithm for adding "durations" to "dateTimes"2 (Biron and Malhotra, 2004).
But they didn't consider at all the alternative (maybe even more popular) intuition (the "history-dependent intuition") we described in the above motivating example.
Biron and Malhotra (2004) also warn that there are cases where the properties of commutativity and associativity cannot hold using the algorithm in XML Schema, and give one example as follows: (2000-03-30 + P1D) + P1M = 2000-03-31 + P1M = 2000-04-30 (2000-03-30 + P1M) + P1D = 2000-04-30 + P1D = 2000-05-01 1  http://java.sun.com/j2se/1.4.2/docs/api/java/util/Calendar.html http://www.w3.org/TR/xmlschema-2/#adding-durations-todateTimes  2  This example shows that adding one month (P1M) and one day (P1D) to March 30, 2000 in different orders using their algorithm would give different destination dates.
Many real-world applications, on the other hand, use a fixed number of days for each month (e.g., car rental companies usually count 28 days as one month), but they also cannot avoid the problem raised when mixing months and days.
For example, Stonebraker (1990) pointed out that the yield calculation on financial bonds uses a calendar that has 30 days in every month for date arithmetic, but 365 days in the year for the actual yield calculation, which inevitably causes inconsistency.
To avoid this problem, Malhotra et.
al.
(2005) proposed, for XQuery and XPath, deriving two new (totally ordered) subtypes, "yearMonthDuration" (in the year-month system) and "dayTimeDuration" (in the day-hour-minute-second system), from the (partially ordered) datatype "duration".
But in this case, temporal arithmetic can only be carried out in the two separate systems, and months and days cannot be computed together.
Both the "history-independent intuition" and the approach with a fixed number of days for each month can be encoded straightforwardly with a few simple axioms or rules.
However, to our knowledge, there has been no serious effort for doing temporal reasoning based on the "history-dependent intuition", which may be the most popular intuition, but the most difficult to model.
In this paper, we present our work on creating a complete set of rules that can handle temporal arithmetic mixing months and days for the "historydependent intuition" (including the motivating example), proposing a notion (called "days lost" DL) to concisely keep track of the history of the computation and explain possible inconsistencies in terms of different desired arithmetic properties.
In Section 2 we describe these desired properties for temporal arithmetic computation.
The notion of "days lost" is introduced in Section 3, and in Section 4 we present the temporal arithmetic rules with examples.
Finally we discuss our future work in Section 5.
Since the problem arise only when months and days are mixed, in this paper we only consider dates with months and days, and all other fields of a date/time (e.g., year, hour) will be omitted in the examples and rules.
First, we introduce some notation and assumptions for later examples and rules.
Notation: * (m, d)DL denotes a date with a month "m", a day "d", and days lost "DL" (see Section 3 for the  * *  details on "days lost").
For example, (2, 28)3 means February 28th with 3 days lost.
DL is omitted when DL = 0.
[m, d] denotes a duration with "m" months and "d" days.
For example, [3, 2] means 3 months and 2 days.
#(m) denotes the number of days in a month "m".
For example #(3) = 31, since there are 31 days in March.
Assumptions: * The year is 2006, so that the number of days in February is 28.
* Dates and durations are in canonical forms, i.e., For dates: 1 <= m <= 12 1 <= d <= last day of the current month, #(m) For durations: 1 <= m < 12 1 <= d < last day of the destination month  2.
Desired Arithmetic  Properties  for  Temporal  Before creating the rules for temporal arithmetic computation satisfying the "history-dependent intuition", a list of desired properties which we hope can hold during the computation are presented first, and these properties were also used to guide the creation of the rules.
The desired properties are as follows, with the most desired ones at the top.
(1) Motivating example for the "history-dependent intuition": (1, 31) + [1, 0] + [1, 0] = (2, 28) + [1, 0] = (3, 31) (2) Addition-Simplicity property: (m1, d1) + [m2, d2] = (m1+m2, d1+d2), if d1+d2 <= #(m1+m2), m1+m2 <= 12 (3) Subtraction-Simplicity property: (m1, d1) - [m2, d2] = (m1-m2, d1-d2), if m1 > m2, d1 > d2, d1-d2 <= #(m1-m2) (4) Subtraction property: (m1, d1) + [m2, d2] = (m3, d3) <=> (m3, d3) - [m2, d2] = (m1, d1) <=> (m3, d3) - (m1, d1) = [m2, d2] (5) Commutativity property: (m1, d1) + [m2, d2] + [m3, d3] = (m1, d1) + [m3, d3] + [m2, d2] (6) Associativity property: {(m1, d1) + [m2, d2]} + [m3, d3] = (m1, d1) + {[m2, d2] + [m3, d3]} Property (1) is actually an instance of associativity, because {(1, 31) + [1, 0]} + [1, 0] = (2, 28) + [1, 0] = (3, 31)  (1, 31) + {[1, 0] + [1, 0]} = (1, 31) + [2, 0] = (3, 31)  3.
Meaning of "Days Lost (DL)" In order to keep track of the history of the temporal arithmetic computation for the "history-dependent intuition", we introduce a notion, called "days lost (DL)".
Its meaning is as follows: (1) If the day of the month is the end of the month (e.g., (2, 28)3), the days lost ("DL") means the number of days lost in the current month.
For example, adding 1 month to January 31st results in February 28th, and 3 days are "lost" in the computation.
Thus we use (2, 28)3 to "remember" that 3 days were lost in February.
(2) If the day of the month is not the end of the month (e.g., (3, 2)3), the days lost ("DL") is just a record of the number of days lost in the past.
For example, adding 1 month and 2 days to January 31st results in March 2nd, where 3 days were "lost" in February.
In fact, "days lost (DL)" not only is used to keep track of the history of the computation, but also plays a crucial role in explaining the inconsistency of the computation results with respect to different desired properties (e.g., commutativity and associativity).
For example, it can be used to explain the inconsistency described in the introduction regarding the temporal arithmetic algorithm in XML Schema (Biron and Malhotra, 2004): (2000-03-30 + P1D) + P1M = 2000-03-31 + P1M = 2000-04-30 (2000-03-30 + P1M) + P1D = 2000-04-30 + P1D = 2000-05-01 The different results are actually due to the "one day lost" in the first computation when adding 1 month (P1M) to 2000-03-31, since there are only 30 days (not 31 days) in April.
We will show in the next section how our rules can be used to deal with such inconsistency.
4.
Temporal Arithmetic Rules In this section, we describe a complete set of rules we created for temporal arithmetic satisfying the "history-dependent intuition", including adding durations to dates, subtracting durations from dates, and computing duration between two dates.
Each rule is presented with one representative example, and  additional examples are shown to demonstrate how to use multiple rules together in the computation.
All the rules can be straightforwardly translated into first-order logic (FOL) axioms, so that they can be incorporated into the OWL-Time (formerly DAMLTime) ontology (Hobbs and Pan, 2004) to give them access to the full ontology of time for temporal reasoning, including representation and reasoning about temporal relations, date-time information, durations, and temporal aggregates (Pan and Hobbs, 2005) such as recurrence events.
One FOL translation of a temporal arithmetic rule is shown at the end of the section.
4.1.
Adding Durations to Dates There are three sets of rules for adding durations to dates: add-decompose, add-month, and add-day.
In the following table, the rules are shown in the left column with index numbers (e.g., (1.1)), and one representative example is shown for each line of the rules in the right column for the demonstration and justification purpose.
Temporal Arithmetic Rule  Example  (1) Add-decompose: (m1,d1)DL+[m2,d2]  (2,28)2+[1,2]  (1.1) =(m1,d1)DL+[m2,0]+[0,d2]  = (2,28)2+[1,0]+[0,2]  (2) Add-month: // N = #(m1+m2) (m1, d1) DL + [m2, 0] = if (d1 = #(m1)) (2.1) (m1+m2, d1+DL), d1+DL<=N  (2,28)2+[2,0] = (4,30)  (2.2) (m1+m2,N)d1+DL-N, d1+DL>N  (3,31)+[1,0] = (4,30)1  else (i.e., d1 < #(m1)) (2.3) (m1+m2, d1) , d1 <= N d1-N  (2.4) (m1+m2, N)  , d1 > N  (3) Add-day:  (3, 2) + [1, 0] = (4, 2) (1,30) + [1,0] = (2,28)2  // N = #(m1)  (m1, d1) DL + [0, d2] = if (d1 = #(m1)) (3.1) (m1+1, d2)DL  (2,28)1 + [0,1] = (3,1)1  else (i.e., d1 < #(m1)) (3.2) (m1, d1+d2) DL, d1+d2 < N  (3,2)3+[0,20] = (3,22)3  (3.3) (m1, d1+d2), d1+d2 = N  (5,5) + [0,26] = (5,31)  (3.4) (m1+1, d1+d2-N), d1+d2 > N  (2,20) + [0,10] = (3,2)  For any given duration, before it adds to a date, add-decompose rule is applied first to separate the months and days by decomposing the duration into two sub-durations with only months and days respectively, and then add months first to the date using add-month rules, then the days using add-day rules.
Two more examples of adding durations to dates are also shown below.
The first one is from the motivating example, and the second one demonstrates the complete procedure (3 steps) for adding durations to dates, when durations have both months and days (the rule applied for each step of the computation is also shown with a corresponding rule index number): 3  Ex.1.
(1, 31) + [1, 0] = (2, 28) // add-month (2.2) (2, 28)3 + [1, 0] = (3, 31) // add-month (2.1) Ex.2.
(1, 29) + [1, 2] = (1, 29) + [1, 0] + [0, 2] // add-decompose (1.1) // add-month (2.4) = (2, 28)1 + [0, 2] = (3, 2)1 // add-day (3.1)  4.2.
Subtracting Durations from Dates There are three sets of rules for subtracting durations from dates: sub-decompose, sub-day, and sub-month: Temporal Arithmetic Rule  Example  (1) Sub-decompose (m1, d1)DL - [m2, d2]  (2,28)2-[1,2]  (1.1) = (m1,d1) DL - [0,d2] - [m2,0]  = (2,28)2-[0,2]-[1,0]  (m1, d1) DL - [0, d2] (2.2)  (m1-1,d1+#(m1-1)-d2)  ,  3  (3,2) -[0,10] = (2,20)  3  (m1, d1)  DL  Example  (1) Sub-between-dates (m2, d2) DL2 - (m1, d1) DL1  = if (d1 = #(m1)) (3.1) (m1-m2, d1+DL), d1+DL <= N d1+DL-N  ,d1+DL>N  (2,28)2 - [1,0] = (1,30) 1  3  2  2  (4,30) - [2,0] = (2,28)  else (i.e., d1 < #(m1)) (3.3) (m1-m2, N)  Ex.4.
(3, 2)1 - [1, 2] = (3, 2)1 - [0, 2] - [1, 0] // sub-decompose (1.1) = (2, 28)1 - [1, 0] // sub-day (2.2) = (1, 29) // sub-month (3.1)  Temporal Arithmetic Rule  // N = #(m1-m2)  - [m2, 0]  (3.2) (m1-m2,N)  (1,31) + [1,0] = (2,28)3 <=> (2,28)3 - [1,0] = (1,31) (2,28)3 + [1,0] = (3,31) <=> (3,31) - [1,0] = (2,28)3 (1, 31) + [1, 0] + [1, 0] = (2, 28)3 + [1, 0] = (3, 31)  There is only one set of rules for computing duration between two dates:  d 1 <= d2 (3) Sub-month  Ex.1.
and Ex.3.
show that the temporal arithmetic rules work for the motivating example for the "historydependent intuition" (desired property 1), and the first part of the "subtraction property" (desired property 2) also holds for the motivating example, because  4.3.
Computing Duration between Two Dates  (4,30)1-[0,15]=(4,15)1 DL  Ex.3.
(3, 31) - [1, 0] = (2, 28)3 // sub-month (3.2) (2, 28)3 - [1, 0] = (1, 31) // sub-month (3.1)  Ex.2.
and Ex.4.
also show that the first part of the "subtraction property" (desired property 2) holds for this example whose duration has both months and days, because (1,29) + [1,2] = (3,2) 1 <=> (3,2) 1 - [1,2] = (1,29)  (2) Sub-day (2.1) = (m1, d1-d2) DL, d1 > d2  For any given duration, before it subtracts from a date, sub-decompose rule is applied first to separate the months and days by decomposing the duration into two sub-durations with only days and months respectively, and then subtract days first from the date using subday rules, then the months using sub-month rules.
Two more examples of subtracting durations from dates are also shown below.
The first one is from the motivating example, and the second one demonstrates the complete procedure (3 steps) for subtracting durations from dates, when durations have both months and days:  d1-N  , d1 > N  (3.4) (m1-m2, d1), d1 <= N  = if (d1 = #(m1)) (1.1) [m2-m1, d2-(d1+DL1)],  (5,31)-(2,28)2=[3,1]  d2 >= d1+DL1 (3,30) - [1,0] = (2,28) (5,16) - [1,0] = (4,16)  (1.2) [m2-m1-1,(d2+DL2)+#(m2-1) -(d1+DL1)], d2<d1+DL1, d2<#(m2)  (3,2)3-(1,31)=[1,2]  (1.3) [m2-m1, 0],d2<d1+DL1, d2=#(m2)  (4,30)1-(2,28)3  else (i.e., d1 < #(m1))  = [2,0]  (1.4) [m2-m1, d2-d1], d2 >= d1  (3,20)-(2,10) =[1, 10]  (1.5) [m2-m1-1, d2],  (3,2) - (1,28) = [1,2]  d2 < d1, d2 < #(m2), #(m2-1) <= d1 (1.6) [m2-m1-1, d2+#(m2-1)-d1],  (3,2)-(1,20) = [1,10]  d2 < d1, d2 < #(m2), #(m2-1) > d1 (1.7) [m2-m1, 0], d2 < d1, d2 = #(m2)  (2,28)1-(1,29)=[1,0]  Two more examples of computing duration between two dates are also shown below.
The first one is from the motivating example, and the second one demonstrates the procedure for computing durations between two dates, when durations have both months and days: Ex.5.
(3,31) - (2,28)3 = [1,0] // sub-between-dates (1.1) (2,28)3 - (1,31) = [1,0] // sub-between-dates (1.3) (3,31) - (1,31) = [2,0] // sub-between-dates (1.1) Ex.1., Ex.3., and Ex.5.
show the complete "subtraction property" (desired property 2) holds for the motivating example, because (1, 31) + [1, 0] = (2, 28)3 <=> (2, 28)3 - [1, 0] = (1, 31) <=> (2, 28)3 - (1, 31) = [1, 0] (2, 28)3 + [1, 0] = (3, 31) <=> (3, 31) - [1, 0] = (2, 28)3 <=> (3, 31) - (2, 28)3 = [1, 0] They also show the "associativity property" holds for the motivating example, because {(1, 31) + [1, 0]} + [1, 0] = (3, 31) <=> (1, 31) + {[1, 0] + [1, 0]} = (1, 31) + [2, 0] = (3, 31) Ex.6.
(3,2)1 - (1,29) = [1,2] // sub-between-dates (1.5) Ex.2., Ex.4., and Ex.6.
also show that the complete "subtraction property" (desired property 2) holds for this example, since (1, 29) + [1, 2] = (3, 2) 1 <=> (3, 2) 1 - [1, 2] = (1, 29) <=> (3, 2)1 - (1, 29) = [1, 2]  Ex.7.
(1, 31) + [1, 2] + [2, 0] = (1, 31) + [1, 0] + [0, 2] + [2, 0] // add-decompose (1.1) = (2, 28)3 + [0, 2] + [2, 0] // add-month (2.2) // add-day (3.1) = (3, 2)3 + [2, 0] = (5, 2) // add-month (3.4) (1, 31) + [2, 0] + [1, 2] = (3, 31) + [1, 2] = (3, 31) + [1, 0] + [0, 2] = (4, 30)1 + [0, 2] = (5, 2)1  // add-month (2.1) // add-decompose (1.1) // add-month (2.2) // add-day (3.1)  Ex.7.
shows that the "commutativity property" holds for this more complicated example, if the supplemental information (days lost "DL") is ignored.
Ex.8.
In the introduction, we showed the following example from XML Schema (Biron and Malhotra, 2004) that demonstrates the case where the properties of commutativity and associativity cannot hold using their temporal arithmetic algorithm: (2000-03-30 + P1D) + P1M = 2000-03-31 + P1M = 2000-04-30 (2000-03-30 + P1M) + P1D = 2000-04-30 + P1D = 2000-05-01 Can our rules handle computation is as follows.
this  problem?
The  (3, 30) + [0, 1] + [1, 0] = (3, 31) + [1, 0] // add-day (3.3) = (4, 30)1 // add-month (2.2) (3, 30) + [1, 0] + [0, 1] = (4, 30) + [0, 1] // add-month (2.3) = (5, 1) // add-day (3.1) The rules not only compute the results exactly based on the "history-dependent intuition", but also include supplemental information (days lost "DL") for explaining the reason why the commutativity property doesn't hold: there was "one day lost" in the first computation when one month is added to March 31.
In fact, the commutativity property would hold, if we specify that (4, 30)1 "equals" to (5, 1).
More generally, the commutativity and associativity properties hold with our temporal arithmetic rules, if we define the month-day equality "more softly" and take into account the effect of "days lost" (assume overflow is properly handled):  (m1, d1) DL1 = (m2, d2) DL2, if m1 = m2 AND (d1 = d2 OR d1 + DL1 = d2 + DL2) We have only given examples in this paper, but we believe and are in the process of demonstrating that the rules satisfy the desired properties in general, given this definition of month-day equality.
4.4.
Translating Rules to FOL Axioms in OWL-Time All the above temporal arithmetic rules can be straightforwardly translated into FOL axioms in OWLTime to give them access to the full ontology of time for temporal reasoning.
For example, the rule "addmonth (2.1)" DL  (m1, d1) + [m2, 0] = (m1+m2, d1+DL), if d1 = #(m1), d1+DL <= #(m1+m2) can be translated as (see (Hobbs and Pan, 2004) for the definitions of the predicates): ([?]
T, t1, t3, m1, m2, m3, d1, d3, DL, DL3, n1, n3) dateOf(t1, m1, d1, DL) [?]
durationOf(T, m2, 0) [?]
begins(t1, T) [?]
ends(t3, T) [?]
dateOf(t3, m3, d3, DL3) [?]
Hath(n1, *Day*, m1) [?]
Hath(n3, *Day*, m3) [?]
d1 = n1 [?]
d1+DL <= n3 [?]
m3 = m1+m2 [?]
d3 = d1+DL [?]
DL3 = 0  5.
Conclusions In this paper, we have presented our work on creating a complete set of rules for temporal arithmetic mixing months and days, based on the "historydependent intuition".
A notion of "days lost" (DL) was proposed for both keeping track of the history of the computation and explaining possible inconsistencies in terms of different desired arithmetic properties, such as the subtraction, commutativity and associativity properties.
Acknowledgement This work was supported by the Advanced Research and Development Activity (ARDA), now the Disruptive Technology Office (DTO), under DOD/DOI/ARDA Contract No.
NBCHC040027.
Any opinions, findings and conclusions or recommendations expressed in this material are those  of the authors and do not necessarily reflect the views of the ARDA or the Department of Interior.
The authors have profited from discussions with Jose Luis Ambite, Hans Chalupsky, Kevin Knight, Rutu Mulkar, Daniel O'Leary, Paul Rosenbloom, and Tom Russ.
Reference Bettini, C., Wang, S. X. and Jajodia, S. 2002.
Solving multigranularity temporal constraint networks, Artificial Intelligence, vol.
140, pp.
107-152.
Biron, P. V. and Malhotra, A.
2004.
XML Schema Part 2: Datatypes Second Edition.
W3C Recommendation.
http://www.w3.org/TR/xmlschema-2/ Chandra, R., Segev, A., and Stonebraker, M. 1994.
Implementing Calendars and Temporal Rules in Next Generation Databases".
In Proceedings of the Tenth International Conference on Data Engineering, pages 264- 273, Houston, Texas.
Dreyer, W, Dittrich, A. Koa, and Schmidt, D. 1994.
Research Perspectives for Time Series Management Systems.
SIGMOD RECORD, Vol.
23, No.
1, pp.
10-15.
Hobbs, J. R. and Pan, F. 2004.
An Ontology of Time for the Semantic Web.
ACM Transactions on Asian Language Processing (TALIP): Special issue on Temporal Information Processing, Vol.
3, No.
1, pp.
66-85.
Malhotra, A, Melton, J., and Walsh, N. 2005.
XQuery 1.0 and XPath 2.0 Functions and Operators.
W3C Candidate Recommendation.
http://www.w3.org/TR/xpath-functions/ Mallya, A. U., Yolum, P., and Singh, M. P. 2004.
Resolving commitments among autonomous agents.
In F. Dignum, editor, Advances in Agent Communication, In Proceedings of the International Workshop on Agent Communication Languages, Germany.
Pan, F. and Hobbs, J. R. 2004.
Time in OWL-S.
In Proceedings of AAAI Spring Symposium on Semantic Web Services, Stanford University, California, pp.
29-36, AAAI Press.
Pan, F. and Hobbs, J. R., 2005.
Temporal Aggregates in OWL-Time.
In Proceedings of the 18th International Florida Artificial Intelligence Research Society Conference (FLAIRS), Clearwater Beach, Florida, pp.
560-565, AAAI Press.
Stonebraker, M. R. Extensibility.
1990.
In M.R.
Stonebraker, editor, Readings in Database Systems.
Morgan Kaufman.
From Language to Time: A Temporal Expression Anchorer Benjamin Han, Donna Gates and Lori Levin Language Technologies Institute Carnegie Mellon University 5000 Forbes Ave, Pittsburgh PA 15213 {benhdj|dmg|lsl}@cs.cmu.edu  Abstract Understanding temporal expressions in natural language is a key step towards incorporating temporal information in many applications.
In this paper we describe a system capable of anchoring such expressions in English: system TEA features a constraint-based calendar model and a compact representational language to capture the intensional meaning of temporal expressions.
We also report favorable results from experiments conducted on several email datasets.
1  Introduction  A key ingredient in incorporating temporal information into natural language applications is the normalization, or anchoring of temporal expressions, i.e., expressions using temporal terms such as 2005, evening, tomorrow, etc.
Some of these expressions can be classified into the following categories: * Explicit expressions are the ones that can be immediately anchored; e.g., June 2005, 1998 Summer, etc.
* Deictic expressions are the ones that form a specific relation with a speech time; e.g., tomorrow, last year, two weeks from today.
* Relative expressions are the ones that form a specific relation with a temporal focus, i.e., the implicit time central to a discourse; e.g., from 5 to 7, on Friday, etc.
* Duration expressions are the ones that describe certain length in time; e.g., for about an hour, less than 20 minutes.
Accomplishing this task requires not only the knowledge about how various temporal primitives are related (e.g., February in a leap year has 29 days), but also how they  interact with one another given a description manifested by an expression.
In this paper we describe a system capable of automatically anchoring the kinds of expressions listed above1 .
The input to the system TEA (Temporal Expression Anchorer) are English sentences with temporal expressions already identified, and the output is the normalization for each temporal expression.
TEA has the following characteristics: (1) it incorporates a constraint-based calendar model to reason with under-specified expressions; (2) it can be extended to deal with new temporal primitives; (3) it captures the intensional meaning of temporal expressions using a compact representational language TCNL (Time Calculus for Natural Language).
A system overview of TEA is given in Fig.
1.
The Finite-state Parser first takes an input sentence and translates its temporal expressions into TCNL formulae.
The temporal references inside the formulae such as focus are then instantiated and any ambiguity is resolved in the Discourse Module.
Finally the Evaluator Module takes the set of processed TCNL formulae and evaluate them to give the normalized times.
At all stages the Calendar Model provides necessary services such as determining granularity of an expression, comparing two expressions chronologically, and solving constraint satisfaction problems induced by TCNL formulae, etc.
Fig.
22 shows an example session using TimeShell3 , an interactive front-end of TEA, to illustrate this process: the Finite-state Parser first transduces the expression "yesterday at 3pm" into its TCNL formula {15hour , 0min , now-|1day |}, the Discourse Module then rewrites the temporal reference now with {2006year , feb, 7day } (speech time), and finally the Evaluator solves the formula to its answer 20060206T1500??.
1 Notable omissions of the expression types include recurrence (e.g., "3pm every Tuesday and Thursday") and rate expressions (e.g., "twice a week").
These are left to the future work.
2 The output are shown in two formats: TCNL and an ISO8601-like form.
In the latter a date is written in the form of YYMMDD and a time point is shown in the form of YYMMDDTHHMMSS; omitted information is indicated by '?'.
3 http://telltime.org  Year component  sentences with temporal expressions  Week component  temporal unit  Year  TEA Finite-state Parser Month  Constraint Solver  * Day  *  TCNL formulae  Calendar Model  Week  *  Day-of-week  Time-of-week *  * Time-of-day  Discourse Module  Hour *  instantiated and disambiguated TCNL formulae  * Minute  ?
* Second  Evaluator Module  X component  sentences with anchored temporal expressions (using ISO8601-like format)  unit constraints alignment constraints is-measured-by relation (* marks a representative) is-periodic-in relation  Figure 3.
A partial model of the Gregorian calendar  Figure 1.
System overview of TEA  2 _ = {2006_year, feb, 3_day} > yesterday at 3pm TCNL: {15_hour, 0_min, now - |1_day|} = {2006_year, feb, 6_day, 15_hour, 0_min} ICal+: 20060206T1500??
_ = {2006_year, feb, 6_day, 15_hour, 0_min} > 2 days after Thanksgiving a year ago TCNL: {{|4_{thu}| @ {_ - |1_year|, nov}} + |2_day|} = {2005_year, nov, 26_day} ICal+: 20051126 _ = {2005_year, nov, 26_day} > on Wednesday TCNL: +{wed} = {104611_week, wed} ICal+: 20051130  Figure 2.
Example session in TimeShell  This result becomes the new focus that the Discourse Module then uses to replace the reference ' ' in the next formula for the expression "2 days after Thanksgiving a year ago".
This second formula also demonstrates how TCNL is able to represent the intensional meaning of Thanksgiving, which is the 4th Thursday in November.
Finally the last expression "on Wednesday" is anchored to a specific day thanks to the prefix '+' in the extremely under-specified formula +{wed}.
This instructs the Evaluator to find the first Wednesday possible on or after the focus.
The rest of the paper details TEA in the reversed order outlined above.
Sec.
2 first introduces the constraint-based calendar model and its core algorithms.
Sec.
3 then gives a concise description of TCNL.
These two sections present an updated account to the work reported in [5] and [6].
The Discourse Module and the finite-state parsing are described in Sec.
4 and Sec.
5.
Experimental result on email corpora is then reported in Sec.
6.
Finally we conclude the paper and outline the future work in Sec.
7.
Calendar Modeling  A calendar in TEA serves the role of a time ontology; i.e., to encapsulate the relations among a set of temporal primitives, or temporal units, so that they can be of use by our representational language TCNL.
In literature there has been many works similar in purpose, such as the DAML Time Ontology [7], Calendar Logic [11], and an algebraic representation of granularity systems proposed in [3].
Our modeling of calendars is distinct in that it views a calendar as a constraint system, namely we treat a temporal unit as a variable with a discrete, finite and fully ordered domain (e.g., the domain of unit month consists of jan < .. < dec), and a temporal expression in natural language in effect assigns values to some of these units.
This modeling has the following advantages: (1) granularity conversion is abstracted and localized in constraints, which makes the model easy to extend; (2) partial assignments to units can be refined by solving the induced Constraint Satisfaction Problems (CSP) [12], thus making the model a perfect fit for reasoning about under-specified expressions (e.g., knowing the date February 29 implies it is in a leap year).
Fig.
3 gives a pictorial view of a partial model for the Gregorian calendar.
The basic building block in our calendar model is a calendar component, which is essentially a souped-up constraint network.
Multiple components are then aligned using constraints (more on this later).
Within a calendar component, in addition to the constraints specifying compatible values among the constrained units (e.g., February in non-leap years cannot have 29 days), the units are also partially ordered using a designated measurement relation (solid arrows in Fig.
3); e.g., month is-measured-by day, or month > day.
A reciprocal relation periodicity is also defined (dashed arrows in Fig.
3): if u1 is-measured-by u2 , u2 is-periodic-in u1 if iterating through the possible values  of u2 does not advance the value of u1 ; e.g., dow (day-ofweek: Monday..Sunday) is-periodic-in week but not periodic in tow (time-of-week: weekdays and weekends), because adding one day to Friday will advance the corresponding tow from weekdays to weekend.
The periodicity relation then allows us to define the concept of anchor paths: a path < un , .
.
.
, u1 , u > is an anchor path of unit u if ui is-periodic-in ui+1 for i = 1...(n - 1), and un is a maximal unit under the measurement relation.
We then say that a set of assignments is anchored at unit u if every unit on the anchor path of u has an instantiation (singleton assignment).
The core operations in our calendar model are constraint propagation and distribution: the former removes incompatible values from the domains of the units (represented by intervals) using the standard AC-3 algorithm4 [9], and the latter iterates through every consistent set of instantiations using a chronological backtracking algorithm (e.g., given "Friday the 13th" find all possible anchored dates).
In TEA the distribution algorithm is tailored to instantiate units in a given ordering (usually specified by an anchor path), and it can also start iterating from a given set of instantiations (e.g., iterating through all possible Friday the 13th starting from January 13, 2006).
Fig.
4 gives a sketch of the distribution algorithm.
As mentioned earlier a temporal expression essentially gives partial assignments to temporal units.
This set of assigned units acts as a view into the underlying calendar model; e.g., the view introduced by expression "February 29" is {month, day}.
We can then define granularity of assignments as the set of minimal units of their view under the measurement relation; e.g., the granularity of "February 29" is {day}.
Granularity conversion is therefore implemented as a graph-theoretic operation to find a new view with the desired set of minimal units; e.g., demoting the granularity of "February 29" to {min} changes its view to {month, day, hour, min}, while promoting it to {year} changes the view to {year}.
Building on the concepts of anchor paths and granularity, we can then determine the chronological ordering of two sets of assignments.
We do so by first comparing two sets of assignments, say a1 and a2 , on the same anchor path: we say a1 is earlier than a2 on an anchor path < un , .
.
.
, u1 , u > iff there exists i <= n such that a1 [uj ] < a2 [uj ] for all j = n .
.
.
i (lexicographic comparison).
We then say a1 is earlier than a2 iff there exists a unit u in the union of both granularities such that a1 is earlier than a2 on the anchor path of u.
E.g., "7am on February 29, 2004" (granularity is {hour}) is earlier than "afternoon on February 29, 2004" (granularity is {tod}, i.e., time-ofday) since the former is earlier than the latter on the anchor path of hour.
4 With  minimum remaining values heuristics.
distribute(a, ordering, a0 ): Input: assignments a and starting instantiations a0 ordering is a list of units Output: the next consistent instantiation of all units i = 0; rest = list of units that are not in ordering do: u = ordering[i]; backtrack = False if a0 [u] exists: if domain of u has value a0 [u]: v = a0 [u] else: backtrack = True remove a0 [u] else: if domain of u has any value: v = the next available value else: backtrack = True if backtrack == False: a[u] = v; remove v from domain of u propagate(a) if consistent: if i == (length of p) - 1: if rest is not empty and there exists no a0 from distribute(a, rest, a0 ): backtrack = True else: yield a else: i = i + 1 else: backtrack = True if backtrack == True: if i > 0: revert a[u] and domain of u to the state before the last assignment i=i-1 else: return  Figure 4.
Sketch of the distribution algorithm Another service provided by our calendar model is the addition operation: adding an integer to an instantiated unit; e.g., adding 1 day to unit day in the assignments "February 29" should result in assignments equivalent to "March 1".
The algorithm (shown in Fig.
5)5 essentially sets up the assignments properly and call the distribution algorithm to accomplish the feat.
For certain units, however, a faster operation is possible.
A call add(a, u, n) can be broken down into add(a, u, l) and add(a, u0 , m) where n = m * k + l and l < k, if unit u is periodic in u0 and every value of u0 is compatible with the same set of values of u (the size of the set is k).
Finally the modularity of our calendar model is achieved by allowing multiple calendar components to be related via 5 This  can be easily generalized to the n < 0 case.
add(a, u, n): Input: assignments a, unit u and integer n > 0 Output: True iff successful.
p = anchor path of u; u0 = p[0] a0 is an empty dictionary for v in p except u0 : a0 [v] = min(a[v]) for every unit v except u0 : a[v] = full domain of v do: if there is a next set of assignments from distribute(a, p, a0 ): n=n-1 if n == 0: return True else: return False  Figure 5.
Sketch of the addition algorithm alignment constraints.
The purpose of such constraints is to establish an order-preserving bijection mapping between the instantiations of the two aligned components.
The pair of units that are aligned between two components are called the portal units of the alignment, and the scope of an alignment is the union of the anchor paths of the portal units.
For example, in Fig.
3 the portal units of the alignment between the year component and the week component are day and dow, and the scope of the alignment constraint is {year, month, day, week, dow}.
Once the non-binary alignment constraint is specified, we can then translate an instantiation of one component to that of the other using the core algorithms described above; e.g., "February 29, 2004" is "Sunday of the 104519th week".
The measurement and periodicity relations can also be extended across aligned calendar components so that operations such as granularity conversion and addition would work properly.
3  Time Calculus for Natural Language (TCNL)  Building on top of the constraint-based calendar model is TCNL, a compact formalism designed to capture the meaning of temporal expressions in natural language.
Compared to many other alternatives in literature such as TOP [2], Timex2 [4] and TimeML/Timex3 [13], TCNL has the following characteristics: (1) it is calendar-agnostic (or ontology-agnostic); (2) it captures the intensional meaning of an expression (e.g., "yesterday" is not represented as a fixed date like 2006-02-02 but as a formula {now-|1day |}; more on this point later); (3) it exposes contextual dependency by using temporal references such as focus; and (4) its type system and operators makes granularity conversion and re-interpretation a transparent process.
There are three types of temporal entities in a TCNL representation: coordinates, quantities and enumerations.
A coordinate represents a set of assignments to temporal units (Sec.
2); e.g., "Friday the 13th" is represented as {fri,13day }.
Semantically a coordinate represents a time point at a certain granularity even when it is under-specified and can be anchored at multiple possible positions on a time line; e.g, the formula above represents a single Friday on the 13th (of some month).
An enumeration, on the other hand, represents a set of time points (sets of assignments), including but not limited to intervals; e.g., [{wed},{fri}] represents "Wednesday and Friday" and [{wed}:{fri}] denotes "Wednesday to Friday"6 .
Finally a quantity represents a certain number of temporal units (e.g., |2day | for "two days") or coordinates (e.g., |2{fri,13day } | for "two Fridays the 13th").
The semantics of a quantity is quite different from that of an interval: the latter must have a starting and an ending point (although they can be under-specified) and must denote a contiguous range on a time line, while a quantity just means a certain amount of "things", and they do not need to be adjacent to one another on a time line (e.g., no two Friday the 13th are adjacent).
Following Sec.
2 the granularity of an entity is then defined as the set of minimal units among those appearing in the representation.
More complex temporal entities can be constructed using operators.
All of the TCNL operators impose type and granularity requirements on their operands; an example is the fuzzy-shifting operators + and -.
In a formula {c+q}, the operand c must be a coordinate while q must be a quantity.
In addition, the granularity of c must be brought to match that of q.
Thus the formula {now+|1day |} ("tomorrow") is evaluated to February 3, 2006 (granularity is {day}) even when the temporal reference now is 10:03pm on February 2, 2006 (granularity is {min}).
Another example is the ordinal operator @, which stipulates that the left operand must be a quantity and the right operand must be an enumeration, and that the granularity of the latter must be brought to match that of the former.
Evaluating a formula such as {|2{sun} |@{may, 2005year }} ("the 2nd Sunday in May 2005"7 ) thus first requires a type coercion from {may,2005year } into the correct enumeration [{sun,104580week }:{sat,104585week }].
The operator @ then selects the 2nd possible coordinate that is a Sunday using the find the nth algorithm shown in Fig.
6.
A summary of all TCNL operators is given in Table 1.
Temporal entities can also be related with one another using relations.
A top-level relational term "re0 " in a host entity e specifies that (e, e0 ) is in relation r. Based on different type requirements TCNL provides five sets of rela6 An interval represented this way always denotes the shortest possible interval; e.g., the example does not denote a time span of more than 7 days.
7 Mother's day; TEA maintains a database of US holidays using formulae like this.
operator + and -  Type requirement CxQ-C  Granularity requirement g(LHS) - g(RHS)  ++ and --  CxQ-C  @  QxE-C  g(LHS) - min(g(LHS)[?
]g(RHS)) g(RHS) - g(LHS)  &  CxC-C CxE-E ExC-E ExE-E  g(LHS) - min(g(LHS)[?
]g(RHS))  Semantics fuzzy forward/backward shifting exact forward/backward shifting ordinal distribution  Example {now+|1day |} ("tomorrow'') {now++|2hour |} ("2 hours from now") {|2{sun} |@{may}} ("the 2nd Sunday in May") {now &{now+|1year |}} ("this time next year") [{15hour }&[{wed}:{fri}]] ("3pm from Wednesday to Friday")  Table 1.
Summary of operators in TCNL; LHS/RHS is the left/right operand, g(e) returns the granularity of e and min(s) returns the set of minimal units among s.  find the nth(n, c, start, end):  Relations  Input: integer n > 0, coordinate c, start and end Output: a coordinate if successful, otherwise None.
<, <=, >=, >  Type requirement QxQ  iter path = union of anchor paths of units in granularity of start sort iter path using the measurement relation  <, <=, >=, >  CxC  b, s, d, de, f, di  CxE  b, s, f, bi  ExC  b, m, o, s, d, f, =, fi, di, si, oi, mi, bi  ExE  do: if there is a next set of assignments a from distribute(c, iter path, start): if a is earlier or equal to end: n=n-1 if n == 0: return a else: return None  Figure 6.
Sketch of the find the nth algorithm  tions (Table 2).
Some examples are {wed, < now} for "a past Wednesday", {now, de {now+|0day |}} for "the rest of today" and [s now] for "from now on"8 .
TCNL also provides two temporal references so representations of temporal expressions can use them to expose contextual dependency.
We have seen reference 'now' used in various examples above: it denotes the speech time and usually it is kept constant during a discourse.
The other reference available is the temporal focus, symbolized by ' ' (underscore).
It is usually moved around in a discourse depending on which temporal location the discourse is focusing on9 .
A simple example is shown below: After the Challenger accident in '86 ({1986year }), shuttle missions were suspended in the next 2 years ([ :{ +|2year |}]).
8 This forms an interval starting from now to a pre-defined maximal coordinate.
9 Another contrast between the two different references is that dectic expressions such as "tomorrow" use speech time ({now+|1day |}) while relative expressions such as "the next day" use focus ({ +|1day |}).
Semantics shorter-than, shorter-than or equal-to, longer-than or equalto, and longer-than before, before or equal-to, after or equal-to, and after LHS is before/starting/during/duringequal/finishing/after RHS; de is defined as (s or d or f).
LHS is a maximal interval that is before/starting at/finishing at/after RHS.
See [1].
Table 2.
Summary of relations in TCNL; LHS/RHS is the left/right operand.
Evaluating the second formula requires instantiating its focus with a previously mentioned time, in this case it is the year 1986.
Managing focus movement (or focus tracking) is then relegated to the Discourse Module (to be described in Sec.
4).
It is worthwhile contrasting the use of a temporal focus in TCNL with similar devices adopted in other formalisms.
For example, in TimeML/Timex3 the attribute anchorTimeID is used in a TIMEX3 tag to "introduce the ID of the time expression to which the TIMEX3 markable is temporally anchored" [13].
An example for expression "two weeks from next Tuesday" is shown below: <TIMEX3 tid="t1" type="TIME" value="2002-08-06" temporalFunction="true" anchorTimeID="t0"> two weeks from next Tuesday</TIMEX3>  The date referred to by t0 (which was introduced earlier in the discourse) is then used to resolve the expression into the value 2002-08-06.
By contrast the same expression is represented in TCNL as {{ +|1{tue} |}++|2week |}.
Using  a placeholder like is more akin to the idea of lazy evaluation: it allows the same meaning representation to be resolved into different denotations, thus enhancing the modularity of our approach10 .
Another relevant observation is that much of the inner arithmetic evident in the TCNL formula is opaque in the Timex3 representation.
It is not always obvious how one can factor out the effect of focus in a TCNL formula, however.
Consider the following example: I am free next week ({now+|1week |}).
How about Friday ({fri})?
We can evaluate the under-specified expression "Friday" here by finding its nearest possible instantiations on or after the focus ("the next week"); i.e., we can rewrite {fri} into {|1{fri} |@{>= }} and evaluate the new formula.
This step is only necessary, however, if the formula cannot be anchored in its original form (e.g., if the second expression is "February 3, 2006", then applying the same rewriting procedure would give us no consistent instantiation).
This dilemma motivates the introduction of two coordinate prefixes: prefix '+'/'-' leading a coordinate indicates that the formula should be rewritten to find the nearest possible instantiation in the future/past of the focus, if necessary.
Thus in the example above "Friday" should be represented as +{fri} instead.
Deciding whether to add a prefix or what prefix to add is again a responsibility of a different module.
A second pair of prefixes is also provided to deal with the effect brought by tense/aspect.
The "Friday" in "the company announced/will announce on Friday'' can denote a Friday either before or after now.
Instead of hard-coding the relation existing between a coordinate with now (e.g., {fri, < now} for the past tense), we use prefix 'f' and 'p' to mark this relation.
Similar to the +/- prefix the insertion of the relational term happens only when necessary.
Finally TCNL provides a handful of functions to represent the meaning of expressions such as "late 2006" and "year-end".
Examples are early(.
), mid(.)
and late(.)
(C - C).
4  Anchoring in Discourse  Two complications arise before the evaluation of a TCNL formula can commence, and they are handled in the Discourse Module in TEA (Fig.
1).
The first is one of ambiguity - an example is ambiguous hour expressions.
In TEA the Finite-state Parser produces multiple formulae for expressions like "at 3" - in this case they are +{3hour } and +{15hour }11 .
To resolve this ambiguity we simply pick the 10 We should also emphasize that one of the design goals for TCNL is to make it easier for machines to generate these representations.
11 Recall from Sec.
3 the prefix + indicates that the formula should be anchored in the future of focus.
one that evaluates to a point closer to the focus; e.g., in the expression above if the focus is at 1 pm, +{15hour } will be chosen (evaluated to 3 pm on the same day) instead of the other (evaluated to 3 am of the next day).
The second complication is the instantiation of temporal references appearing in a formula (both speech time 'now' and focus ' ').
While instantiating now is straightforward (e.g., experiments reported in Sec.
6 use email time stamp as the speech time), focus instantiation is more challenging.
The current implementation of TEA uses a slightly constrained recency-based method for "tracking" focus: we simply pick the most recent formula prior to the one being evaluated to instantiate the focus, except when that formula comes from a noun-modifying temporal expression.
The exception is motivated by examples such as this: We received a copy of 2005 report and will send you our analysis by Sunday ({<= +f{sun}}).
(now = {feb,3day ,2006year }) Clearly {<= +f{sun}} is not a Sunday in 2005.
The conjecture is that a noun-modifying temporal expression usually serves as a temporal co-reference instead of introducing a new temporal entity into the discourse, and this reference has a more confined effect in anchoring the subsequent expressions.
We should emphasize that much of the current implementation of the Discourse Module is far from perfect (evident in the results shown in Sec.
6) and requires more work in the future.
5  Parsing Temporal Expressions  The Finite-state Parser in TEA (Fig.
1) first identifies all verb chunks in an input sentence and associates each temporal expression with its nearest verb chunk.
Each expression with the tense/aspect information of its associated verb chunk is then used to build a TCNL formula.
The formula-building process essentially breaks down an expression and constructs the representation bottom-up.
The compositional nature of TCNL makes this a relatively painless process: for example, given an expression "Friday last week", the TCNL formulae for "Friday" ({fri}) and "last week" ({now-|1week |}) are first built and then combined to give the final representation ({fri,{now-|1week |}}).
Note that we are not required to produce a single "normalized" representation for every equivalent expression; e.g., the formula {now-|1{fri} |} parsed from "last Friday" would be evaluated to the same date.
This makes grammar development a much easier task12 .
12 The grammar used in the experiments reported in Sec.
6 has 100 rules, including many rules for major US holidays.
Another interesting note is that the interpretation of an expression can be affected by the granularities of its subexpressions.
Take the following pair of expressions for example: "Tuesday before Christmas" = {tue, < {|25day |@{dec}}} "Tuesday before 6pm" = {< {tue,18hour }, de {tue}} Both of the expressions share the same "X before Y " pattern, but their interpretations are different (see Table 2 for relation de).
The key to discriminate the two is to compare the granularities of X and Y : if Y if at a higher granularity (Sec.
2) then the first interpretation should be adopted.
This observation has persuaded us to provide mechanisms for "estimating" the granularity of a formula (without first evaluating it) and making it available to the parser13 .
6  Experiments and Results  We have tested the effectiveness of TEA over time on several email corpora.
Emails are of particular interest to us due to our work in project RADAR14 : the project aims at building personal agents capable of scheduling meetings among different users.
Understanding the meaning of temporal expressions is therefore a crucial step.
The email dataset used in our development and testing were collected from MBA students of Carnegie Mellon University over the year 1997 and 1998.
The 277 students, organized in approximately 50 teams of 4 to 6 members, were participating in a 14-week course and running simulated companies in a variety of market scenarios [8].
For our study, 1,196 scheduling-related emails were manually selected from the 15,000+ dataset and were randomly divided into five sets (email1 to email5).
Only four of them are used in the results reported here: email1 was used to establish our baseline, email2 and email5 were used for development, and part of email4 was used for testing.
The temporal expressions in all of the datasets were initially tagged using rules developed for MinorThird15 , and subsequently corrected manually by two of the authors.
Table 3 shows some basic statistics of the datasets16 , and Fig.
7 shows a sample email from the datasets (edited).
It is worth noting that much of the previous work devoted on recognizing and normalizing temporal expressions have focused on newswire texts.
Distribution-wise emails 13 This is not always possible as temporal references and functional terms might appear in a formula.
14 Reflective Agent with Distributed Adaptive Reasoning.
http://www.radar.cs.cmu.edu/external.asp 15 http://minorthird.sourceforge.net/ 16 The percentages in some rows do not add up to 100% because some expressions like coordination can be classified into more than one type.
email1  # of # of explicit emails tempex 253 300 3 (1%)  email2  253  344  email4 (part.)
email5  149  279  126  213  19 (5.5%) 71 (25.4%) 14 (6.6%)  deictic  relative  duration  139 (46.33%) 112 (32.6%) 77 (27.6%) 105 (49.3%)  158 (52.67%) 187 (54.4%) 108 (38.7%) 92 (43.2%)  N/A 27 (7.8%) 22 (7.9%) 3 (1.4%)  Table 3.
Basic statistics of the email datasets Date: Thu, 11 Sep 1997 00:14:36 -0500 I have put an outline out in the n10f1 OpReview directory... (omitted) We have very little time for this.
Please call me Thursday night to get clarification.
I will need graphs and prose in files by Saturday Noon.
- Mary ps.
Mark and John , I waited until AFTER midnight to send this .
Figure 7.
A sample email (edited) exhibit a very different nature: in [10] for example it was reported that the proportion of explicit expressions is about 25% in the the North American News Corpus.
In contrast the same type of expressions accounts for only about 9.5% in the email datasets we use.
Other characteristics of emails comparing to newswire include having a higher rate of human errors17 and featuring more "creative" writing such as using tables, bullet lists, abbreviations, etc.
We first developed a prototype system and established our baseline over email1 (50%).
The system at that time did not have any focus tracking mechanism (i.e., it always used the time stamp as the focus), and it did not use any tense/aspect information.
We then gradually developed TEA to its current form using email1, email2 and email5.
During the process we added the recency-based focus tracking mechanism, incorporated the tense/aspect information into each TCNL formula (via coordinate prefixes), and introduced several representational improvements.
Finally we tested the system on the unseen dataset email4, and obtained the results shown in Table 4.
Note that the percentages reported in the table are accuracies, i.e., the number of correctly anchored expressions over the total number of temporal expressions over a dataset, since we are assuming 17 This includes typos and use of incorrect expressions; e.g., using "tomorrow" in emails sent after midnight when "today" was intended.
Accuracy email1 (test) email2 (dev) email5 (dev) email4 (test)  50% 78.2% 85.45% 76.34%  Parsing errors N/A 10.47% 5.16% 17.92%  Human errors N/A 1.7% 1% < 1%  Anchoring errors N/A 9.63% 8.39% 5.74%  Table 4.
Development and testing results  correct tagging of all of the expressions.
Also note that the parsing errors referred to in Table 4 were brought by the incorrect/missing TCNL formulae produced by the Finitestate Parser.
Our best result was achieved in the dev set email5 (85.45%), and the accuracy over the test set email4 was 76.34%.
Overall the accuracy numbers are all compared favorably to the baseline.
To put this performance in perspective, in [14] a similar task was performed over transcribed scheduling-related phone conversations.
They reported an average accuracy 80.9% over the CMU test set and 68.9% over the NMSU test set.
Although strictly speaking the two results cannot be compared due to differences in the nature of the corpora (transcription vs. typing), we nevertheless believe it represents a closer match compared to the other works done on the newswire genre.
It should be noted that [14] also adopted a recency-based focus tracking method.
7  Conclusion and Future Work  In this paper we described a system capable of anchoring temporal expressions in English.
System TEA features an extensible constraint-based calendar model and a compact representational language TCNL to capture the intensional meaning of temporal expressions.
We also reported favorable results from our experiments of using TEA on several email datasets.
Looking into the future we would like to extend the implementation of TCNL to allow representing the meaning of recurrence expressions (e.g., "every Wednesday at 6pm").
Currently this is possible only for limited expressions such as "6pm from Wednesday to Friday" ([{18hour }&[{wed}:{fri}]], which produces three coordinates when iterated).
An extension is to introduce a pattern construct into an enumeration formula to denote recurrence (e.g., [{18hour }&[{*wed}]] for "every Wednesday at 6pm").
We are also planning to use TEA on newswire texts in order to produce anchored event structures.
Although the recency-based focus model so far has served us well in the email genre, we might need to devise a more elaborated tracking mechanism to account for the unique rhetorical structure often exhibited in newswire.
Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.
NBCHD030010.
Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of Interior-National Business Center (DOI-NBC).
References [1] J. F. Allen.
Towards a General Theory of Action and Time.
Artificial Intelligence, 23:123-154, 1984.
[2] I. Androutsopoulos.
Temporal Meaning Representations in a Natural Language Front-end.
In M. Gergatsoulis and P. Rondogiannis, editors, Intensional Programming II (Proceedings of the 12th International Symposium on Languages for Intensional Programming, Athens, Greece, 1999.
[3] C. Bettini, S. Jajodia, and S. X. Wang.
Time granularities in database, data mining, and temporal reasoning.
SpringerVerlag, Berlin, 2000.
[4] L. Ferro, L. Gerber, I. Mani, B. Sundheim, and G. Wilson.
TIDES 2005 Standard for the Annotation of Temporal Expressions.
Technical report, MITRE, April, 2005.
[5] B. Han and M. Kohlhase.
A Time Calculus for Natural Language.
In The 4th Workshop on Inference in Computational Semantics, Nancy, France, September 2003.
[6] B. Han and A. Lavie.
A Framework for Resolution of Time in Natural Language.
TALIP Special Issue on Spatial and Temporal Information Processing, 3(1):11-32, March 2004.
[7] J. R. Hobbs, G. Ferguson, J. Allen, P. Hayes, I. Niles, and A. Pease.
A DAML ontology of time, Aug 23 2002.
[8] R. E. Kraut, S. R. Fussell, F. J. Lerch, and A. Espinosa.
Coordination in teams: Evidence from a simulated management game.
Journal of Organizational Behavior, to appear, 2004.
[9] A. K. Mackworth.
Consistency in networks of relations.
Artificial Intelligence, 8:99-118, 1977.
[10] I. Mani, B. Schiffman, and J. Zhang.
Inferring Temporal Ordering of Events in News.
In Proceedings of the Human Language Technology Conference (HLT-NAACL'03)., 2003.
[11] H. Ohlbach and D. Gabbay.
Calendar logic.
Journal of Applied Non-classical Logics, 8(4):291-324, 1998.
[12] Z. Ruttkay.
Constraint Satisfaction - a Survey.
Technical Report 11(2-3), CWI, 1998.
[13] R. Sauri, J. Littman, B. Knippen, R. Gaizauskas, A. Setzer, and J. Pustejovsky.
TimeML Annotation Guidelines, Version 1.2.1, January 31 2006.
[14] J. M. Wiebe, T. P. O'Hara, T. Ohrstrom-Sandgren, and K. J. McKeever.
An Empirical Approach to Temporal Reference Resolution.
Journal of Artificial Intelligence Research, 9:247-293, 1998.
Counterexample-Guided Abstraction RefinementPS Edmund Clarke School of Computer Science Carnegie Mellon University Pittsburgh, USA edmund.clarke@cs.cmu.edu  Abstract The main practical problem in model checking is the combinatorial explosion of system states commonly known as the state explosion problem.
Abstraction methods attempt to reduce the size of the state space by employing knowledge about the system and the specification in order to model only relevant features in the Kripke structure.
Counterexample-guided abstraction refinement is an automatic abstraction method where, starting with a relatively small skeletal representation of the system to be verified, increasingly precise abstract representations of the system are computed.
The key step is to extract information from false negatives ("spurious counterexamples") due to overapproximation.
The methods for alleviating the state explosion problem in model checking can be classified coarsely into symbolic methods and abstraction methods [6].
By symbolic methods we understand the use of succinct data structures and symbolic algorithms which help keep state explosion under control by compressing information, using, e.g., binary decision diagrams or efficient SAT procedures.
Abstraction methods in contrast attempt to reduce the size of the state space by employing knowledge about the system and the specification in order to model only relevant features in the Kripke structure.
An abstraction function associates a Kripke structure  with an abstract Kripke structure  such that two properties hold:  This research was sponsored by the Semiconductor Research Corporation (SRC) under contract no.
99-TJ-684, the National Science Foundation (NSF) under grant no.
CCR-9803774, the Office of Naval Research (ONR), the Naval Research Laboratory (NRL) under contract no.
N0001401-1-0796, and by the Defense Advanced Research Projects Agency, and the Army Research Office (ARO) under contract no.
DAAD19-01-1-0485, and the General Motors Collaborative Research Lab at CMU.
The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of SRC, NSF, ONR, NRL, DOD, ARO, or the U.S. government.
- Feasibility.
 is significantly smaller than  .
- Preservation.
 preserves all behaviors of  .
Preservation ensures that every universal specification which is true in  is also true in  .
The converse implication, however, will not hold in general: a universal property which is false in  may still be true in  .
In this case, the counterexample obtained over  cannot be reconstructed for the concrete Kripke structure  , and is called a spurious counterexample [10], or a false negative.
An important example of abstraction is existential abstraction [11] where the abstract states are essentially taken to be equivalence classes of concrete states; a transition between two abstract states holds if there was a transition between any two concrete member states in the corresponding equivalence classes.
In certain cases, the user knowledge about the system will be sufficient to allow manual determination of a good abstraction function.
In general, however, finding abstraction functions gives rise to the following dichotomy:  - If  is too small, then spurious counterexamples are likely to occur.
- If  is too large, then verification remains infeasible.
Counterexample-Guided Abstraction Refinement (CEGAR) is a natural approach to resolve this situation by using an adaptive algorithm which gradually improves an abstraction function by analysing spurious counterexamples.
(i) Initialization.
Generate an initial abstraction function.
(ii) Model Checking.
Verify the abstract model.
If verification is successful, the specification is correct, and the algorithm terminates successfully.
Otherwise, generate a counterexample  on the abstract model.
(iii) Sanity Check.
Determine, if the abstract counterexample  is spurious.
If a concrete counterexample  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  I can be generated, the algorithm outputs this counterexample and terminates.
[7]  (iv) Refinement.
Refine the abstraction function in such a way that the spurious counterexample I is avoided, and return to step (ii).
[8]  Using counterexamples to refine abstract models has been investigated by several researchers beginning with the localization reduction of Kurshan [19] where the model is abstracted/refined by removing/adding variables from the system description.
A similar approach has been described by Balarin and Sangiovanni-Vincentelli in [1].
A systematic account of counterexample guided abstraction refinement for CTL model checking was given in [10, 8].
Here, the initial abstraction is obtained using predicate abstraction [17] in combination with a simple static analysis of the system description; all other steps use BDD-based techniques.
The use of tree-like counterexamples guarantees that the method is complete for ACTL.
During the last few years, the CEGAR paradigm has been adapted to different projects and verification frameworks, both for hardware and software verification [20, 16, 14, 13, 3, 4, 2, 9, 7, 18, 5].
The major improvements to the method include, most notably, the integration of SAT solvers for both verification and refinement, and the use of multiple spurious counterexamples.
It is well-known that most abstraction methodologies can be paraphrased in the framework of abstract interpretation by Cousot and Cousot [12].
Giacobazzi and Quintarelli [15] have shown that, not surprisingly, this holds true for counterexample-guided abstraction refinement as well.
The practical and computational significance of such embeddings for verifying real-life systems however remains controversial.
[9]  [10]  [11]  [12]  [13] [14]  [15]  [16]  [17] [18]  References [1] F. Balarin and A. L. Sangiovanni-Vincentelli.
An iterative approach to language containment.
In Computer-Aided Verification, 1993.
[2] T. Ball and S. K. Rajamani.
Getting abstract explanations of spurious counterexamples in C programs, 2002.
Microsoft Technical Report MSR-TR-2002-09.
[3] S. Barner, D. Geist, , and A. Gringauze.
Symbolic localization reducation with reconstruction layering and backtracking.
In CAV 2002, volume 2404 of LNCS, pages 65-77, 2002.
[4] S. Chaki, J. Ouaknine, K. Yorav, and E. M. Clarke.
Multilevel abstraction refinement for concurrent C programs.
2002.
Submitted for Publication.
[5] E. Clarke, S. Chaki, S. Jha, and H. Veith.
Strategy guided abstraction refinement, 2003.
Manuscript.
[6] E. Clarke, O. Grumberg, S. Jha, Y. Lu, and H. Veith.
Progress on the state explosion problem in model checking.
[19] [20]  In Informatics, 10 Years Back, 10 Years Ahead, volume 2000 of LNCS, pages 176-194, 2001.
E. Clarke, A. Gupta, J. Kukula, and O. Strichman.
SAT based abstraction - refinement using ILP and machine learning techniques.
volume 2404 of LNCS, pages 265-279, Copenhagen, Denmark, July 2002.
E. Clarke, S. Jha, Y. Lu, and H. Veith.
Tree-like counterexamples in model checking.
In Proc.
Logic in Computer Science (LICS), 2002.
E. M. Clarke, A. Fehnker, Z. Han, B. H. Krogh, O. Stursberg, and M. Theobald.
Verification of hybrid systems based on counterexample-guided abstraction refinement.
In TACAS'03, pages 192-207, 2003.
E. M. Clarke, O. Grumberg, S. Jha, Y. Lu, and H. Veith.
Counterexample-guided abstraction refinement.
In Computer Aided Verification, pages 154-169, 2000.
E. M. Clarke, O. Grumberg, and D. E. Long.
Model checking and abstraction.
ACM Transactions on Programming Languages and Systems, 16(5):1512-1542, September 1994.
P. Cousot and R. Cousot.
Abstract interpretation : A unified lattice model for static analysis of programs by construction or approximation of fixpoints.
ACM Symposium of Programming Language, pages 238-252, 1977.
S. Das and D. Dill.
Successive approximation of abstract transition relations.
In LICS, pages 51-60, 2001.
S. Das and D. Dill.
Counter-example based predicate discovery in predicate abstraction.
In Formal Methods in Computer-Aided Design, pages 19-32, 2002.
R. Giacobazzi and E. Quintarelli.
Incompleteness, counterexamples and refinements in abstract model checking.
In SAS'01, pages 356-373, 2001.
M. Glusman, G. Kamhi, S. Mador-Haim, R. Fraer, and M. Vardi.
Multiple-counterexample guided iterative abstraction refinement: An industrial evaluation.
In TACAS'03, pages 176-191, 2003.
S. Graf and H. Saidi.
Construction of abstract state graphs with PVS.
In Computer-Aided Verification, June 1997.
T. A. Henzinger, R. Jhala, and R. Majumdar.
Counterexample guided control.
In ICALP, 2003.
To appear.
R. P. Kurshan.
Computer-Aided Verification of Coordinating Processes.
Princeton University Press, 1994.
Y. Lakhnech, S. Bensalem, S. Berezin, and S. Owre.
Incremental verification by abstraction.
pages 98-112, 2001.
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE
An Efficient Algorithm for Minimizing Time Granularity Periodical Representations Claudio Bettini Sergio Mascetti DICo - University of Milan, via Comelico 39, 20135 Milan, Italy {bettini,mascetti}@dico.unimi.it Abstract This paper addresses the technical problem of efficiently reducing the periodic representation of a time granularity to its minimal form.
The minimization algorithm presented in the paper has an immediate practical application: it allows users to intuitively define granularities (and more generally, recurring events) with algebraic expressions that are then internally translated to mathematical characterizations in terms of minimal periodic sets.
Minimality plays a crucial role, since the value of the recurring period has been shown to dominate the complexity when processing periodic sets.
1 Introduction Periodicity is a property used to describe and reason about temporal phenomena, and it has been the subject of research in knowledge representation (see e.g., [8, 12]), in databases, (see e.g., [7, 14, 13, 11]), and in temporal logic (see e.g., [15]).
Periodicity can be used, for example, to represent recurring events in a knowledge base, or to store data with a recurring validity period in a database.
Periodicity in particular plays a relevant role for the finite representation of time granularities; for example, the set of Mondays can be represented by a specific date for one Monday and by a recurring period of one week (7 days).
Considering that months have different lengths, years have leap years and their exceptions, and that granularities also include non standard ones like banking days or academic semesters, it is quite clear that the recurring period can become large and possibly difficult to identify.
Several symbolic formalisms have been proposed in the literature to represent granularities without having the user to explicitly give the formulas defining the periodicity.
For example, the slicing and dicing operators proposed in [6] can be used to represent the set of all second and third week of each month of January by the expression [2,3]/Weeks:during:January.
Simi-  larly, the expression all.Years + {2,4}.Months + {1}.Days  2.Days in the formalism proposed in [10] denotes the first 2 days of February and April of each year.
Similar, but more expressive formalisms are defined in [1], which is an extension of [6], and in [9] that introduces a new rich set of algebraic operators.
A mapping from the algebraic expressions to the corresponding mathematical periodic representations is not provided in the above papers.
A mapping from the algebra in [9] is provided in [3].
A mapping from the algebras in [6] and [10] is embedded in the proofs of [1].
However, these mappings do not guarantee to return a minimal representation, i.e., a representation using the smallest possible period value, and devising such a minimal mapping turns out to be a difficult task.
On the other side, the period value greatly affects the performance of operations on periodic sets.
For example, the GSTP system for granularity constraint reasoning ([2]) performs constraint satisfaction by applying operations on granularities represented as sets of periodic sets of integers.
The complexity is dominated by the least common multiple of the period values of the involved granularities.
A minimal periodical representation is not only desirable for processing granularities, but it would also provide a common low-level representation for expressions specified by users in their favorite symbolic formalism.
The technical contribution of this paper is an efficient algorithm to reduce the periodic representation of a time granularity to a minimal representation.
An immediate application of the algorithm is in a post-processing step in the conversion from an algebraic specification of a time granularity to its mathematical periodic representation.
From a practical point of view, this result allows users to intuitively define granularities (and more generally, recurring events) and having the system processing the underlying periodic sets as efficiently as possible.
The algorithm has a worst case complexity of O(n3/2 ) where n is the period of the input granularity representation.
The rest of the paper is organized as follows: in Section 2  we formally introduce time granularities and other notions that are needed to formulate our technical results.
In Section 3 we present the algorithm for period minimization, and in Section 4 we show its applications and briefly report on its implementation.
Section 5 concludes the paper.
Intuitively, H  G means that each granule of G is a union of some granules of H. For example, day  week since a week is composed of 7 days and day  b-day since each business day is a day.
Granularities are said to be bounded when L has a first or last element or when G(i) = [?]
for some i [?]
L. In this paper, for simplicity, we assume that all granularities are unbounded.
We assume the existence of an unbounded bottom granularity, denoted by [?]
which is full-integer labeled and groups into every other granularity in the system.
Since we are particularly interested in granularities which can be expressed as periodic repetitions of granules of other granularities (in particular a bottom granularity), we formally define the following relationship2  2 Periodic Representation of Granularities A comprehensive formal study of time granularities and their relationships can be found in [4].
In this paper, for lack of space we only introduce notions that are essential to show our results.
In particular, we report here the notion of labeled granularity which was proposed for the specification of a calendar algebra [9] 1 .
Granularities are defined by grouping sets of instants into granules.
For example, each granule of the granularity day specifies the set of instants included in a particular day.
A label is used to refer to a particular granule.
The whole set of time instants is called time domain, and for the purpose of this paper the domain can be an arbitrary infinite set with a total order relationship, <=.
Definition 3 A labeled granularity H groups periodically into a labeled granularity G if H groups into G and there exist positive integers N and P such that (1) for each label i of G, i + N is a label of G unless i + N is greater than the greatest k label of G, and (2) for each label i of G, if G(i) = r=0 H(jr ) and G(i + N ) is a non empty granule  of G then G(i + N ) = kr=0 H(jr + P ), and (3) if G(s) is the first non-empty granule in G (if it exists), then G(s + N ) is non empty.
Definition 1 A labeled granularity is a pair (L, G), where L is a subset of the integers, and G is a mapping from L to the subsets of the time domain such that for each pair of integers i and j in L with i < j, if G(i) = [?]
and G(j) = [?
], then (1) each element in G(i) is less than every element of G(j), and (2) for each integer k in L with i < k < j, G(k) = [?].
The groups periodically into relationship is a special case of the group-into relationship characterized by a periodic repetition of the "grouping pattern" of granules of H into granules of G. Its definition may appear complicated but it is actually quite simple.
Since H groups into G, any granule G(i) is the union of some granules of H; for instance assume it is the union of the granules H(a1 ), H(a2 ), .
.
.
, H(ak ).
Condition (1) ensures that the label i + N exists (if it is not greater than the greatest label of G) while condition (2) ensures that, if G(i + N ) is not empty, then it is the union of H(a1 + P ), H(a2 + P ), .
.
.
, H(ak + P ).
Condition (3) simply says that there is at least one of these repetitions.
We call the parameters P and N in Definition 3, a period and its associated period label distance, respectively.
We also denote by R the number of granules of G corresponding to each group of P consecutive granules of [?].
More formally, R is equal to the number of labels of G greater than or equal to i and smaller than i + N where i is an arbitrary label of G. Note that R is not affected by the value of i.
Note that the period is an integer value.
For simplicity we also use the expression "one period of a granularity G" to denote a set of R consecutive granules of G. In general, the periodically-groups-into relationship guarantees that granularity G can be finitely described (in  When L is exactly the integers, the granularity is called "full-integer labeled".
When L = Z+ we have the same notion of granularity as used in several applications (e.g., [4]).
However, in general, the set L of labels can be an arbitrary subset of (possibly noncontiguous) integers and these labels are used to identify granules.
Note that each labeled granularity can use a different set of labels.
For example, following this labeling schema, if we assume to map day(1) to the subset of the time domain corresponding to January 1, 2001, day(32) would be mapped to February 1, 2001, b-day(6) to January 8, 2001 (the sixth business day), and month(15) to March 2002.
Several interesting relationships can be defined among granularities.
The first of these is called group into and defines a partial order over the set of all granularities.
Definition 2 If G and H are granularities, then H is said to group into G, denoted H  G, if for each non-empty granule G(j), there exists  a (possibly infinite) set S of labels of H such that G(j) = i[?
]S H(i).
1 Labeled granularities are an extension of the more standard notion of granularities provided in [4].
2 This is simply an extension to labeled granularities of the analogous relation defined for "regular" granularities (see e.g., [4]).
2  terms of granules of H), providing the following information: (i) a value for P and N ; (ii) the set LG of labels of G in one period of G; (iii) for each j [?]
LG , the finite set Sj of labels of H, describing the composition of G(j); (iv) the labels of first and last non-empty granules of G, if their values are not infinite.
In the following, we call explicit granules the granules that have a label in LG .
A granularity G can have several periodical representations in terms of the bottom granularity; Indeed if P is a period value for G, then any multiple of P is also a period for G. Moreover, for each period value, different sets of consecutive granules can be used to describe the explicit granules contained in one period.
Among all possible pairs (P, N ) characterizing a periodically-groups-into relationship, there exist a pair (P  , N  ) such that P  is the smallest period value in all pairs.
The value P  is called the minimal period for the G and any representation adopting that value for the period is called minimal.
In order to distinguish different representations of the same granularity, the notation "G1 ", "G2 ", .
.
.
is used (read "representation 1 of G", "representation 2 of G", .
.
.
).
given granule z of another granularity G. For example, we may wish to find the month (an interval of the absolute time) that includes a given week (another interval of the absolute time).
This transformation is obtained with the up operation.
Formally, for each label z [?]
LG , zH G is undefined if  z  [?]
LH s.t.
G(z) [?]
H(z  ) ; otherwise, zH G = z ,   where z is the unique index value such that G(z) [?]
H(z ).
The uniqueness of z  is guaranteed by the monotonicity of granularities.
The notation zH is used when the source granularity can be left implicit (e.g., when we are dealing with a fixed set of granularities having a distinguished bottom granularity).
Another direction of the above transformation is the down operation: Let G and H be granularities such that H G  H, and z an integer.
Define  	z G as the set S of labels of granules of G such that j[?
]S G(j) = H(z).
This function is useful for finding, e.g., all the days in a month.
Example 1 Figure 1 shows how the same granularity can be represented using (1) period equal to 4 and the granules labeled with 1 and 2 as the explicit granules; (2) period equal to 4 and the granules labeled with 2 and 3 as the explicit granules or (3) period equal to 8 and the granules labeled with 2, 3, 4 and 5 as the explicit granules.
In the figure, the dotted curly brackets represent the explicit granules.
In this section we present an algorithm that, given a granularity representation G1 , computes the minimal period for G; in Section 3.4 we show how, given this value, a full characterization of a minimal representation can be obtained.
The input is a periodical representation G1 in terms of the bottom granularity; i.e.
the period PG1 , the period label distance NG1 , and the set of sets Sk with k = 0 .
.
.
NG1 - 1 such that, for an arbitrary a [?]
Z, Sk = 	k + a G if k [?]
LG1 , Sk = [?]
otherwise.
Note that, given this representation, the number R of granules in one period can be trivially computed.
^  3 An Algorithm for Period Minimization  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  G  3.1 The algorithm  G1 G2 G3  1  2  3  4  5  6  7  8  1  2  3  4  5  6  7  8  1  2  3  4  5  6  7  8  1  2  3  4  5  6  7  8  The main idea of the algorithm is that if G1 is not minimal, then there exists a minimal representation G2 such that PG1 = PG2 * m with m [?]
N+ .
Therefore, the goal of the algorithm is finding m: once it is found the output is simply PG1 1 m .
If G is minimal, then m = 1.
Clearly, m must be a divisor of PG1 and we will prove that m must also be a divisor of NG1 and RG1 .
Hence, the algorithm first computes the set S of possible values of m, then, for each n [?]
S, it checks if there exists a periodical P N representation G3 such that PG3 = nG1 and NG3 = nG1 .
Since the value of PG3 is inversely proportional to the value of n, the algorithm starts considering the integers in S from the biggest down to the smallest.
The execution terminates when the first representation G3 is found.
A non trivial part of the algorithm is checking if G can be represented using a period P and a period label distance N .
In general, this requires to prove that P and N satisfy the three conditions of Definition 3; however in this particular  Figure 1.
Different periodical representations of the same granularity  2.1 Granularity Conversions When dealing with granularities, we often need to determine the granule (if any) of a granularity H that covers a 3  For instance, 1G = 1 and 1 + P G = 7G = 4 = 1 + N ; 4G is undefined and 4 + P G = 10 is undefined.
Algorithm 1 minimizePeriod * Input: a periodical representation G1 ; * Output: the minimal period for G;  ^  * Method: 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15:  compute the set S whose elements are gcd(PG1 , NG1 , RG1 ) and its factors.
b := min(	min(LG1 ) G ); t := max(	max(LG1 ) G ) for all n [?]
S, from the biggest down to the smallest do P := PG1 /n; N := NG1 /n; failed := false for (k = b; k < t [?]
failed=false; k++) do if (kG is undefined) then if (k + P G is defined) then failed := true else if (k + P G is undefined) then failed := true else if (k + P G = kG + N ) then failed := true end if end for if (failed= false) then return P end if end for return PG1  G G1 n=6 n=3 n=2    undefined kG + N  if kG is undefined otherwise  1  2  3  4  5  6  7  8  9  1  2  3  4  5  6  7  8  9  1  2  3  4  5  6  7  8  9  1  2  3  4  5  6  7  8  9  1  2  3  4  5  6  7  8  9  Figure 2.
Graphical representation of the granules involved in Example 2  instance of the problem, it is known that G admits the periP 1 odical representation G1 and that [?
]k [?]
N+ s.t.
P = G k N and N = kG1 .
Therefore, it can be derived that the third condition of Definition 3 is always satisfied and the other two conditions are verified if and only if [?
]k [?]
K: k + P G =  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  3.2 Correctness Theorem 1 Given a periodical representation of granularity G, the algorithm minimizePeriod computes the minimal period for G. Proof.
To prove the theorem it is necessary to show three intermediate results.
(1)  Lemma 1 Given a granularity G, [?
]l, l [?]
R+ s.t.
for each P P representation Gi of G, NGii = l and RGii = l .
where K = {j [?]
L[?]
|min(	min(LG1 ) G ) <= j < max(	max(LG1 ) G )}.
Note that, since L[?]
= Z, K is an integers interval and, by definition of K and L, |K| <= P .
Therefore, the problem can be solved by verifying (1) with k ranging on a finite set.
G  G  Lemma 2 If G1 and G2 are two periodic representations of granularity G and G1 is minimal, then [?
]a [?]
N+ s.t.
(i)PG2 = aPG1 , (ii)NG2 = aNG1 ; (iii)RG2 = aRG1 .
Example 2 Figure 2 shows granularity G and its nonminimal periodical representation G1 (the dotted curly bracket indicates the explicit granules of G1 ).
Since PG1 = 12 and NG1 = RG1 = 6, the algorithm derives S = {2, 3, 6}.
It is not possible to represent G with a period P = 12/6 = 2 and a period label distance N = 6/6 = 1 since 2G is defined while 2 + P G = 4G is undefined.
Analogously, it is not possible to represent G with a period P = 12/3 = 4 and a period label distance N = 6/3 = 2 since 2G is defined while 2 + P G = 2 + 4G is undefined.
However it is possible to represent G with a period P = 12/2 = 6 and a period label distance N = 6/2 = 3;  Lemma 3 Let G be a granularity, G1 one of its possible representations and n a positive integer.
It is possible to P represent G with a period P = nG1 and a period label N distance N = nG1 if and only if Condition (1) in Section 3.1 is verified.
Assuming G1 is the representation of G given as input to the algorithm, from Lemma 2 follows that if there exists a minimal representation G2 s.t.
PG2 < PG1 , then [?
]n [?]
N+ s.t.
n > 1, PG1 = nPG2 , NG1 = nNG2 and RG1 = 4  nRG2 .
Clearly the set of possible values for n is the set S containing gcd(PG1 , NG1 , RG1 ) and its factors.
The condition P = PG1 /n with n [?]
S is necessary but not sufficient for the existence of a periodical representation that has P as the period.
We now show that the algorithm correctly verifies if there is a periodical representation having PG1 /n as the period.
For each n [?]
S, the value of the variable failed is first set to false; then Condition (1) is checked for each k [?]
K, and, if the condition is not satisfied, then the value of failed is set to true.
Therefore if failed = false when the for cycle terminates, then Condition (1) is verified and, by Lemma 3, there exists a periodical representation having PG1 /n as period.
Finally we show that it is correct to stop the evaluation of values in S as soon as a valid representation is found.
Indeed, let S  [?]
S be the set s.t.
for each i [?]
S  there exists a representation of G having PG1 /i as period.
Then the representation having P = PG1 /max(S  ) as period is minimal.
Suppose by contradiction that P is not the minimal representation, then [?
]P  s.t.
P  < P and P  is the period of the minimal representation of G. From Lemma 2 P P follows that [?
]m, n [?]
N+ s.t.
P  = m ; and P = nG1 ; Then P G1 P  = m*n .
This leads to a contradiction since m * n [?]
S  , m * n > n and n = max(S  ).
The last step of the algorithm is correct since if it is not possible to identify a representation for any n [?]
S, then the representation given as input is minimal and its period is returned.
  In the worst case, the number of times the algorithm performs the innermost for cycle (Algorithm 1, line 5) is |S|.
By definition of S, it follows that |S| < d(PG1 ) where d(n) indicates the number of divisors of n. The innermost for cycle performs, for each k from b to t, two up operations (*).
By definition of b and t, it follows that t - b <= PG1 , and since the up operation can be executed in constant time, the for cycle can be performed in time O(PG1 ).
As well known in number theory, [?]
if d(n) is the number of divisors of n, then, d(n) < 2 n. Hence, the for  cycle is always executed a number of times less than 2 PG1 , then the thesis follows.
 Note that a better upper bound for the dimension of S can be found.
Indeed, let g = gcd(PG1 , NG1 , RG1 ), then |S| = d(g) - 1 4 .
Clearly g <= PG1 and, since g is a divisor of PG1 , d(g) <= d(PG1 ).
Despite a detailed average-case analysis of time complexity is out of the scope of this paper, note that, in most practical applications of the algorithm, g << PG1 ; Therefore, d(g) n<< d(PG1 ).
Moreover, Theorem 318 in [5] states: " i=1 d(n) ~ n ln n"; hence, in the average case, |S| << d(PG1 ) where d(PG1 ) ~ ln(PG1 ).
3.4 Characterization of a minimal representation Here we show how, given a granularity G, its representation G1 and its minimal period P , it is possible to fully characterize a minimal representation G2 of G. Clearly PG2 = P and, from Lemma 2, NG2 = NG1 * P/PG1 .
The set of the explicit granules of G2 is the set of granules of G having labels in LG2 = {i [?]
LG1 |min(LG1 ) <= i < min(LG1 ) + NG2 }.
Since LG2 [?]
LG1 , the composition of each explicit granule G(j) with j [?]
LG2 in terms of [?]
is the same provided in G1 .
Finally, note that if G is bounded, the value of the bounds is the same independently from the representation.
3.3 Time complexity analysis Before presenting our formal result on the time complexity of the algorithm minimizePeriod, we show how it is possible to perform the up operation (*) in constant time.
Indeed, from the explicit granules of G, it is possible to create an array A of size PG1 that represents how the granules of [?]
are mapped into the explicit granules of G1 .
If b = min(	min(LG1 ) G ), then, for each j = 0 .
.
.
PG1 - 1, A[j] = null if b + jG is undefined, A[j] = b + jG G otherwise3 .
Using this data structure for each  i,fi i can i-b be computed as A[j] + aNG1 where a = P 1 and j = G i - b - aPG1 .
4 Implementation and applications In the last years, an application for performing temporal constraint reasoning with granularities has been developed at the University of Milan, and it is currently available as a web service (GSTP, see [2]).
The system still misses a user friendly formalism and interface to define new granularities.
Recently, the GRC (Granularity Representation Converter) application has been integrated with GSTP; its task is to perform conversions from the Calendar Algebra of [9] to the periodical representation, following the results in [3].
GRC allows users to specify the granularities appearing in the constraints by Calendar Algebra expressions.
Since  Theorem 2 The worst case time complexity of the algo3 rithm minimizePeriod is O(n 2 ) where n is the period of the input periodical representation.
[?]
Proof.
The set S can be computed [?]
in time O( n) by considering each integer from 2 to n and checking if it is a divisor of P , N and R. 3 Note  4 The "-1" comes from the consideration that the value 1 is not included in S.  that A can be built in time O(PG1 ).
5  the performance of GSTP is strongly affected by the period value of the granularities appearing in the constraints, a central task of GRC is to generate minimal representations.
Because the results in [3] do not guarantee minimality, the implementation of the minimizePeriod algorithm becomes an essential module of GRC.
A stand alone version of the software has been realized too, and it is mainly used for testing the correctness of the implementation and its performance.
The empirical performance results we obtained confirm an almost linear behavior.
To give an idea of the actual performance, we consider the representation of a granularity involving leap years and leap year exceptions.
In this case, the input representation has a period of 400years in terms of hours (about 3.5millions of hours), and the algorithm runs in less than a second on a standard PC (a Pentium M 1,7 Ghz).
[3] C. Bettini, S. Mascetti, X. Wang.
Mapping Calendar Expressions into Periodical Granularities.
In Proc.
of 11th International Symposium on Temporal Representation and Reasoning, pp.
96-102, IEEE Computer Society, 2004.
[4] C. Bettini, X. Wang, S. Jajodia, Solving MultiGranularity constraint networks, Artificial Intelligence, 140(1-2):107-152, 2002.
[5] G. H. Hardy, E. M. Wright, An Introduction to the Theory of Numbers.
Oxford University Press, 1960.
[6] B. Leban, D. Mcdonald, and D. Foster, A representation for collections of temporal intervals, in Proc.
of the American National Conference on Artificial Intelligence, pp.
367-371, AAAI Press, 1986.
[7] F. Kabanza, J.-M. Stevenne, P. Wolper.
Handling Infinite Temporal Data.
In Proc.
of ACM PODS, pp.
392- 403, 1990.
5 Conclusions and future works We presented an algorithm that, given a periodic representation of a time granularity G, computes the minimal period, and hence provides a minimal representation of G. The algorithm can be used to ensure the minimality of representations that are directly generated by a user or that are the result of a conversion from an algebraic expression.
As a future work, we are planning to implement a graphical user interface that supports the user in the definition of calendar algebra expressions; The minimizePeriod algorithm will be used to ensure minimality of the underlying periodical representations.
Analogous tools may be developed, by using the same algorithm, for other symbolic formalisms (e.g., [6, 10]).
The various tools will have the advantage of interoperability, since they will be based on a common underlying periodic representation.
[8] Robert A. Morris, William D. Shoaff, Lina Khatib.
Domain-Independent Temporal Reasoning with Recurring Events.
Computational Intelligence, 12: 450-477, 1996.
[9] P. Ning, X. Wang, S. Jajodia.
An Algebraic Representation of Calendars.
Annals of Mathematics and Artificial Intelligence 36(1-2): 5-38, 2002.
[10] M. Niezette and J. Stevenne, An efficient symbolic representation of periodic time, in Proc.
of International Conference on Information and Knowledge Management, pp.
161-168, ACM Press, 1992.
[11] P. Revesz, M. Cai Efficient Querying of Periodic Spatiotemporal Objects, in Lecture Notes in Computer Science, Volume 1894, Page 396, Springer, 2000.
6 Acknowledgements  [12] Paolo Terenziani, Integrating Calendar Dates and Qualitative Temporal Constraints in the Treatment of Periodic Events.
IEEE Trans.
on Knowledge and Data Engineering, 9(5): 763-783, 1997.
This work has been partially supported by Italian MIUR (FIRB "Web-Minds" project N. RBNE01WEJT 005).
The authors also wish to thank Lavinia Egidi for her support on some issues in number theory.
[13] D. Toman, J. Chomicki Datalog with Integer Periodicity Constraints, in Journal of Logic Programming, 35:3, 263-90, 1998.
References  [14] Alexander Tuzhilin, James Clifford.
On Periodicity in Temporal Databases.
Information Systems, 20(8): 619- 639, 1995.
[1] C. Bettini, R. De Sibi.
Symbolic Representation of User-Defined Time Granularities, Annals of Mathematics and Artificial Intelligence, 30(1-4):53-92, 2000.
[15] Pierre Wolper.
Temporal Logic Can Be More Expressive.
Information and Control, 56(1/2): 72-99, 1983.
[2] C. Bettini, S. Mascetti.
V. Pupillo A system prototype for solving multi-granularity temporal CSP.
CSCLP 2004, LNAI 3419, pp.
142-156, Springer, 2005.
6
Towards a Theory of Movie Database Queries Bart Kuijpers University of Limburg (LUC) Department WNI B-3590 Diepenbeek, Belgium bart.kuijpers@luc.ac.be  Jan Paredaens University of Antwerp (UIA) Dept.
of Math.
& Computer Science Universiteitsplein 1 B-2610 Antwerpen, Belgium pareda@uia.ua.ac.be  Dirk Van Gucht Indiana University Computer Science Dept.
Bloomington, IN 47405-4101, USA vgucht@cs.indiana.edu  Abstract We present a data model for movies and movie databases.
A movie is considered to be a 2-dimensional semialgebraic figure that can change in time.
We give a number of computability results concerning movies: it can be decided whether a frame of a movie is only a topologically transformation of another frame; a movie has a finite number of scenes and cuts and these can be effectively computed.
Based on these computability results we define an SQLlike query language for movie databases.
This query language supports most movie editing operations like cutting, pasting and selection of scenes.
1.
Introduction We present a data model for movies and movie databases.
We consider a movie to be an infinite sequence of 2dimensional figures that evolve in time.
Each figure consists of a possibly infinite number of points in the 2-dimensional plane.
A recent and much acclaimed method for effectively representing infinite geometrical figures is provided by the constraint database model, that was introduced by Kanellakis, Kuper and Revesz in their 1990 seminal paper [10] (an overview of the area of constraint databases can be found in [13]).
In this model, a 2-dimensional geometrical figure is finitely represented by means of a Boolean combination of polynomial equalities and inequalities.
These involve polynomials with two real variables that represent the  spatial coordinates of a point in the plane.
The set of points on the upper half of the unit circle, for instance, is in this context given by         fi    fi   In more mathematical terminology, these figures are called semi-algebraic sets and for an overview of their properties we refer to [3, 6].
This way of representing fixed figures can easily be adapted to describe figures that change.
Indeed, we can add a time dimension and consider geometrical objects in 3-dimensional space-time that are described by polynomial equalities and inequalities that also have a time variable fi.
This gives us a data model for movies.
Figure 1 gives an example of a movie, in particular a potential scene from Star Trek.
In this short movie the starship Enterprise remains at a constant position in space and can therefore be described   fi    by formula  fifi   fi  fi     in which fi is lacking.
A fired photon torpedo follows the dotted line (between the moments fi   and fi  fi) an then explodes (depicted as increasing dotted circles, between fi  fi and fi  ).
At the bottom of Figure 1 three frames of the movie are shown: at fi  fi fi and .
The complete movie can be described by the set     fi    fifi      	 fi 	   (      fi   	 fi 	 fi       	 fi   fi  fi  fi 	 )   The movie of Figure 1 can be used to illustrate a number of properties that all movies in this model have in common.
       Figure 1.
USS Enterprise firing a photon torpedo at a (cloaked) Klingon vessel.
For instance, between fi  fi and fi   the movie frames change continuously and all frames (fi  fi 	 ) are, topologically seen, the same.
We will call such a sequence of frames a scene of the movie.
Moments in which the movie changes discontinuously are referred to as cuts in the movie.
In the movie of Figure 1 there is, for instance, a cut at fi  fi: a point changes into an increasing circle.
The movie of Figure 1 has five scenes and six cuts (start and end of the movie included).
We remark that our notion of scene is finer that the cinematographic notion of scene.
We will show that the number of cuts and scenes in a semi-algebraic movie is always finite and that a representation of them by means of polynomial constraints can be effectively computed.
A key ingredient in this computation is a decision procedure for testing whether two movie frames can be topologically transformed into one another, i.e., whether they are homeomorphic.
Although deciding whether two 2-dimensional semi-algebraic sets are homeomorphic is a result that belongs to the mathematical folklore, a written proof of it is not to be found in the mathematical literature [15, 18].
We give a decision procedure and we also generalize it to parameterized frames: there is an algorithm that, given two movie frames that depend on time parameters fi	 and fi , produces a formula built with conjunction and disjunction from formulas of the form   fi  	 and fi    ( 	   constants and   fi ) that expresses, in function of fi 	 and fi , whether the frames are homeomorphic.
Finally, we define an SQL-like language to query movie databases.
This language is based on the above computability results and on a well-known language to query databases in the constraint model, namely the relational calculus augmented with polynomial inequalities [10, 16, 13].
It follows from a result by Tarski that the latter language is also  effective [20] (although variables range over the real continuum).
Our query language supports all basic movie editing operations like selecting scenes that satisfy some condition, composing several scenes into a movie, removing scenes, etc.
It also allows for the manipulation of single scenes and even of single frames.
This paper is organized as follows.
In Section 2, we formally define the notion of movie, frame, scene and cut.
Procedures to decide homeomorphism of frames and to compute the scenes and cuts of a movie are given in Section 3.
In Section 4, we present a query language and discuss expressibility issues.
2.
Movies, Frames, Scenes and Cuts We denote the set of the real numbers by .
In the following we will consider planar figures that change in time.
A moving figure is described by means of an (often infinite)  , where and  represent set of tuples    fi in the spatial coordinates of a point in the 2-dimensional real and fi represents the time coordinate in .
We first plane define the notion of a movie.
Definition 2.1 A movie is a set       fi       	 fi 	 	     fi  where  and 	 are real algebraic numbers and where    fi is a formula built with the logical connectives    from atomic formulas of the form    fi  , with    fi a polynomial with real algebraic coefficients and real variables   fi.
The numbers  and 	 are called the beginning and end of the movie respectively and are de and .
noted by A movie database is a finite set of movies.
   fi       A set    fi    fi       fi is a scene of  if  is a maximal open interval in fi   	 fi 	 	 such that  is continuous in each fi     .
   A point fi  in which  is not continuous is called a cut   in .
fi    fi    	  Figure 2.
An example of a movie.
Figure 2 depicts the movie    fi     fi 	 	  (  fi 	 fi     fi     fi  fi 	 	     fi      fi  fi  	) in the space  .
This movie shows at its beginning (i.e., at fi   fi) a single point in the origin.
Then it shows a disk whose radius increases and later decreases and ends in a point at moment fi  fi, followed by a circle whose radius increases, decreases, increases and then shrinks to a point.
Finally, for   fi 	 	, this movie shows nothing.
fi 	  Definition 2.2 Let  be the movie    fi     	 fi 	 	     fi and let  	 fi  	 	.
The set        fi   is called the frame of the movie  at the moment fi  and is denoted by  Az .
  For the movie of Figure 2, for instance, the frame  A 	 is the origin,   is the closed unit disk and   is the empty set.
We can use this same example to illustrate the notion of a scene.
For  fi  fi  fi, the movie of Figure 2 shows a disk on which is zoomed into and then zoomed out of.
This continuous sequence of frames will be called a scene.
Also for fi  fi  , we have a scene in which a circle is continuously deformed.
Scenes are separated by cuts.
In the following definition these concepts are formalized.
The notion of continuity that we will give may seem rather involved.
It corresponds, however to the intuitive notion of acontinuously changing,a as illustrated by the above example.
   fi   	fi	 Definition 2.3 Let   	     fi be a movie and let  	 fi   	 	.
  is continuous on the right in fi   if there exists an    and a continuous (in fi) series               fi  	 fi 	 fi    of homeomorphisms of such that  Az    for all fi  fi   fi   .
is continuous on the left in fi   is defined similarly.
And  is continuous in fi   if it is continuous both on the right and on the left in fi   .
The movie of Figure 2 is continuous in every fi in the open intervals  fi fi, fi  and  	.
These intervals therefore determine the three scenes of the movie.
There are four cuts in the movie of Figure 2: in fi   fi, fi,  and 	.
Remark that the beginning and the end of a movie are always cuts.
3.
Computability Results In this section, we present two computability results concerning movies.
First, we show that it is decidable whether two (parameterized) frames are homeomorphic.
Next, we will show that a movie has a finite number of scenes and cuts and that formulas describing them can be computed from the formula that defines the movie.
A key lemma in this context is the following.
Lemma 3.1 It is decidable whether two movie frames are isotopic1 , and also whether they are homeomorphic.
Proof (sketch).
Let  and  be two movie frames.
 and  are homeomorphic if and only if  is isotopic to  or to a reflection of  (see, e.g., [7, 14, 19]).
It therefore suffices to prove that it is decidable whether  is isotopic to  .
The algorithm to decide isotopy first computes for   , respectively  the labeled planar graph embedding   as follows.
The nodes of   are the asingular pointsa of  , i.e., the points that do not belong to the topological interior of  or the complement of  , nor to a topologically smooth border of  (see [12] for a formal definition) together with the lowest left most points on each closed curve of the topological border of  on which there is no singular point.
As an illustration, we take the frame  shown in (a) of Figure 3.
The singular points of  are  	   and  .
The closed polygon in the right upper corner of  does not contain a singular point and has more than one most left point.
Of these the lowest is picked:   .
We remark that the singular points can be computed by means of a first-order formula in the theory of the real numbers.
It was shown by Tarski that this theory is effective [20], and symbolic algorithms for the first-order theory of the reals [1, 5, 17] can effectively compute the nodes.
The computation of the other nodes can be performed via a Cylindrical Algebraic Decomposition (CAD) [5] (see also [1]).
1 Isotopic means homeomorphic by an orientation preserving homeomorphism.
fi  fi    fifi fi   fi    fi  fi  fi fifi  A"      fi  fi  fi  (a)  (b)  Figure 3.
A frame of a movie  (a) and its graph  (b).
In the graph   (see (b) in Figure 3) these nodes are labeled with typed labels:   for nodes that belong to  and A  labels for nodes that do not belong to  .
Next, the connected components of the intersection of  with the topological border of  minus the labeled points are computed.
These form edges of   and are labeled with labels of type   .
Similarly,  A  labels are given to the connected components of the border that does not belong to  .
The topological border of a frame can be computed in the first-order theory of the reals.
The computation of connected components of a frame is described in [4, 8, 9].
Finally, the areas formed by the graph embedding are computed and labeled   , respectively A  depending on their containment in  .
Let the sets of labeled nodes, edges and areas be called  ,  and  , respectively.
From results in [11] it follows that  and  are isotopic if and only if there exist bijections 	   fi   , 	  fi   , and 	fi  fi   that map -labels to -labels and  -labels to  -labels and that preserve the clockwise occurrence of edges and areas around each of the labeled nodes.
These conditions can be verified.
It is there fore decidable whether two movie frames are isotopic.
It should be remarked (details omitted) that it can be decided whether two frames  and  are isotopic in polynomial time (in the size of the polynomial constraint formulas that describe  and  ).
Theorem 3.1 Let 	 and  be two movies.
There is an algorithm that on input these two movies produces a formula AA" AAz fi	  fi  built with conjunction and disjunction from formulas of the form   fi  	 and fi    ( 	   constants and   fi ) that expresses, in function of fi 	 and fi , whether the two frames 	A" and Az are isotopic (the same is true for homeomorphic).
Proof (sketch).
Let  be the movie    fi     	 fi 	 	     fi Collins proves that from the formula  	 fi 	 	     fi a Cylindrical Algebraic Decomposition (CAD)  of    can be computed such that  each cell in  entirely belongs to  or to the complement of  [5] (see also [1]).
 induces a CAD   of the  fiplane which in its turn induces a CAD   of the fi-axis.
For the movie of Figure 2 this is illustrated in Figure 4.
The cells of  are points, lines, curves, 2-dimensional surfaces and 3-dimensional areas that are built as stacks on the different cells of  .
The cells of  are the (black and grey) dots, arcs and patches of white space in Figure 4 (compare with Figure 2).
  consists of the grey dots on the fi-axis and the open intervals determined by them as shown in Figure 4.
The cells of  are stacks built on these points and intervals.
It can be easily shown that the movie  is continuous in each fi that belongs to one of the open intervals of   .
So,  remains isotopic in these intervals.
The only possible cuts of the movie are therefore the points of   (grey dots).
The algorithm we seek could therefore work as follows.
Compute, using Collinsas CAD algorithm, both for  	 and  the representatives of all the cells in the induced CAD  .
Let these be 	        	 and 	          respectively.
Decide, using Lemma 3.1, for each pair   in the set 	      	  	        , whether 	 is isotopic to  .
If they are, and if  represents, letas say an interval   fi	  	 of the induced CAD of the fi-axis in  	 and if  represents, letas say a point   in the induced CAD of the fi-axis in  , then we add the conjunction   fi 	  	  fi    as a disjunct to the formula  AA" AAz fi	  fi .
 To illustrate the previous theorem, we give  AA fi	  fi for the movie  of Figure 2:  (    A"       A"    A"   (    A"     A"  fi    Az     Az  fi    A"   fi          Az    A"  Az         Az     Az  fi  )  Az  fi  )  fi   Theorem 3.2 A movie has a finite number of scenes and cuts.
Polynomial constraint formulas describing them can be effectively computed.
   fi    Proof (sketch).
Let  be the movie  	 fi 	 	     fi Consider again a CAD of ,     fi    	  Figure 4.
The induced CADs  (black and grey) and  (grey only) for the movie of Figure 2. as in the proof of the previous theorem.
From the proof of the previous theorem it is clear that there are only a finite number of cuts and scenes.
The cuts are among the points of the induced CAD   .
Not all these points must be cuts, however.
For the example of Figures 2 and 4, the grey dots at fi   fi fi  and 	 are cuts.
The one at fi  	, however, is not.
For what concerns the computation of the scenes and cuts, we first compute the candidate points for cuts (namely, the points in  ).
It remains to be tested whether  is continuous in these points.
Let fi   be a point in  different from the beginning or end of .
The test for continuity of  in fi  is two-fold: 1. test whether the frame  Az is isotopic to two frames Az A  and Az  in the neighboring intervals; 2. test for each   fi      (resp.
 ) whether for each small enough    and each A   there exist a points Az   Az  and AzAz   AzAz  at distance at most A from    such that Az   Az  fi       (resp.
 ) and AzAz  AzAz  fi        (resp.
 ).
The first test can be performed using the techniques outlined in Lemma 3.1.
The second test can be expressed as a sentence in the first-order theory of the reals and is therefore effective [20].
Condition 1 is clearly necessary for continuity.
It is, however, not sufficient.
At a cut, a frame can, for instance, just jump to a different location.
This would not violate Condition 1.
Condition 2 guarantees that there is not a jump.
When it is decided which of the points of   are cuts, the computation of the scenes is straightforward.
Collinsas algorithm produces polynomial constraint formulas for all the cells in a CAD.
The scenes and cuts can therefore also be given by means of their defining polynomial   constraint formula.
4.
Querying Movie Databases In this section, we present an effective SQL-like language to query movie databases: SQL.
This language is based on the computability results of the previous section and on a well-known query language for databases in the constraint databases model, namely the relational calculus augmented with polynomial constraints.
First, we define Definition 4.1 A movie database query is a mapping that   maps every -tuple of movies to a movie.
In the following, we will consider queries that have parameters 	      	 .
The calculus: We will use the relational calculus augmented with polynomial constraints, the calculus for short, as an essential part of SQL.
The calculus was introduced and studied in, e.g., [10, 16] (see also [13]).
A calculus formula    	          	      	   is built from the atomic formulas    fi (  fi     ) and  	         ( a polynomial), the logical connectives    and the quantifiers  .
The calculus formula               (                	   fi), for instance, defines the moments when there appears a circle (as a subset) in movie 	 .
The language SQL: An elementary query in SQL is of the form  fi  	      fi          	 fi 	 	  ,  where  and 	 are real algebraic numbers and  is a condition that can be expressed by means of the usual logical  connectives and quantifiers, calculus expressions, other elementary SQL queries and the following primitives:    	     fi   	 fi 	 	     fi 	      	    , with  a calculus formula and  and  computable 2 functions that work on inputs 	      	  and  	      	  ( a natural number), respectively, and return natural numbers.
This primitive returns the movie consisting of the scenes and cuts in the movie defined by  	 fi 	 	     fi 	      	  whose sequence numbers are  fi 	      	    	      	        	      	  	      	  (in that order and consecutive);          fi   	 fi 	 	     fi 	      	  , with  a calcuIt relus formula and  a natural number.
turns the -th cut in the movie defined by  	 fi 	 	     fi 	      	 ; fi	     fi   	 fi 	 	  	   fi 	      	 A"     fi   	 fi 	 	     fi 	      	 Az , with 	 and  calculus formulas.
It expresses the condition on fi 	 and fi that tells us when the two given frames are isotopic (as discussed in Theorem 3.1);    fi     fi   	 fi 	 	     fi 	      	  with  a calculus formula.
Also     fi   	 fi 	 	     fi 	      	 , with  a calculus formula.
They express the beginning and end of the movie defined by  	 fi 	 	     fi  	      	 .
Furthermore, from elementary queries more complicated queries can be constructed by composition, which we denote by A.
For two movies  	 and  , 	 A  is defined to be the movie consisting of  	 without its last frame, immediately followed by  .
The result of an SQL query is a movie.
The meaning of these queries is the obvious one and will be illustrated by the consequent examples.
The function of the -part of an elementary query is to indicate which of the input movies are under consideration.
It should be noted that some of these primitives are redundant and are only added for ease  of use.
From the proof of Theorem 3.2 it follows that can be expressed in terms of .
Finally, we remark that  fi	     Theorem 4.1 SQL queries are computable.
2 This means computable on the polynomial constraint formulas that define the movies.
Proof (sketch).
Let  be a SQL query.
Given the polynomial constraint formulas of the input movies  	      	 , we can, by Theorems 3.1 and 3.2 replace all the occur,  and  in  by conrences of crete constraint formulas.
Also the beginning and end of movies can be computed (from a CAD, for instance).
 and  can be reTherefore all occurrences of placed by concrete constraint formulas.
This way we obtain a first-order formula over the reals, that can possibly contain quantifiers.
From Tarskias quantifier elimination theorem it follows that these can be eliminated [20].
This yields a polynomial constraint formula for the output of the query .
The output is guaranteed to be a movie by the syntactic condition  	 fi 	 	 that appears in the 	  part of elementary SQL queries.
   	    fi	  fi    We give some examples of SQL queries, that illustrate that all the basic movie editing operations can be performed in SQL: Example 1: aGive all the frames of the movie  	 that are homeomorphic to a circlea is expressible by the elementary query  fi  	      fi  	  fi 	  	 fi  fi 	  	   	   fi  fi	  !   !
 fi 	.
  Example 2: aGive all the scenes of the movie  	 of which all frames are homeomorphic to a circlea is a variation on the query of Example 1 and it is expressible by the elementary query  fi  	      fi  	  fi 	  	 fi  fi 	  	  	 	      fi  fi	  !   !
 fi 	 	   ,   where  is the function that returns the number of scenes of the input movie and  is the identity function of the natural numbers.
Example 3: aRemove scene 2 from movie  	 a is expressed by  fi  	      fi  	  fi 	  	 fi  fi 	  	  	 	     fi    fi  where  is the function that maps 1 to itself and all   to   fi.
fi  Example 4: aGive me the first scene of movie  	 followed by the second scene of movie  followed by movie   a is a query that manipulates complete scenes.
It can be expressed as the composition of the elementary queries given by the following three expressions:     fi  fi  	   	  fi  	   fi 	 	 fi fi 	 fi  fi 	  	 	  fi fi  	 	 fi fi   fi    fi  fi  	     fi 	   fi  	 fi  fi 	  	   fi   	   fi    fi    fi  fi  	   fi  	 fi  fi 	         fi  Example 5: A query of particular interest for the Star Trek movie of the Introduction: aGive all frames of  	 that contain a photon torpedo (i.e., an isolated point)a can be expressed as     fi  	  fi 	 	 fi  fi 	  	   	   fi        (	       fi       !
 	  ! fi        !
          !
   )  Example 6: The following query manipulates the frames of a scene.
It also reverses a complete scene.
aPlay movie  	 at double speed followed by the reversed play of movie  turned upside downa is expressed as the composition of the movies expressed by the following elementary queries  fi  	      fi  	  fi 	 	 fi  fi 	  	   	   fi  fi  	      fi      fi	     	 fi  fi          fi     fi  	  	 	  fi fi   fi  fi 	 	 fi fi  fi  fi  fi 	 	  fi fi  	 	  fi fi  Example 8: The next example manipulates complete scenes without touching their individual frames.
aReturn the movie consisting of the scenes of movie  	 but played in reversed ordera is expressed by  fi  	     where the numbers fi and  stand for the constant functions to fi and .
fi  	   Example 7: The next example manipulates one scene.
aReturn the first half of the first scene of movie  	 a is expressed by     fi  	  fi 	  	 fi  fi 	  	   	 	       fi  where  is the function that maps  to      fi.
Example 9: The last example returns a certain computable scene.
aReturn the middle scene of movie  	 a is expressed by  fi  	      fi  	  fi 	  	 fi  fi 	  	   	 	  fi     fi  where  is the function that maps every natural number to   fi if the number of scenes is odd and to  if it is even.
5.
Discussion and conclusion We have presented a data model for movies and movie databases, in which a movie is considered to be a 2dimensional semi-algebraic figure that can change in time.
We have given a number of computability results concerning movies: homeomorphism and isotopy of movie frames are decidable; a movie has a finite number of scenes and cuts and these can be effectively computed.
Based on these computability results we have defined an SQL-like query language for movie databases.
This query language supports most movie editing operations like cutting, pasting and selection of scenes.
We remark that the presented model is very elementary and that it can be extended and made more suitable for practical applications in many ways.
For instance, the movies that we consider here have frames that are purely black and white (even without variations of grey and certainly without colors).
Movies have one single aimagea, whereas many  movies (e.g., cartoon movies) are multi-layered.
The presented model can be extended to cope with this.
Finally, we remark that our model cannot straightforwardly be applied to existing movies.
The problem of converting cinematographic movies, for instance, into the proposed model is beyond the scope of this paper.
We remark however that a number of 3D animation tools and virtual reality environments work with data that can be readily converted into the constraint model.
3D Studio Max [21] and Virtual Reality Modeling Language (VRML) [2] are examples of such environments.
Acknowledgements.
The authors are grateful to Sofie Haesevoets for helpful comments and suggestions to improve the paper.
References [1] D.S.
Arnon.
Geometric reasoning with logic and algebra.
Artificial Intelligence, 37, pages 37a60, 1988.
[2] G. Bell, A. Parisi, and M. Pesce.
The Virtual Reality Modeling Language.
www.vrml.org/VRLM1.0/vrml10c.html [3] J. Bochnak, M. Coste, and M.-F. Roy.
G eEomeEtrie AlgeEbrique ReEelle.
Springer-Verlag, 1987.
[9] J. Heintz, M.-F. Roy, and P. SolernoE.
Description of the Connected Components of a Semialgebraic Set in Single Exponential Time.
Discrete and Computational Geometry, 6, pages 1a20, 1993.. [10] P.C.
Kanellakis, G.M.
Kuper, and P.Z.
Revesz.
Constraint query languages.
Journal of Computer and System Sciences, 51(1), pages 26a52, August 1995.
[11] B. Kuijpers, J. Paredaens, and J.
Van den Bussche.
Lossless Representation of Topological Spatial Data.
In M. Egenhofer and J.
Herring, editors, Advances in Spatial Databases, 4th International Symposium, SSDa95, volume 951 of Lecture Notes in Computer Science, pages 1a13, Springer-Verlag, 1995.
[12] B. Kuijpers, J. Paredaens, and J.
Van den Bussche.
On topological elementary equivalence of spatial databases.
In F. Afrati and Ph.
Kolaitis, editors, 6th International Conference on Database Theory (ICDT a97), volume 1186 of Lecture Notes in Computer Science, pages 432a446, Springer-Verlag, 1997.
[13] G. Kuper, L. Libkin, and J. Paredaens.
Constraint databases.
Springer-Verlag, 2000.
[14] E.E.
Moise.
Geometric Topology in Dimensions 2 and 3, volume 47 of Graduate Texts in Mathematics.
Springer-Verlag, 1977.
[15] A. Nabutovsky.
Personal communication.
June 1997.
[4] J.
Canny, D. Grigoraev, and N.N.
Vorobjov jr. Finding Connected Components of a Semialgebraic Set in Subexponential Time.
Applicable Algebra in Engineering, Communication and Computing, 2, pages 217a238, 1992.
[5] G.E.
Collins.
Quantifier elimination for real closed fields by cylindrical algebraic decomposition.
In volume 33 of Lecture Notes in Computer Science, pages 134a183.
Springer-Verlag, 1975.
[6] M. Coste.
Ensembles semi-algeEbriques.
In G eEometrie AlgeEbrique ReEelle et Formes Quadratiques, volume 959 of Lecture Notes in Mathematics, pages 109a138.
Springer-Verlag, 1982.
[7] R.H. Cromwell and R.H. Fox.
Introduction to Knot Theory, volume 57 of Graduate Texts in Mathematics.
Springer-Verlag, 1977.
[8] J. Heintz, T. Recio, and M.-F. Roy.
Algorithms in Real Algebraic Geometry and Applications to Computational Geometry.
DIMACS Series in Discrete Mathematics and Theoretical Computer Science, Volume 6, pages 137a163, 1991.
[16] J. Paredaens, J.
Van den Bussche, and D. Van Gucht.
Towards a theory of spatial database queries.
In Proceedings 13th ACM Symposium on Principles of Database Systems, pages 279a288.
ACM Press, 1994.
[17] J. Renegar.
On the computational complexity and geometry of the first-order theory of the reals.
Journal of Symbolic Computation, 13, pages 255a352, 1989.
[18] M.-F. Roy.
Personal communication.
May 1997.
[19] J. Stillwell.
Classical Topology and Combinatorial Group Theory, volume 72 of Graduate Texts in Mathematics.
Springer-Verlag, 1980.
[20] A. Tarski.
A Decision Method for Elementary Algebra and Geometry.
University of California Press, 1951.
[21] 3D Studio MAX.
http://www.max3d.com/.
Engineering Time in Medical Knowledge-Based Systems through Time-Axes and Time-Objects E.T.
Keravnou Department of Computer Science, University of Cyprus Kallipoleos 75, P.O.Box 537, CY-1678 Nicosia, Cyprus email: elpida@turing.cs.ucy .ac.cy  Abstract  'natural ' granularities are lunar months, calendar months, years, etc, respectively.
Starting from the premise that time representation and temporal reasoning must constitute integral aspects of a competent, knowledge-based, medical system, the paper presents the relevant requirements and discusses their realisation in terms of a generic temporal kernel to be embedded in such a system.
The kernel has a layered architecture where the bottom laver gives the ontological primitives and their associated axioms, and rhe higher layers implement the required temporal reasoning.
The principal primitives of the ontology are the time-axis and the time-object.
1.2 Model of occurrences An appropriate model for occurrences must support the following: a Absolute and relative occurrences.
"No ossification of knee epiphyses at birth" expresses an occurrence in absolute terms, ie with respect to some fixed point, while "narrow thorax until kyphoscoliosis" expresses an occurrence, "narrow thorax", relative to another occurrence, "kyphoscoliosis".
e Absolute and relative vagueness.
"Wide triradiate cartilage upto about the age of 11 years" exprcsses absolute vagueness while "nausea precedes or coincides with the headache" expresses relative vagueness.
a Absolute and relative duration.
"Two days of headache" expresses the duration of headache in absolute terms while "headache during the nausea" does so in a relative way.
a Incompleteness.
Patient information is often expressed in a temporally discrete and thus temporally incomplete fashion, eg the record of some patient could include statements like "mild scoliosis at the age of 2 years" and "severe scoliosis at the age of 7 years" without any mention of the status of scoliosis (absent or present and of what severity) at any other points in time.
a Point and interval occurrences.
The same occurrence can be expressed with respect to different temporal granularities, thus giving it point or interval status (in a conceptual rather than real-life sense) depending on the temporal context of reference.
For example "I had flu for most of January 1995" expresses an interval occurrence at the granularity of days but a point occurrence at the granularity of months.
With respect to interval occurrences the issue of convexity, or nonconvexity [ 131, arises (convexity implies that the  1 Time representation requirements for medical problem solving Time is intrinsically relevant to medical problem solving.
Time representation requirements for medical tasks such as diagnosis, prognosis, monitoring, therapy planning, ctc.
dcmand more variety in expression and higher levels of abstraction than appears to be supported by well known, general theories of time proposed in the AI literature [1],[3],[ 121.
There are two basic issues here: how to model time per se and how to model time-varying situations or occurrences.
1.1 Model of time Real time is infinite and dense.
An abstraction of reality which models time as a single time-line (either in dense or discrete terms), a model often adopted in temporal databases and other applications, does not provide the appropriate abstraction for medical applications where a richer model providing a multidimensional structure to time, through a number of interrelated, conceptual temporal contexts, and niultQle granularities, is often required.
Examples of conceptual temporal contexts are the various developmental periods, eg fetal-period, infancy, early-childhood, etc whose  160 0-8186-7528196$5.00 0 1996 IEEE  ~  unfolding of the Occurrence entails some sort of activity throughout the particular period of time that defines its lifetime).
Compound occurrences.
Disease processes and therapeutic interventions define compound occurrences.
Compound occurrences are categorised into: periodic occurrences (eg "headache every morning for about two hours over a period of one week which worsens each day", "administration of drug x every four hours until the pain stops but not for more than two days", etc); temporal trends that describe changes and their direction and rate of change, eg "low pressure increasing slowly", which in turn indicate whether a situation is normal or whether it is converging towards, or diverging away from, normality and at what rate; and general temporal patterns, eg a sequence of meeting trends, a set of relative occurrences, a set of causally related occurrences, etc.
The modelling of compound occurrences requires mechanisms for abstraction and refinement.
Cuusulity.
Changes are explained through causal relations and hence time is directly relevant to causality.
The temporal principle underlying causality is that an effect cannot precede its cause.
unintentionally cultivate the treatment of time as an accessory.
To achieve the above, an ontology must force time to be an integral aspect of the domain entities that constitute the processing elements of the problem solver.
This the proposed ontology aims to achieve through its time-object primitive, which in addition results in the simple integration and uniform representation of structural, causal, and temporal knowledge.
Furthermore, the ontology aims to provide the necessary temporal abstraction through multiple temporal granularities.
The principal ontological classes are the time-axis and the time-object that respectively provide a model of time and a model of occurrences.
Time-axes provide conceptual contexts for the definition of time-objects and hence they result in organising time-objects in meaningful (temporal) clusters.
A time-axis is expressed at the granularity relevant to its semantics and multiple granularities are therefore appropriately modelled by confining their usage to relevant, conceptual, temporal contexts.
The notion of an (anonymous) time-axis is not original per se.
However, the notion of multiple, conceptual (ie named), interrelated, time-axes is.
Multiplicity of time-axes is necessary for the natural representation of some problem.
For example the ossification process for the first cervical vertebra of the spine begins at the second lunar month and terminates at the 25th year of age.
The initiation of this process is expressed at the granularity of lunar months and its terrnination at the granularity of years.
The time-values concerned refer to conceptual periods of time, namely fetal-period and maturity respectively.
The lifetime of the particular ossification process extends over fetalperiiod, infancy, childhood, puberty and part of maturity.
This process is temporally decomposed into a number of subprocesses.
A process and its subprocesses are modelled as time-objects.
Time-objects are dynamic entities.
The existence of some time-object could be expressed with respect to multiple temporal contexts (ie time-axes) and hence granularities, either in absolute or relative terms.
In addition, the existence of a time-object, in some context, can be expressed with a degree of vagueness.
Three important types of relations are defined for time-objects: structural, causal, and temporal relations.
The ontological classes are: Tunits,the set of time-units.
Axes, the set of discrete time-axes.
Times(a), the sequence of literal time-vulues on time-axis a. Tobjects, the set of time-objects.
Tobjects(a) (cTobjects), the set of time-objects that have a valid existence on time-axis a. Pobjects(a) (G Tobjects(a)), the set of time-objects that have a point existence on time-axis a. Zobjects(a) (E Tobjects(a)), the set of time-objects that have an interval existence on time-axis a.
2 Ontological primitives The relative simplicity and crimness of the Drimitives and reasoning of most widely adopted general t'heories of time which in fact justify the wide interest in them, unfortunately do not render their expressive power sufficient enough for a number of real-life medical problems where happenings are not as simple and as orderly as someone walking on a street or getting a promotion.
Such theories do not provide an adequate level of abstraction for knowledge engineering purposes in that their primitives, often the time-point, the timeinterval, or the event, are too 'primitive' and not at a knowledge level from the perspective of modelling complicated, dynamic, domain concepts such as disease processes or therapeutic interventions.
Another indirect limitation is based on the fact that still in many knowledge-based problem solvers temporal reasoning is seen as an accessory rather than as an integral aspect of the problem solver's reasoning.
For example the temporal reasoner could be a background process demoted to the role of preprocessing data (from some temporal database) in order to select or derive the information to be fed to the problem solver; the problem solver, therefore, is not reasoning with time or change in any conscious way.
An ontology of time that in a sense 'forces' time to be treated as an integral aspect of the particular problem solving and hence for temporal reasoning to constitute a very conscious activity on the part of the problem solver has significant advantages over an ontology that does not and which may even  161  0  Props, the set ofproperties.
Props(ji) (cProps), the set of properties of relevance to time-unit p,  certainty.
Thus the duration of a non-closed existence of some time-object can only be shortened, If z does not have a valid existence in the context of time-axis a,then &,(a)= 1.
Hence a time-object can exist as a point-object on some time-axis but as an interval-object on another timeaxis.
In the former case the extent of the time-object is less that the time-unit of the particular time-axis.
A special moving time-object is now which exists as a point-object on any relevant time-axis and functions to partition time-objects into past, hture, or ongoing.
A time-object whose existence is expressed with respect to an abstractkoncrete time-axis is an abstract (generic)/concretetime-object.
Regarding temporal relations between time-objects, Allen's set [l] has been adapted and extended to fit the discrete, multidimensional, and multigranular, model of time.
More specifically a time-axis constitutes an argument of these relations, instances of which may be derived from absolute existence expressions; furthermore new relations based on temporal distance are added and usehl disjunctions are directly defined.
The structural relations between time-objects are isa-component - of, and its inverse contains, and variant-component, and its inverse v a r i a n t - contains; the latter two express conditional containment.
2.1 Time-units and time-axes Time-units define the possible granularities: scale(p,p 'A) gives the scale relation between granularities p and p'.
A time-axis, a, is expressed discretely at a specific granularity, p, in terms of a sequence of timevalues, Times&) = {t,,t, ,..,ti,..,f } , given with respect to the origin of the time-axis.
The origin of an abstract time-axis denotes a generic time-point; when an abstract time-axis is instantiated its origin gets bound to a real time-point.
Hence a concrete time-axis is 'linked' to the universal, real-time, axis.
The basic relation between time-axes is t-link(a,t,a ',t 7 that links a lime-value on one time-axis with a time-value on another time-axis.
Other relations, eg concurrent, includes, intersects-with, etc can be expressed through t-link.
Some of the time-axes are defined as spanning a chaining sequence of other timeaxes.
A spanning time-axis has a hybrid granularity inherited from its components.
For example lifetime could be a spanning time-axis encompassing fetal-period, infancy, etc.
2.2 Time-objects  contains(z ,T ) 1 J C= v a r i a n t - contains(z.,z.,Cs) A  A time-object, z, is a dynamic entity for which time constitutes an integral aspect.
It is viewed as a tight coupling between a propert?, and an existence, where its existence can be expressed with respect to different time-axes.
Its existence with respect to the most appropriate time-axis for it, is called the time-object's main existence.
Thus z = <xT,sT> where T C ~E Props is the property of z (n is a selector function, and the notation TC, stands for function TC applied to argument z), and hnction E~ : Axes -+ Eexps is the existence function of z.
The domain of E, is the set of time-axes and its range is the set of absolute existence expressions.
An absolute existence expression gives the (earliest) initiation and (latest) termination of some existence.
If time-object, 5 , has a valid existence on some time-axis, a, then E,(OI) = < t , , t p ; t,, tf E Times(a); t, I tf; and the status E {closed, open, o p e n - f r o m - l e f t , open-from-right, moving}.
If there is openness in some valid existence of a time-object then its actual initiation and/or termination is not known but approximated through an admissibility margin.
The initiation and termination admissibility  conds-hold(cs) I J A variant component can only be assumed, in some situation (eg a specific patient), if the specified conditions are satisfied.
A compound time-object has a valid existence under any context in which at least one of its components has a valid existence, and a time-object exists within the one that contains it.
Temporal views of a compound time-object, from the perspective of specific temporal contexts, can thus be defined.
Finally, relation causes between a pair of (abstract) time-objects zi and z. specifies various constraints (temporal and other) that need to be satisfied in order for a causality-l i n k to be established to hold between a pair of concrete instances of zi and zj.
A generic constraint, that always needs to be satisfied, is that a potential effect cannot precede its potential cause.
c a u s a l i t y - link(Z.,t.,Cf) I J c causes(zi,zj,Cs,Cf) A conds-hold(cs) A 7s t a r t s -before(?.
Z.)
J' 1 The fourth argument of relation causes denotes a  margins are given by <t ,t > and < tf ,tf > respectively where t,.
= le-fr (a)and %"=ri-fr,(a) (functions le-fr and  certainty factor, ie even if all specified conditions are  satisfied, in some situation, still it may not be definite that the particular c a u s a l i t y - l i n k actually exists in that situation.
The uncertainty is due to knowledge incompleteness.
Trends and periodic occurrences are modelled as compound time-objects, subsuming a number of other time-objects at a lower data and/or temporal level.
A generic periodic time-object is defined through a  ri-fr respectively stand for left-freedom and rightfreedom).
The existence of an open time-object, on a given time-axis, is therefore defined through an intial period of uncertainty, an in between period of certainty and a final period of uncertainty.
If the earliest termination of an open time-object precedes or coincides with its latest initiation then there is no period of  162  repetition element, a repetition pattern, and a progression pattern.
Some of the axioms are treated as deductive rules (eg the causality axioms) and others as integrity constraints (eg the containment axioms), a distinction adopted from [ 151.
A dletailed comparison between the proposed ontology and three widely adopted time ontologies, namely Allen's interval-based temporal logic [I], Kowalski and Sergot's event calculus [12], and Dean and McDermott's time token maps [3] is given in [7].
2.3 Properties Properties (ontology class Props), that constitute the other half of time-objects, are atomic or compound (negations, disjunctions, or conjunctions), passive or active, and some are time-invariant.
Examples of properties are "sex male", "sore throat", "severe coughing", etc.
A property is associated with relevant granularities, eg "headache present" is associated with hours and days, but not months or years.
This way the time-axes meaningful to a property or the subset of properties that can be instantiated in the context of some time-axis, a,AxisP(a), can be defined.
A property, p, either has an infinite persistence, ir&er(p), or a finite persistence, jnper(p).
In the latter case the following are additionally specified: whether the property can reoccur (multiple instantiations of the given property in the same context are possible); maximum and minimum durations (max-dur, min-dur) under any of the relevant granularities, where the default is any duration; and a default admissibility margin for the initiation of any instantiation of the property, under a specific relevant temporal context (earliest-init, latest-init), which if not specified is assumed to be the entire extent of the particular time-axis.
In addition, the proposed ontology adopts the semantnc attributes of properties specified by Shoham, eg downward hereditary, upward hereditary, solid, gestalt, etc [19].
Causality, with explicit temporal constraints, is specified at the level of properties as well, through relation cause - spec which is a 6-place relation where the first two arguments are properties, the third is a granularity, the fourth and fifth are sets of relative and absolute temporal constraints respectively, and the last argument is a certainty factor.
This relation also enables the derivation of the existence of a causality-link between a pair o f (concrete) time-objects through the following axiom.
causality-link(Z.,Z.,Cf) ' J C= cause-spec(pi,pj,p,TRel,Css,cf)A '7t(ti) = p. A '7t(Z.)
= p. A satisfied(Ti,z.,piTRel3 A satisfied(zi,zj,p,Css)A  3 Medical problem solver with integral temporal reasoning 'flie ontological primitives and their associated axioms form the ground layer of the embedded temporal kernel.
This is the layer that provides the interface with the (medical) system's knowledge-base and (patient) datalbase.
The other layers of the temporal kernel provide various temporal reasoning fimctionalities.
3.1 Temporal reasoning requirements 'The required temporal reasoning functionalities are classified, in ascending order of level, into mapping and clipping, derivation, and consistency and querying, functionalities.
These are listed below: Mapping and clipping: -- Determining bounds for absolute existences of occurrences, ie determining admissibility margins for initiations and terminations of occurrences.
-- Mapping occurrences across temporal contexts.
-- Detecting direct conJicts and clipping persistences.
Derivation: There are two types of derivations; deriving new occurrences, or deriving new relations between occurrences.
-- Deriving new occurrences through merging (see section 3.3).
-- Deriving new occurrences through decomposition.
The potential components of compound occurrences are inferred.
-- Deriving new occurrences through temporal data abstraction: a. Persistence derivation (see section 3.3).
b. Deriving temporal trends.
c. Deriving periodic occurrences.
d. Deriving potential clusters of related occurrences.
This is the opposite of the decomposition derivation.
-- Deriving causal antecedentskonsequents of some occurrence.
-- Deriving a relation between a pair of occurrences.
The types of relations are temporal, structural, and causality links.
Consistency and querying:  -starts -betJore(zj,zi)  Other property relations include exclusion, necessitation, etc.
In summary the ontological primitives provide the necessary conceptual abstraction for the adequate modelling of medical concepts (disorders, patient data, therapeutic-actions) [6].
Medical knowledge-bases and patient databases are viewed as collections of abstract and concrete time-objects respectively (and their relevant time-axes) and hence time becomes an integral aspect of the particular applications.
All the ontological classes and their associated axioms are discussed in detail in [7].
--  Estahlishing the overall consistency of some world  of occurrences.
163  occurrences.
A (hypothesised) occurrence, of any degree of complexity is queried against some world.
- Deriving the state of some world at a particular rime.
In an explorative mode, the problem solver may need to be informed about what is believed to be true, at some specific time, in some specific context.
The query may be expressed relative to another point in time which defaults to now, eg at time point t, what was/is/will be believed to be true during some specified period p?
Below we give algorithm for some of the mapping and clipping, and derivation functionalities.
- Querying  a refers to the main existence o f t , and returns a list of direct conflicts.
detect-conflicts obsaxs let conflicts be initialised to the empty list for each (qa)E obsaxs do for each (z ',a')that follows (t,a)in obsaxs do if ITT excludes E , , then let a' be the finest granularity time-axis between U and a' if &$a*) # IA &,.
(a* )#I then If 1 dis jo i n t(a*,T,T ') then enter ( t , z ' , a ' ) into conflicts else let a*be the finest granularity time-axis amongst the remaining relevant time-axes on which both z and z ' have a valid existence if there is such a time-axis and  3.2 Mapping and clipping functionalities First we discuss the mapping of time-objects across time-axes.
In order to map a time-object, z, from timeaxis, a, to time-axis, a',the following conditions must be satisfied: e The property of t, IT,: can be instantiated in the context of time-axis a', le K , E AxisP(a').
e The scale relation between p and p', the respective time-units for a and a', is known.
A linkage between the two time-axes is specified.
This means that an instance of relation t-link involving the relevant pair of time-axes is designated as defining the (basic) linkage between the two.
If any of these conditions is not satisfied then E,(u')= 1.A basic function of the mapping operation, map-Val, is to map a time-value, t E Times(@, from a to a' [7].
If a particular time-value cannot be mapped then map-val returns 1.
The mapping operation essentially involves mapping the base (t,) and limit (tf>time-values of t onto a'.
Further, if the existence of t on U entails uncertainty, then time-values le-frT(a) and ri-frT(a), if applicable, are also mapped.
Overall, there are four cases of mapping: mapping an interval-object to a coarser/finer granularity time-axis; and mapping a point-object to a coarsedfiner granularity time-axis.
Information is lost when the existence of a time-object is mapped onto a coarser granularity time-axis.
It is therefore quite possible that map(map(&,(a),a'),a) # &,(U).
Since every time-object has a main existence, to avoid the above problem, where possible, mappings should be from the main existence.
Next we discuss direct conflict detection and clipping of persistence.
Two time-objects are implicated in a direct conflict, if their existences are not disjoint and their properties are mutually exclusive; they are implicated in an indirect conflict if there is a direct conflict between one of these and a derivative of the other time-object.
A conflict is denoted by a triple ( t , z ' , a ) that gives the pair of time-objects andVhe time-axis in whose context the conflict is established.
Once a conflict is established, the next step is to try and resolve it.
Algorithm .fir detecting direct conflicts: The algorithm where accepts a list of time-object, time-axis pairs, (t,a),  disjoint(d,T,t')  then enter (z,t ',a*) into conflicts end for end for return conflicts end detect-conflicts Algorithm .for resolving Conflicts by clipping persistences: The algorithm uses auxiliary functions apparent-overlap, terminating-relation, and clip-persistence.
Predicate function apparent-overlap accepts a conflict and returns true if the relevant overlap could be apparent.
It is based on the heuristic that two time-objects which are disjoint under some granularity may appear as coinciding (eg if both of them are mapped onto the same point existence) or chaining, under a coarser granularity.
Function terminating-relation accepts a conflict, the relevant domain of time-objects, and an operation mode, reactive (meaning that (therapeutic) actions are only instigated in order to combat some established abnormality, and hence such actions could not have been started prior to what they aimed to terminate), or proactive (where therapeutic actions could also be instigated in order to prevent an anticipated, future, undesired happening; such actions would normally commence prior to what they aim to prevent).
The function returns a triple of values, where either all three values are equal to nil or they denote three time-objects (t. z.,~'), meaning that z' causes z. in order to terminate ti J (t:;nk-object Z' therefore denotes a terminating action for zi); time-objects zI and z. are the two specified in the parameter ofthe function,Jbut not necessarily in the given order.
terminating-relation (z,z',a)tobs mode if the mode is proactive or the initiations of z and t' on a coincide then the following reasoning is repeated for either combination of the two time-objects, ie ( t , , .
~is~set ) both to (z,z') and to (z',t) elsif t begins before z ' on U then zI is set to t and zj is set to z'  164  else ti is set to t' and t. is set to t J end if let relact E tobs such that the property for each of the selected time-objects denotes a potential terminating action for the property of zi repeat for each t' E relact if a c a u s a l i t y - l i n k can be established from t' to t. J and 2' does not begin before tion U (if the reasoning is done in a reactive mode), then 't is the rellevant action until a relevant action is found if a relevant action, t', is found then retum (ti,tj,t*)else return (nilpilpil) end terminating-relation Finally function clip-persistence accepts a pair of time-objects (t,t') and modifies the main existence of z to the minimum necessary so that this is disjoint from the existence of t'.
If in order to do so the whole existence of t is revoked or its duration drops below a specified minimum for the relevant granularity then the hnction returns false else it returns true.
The main algorithm accepts a list of conflicts, the relevant domain of time-objects, and an operation mode (reactive or proactive).
It returns a triple (appoverlaps, unresolvedc, revokedobs) where appoverlaps gives the conflicts that it has not investigated because the relevant overlap may well be apparent, unresolvedc gives the conflicts that it has not managed to resolve, and revokedobs gives the time-objects whose existence is revoked in order to resolve the conflict.
and cl is the cluster of existing time-objects subsumed by it end merging-deriv The relevant subset of time-objects is defined as: relobs(p,u) = { z I t E Tobjects(a); x a p}.
Thus the set of relevant periods of time on a, f(p,a), is given by: I(p,u) = {&,(U)1 t E relobs(p,a)}.
Let int c I(p,u); int conistitutes a conjoined chain iff its elements collectively cover all time-values included in the minimal period of time that spans all elements of int: con-ioined-chain(int) e t, t E extent(int) (3 t p E int such that t is included in tp) where extent(int) is the sequence of time-values  'p'  tb = min,< t, I <tet,,tf > ,< > E int } ,t = max, { tf I < t,,tf ,< > E int } p is the granularity of a Let int 5: Int 5: I(p,u); int constitutes a maximal conjoined chain in Int, iff it is a coinjoined chain and no other subset of Int constitutes a conjoined chain that strictly includes int's chain.
mar;-conj-chain(int,Int) e (conjoined-chain(int) A 1(3 int' Int such that (conjoined-chain(int') A extent(int) c extent(int')))) Let int c I(p,u), where conjoined-chain(int).
The minimal existence that spans all elements of int is given by function chain: cham(int) = <ts,,tf ,p where t,.
= min, {t I <ts-> E int} tf = maxp (if I <-,t+> E int} c1= (<t,.,closed> E int) v (<t,.-,open- from-right> E int) c2 = (<-,tf .,closed> E int) v (<-,t, ,,open-from-left> E int) q = closed, if c1 A c2 ,C = open- f rom-right, if C1 A l C2 <=open-from-left,if-c, A C ~ 5 = o p e n , i f l ~A ~ ~ C ~ Let int c I(p,a).
The minimum set of maximal existences that collectively cover all elements of int is given by function merge: merge(int) = {chain(i) I i E power(int); max-conjchain(i,int)} The refined algorithm is now given.
Algorithmfor deriving time-objects lhrough merging merging-deriv (p,u) let result be initialised to the empty list if p is a concatenable property then do: let relobs = (z I t E Tobjects(u); x, a p} int = (eT(u)I t E relobs} maxexist = merge(int) repeat for each maxe E maxexist let ts = { t I z E relobs ; I  3.3 Derivation functionalities Derivation operations aim to derive, from the specified world of occurrences, new occurrences or new relations between occurrences.
A derivation is nondirected, if only the type is specified, or directed if a derivation pattern or template is specified which is matched against the particular world.
First we discuss the case of a semidirected merging derivation where the property concerned, p, and the time-axis, U,are specified.
The property p is assumed to be concatenable [19], and it may well be a compound property such as a disjunction of related properties, eg p mild-coughinq V moderate-coughing coughing.
The algorithm is outlined below:  V  I  severe-  merging-deriv ( p a ) do the following: 1. select the relevant subset of time-objects from the particular world, relobs(p,a) 2. partition relobs into clusters that constitute maximal, conjoined, chains 3. for each cluster, derive the time-object that subsumes it end return the list of associations (z,cl) where z is a newly derived time-object (xT= p)  165  extent {.$a)) c extent {maxe}} if 3 (maxe',zs) E result then replace this entry of result with the new entry (chain {maxe, maxe'}, zs) else enter the association (maxe,zs) into result end repeat return result end merging-deriv As a side point we define strong and weak chains of time-objects.
Let zs G relobs(p,a), int = {&,(a)1 z E zs) E I(p,a), such that conjoined-chain(int).
A conjoined chain formed by the existences of a set of time-objects is considered strong if the chain is not likely to break due to potential uncertainties regarding its elements: strong-chain(ts) e conjoined-chain(int ') where int' = {< le-frT(a)ji-frT(a),ciosed> I z E zs; le-fr,(a) 5 ri-fr,(a)) Thus, weak-chain(zs) = 7 strong-chain(zs) Next we discuss one form of temporal data abstraction, persistence derivation.
The problem of persistence derivation is explained through an example.
Suppose there is some patient whose record includes two radiographs of the spine, one taken at the age of 2 years and the other at the age of 4 years.
Both radiographs show the presence of abnormality "kyphoscoliosis".
These observations correspond to two point-objects at the granularity of years.
The problem is how to see beyond these two discrete sightings, both forwards and backwards in time, eg what can be inferred about the existence of this condition with regard to the particular patient, before the age of 2 years, between the ages of 2 and 4 years, and after the age of 4 years?
Disorder expectations usually refer to interval occurrences and hence persistence derivation is a necessary fbnctionality for the proper matching of a disorder profile against a patient profile.
The algorithm given below operates in a nondirected fashion; it accepts a time-axis, a, and a collection of time-objects, which have the same property, and exist as point-objects under 01, and returns the interval-objects that represent distinct, maximal occurrences of the particular property, under the given temporal context.
Algorithmfor persistence derivation derive-persistence a tobs let p be the time-unit for cx p be the property shared by the elements of tobs z, ,z2,...,z,, ..., t, be the elements of tobs in ascending temporal order t,,t, ,..., t,, .,..,t, E Times(a) be the respective positions of t,.t2...,zi ,..,z, on cc iobs be initialised to the empty set if p is an infinitely persistent property ;;case 1 then enter into iobs a sin$le interval-object with tf = upper-time(a) ;; termiriation t, = earliest-init(p,a) ;;earliest initiation ts.
= minp{latest-init(p,a),t,)};;lolest initiation  if t, > ts.
then a conflict is noted elsif p IS a finitely persistent, but not a reoccurring, property ;;case 2 then enter into iobs a single interval-object with t, = maxiearliest-init(p,a),(t, -I.I max-dur(p,a))} t,.
= minit,, t, -I.I mindur(p,a),latest-init(p,a)} tf = max{(t, +p min-dur(p,a)), b} t,, = (ts.
+p max-dur(p,a)) ;;earliest termination if ts > t,.
or tf , > tf then a conflict is noted else ;; p is a finitely persisting, reoccurring, ;;property (case 3 ) let uncovered-obs be initialised to the temporal sequence z, ,z2,...,zi,..,z, repeat 1. select the longest initial segment of uncovered-obs that constitutes a nonconflicting instance of case 2 2. enter the interval-object that corresponds to the selected segment into iobs 3. remove selected segment from uncovered-obs until uncovered-obs is the empty sequence return iobs end derive-persistence The clustering of the point-objects performed under case 3 determines significant temporal distances (on the basis of the property concerned) between neighbouring objects which sign@ separate incidents.
The derivation of temporal trends is based on a modification of the above algorithm [7].
4 Validation The original motivation for the ontology came mainly through the SDD system, a diagnostic expert system for the domain of skeletal dysplasias [l 11, although many of the temporal requirements identified for medical problem solving were in parallel detected in the context of other research projects whose scope was the modelling of biochemical and other industrial processes [4],[6].
In all these applications the relevance of temporal notions such as periodicity, trends, delays, prematurity, etc, was quite evident and hence of the notions of temporal distance and absoluteness, as well as the need for different granularities and conceptual temporal contexts.
In addition, through such practical work, it was appreciated that the ontological primitives had to be at a high level of abstraction (knowledge level) to adequately support the knowledge engineering of complex dynamic processes.
The original ideas, which were demonstrated through SDD's temporal reasoner [lo], have been considerably enhanced and consolidated in the context of the CEC AIM Project GAMES I1 whose objective was to develop a theoretical framework and practical tools for the development of medical knowledge-based systems.
The proposed ontology and temporal reasoning mechanisms constitute part of the overall GAMES I1 product [20].
166  Furthermore, the ontology has been applied in building a simple prototype system modelling the normal ossification processes for parts of the human skeleton, and has been utilised in formulating a temporal model for medical diagnostic reasoning [9].
The plan for the immediate future is to filly implement the ontology and associated temporal reasoning algorithms in the form of an efficient and reusable temporal kernel for knowledgebased problem solvers, primarily for medical applications, but for other applications as well.
Time", Artjlkial Intelligence, Vo1.23, pp.123-154, 1984.
L. Chittaro, M. Del Rosso, and M. Dojat M., "Modeling Medical Reasoning with the Event Calculus: an application to the management of mechanical ventilation", Proc AIME 95, Lecture Notes in Artificial Intelligence, Vol.
935, Springer, pp.79-90, 1995.
T. Dean and D. McDermott, "Temporal Data Base Management", Artificial Intelligence, Vo1.32, pp.
1-55, 1987.
G.A.
Dervakos, J.M.
Woodley, E.T.
Keravnou, J. Washbrook and M.D.
Lilly, "Development of a KBS for Biotransformation Process Design, Proc.
I.CHEME.E.
Symposium Series N0.114, pp.283-291, 1989.
LJ.
l-iaimowitz and LS.
Kohane, "Managing Temporal Worlds for Medical Trend Diagnosis", to appear in [8].
E.T.
Keravnou, "Modelling Medical Concepts as TimeObjects", Proc AIME 95, Lecture Notes in Artificial Intelligence, Vol.
935, Springer, pp.67-78, 1995.
E.T.
Keravnou, "An Ontology of Time Using Time-Axes and Time-Objects as Primitives", Technical Report, Department of Computer Science, University of Cyprus, 1995.
E.T.
Keravnou (ed.
), Temporal Reasoning in Medicine, special issue of ArtiJicial Intelligence in Medicine, Vo1.8, No.3, June 1996.
E.T.
Keravnou E.T., "Temporal Diagnostic Reasoning Based on Time-Objects", to appear in [8] E.T.
Keravnou, J. Washbrook, "A Temporal Reasoning Framework Used in the Diagnosis of Skeletal Dysplasias", Artificial Intelligence in Medicine, V01.2, NOS, pp.239-265, 1990.
E.T.
Keravnou, F. Dams, J. Washbrook, C.M.
Hall, R.M.
Dawood and D. Shaw, "Modelling Diagnostic Skills in the Domain of Skeletal Dysplasias", Computer Methods and Programs in Biomedicine, Vo1.45, pp.239-260, 1994.
R. Kowalski and M. Sergot, "A Logic-Based Calculus of Events", New Generaiion Comput, V01.4, pp.67-95, 1986.
P. Ladkin, "Primitives and Units for Time Specification", Proc AAAI-86, pp.354-359, 1986.
W. Long, "Temporal Reasoning for Diagnosis in a Causal Probabilistic Knowledge Base", to appear in [8].
J. Mylopoulos, A. Borgida, M. Jarke and M. Koubarakis, "Telos: A Language for Representing Knowledge About Information Systems (Revised)", Technical Report KRRTR-89-1, Department of Computer Science, University of Toronto, 1990.
A. Riva and R. Bellazzi R., "Learning Temporal Probabilistic Causal Models from Longitudinal Data", to appear in [SI.
B. Rosser, J. Washbrook, J. Campbell, E.T.
Keravnou and D. Long, "A Framework for Time Dependent Reasoning Systems", Product PI 1 1-1, Esprit Project P2409 (EQUATOR), 1989.
Y. Shahar and M.A.
Musen, "Knowledge-Based Temporal Abstraction in Clinical Domains", to appear in  5 Conclusions Time is essential in problem solving.
With regard to medical problem solving time is intrinsically relevant to all medical concepts [6] (disorders, patient data, therapeutic interventions) and hence temporal reasoning ought to constitute an integral aspect of the overall problem solving.
Many of the general theories of time proposed in the AI literature [1],[3],[12] focus on natural language processing, or a specific class of problem solving tasks, usually planning or monitoring, or the management of temporal databases.
Some of these general theories have been effectively adapted in the context of specific medical applications [2].
By and large the work done in temporal reasoning for medical applications focuses on temporal data abstraction and temporal causality [5],[ 14],[ 16],[ 181.
The overall aim of the research presented in this paper is to develop an ontology of time, and associated temporal reasoning, based on a global analysis of time representation requirements for different medical tasks (diagnosis, monitoring, therapy planning) [8] and to effectively implement these through a generic temporal kernel.
The ontology must provide the required level of abstraction for knowledge modelling purposes and its usage must result in treating time as an integral aspect of the application.
The chosen ontological primitives, the timeaxis and the time-object, satisfy these design objectives and in addition the notion of a time-object integrates in a uniform and natural way temporal, structural, and causal knowledge.
However, the high power of expression provided by the proposed ontology is associated with relevant computational overheads.
Acknowledgements The research reported here has been partly supported by the University of Cyprus, the Leverhulme Trust (UK) (under projects SDD and MSD), and the CEC (under AIM Project A2034 GAMES 11).
The development of the ontology has benefit from discussions with other participants on these projects, in particular John Washbrook, Hauke Kindler and Mark Leaning.
In addition Kazem Sadegh-Zadeh, John Mylopoulos, and the three unknown referees have offered useful suggestions.
[W.  Y. Shoham, "Temporal Logics in A I Semantical and Ontological Considerations", Artificial Intelligence, V01.33, pp.89-104, 1987.
G. van Heijst, M. Ramoni, G. Schreiber and M. Stefanelli (eds.
), CEC AIM Project A2034 GAMES-II Final Report, 1995.
References [l]  J.F.
Allen, "Towards a General Theory of Action and  167
Deciding consistency of a point-duration network with metric constraints Isabel Navarrete Facultad de Informatica Universidad de Murcia Campus de Espinardo, Murcia 30.071, Spain inava@dif.um.es Abdul Sattar School of Information Technology, Faculty of Engineering and Information Technology Griffith University PMB 50 Gold Coast Mail Centre, Queensland 9726, Australia a.sattar@gu.edu.au Roque Marin Facultad de Informatica Universidad de Murcia Campus de Espinardo, Murcia 30.071, Spain roque@dif.um.es  Abstract  We introduce a new model, MPDN, for quantitative temporal reasoning with points and durations, that supposes an extension of the TCSP formalism and previous point-duration network models.
The problem of deciding consistency for a MPDN is shown to be NP-complete.
So, we identify a tractable fragment, named simple MPDN, that subsumes the STP model and allows for duration reasoning.
Necessary and sufficient conditions for deciding consistency of a simple MPDN are used to design an algorithm for consistency checking, whose time complexity is cubic in the number of variables.
This is a significant improvement, not only in computational complexity but also in simplicity, over previous non-specific algorithms that can be applied to solve the consistency problem.
Keywords: Temporal representation and reasoning in AI, temporal constraint reasoning, point and duration reasoning.
1  Introduction  In a wide variety of situations, problem solving tasks require a rather extensive knowledge and reasoning about time.
In most applications, knowledge of temporal constraints is expressed in terms of relations between time objects (intervals, points or durations).
Several constraint-based systems have been proposed for temporal constraint reasoning, mainly concentrated on two approaches: qualitative formalisms [1, 14] and quantitative or metric models [3].
Later efforts [6, 10] have been made on integrating information between time points and intervals in a single model.
Some researchers have presented systems that support qualitative and/or quantitative constraints between durations [1, 13, 12, 11].
The need for this kind of models is well argued in the literature.
In this paper, we present a new point-duration network model with metric constraints, MPDN, which subsumes some previous formalisms for temporal reasoning with points and/or durations [3, 11].
Since the consistency problem for these models is NP-complete, we cannot expect better complexity results for a MPDN.
So, we  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  introduce a tractable fragment, simple MPDN, which is a nice tradeoff between expressive power and time complexity.
The consistency checking algorithm we provide for a simple MPDN supposes a significant improvement over previous non-specific algorithms that can be applied to solve the consistency problem [7, 5, 13].
Example 1 To illustrate the usefulness of the new tractable fragment we adapt the example proposed in [10] and we include additional information about durations of events.
The temporal information provided in this story can be managed with a simple MPDN, as we will show later.
"Bob, Fred and John work for a company that has main office in Los Angeles.
It takes John less than 20 minutes and Fred 15-20 minutes to get to work.
Today John left home between 7:05-7:10 a.m. and Fred arrived at work between 7:50-7:55 a.m. We know that Fred and John met a traffic light on their way to work.
Today Bob left home before 7:45 a.m. and takes 5-10 minutes less than Fred to go to work".
2  Preliminaries  A metric constraint (or quantitative constraint) is represented by a set of real intervals C = {I1 , .
.
.
, Ik }.
A unary metric constraint Ci restricts the domain of variable xi to the given set of intervals, while a binary metric constraint Ci,j restricts the feasible values for the time-distance xj - xi .
A TCSP [3] is a binary network involving a set of point variables and sets of unary and binary metric constraints among them.
A special case is an STP where each constraint is given by a single interval.
We will use the metric algebra [3], that is provided with operations of set intersection, inverse (-1 ) and composition ([?])
of metric constraints.
Given two quantitative constraints C and C  , the composition C [?]
C  can be computed as the union of pair-wise sum of intervals, that is:  C [?]
C  = Ij [?
]C,Ik [?
]C  Ij + Ik = [aj , bj ] + [ak , bk ] = [aj + ak , bj + bk ] =  The inverse C -1 is obtained as C -1 Ij-1 | Ij [?]
C , where the inverse of an interval [a, b] is the interval [-b, -a].
Definition 1 A metric point-duration network, MPDN, is an structure SP D = NP , ND , Rel(P, D)fi formed by two TCSPs, NP and ND , and a set of ternary constraints Rel(P, D) relating points and durations, where: * NP is determined by a set P = {p1 , .
.
.
, pn } of time-point variables that take values over R+ 0, and a set of unary and binary metric constraints between points.
* ND is given by a set D = {dij | pi , pj [?]
P } of duration variables over R+ 0 and a set of unary and binary metric constraints between durations.
* Rel(P, D) is given by triplets of real values, (Pi , Pj , Dij ), for points and durations that satisfy the Euclidean distance equation dij = |pi - pj |, for echa duration dij .
A MPDN with n points and d durations is consistent if at least one solution S = (AP , AD ) exists, where AP is a n-tuple of pairs, where each pair < pi , Pi > denotes the assignment of a value Pi to point variable pj , and AD is a d-tuple of pairs < dij , Dij >, where a value is assigned to each duration variable, so that all unary, binary and ternary constraints are satisfied.
A constraint is feasible is there is a solution that satisfies this constraint.
The minimal MDPN [3] equivalent to a given one is represented by all feasible unary and binary constraints, what means that these constraints are as explicit as possible.
Definition 2 A simple MPDN is a MPDN such that each network NP and ND represents an STP, and for every duration dij it must be pi <= pj , that is Ci,j [?]
[0, [?]).
This way Rel(P, D) is obtained upon the linear equation dij = pj -pi for each duration dij .
In order to deal with consistency and other reasoning tasks, it is useful to represent a MPDN (or simple MPDN) by means of two directed constraint graphs, GP = (VP , EP ) and GD = (VD , ED ), that explicitly show binary constraints affecting points and durations, respectively.
One node represents a point or duration variable, that may be labelled with its unary constraint.
Each arc is labelled with the corresponding binary constraint.
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  Example 2 The temporal information given in the story of example 1 can be managed with a simple MPDN.
Let b- , b+ , f - , f + , j - , j + denote the time points that Bob, Fred and John, respectively, leave home and arrive at the office.
The constraint graphs GP and GD depicted in figure 1, show the unary and binary metric constraints between points and durations that can be extracted from the story.
All arcs are supposed to be labelled with (0, [?
]), which is equivalent to the qualitative relation < [10].
All times in Gp are relative to the "beginning of the world", chosen at 7:00 a.m. For instance, from the given information that Fred arrives at work between 7:50-7:55 a.m., the domain of f + is restricted to the time interval (50,55).
The duration of Fred going to work is also limited to (15,20).
The incomplete qualitative information that "Fred and John met at a traffic light on their way to work" can be interpreted as the IA-relation [1] {start, started-by, during, contain, finish, finished-by, overlapped, overlapped-by, equal} between the two interval events of can be represented by a conjunction of PA-relations (in this case <-relation or equivalently (0, [?]))
between the endpoints of the intervals.
(5.10) j- 6  f-  Graph Gp - j+ 3 ~ -  b-  (50,55) f-  (5.10)  j-j+  Definition 3 Let SSM P D be a simple MPDN.
The STP-P of SSM P D is a constraint graph whose nodes represent the points in NP plus the beginning point p0 , and for every pair of points pi , pj there is an arc Ti,j  i -- j, such that, * Ti,j = Ci,j [?]
Cij , if dij [?]
D * Ti,j = Cj , if i = 0 * Ti,j = Ci,j , otherwise.
Definition 4 Let SSM P D be a simple MPDN.
The STP-D of SSM P D is a constraint graph whose nodes represent the durations in ND plus the null-duration d00 , and for every pair of durations dij , dkm there is Tij,km  an arc ij -- km, such that, * Tij,km = Cij,km if ij  = 00.  ?
* Tij,km = Ckm [?]
Ck,m if ij = 00.  b-  Graph Gd b- b+  (0,45)  p0 and pi .
Similarly, a unary constraint Cij for a duration dij can be expressed as a binary constraint C00,ij between the null duration d00 and dij .
Notice that the domain (i.e., unary metric constraint) Cij for duration dij can be considered as a binary metric constraint Ci,j between points pi and pj , since dij = pj - pi .
For instance, in the graph Gp of figure 1, binary constraint between points f - and f + may be updated to Cf - ,f + = (15, 20), since Cf - f + = (15, 20).
Now we define two STPs associated to binary constraints for points and durations in a simple MPDN.
3  Tractability of the simple MPDN fragment  (15,20)  - f -f +  (0,20)  Figure 1.
Constraint graphs for example 1  A unary constraint Ci for a point pi can be turned into a binary constraint C0,i between the beginning point  The main temporal reasoning task within a temporal constraint model is determining the consistency or satisfiability of the network.
The consistency problem for either PDN and APDN point-duration formalisms is NP-complete [11] and tractable classes of these models have been identified, but they have a limited expressive power.
In this work we identify a new tractable and quite expressive fragment for handling with metric temporal information between points and durations, as we show in the next theorem.
Theorem 1 The consistency of a simple MPDN can be decided in polynomial time.
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  Proof.- It is easy to see that all the constraints in a simple MPDN can be turn into Horn constraints [7, 5], i.e., a set of disjunctions of weak linear inequalities and linear inequations, with at most one inequality per disjunction.
Hence, Koubarakis' C ONSISTENCY algorithm [7], which is polynomial, can be used in the simple MPDN fragment.
 The major drawback of Koubarakis' algorithm is its high complexity.
It is based on the application of a polynomial linear programming method with high complexity1 .
Hence, some specific and efficient method is needed for our simple MPDN fragment.
For this reason we are going to investigate under which conditions one has the certainty that a simple MPDN is consistent.
Suppose we have three pi , pj , pk points and pairs of durations in which these points are involved.
The following equations show the influence of binary constraints between points over binary constraints between durations and vice versa: dik - dij = (pk - pi ) - (pj - pi ) = pk - pj  (1)  dkj - dij = (pj - pk ) - (pj - pi ) = pi - pk  (2)  Definition 5 Upon equations (1),(2) we say that a simple MPDN satisfy PD3-conditions if and only if, [?]
dij , dik [?]
D : Tij,ik = Tj,k [?]
dij , dkj [?]
D : Tij,kj = Tk,i where each binary metric constraint corresponds to the constraint given by the STP-P or STP-D of the simple MPDN.
Suppose now we have four points pi , pj , pk , pm and pairs of durations in which these points appear.
The influence of binary constraints between points over binary constraints between durations and vice versa is shown in the following equations: pm - pj = (dkm - dij ) + (pk - pi ) (3) pk - pi = (dij - dkm ) + (pm - pj ) (4) dkm - dij = (pi - pk ) + (pm - pj ) (5) dkm - dij = (pm - pj ) - (pk - pi ) = djm - dik (6) dkm - dij = (pi - pk ) - (pj - pm ) = dki - dmj (7) 1 A linear programming problem in n variables can be solved in O(n5 log T ) arithmetic operations on numbers with O(n3 log T ) digits, where T is the maximum absolute value of the entries.
Definition 6 Upon equations (3)-(7) we say that a simple MPDN satisfy PD4-conditions if and only if, [?]
pi , pj , pk , pm [?]
P, [?]
dij , dkm [?]
D :   Tj,m [?]
Tij,km [?]
Ti,k T [?]
Tkm,ij [?]
Tj,m  i,k Tij,km [?]
Tk,i [?]
Tj,m [?]
dij , dkm , dik , djm [?]
D : Tij,km = Tik,jm [?]
dij , dkm , dki , dmj [?]
D : Tij,km = Tmj,ki We refer to PD3 and PD4-conditions altogether as PDconditions.
Theorem 2 A simple MPDN is consistent if STP-P is path consistent [9], STP-D is path consistent and PDconditions are satisfied.
Proof.- Let SSM P D =< NP , ND , Rel(P, D) > be a simple MPDN.
Satisfiability of binary constraints alone is guaranteed since both binary networks NP and ND , represented as STP networks, are path consistent and minimal and so consistent [3].
We show that ternary constraints in Rel(P, D) are also satisfied if all PD-conditions hold.
Suppose SSM P D is inconsistent, but STP-P and STP-D are path consistent.
The only source of inconsistency is due to the influence of binary constraints between points over binary constraints between durations and vice versa.
This influence has been shown in the equations (1)(7), upon which PD3 and PD4-conditions have been obtained.
Then, if SSM P D is inconsistent this is because some PD-condition is not satisfied.
For instance, if Cj,k = [-10, -8] and Cij,ik = [7, 12], the the simple MPDN is inconsistent because there is no assignment for points and durations such that ternary constraints dij = pj - pi and dik = pk - pi are satisfied.
Indeed, it must be 7 <= dik - dij <= 12, but 7 <= (pk - pi ) - (pj - pi ) <= 12, so that 7 <= pk - pj <= 12, which is not consistent with Cj,k = [-10, -8].
The inconsistency is due to Cij,ik [?]
Cj,k = [?
], so that one PD3-condition is not satisfied.
All the inconsistencies we can find in a simple MPDN with path consistent STP-graphs are due to empty intersections in PD3 or PD4 conditions.
Otherwise the network would be consistent.
In summary, conditions of theorem ??
are sufficient conditions to ensure the consistency of simple MPDN.
But necessary conditions are given by the fact that the STP-P and STP-D must be consistent and each intersection that substitutes = or [?]
in PDconditions must be non empty.
  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  4  Consistency checking  We present now a consistency algorithm CONS (figure 2) that takes as input a simple MDPN and return TRUE when the network is consistent and FALSE otherwise.
This algorithm is based on Mackworth's PC-2 algorithm [9] for achieving path consistency.
Our goal is to accomplish, if possible, path consistency in the STP-P and STP-D graphs at the same time that satisfiability of PD-conditions is guaranteed.
Every different triple of point or duration-nodes, representing paths of length two, are added to the sets QP and QD , respectively.
For every triple we check if the label (constraint) of one of the arcs changes as a product of a composition operation or as a consequence of a call to procedures PD3 (figure 3) or PD4 (figure 4), that are used to ensure the satisfiability of PD-conditions.
When a label of an arc changes, its effect must be propagated, so we use a function REL PATHS that take an arc (between point or durations) and returns a set of triples representing all paths of length two in which the arc participates.
SMPDN  1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.  repeat while QP = [?]
delete path (i, j, k) from QP ; t - Ti,k [?]
(Ti,j [?]
Tj,k ); PD3 (i, j, k, ); if Ti,k = t then Ti,k - t; if Ti,k = [?]
then exit (FALSE); QP - QP [?]
REL - PATHS (i, k, QP ); PD3 (i, j, k); if dik [?]
D then QD - QD [?]
REL - PATHS (00, ik, QD ); while QD = [?]
delete path (ij, pq, km) from QD ; t - Tij,km [?]
(Tij,pq [?]
Tpq,km ) if ij, km = 00 then if i = k then PD3 (j, i, m) else if j = m then PD3 (i, j, k) else PD4 (ij, km); if Tij,km = t then Tij,km - t; if Tij,km = [?]
then exit (FALSE); QD - QD [?]
REL - PATHS (ij, km, QD ); if ij = 00 then QP - QP [?]
REL - PATHS (k, m, QP ); if km = 00 then QP - QP [?]
REL - PATHS (i, j, QP ); if ij, km = 00 then if i = k then PD3 (j, i, m, ) else if j = m then PD3 (i, j, k) else PD4 (ij, km); until (QP = [?])
[?]
(QD = [?
]);  Figure 2.
Algorithm CONS - SMPDN (SSM P D )  1. if dji , djk [?]
D then 2. t - Tji,jk [?]
Ti,k ; if t = [?]
then exit (FALSE); 3. if Ti,k = t then 4.
Ti,k - t; QP - QP [?]
RELATED - PATHS (i, k, QP ); 5. if dik [?]
D then 6.
QD - QD [?]
RELATED - PATHS (00, ik, QD ); 7. if Tji,jk = t then 8.
Tji,jk - t; QD - QD [?]
RELATED - PATHS (ji, jk, QD ); 9. end-if;  Figure 3.
Procedure PD3 (i, j, k)  Theorem 3 The algorithm CONS - SMPDN correctly checks for consistency in a simple MPDN in O(n3 + d3 ), where n is the number of points, d is the number of durations.
Proof.- The correctness of the algorithm follows from the correctness of PC-2 algorithm for deciding consistency in the STP model [3] and necessary and sufficient conditions to ensure consistency of a simple MPDN (see theorem 2).
When the algorithm returns FALSE then the simple MPDN is inconsistent since necessary conditions for consistency are not satisfied (some constraint becomes empty).
When the algorithm returns TRUE the network is consistent, since sufficient conditions for consistency are fulfilled.
For the analysis of the time complexity we take as a reference the time complexity of PC-2 applied to an STP, which is O(n3 ) for n points [3].
In addition to the paths processed by PC-2, either in STP-P or STPD (O(n3 + d3 ) in total), here when a binary constraint Ti,j changes then at most O(d) paths are added to QD (if dij [?]
D) and when a binary constraint between durations changes then at most O(n) paths are added to QP (for constraints of the form T00,ij ).
In total, no more than O(n3 + n2 x d + d2 x n + d3 ) = O(n3 + d3 ) paths are processed.
 Since we know how to solve a simple MPDN, a backtracking algorithm could be devised for solving a MPDN following the same idea proposed for a TCSP [3], where, in addition, the network for durations must be considered now.
The consistency algorithm for a MPDN must find a consistent simple MPDN extracted from the input network.
If such a consistent subnetwork cannot be found the MPDN is inconsistent.
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.  if dik , djm [?]
D then t - Tij,km [?]
Tik,jm ; if t = [?]
then exit (FALSE); if Tij,km = t then Tij,km - t; QD - QD [?]
REL - PATHS (ij, km, QD ); if Tik,jm = t then Tik,jm - t; QD - QD [?]
REL - PATHS (ik, jm, QD ); t - Tj,m [?]
(Tij,km [?]
Ti,k ); if t = [?]
then exit (FALSE); if Tj,m = t then Tj,m - t; QP - QP [?]
REL - PATHS (j, m, QP ); if djm [?]
D then QD - QD [?]
REL - PATHS (00, jm, QD ); t - Ti,k [?]
(Tkm,ij [?]
Tj,m ); if t = [?]
then exit (FALSE); if Ti,k = t then Ti,k - t; QP - QP [?]
REL - PATHS (i, k, QP ); if dik [?]
D then QD - QD [?]
REL - PATHS (00, ik, QD ); t - Tij,km [?]
(Tk,i [?]
Tj,m ); if t = [?]
then exit (FALSE); if Tij,km = t then Tij,km - t; QD - QD [?]
REL - PATHS (ij, km, QD ); end-if;  the office.
* Bob said he arrived at 7:40.
John was there and his boss was lying on the floor.
* Fred said he arrived at 7:53 and, apart from the dead body, Bob and John were there with angry faces".
What can be said about these declarations above?
Applying our algorithm CONS - SMPDN to the corresponding simple MPDN (see example 2) give rise to the minimal STP-D depicted in figure 5.
Part of the minimal STP-P that is relevant at this point is also shown in the figure.
Fred certainly was not at the office at 7:25, when the crime took place, but John and Bob could have been there.
Bob and John's declarations are inconsistent, so one of them may be the murder.
The inspector has found out (with the help of the minimal STP-P) that it was impossible that John arrived at the office at 7:35 a.m. John lies, but is he the murder?
This is another question.
STP-P  Figure 4.
Procedure PD4 (ij, km)  (5,30) Theorem 4 The consistency problem for a metric point-duration network is NP-complete.
Proof.- A non-deterministic algorithm can check for consistency in a MPDN in polynomial time, using the algorithm CONS - SMPDN with each simple subnetwork extracted from the MPDN.
So, solving a MPDN is NP.
Moreover, it is NP-complete because deciding consistency of a TCSP is NP-complete [3] and this model can be considered as a special case of a MPDN with no duration variables.
 Example 3 The story of example 1 continues... "Bob, Fred and John's boss has been found murdered this morning at office.
The police inspector has been asking some questions and he has found out that:  (5,10) -j -  p0 (0,45)  (5,60)    b-  w -  (5,15)  b+  z + j :  (0,20)  (50,55) zf + (30,40) j  f-  1  (15,20)  - +  d00  (5, 10) (5,15) 1b b q (-15,15) - + (15,20) -f f  (0,20) s  ?
  j-j+  (-20,5)  Minimal STP-D  * A neighbor heard a shot gun at 7:25 a.m. * John declared to arrive at the office at 7:35 a.m. His boss was already dead and Bob also was at  Figure 5.
Minimal STP-D and STP-P  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  5  Discussion  Two point-duration network based models has been previously identified [11], PDN and APDN.
The first one consists on two point algebra (PA) networks [14] related by ternary constraint as in Rel(P, D) and the second one is given by two augmented PA networks [10] with ternary constraints.
The APDN model is an extension of a TCSP which allows for duration reasoning [11].
Here we further extend the APDN since point and duration variables are now related by metric constrains, and every qualitative relation in the PA can be turned into a metric constraint [10].
Notice that each duration variable dij in the MPDN model represents the elapsed time between two temporal points pi and pj , but it does not supposes anything about the relative position of these points.
This is because we use the Euclidean distance dij = |pi - pj | to relate points with durations.
It is worth noting that the these constraints cannot be represented with disjunctive linear relations (DLRs) [5, 7], which is known to be a very expressive formalism to deal with temporal constraints.
The tractable fragment, simple MPDN, we identify here is not very restrictive since condition pi <= pj for a duration dij is not so strong.
This is similar to say, in the interval algebra context [1], that the start point of a time interval is before its end point.
A simple MPDN can be represented as a pre-convex PIDN [13].
In this fragment of the PIDN model, checking for consistency requires a 4-consistency algorithm which is at least O(t4 ) [2], being t the number of temporal objects (points or intervals).
New composition, inverse and intersection operations must be used with de consistency algorithm, which is not shown in [13].
Our consistency checking algorithm for a simple MPDN is an easy path consistency like algorithm and it improves the complexity of the algorithm for the pre-convex PIDN fragment by at least a linear factor- a significant speed up.
This is because, in order to represent p points and d durations in a pre-convex PIDN, one needs O(max(p / 2, d)) objects, so that when d > p / 2 our algorithm is always better and when d <= p / 2 our algorithm is better for p > 4.
6  Conclusion  We have introduced a new model, MPDN, for metric temporal reasoning with points and durations, that supposes an extension of the TCSP formalism [3] and previous point-duration network models [11].
The problem of deciding consistency for a MPDN is shown to be NP-complete.
We have identified a tractable fragment of the MPDN model which reflects a good tradeoff between expressive power and complexity.
This class, simple MPDN, allows representing points and durations as temporal objects and can manage with simple metric constraints.
We have investigated necessary and sufficient conditions to ensure the consistency of a simple MPDN and, using these conditions, a constraint propagation algorithm for consistency checking has been designed.
With our consistency algorithm, non-binary constraints (e.g., ternary relations and 4-ary implicit constraints among points involved in a binary constraint between duration) are managed with a formalism that integrates two well studied binary constraint networks.
The time complexity of the algorithm is cubic in the number of variables what supposes a nice improvement, not only in computational complexity, but also in simplicity, over previous non-specific algorithms that can be applied to solve the consistency problem.
An easy extension to the simple MPDN fragment is to cope with binary constraints expressed as intervals with holes, so that we can obtain algorithms for consistency checking and finding the minimal network following the ideas presented in [4, 8].
We are also working in an algorithm for finding a solution to a simple MPDN that it may be useful for several applications, specially for those where the STP fragment [3] has been used.
Acknowledgments  This work has been partially supported by the Spanish MCyT under projects TIC2001-4936-E and TIC2000-0873-C02-02.
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  References [1] J. Allen.
Maintaining knowledge about temporal intervals.
Communications of the ACM, 26(11):832-843, 1983.
[2] M. Cooper.
An optimal k-consistency algorithm.
Artificial Intelligence, 41:89-95, 1989/90.
[3] R. Dechter, I. Meiri, and J. Pearl.
Temporal constraint networks.
Artificial Intelligence, 49:61- 95, 1991.
[13] A. Pujari and A. Sattar.
A new framework for reasoning about points, intervals and durations.
In Proceedings of IJCAI-99, pages 1259-1264, 1999.
[14] M. Vilain and H. Kautz.
Constraint propagation algorithms for temporal reasoning.
In Proceedings of the National Conference on Artificial Intelligence (AAAI-86), pages 377-382, 1986.
[4] A. Gerevini and M. Cristani.
On finding a solution in temporal constraint satisfaction problems.
In Proceedings of IJCAI-97, pages 1460-1465, 1997.
[5] P. Jonsson and C. Backstrom.
A unifying approach to temporal constraint reasoning.
Artificial Intelligence, 102(1):143-155, 1998.
[6] H. Kautz and P. Ladkin.
Integrating metric and qualitative temporal reasoning.
In Proceedings of AAAI-91, pages 241-246, 1991.
[7] M. Koubarakis.
Tractable disjunctions of linear constraints.
Lecture Notes in Computer Science, 1118:297-307, 1996.
[8] M. Koubarakis.
From local to global consistency in temporal constraint networks.
Theoretical Computer Science, 173(1):89-112, 1997.
[9] A. Mackworth and E. Freuder.
The complexity of some polynomial network consistency algorithms for constraint satisfaction problems.
Artificial Intelligence, 25:65-74, 1985.
[10] I. Meiri.
Combining qualitative and quantitative constraints in temporal reasoning.
Artificial Intelligence, 87:343-385, 1996.
[11] I. Navarrete, A. Sattar, R. Wetprasit, and R. Marin.
On point-duration networks for temporal reasoning.
Artificial Intelligence, 140:39-70, 2002.
[12] A. Pujari, G. Kumari, and A. Sattar.
Induqualitative-interval-duration network.
In Australian Joint Conference on Artificial Intelligence, pages 291-303, 1999.
Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE
2013 20th International Symposium on Temporal Representation and Reasoning  Event Algebra for Transition Systems Composition Application to Timed Automata Elie FARES  Jean-Paul BODEVEIX  Mamoun FILALI  IRIT, UniversiteE de Toulouse  IRIT, UniversiteE de Toulouse  IRIT, UniversiteE de Toulouse  AbstractaFormal specidZcation languages have a lot of notions in common.
They all introduce entities usually called processes, offer similar operators, and most importantly dedZne their operational semantics based on labeled transition systems (LTS).
However, each language dedZnes specidZc synchronizing and/or memory structures.
For instance, in CSP, the synchronization is dedZned between identical events, while in CCS and in synchronization vectors-based views it is dedZned respectively between complementary events or between possibly different events.
In this paper, we aim at capturing some similarities of specidZcation languages by dedZning a label-based composition formal framework.
Firstly, we dedZne a high-level synchronization mechanism in the form of an abstract label structure.
We then couple this label structure with several compositional operations and properties.
Secondly, we introduce an LTS-based behavioral framework and dedZne a unique LTS composition operator which is reused to dedZne syntactic composition of extended transition systems and a compositional semantics.
high-level synchronization mechanism in the form of a label structure.
The label structure is abstract enough [4] so that both homogeneous and heterogeneous system synchronization could be described.
It is equipped with a composition operator which encapsulates the specidZc composition laws of each language and would further serve as a parameter of the behavioral framework.
Thanks to the separation between the composition laws and the behavioral framework, the latter, which is based on LTS, offers a unique LTS composition which is reused to dedZne syntactic composition of timed automata and a compositional semantics.
The idea of a label structure or similar constructs is not new since it appears in earlier studies [14], [15], [18], [13].
In [14], [15], the label composition operator appears under a functional form (the same as ours, see Label Structure : Section II) but the authors do not go beyond this dedZnition, while in [18], [13], it appears under a relational form.
In these latter studies, the authors are interested in reaching generic semantic rules for process calculi behavioral operators (predZx, choice...).
This is orthogonal to our goal that consists in reaching a label-based composition framework for specidZcation languages.
I. I NTRODUCTION For the past three decades, specidZcation languages such as CSP [17], CCS [16], LOTOS [10], Altarica [5], and BIP [6] have proven valuable in the specidZcation and design of concurrent and distributed systems.
The behavioral aspects of these languages share a common base since they all dedZne their operational semantics in terms of labeled transition systems (LTS).
Yet, the difference lies in the synchronizing structure of the labels of these systems.
For example in CSP the synchronization is dedZned between two identical events, while in CCS and in synchronization vectors-based views, it is dedZned respectively between complementary events or between possibly different events.
Through the years, the basic versions of some of these languages have been extended by time, memory, and priority notions.
Accordingly, other formalisms have emerged in order to model the semantics of these extensions.
For example, we can cite Alur and Dillas timed automata [3] and Henzinger et alas timed transition systems [11] that both capture the time addition or the semantic model of [19] used to model the priorities.
However, even though the rules of the composition operations of these formalisms are the same in nature (synchronous and asynchronous rules), they are well distinguished in reality, maybe because of the specidZc attributes that come with each formalism.
A distinct composition operation is then introduced for each dedZned formalism.
In this paper, we aim at capturing some similarities of specidZcation languages by providing a semantic framework for system composition.
For this purpose, we introduce a 1530-1311/13 $26.00 AS 2013 IEEE 1550-1311/13 DOI 10.1109/TIME.2013.23  Our contribution can be viewed from two perspectives.
One way is to see this work as an abstraction of the composition of different behavioral formalisms via a separation of the labels composition laws of each language and a reuse of the LTS composition.
In fact, depending on the language, the label structure is dedZned and instantiated differently.
Using the common framework, one would then proceed by giving the semantics of other behavioral operators of the specidZcation language in question.
Another way to see our label structure and their associated operations is as a generalization of the composition functions given and used in [14], [15].
Indeed, we show how such abstract composition functions (or label structures in our terms) may be implemented and instantiated to simulate existing synchronization mechanisms.
Most importantly, we push forward this work by giving new dedZnitions, properties, and operations to manipulate such label structures.
The rest of the paper is organized as follows.
In the second section, we start by dedZning the label structure along with its associated properties and operations.
In the third section, we dedZne our behavioral framework and show how it reuses the label structure notions.
We conclude the paper in the fourth section.
111 125  II.
L ABEL S TRUCTURE We start by describing the labels of transition systems by means of a label structure which is later used as an attribute of transition systems.
DedZnition 1 (Label Structure): A label structure is a tuple L, 1 where L is a set of labels and (1: L A L  L) is a partial binary composition operator over L. The function is partial because some composition may be blocked since 1 describes exclusively synchronous compositions.
The asynchronous aspects are covered later (see LTS composition).
Our composition then models the following cases : 1) A successful synchronization between l and l that results in l 1 l a L. / 2) A blocking synchronization between l and l : (l, l ) a dom(1).
Let the reader not confuse our label structure with other event structuring propositions, namely with the event structures [8].
Event structures model the occurrence of events during the system execution via the introduction of a causal dependency relation and a condZict relation between the events.
In our case, we introduce a label structure which models the way the labels (i.e., events) are statically composed.
DedZnition 2 (Commutativity of a Label Structure): Given a label structure LS = L, 1, LS is said to be commutative if its composition operator 1 is commutative.
Formally, for l1 , l2 and l3 a L, LS is commutative if :  b) Basic CSP Synchronizing Structure: Here, we model the case of the completely synchronous composition of CSP.
For C a set of communication ports, a synchronizing structure on C is the label structure : SyncCSP = C, (c1 , c2 ) a c1 if c1 = c2  The synchronization of two ports of the set C is only dedZned when these two ports are the same.
Otherwise an interleaving occurs.
c) CCS Synchronizing Structure: For C a set of events, C?
= {c?
| c a C} and C!
= {c!
| c a C}, this is represented in our label structure as follows : SyncCCS = C?aSC!aS{D }aS{(c!, c?)
a D | c a C}, {(c!, c?)
a D | c a C} B. Label Structure Properties We give the frequent label structures properties used in this paper.
Property Idempotency Unique Composition Diagonality  DedZnition al a L, (l, l) a dom(1) aSS l 1 l = l al1 l2 a L, (l1 , l2 ) a dom(1) a / dom(1) al a L, ((l1 1 l2 ), l) a aSS(l, (l1 1 l2 )) a / dom(1) al1 l2 a L, (l1 , l2 ) a dom(1) a l1 = l2  Example SyncCSP , T S SyncCCS SyncCSP , T S  We denote by ACI the conjunction of the associativity, commutativity and idempotence properties.
A label structure fuldZlling the ACI property is seen as a join semi-lattice where 1 is interpreted as the join operator and the partial order relation l a$?
l is dedZned by l a$?
l  l 1 l = l .
(l1 , l2 ) a dom(1) a (l2 , l1 ) a dom(1) aSS l1 1 l2 = l2 1 l1 DedZnition 3 (Associativity of a Label Structure): Given a label structure LS = L, 1, LS is said to be associative if its composition operator 1 is associative.
Formally, for l1 , l2 and l3 a L, LS is associative if it satisdZes the following conditions 1) (l1 , l2 ) a dom(1) aSS ((l1 1 l2 ), l3 ) a dom(1) a (l2 , l3 ) a dom(1) aSS (l1 , (l2 1 l3 )) a dom(1).
This means that independently of the composition order, they are both dedZned.
2) (l1 , l2 ) a dom(1) aSS ((l1 1 l2 ), l3 ) a dom(1) a ((l1 1 l2 ) 1 l3 ) = (l1 1 (l2 1 l3 )).
This means that independently of the composition order, they both lead to the same result.
DedZnition 4 (Stability of a Set of Labels): Given a label structure LS = L, 1 and a label set G a L, we say that G is stable over LS if a(l1 , l2 ) a dom(1), l1 1 l2 a G a l1 a G aSS l2 a G, and a(l1 , l2 ) a dom(1), l1 a G a" l2 a G a l1 1 l2 a G.  C. Composition of Label Structures We dedZne the product and the sum of two label structures.
The product operation builds new labels as pairs of the composed labels.
For example, this is used when composing synchronization and memory access labels.
Unlike the product operation, the labels of the sum operation are dedZned over the union of the composed labels.
This is used when composing synchronization and time labels to specify that only one of the events may occur at one time and not simultaneously.
1) Product of Label Structures: Given two label structures L, 1 and L , 1 , their product ranges over the set P = (L aS {fi}) A (L aS {fi })\{(fi, fi )} where fi (resp.
fi ) is a new element of L (resp.
L ) supposed to be neutral 1 for the 1 operator of its respective label structure.
For l1 , l2 a L and l1 , l2 a L , the composition of (l1 , l1 ), (l2 , l2 ) is dedZned only if the composition of l1 and l2 and the composition l1 and l2 are both dedZned.
A. Label Structure Examples a) Time Label Structure: For I a time domain, e.g., non negative real numbers, naturals .
.
.
, equipped with a binary associative operator + and a neutral element 0, we introduce the time structure T S on the domain I.
Its composition operator is only dedZned between identical time labels I' and returns the label itself.
 P,  T S = I, (I'1 , I'2 ) a I'1 if I'1 = I'2   1 If  126 112  L, 1 a L , 1  = (l1 , l1 ), (l2 , l2 ) a (l1 1 l2 , l1 1 l2 ) if (l1 , l2 ) a dom(1) aSS (l1 , l2 ) a dom(1 )     L had already a neutral element , we suppose that n 1  =  1 n = n.  2) Sum of Label Structures: Given L, 1 and L , 1 , their sum ranges over the union of LaV aS aV L where LaV = {laV | l a L} and aV L = {aV l | l a L }.
L, 1 a L , 1  = LaV aS aV L ,    aV  l1 aV , l2 aV a (l1 1 l2 ) if (l1 , l2 ) a dom(1) aV l1 , aV l2 a aV (l1 1 l2 ) if (l1 , l2 ) a dom(1 )     Proposition 1 (Preservation of ACI ): Given LS and LS  , if LS and LS  satisfy one of the ACI properties then LS a LS  and LS a LS  satisfy this same property.
A label structure transformation is used to map labels from a label structure to another.
We start by giving the dedZnition of a transformation followed by instances of such transformations.
DedZnition 5 (Transformation): A transformation f between two label structures LS1 = L1 , 11  and LS2 = L2 , 12  is dedZned as a partial morphism f from LS1 labels to LS2 labels such that : aV aV  Inl Inr  aa a a  Outl Outr Extl Extr P rjl P rjr  a  a  a  a  a  a aa a  a aa  Signature Embedding LS1  LS1 a LS2 LS2  LS1 a LS2 Retraction LS1 a LS2  LS1 LS1 a LS2  LS2 Extension LS1  LS1 a LS2 LS2  LS1 a LS2 Projection LS1 a LS2  LS1 LS2 a LS1  LS2  (tr : LS1  LS2 ) a LS1 a LS  LS2 a LS  T r Inr (aLSa LSa )  (tr : LS1  LS2 ) a LS a LS1  LS a LS2  T r Extl (aaLS aLS )  (tr : LS1  LS2 ) a LS1 a LS  LS2 a LS  T r Extr (aLSa LSa )  (tr : LS1  LS2 ) a LS a LS1  LS a LS2  aV  l1  a aV tr(l1 ) laV  a laV  aSS a"(l1 , l2 )  a (tr(l1 ), l2 ) (, l2 )  a (, l2 ) aS(l , )  a (tr(l ), ) 1 1 aSS a"(l1 , l2 )  a (l1 , tr(l2 )) (, l2 )  a (, tr(l2 )) aS(l , )  a (l , ) 1 1  A.
Labeled Transition System (LTS) DedZnition 6 (Labeled Transition System LTS): Given LS = L, 1, a labeled transition system L over LS adenoted as LLS a is dedZned as Q, Q0 a Q, T a Q A L A Q where Q, Q0 , T denote respectively the sets of states, initial states, and transitions.
We denote by LT SLS the set of LTSs over LS.
l We write q a q  for an element (q, l, q  ) of T .
Furthermore, we dedZne the alphabet of an LLS adenoted as IaLLS a as the set of labels that are actually used by the transitions of LLS : l IaLLS = {l a L | aq q  , q a q  a T }.
DedZnition 7 (Bisimulation): Given ALS = Qa , Q0a , Ta  and CLS = Qc , Q0c , Tc , a relation R a Qc A Qa dedZnes a simulation between CLS and ALS denoted as CLS fiR ALS iff : (1) aqc0 a Q0c , aqa0 a Q0a such that (qc0 , qa0 ) a R and (2) l aqc , qc , qa , l if qc a qc and (qc , qa ) a R, aqa a Qa such that l qa a qa and (qc , qa ) a R. Two LTSs LLS and L LS are said to be bisimilar through the relation R a Q A Q denoted as LLS R L LS if LLS fiR L LS and L LS fiRa1 LLS .
Furthermore, we say that LLS and L LS are state-bisimilar if transition labels are not required to match.
DedZnition 8 (LTS Diagonality, Idempotency and Determinism): An LTS is said to be diagonal (resp.
idempotent) if the restriction of its label structure to the LTS alphabet is diagonal (resp.
idempotent).
An LTS is said to be deterministic l l if whenever q a q  and q a q  then q  = q  .
In the rest of the paper, this set of LTS properties will be named DID.
1) LTS Composition: Given two LTSs dedZned over L, 1, a set S a L denoting the allowed synchronization results, and two sets of labels Al and Ar denoting respectively the left and right interleaving labels, the label composition function 1 is A A extended to an LTS composition function l 1 r as follows:  We write f : LS1  LS2 to denote such transformations.
1) Basic Transformations: Given two label structures LS1 and LS2 , we dedZne label structure transformations which are used to embed a label into a sum of labels, destruct a label sum, extend a label to a couple of labels, or also project a couple of labels to an element of the couple.
These transformations are given in the following table : Notation  T r Inl (aaLS aLS )  tr  Transformation l1 aV  a tr(l1 )aV aV l  a aV l  III.
B EHAVIORAL F RAMEWORK  ran(11 ) a dom(f ).
al, l a dom(f ), (l, l ) a dom(11 ) a (f (l), f (l )) a dom(12 ).
al, l a dom(f ), (l, l ) a dom(11 ) a f (l 11 l ) = f (l) 12 f (l ).
Name  Signature  We note here that the transformation properties are satisdZed by the resulting functions.
The previous four transformations also preserve the injectivity of tr.
D. Label Structure Transformations  aV  Name  DedZnition l a laV l a aV l laV a l aV l a l l a (l, fi) l a (fi, l) (l, fi) a l (fi, l) a l  It is not difdZcult to see that the transformation properties (DedZnition 5) are satisdZed by the transformations we have dedZned.
We note that all these transformations are injective.
2) High-Level Transformations: Given the label structures 2 LS, LS1 , LS2 , and aLS LS1 a transformation from LS1 to LS2 , we dedZne the following four high level transformations :  S  Q1 , Q01 , T1  1 Al  S  Ar  Q2 , Q02 , T2  = Q1 A Q2 , Q01 A Q02 , T   where the set T is dedZned by the following rules :  127 113  l q1 a1T1  q1fi  l q2 a2T2  q2fi  (l1 , l2 ) a dom(1) aSS (l1 1 l2 ) a S l 1l  (q1 , q2 ) 1aT2 (q1fi , q2fi )  Corollary 2: Given an associative label structure LS = L, 1, the label set S a L, the LTSs L1LS , L2LS , and L3LS we have : 1) L1 1(L2 1L3 )  (L1 1L2 )1L3 if S, S  are  S YNC  S  l  q1 a1T1 q1fi l1 a Al l  (q1 , q2 ) a1T (q1fi , q2 ) l  q2 a2T2 q2fi l2 a Ar l  (q1 , q2 ) a2T (q1 , q2fi )  I NTERLEAVINGR  S is omitted when it is equal to L. In this case, if Al = Ar = a then 1 is a fully synchronous composition operator.
Theorem 1 (Bisimulation Compatibility ): Given the LTSs L1 , L 1 , L2 , and L 2 dedZned over the label structure LS, we have : L1  L2 aSS L 1  L 2 a L1 l 1 A  Ar  S  L 1  L2 l 1 A  S  Ar  S  A  L1  1   Ar1  [f ]Q, Q0 , a1  = Q, Q0 , a2 = {(q, f (l), q  ) | l a dom(f ) aSS (q, l, q  ) aa1 }  A  S1  (L2  Al2  1   Ar2  S2  Proposition 5 (Transformation Bisimulation Compatibility): Given LS1 = L1 , 11 , LS2 = L2 , 12 , two LTSs L1 and L2 both over LS1 , and a transformation f : LS1  LS2 , L1  L2 a [f ](L1 )  [f ](L2 ).
Theorem 3 (Transformation Compositionality): Given LS1 = L1 , 11 , LS2 = L2 , 12 , two LTSs L1 and L2 both over LS1 , and an injective transformation f : LS1  LS2 such that dom(f ) is stable over LS1 , we have : A  [f ](L1 l 1 S  L3 )  Al1  1  S1  Ar1  L2 )  Al2  1  S2  Ar2  Ar  L2 )  [f ](L1 )  f (Al )   1  f (S)  f (Ar )  [f ](L2 )  Proof: We only sketch the proof of the synchronous case.
Given LS1 , LS2 , LLS1 , and L LS1 , we prove that the two sides are bisimilar through the identity relation.
The proof is based on showing that each transition of the dZrst system can be found in the second system and vice versa.
It is depicted in the following implications which can be read from bottom to top and vice versa from either sides of the parentheses.
The main points of this proof are dZrst the use of the stability hypothesis so that we conclude that when l a dom(f ) then l1 , l2 a dom(f ) and conversely, second the use of the injectivity of f in order to connect the two branches of the proof.
 (L1  S  The label structure of an LTS may be changed in a composition such as making local a global event (CSP hide) or changing its name (CSP Rename).
Here, we consider some LTS labels transformations by extending the label structure transformations to LTS transformations.
DedZnition 9 (LTS Transformation): Given two label structures LS1 and LS2 , and a transformation f : LS1  LS2 , we dedZne [f ] : LLS1 a LLS2 as :  Theorem 2 (Associativity of l 1 r ): Given LS = L, 1 S , the label sets Al1 , Ar1 , Al2 , Ar2 , S1 , S2 a L, and the LTSs L1LS , L2LS , and L3LS .
If LS is associative, S1 , S2 , Ar1 , Al2 are stable over LS, and either one of the following conditions is satisdZed : 1) Ar2 aS IaL3 = a and Al1 aS IaL1 = a.
2) Al1 a Al2 , Ar2 a Ar1 , S2 aS Al1 = a, and S1 aS Ar2 = a.
3) Al1 a Al2 , Ar2 a Ar1 , S a Al1 , S a Ar2 , and S1 = S2 .
We have : Al1  S  B. LTS Transformations  L 2  This theorem allows us to reason by making use of substitution by bisimulation.
Proposition 2 (Synchronous Composition ): Given two LTSs L1 and L2 dedZned over the label structure LS, we have L1 1L2  L1 1L2 if S is stable over LS, IaL1 a S, and S IaL2 a S. A A two Proposition 3 (Commutativity of l 1 r ): Given S LTSs L1 and L2 dedZned over the label structure LS, we have A A A A L1 l 1 r L2  L2 r 1 l L1 if LS is commutative.
S  S  stable over LS 2 .
This dZrst proposition is the same as the weak associativity theorem of the CSP generalized parallel operator [17].
The hypothesis added in our context are satisdZed by the label structure associated to CSP.
2) L1 1(L2 1L3 )  (L1 1L2 )1L3 if S, S  are S S stable over LS and IaL1 a S. Proposition 4 (Idempotency of 1): Given LS = L, 1 and LLS , if LLS is DID then 1 is idempotent meaning that LLS 1LLS  LLS .
I NTERLEAVINGL  L3  The conditions of this theorem are only sufdZcient conditions and have been selected in order to dZt with our needs.
Other conditions can be found in other contexts, for example the CSP context [17].
We note how the third set of sufdZcient conditions satisfy the CCS parallel composition.
In this case, this result can be instantiated by taking S = Al1 = Al2 = Ar1 = Ar2 = L which leads to the following CCS associativity corollary.
Corollary 1 (CCS Associativity): The CCS parallel composition operator is associative.
Other associativity corollaries may also be deduced such as the followings :  Proposition 6 (Preservation of DID properties): Given two label structures LS1 , LS2 , an injective transformation f : LS1  LS2 , and an LTS L over LS1 , each of the 2S  128 114  is the complement of S.  a  l  l  l1 fi l2 fi 1 2 fi fi q1 , q2 a q2 , l = l1 1 l2 q1 a q1 , l1 a dom(f ) q2 a q2 , l2 a dom(f ) q1 a a a ) f (l f (l2 ) l 1 a (q1 , q2 ) a (q1fi , q2fi ) , l a dom(f ) q1 a q1fi q2 a q2fi l = l1 1 l2 a f (l)  (q1 , q2 ) a (q1fi , q2fi )  (q1 , q2 )  DID properties is preserved by the transformation f .
Formally, for P a DID property we have : P (L) a P ([f ]L).
f (l)=f (l1 )1f (l2 )  aa  a a a a a   (q1fi , q2fi )  3) Timed Automata Label Structure: Given a set of clocks C, we dedZne a timed automaton as an LTS such that its transitions are labeled by communication channels (dedZned by some label structure LS), guards (the label structure G) and reset actions (the label structure A).
For this moment, LS is left undedZned and can either model the CCS-based synchronization or the CSP-based one.
Given a label structure LS, a timed automaton (TA) over LS is an LTS over LS a G a A.  IV.
T IMED S YSTEMS A.
Timed Transition Systems (TTS) DedZnition 10 (TTS): Given a label structure LS = L, 1, a Timed Transition System (TTS) over LS is an LTS over LS a T S. DedZnition 11 (TTS composition): Thanks to the introduction of our label structure, the TTS composition is the composition of the underlying LTSs.
T ALS = LT SLSaGaA DedZnition 12 (TA Composition): Thanks to our label structure, the TA composition is dedZned as the composition of the underlying LTS systems.
4) TA Semantics: The semantics of a timed automaton is a TTS over LS a G a A.
In the following we denote GA for (G a A).
Lemma 1: If LS is associative (commutative) 3 then LS a GA a T S is associative (commutative).
Proof: G, A, and T S are associative (commutative).
By Proposition 1, a and a preserves the associativity (commutativity).
The TA semantics is given via a composition with a clock manager Clk dedZned over GA a T S (Fig 2).
B.
Timed Automata (TA) We consider dZrst a dedZnition of timed automata [3] in which no invariants are associated to its locations (this is close to a timed graph [2] since neither invariants nor committed states are modeled).
The transitions are in the form of guard/event/reset where the guards contain a conjunction of constraints represented as clock intervals and the reset actions consist in a set of clocks to be reset.
This is represented as a product of three label structures.
The dZrst manages the synchronization events, the second manages clock guards, and the third manages the clock reset.
In the rest of this paper, we consider a set C of clocks and a time domain I (e.g.
R+ ).
1) Guard Label Structure: Based on the Alur Dill timed automata [3], a guard is a conjunction of interval constraints associated to clocks.
Here, this is modeled as a function C a 2I .
The guard label structure is dedZned as :       fifi     	      	       G  C a 2I , (g1 , g2 ) a (c a g1 (c) aS g2 (c)) The guard label structure is ACI.
2) Action Label Structure: Based on the Alur Dill timed automata [3], an action associated to a discrete transition can reset some clocks while keeping the other clocks managed by the current timed automaton unchanged.
In order to allow the composition of reset actions, the clocks not managed by a given timed automaton are left undetermined.
Consequently an action is modeled by two disjoint sets r denoting the clocks to be reset and u denoting the clocks to be left unchanged.
Their composition is dedZned by respectively the union of the reset sets and the union of the unchanged sets provided that the reset and the unchanged sets are disjoint.
Fig.
1.
	   fifi   	  Semantics of TA via a Composition of Two LTSs  a) Clock Manager: The Clk automaton contains variables (denoted as cE) corresponding to the clocks c of C. It has two types of transitions.
The dZrst type correspond to transitions of time evolution labeled by aV I' in which after each possible delay all of the clocks are incremented by the amount of this delay.
The second type correspond to discrete aV transitions labeled by g, (r, u) in which certain clocks are checked against their guard constraints (cE a g(c)), and clocks belonging to r are reset.
In order to impose the determinism of Clk, we suppose that raSu = C in Clk, but we synchronize labels l of ta with labels l of Clk when l a$?
l .
We recall that  A   {(r, u) a 2C A 2C | r aS u = a}, ((r1 , u1 ), (r2 , u2 )) a (r1 aS r2 , u1 aS u2 ) if r1 aS u2 = r2 aS u1 = a The action label structure is ACI.
3 This  129 115  is veridZed for both of CCS and CSP.
this partial order relation a$?
has been dedZned from the join operator in Section II-B.
Since Clk is diagonal, idempotent, and deterministic, it follows that Clk1Clk  Clk (Proposition 4).
b) Reconstructing the TA Semantics: The TA semantics is dedZned by means of a composition between the syntactic ta and Clk where ta transmits the clock commands to Clk.
Since the LTS composition is dedZned over the same label structure, then the label structures on which ta and Clk are dedZned have to be adapted so that they both become dedZned over LSaGAaT S. More precisely, we use the left embedding transformation for ta and the transformed right extension for Clk.
This is formally dedZned as : S [[ta]] = (ta aLSaGAaT  LSaGA  1  (LSaGA)aV  Theorem 5: Given a timed automaton where each transition is labeled by an action (not by fi labels introduced by the label structure product), its standard and proposed (revised) semantics are state-bisimilar through the identity relation.
C. Timed Automata with Invariants We now add state invariants to timed automata as dedZned in [12].
1) Invariant Label Structure: Here we consider an invariant to be an upper bound constraint that may be associated to each clock.
It is dedZned as a partial function from clocks to the time domain I.
The composition of two invariants associates to each clock, when it exists, the minimum of the two bounds.
The invariant label structure is dedZned as :  S LSaGA  Clk(aaT )) aT S aGA  Theorem 4 (TA Semantics Compositionality): Given two timed automata ta1 and ta2 , [[ta1 1ta2 ]]  [[ta1 ]]1[[ta2 ]].
Proof: This proof is based on proving the bisimulation between the semantics of the composition of ta1 and ta2 and the composition of their semantics.
We start by unfolding the semantics of the TA composition [[ta1 1ta2 ]] and by applying a sequence of bisimulations we reach [[ta1 ]]1[[ta2 ]].
In the S following proof we denote  1 aV  by  and aLSaGAaT LSaGA  I   C  I, (i1 , i2 )  a (c  a  The invariant label structure is ACI.
2) Timed Automata with Invariants Label Structure: Given a set of clocks C, we dedZne a timed safety automaton (TSA) as an LTS such that its transitions are labeled by communication channels (dedZned by some label structure LS), guards (the label structure G) and reset actions (the label structure A).
Furthermore, invariants which are usually attached to locations, are here stored on special looping transitions in order to synchronize with the Invariant Clock controller (IClk in paragraph IV-C3a).
Given a label structure LS, a timed safety automaton over LS is an LTS over LS a G a A a I.
(LSaGA)  S by aaaT .
a  S S S S (ta1 aaaT Clk aLSaGAaT )1(ta2 aaaT Clk aLSaGAaT ) a a GAaT S GAaT S   {Associativity : Corollary 1.2} S S S S (ta1 aaaT (Clk aLSaGAaT 1(ta2 aaaT Clk aLSaGAaT ))) a a GAaT S GAaT S  {Commutativity : Proposition 3} aaT S aaT S LSaGAaT S LSaGAaT S (ta1 aa ((ta2 aa Clk aGAaT S )1Clk aGAaT S ))  {Associativity : Corollary 1.2} S S S S (ta1 aaaT (ta2 aaaT (Clk aLSaGAaT 1Clk aLSaGAaT ))) a a GAaT S GAaT S  {Idempotency : Proposition 4} S S S (ta1 aaaT (ta2 aaaT Clk aLSaGAaT )) a a GAaT S  {Associativity : Corollary 1.1} aaT S aaT S LSaGAaT S ((ta1 aa ta2 aa )  Clk aGAaT S )  {Synchronous Composition : Proposition 2} aaT S aaT S LSaGAaT S ((ta1 aa 1ta2 aa )  Clk aGAaT S )  T SALS = LT SLSaGaAaI DedZnition 13 (TSA Composition): Thanks to our label structure, the TSA composition is dedZned as the composition of the underlying LTS systems.
3) TSA Semantics: The semantics of a timed safety automaton is a LTS over LS a GA a I a T S. Lemma 2: If LS is associative (commutative) then LS a GA a I a T S is associative (commutative).
Proof: G, A, I and T S are associative (commutative).
By Proposition 1, a and a preserves the associativity (commutativity).
The TSA semantics is given via a composition with an invariant clock manager IClk dedZned over GA a I a T S (Fig 2).
a) Invariant Clock Manager : The IClk automaton extends the Clk automaton by constraining the time elapsing.
It synchronizes on the invariant specidZed by the user-provided timed automaton, and on any GA label operator that is equal to the GA label provided by the timed automaton.
Since IClk is diagonal, idempotent, and deterministic, it follows that IClk1IClk  IClk (Proposition 4).
 {Transformation Compositionality : Theorem 3} S S ((ta1 1ta2 ) aaaT  Clk aLSaGAaT ) a GAaT S  Of course, the hypothesis of the applied results have been aV veridZed.
Namely, (LS a GA) is stable over LS a GA a T S, aV LSaGAaT S a (LS a GA) , DID is preserved by the Iata1 aLSaGA transformations, Clk veridZes the DID properties, the product and the transformation are compatible w.r.t bisimulation and Lemma 1.
5) Comparison with Standard TA Semantics: We now state the equivalence between our TA semantics and the standard one.
We start by dedZning the standard TA semantics by the function [[ ]]std : LT SLSaGA a LT SLSaGAaT S such that [[Q, Q0 , a]]std = Q A (C a R+ ), Q0 A {c : C a 0}, as  where : (l,(g,(r,u)))  q aaaaaaaaq fi ,   caC  v(c)ag(c) ,   car  v fi (c)=0 ,   cau  aSS a"min(i1 (c), i2 (c)) if c a dom(i1 ) aS dom(i2 ) i1 (c) if c a dom(i1 ) \ dom(i2 ) ) aSi (c) if c a dom(i ) \ dom(i ) 2 2 1  v fi (c)=v(c)  (l,(g,(r,u)))  (q,v)aaaaaaaas (q fi ,v fi ) I'  (q,v)as (q,v+I')  130 116  or by composing the original LTS with the one provided with the extension.
We show that these two methods are equivalent and that, provided some hypothesis, the semantics of the extension is compositional.
	     fi     	          	    Fig.
2.
	  A. Semantic Extension Given a label structure LS1 , we introduce an extension label structure LS2 supposed to be ACI, and its semantics dedZned by an LTS C(the controller) over LS2 a LS3 .
We dedZne the extended semantics by the function [[ ]]std : C LT SLS1 aLS2 a LT SLS1 aLS2 aLS3 such that [[L = 0 0 Q, Q0 , a]]std C = Q A QC , Q A QC , as  where :   fi     Semantics of TSA via a Composition of Two LTSs  b) Reconstructing the TSA Semantics: As before, the TSA semantics is dedZned by means of a composition between the syntactic tsa and IClk.
The semantics of a timed safety automaton is reconstructed as follows :  lfi2 aV  (l1 ,l2 )  qa aaaaqfi , qC aaaqCfi , l2 a$?l2fi (l1 ,l2 )aV  fi ) (q,qC )aaaaas (q fi ,qC aVl  (l,)  q aaaq fi  fi qC aqC  fi ) (q,qC )aaaas (q fi ,qC  fi ) (q,qC )as (q,qC  aVl  (l,)aV  [[tsa]] =  IaT S )1 (tsa(aLSaGAa LSaGAa aI  S LSaGA Clk(aaIaT )) aIaT S aGA  The semantic LTS is dedZned over the product of the state space of the syntactic LTS L and the controller C. Its transitions are built by joining transitions on LS1 a LS2 and adding transitions over LS3 .
In order to allow the composition with user-given LTSs, we add non-determinism through the introduction of the label l2 such that l2 a$?
l2 .
Coming back to Timed Automata, this corresponds to making at least the resets and the unchanged actions asked by the user.
Unreferenced clocks can be freely modidZed which allows parallel LTSs to impose their own modidZcations.
This semantics is shown to be equivalent to the following one which reuses the label structure operators  Theorem 6 (TSA Semantics Compositionality): Given two timed safety automata tsa1 and tsa2 , [[tsa1 1tsa2 ]]  [[tsa1 ]]1[[tsa2 ]].
Proof: This proof is similar to the TA Semantics Compositionality proof.
4) Comparison with Standard TSA Semantics: We now state the equivalence between our TSA semantics and the derived standard one in which a specidZc encoding of the state invariant is taken into account.
We start by dedZning the standard TSA semantics by the function [[ ]]std : LT SLSaGAaI a LT SLSaGAaIaT S such that [[Q, Q0 , a]]std = Q A (C a R+ ), Q0 A {c : C a 0}, as  where : (l,(g,(r,u)))  q aaaaaaaaq , fi   caC  v(c)ag(c) ,   car  fi  v (c)=0 ,   cau  1 aLS2 aLS3  [[L]]C = (L aLS LS1 aLS2  fi  v (c)=v(c)  i  3 LS1 aLS2  C(aaLS )) aLS3 aLS2  Theorem 8 (Extension Semantic): Given two LTSs L over LS1 a LS2 and C over LS2 a LS3 with LS2 ACI, [[L]]std C and [[L]]C are state-bisimilar through the identity relation if the following conditions hold : aV transitions of L are not of the form (l, fi), aV the LS2 labels of the transitions of C are maximal for a$?.
(l,(g,(r,u)))  (q,v)aaaaaaaas (q fi ,v fi ) q aq  1  (LS1 aLS2 )aV  acaC,v(c)+I'a$?i(c) I'  (q,v)as (q,v+I')  Remark that a disjunctive invariant can be modeled using several looping transitions (second rule).
Theorem 7: Given a timed safety automaton where each transition is labeled by an action (not by fi labels introduced by the label structure product), its standard and proposed (revised) semantics are state-bisimilar through the identity relation.
B. Compositionality The compositionality result concerning the parallel operator of timed automata can be generalized as follows : Theorem 9 (Generalized Compositionality): Given two LTSs L1 and L2 over LS1 a LS2 and a controller C over LS2 a LS3 , we have  V. T OWARDS G IVING L IFE TO L ABEL S TRUCTURES In this section, we show how the previous semantic dedZnitions could be generalized by attaching behaviors to label structures.
For this purpose, we abstract the timed automata semantic construction by associating a behavior to label structures in the form of an LTS.
Starting from an LTS built on a given label structure, a syntactic extension can be reached by composing labels with those of a new label structure (for instance, as we have seen, extending LTS to Timed Automata by adding action and guard labels).
The corresponding semantics can be dedZned either by overlaying  [[L1 1L2 ]]C  [[L1 ]]C 1[[L2 ]]C if the following conditions hold : aV LS1 and LS3 are associative and commutative.
aV LS2 is ACI.
aV C is DID.
This theorem has a similar proof as the timed automata one.
Furthermore, all the hypothesis are satisdZed in the timed automata context.
131 117  [2] R. Alur, C. Courcoubetis, and D. Dill.
Model-checking in dense realtime.
Inf.
Comput., 104(1):2a34, May 1993.
[3] R. Alur and D. L. Dill.
A theory of timed automata.
Theor.
Comput.
Sci., 126(2):183a235, Apr.
1994.
[4] F. Arbab.
Abstract behavior types: a foundation model for components and their composition.
Sci.
Comput.
Program., 55(1-3):3a52, Mar.
2005.
[5] A. Arnold, G. Point, A. Griffault, and A. Rauzy.
The AltaRica formalism for describing concurrent systems.
Fundam.
Inf., 40(2-3):109a124, 1999.
[6] A. Basu, M. Bozga, and J. Sifakis.
Modeling heterogeneous real-time components in bip.
In SEFM, pages 3a12.
IEEE Computer Society, 2006.
[7] G. Behrmann, A. David, and K. G. Larsen.
A tutorial on uppaal.
In M. Bernardo and F. Corradini, editors, International School on Formal Methods for the Design of Computer, Communication, and Software Systems, SFM-RT 2004.
Revised Lectures, volume 3185 of Lecture Notes in Computer Science, pages 200a237.
Springer Verlag, 2004.
[8] W. Brauer, W. Reisig, and G. Rozenberg, editors.
Petri Nets: Central Models and Their Properties, Advances in Petri Nets 1986, Part II, Proceedings of an Advanced Course, Bad Honnef, 8.-19.
September 1986, volume 255 of Lecture Notes in Computer Science.
Springer, 1987.
[9] P. Farail, P. GaudZllet, F. Peres, J.-P. Bodeveix, M. Filali, B. Berthomieu, S. Rodrigo, F. Vernadat, H. Garavel, and F. Lang.
FIACRE: an intermediate language for model veridZcation in the TOPCASED environment.
In European Congress on Embedded Real-Time Software, ERTSa08, 2008.
[10] I. O. for Standardization.
Information processing systems - open systems interconnection - LOTOS - a formal description technique based on the temporal ordering of observational behaviour.
International standard.
ISO, 1989.
[11] T. Henzinger, Z.
Manna, and A. Pnueli.
Timed transition systems.
In J. de Bakker, C. Huizing, W. de Roever, and G. Rozenberg, editors, RealTime: Theory in Practice, volume 600 of Lecture Notes in Computer Science, pages 226a251.
Springer, 1992.
10.1007/BFb0031995.
[12] T. A. Henzinger, X. Nicollin, J. Sifakis, and S. Yovine.
Symbolic model checking for real-time systems.
Inf.
Comput., 111(2):193a244, 1994.
[13] T. Hoare and P. OaHearn.
Separation logic semantics for communicating processes.
Electron.
Notes Theor.
Comput.
Sci., 212:3a25, Apr.
2008.
[14] H. HuEttel and K. Larsen.
The use of static constructs in a model process logic.
In A. Meyer and M. Taitslin, editors, Logic at Botik a89, volume 363 of Lecture Notes in Computer Science, pages 163a180.
Springer Berlin Heidelberg, 1989.
[15] K. Larsen, P. Pettersson, and W. Yi.
Model-checking for real-time systems.
In H. Reichel, editor, Fundamentals of Computation Theory, volume 965 of Lecture Notes in Computer Science, pages 62a88.
Springer, 1995.
10.1007/3-540-60249-6 41.
[16] R. Milner.
Communication and concurrency.
Prentice Hall International, 1995.
[17] A. W. Roscoe.
The Theory and Practice of Concurrency.
Prentice Hall, 1997.
[18] A. W. Roscoe.
On the expressiveness of CSP.
Technical report, www.cs.ox.ac.uk/files/1383/complete(3).pdf, 2011.
[19] E. Sekerinski and K. Sere.
A theory of prioritizing composition.
Technical report, 1996.
VI.
C ONCLUSION We have presented a formal semantic framework for studying, dedZning, and manipulating the composition of extended transition systems based on the composition of their labels.
The framework is based on the idea of dedZning a label structure containing a composition operator.
Depending on the language in question, a different label structure is dedZned and thus different composition laws are integrated.
The label structure is then used as a parameter of labeled transition systems which describe the common semantic domain of the considered languages.
We believe that the suggested parametrization of the behavioral framework is a promising work and may represent, especially with the perspectives we have, the dZrst step towards giving a unidZed formal semantic framework for different process algebras and specidZcation languages.
In our study, we emphasize on composition operators of process algebras without concentrating on other behavioral operators.
In this context, we have pushed forward existing work of similar structures [14], [18], [13] by offering a richer set of operations and properties such as the composition of label structures and transformations between label structures.
Following our technique, the composition of different LTS extensions, whether it is a syntactic model or a semantic model, is captured by a unique composition operation dedZned on LTS.
This is a direct result of the separation between the label structure and the behavioral framework.
This result is different than what can be found in the literature since with each system, a different composition operation is provided.
This can be seen classically in the composition operations of LTS and TTS.
Even though a TTS is exactly an LTS having additionally time transitions, usually its composition operation does not reuse the LTS one.
Furthermore, generic results concerning label structures and LTS transformations are applied to establish well known properties of high-level structures such as the dedZnition of timed automata semantics.
We have shown that these semantics match with the standard timed automata semantics and that the timed automata are compositional w.r.t the parallel operator.
We are now working on a dual view of this work which consists in coupling our label structures to states.
This will help us to naturally take into consideration state-based mechanisms such as the the committed states of UPPAAL [7].
We are also working on dedZning the formal semantics of real time languages (BIP [6] and FIACRE [9]).
Namely, we are interested in extending our label structure with priorities which are present in all the three cited languages.
Another extension is to revisit this work by incorporating some categorical dZavor.
Finally, all the theorems related to the presented framework have been validated in the proof assistant Coq.
The Coq theory may be found at [1].
Acknowledgement: The authors would like to thank the reviewers for thoroughly reading the paper.
R EFERENCES [1] http://www.irit.fr/~Jean-Paul.Bodeveix/COQ/LblStr.
132 118
A Finite-State Approach to Event Semantics Tim Fernando Computer Science Department Trinity College, Dublin 2, Ireland Tim.Fernando@cs.tcd.ie  Abstract Events employed in natural language semantics are characterized in terms of regular languages, each string in which can be regarded as a motion picture.
The relevant finite automata then amount to movie cameras/projectors, or more formally, to finite Kripke structures with partial valuations.
The usual regular constructs (concatenation, choice, etc) are supplemented with superposition of strings/automata/languages, realized model-theoretically as conjunction.
(i) a (finite, 2-pointed) frame N, A, 0, 1 to be a finite set N of nodes, a set A a N A N of arcs, and distinguished nodes 0 and 1 (which often but not always are the numbers 0 and 1 ordinarily denote), and (ii) a (IS-)event-automaton E to be a frame NE , AE , 0E , 1E and labeling function lE : NE a Pow(IS) that maps a node n a NE to a set lE (n) a IS of formulas.
To illustrate, the very rudimentary picture (2) of die(Romeo) is provided by the event-automaton {0, 1}, {(0, 1)}, 0, 1, l where dom(l) = {0, 1}, l(0) = {alive(Romeo)} and l(1) = {dead(Romeo)}.
alive(Romeo) aa dead(Romeo)  1.
Introduction  (2)  Due in no small measure to [2], events of some form or another have become a common tool for semantically analyzing expressions of change in English (e.g.
[15, 6, 9]).
Under this approach, a sentence such as (1) is taken to describe an event of Mary swimming a mile, culminating in the past.
Alternative analyses of die(Romeo) are, of course, possible, the idea being to  (1) Mary swam a mile.
The language L(E) of E is  Such events are formulated below as runs of machines that collectively constitute a causal order around which to explain temporality in natural language ([11, 18]).
Similar ideas have been developed in [19, 13, 12, 17, 5], the distinctive feature of the present proposal being the use of a finite automaton for a declarative representation (as opposed to a procedural implementation) of a fragment of a first-order model.
That fragment is given by strings accepted by the automaton a that is to say, by motion pictures taken by a movie camera (or, passing from accepting to generating devices, by films played by a movie projector).
2.
Event-types as automata/languages Formally, it is convenient to present the relevant automata as Kripke models over some finite set IS of formulas (roughly the propositional fluents in [10]), defining  (i) generalize (0, 1) to a path 0 AV AV AV 1 from 0 to 1, and (ii) label a node n on that path by a set of formulas (true at n).
L(E) = {lE (0E ) AV AV AV lE (1E ) | 0E AV AV AV 1E a Path(E)} where Path(E) consists of strings n1 AV AV AV nk a NE + such that n1 = 0E , nk = 1E and (ni , ni+1 ) a AE for 1 a$?
i < k. Clearly, L(E) is just the language accepted by the finite automaton with initial node START a NE , accepting node 1E , and labeled transitions lE (m)  n aa m for (n, m) a AE lE (0E )  plus START aa 0E .
(That is, the finite automaton for E is obtained by moving node labels over to arcs that point to the node, throwing in an extra node START to point to 0E .)
Conversely, an obvious limitation on an event-automaton E is that it accepts only non-empty strings, any two of which begin with the same symbol and end with the same symbol.
This limitation can be overcome by permitting the empty  Proceedings of the Ninth International Symposium on Temporal Representation and Reasoning (TIMEa02) 1530-1311/02 $17.00 AS 2002 IEEE  string  to label 0E and/or 1E , so as to implement nondeterministic choice + as follows.
E E + E  =  []      E    []  As it turns out, it suffices to allow lE (0E ) = lE (1E ) =  in order to capture all regular languages.
Proposition 1.
For every regular language L a Pow(IS)a , there is a finite set E[L] of IS-event-automata, the sum of which accepts the non-empty strings in L  L a {} = L(E) .
EaE[L]  Proof.
Working with regular expressions L, we define E[L] by induction.
Let E[L + L ] be E[L] aS E[L ], and E[LL ] be {EE  | E a E[L], E  a E[L ]} unioned with E[L] if  a L and/or E[L ] if  a L (defining EE  in the obvious way).
As for La , form N = {(n, E) | E a E[L], n a NE } A = {((n, E), (m, E)) | E a E[L], (n, m) a AE } aS {((1E , E), (0E  , E  )) | E, E  a E[L]} l(n, E) = lE (n) and set E[La ] = {N, A, (0E , E), (1E , E), l | E a E[L]}.
  2.1.
Moens-Steedman and the Vendler classes Applying [11], let 0 and 1 be preparatory and consequent states respectively, and let a durative event(-type) E come with an inceptive event E i and a culminative event E c , the consequent state of E i being the preparatory state of E c , termed the progressive state pE of E pE  = 1E i = 0 E c .
Extracting a loop pE a pE from the equality 1E i = 0E c and taking 0E = 0E i and 1E = 1E c , we get the transitions AE  = 0 E a p E a p E a 1E .
From this, Vendleras well-known aspectual classifications drop out once pE is related to 1E and 0E .
For activities E like aswim,a pE = 1E inasmuch as every subtransition of a swim counts as a swim.
By contrast, for accomplishments E such as aswim a mile,a no proper subtransition from 0E can end at 1E .
(This is the adirecteda analog of quantization [9]).
The contrasting factive entailments of activities Mary was swimming  |= Mary swam  and accomplishments Mary was swimming a mile  |= Mary swam a mile  can then be linked to the test: is pE = 1E ?
Identifying statives E with the equations 0E = pE = 1E , the oddness (or markedness) of the progressive form of stative verbs might be blamed on the equality of progressive and simple forms (making the progressive operator, as it were, semantically redundant) or, focusing on 0E , the lack of progress from 0E (insofar as 0E = 1E ).
Pushing this line further, let pE = 0E for achievements E such as awina and abegin,a reducing the difference between achievements and accomplishments mentioned, for example, in [6], pp 560-561, to whether or not pE = 0E (pE = 1E holding in neither case).
Path(E) 01+ 0p+ 1 1+ 0+ 1  Vendler-class(E) activity accomplishment stative [0 = 1] achievement  pE = 1E + a + a  pE = 0 E a a + +  Table 1.
A first stab.
Is Table 1 faithful to a reading of 0E and 1E as the preparatory and consequent states of E?
Do we want to confuse the progressive state of aswimminga with the consequent state for ahave swuma?
The answer to both questions must surely be: no.
Which is not to say that Table 1 is all wrong.
But rather that it ought to be re-interpreted with the identifications a0=preparatorya and a1=consequenta relaxed.
Table 1 gives only a partial picture a namely, that concerning some formula D expressing culmination of (an event-occurrence of type) E. Indeed, the entries 01+ and 0+ 1 in Table 1 are reminiscent of the constructs Con-BEC(D) and Min-BEC(D) in [13], where the Vendler classes are characterized by binary features [Aa for] and [Aa Prog] that Table 1 interprets according to (3).
(3) E is [+ for] iff pE = 1E .
E is [+ Prog] iff pE = 0E .
(3) leaves out the labeling lE that turns a frame into a Kripke model, suggesting that aspectual properties we ascribe to E might be reducible to the frame of E.  2.2.
Framing and/or simulating aspect?
Testing the hypothesis that aspect can be confined to frames calls for a fuller account of aspect than that offered by Table 1.
[19] provides an elegant analysis of aspect, with arcs in a frame beefed up to arrows ([20]).
Most (if not all) the ideas in [19] can, I suspect, be reformulated in terms of event-automata, with a single arrow blown up to a (sub)frame (of a monster frame).
To my knowledge, such  Proceedings of the Ninth International Symposium on Temporal Representation and Reasoning (TIMEa02) 1530-1311/02 $17.00 AS 2002 IEEE  a reformulation has yet to be carried out, or shown conclusively to be impossible (or a step backwards).
Whether or not [19] provides, on balance, evidence for the reducibility of aspect to frames (be they irreducibly populated by arrows or not), I think it is fair to describe the focus of [19] as being a frame (drawn there as figure 12, page 98) wherein to locate certain formulas (written there D, Cl(D), P f (D), etc).
The attention paid to the frame is well-deserved inasmuch as it fleshes out a temporal ontology for aspect, with arrows for the progressive, the perfect, etc.
Further afield, aan active computational representation for verb semantics called x-schemasa is presented in [1] that analyzes aspect ain terms of the context-sensitive interaction between verb-specific x-schemas and a controller x-schema that captures important regularities in the evolution of events.a The controller x-schema goes beyond the temporal ontology of [19] in recognizing points at which events are suspended, resumed, etc.
Indeed, x-schemas go beyond much of formal natural language semantics in offering a cognitive processing picture with asimulative inference,a as opposed to a model-theoretic account oriented around (not so much a mind that processes language as) an external reality that language describes.
That said, there is a growing appreciation within model-theoretic semantics of the importance of cognitive considerations (e.g.
[18, 5]).
A model-theoretic account that says nothing about cognitive mechanisms can hardly be a complete theory of language.
But this does not render the admittedly incomplete pictures offered by traditional model-theoretic analyses irrelevant.
If programming languages have denotational semantics distinct from their operational semantics, why not natural languages?
It is precisely to understand what computational accounts such as [12] come to a and the proliferation of concurrency models suggests there are bound to be many such accounts a that one abstracts away as much of their computational details as one can usefully get away with.
(Exactly what is useful is, alas, a matter of taste.)
Getting back to the specifics at hand, we have from page 9 of [18] the following claim: aspectual categories like activity and accomplishment are ways of viewing a happenning, rather than intrinsic properties of verbs and associated propositions, or of objective reality and the external world.
Keeping in mind the motion picture-camera/projector metaphor previously mentioned, it is natural to associate (i) aways of viewinga with an event-automaton E (or, more narrowly, the frame of E) and, on the other hand, (ii) the aassociated propositions,a aobjective reality and the external worlda with what E is about.
But what is E about?
Notice that Table 1 falls short of a model-theoretic analysis of say, [Aa for] and [Aa Prog] under (3).
In particular, it is natural to ask: how can we make precise what information the nodes 1, 0 and p encode in Table 1?
A brief answer is: apply the labeling lE of E to 1, 0 and p. Passing from Path(E) to L(E), our emphasis shifts from the mechanism E to the description L(E) of athe external worlda that E contributes (as ASS3 below spells out).
2.3.
Superposition: from N and Pow(IS) to Cn Apart from the usual regular constructs composing finite automata with each other, the particular alphabet Pow(IS) suggests aligning two strings Ia1 AV AV AV Iak and Ia1 AV AV AV Iak of equal length against each other to form a string (Ia1 aS Ia1 ) AV AV AV (Iak aS Iak ) of length k, where the ith-symbol Iai aS Iai is Iai and Iai superimposed on each other.
For instance, taking Ia Ia1 Ia2 Ia3  = = =  {swim(Mary)} {in(Mary,Ireland)} {in(Mary,IrishSea)}  =  {in(Mary,Wales)} ,  the string IaIaIa portraying Mary swimming can be componentwise superimposed on Ia1 Ia2 Ia3 to give the string (Ia aS Ia1 ) (Ia aS Ia2 ) (Ia aS Ia3 ) depicting Mary swimming from Ireland to Wales.
Now, stepping from strings up to languages L and L , let L &aS L  = {(Ia1 aS Ia1 ) AV AV AV (Iak aS Iak ) | k aL 1, Ia1 AV AV AV Iak a L, Ia1 AV AV AV Iak a L } ,  while over event automata E and E  , define an eventautomaton E AaS E  by NEAaS E  = NE A NE  , 0EAaS E  = (0E , 0E  ), 1EAaS E  = (1E , 1E  ), and AEAaS E   = {((n, n ), (m, m )) | (n, m) a AE and (n , m ) a AE  }  lEAaS E  (n, n ) = lE (n) aS lE  (n ) .
E AaS E  is the (unconstrained) concurrent composition of E and E  , with language L(E AaS E  ) =  L(E) &aS L(E  ) .
But should we be allowed to superimpose any two pictures Ia and Ia on each other?
If pictures are assumed to be complete descriptions (as the sets labeling a Kripke model  Proceedings of the Ninth International Symposium on Temporal Representation and Reasoning (TIMEa02) 1530-1311/02 $17.00 AS 2002 IEEE  ordinarily are), then they can be superimposed only on themselves, suggesting that we restrict the nodes of EAaS E  to the pullback     {(n, n ) a NE A NE  | lE (n) = lE  (n )}.
This restriction, call it P (E, E  ), yields the usual construction intersecting languages L(E) and L(E  ) L(P (E, E  )) =  L(E) aS L(E  ) .
On the other hand, if pictures are understood as incomplete (as we shall), then some middle ground between E AaS E  and P (E, E  ) might be sought, allowing the superposition of some but not all pairs of pictures.
(Take aMaryswimminga and atwo-ticks-of-an-hour clocka versus aMarysleepinga and aMary-wide-awake.a) Accordingly, let us weaken the requirement on acceptable node pairs n, n from lE (n) = lE  (n ) to lE (n)aSlE  (n ) being, in a precise sense, legitimate.
To pick out what pictures an event-automaton can frame, let us henceforth assume IS comes equipped with a nonempty family Cn a Pow(IS) that is a-closed (ie for all Ia a Cn, Pow(Ia) a Cn), with the intended reading Ia a Cn  Returning to ASS2.1, with an event-automaton E replaced by a language L, let us define analogs of 0E , pE and 1E as pictures Ia(L), I'(L) and D(L) a Pow(IS) of the inceptive, progressive and culminative stages of L given by  Ia(L) = {D1 | D a L}  I'(L) = {Di | i > 1 and D a L with length > i}  D(L) = {Di | i aL 1 and D a L with length i} where Di is the ith-symbol of the infinite string Daa obtained by concatenating D to the left of the infinite string aa of aas.
Next, instead of forming 0E a pE a pE a 1E , define the aMoens-Steedmana language MS(L) =  = (L &aS L ) aS Cn+ = {(Ia1 aS Ia1 ) AV AV AV (Iak aS Iak ) | k aL 1, Ia1 AV AV AV Iak a L, Ia1 AV AV AV Iak a L  and Iai aS Iai a Cn for 1 a$?
i a$?
k}      of &aS .
As for E AaS E , let E ACn E be the restriction of E AaS E  to the set of nodes {(n, n ) a NE A NE  | lE (n) aS lE  (n ) a Cn} provided this set includes both (0E , 0E  ) and (1E , 1E  ); otherwise, let E ACn E  be the event-automaton {0, 1}, a, 0, 1, l where l(0) = l(1) = a with empty language.
Proposition 2.
For all event-automata E and E  , L(E ACn E  ) = L(E) &Cn L(E  ) .
Henceforth, we focus on regular languages over Cn, as opposed to event-automata (linked to these languages according to Propositions 1 and 2).
This has a technical advantage illustrated by the ease in formulating Proposition 3.
For all languages L, L and L a Pow(IS)a ,  Ia(L) I'(L)+ D(L)  (which differs from Table 1 in yielding strings only with length aL 3) and in place of (3), let  iff Ia is consistent/conceivable/a cartoon  for every Ia a IS.
Cn induces the refinement L &Cn L  2.4.
Table 1 and (3) revisited with superposition  for(L) =  a D(L)+ a  prog(L) =  a Ia(L) a  +  where AV is a negation operation on subsets I, of IS such that I, aS I, a Cn and for all Il a Cn, Il aS I, a Cn  or  Il aS I, a Cn .
Finally, we turn Table 1 into the definitions Activ(L) = MS(L) &Cn for(L) &Cn prog(L) Accmp(L) = MS(L) &Cn fora (L) &Cn prog(L) Stat(L) = MS(L) &Cn for(L) &Cn proga (L) Achie(L) = MS(L) &Cn fora (L) &Cn proga (L) where +  fora (L) = a D(L) a proga (L) = a Ia(L)+ a .
Having used the function MS to motivate the definitions of for(L), prog(L), fora (L) and proga (L), notice that MS contributes nothing to differentiating v(L) for v a {Activ, Accmp, Stat, Achie}.
And instead of defining fora (L) and prog(L) in terms of negation AV on subsets of IS, we might specify how a function on languages classifies languages.
Given an arbitrary function f on languages, let us say  (a) L &Cn L = L &Cn L  (i) L is [af ] if L&Cn f (L) = a  (b) (L &Cn L ) &Cn L = L &Cn (L &Cn L )  (ii) L is f -acceptable if L&Cn f (L) = a, and  (c) L &Cn a+ = L iff L a Cn+ .
(iii) L is [+f ] if L is f -acceptable and L = L&Cn f (L).
Proceedings of the Ninth International Symposium on Temporal Representation and Reasoning (TIMEa02) 1530-1311/02 $17.00 AS 2002 IEEE  Notice that if L is Activ-acceptable, then L is for- and progacceptable, and Activ(L) is [+ for] and [+ prog].
Similar remarks can be made about the other v(L)as.
Under the definitions above, [+ for] amounts to a sortal/aspectual restriction that afora imposes on the verb phrase with which it combines.
(Furthermore, that restriction is treated along the lines of the approach to presupposition in [7], with L satisfying the presuppositions of afora precisely if L is [+ for].)
Passing again to arbitrary functions f on languages, let us call f Cn-conjunctive if f (L) =  f (f (L))  f (L) = f (L) =  f (L) &Cn f (L) f (L &Cn f (L))  example a aread for an hour,a analyzed as (a {read}+ ) &Cn for(a {read}+ ) &Cn ({time(x)} a+ {time(y), hour(x, y)}) = {time(x)} {read}+ {read, time(y), hour(x, y)} with parameters x and y, and restrictions time(x), time(y), hour(x, y) that we will return to in the next section.
3.
Event-tokens and models  for all languages L that are f -acceptable.
Our attention in this section shifts from event-types to event-tokens, embedded in a model of reality that the formulas in IS describe.
An important part of that model is a temporal frame (Ot, SD ) consisting of a asuccessora relation SD a Ot A Ot on a set Ot of aobservation times.a  Proposition 4.
Examples.
(a) If f is Cn-conjunctive, then for every f -acceptable language L, both f (L) and L&Cn f (L) are [+f ].
(b) Each of Activ, Accmp, Stat, Achie is Cn-conjunctive, as are the functions sending L to a D(L)+ D(L) and Ia(L) Ia(L)+ a (which are slight variants of for(L) and proga (L), respectively).
Pausing for an example, consider the following pair from [1].
(4)  a.
She read the book for an hour.
b.
She read the book in an hour.
On the surface, (4) poses a challenge to the prohibition against afora and aina being able to fill the same holes (ie [+ for]=[a in]).
But, as pointed out in [1], (4b) entails that ashe finished reading the booka whereas (4a) does not.
That is, aread the booka amounts in (4a) to aread parts of the booka and in (4b) to aread the entire book.a It is well-known (e.g.
[9, 13]) that the argument of a verb can shape the aspectual property of the verb phrase, so that, in particular, aread parts of the booka is naturally conceptualized as an activity, whereas aread the booka (or especially, aread the entire booka) is an accomplishment (that culminates with all unread parts of the book consumed).
Thus, afor an houra combines easily with aread parts of the book,a while ain an houra modifies aread the entire book.a But what does &Cn have to do with any of this?
Following the widespread use of conjunction in event semantics, we can apply &Cn to combine not only afor an houra with aread parts of the booka but also the argument aparts of the booka (or athe entire booka) with aread.a That is, &Cn is offered here as a tool for the logical investigation that lexical semantics richly deserves (e.g.
[3, 15]).
Focusing on time, let us work out a simple  (i) Ot is the set of integers, and SD is the usual successor (+1) function.
(ii) Ot a Pow(Pt) a {a} for some set Pt of points linearly ordered by <D , and SD is the set of all pairs (t, t ) a Ot A Ot such that (ax a t)(ay a t ) x <D y and not (at a Ot) t a gap(t, t ) where gap(t, t ) is {z a Pt a (t aS t ) | (ax a t)(ay a t ) x <D z <D y}.
(The second conjunct excludes gaps containing observation times, guarding against insertion anomalies.)
More concretely, if Pt is the set  of real numbers, I' is some positive number (fixing a level of granularity/degree of error-tolerance) and Ot is the set of non-empty open intervals o(p, q) =  {r a  | p < r < q}  with rational end-points p, q such that q a p > I', then for all o(p, q), o(p , q  ) a Ot, o(p, q) SD o(p , q  ) iff 0 a$?
p a q a$?
I' .
Note that Ot is countable and SD is not functional.
It will suffice throughout this section to equate an eventtype with a language L a Cn+ , the strings from which we anchor in a temporal frame as follows.
An event(-token) of event-type L is a function e from some finite subset {t1 , t2 , .
.
.
, tk } of Ot to Cn such that t1 SD t2 SD AV AV AV SD tk  and  e(t1 )e(t2 ) AV AV AV e(tk ) a L .
To state that e is an event of event-type L, we write  Proceedings of the Ninth International Symposium on Temporal Representation and Reasoning (TIMEa02) 1530-1311/02 $17.00 AS 2002 IEEE  e:L.  For typical choices of SD , at most one SD -chain can be arranged from a finite subset dom(e) of Ot, whence e picks out at a unique string in L. Also, the definition of ae : La can be relativized to a binary relation SD that depends on L, but for simplicity, we will leave SD fixed in the background.
Assuming an observation time can occur at most once in an SD -chain (ie t1 SD t2 SD AV AV AV tk and i = j imply ti = tj ), the totality of events of event-type L is essentially L&Cn L(SD ) where L(SD ) is the set of finite SD -chains.
In general, however, L(SD ) is not a regular language, and the special role it plays in time-stamping L makes it natural to present an event as a function whose domain is a finite subset of Ot.
To form a model from events, we must spell out what contribution an event e makes.
A simple answer is that e contributes the set a(e) =  {@(D, t) | t a dom(e) and D a e(t)}  of formulas, where a@(D, t)a is some formula saying: D holds at t. Recalling the formulas time(x), time(y) and hour(x, y) from the end of section 2, we might equate @(time(x), t) with ax = ta, @(time(y), t) with ay = ta and @(hour(x, y), t) with ahour(x, y)a (the dependence on t of hour(x, y) being spurious).
By contrast, for read (and many other IS-formulas), it is useful to construe @(read,t) as literally the string a@(read, t)a.
In view of these differences, let us partition IS into three sets IS = IS= aS ISg aS ISl where IS= consists of IS-formulas such as time(x) that translate to equations, ISg consists of aglobala IS-formulas such as hour(x, y) that are independent of t, and ISl consists of alocala IS-formulas such as read.
From IS= -formulas in e, we form the substitution I,e  =  3.1.
Lumping events into forcing-conditions Next, generalizing from event(-token)s to the set Ot ( Cn of partial functions (p, q, .
.
.)
from Ot to Cn, let  be the partial order on Ot ( Cn comparing information content as follows pq  iff dom(p) a dom(q) and (at a dom(p)) p(t) a q(t) .
(The intuition is that p  q says q is at least as informative as p.) Given a collection ET of event-types and a partial function p : Ot ( Cn, let ET(p) be the set of ET-events -contained in p ET(p) =  {e | e  p and (aL a ET) e : L} ,  this being the bit of reality ET carves up from p. Fix an expansion Time = (Ot, SD , .
.
.)
of (Ot, SD ) to the vocabulary (aka signature, language, set of non-logical symbols) of ISg so that every D a ISg can be judged to be true or false in Time.
Also, to extract a model from a partial function p : Ot ( Cn, we have to be careful about overlapping observation times, which we henceforth assume is given by a family OD a Pow(Ot) a {a}.
(For example, if Ot a Pow(Pt) a {a}, then  OD consists of all non-empty families T a Ot such that T = a.)
Let (i) P (ET, Time, OD ) be the set of partial functions p : Ot ( Cn such that for all e a ET(p) and D a ag (e), Time |= D , and for all T a OD where T a dom(p),  p(t) a Cn  {(x, t) | t a dom(e) and time(x) a e(t)}  which we then apply to the rest of the IS-formulas in e to get ag (e)  = {D[I,e ] | (at a dom(e)) D a e(t) aS ISg }  al (e)  = {@(D[I,e ], t) | t a dom(e) and D a e(t) aS ISl }  with the understanding that D[I,e ] is falsum aL if I,e is not functional, else D[I,e ] is D with every variable x a dom(I,e ) replaced by I,e (x).1  taT  (ii) v(IS) be the vocabulary consisting of unary relation symbols @(D, AV) for every D a ISl (with a@(D, t)a to be read: aD holds at ta), and (iii) fET be the function that maps a partial function p : Ot ( Cn to  al (e) .
fET (p) = eaET(p)  1A  substitution pairs variables (or parameters) with terms a the terms in this case being observation times.
I,e provides a means of binding variables x a dom(I,e ) locally to e, allowing multiple instantiations of x to co-exist (in a model).
That is, instead of proliferating alphabetic variants of an event-type, the event-type is construed as parametric, with x as a temporal parameter that an event e instantiates as I,e (x).
The thematic arguments of an event-type (e.g.
agent, patient) can also be presented as parameters, provided an event specifies instantiations of these parameters.
For the sake of simplicity, parameters and substitutions are confined here to observation times.
Let us define a IS(Time)-model to be an expansion of Time to the vocabulary v(IS, Ot) (= v(IS) aS Ot) where the constants t (in Ot) are interpreted as t. (To simplify notation, we do not distinguish between t qua constant, and t qua semantic interpretation.)
Now, given an element pE of P (ET, Time, OD ), how might we build a IS(Time)-model that contains ET(pE)?
Allowing for IS(Time)-models that  Proceedings of the Ninth International Symposium on Temporal Representation and Reasoning (TIMEa02) 1530-1311/02 $17.00 AS 2002 IEEE  may or may not contain events beyond those in ET(pE), let us pass from pE to a set P a P (ET, Time, OD ) with least element pE.
(To restrict events to those in ET(pE), take P = {pE}.)
Applying the aforcinga machinery in, for example, [8],2 let us define a forcing predicate ||aP (which we simplify to ||a) that relates forcing-conditions p a P to v(IS, Ot)-formulas (closed under AZ, a" and a) as follows:  (a) A generic set G induces a IS(Time)-model Time(G) such that for every v(IS, Ot)-formula D, Time(G) |= D  (b) Assuming ISl and Ot are countable, p ||aw D  iff for every generic G s.t.
p a G, Time(G) |= D .
(i) basing ||a on fET , p ||a @(D, t)  iff @(D, t) a fET (p)  for all D a ISl and t a Ot (ii) confining our search for AZD-counterexamples to qas in P such that p  q, p ||a AZD iff not (aq a P ) (p  q and q ||a D) (iii) externalizing a" to non-deterministic choice +, p ||a D a" D  iff p ||a D or p ||a D  Forcing-conditions p span the divide between events e a ET(p) and IS(Time)-models Time(G), for generic G p. Given P a P (ET, Time, OD ), let MOD(P ) be the set of IS(Time)-models generated by P -generic sets MOD(P ) = {Time(G) | G is P -generic} , and (going down , rather than [as is the case for generic sets] up), let PET be P with all its ET-events  ET(p) .
PET = P aS paP  The following proposition is easily proved.
(iv) restricting a to Ot, p ||a axD iff p ||a D[x/t] for some t a Ot .
3.2.
Between events and worlds Applied twice, AZ yields a notion of weak forcing ||aw p ||aw D  iff (ap a G) p ||a D .
iff p ||a AZAZD  that extends ||a in a manner that can be characterized by IS(Time)-models generated by certain subsets of P .
More specifically, a subset G of P is (P -)generic if for all p, q a P, (i) whenever p a G and q  p, q a G (ii) whenever p, q a G, there exists r a G such that p  r and q  r (iii) for every v(IS, Ot)-formula D, there exists r a G such that r ||a D or r ||a AZD.
Let us record fundamental results explained in [8] as Fact 5.
2 In the terminology of [8], we get a forcing property P, a$?, f fi (over the base vocabulary v(IS) and set Ot of constants) where a$?
is the restriction of  to P , and f is a function with domain P mapping p a P to f (p) = fET (p) aS {at = ta | t a Ot}.
As our only constants are those from Ot (no two of which are to be semantically identified), equality is trivial and is accordingly left out above.
fi It is perhaps also worth noting that [8] allows infinitary disjunctions , which should come in handy for infinitary +.
As it is, we can (in line with a finite-state perspective) limit the forcing-conditions in the present section to finite functions, provided we do not require that a generic set be represented by a single forcing-condition.
Proposition 6.
If P a P (ET, Time, OD ), then PET a MOD(PET ) =  P (ET, Time, OD ) MOD(P )  and ||aP is the restriction of ||aPET to P .
Why  bother forming PET ?
Because in PET , events e a paP ET(p) count as forcing-conditions, allowing us to ask of a v(IS, Ot)-formula D whether or not e forces D. But beyond D of the form @(D, t), what else is there to ae ||a Da?
Simplifying notation, let us henceforth assume P = PET .
Observe that for every ET-event e a P , D a ISl and t a Ot, e ||a @(D, t) iff t a dom(e) and D a e(t) and if IS is closed under negations az D (with {D, az D} a Cn), e ||a @(az D, t)  implies  e ||a AZ@(D, t) ,  the converse of which does not, in general, hold.
Readers familiar with [14] might liken the discrepancy here to that between constructible falsity (az) and intuitionistic negation (AZ).
More specifically, AZ brings the full space P of forcingconditions into the picture, denying the existence of a extension p of e in P such that p ||a @(D, t), whereas az requires local, positive evidence e. Indeed, the double negation translation AZAZ underlying ||aw weakens the requirement of local, positive evidence to e ||a AZAZD  iff (ap a P s.t.
e  p) (aq a P s.t.
p  q) q ||a D ,  Proceedings of the Ninth International Symposium on Temporal Representation and Reasoning (TIMEa02) 1530-1311/02 $17.00 AS 2002 IEEE  allowing for the possibility that e ||aw @(D, t) but not e ||a @(D, t).
That is, e may not be enough to settle ae ||a AZAZ@(D, t)a, although arguably, if e ||a AZAZD, then, as e  e a P , there is positive evidence q " e in P for D (except that it alone may not suffice).
At stake between az and AZ is the distinction between explicit and implicit information.
ae ||a @(az D, t)a says that explicit in e is information for az D at t, whereas ae ||a AZ@(D, t)a claims only that information against D at t can be inferred from e, possibly with the aid of P .
The question arises: how do we pick P and/or MOD(P )?
I hope to report on this matter elsewhere.
4.
Conclusion The finite-state approach to event semantics above is presented in two parts: one centered around event-types, formulated as finite automata, or more abstractly, regular languages; and the other around event-tokens, grounding strings from an event-type in a model.
The strings are built from a set IS of formulas, which correspond to the propositional fluents of [10].
It is natural to ask: where in the present approach are the situations?
Given a generic set G and a time t a Ot, one might expect to reconstruct a situation, understood as athe complete state of the universe at an instant of timea [10], from the snapshot {D | (ap a G) p ||a @(D, t)}.
Evidently, the presentation above is not oriented around situations.
Much more prominent is partiality, embodied in the first part by ACn /&Cn (playing the role of conjunction in a Davidsonian approach to event modification), and in the second part by  (linking events to worlds, to give the pictures in IS model-theoretic bite).
Speaking of partiality, it bears stressing that the present approach puts finite automata in the service of declarative, as opposed to operational, semantics, shying away from details of how language is processed.
Despite this limitation, I do think that (i) a useful temporal ontology reflecting aways of viewinga ([18]) can be fashioned from finite automata (perhaps with help from [19, 13]), and that (ii) the restriction to regular languages ought to have positive consequences for both the representational and inferential aspects of the frame problem ([10]) for natural language ([18]).
Obviously, there is much work to be done.
As to what has been carried out, I close with the note that it was conceived as part of a model-theoretic re-interpretation of propositions-as-types (applied to natural language in [16]), pushing typed It-calculi analyses of logical connectives down to the sub-atomic (lexical) level through finite-state  methods.
(The interested reader is referred to the constructive eventuality assumption in [4], ASS3.1.)
References [1] N. Chang, D. Gildea, and S. Narayanan.
A dynamic model of aspectual composition.
In Proc.
CogSci 98, 1998.
[2] D. Davidson.
The logical form of action sentences.
In N. Rescher, editor, The Logic of Decision and Action.
University of Pittsburgh Press, 1967.
[3] D. R. Dowty.
Word Meaning and Montague Grammar.
Reidel, Dordrecht, 1979.
[4] T. Fernando.
Conservative generalized quantifiers and presupposition.
In Proc.
Semantics and Linguistic Theory XI.
Cornell University, 2001.
[5] F. Hamm and M. van Lambalgen.
Event calculus, nominalization, and the progressive.
Available from www.semanticsarchive.net, 2000.
[6] H. Kamp and U. Reyle.
From Discourse to Logic.
Kluwer Academic Publishers, Dordrecht, 1993.
[7] L. Karttunen.
Presupposition and linguistic context.
Theoretical Linguistics, pages 181a194, 1974.
[8] H. J. Keisler.
Forcing and the omitting types theorem.
In M. Morley, editor, Studies in Model Theory.
The Mathematical Association of America, 1973.
[9] M. Krifka.
Nominal reference, temporal constitution and quantification in event semantics.
In R. Bartsch, J. van Benthem, and P. van Emde Boas, editors, Semantics and Contextual Expressions.
Foris, Dordrecht, 1989.
[10] J. McCarthy and P. Hayes.
Some philosophical problems from the standpoint of artificial intelligence.
In M. Meltzer and D. Michie, editors, Machine Intelligence 4.
Edinburgh University Press, 1969.
[11] M. Moens and M. Steedman.
Temporal ontology and temporal reference.
Computational Linguistics, 14:15a28, 1988.
[12] S. Narayanan.
Reasoning about actions in narrative undertanding.
In Proceedings of IJCAI a99.
Morgan Kaufmann, San Francisco, 1999.
[13] R. Naumann.
Aspects of changes: a dynamic event semantics.
J.
Semantics, 18:27a81, 2001.
[14] D. Nelson.
Constructible falsity.
J.
Symbolic Logic, 14(1):16a26, 1949.
[15] T. Parsons.
Events in the Semantics of English: A Study in Subatomic Semantics.
MIT Press, Cambridge, MA, 1990.
[16] A. Ranta.
Type-Theoretical Grammar.
Oxford University Press, Oxford, 1994.
[17] R. Reiter.
Narratives as programs.
In Principles of Knowlege Representation: Procedings of KR 2000.
Morgan Kaufmann, San Francisco, 2000.
[18] M. Steedman.
The Productions of Time.
Draft, ftp:// ftp.cogsci.ed.ac.uk/pub/steedman/temporality/temporality.ps.gz, July 2000.
[19] S. Tojo.
Event, state and process in arrow logic.
Minds and Machines, 9:81a103, 1999.
[20] J. van Benthem.
A note on dunamic arrow logic.
In J. van Eijck and A. Visser, editors, Logic and Information Flow.
MIT Press, Cambridge, MA, 1994.
Proceedings of the Ninth International Symposium on Temporal Representation and Reasoning (TIMEa02) 1530-1311/02 $17.00 AS 2002 IEEE
Model-Based Visualization of Temporal Abstractions Yuval Shahar and Cleve Cheng Stanford Medical Informatics 251 Campus Drive, Medical School Office Building (MSOB) x215 Stanford University, Stanford, CA 94305-5479, USA email: shahar,Ccheng@smi.stanford.edu Address and details of Corresponding Author: Yuval Shahar, M.D., Ph.D. Assistant Professor of Medicine and Computer Science Stanford Medical Informatics Medical School Office Building x215 251 Campus Drive Stanford University Stanford, CA 94305-5479 fax: +1-650-725-7944 tel.
: +1-650-725-3393 email: shahar@smi.stanford.edu WWW: http://smi.stanford.edu/people/shahar/index.html  Suggested Running Title: Model-Based Visualization of Temporal Abstractions  1  Abstract We describe a new conceptual methodology and related computational architecture called Knowledge-based Navigation of Abstractions for Visualization and Explanation (KNAVE).
KNAVE is a domain-independent framework specific to the task of interpretation, summarization, visualization, explanation, and interactive exploration, in a context-sensitive manner, of time-oriented raw data and the multiple levels of higher-level, interval-based concepts that can be abstracted from these data.
The KNAVE domain-independent exploration operators are based on the relations defined in the knowledge-based temporal-abstraction problem-solving method, which is used to abstract the data, and thus can directly use the domain-specific knowledge base on which that method relies.
Thus, the domain-specific semantics are driving the domain-independent visualization and exploration processes, and the data is viewed through a filter of domain-specific knowledge.
By accessing the domain-specific temporal-abstraction knowledge base and the domain-specific time-oriented database, the KNAVE modules enable users to query for domain-specific temporal abstractions and to change the focus of the visualization, thus reusing for a different task (visualization and exploration) the same domain model acquired for abstraction purposes.
We focus here on the methodology, but also describe a preliminary evaluation of the KNAVE prototype in a medical domain.
Our experiment  incorporated seven users, a large medical patient record, and three complex temporal queries, typical of guideline-based care, that the users were required to answer and/or explore.
The results of the preliminary experiment have been encouraging.
The new methodology has  potentially broad implications for planning, monitoring, explanation, and interactive data mining of time-oriented data.
Keywords: temporal reasoning; temporal abstraction; information visualization; exploration; data mining 2  3  1 TEMPORAL ABSTRACTION AND INFORMATION VISUALIZATION Many domains of human endeavor and scientific research require the collection of substantial numbers of data over time, and the abstraction of those data into higher-level concepts and patterns meaningful for that domain, a task we call temporal abstraction.
It is impossible to consider data trends, evolution of processes, or causality without an explicit representation of time.
This is especially true in medical domains, such as when monitoring and planning therapy for patients who have chronic diseases (e.g., cancer, AIDS, or diabetes) (Figure 1).
Visualization and exploration of information in general, and of large amounts of time-oriented data in particular, is essential for effective decision making.
Examples in medical domains include deciding, during therapy by a particular clinical guideline, whether a patient, who had previous bone-marrow transplantation, had more than two episodes of bone-marrow toxicity of severity grade 1 or higher, each lasting at least a week.
The severity classification is often specific to the context of therapy by a particular clinical guideline, which includes a drug that might be toxic to the bone marrow (see Figure 1).
Examples in engineering domains include deciding whether a particular traffic control action, such as diverting traffic in a certain congested highway zone, has been effective, as judged by the resulting observed pattern of traffic over time, compared to the pattern expected in that context.
Thus, effective visualization often requires preprocessing to abstract the data, using domain-specific knowledge, into meaningful interpretations.
4  PAZ protocol  BMT  Expected CGVHD M[0] Platelet counts (* )  [?]
[?]
* *  150K  [?]
*  [?]
*  [?]
[?]
[?]
[?]
* * * *  M[1]M[2]M[3] M[1]  [?]
*  100K  [?]
[?]
[?]
*  * *  0  50  100  .
200  [?]
*  [?]
*  [?]
*  M[0] Granu locyte count ([?])
[?]
[?]
[?]
* * *  2000 1000  400  Time (days)  Figure 1: Temporal abstraction in a medical domain.
Raw data are plotted over time at the bottom.
External events and the abstractions computed from the data are plotted as intervals above the data.
BMT = a bone-marrow transplantation = event; PAZ = a therapy protocol for treating chronic graft-versus-host disease (CGVHD), a complication of BMT; event; * = platelet counts; [?]
= granulocyte counts; = context interval; = abstraction interval; M[n] = bonemarrow-toxicity grade n.  Larkin and Simon (1987) have demonstrated that the usefulness of visual representations is mainly due to (1) reduction of logical computation through the use of direct perceptual inference, and (2) reduction of necessary search for information through the use of efficient graphical representations.
Indeed, we have noted in the past (Shahar and Musen, 1996) that experts in therapy of diabetes preferred graphical charts over data tables for the purpose of noticing meaningful diurnal trends in blood-glucose values and insulin administrations.
The ability to automatically create, visualize, and explore interval-based abstractions of timestamped data in context-sensitive manner in multiple domains, such as in various medical domains, has several useful implications: 1.
Data summaries of time-oriented electronic data have an immediate value to a human user, such as to a physician scanning a long patient record for meaningful trends.
2.
Machine-readable temporal abstractions support recommendations by intelligent decision-support systems.
5  3.
Abstractions support monitoring of plans, such as therapy plans, during execution.
4.
Meaningful time-oriented contexts that are induced from the data enable generation of context-specific abstractions and maintenance of several interpretations of the same data within different contexts.
5.
Temporal abstractions are helpful for explanation of recommended actions by an intelligent system.
6.
Temporal abstractions are a useful representation of intentions of plans, such as goals of clinical guidelines.
7.
Visualization and exploration of time-oriented data and its multiple-level abstractions supports a form of manual interactive data mining of time-oriented databases.
Thus, it is highly desirable to assist users who need to understand large amounts of timeoriented data, such as physicians who monitor patients over significant periods, by providing a useful visualization of these data and of their abstractions.
The abstractions being visualized must be specific to the context in which the data were acquired, and should include significant amounts of domain-specific interpretation, thus reducing considerably the information overload on the user.
To further reduce the information overload, succinct summaries must include not only abstractions of the data that hold over time points, such as "moderate anemia at 9:00 a.m. on January 5 1997," but also conclusions that hold over time intervals, such as "5 months of decreasing liver-enzyme levels in the context of recovering from hepatitis."
Finally, domainspecific abstraction, visualization and exploration of time-oriented data and meaningful concepts implied by these data requires acquisition and disciplined maintenance of knowledge of domainspecific time-oriented properties of the particular data.
Examples of such domain-specific  knowledge include meaningful classifications into a more abstract temporal patterns, knowledge  6  of whether similar patterns can be joined or should be considered as separate episodes, and an indication of how data should be best displayed over time.
One way of offering a data abstraction, visualization, and exploration service to users, which we propose in this paper, is to generate automatically short, informative, context-sensitive summaries of the data, at various levels of abstraction (see Figure 1), and to enable users to query, visualize, and explore these abstractions interactively.
In this paper, we discuss in depth a specific domain-independent conceptual methodology and a related software architecture that we have designed, which embody this philosophy.
Our approach is based on our previous theoretical and practical development of a domainindependent method that performs automated abstraction of raw time-oriented data into meaningful concepts, using access to a declarative representation of domain-specific knowledge.
We are also exploiting insights that we have gained from that research and from other studies we performed, regarding the appropriate organization and acquisition of the domain-specific knowledge that is needed for performance of the temporal-abstraction task by our method.
Finally, we are capitalizing on several additional computational modules that we have developed previously, such as for mediation of temporal queries to a time-oriented database.
This paper focuses on (1) our abstraction and visualization methodology, and in particular, on the semantics of its interactive exploration operators, and (2) the computational architecture that is implied by our methodology.
We do not aim in this paper to either justify or evaluate a particular graphical display.
However, we will demonstrate and clarify our ideas regarding knowledge-based interactive exploration by using a preliminary software prototype that we have designed and that implements the core functionality of our architecture.
We will also present a developmental evaluation of that prototype in a medical domain, to demonstrate the feasibility of our approach.
7  In the following sections, we start by presenting the knowledge-based visualization and exploration architecture.
We then explain briefly the temporal-abstraction methodology and computational infrastructure underlying that architecture; we have discussed these components at length in previous publications.
We then describe in depth the new query, visualization, and exploration methodology, using our prototype for demonstration purposes.
We briefly describe a preliminary evaluation of that prototype that we have performed within a small group of users.
We then discuss our work in the context of previous research pertinent to the several subtasks performed by our architecture.
We conclude with a note on our current and future work.
2 THE KNAVE ARCHITECTURE Our preliminary discussion of the task of knowledge-based abstraction, visualization and exploration of time-oriented abstractions highlights that fact that performance of this task requires an integrated solution to three subtasks: temporal abstraction, knowledge acquisition, and information visualization.
In particular, we would claim that an architecture offering a knowledge-based solution to the temporal visualization and exploration task must include three components: (1) The time-oriented database (and a method to access it effectively); (2) a knowledge-base of domain-specific properties; and (3) a module for display and exploration of the data.
Furthermore, an appropriate solution that addresses the requirements mentioned in Section 1 must also consider the distributed nature of time-oriented databases (e.g., patient databases), knowledge bases, and problem-solving processes.
Bearing all these logical  considerations in mind, we have designed an architecture, specific to discussion of the task of abstraction, visualization and exploration of time-oriented data and its abstractions, that capitalizes on recent advances in the understanding of all three tasks.
8  We refer to our  architecture as Knowledge-based Exploration of Abstractions for Visualization and Explanation (KNAVE).
The KNAVE architecture is presented in Figure 2.
The KNAVE architecture comprises three types of modules, including several components (see Figure 2).
Several of these components (in particular, the temporal-abstraction module and knowledge-acquisition tool) have been topics of our previous research.
9  Temporal- and statistical-abstraction server (Tzolkin)  Domain Experts  Resume  KA Tool  Chronus Controller  Domain ontology server DB  DB  DB  KB  Visualization and exploration module End Users Graphical Interface  KB  KB  Computational Module  Figure 2: The KNAVE architecture.
End users interact with the visualization and exploration module to ask temporal queries.
These queries are answered by the temporal- and statistical-abstraction service, using domain-specific data from the appropriate database, and temporal-abstraction knowledge from the appropriate knowledge base (ontology).
The visualization and exploration module allows users to explore the resultant abstractions, using temporal-abstraction and visualization knowledge from the domain's ontology.
Arrow direction indicates data flow.
DB = database; KB = knowledge base; KA = knowledge-acquisition.
In the KNAVE architecture, users (e.g., physicians) send complex temporal and statistical queries at varying levels of abstraction from local workstations (through the visualization module) to the temporal- and statistical-abstraction (TSA) server.
The TSA server is a database mediator (Wiederhold, 1992) that accesses multiple domain-specific time-oriented databases to provide answers to the queries, using the RESUME knowledge-based temporalabstraction system, which we have developed in previous research.
Typically, time-oriented databases include only raw time-oriented data, and no abstractions of those data; examples include databases of electronic medical records.
To interpret the data correctly, the TSA server 10  uses knowledge about temporal properties of the data by accessing the appropriate domainspecific knowledge base (through the ontology server).
Thus, the TSA server is a temporal mediator to the time-oriented databases.
To build the TSA server, we are using the Tzolkin temporal mediator, also developed in our previous research (Nguyen et al., in press).
Domain experts maintain the domain-specific temporal-abstraction knowledge bases using graphical knowledge-acquisition tools that we have developed.
Domain-specific knowledge includes  visualization knowledge and user preferences as well as temporal-abstraction knowledge.
The domain experts, the TSA server, and the local visualization and exploration processes access the domain-specific knowledge bases through the domain ontology server.
The visualization and exploration client is an innovative computational module that we have developed recently.
The visualization and exploration module is intended for use by all target users (e.g., physicians who need to summarize and explore patient data); it coordinates all of the distributed services.
The visualization and exploration module includes two components: (1) a computational module, which processes information obtained from the TSA and domain-ontology servers to support tasks such as interactive semantic exploration (e.g., "From what data is this bone-marrowtoxicity pattern abstracted?
"), dynamic explanation (e.g., "What classification knowledge was used to create this abstraction?")
and dynamic hypothetical exploration (e.g., "What would the abstract pattern look like if the patient's platelet count were, in fact, lower on 1/6/98?
"), and (2) a user interface module, which displays the various windows and widgets that make up the KNAVE display interface and which drive the computational component.
The computational component filters the information coming from the TSA and ontology servers to construct an internal representation which is efficient for the task of interactive exploration.
Thus, the  representation of temporal abstractions and relations between them is separate from the process of displaying the information.
11  2.1 The Knowledge-Based Visualization and Exploration Method The methodology underlying KNAVE is based on our previous work on the knowledge-based temporal-abstraction method (Shahar, 1997).
This method performs the temporal-abstraction task (see Section 1).
The knowledge-based temporal-abstraction method depends on four welldefined domain-specific knowledge types: structural, classification (functional), temporalsemantic (logical), and temporal-dynamic (probabilistic) knowledge.
Values for the four  knowledge types are specified within the domain's temporal-abstraction ontology (which includes properties of measurable parameters, external actions, and contexts induced by parameters or actions) when developing a temporal-abstraction system, and are acquired from domain experts through a specialized graphical knowledge-acquisition tool.
The knowledgebased temporal-abstraction method has been implemented in the RESUME system (Shahar and Musen, 1993), and has been evaluated in multiple medical (Shahar and Musen, 1996) and engineering (Shahar and Molina, 1998) domains with encouraging results.
Although all of the computational visualization and exploration operators used in the KNAVE system are domain independent, these operators are mapped to the domain-specific knowledge maintained by the ontology server.
That knowledge, in turn, has been acquired from domain experts for the purpose of abstraction of the domain-dependent data over time.
Thus, the  domain-specific semantics are driving the visualization and exploration processes.
These  semantics are defined by the ontology of the knowledge-based temporal-abstraction method.
The KNAVE framework enables users to query for domain-specific abstractions and to change the focus of the visualization in a natural manner, by reusing the same domain model that has been acquired from domain experts to perform the temporal-abstraction task.
Typically, that task is a part of planning, monitoring, data interpretation, or other applications.
12  The KNAVE  framework's interactive-exploration operators reuse the well-defined, domain-independent semantic links in the domain's temporal-abstraction ontology, thus defining a conceptual knowledge-based visualization and exploration method.
In the knowledge-based  visualization and exploration method, exploration of predefined relations in the domain's (temporal-abstraction) ontology causes certain computational transformations in the displayed abstractions.
For example, exploration of a functional-dependency link related to a visualized abstraction leads to the display of the data and/or abstractions from which the visualized abstraction is derived.
Different combinations of parallel motion along several semantic links in the domain's temporal-abstraction ontology define precisely several semantic-zoom operators.
These operators are augmented by purely syntactic visualization operators, such as overlay, magnification, and temporal scrolling.
A rather unique feature of exploring time-oriented data and temporal abstractions is that each type of data or abstraction has certain ranges of temporal granularity within which it is meaningful to visualize it (e.g., hours to days) and beyond which it should be visualized differently by using some domain-specific aggregation operator (e.g., mean, standard deviation, distribution, etc).
The KNAVE framework enables users to change on the fly the temporal granularity level of the interface (i.e., the relevant resolution of the presentation and interpretation process, as opposed to a syntactic zoom, which involves only graphical magnification of the data).
As we show when discussing the explanation operators, the KNAVE framework provides several mechanisms for explanations of data interpretations in a natural fashion by capitalizing on the direct access to the domain's temporal-abstraction ontology (e.g., zooming on the data from which a pattern is derived, or displaying dynamically the classification knowledge relevant to the creation of an abstraction, by retrieving it from the domain's ontology).
13  Finally, the KNAVE architecture lends itself well to explicit representation of domain-specific and individual-user preference models.
These models can be customized at the level of either the graphical display or the semantics of the knowledge-based exploration model.
Thus, the data- and knowledge-flow process inherent in the KNAVE architecture can be summarized as follows: 1.
The knowledge-based temporal-abstraction methodology is used to acquire, represent, and maintain an explicit representation of domain-specific temporal-abstraction knowledge as part of the domain's ontology, using semi-automated graphical knowledge-acquisition tools.
2.
The computational temporal-abstraction mechanisms in the TSA server create domainspecific, context-sensitive temporal abstractions of the data.
3.
At runtime, direct access is provided to the domain's temporal-abstraction ontology.
Direct access supports the formulation of the user's initial queries, the creation of the resulting static temporal abstractions, and the dynamic exploration of these abstractions by moving along predefined ontological temporal-abstraction semantic links, such as ABSTRACTED-FROM, IS-A,  or SUBCONTEXT.
Thus, the user explores multiple levels of  domain-specific abstractions in a domain-independent, but task-specific, manner, and the domain-specific ontology is driving the exploration process.
4.
Additional temporal-browsing operators are provided to the user; several are purely syntactic and do not depend on domain knowledge, and several use the direct access to the domain's ontology, such as determination of which data or abstractions are relevant for visualization at certain temporal-granularity ranges, and what alternative representations are useful at other temporal resolution levels.
14  5.
During the exploration process, the user can obtain context-sensitive explanations to questions such as "what classification function defines the creation of this abstraction from those data?"
by exploiting the direct access to the domain's temporal-abstraction ontology.
3 KNOWLEDGE-BASED TEMPORAL-ABSTRACTION In this section, we briefly describe our previous work on the knowledge-based temporalabstraction method, several of our theoretical and practical results, and the computational modules relevant to the KNAVE architecture that we have developed as part of that work.
These theoretical results and computational components support directly several of the knowledgerepresentation, knowledge-acquisition, and computational requirements of the KNAVE methodology.
Furthermore, the semantics of the knowledge-based temporal-abstraction method define, in turn, the semantics of the knowledge-based query, visualization, and, in particular, exploration operators.
Thus, the KNAVE framework can be viewed as an innovative reuse of the temporal-abstraction ontology and of the computational mechanisms used by the knowledgebased temporal-abstraction method within a new conceptual and computational framework that supports a different task.
That task consists of providing interactive visualization and  exploration of time-stamped data and their multiple-level time-oriented abstractions.
The temporal-abstraction task is an interpretation task: given time-stamped parameters (raw and abstracted data), external events, and the user's abstraction goals, produce time-interval- based abstractions of the data that interpret past and present states and trends and that are relevant for a given set of goals (Shahar, 1997) (see Figure 1).
The objective of our previous research was to develop a theoretical framework and an accompanying technology that can solve the temporal-abstraction task and thus provide concise, informative, context-sensitive summaries of time-oriented data stored on electronic media, such as medical records.
Predefined or ad hoc 15  queries would be answered at various levels of abstraction.
The output of such a tool, as explained in Section 1, is useful for multiple tasks, both to human users and to intelligent decision-support systems.
3.1 The Knowledge-Based Temporal-Abstraction Problem-Solving Method The framework that we have developed for solving the temporal-abstraction task is an extension of our previous work on temporal-abstraction mechanisms (Shahar et al., 1992; Shahar and Musen, 1993).
We have defined a general problem-solving method (Eriksson et al., 1995) for interpreting data in time-oriented domains, with clear semantics for both the problem-solving method and its domain-specific knowledge requirements: the knowledge-based temporalabstraction method (Shahar, 1997).
This method comprises a knowledge-level (Newell, 1982) representation of the temporal-abstraction task and the knowledge required to solve that task.
The knowledge-based temporal-abstraction method has a formal model of input and output entities, of their relations, and of properties associated with these entities (Shahar, 1997).
The knowledge-based temporal-abstraction method decomposes the temporal-abstraction task into five parallel subtasks: temporal context restriction, vertical temporal inference, horizontal temporal inference, temporal interpolation, and temporal pattern matching.
The five subtasks of the knowledge-based temporal-abstraction method are solved respectively by five temporalabstraction mechanisms (nondecomposable computational modules).
The temporal-abstraction mechanisms produce output abstractions of several abstraction types: state (e.g., LOW), gradient (e.g., INCREASING), rate (e.g., FAST), and pattern (e.g., QUIESCENT-ONSET).
The context-forming mechanism creates context intervals over which hold interpretation contexts (Shahar, 1998).
Context intervals create a relevant frame of reference for interpretation 16  and enable the temporal-abstraction mechanisms to focus only on abstractions relevant for particular contexts, thus creating interpretations that are context-specific and avoiding unnecessary computations.
Interpretation contexts are induced dynamically at runtime by the presence of a context-forming proposition, not necessarily with the same temporal scope (e.g., the event of administration of AZT might create a delayed, future interpretation context of potential toxicity due to AZT).
The contemporaneous-abstraction mechanism abstracts one or more parameters and their values, attached to contemporaneous time points or time intervals, into a value of a new, abstract parameter.
Thus, it performs a classification of a given parameter's value or a computational transformation of the values of several parameters, using a classification function.
ABSTRACTED-INTO  An  relation exists between input and output parameters.
The temporal-inference mechanism performs two subtasks: (1) inference of specific types of interval-based logical conclusions, given interval-based propositions, using a deductive extension of Shoham's temporal-semantic properties (Shoham, 1987) (e.g., unlike two consecutive periods of anemia, two episodes of 9-month pregnancies cannot be summarized as an episode of an 18month pregnancy, since they are not concatenable, a temporal-semantic property (Shoham, 1987)), and (2) determination of the domain value of an abstraction created from two meeting abstractions (e.g., for a gradient abstraction, DECREASING [?]
SAME = NONINCREASING (Shahar et al., 1992)).
The  temporal-interpolation  mechanism bridges  gaps  between  temporally-disjoint  propositions of similar types, using domain-specific temporal-dynamic knowledge about the dynamic behavior of the parameters involved (Shahar, 1997).
The temporal-interpolation  mechanism uses local and global truth-persistence functions to join temporally disjoint abstractions when values for direct determination of the abstractions are missing (Shahar, 1999).
17  The temporal-pattern-matching mechanism matches predefined linear (sequential) and periodic (cyclical or episodic) temporal patterns, as well as runtime temporal queries, with data, concluded abstractions or patterns, and external events.
The output is a pattern abstraction, which holds over an interval.
Complex combinations of temporal and value constraints can be used to define linear and periodic patterns (Chakravarty and Shahar, 1999).
The temporal-abstraction mechanisms require four domain-specific knowledge types for any particular domain: (1) structural knowledge (e.g., ABSTRACTED-INTO relations); (2) classification knowledge (e.g., definition of a parameter range as LOW); (3) temporal-semantic knowledge (e.g., the concatenable property); and (4) temporal-dynamic knowledge (e.g., persistence of a proposition over time when data is unavailable).
The input to the temporal-abstraction task is a set of measured time-stamped parameters (e.g., temperature), external events (e.g., insulin injections), abstraction goals (e.g., diabetes therapy), and domain-specific temporal-abstraction knowledge.
The output of the temporal-abstraction task is a set of interval-based, context-specific parameters or patterns at the same or at a higher level of abstraction and their respective values (e.g., "a period of 5 weeks of severe anemia in the context of therapy with AZT").
An abstraction of a parameter (e.g., the state of the hemoglobinlevel) is also a parameter.
Time intervals are constructed from pairs of time stamps; time points are zero-length intervals.
We call the structure {<parameter, value, context>, interval} a  parameter interval; it denotes that the parameter proposition "the parameter has a particular value given a specific context of interpretation" holds during a specific time interval.
Propositions hold only over time intervals.
If the parameter is an abstract (computed) parameter, such a structure is called an abstraction.
A pattern interval is defined in similar fashion.
Output abstractions or patterns hold within the interpretation contexts created by the contextforming mechanism.
18  A temporal-abstraction ontology includes (1) a parameter ontology--a theory of the relevant parameters and their temporal properties in the domain and the relations among these parameters (e.g., IS-A, ABSTRACTED-INTO), (2) a pattern ontology, which defines all patterns and their relations to other patterns and parameters (e.g., a linear pattern can have a COMPOSEDFROM relation INTO  to an abstraction, which for practical purposes can be viewed as an ABSTRACTED-  relation), (3) an event ontology, which includes external events (e.g.
medications), their  interrelations (e.g., PART-OF relations) and properties, (4) a context ontology, which includes interpretation contexts (e.g., the temporal context defined by the effect of a drug) and relations (e.g., SUBCONTEXT) among interpretation contexts, (5) an abstraction-goal ontology, which includes all abstraction goals (which can induce contexts; e.g., monitoring of diabetes therapy) and their IS-A relations; and (6) all relations between inducing propositions and induced contexts (e.g., INDUCED-CONTEXTS).
3.2 The RESUME Knowledge-Based Temporal-Abstraction System In our previous research, we have implemented the knowledge-based temporal-abstraction method as the RESUME system (Shahar and Musen, 1993; Shahar and Musen, 1996) in the CLIPS shell (Giarratano and Riley, 1994).
RESUME generates temporal abstractions, given time-stamped data and events, and the domain's temporal-abstraction ontology.
RESUME is composed of a temporal-reasoning module (the five temporal-abstraction mechanisms), a static temporal-abstraction domain knowledge base (the temporal-abstraction ontology), and a dynamic temporal fact base that stores input intervals representing external events, abstractions, and raw data and output interval-based interpretation contexts and abstractions.
Updates that cause retraction of previously concluded abstractions are propagated by a truth-maintenance system (Shahar and Musen, 1996).
Apart from conceptual advantages, such as facilitation of 19  knowledge-acquisition and maintenance, the RESUME architecture has several computational advantages (Shahar and Musen, 1996; Shahar, 1997).
3.2.1 Application of The RESUME System As part of our previous research, we tested the RESUME system in several different clinical and engineering domains: protocol-based care (experimental therapy of AIDS patients, therapy of chronic graft-versus-host disease, and prevention of AIDS-related complications) (Shahar and Musen, 1993; Shahar and Musen, 1993); monitoring of children's growth (Kuilboer et al., 1993); therapy of patients who have insulin-dependent diabetes (Shahar and Musen, 1996); assessment of the quality of guideline-based care, when the intentions of the guideline-designers are expressed as temporal abstractions to achieve or avoid (Shahar et al., 1998); and even monitoring of traffic and evaluation of traffic-control actions (Shahar and Molina, 1998).
We evaluated the feasibility of knowledge acquisition, representation, and maintenance, and applied the knowledge-based temporal-abstraction method to various test cases.
We found that both general temporal-abstraction computational knowledge and domain-specific temporal-abstraction knowledge were reusable, and that the results correlated with abstractions created manually by domain experts.
Furthermore, the experiments in the traffic-control domain emphasized the generality of our methodology and its potential applicability not only to time-oriented data, but also to abstraction of data measured over any linear distance measure, and in particular, over linear space (e.g., spatial abstraction of data from traffic sensors along a highway).
3.3 The Tzolkin Temporal Mediator We have incorporated the RESUME system within a domain-independent temporal mediator (Wiederhold 1992; Das et al., 1994), the Tzolkin system (Nguyen et al., in press).
The Tzolkin  20  system combines the RESUME temporal-reasoning system with the Chronus temporalmaintenance system (Das and Musen, 1994), which can access and manage a time-oriented relational database.
Tzolkin answers all temporal-abstraction queries referred to a time-oriented database by analyzing the query, retrieving the relevant data from the database and knowledge from the domain's temporal-abstraction ontology, and returning the appropriate abstractions.
The Tzolkin system is now an integrated key component in the EON architecture for guidelinebased medical care (Musen et al., 1996), which is being evaluated in domains such as guidelinebased breast-cancer therapy, and is part of an industrial collaboration.
3.4 The Temporal-Abstraction Knowledge-Acquisition Tool As part of our previous research, we have constructed a graphical knowledge-acquisition tool for automated acquisition of temporal-abstraction knowledge from domain experts (Stein et al., 1996), using tools from the Protege project.
The Protege framework (Musen, 1992; Puerta et al., 1992; Musen et al., 1995; Tu et al., 1995; Musen, 1998) is a workbench for construction and use of ontologies, for creation of domain-specific knowledge-acquisition tools derived from ontologies, and for entering of domain knowledge into these knowledge-acquisition tools.
One advantage of the Protege approach is the production, given the relevant problem-solving-method and domain ontologies, of automated knowledge-acquisition tools, tailored for the selected problem-solving method and domain.
Recent evaluation of the usability of the temporal-  abstraction knowledge-acquisition tool, using experts from several different clinical domains, has been quite encouraging (Shahar et al., in press).
21  4  IMPLEMENTATION OF THE KNOWLEDGE-BASED VISUALIZATION AND EXPLORATION METHOD WITHIN THE KNAVE ARCHITECTURE In this section, we explain the semantics of the knowledge-based visualization and exploration  method by walking through an example in the domain of guideline-based care.
The example is annotated by images from our prototype of the visualization and exploration module (originally developed in Visual Basic, and currently re-implemented in Java on a Windows/NT system).
Our aim is not to claim any particular advantages to our display interface, but rather (1) to clarify the semantic underpinnings of the conceptual knowledge-based visualization and exploration method, and (2) to demonstrate the feasibility of implementing that method.
4.1 The Temporal-Query Module Starting a KNAVE session requires the user to select a domain ontology (e.g., "protocol-based care") via a direct link to the domain-ontology server.
Once direct access to that ontology exists, the user can formulate a specific query for some entity, such as an abstract parameter or an external event (or intervention, as that particular knowledge role is relabeled by the display customization in a medical domain), by browsing the respective ontology.
For example, using the static interface, the user can ask "show me all periods of bone-marrow toxicity of any grade in the past 80 days, in the context of therapy by a Prednisone-Azathioprine (PAZ) protocol" (a guideline for treatment of chronic graft-versus-host disease, a complication of bone-marrow transplantation) (Figure 3).
22  Figure 3: The temporal-abstraction query interface of the KNAVE system in a medical domain.
A temporal query defines the parameter type (e.g., bone-marrow toxicity), type of abstraction (e.g., STATE), parameter value restriction, if desired (e.g., any value), the time span (e.g., past 80 days), and the context (e.g., therapy by the PAZ protocol) (see Figure 3).
The display of the resultant temporal-abstraction intervals over time, which are returned by the TSA server, is quite straightforward, as will be seen in the next section, when we discuss the exploration operators.
We have chosen simple linear representations of temporal intervals, as well as several standard visualizations of statistical descriptions (scattergrams, pie charts, distribution histograms, etc) when called for.
We based the design on feedback from users in several different domains, mostly medical ones.
Users often require more than piece of  information at the same time (e.g., both the answer to a query regarding certain abstractions and the answer to a query regarding certain contemporaneous external events).
Results appear in separate windows or panels, but temporal alignment among different windows and/or panels is the default.
23  4.2 The Dynamic Knowledge-Based Semantic-Exploration Operators Users benefit greatly from direct interaction with the data and its temporal abstractions.
We have noticed this benefit in previous experiments with automated temporal abstraction.
The need for interaction was anticipated in previous medical-record-summarization programs, such as Downs' program (Downs 1987) and de Zegher-Geets' IDEFIX program (de Zegher-Geets, 1987).
Both of these systems generated graphic displays of time-oriented clinical data at several levels of abstraction, and allowed a limited amount of interactive manipulation.
To exploit the full power of the temporal-abstraction mediator module (Tzolkin), and to support a domain-specific meaningful dialog, users need to be able to tap into the full knowledge contained within the domain model while interacting with the system.
Once initial queries are answered, typical exploratory queries include questions such as "Show me other siblings classes at the same level of abstraction" (e.g., other hematological abstractions); "Show me the same data and resultant abstractions in a more specific context" (e.g., when a certain drug was given as part of the guideline); "What is the distribution of some raw or abstract data type (e.g., bonemarrow toxicity) during some period?"
and "What if the data were a bit different: How would it affect the resultant patterns?"
etc.
Thus, we have implemented a module that performs knowledge-based exploration through the visualized data and its temporal abstractions.
This module is the core of the KNAVE architecture.
We enable users to change the focus of the visualization by exploiting the domain knowledge that has been acquired from domain experts to support the creation of the temporal abstractions.
In particular, we are using semantic links in the domain's temporal-abstraction ontology of parameters, interpretation contexts, and external events.
24  We have implemented six main semantic-exploration operators, corresponding to analog relations in the temporal-abstraction ontology (see Figures 4 and 5): 1. generalization/specialization of the parameter or pattern (e.g., from the white blood-cell count parameter to the class of hematological parameters and vice versa) using the IS-A semantic links among parameters; 2. functional dependency among parameters or patterns (e.g., from a bone-marrow toxicity abstraction in the PAZ context to the platelet-state and granulocyte-state abstractions defining it, and eventually, to the platelet-count and granulocyte-count parameter values defining these abstractions) using the ABSTRACTED-FROM or COMPOSED-FROM semantic links (Figure 5); 3. generalization/specialization of the interpretation context (e.g., from the preprandial (before meal) to the prebreakfast context in diabetes therapy) using the IS-A semantic links among interpretation contexts; 4. relations among contexts (e.g., from the context induced by the PAZ-therapy event to the context induced by a specific phase of that PAZ event, not necessarily contemporaneously) moving along the SUBCONTEXT relation; 5. generalization/specialization of the external event (e.g., from the insulin-administration event to regular-insulin administration) using the IS-A semantic links among events; 6. relations among events (e.g., from the PAZ-therapy event to a specific phase of the PAZ protocol in which a particular drug had been administered) moving along the SUBPART relation among events.
The knowledge-based semantic-exploration operators are demonstrated in Figure 4 as a set of seven hierarchical browser trees.
Each tree focuses on one semantic relation, and usually also on its inverse.
However, capitalizing on the initial feedback from our domain experts, we have split 25  the dependency relation into the abstracted-from (derived from) and its inverse, abstracted-into (supports) relations to facilitate browsing.
(This relation is a many-to-many relation, and thus cannot be depicted as a tree.)
All browser trees are linked to the domain's temporal-abstraction ontology and enable dynamic exploration of that ontology.
Figure 4: The interface to the dynamic knowledge-based semantic-exploration operators in KNAVE.
Contexts and events ("interventions" in medical domains) are shown at the top.
The results of the initial temporal query for bone-marrow toxicities displayed in Figure 3 are shown in the main panel.
The seven browsers enable semantic explorations of different relations in the domain's temporal-abstraction ontology.
The ABSTRACTED-FROM many-to-many relation had been split into the DERIVED-FROM and SUPPORTS relations to facilitate its exploration.
26  Figure 5: The result of an ABSTRACTED-FROM exploration query, given a bone-marrow-toxicity abstraction.
The user has explored the bone-marrow-toxicity abstraction, moving along the ABSTRACTED-FROM relation to the granulocyte-state abstraction, one of the parameters from which it is derived.
The user continued to explore the ABSTRACTED-FROM relation, arriving at the level of the granulocytes parameter, the raw data from which the granulocyte-state abstract parameter is derived.
Figure 5 demonstrates the results of a motion along the ABSTRACTED-FROM (derived from, as the medical-domain customization prescribed in this case) semantic link, following an initial query that displayed a bone-marrow-toxicity abstraction interval.
The user chose to perform a "depth-first" exploration into the granulocyte-state supporting abstraction, and into the granulocyte-count raw data supporting that abstraction.
The user could also display the  parameters supporting the bone-marrow-toxicity abstraction in a "breadth-first" manner, displaying both the granulocyte-state and platelet-state abstractions from which the bonemarrow-toxicity abstraction is derived (see Figure 4).
27  4.2.1 Additional Temporal-Syntactic and Temporal-Semantic Exploration Operators We have added several additional operators that facilitate browsing and exploration; several of these are purely syntactic (context-free, depending only on the data), and several are semantic, in the sense that they rely on access to the domain ontology and its semantics.
For instance, to facilitate browsing of data and abstractions we have added a syntactic temporal-zoom (magnification) operator.
Thus, selecting this operator and one panel or a part of it in a visualization window (e.g., just platelet counts) expands the display of the data within the selected time and data period to fill the whole screen, magnifying the display and enabling the user to inspect data or abstractions shown during that time interval more closely.
We have also added operators such as temporal overlay of different types of data along the same timeline (Cousins and Kahn, 1991), horizontal scrolling along the temporal dimension, and vertical scrolling along the abstraction dimension.
We have also added standard descriptive statistical operators that work at any level of abstraction, such as showing the distribution of the granulocyte-state abstraction values during a selected period.
4.2.1.1 The temporal-semantic-zoom operator To facilitate summarization and exploration of large amounts of data over significant time periods, we are enabling users to change dynamically the temporal granularity of the interactive interpretation and visualization process.
Often, users need to examine data at different temporal resolution levels (e.g., minutes, days, or years).
Furthermore, certain levels of temporal  granularity (e.g., weeks or days) are not meaningful for visualization of certain types of data (e.g., heart rates in the intensive-care-unit context need to be shown at the level of seconds or minutes, in which these data are typically acquired).
Thus, each type of data typically has certain ranges of temporal granularity within which visualization is meaningful and beyond which the data 28  should be visualized differently, by using a domain-specific aggregation operator (e.g., by using descriptive statistics such as minimum, maximum, mean, standard deviation, distribution, etc., or moving to a higher abstraction of the parameter).
We enable users to change on the fly the temporal granularity level of the interface (i.e., the relevant resolution of the presentation and interpretation process, as opposed to the syntactic temporal zoom, which involves only simple graphical magnification of the data display but adds no details or information).
The knowledge of (1) which temporal-granularity ranges are  meaningful for each parameter in each context, and (2) what aggregation operators should be used automatically when the granularity chosen is out of that range, is acquired and added to the domain model as part of the visualization knowledge.
This knowledge is acquired from the domain experts in the process of acquiring the temporal-abstraction knowledge.
The need for explicit representation of relevant temporal-granularity ranges in the domain's model was noted by Cousins and Kahn (1991), although their main goal was the development of a syntactic model for visualization of time-oriented clinical data, rather than exploration of the data abstractions using the domain's knowledge.
When domain-specific visualization knowledge is lacking, the temporal-semantic-zoom operation uses display heuristics of the type that have been explored in previous work on automated design of user interfaces (Gnanamgari, 1981; Arens et al., 1994).
The heuristics use only syntactic properties of the data; for instance, quantitative, numeric data are usually summarized using mean and standard deviation; qualitative, symbolic data are usually be summarized using distribution histograms or pie charts (if all allowed values are known) at the appropriate level of temporal granularity (e.g., one histogram per month, if month is the minimal temporal unit).
29  4.2.2 Interactive Knowledge-Based Explanation Users often require meaningful explanations for either data interpretations or action recommendations, such as when a physician examines the visual results of a temporal query to a patient's record.
Using the tight link between the KNAVE core visualization client and the domain ontology server (see Figure 2), we provide users with various types of context-sensitive explanations.
For instance, we may need to answer a query such as "why is this interval characterized as bonemarrow toxicity grade 3?"
(in addition to the query "from which data is it abstracted?"
which can be answered by semantic exploration along the ABSTRACTED-FROM link).
To answer that query, we retrieve from the domain's ontology the knowledge (in this case, a classification function that is represented in the knowledge-acquisition tool and the temporalabstraction ontology of the domain as a table) that defines the abstraction of bone-marrow toxicity grades from the toxicity levels of platelets and granulocytes in a context-sensitive fashion (e.g., specific to the PAZ-therapy context in which the abstraction was created) (Figure 6).
Figure 6: Providing an explanation to a query about the bone-marrow-toxicity abstractions shown in Figure 4 by displaying a classification (functional-dependency) table from the temporal-abstraction knowledge base.
30  4.2.3 Special Interactive Operators: Dynamic Manipulation of Data and Interpretation Contexts Apart from the exploration operators described in this section, which involve the user only in the sense of applying predefined exploration operators and which we had implemented in straightforward fashion, there are two types of interactive exploration operations which involve direct manipulation, by the user, of the explored contents.
These special operators pose  significant demands on the conceptual and practical communication capabilities within any knowledge-based visualization and exploration framework, and in particular between the three KNAVE modules.
Based on the insights we have gained from development of the KNAVE prototype, we are adding these two operator types to the current implemented prototype.
4.2.3.1 Controlling the overall goal of the abstraction process Often, the overall context for the abstraction process can be induced automatically from the database record and the domain's temporal-abstraction ontology.
For instance, therapy by the PAZ protocol induces a PAZ-therapy interpretation context, contemporaneous with the intervention interval; within that time interval, clinical data (e.g., hematological parameters) will be interpreted by the RESUME system in a manner specific to that context.
However, often the appropriate context for the abstraction process cannot be derived automatically from the data.
For instance, when a physician examines the record of a patient who is known to have, among others problems, diabetes, she would like to visualize a summary of the patient's data during the past four weeks, from the point of view of supporting a diabetes therapy goal.
She would not want to interpret all types of data in all potentially useful contexts, and certainly not throughout the patient's history.
This goal, however, is a conceptual one, existing only in the mind of the care provider.
It needs to be stated explicitly, so that it can be part of the database (at least  31  temporarily) and thus can induce the appropriate interpretation context(s) (through the contextforming mechanism).
Thus, we are currently adding an interactive dialog that enables users, during the visualization session, to dynamically add to the interpretation process abstraction goals (Shahar, 1997), whose main object is to set the overall context(s) and relevant time span(s).
The overall context enables the context-sensitive mechanisms of the RESUME system to create meaningful abstractions specific to that context.
Such a goal-oriented mode of performing the abstraction process is both necessary, efficient, and supports a useful dialog with the end user.
4.2.3.2 Interactive hypothetical queries Users often wonder if another specific datum would have changed the overall view of some segment of the data, when either added or removed.
Thus, we are currently adding the capability to enable at runtime addition and retraction of data to and from the KNAVE dynamic memory.
Thus, we enable the user to ask hypothetical ("what-if") queries by hypothetically asserting or retracting data (e.g., "what if the platelet count during the previous visit was in fact 50,000 and not 80,00?")
and by visualizing the resulting abstractions (e.g., the bone-marrow-toxicity grade might change to Grade III during the time interval corresponding to that visit, while certain abstraction might disappear).
Both the abstraction capability and the what-if queries are already supported by the RESUME system and its specialized truth-maintenance system, which maintains logical dependencies among data and their abstractions (Shahar and Musen, 1996).
The RESUME truth-maintenance system supports the nonmonotonic nature of temporal abstraction, whose conclusions are always potentially defeasible by additional data.
32  4.3 Explicit Representation of Domain-Specific And Individual User Models The KNAVE system should fit its interface terms and semantics (beyond the temporalabstraction ontology) to the needs of each domain, and should accommodate the needs of different user groups and even individual users within each domain.
Thus, we enable a  customization of the interface terms and behavior for each domain, including the listing of the relevant groups of users in each domain (e.g., physicians, nurses, etc., in the case of medical domains) and their default preferences, and by also enabling customization by individual users in that domain.
These customizations are part of the domain's (visualization) model.
Thus, in the case of medical domains, we have added defaults specifying how to present data to various groups of care providers such as nurses, interns, attending physicians, social workers, etc.
Each individual user can further edit her group profile to create her own "user profile" and save it for future use.
Examples include preferences such as whether to show just top level abstractions or all intermediate ones when returning the top-level abstraction in response to a temporal query (and similarly, whether to show just overall events or also their parts); over what span of time to start the initial query; what classes of data should be retrieved by the initial query (e.g., a diabetes-therapy expert might want to always start a visualization session by querying for the presence of certain periodic abstractions); and what types of abstractions are, in general, relevant to the user (e.g., an intensive care nurse might be interested, as a default, only in rate and gradient abstractions of certain classes of parameters such as blood gases and vital signs).
The user interface module is being designed using a model-based approach.
This means that in addition to the temporal-abstraction ontology, which is used to interpret data, there is also a domain model for the display of the data and its abstractions.
Thus, the domain display model includes a set of arguments for modifying the layout, order of presentation, and customization of  33  the behavior of semantic-exploration operators.
Examples of using the default model for the set of medical domains can be found within all screen displays shown in this paper.
For instance, note the substitution of widget labels to provide domain-specific naming schemes.
In the  medical domain, concepts such as "primitive parameters", "abstract parameters", and "events," which originate from the ontology of the knowledge-based temporal-abstraction problem-solving method, were mapped (Gennari et al., 1994), respectively, to the terms "empirical data", "derived parameters", and "interventions" which are the default mappings for a medical domain.
The domain model thus specifies a default user profile.
This default profile can then be customized to fit the needs of particular user groups (such as nurses) within the domain.
Group profiles can be further specialized to accommodate individual users.
4.4 Implications of the KNAVE Architecture for the Temporal Abstraction Framework We have found it necessary to make several enhancements to the implementation of the knowledge-based temporal-abstraction method in the RESUME system and to the temporalabstraction knowledge-acquisition tool to support the KNAVE visualization and exploration requirements.
Most of these enhancements or their analogs would be expected even if the implementation details were quite different.
1.
We have enhanced the RESUME system to enable creation of more complex abstraction patterns, and in particular, patterns comprising both temporal and statistical aspects.
For example, a typical need in the diabetes-therapy domain is to visualize an increasing gradient of the weekly variance of the blood-glucose after breakfasts over the past 6 weeks.
Such an abstraction, however, requires the knowledge to create the correct  interpretation context.
In this example, it is a prospective postprandial (after meals) context of the nonconvex type (i.e., composed of temporally disjoint intervals), specialized 34  to the post-breakfast context, that is induced by the existence of breakfast events in the database (Shahar, 1998).
Then, a purely statistical abstraction (variance) needs to be computed over each relevant time granule (here, a week).
Finally, the resulting (gradienttype) abstraction should be able to serve as input to a higher-level pattern.
2.
We have enhanced the graphical temporal-abstraction knowledge-acquisition tool so as to acquire more complex linear temporal patterns involving temporal-distance, temporalrelations, and parameter-value constraints.
These patterns are first-class entities in the domain's ontology.
Linear patterns have COMPOSED-OF relations to their components.
Components are abstractions or patterns that have internal, local constraints on start time, stop time, duration, and value of the abstraction.
Global qualitative and quantitative temporal and value constraints exist between components.
Accordingly, we have enhanced the RESUME temporal-pattern matching mechanism to detect and create these patterns at runtime.
3.
We have added the ability to represent and detect periodic temporal patterns and, in general, any repeating pattern of a certain class.
We represent periodic patterns as a repeating linear pattern with certain constraints such as cardinality and temporal-gap (e.g., each day, or 2 to 4 times a week) (Chakravarty and Shahar, 1999).
Periodic patterns can be specified in the graphical knowledge-acquisition tool, abstracted by the RESUME system, and visualized as regular interval-based abstractions within the KNAVE system.
4.
We have enhanced the knowledge-acquisition tool to enable acquisition of necessary temporal-visualization knowledge.
Examples include the relevant temporal-granularity ranges that should be used for visualization of each parameter, the default abstraction or statistical functions that should be used automatically when the visualization granularity is  35  outside of the scope of the relevant temporal granularity, and domain-specific interface customizations.
5.
Certain changes needed to be carried out with respect to the fashion in which any temporal mediator, and in particular the Tzolkin control module, coordinates the actions of the RESUME temporal-abstraction module and the Chronus temporal-maintenance module.
For instance, intermediate abstractions created by the RESUME system need to be returned as part of the output, for exploration purposes.
Currently, we are accessing the RESUME conclusions directly.
In the future, queries to the Tzolkin module will require more expressive semantics.
5 PRELIMINARY EVALUATION OF THE KNAVE FRAMEWORK As emphasized in the introduction, this paper focuses on the knowledge-based visualization and exploration methodology and its semantics, and the architecture implied by that method.
However, to demonstrate the feasibility of the methodology before we proceeded with a largescale implementation, we have performed a preliminary developmental assessment of a prototype version of the KNAVE core module (the visualization and exploration components) in the domain of protocol-based care in clinical medicine (Cheng et al., 1997).
The preliminary  evaluation can be viewed as a pilot study to test our hypothesis that a task-specific knowledgedriven visualization and exploration system is both feasible and useful.
This evaluation also provided preliminary feedback from potential users in various different medical domains.
To focus on a preliminary evaluation of the visualization and exploration module in a medical domain, we ignored the communication links.
Both the time-oriented patient data and its  abstractions (generated by the RESUME system) and the domain temporal-abstraction ontology were represented as simple files on the same workstation.
A single patient record was created 36  from two patient files (one presenting a patient after bone-marrow transplantation and treated by an experimental protocol to counter the complication of graft-versus-host disease, and the other presenting a patient was treated by an experimental protocol for AIDS therapy), since we wanted to demonstrate several different interpretation contexts.
The patient was presented to the users as an AIDS patient who underwent a bone-marrow transplantation for therapy of oncological complications.
The prototype, whose screen displays were shown in Figures 3 to 6, included a subset of the static and dynamic, semantic and syntactic operators that we are implementing in the reengineered system.
Seven users with varying medical and computer-use backgrounds were given a 10-minute explanation of the knowledge-based visualization and exploration framework; the users were not otherwise familiar with the KNAVE system.
The users were then requested to answer within 20 minutes three complex temporal queries about the particular set of data we used, using only the new interface and a brief introduction to the exploration interface.
The queries were realistic and came from the domain of guideline-based care in oncology and AIDS therapy.
The queries were designed to test multiple aspects of the KNAVE architecture, while being typical of tasks that clinicians are required to perform as part of treating patients using complex clinical guidelines, as is common in the domains used here.
The first query inquired whether there was ever any episode of at least one week of bone-marrow toxicity grade II or more, in the context of the prednsone/azathioprine (PAZ) protocol for therapy of graft-versushost disease, and if so, on which data that conclusion was based.
The task involved query, visualization, and some explanatory exploration  (see Figures 4 and 5).
The second query  introduced the concept of different interpretation contexts, and asked about whether certain values of the hematological-state abstraction in the CCTG-522 AIDS-therapy experimental protocol can be found during any period.
The third task forced the users to use the statistical-  37  abstraction and visualization capabilities as well, since it focused on finding the distribution of the hemoglobin-state abstraction values in the CCTG-522 context over the past 80 days.
The results, although obviously limited by the small number of users and the informal questions they were asked following the experiments, were quite encouraging with respect to the users' subjective enthusiasm towards the framework's potential and the objective capability of the prototype to provide visual answers to the various queries that the users tried to answer.
All users completed the three tasks (that is, answered the three complex temporal queries) within 20 to 180 seconds overall; six of seven users performed the three tasks within less than 90 seconds.
The users emphasized the importance of being able to get immediate explanations to the visualized abstractions, and seemed to like especially the combination of temporal and statistical abstractions.
One of the lessons we learned, which we will emphasize in the final implementation, was the importance of redundancy.
Somewhat surprisingly, different users found answers to the same queries using several (up to four) different paths to get to the same visualized set of abstractions (e.g., through the semantic-exploration interface, by going back to the initial temporal-query interface, by using the navigation drop menu, or by using a short-cut mouse right-button function).
6 DISCUSSION AND COMPARISON WITH RELATED WORK It is difficult to compare the results of the preliminary evaluation to any current automated interface, since performance of the task requested of the users requires having both the ability to abstract clinical data over time into multiple levels of concepts, and the access to a suitable domain-specific knowledge base (such as the temporal-abstraction ontology).
We know of no such architecture other than ours.
One might compare the performance of the users of KNAVE 38  to the manual one we witnessed while evaluating the RESUME system in the diabetes domain (Shahar and Musen, 1996).
In that domain, manual abstraction of somewhat similar queries required up to 10 minutes per query by either of the two experts (one of whom participated as a user in the preliminary KNAVE evaluation).
We also plan to compare KNAVE to an online interface, such as an electronic spreadsheet, that has charting capabilities but no explicit, application-independent, knowledge representation.
Visualization of information in general and of large amounts of time-oriented data in particular is essential for effective decision making.
Much effort had been put in the past into creation of effective visualizations for information; an excellent example is the classic series of books by Edward Tufte on methods to display information (Tufte 1983, 1990, 1997).
Previous work had typically focused on exploring separately three different subtasks of the problem we are tackling: temporal abstraction, information visualization, and knowledge acquisition.
It is not a coincidence that these three subtasks are represented explicitly in the KNAVE architecture as three distributed modules.
Several approaches have been applied to the task of abstraction of time-oriented data into higher-level concepts, especially in medical domains, in which both large amounts of data and considerable knowledge are available (Fagan, 1980; Downs et al., 1986; De Zegher-Geets, 1987; Kohane, 1987; Russ, 1989; Kahn, 1991; Larizza et al., 1992; Haimovitz and Kohane, 1993).
None of these approaches, however, emphasized the need for a formal representation that facilitates acquisition, maintenance, sharing, and reuse of the required temporal-abstraction knowledge; this emphasis is the focus of our previous and current research.
Furthermore, previous temporal-abstraction approaches were typically not geared for use in a runtime system for visualization of and exploration through the domain-specific abstractions, and would not support a domain-independent interface.
39  Researchers in the areas of presentation and display techniques in general (Tufte 1983, 1990), visualization of clinical time-oriented data (Cousins and Kahn, 1991; Powsner and Tufte, 1994; Plaisant et al., 1996), and human-computer interfaces, in particular information visualization (Rao et al., 1992; Roth et al., 1994; Shneiderman, 1994; Arens et al., 1994; Zhou and Feiner, 1996; Koljejchick et al., 1997; Rohrer and Swing, 1997; Wright, 1997; Becker, 1997) have developed useful visualization techniques for static presentation of raw time-oriented quantitative data and for browsing information.
These techniques use various statistical and graphical methodologies, such as scattergrams, pie charts, bar charts, three-dimensional representations (Mackinlay et al., 1994; Carpendale et al., 1997) and their derivative techniques.
These display methods, however, typically do not focus on visualization of domain-specific temporal abstractions (as opposed to raw data) and on the issue of dynamic exploration, using a domain-independent method, through multiple levels of these abstractions using domain-specific knowledge.
The reason for that omission is that such capabilities require formal, domainindependent  representations  of  the  domain-specific  temporal-abstraction  knowledge,  considerable effort in modeling the visualized domain, and the availability of computational mechanisms for creation of the relevant abstractions.
The past decade has witnessed considerable advances in semiautomated methods for knowledge acquisition and knowledge representation, based on approaches that operate at the knowledge level (Newel, 1982) and that assume task-specific but domain-independent problemsolving methods (Clancey, 1985; Chandrasekaran, 1986; McDermott, 1988; Musen, 1989; Weilinga et al., 1992, Genesereth and Fikes, 1992; Gruber, 1993; Eriksson et al., 1995) which often succeed in alleviation of the knowledge-acquisition bottleneck.
However, these methods are not often associated with runtime end-user applications; instead, they focus on use by knowledge engineers and domain experts.
In the case of visualization tools in particular, existing 40  frameworks are typically intended to assist in visualization and exploration of knowledge by knowledge engineers, rather then exploration of data by domain end users (Jones, 1988; Eisenstadt, 1990).
There also has been considerable useful work on model-based design of graphic user interfaces (Puerta, 1997).
Such work considers domain ontologies and properties in designing a specific interface for domain applications.
There has also been very interesting work on  automated, task-analytic design of user interfaces, which considers domain properties as well as the desiderata of different tasks in the application domain (Casner, 1991).
Our approach,  however, is quite different.
We have designed a visualization and exploration interface that is not specific to any application domain (apart from certain domain- and user-specific customizations), but that is specific only to a well defined set of tasks involving exploration of time-oriented data and its abstractions.
Furthermore, the approach is specific to a particular problem-solving method (the knowledge-based temporal-abstraction method) and its domainindependent method ontology.
Thus, we are not attempting to design automatically the user interface based on analysis of either the task (e.g., temporal abstraction) or the application domain (e.g., oncology).
Instead, the temporal-exploration task has been analyzed in our  methodology ahead of time, and the domain's ontology is mapped to the terms of the formal ontology of the knowledge-based visualization and exploration method (an extension of the ontology of the knowledge-based temporal-abstraction method).
The semantics of that problemsolving method also determine the meaning of most of the exploration operators.
Therefore, the user interface is essentially similar (except for group and individual preferences) in all domains in which the temporal-exploration task needs to be solved.
The drawback of our task-specific approach is therefore a trade off: We do not and cannot design new interfaces on the fly for specific application domains, or use any domain-specific 41  (rather than task-specific) graphical paradigms.
Examples might include drawing pediatric  growth charts as nonlinear curves, as they are often used by pediatricians to record the raw data, or using helices and other domain-specific conventions to represent biomolecular information such as DNA coils.
However, once data are abstracted into intervals and presented in the context of the temporal-abstraction (or any other linear-abstraction) task, even when the raw data use domain-specific graphics, they might be readily understood by domain experts, as our experience has shown in the domain of monitoring of children's growth (Kuilboer et al., 1993).
We can summarize and highlight the main features of our methodology in somewhat more detail, using terminology that has been found useful in the knowledge-acquisition research area.
We have presented in this paper a conceptual and computational knowledge-based framework for interactive visualization and exploration of time-oriented data and their multiple levels of temporal abstractions that is quite different from previous work.
In this framework, the  semantics of the querying, visualization, exploration, and explanation operators are defined by the domain-independent ontology of the knowledge-based temporal-abstraction problem-solving method.
The semantic-exploration operators allow the runtime user to explore visually the domain-specific temporal-abstraction (sub) ontologies, thereby leading to reciprocal visual exploration through the multiple levels of temporal abstractions of the particular time-oriented database that is queried.
Thus, although the KNAVE interface is essentially the same (apart from certain domain- and user-dependent customizations) in every domain, and the semantics of the syntactic and semantic exploration operators are identical, the resultant browsing process is specialized to the domain's parameter, event, context, and abstraction-goal (sub) ontologies, which the knowledge-based temporal-abstraction ontology comprises.
The visualization and exploration modules of the KNAVE architecture and the inference structure underlying them can therefore be viewed as part of a new problem-solving method, 42  specific to the task of visualization of and exploration through time-oriented data and its temporal abstractions.
The ontology of this knowledge-based visualization and exploration method is a superset of the ontology of the knowledge-based temporal-abstraction problemsolving method, augmenting it with additional knowledge such as new classes (e.g., statistical abstractions) and properties (e.g., visualization knowledge, such as what temporal resolution range is relevant for the visualization of each parameter) that are specific to the interactivevisualization task.
The domain-specific temporal-abstraction knowledge acquired from domain experts is thus reused by a different problem-solving method, which solves a different task (visualization and exploration, rather than temporal abstraction) and is shared by different applications within the same domain.
The new method also reuses the computational  mechanisms of the knowledge-based temporal-abstraction problem-solving method and augments them by the visualization and exploration mechanisms.
Note that, using the concept of syntactic and semantic (knowledge-based) exploration operators, we can more accurately define, conceptually and practically, several categories of composite exploration operators, each of which can be defined as a specific combination of semantic-exploration and syntactic temporal-exploration operations.
For instance, one variation of what has often been called "drilling down the data" in certain information-visualization systems, could be more formally defined as a composite exploration operator that consists of a syntactic temporal zoom (simple magnification) combined with moving along an ABSTRACTEDFROM  semantic link.
Other types of "zooming," differing significantly from each other, can  likewise be easily defined.
The applicability of the RESUME system to strikingly different time-oriented domains such as guideline-based medical care, monitoring of children's growth, therapy of diabetes, and traffic control suggests that the new knowledge-based visualization methodology will be widely 43  applicable as well.
Furthermore, the abstraction experiments within the traffic-control domain (Shahar and Molina, 1998) have demonstrated that the knowledge-based temporal-abstraction methodology is useful for acquisition and maintenance of other types of linear-abstraction knowledge, such as spatial-abstraction knowledge.
These experiments suggest similar  applicability of the knowledge-based visualization methodology to domains that employ linear distance measures and to the semiautomated (interactive) support of tasks such as monitoring, planning, explanation, and data mining.
The evaluation of the final reengineered KNAVE system in several different domains will provide further insights into these intriguing hypotheses.
7 FINAL NOTES: CURRENT AND FUTURE WORK Based on the conceptual and practical insights we have gained from our work on the KNAVE architecture, its preliminary prototype, and its developmental evaluation, we are currently in the process of reimplementing our visualization and exploration module prototype (originally implemented in Visual Basic) using the Java programming language.
The architecture, however, has not changed, neither did the semantics of the visualization and exploration operators.
The KNAVE architecture comprises three main conceptual components (see Figure 2).
Thus, communication links exist between the knowledge-based visualization and exploration module, the domain ontology server, and the temporal-database mediator, with an integrated view in mind (Wiederhold et al., 1986).
The communication protocols support both the static and dynamic data (raw input), information (abstracted conclusions) and knowledge (from the domain's temporal-abstraction ontology) requirements of the KNAVE system.
The communication links to both the ontology server and the database have been implemented using the common object request broker architecture (CORBA) standard.
The CORBA standard is a specification standard, increasingly adapted by the software industry, that describes how software components 44  can interoperate across networks, languages, and platforms.
The use of CORBA makes it straightforward for the components that we are using to be developed in various programming languages, to reside on different computers, and to communicate with one another over the Internet.
The CORBA standard provides an standard interface that allows arbitrary software modules to pass data to the knowledge-based components, and to receive the resulting inferences in a transparent fashion.
In the medical domain, users select the patient record to be explored and the relevant domain temporal-abstraction knowledge base to be used to interpret that record (e.g., protocol-based care or diabetes).
Selection of the knowledge base enables runtime creation of a domain-specific temporal-abstraction browser (a somewhat compact version of the seven browsers seen in Figure 4) and thus formulation of a domain-specific query.
The results are displayed in the visualization window.
The most common exploration operators (e.g., drilling down the abstraction hierarchy) are part of the window, so users need only return to the full domain browser for significantly new queries.
New queries (or explorations) add new panels to the same window, unless the user elects to open a new window.
Given the importance of feedback, we are performing developmental assessments and evaluations from the very beginning of the final implementation of the KNAVE architecture, using our collaborators in several medical domains and clinical data sets, several of which we already have, and several of which our collaborators supply as necessary (e.g., a large data set in the bone-marrow transplantation domain).
We are also evaluating the overall KNAVE architecture within EON, an architecture for guideline-based medical care (Musen et al., 1996).
EON is being applied to domains such as protocol-based care for AIDS patients and oncology.
45  Our work in the traffic-control domain (Shahar and Molina, 1998) potentially provides a significantly different domain for assessing the effectiveness of our domain-independent methodology.
It is our hypothesis that our methodology is useful in most domains in which data are captured over a linear distance measure (such as uni-dimensional space along each highway, and the evolution of the spatial abstractions over time), and in which there exist several levels of abstraction of the data.
Once the final reengineering of KNAVE is done, we will perform a larger and more formal evaluation of the KNAVE framework, including more users, qualitative and quantitative questionnaires, "thinking aloud" experiments, inclusion of well-defined target user behaviors, and application of standard methods for interface usability assessments (Gould, 1988).
We will try to assess the benefits of the new methodology versus browsing the data on paper or in a standard online format such as an electronic spreadsheet.
Our ultimate vision is to enable exploration of any distributed time-oriented database (e.g., on the internet), using an appropriate existing temporal-abstraction ontology (e.g., a knowledge base located somewhere else on the World Wide Web), from any visualization and exploration workstation.
Needless to say, this vision requires acquisition of significant amounts of temporalabstraction knowledge.
We have taken the first steps towards formalization and acquisition of that knowledge in our previous work, and are now utilizing it to enable exploration of the resultant temporal abstractions.
Acknowledgements This work has been supported by grants LM06245 from the National Library of Medicine and IRI-9528444 from the National Science Foundation.
Drs.
Daniel Stites, Lawrence Basso, Darrel Wilson, and Herbert Kaizer provided input regarding the design of the preliminary interfaces for 46  both the graphical knowledge-acquisition tool and the KNAVE visualization and exploration module, and many users tested our prototype in the evaluation study.
Dr. Angel Puerta provided useful references and advice in the preliminary evaluation phase.
References ALLEN, J.F.
1984.
Towards a general theory of action and time.
Artificial Intelligence 23: 123- 154 .
ARENS, Y., L. MILLER, S. C. SHAPIRO, and N. K. SONDHEIMER.
1994.
Automatic construction of user-interface displays.
In Proceedings of the Seventh National Conference on Artificial Intelligence, St. Paul, Minnesota, pp.
808-813.
BECKER, B. G. 1997.
Using MineSet for knowledge discovery.
IEEE Computer Graphics and Applications, 7/8: 75-78.
BLUM, R.L.
1982.
Discovery and representation of causal relationships from a large timeoriented clinical database: The RX project.
In Lindberg, D.A.
and Reichartz, P.L.
eds, Lecture Notes in Medical Informatics, volume 19, New York: Springer-Verlag.
CARPENDALE, M. S. T., D. J. COWPERTHWAITE, and F. D. FRACCHIA.
1997.
Extending distortion viewing from 2D to 3D.
IEEE Computer Graphics and Applications, 7/8: 42-51.
CASNER, S. M. 1991.
A task analytic approach to the automated design of graphic presentations.
ACM Transactions on Graphics 10(2):, 111-151 CHAKRAVARTY, S., and Y. SHAHAR.
1999.
A constraint-based specification of periodic patterns in time-oriented data.
In Proceedings of the 1999 Sixth International Workshop on Temporal Representation and Reasoning Time-99, Orlando, Florida, pp.
29-40.
CHANDRASEKARAN, B.
1986.
Generic tasks in knowledge-based reasoning: High-level building blocks for expert system design.
IEEE Expert, 1: 23-30.
CHENG, C., Y. SHAHAR, A. R. PUERTA, and D. P. STITES.
1997.
Exploration and visualization of abstractions of time-oriented clinical data.
Section on Medical Informatics Technical Report No.
SMI-97-0688, Stanford University, CA.
CLANCEY, W. J.
1985.
Heuristic Classification.
Artificial Intelligence, 27: 289-350.
COUSINS, S. B., and M. G. KAHN.
1991.
The visual display of temporal information.
Artificial Intelligence in Medicine, 3: 341-357.
DAS, A. K., and M. A. MUSEN.
1994.
A temporal query system for protocol-directed decision support.
Methods of Information in Medicine, 33(4): 358-370.
DAS, A. K., Y. SHAHAR, S. W. TU, and M. A. MUSEN.
1994.
A temporal-abstraction mediator for protocol-based decision support.
In Proceedings of the Eighteenth Annual Symposium on Computer Applications in Medical Care, Washington, DC, pp.
320-324.
DE ZEGHER-GEETS, I. M. 1987.
IDEFIX: Intelligent summarization of a time-oriented medical database.
M.S.
Dissertation, Medical Information Sciences Program, Stanford University 47  School of Medicine, June 1987.
Knowledge Systems Laboratory Technical Report KSL-8834, Department of Computer Science, Stanford University, Stanford, CA.
DOWNS, S. M., M. G. WALKER, and R. L. BLUM.
1986.
Automated summarization of on-line medical records.
In MEDINFO '86: Proceedings of the Fifth Conference on Medical Informatics, Edited by R. Salamon, B. Blum, and M. Jorgensen, North-Holland, Amsterdam, pp.
800-804.
EISENSTADT, M., J. DOMINGUE, T. RAJAN, and E. MOTTA.
1990.
Visual knowledge engineering.
IEEE Transactions on Software Engineering, 116(10): 1164-1177.
ERIKSSON, H., Y. SHAHAR, S. W. TU, A. R. PUERTA, and M. A. MUSEN.
1995.
Task modeling with reusable problem-solving methods.
Artificial Intelligence, 79(2): 293-326.
FAGAN, L. M. 1980.
VM: Representing Time-Dependent Relations in a Medical Setting.
Ph.D. dissertation, Department of Computer Science, Stanford University, Stanford, CA.
GENESERETH, M. R., and R. E. FIKES.
1992.
Knowledge Interchange Format, Version 3.0 Reference Manual.
Technical Report Logic-92-1, Computer Science Department, Stanford University, Stanford, CA.
GENNARI, J. H., S. W. TU, T. E. ROTHENFLUH, and M. A. MUSEN.
1994.
Mapping domains to methods in support of reuse.
International Journal of Human-Computer Studies.
41: 1994 399-424.
GIARRATANO, J., and G. RILEY.
1994.
Expert Systems: Principles and Programming.
Boston, MA: PWS Publishing Company.
GNANAMGARI, S. 1981.
Information Presentation Through Default Displays.
Ph.D. dissertation, University of Pennsylvania.
GOULD, J. D. 1988.
How to design usable systems.
In Handbook of Human-Computer Interaction.
Edited by M. Helander, Elsevier Science Publishers, pp.
757-789.
GRUBER, T. G. 1993.
A translation approach to portable ontology specification.
Knowledge Acquisition, 5: 117-243.
HAIMIOWITZ, I. J., and I. S. KOHANE.
1993.
Automated trend detection with alternate temporal hypotheses.
In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, San Mateo: Morgan Kaufmann, pp.
146-151.
JONES, S. 1988.
Graphical interfaces for knowledge engineering: An overview of relevant literature.
Knowledge Engineering Review, 3(3), 221-248.
KAHN, M. G., C. A. ABRAMS, S. B. COUSINS, J. C. BEARD, and M. E. FRISSE.
1990.
Automated interpretation of diabetes patient data: Detecting temporal changes in insulin therapy.
In Proceedings of the Fourteenth Annual Symposium on Computer Applications in Medical Care.
Edited by R. A. Miller.
Los Alamitos: IEEE Computer Society Press, pp.
569-573.
KAHN, M. G. 1991.
Combining physiologic models and symbolic methods to interpret timevarying patient data.
Methods of Information in Medicine, 30: 167-178.
KOHANE, I. S. 1987.
Temporal reasoning in medical expert systems.
Technical Report 389, Laboratory of Computer Science, Massachusetts Institute of Technology, Cambridge, MA.
KOLOJEJCHICK J., S. F. ROTH, and P. LUCAS.
1997.
Information Applications and Tools in Visage.
IEEE Computer Graphics and Applications, 7/8: 32-41.
48  KUILBOER, M. M., Y. SHAHAR, D. M. WILSON, and M. A. MUSEN.
1993.
Knowledge reuse: Temporal-abstraction mechanisms for the assessment of children's growth.
In Proceedings of the Seventeenth Annual Symposium on Computer Applications in Medicine, Washington, DC, pp.
449-453.
KUMAR, H.P., C. PLAISANT, and B. SHNEIDERMAN.
1997.
Browsing hierarchical data with multi-level dynamic queries and pruning.
International Journal of Human-Computer Studies 46(1) 103-124.
LARKIN, J., and H. SIMON.
1987.
Why a diagram is sometimes worth 10,000 words.
Cognitive Science, 11: 65-99.
LARIZZA, C., A. MOGLIA, and M. STEPHANELLI.
1992.
M-HTP: A system for monitoring hearttransplant patients.
Artificial Intelligence in Medicine, 4: 111-126.
MACKINLAY J., G. G. ROBERTSON, and S. K. CARD.
1994.
The perspective wall: Detail and context smoothly integrated.
In Proceedings of CHI '94, pp.
173-179.
MCDERMOTT, J.
1988.
Preliminary steps toward a taxonomy of problem-solving methods.
In Automating Knowledge-Acquisition for Expert Systems.
Edited by S. Marcus.
Boston: Kluwer, pp.
225-256.
MIKSCH, S., Y. SHAHAR, and P. D. JOHNSON.
1997.
Asbru: A task-specific, intention-based, and time-oriented language for representing skeletal plans.
In Proceedings of the Seventh Workshop on Knowledge Engineering Methods and Languages KEML-97, Open University, Milton Keynes, UK.
MOLINA, M., and Y. SHAHAR.
1996.
Problem-solving method reuse and assembly: From clinical monitoring to traffic control.
In Proceedings of the Tenth Banff Knowledge Acquisition for Knowledge-based systems Workshop, Banff, Alberta, Canada, Vol.
1, pp.
7-1--7-20.
MUSEN, M.A.
1989.
Automated Generation of Model-based Knowledge-Acquisition Tools.
Morgan Kaufmann, San Mateo, CA.
MUSEN, M.A.
1992.
Dimensions of knowledge sharing and reuse.
Computers and Biomedical Research 25, 435-467.
MUSEN, M.A., J. H. GENNARI, H. ERIKSSON, S. W. TU, and A. R. PUERTA.
1995.
PROTEGEII: Computer support for development of intelligent systems from libraries of components.
In MEDINFO '95: Proceedings of the Eighth World Congress on Medical Informatics, Vancouver, British Columbia, pp.
766-770.
MUSEN, M. A., S. W. TU, A. K. DAS, and Y. SHAHAR.
1996.
EON: A component-based approach to automation of protocol-directed therapy.
Journal of the American Medical Association, 3(6): 367-388.
MUSEN, M. A.
1998.
Domain ontologies in software engineering: Use of Protege with the EON architecture.
Methods of Information in Medicine, 37: 540-550.
NEWELL, A.
1982.
The knowledge level.
Artificial Intelligence, 18: 87-127.
NGUYEN, J., Y. SHAHAR, S. W.
TU., A. K. DAS, and M. A. MUSEN.
In press.
Integration of temporal reasoning and temporal-data maintenance into a reusable database mediator to answer abstract, time-oriented queries: The Tzolkin system.
Journal of Intelligent Information Systems.
49  PLAISANT, C., B. MILASH, A.
ROSE, S. WIDOFF, and B. SHNEIDERMAN.
1996.
Lifelines: Visualizing Personal Histories.
Proceedings of CHI '96, Vancouver BC, pp.
221-227.
POWSNER, S. M. and E. R. TUFTE.
1994.
Graphical Summary of Patient Status.
Lancet, 344: 386-389.
PUERTA, A. R., J. W. EGAR, S. W. TU, and M. A. MUSEN.
1992.
A multiple-method knowledgeacquisition shell for the automatic generation of knowledge-acquisition tools.
Knowledge Acquisition, 4: 171-196.
PUERTA, A.
1997.
A model-based interface development environment.
IEEE Software, 14(4): 40-47.
RAO, R., S. CARD, H. JELLINEK, J. MACKINLAY, and G. ROBERTSON.
1992.
The Information Grid: A Framework for Information Retrieval and Retrieval-Centered Applications.
In Proceedings of the ACM Symposium on User Interface Software and Technology '92, Monterey, CA, pp.
23-32.
ROHRER, R., and E. SWING.
1997.
Web-based information visualization.
IEEE Computer Graphics and Applications, 7/8: 52-59.
ROTH, S. F., J. KOLOJEJCHICK, J. MATTIS, and J. GOLDSTEIN.
1994.
Interactive graphic design using automatic presentation knowledge.
In Proceedings of CHI '94, pp.
112-117.
RUSS, T.A.
1989.
Using hindsight in medical decision making.
In Proceedings of the Thirteenth Annual Symposium on Computer Applications in Medical Care, Washington, D.C. Edited by L. C. Kingsland, IEEE Computer Society Press, Los Alamitos, CA, pp.
38-44.
SHAHAR, Y., S. W. TU, and M. A. MUSEN.
1992.
Knowledge acquisition for temporalabstraction mechanisms.
Knowledge Acquisition, 4: 217-236.
SHAHAR, Y., and M. A. MUSEN.
1993.
RESUME: A temporal-abstraction system for patient monitoring.
Computers and Biomedical Research, 26:, 255-273.
Reprinted in Yearbook of Medical Informatics 1994.
Edited by J. H. van Bemmel, and A. T. McRay, F.K.
Schattauer and The International Medical Informatics Association, Stuttgart, Germany, pp.
443-461.
SHAHAR, Y., and M. A. MUSEN.
1996.
Knowledge-based temporal abstraction in clinical domains.
Artificial Intelligence in Medicine 8 3, 267-298.
SHAHAR, Y.
1997.
A framework for knowledge-based temporal abstraction.
Intelligence, 90(1-2): 79-133.
Artificial  SHAHAR, Y.
1998.
Dynamic temporal interpretation contexts for temporal abstraction.
Annals of Mathematics and Artificial Intelligence, 22(1-2): 159-192.
SHAHAR, Y., and M. MOLINA.
1998.
Knowledge-based spatiotemporal abstraction.
Pattern Analysis and Applications, 1(2): 91-104.
SHAHAR, Y., S. MIKSCH, and P. D. JOHNSON.
1998.
A task-specific ontology for the application and critiquing of time-oriented clinical guidelines.
Artificial Intelligence in Medicine, 14: 2951.
SHAHAR, Y.
1999.
Knowledge-based temporal interpolation.
Journal of Experimental and Theoretical Artificial Intelligence, 11: 123-144.
50  SHAHAR Y., H. CHEN, D. P. STITES, L. M. BASSO, H. KAIZER, D. M. WILSON, and M. A. MUSEN.
In press.
Semiautomated acquisition of clinical temporal-abstraction knowledge.
The Journal of the American Medical Informatics Association.
SHNEIDERMAN, B.
1994.
Dynamic Queries for Visual Information Seeking.
IEEE Software, 11(6): 70-77.
SHOHAM, Y.
1987.
Temporal logics in AI: Semantical and ontological considerations.
Artificial Intelligence, 33: 89-104.
SNODGRASS, R., and I. AHN.
1986.
Temporal databases.
IEEE Computer, 19: 35-42.
STEIN, A., M. A. MUSEN, and Y. SHAHAR.
1996.
Knowledge acquisition for temporal abstraction.
Proceedings of the 1996 AMIA Annual Fall Symposium formerly the Symposium on Computer Applications in Medical Care, Washington, DC, pp.
204-208.
TUFTE, E. R. 1982.
The Visual Display of Quantitative Information.
Graphics Press, CT. TUFTE, E. R. 1990.
Envisioning Information .
Graphics Press, CT. TUFTE, E. R. 1997.
Visual Explanations.
Graphics Press, CT. TU, S. W., M. G. KAHN, M.A.
MUSEN, J.C. FERGUSON, E.H. SHORTLIFFE, and L.M.
FAGAN, Episodic skeletal-plan refinement based on temporal data, Communications of the ACM 3212 1989 1439-1455.
TU, S. W., Y. SHAHAR, J. DAWES, J. WINKLES, A. R. PUERTA, and M. A. MUSEN.
1992.
A problem-solving model for episodic skeletal-plan refinement.
Knowledge Acquisition, 4: 197-216.
TU, S. W., H. ERIKSSON, J. H. GENNARI, Y. SHAHAR, and M. A. MUSEN.
1995.
Ontology-based configuration of problem-solving methods and generation of knowledge-acquisition tools: Application of PROTEGE-II to protocol-based decision support.
Artificial Intelligence in Medicine, 7(3), 257-289.
WIEDERHOLD G., R. L. BLUM and M. G. WALKER.
1986.
An integration of knowledge and data representation.
In On Knowledge Base Management Systems: Integrating Artificial Intelligence and Database Technologies.
Edited by M. L. Brodie, J. Mylopoilos, and J. W. Schmidt, Springer-Verlag, pp.
431-444.
WIEDERHOLD G. 1992.
Mediators in the architecture of future information systems.
IEEE Computer, 3: 38-49.
WEILINGA, B., A. T. SCHREIBER, and J. BREUKER.
1992.
KADS: a modeling approach to knowledge engineering.
Knowledge Acquisition, 4: 5-53.
WRIGHT, W. 1997. Business visualization applications.
Applications.
7/8: 66-70.
IEEE Computer Graphics and  ZHOU, M., and S. FEINER.
1996.
Data characterization for automatically visualizing heterogeneous information.
In Proceedings of Information Visualization '96, pp.
13-20.
51
Qualitative and Quantitative Temporal Constraints About Numerically Quantified Periodic Events Paolo Terenziani Dipartimento di Informativa, Universita' di Torino Corso Svizzera 185, 10149 Torino, Italy Phone: +39 117429244; E-Mail: terenz@di.unito.it  Abstract  facilities to deal with user-defined calendric definitions (see, e.g., [3,9, 15, 161).
On the other hand, many approaches in AI focused on the treatment of qualitative relations between periodic events (see, e.g., [7,8,10,11, 12, 13, 22]), such as the "after" relation in Ex.2 Ex.2 "Each correction of the test of French (for the class the test" 111-A) is In particular, following Allen's approach to the treatment of qualitative relations [11 (between non-periodic events), also some approaches to periodic events are algebraic approaches,in which (i) a specialised formalism is devised to represent temporal information about periodic events and (ii) the operations of intersection and composition (which must be closed with respect to the representation formalism) are defined; path-consistency algorithms repeatedly applying intersection and composition are then used in order to perform temporal reasoning (consider, e.g., [ l l , 12, 13, 221).
However, none of the previous approaches took into account the fact that, in general, qualitative relations are not "absolute", but only holds in a specific frame time (e.g., "between 11-9-95 and 14-6-96" in Ex.3) and in a given period (e.g., "first Tuesday of the month", in EX.^), so that one could consistently assert, e.g., Ex.3 and Ex.3': Ex.3 "Between 11-9-95 and 14-6-96, each first T u e s w of the month the class 111-A had a test of French before an hour of Mathematics" Ex.3' "Between 11-9-95 and 14-6-96, each last Tuethe month the class 111-A had a test of French & an hour of Mathematics" The goal of our work is that of providing an Allen'slike algebraic approach dealing with these and others (see below) phenomena arising when dealing with periodic events.
In [17,20], we proposed an approach dealing with frame times, periods and qualitative relations between periodic events.
In this paper we address the extensions needed in order to face the following 3 tasks: (1)Dealing with numeric quantifiers stating the number of repetitions of events.
Numeric quantifiers are pervasive  The paper describes an integrated temporal formalism which deals with ( i ) quantitative information about the frame of time and the user-defined calendar-dates (periods) in which periodic events are located, (ii) (possibly multiple) numeric quantifiers indicating the number of repetitions of events and (iii) qualitative relations between periodic events.
The paper defines the operations of intersection and composition of temporal specifications in the given formalism, which are used in order to perform temporal reasoning.
An algorithm supporting specialised forms of reasoning about the number of repetitions of events is also described.
Moreover, the paper introduces an expressive query language for extracting diflerent types of temporal constraints from a knowledge base of temporal specifications in the formalism, sketching the reasoning algorithms needed to answer the queries.
1.
Introduction The interest towards the treatment of periodic (repeated [5,11 I) events is rapidly increasing in the scientific community.
In particular, periodic events are widely studied in temporal databases (TDB) and in Artificial Intelligence (AI).
In fact, periodic events are involved in many "intelligent" activities, such as planning, scheduling, workflow analysis, office automation.
A main problem in the treatment of periodic events is that different types of temporal information have to be taken into account.
For example, in TDB, most attention is devoted to the treatment of calendar-dates(see, e.g., [2, 3, 4, 6, 14, 15, 161), which represent the period upon which periodic events occur (e.g., "Every first Tuesday of the month" in Ex.1).
Ex.1 "Everv first Tuesdav.
of the month the class 111-A has a test of French" In particular, since different calendric systems are used for specifying the periods (depending e.g., from cultural and social factors [16]), many approaches provided  94  0-8186-7937-9/97 $10.00 0 1997 IEEE  in such a way that temporal constraints such as those discussed in the introduction can be expressed, and pathconsistency on a set of temporal specifications in our formalism can be computed in polynomial time.
Our formalism provides three basic types of temporal specifications, which can be expressed according to the simplified syntax shown below (" ( )" indicates optionality; the syntax in this paper has been simplified, to the sake of clarity and brevity.
We also provide "syntactic sugar" to help the user; e.g., to avoid that s h e has to specify many times the same frame-time -or period- for different specifications).
<TEMPORAL-SPECIFICATION> ::= <LOC> I <QUAL> I <LOC-QUAL> <LOC> ::= <Frame> <Periodic-Event> (<Num>) EACH <period-Name> cQUAL>::=cFrame> EACH (mum>) <Periodic-Event> <Qual-Rel> (<Num>)<Periodic-Eveno <LW-QUALx:= <Frame> { <Num>) EACH <Period-Name> (<Num>) <Periodic-Eveno CQual-Reb (<Num>) <Periodic-Event>  in activities such as planning, scheduling, work flow analysis, manufacturing, office automation.
In this paper we deal with a wide range of uses of numeric quantifiers: (i) to state the number of repetitions of events in a given period (and at a given frame time), as, e.g., in Ex.4, (ii) to state the cardinality of the collections of instances of two given periodic events which are in a given qualitative relations, such as in "to open the strong-room, turn the key lhree times to the right and then lwo times to the left", or in Ex.5.
Moreover, (iii) we also allow the use of both types of constraints in the very same temporal specifications,see, e.g., Ex.6 Ex.4 "Between 11-9-95and 14-6-96,the class 111-A had an hour of Mathematics lwo times each Monday" Ex.5 "Between 11-9-95and 14-6-96,each Tuesday the class 111-A had an hour of Mathematics before two hours of Physics" .
each Ex.6 "Between 11-9-95and 14-6-96,*three omes week the class 111-A had &&?hours of Physics before hours of Chemistry" (2) Supporting specialised forms of reasoning about the number of repetitions of periodic events in periods.
<Frame> indicates a frame of time (time interval) ranging from a starting point to anending point, and is represented by a pair of dates (e.g., [ll-9-95,14-6-96]).
cNum> is a numeric quantifier representing the exuct number of repetitions of the events (in the form "ntimes").
It can be omitted; in such a case, it assumes the default value "exactly once".
<Periodic-Event> is the representation of an event repeated in time.
The algebraic approach we propose is independent of the representation of periodic events (for the sake of clarity, in the following we simply associate an identifier to each periodic event e.g., "correction-testFrench-111-A -see [18] for an alternative representation in first order logic).
cQual-Rel> is a qualitative relation between the temporal extent of two periodic events, expressed using any relation in Allen's Interval Algebra [ll.
<Period-Name> is a user-defined identifier of a period (calendric definition; e.g., " 1st-Tuesdays-ofMonths*").
In our approach, the definitions of the periods must be provided by the user using a slight adaptation of Leban's language [9], which we discussed in 1201.
The specifications 01 type <LO& allow one to deal with the quantitativetemporal constraints; they relate a periodic event to the period and frame time in which it happens.
For example, (given a definition of the Period-Name "Mondays*" in Leban's language), the temporal constraints in Ex.4 can be represented as shown in (Sl), with the meaning that, for each week strictly contained in the frame timc "between 11-9-95 and 14-69 6 , there are exactly two instances of an hour of lesson of Mathematics of the class 111-A (identifier H-Muth-lll_A*)  (3) Answering different types of queries.
Despite the fact that querying is the most common way of interacting with a temporal manager, the problem of providing a rich query language and of supporting the reasoning procedures needed in order to answer queries has often been only marginally considered in the AI literature.
The paper is organised as follows.
In section 2, we introduce our formalism dealing with periods, numeric quantifiersand qualitativerelations between periodic events and we define the operations of intersection and composition used by a path-consistency algorithm to perform temporal reasoning.
Section 3 describes an algorithm for reasoning about the number of repetitions of events.
Section 4 deal with different types of temporal queries, and sketches the reasoning algorithms to deal with them.
Section 5 proposes comparisons and conclusions.
In this paper, we describe the semantics of our high-level language in an informal way.
In [lS] we defined a logic for dealing with the semantics of temporal constraints about periodic events.
In [19] we applied such a logic to state in a formal (logical) way the meaning of the temporal specifications of the high-level formalism introduced in this paper, and to prove the corretness of our algebraic operations (see Property 1 in section 2.2.).
These issues are not dealt with in this paper, for the sake of brevity.
Temporal constraints about numerically quantified periodic events 2.
2 .
1 The temporal formalism Our "high-level"temporal formalism has been defined  95  First of all, no intersection or composition must be performed in the non-intersccting parts of the frame times.
Moreover, the result of intersection and composition of two specifications S' and S" in the common part of the frame times depends on the relation holding between the periods in S' and in S" and on the quantifiers.
Consider, for instance, the intersection of (S4) and (S5).
The AFTER relation in (S4) is not possible between 11-9-95 and 14-6-96,and must be ruled out.
More specifically,the intersection of (S4) and ( S 5 ) gives as result the specifications (S6), (S7), and (S8).
However, in case we substitute the period Mondays* instead of Tuesdays* in (S5), the intersection would provide as result the two input specifications unchanged.
Also quantifiers must be checked; e.g., in case we substitute EACH Tuesdays* 2times test-French-111-A* instead of EACH Tuesdays* test-French-111-A* in (S5), we have an inconsistency.
Thus, we cannot propose a compact definition of intersection and composition such as in [1,11,12,13]: we need a definition by cases based on the relation holding between the periods in the specifications and on the quantifiers.
contained in it (in this paper, we choose to associate an event to each hour of lesson of a given class).
[11-9-95,14-6-%I H-Marh-III-A* 2-times (S 1) EACH Mondays* <QUAL> specifications deal with "periodindependent" qualitative temporal relations between periodic events (in a given frame time).
They can be used to express "absolute" (independent of the period) temporal constraints in a given frame time.
E.g., the temporal constraint in Ex.2 can be represented in our high-level language as shown in (S2), with the meaning that, in the frame time (-=,+w), there is a bijective 1:l relation (called c o r r e l a t i o n [12,17,20]) between instances of correction-test-French-III-A* and of test-French-III-A* such that the temporal relation AFTER holds between each correlatedpair.
(S2) (-m,+-) EACH correction-test-French-III-A* (AFTER) test-French-III-A* cLOC-QUAL> specifications deal with "perioddependent" qualitative relations between periodic events, i.e., qualitative relations holding at specific periods of time (i.e., with mixed qualitative and quantitative temporal constraints).
E.g., the temporal constraints in Ex.6 can be represented in our formalism as shown in (S3).
(S3) [ll-9-95,14-6-96] 3-times EACH Weeks* 2-times H-Phys-III-A* (BEFORE) 2-times H-Chem-III-A* The meaning of (S3) is that there is a bijective relation (correlation) holding between collections of two instances of H-Phys-III-A* and collections of two instances of H-Chem-III-A* such that, for each instance of Weeks* during [I 1-9-95,14-6-96]there are exactly three correlated pairs of a collection of two instances of H-Phys-III-A* and a collection of two instances of HChem-III-A*, and the temporal relation between the temporal extent of each correlated pair is AFTER.
We define the temporal extent of a collection C=(el, ...,en) of n instances of an event as the minimal convex time interval covering the temporal extents (time intervals) of el.
...,en [18].
.
(S4) [ll-9-95,31-7-96]EACH 1st-Tuesdays-of-Months* test-French-III-A* (BEFORE,AFTER) H-Math-III-A* ( S 5 ) [I 1-9-95,14-6-96]EACH Tuesdays* test-French-III-A* (BEFORE,MEETS)H-Math-III-A* (S6) [I 1-9-95,146961EACH Ist-Tuesdays-of-Months* test-French-111-A* (BEFORE) H-Math-III-A* (S7) [ll-9-95,14-6-96]EACH Tuesdays* test-French-III-A* (BEFORE,MEETS)H-Math-III-A* (S8) (14-6-96,31-7-961EACH 1st-Tuesdays-of-Months* test-French-III-A* (BEFORE,AFER) H-Math-III-A* 2.2.1 Relations between periods.
In [17,20] we distinguished among six basic relations between periods, plus their inverses (indicatcd by - l ; the inverse of =T is =T; the inverse of # is #), which are exhaustive and mutually exclusive.
In the following, we sketch the six relations (C1* and C2* indicates two periods): C1* =T C2* (read as: C1* and C2* are temporally equal) iff there is a bijection between instances of C1* and instances of C2*, and Allen's relation EQUAL holds between each pair of corresponding instances.
C1* 4 C2* (Cl* is more specific than C2*) iff for each instance of C1* there is exactly one instance of C2* which properly contains i t and, conversely, for each instance of C2* there is exactly one instance of C1* which is properly contalned in it (bijection) (e.g., Tuesdays* 4 Weeks*).
C1* C C2* (Cl* is a rcstriction of C2*) iff for each instance of C1* there is ;in instance of C2* which is temporally equal to it, but not vice versa (e.g., Mondays* E Days*).
2 .
2 Temporal reasoning As in many AI approaches to qualitative temporal constraints (see, e.g., the survey in 121]), in our proposal temporal reasoning is performed by the algebraic operations of inversion (which is not discussed here, for the sake of brevity), intersection and composition.
However, we also consider periods, frame times and numeric quantifiers.
Thus, we had to face new problems when defining intersection and composition (see [ 17,201 for a detailed discussion; for the sake of brevity, in this section we only consider temporal specifications of type cLOC-QUAL>; the treatment of the other types of specification is simpler, and can be found in [19]).
96  7 and 14 times each week respectively (but we have an inconsistency if we substitute 5 to 7 in (S12)).
(S12) [ 11-9-95,14-6-96]7-times EACH Weeks* H-Math-III-A* (BEFORE) 2-times H-Phys-III-A* (S13) [ll-9-95,14-6-%I EACH Days* H-Math-III-A* (BEFORE) 2-times H-Phys-III-A* The definition of intersection of two specificationsof the form of (S9) and (S10) above is summarised in Table l(a); R indicates the intersection between the Interval Algebra relations R1 and R2 and RF(Cl*,C2*) indicates the relative frequency of C 1* with respect to C2* .
Composition operatcs on two cLOC-QUAL> specifications of the form of (S 14) and (S 15) and checks the consistency of the number of repetitions of the common event (b* in (S14)-(S15)).
(S14)I nz-times EACH C1* n1'-times a* R1 n3'4imes b* (S15) I n2"-times EACH C2* n1"-times b* R2 n3"-times d* If there is no inconsistency, and the numeric quantifiers in the two specifications are the same, new temporal specifications in our formalism can be inferred, depending on the relations between the periods C1* and C2*.
E.g., since Tuesdays* =T Tuesdays*, the composition of (S7) above and (S 16) gives as result (S 17) [l l-9-95,14-6-96] EACH Tue~days* (S16) H-Math-III-A* (BEFORE) 2-times H-Phys-III-A* (S17) [l l-9-95,14-6-%] EACH Tuesdays* test-French-III-A* (BEFORE) 2-times HPhys-III-A* On the other hand, the composition of (S7) and (S16) (obtained from (S 16) by substituting Weeks* to Tuesdays*), cannot provide any new specification expressible in our formalisni (this fact is indicated by "NO NEW SPECIFICATION" in Table l(b)).
In fact, it would not be correct to infer either (S18) or (S19).
In fact, (S18) states that H-Phys-III-A* occurred (exactly 2 times) each Tuesday (while from (S16) we only have that it occurred 2 times during the wcek); (S19) states that test-French-III-A* occurred exactly once each week (while from (S7) we only have that it occurred exactly once each Tuesday, so that it could also occur some other times, e.g., on Wednesday).
(S 16') 111-9-95,14-6-96]EACH Weeks* H-Math-III-A* (BEFORE) 2-times HPhys-III-A* (S18) [ll-9-95,14-6-%1 EACH Tuesdays* test-French-111-A* (BEFORE) 2-times HPhys-III-A* (S19) [ll-9-95,14-6-96]EACH Weeks* test-French-III-A* (BEFORE) 2-times H-Phys-III-A* The definition of composition of two specifications of the form of (S14) and (S15) is summarised in Table l(b), where R represents the composition of the qualitative relations R1 and R2 in Allrn's Interval Algebra.
Relation indicates the relation between the periods C1* and C2*.
CI* EINC c2* (CI* is an inclusion restriction of C2*) iff for each instance of C1* there is an instance of C2* which properly contains it, but not vice versa (e.g., Christmas* aZINc Months*).
C1* cI C2* (C1* is more frequent than C2* in the frame time I) covers the cases where two assertions such as (i) "evl* happens exactly once each Cl*" and (ii) "evl* happens exactly once each C2*" are inconsistent in a frame time I (e.g., Days* CIWeeks* in [l-1-91,l-6-91]).
C1* # C2* (C1* and C2* are temporally incomparable) iff none of the above relations (or their inverses) hold between C1* and C2* (e.g., Mondays* ## Tuesdays*).
In [20] we widely debated the rationale beyond such a distinction, and described a set of heuristic rules for determining automatically which one of the 10 (the 6 above plus 4 inverses) relations holds between two userdefined periods specified using Leban's formalism [9].
Such a set of heuristic rules is not complete, but it is powerful enough to cover non-exceptional cases (remember that, in our approach, the user is completely free in introducing new period definitions, using Leban's formalism), as we discussed in [20].
In case a relation between two user-defined periods is not discovered by the heuristic rules, such a relation is asked to the user.
2.2.2 Definitions of the operations.
Intersection operates on two cLOC-QUAL> temporal specifications of the form of (S9) and (S10) (I stands for the intersection of the frame times), and checks whether R, the intersection in Allen's Interval Algebra of the qualitative relations R1 and R2, is the empty set (in such a case, an inconsistency is reported) and the consistency of the numeric quantifiers, depending on the relations between C1* and C2*.
(S9) I n2'4mes EACH C1* n1'-times a* R1 n3'-times b* (S10) I nY-times EACH C2* n1"-times a* R2 n3"-times b* For instance, in case C1* =T C2* or C1* E C2*, the numeric quantifiers in the two specifications must be exactly the same.
Otherwise, (S9) and (S 10) would state a different number of instantiations of a* and/or b* in the given period, or a different grouping of these instances into collections.
For instance, (S3) above and (S 11) are inconsistent since they involve different groupings of instances of H-Phys-III-A* and H-Chem-III-A*.
(Sll) [ll-9-95,14-6-96] 2-times EACH Weeks* 3-times HPhys-III-A* (BEFORE) 3-times HChem-III-A* On the other hand, in case C1* c1C2* the relative frequency of the two periods have to be taken into account.
For example, (S12) is consistent with (S13), since they both state that H-Math-III-A* and H-Phys-III-A* occur  97  Relation  is cubic in the number of periodic events.
PCforPE can be applied also to the temporal specifications discussed in this paper, using the definitions of intersection and composition in this paper instead of the definitions in [203 (where numeric quantifiers had not been taken into account).
Given Property 1, Corollary 1 holds; moreover, we also proved Property 2 [ 191:  Result  Corollary 1.
PCforPE is correct and does not lose information Property 2.
The algebrn defined by the formalism described in section 2.1 and by the operations of intersection and composition above (and of inversion) is a conservative extension of the algebra in 1201.
I n2'-times EACH C1* n1'-times a* R n3'4mes b*.
I C-times EACH C2* n1"-times a* R2 n3"-timesb* else INCONSISTENT if nl'=nl" and n2'S n2" and n3'=n3"and Rd) then I n2'-times EACH C1* n1'-times a* R ny-times b*, I nT-times EACH C2* n1"-times a* R2 n3"-timesb* else INCONSISTENT if n l ' a l " and n2"W=RF(Cl*,C2*) and nY=n3" and R d ) then I &?-timesEACH C1* n1'-times a* R n3'-times b* else INCONSISTENT I n2'4mes EACH C1* n1'-times a* R1 ny-times b*, I n2"-times EACH C2* n1"-times a* R2 n3"-timesb'  3 Repetitions-check algorithm PCforPE, repeatedly applying the operation of intersection (and composition), checks for some types of inconsistencies.
However, intersection (and composition) operates on two specifications at a time, while certain inconsistencies on the number of repetitions of an event E can only be detected considering all the specifications concerning E at the same time.
For example, (a), (b), and (c) are pairwise consistent (thus no inconsistency is found repeatedly applying intersection), but conjuntively inconsistent: (a) eventx exactly twice each Monday, (b) eventx exactly twice each Tuesday, and (c) eventx exactly three times each week (for the sake of brevity, in the following we operate as if all the specifications had the very same frame time -the extensions to deal with multiple different frame times are obvious).
The algorithm to detect these types of inconsistencies is based on E-P-REPETITION-CONSIST(E,P,KB) below, which checks the consistency of the number of the repetitions of the event E in the period P, given a knowledge base KB of specifications in our formalism (henceforth we denote set membership by IN,to distinguish it from the relation E indicating temporal restriction between periods).
Get-repetitions(E,S) is a function retrieving the number of repetitions of the event E, given the specification S only.
Period(S) is a function extracting the period from a specification S .
For example, given (S3) above we have Get-repetitions(H-Phys-III-A*,S3) = 6; Period(S3) = Weeks*.
Relative-Frequency(P1 ,P2) is defined by cases: (1)Relative-Frequency(P1 ,P2)=1 if the relation between the two periods P1 and P2 i 3 =T or 4 or E or aZINC or 4-1 or E -1 or ~ 1 N c -or l e-1; e.g., Relative-Frequency(Da ys* ,Mondays*)=1; (2) Relative-Frequency(P1,P2) = the relative frequency of P1 and P2 if the relation between P1 and P2 is cIor cI-l  Table 1(a).
Intersection  =T I  <  if n 2 ' a 2 and n 3 ' a l " then 1 n2'-times EACH C1* n1'-times a* R n3"-times d* else INCONSISTENT if (n3'=nl") AND (n2'S n2") then NO NEW SPECIFICATION else INCONSISTENT  1  I n2'-times EACH C1* n1'-times a* R n3"-times d*  I else INCONSISTENT #  I NO NEW SPECIFICATION  Table 1(b).
Composition  In [19], we provided a logical formalization of (i) the temporal specifications in our formalism, and (ii) the relations between periods (e.g., 4 ), and used this first order temporal logic to prove Property 1: Property 1.
Our operations of intersection and composition are correct and do not lose information.
Property 1 grants that our operations of intersection and composition can be seen as a compilation of a set of logical inferences that could also be performed (in a less efficient way), e.g., by a theorem prover for our logic.
In our approach, temporal reasoning is performed by PCforPE [20], a path-consistency algorithm repeatedly applying intersection and composition.
PCforPE extends Allen's algorithm [ 13 to deal with temporal constraints between periodic events.
The time complexity of PCforPE  (in such a case, it gives thc same values as the function RF used in the definitions of intersection and 98  composition; e.g., Relative-Frequency(Days*,Weeks*) = RF(Days*,Weeks*)=7); (3) Relative-Frequency(P1 ,P2) = 0 otherwise.
T_Disjoint(Pl,P2) is a predicate which is true if the two periods P1 and P2 are temporally disjoint (e.g., Mondays and Tuesdays); T-Cover(Pl,P2) is true if the instances of P1 temporally cover completely the instances of p2 (e.g., TCover(Days*,Weeks*) is true.
Relative-Frequency(P,Period(S'))=l);  if P C,-l Period(S') A and T-Covers(P,Period(S')) the number of repetitions is the product of Get-repeti tions(E,S) with the relative frequency between P and Period(S') (e.g., P=Weeks*, Period(S)=Days*,Relative-Frequency(Period(S),P)==7).
On the other hand, all the specifications S' in LOC-SPEC such that P is in the relation E - l or 4 - l or C I - l Period(S') and not TCover(Period(S'),P) with Period(S') provide a lower limit for the number of repetitions of E in P. For example, let us suppose to have (a) E 2-times EACH Monday; (b) E 2-times EACH Sunday; (c) E 3times EACH Week-end; and that the period P we consider is Week.
From (a) and (b) we have that E happened at least 2 times each week, and from (c) we have that E happened at least 3 times each week.
However, since Monday and week-end are temporally disjoint, from (a) and (c) we have that E happened at least 5 times each week.
In general, one has to consider all the maximal sets of specifications such that all the periods they contain are temporally disjoint, and evaluate (using addition) the number of repetitions for each set.
Then, the maximum of these number can be taken as lower bound for the repetitions of the event in P. In the example above the maximal set of specifications containing temporally disjoint sets of periods are ((a),@)) and ((a),(c)) , and the number of repetitions is 4 for the first set, and 5 for the second, so that 5 can be taken as lower bound.
Finally, all the specifications S' in LOC-SPEC such that P is in one of the other relations (e.g., #) with Period(S') do not provide any constraint on the number of repetitions of E in P (e.g., if P=Mondays and S=E 2-times EACH Friday).
Thus, E-P-REPETITION-CONSIST checks that all the specifications S in LOC-SPEC such that P is in the relation =T or E or CI-l and T-Covers(PQeriod(S')) with Period(S') provide the same number of repetitions for E (let X be such a number), and that the specifications S' in LOC-SPEC such that P is in the relation E -l or 4-l or c I-1 Period(S') and not T-Cover(Period(S'),P) with Period(S') provide a lower limit for the number of repetitions of E which is not greater than X.
The overall check on the consistency of the number of repetitions of events in a KB can be simply performed by repeatedly applying the algorithm E-P-REPETITION-CONSIST above until all events and pcriods in KB are considered, or an inconsistency is detected, and runs in a time linear in the number of periodic events.
E-P-REPETITION-CONSIST(E,P,KB) LOC-SPEC t set of all specifications S of type <LOC> or cLOC-QUAL> in KB involving the event E; EXACT-SPEC t set of all specifications S' in LOC-SPEC such that P =T Period(S') or P E Period(S') or (P cI-lPeriod(S') and TCover(Period(S'),P)); ATLEAST-SPEC t set of all specifications S' in LOC-SPEC such that P E -l Period(S') or P 4-1 Period(S'); FOR EACH specification S IpI EXACT-SPEC QQ Get-repetitions(E.S) * Relative-Frequency(od(S),P); Check that the number of repetitions is the same for each S in EXACT-SPEC.
QQ E the number of repetitions is the same THEN  BEGIN let X be the number of repetitions; DISJ-A-S t set of maximal sets of specifications S' in ATLEAST-SPEC such that '$ Set IN DISJ-A-S, '$ S1, S2 JJ$ Set, the followig hold: (Period(S1) # Period(S2) AND T-Disjoint(Period(S1)Qeriod(S2)))  Yt  M%et  D1SJ-A-s ('  s IN set  Get-repetitions(E,S) * Relative-Frequency(Period(S),P); E X c Y THEN RE"RN("1nconsistent") ELSE RETURN("0K")  EM2 ELSE RETURN("1nconsistent") E-P-REPETITION-CONSIST is based on the 10 relations between periods in section 2.2.1 (plus the predicates T-Disjoint and TCover).
The idea is the following.
Given a period P, and the set LOC-SPEC of temporal specifications concerning the periodicity of an event E, all the specifications S' in LOC-SPEC such that P is in the relation =T or E or C I - l with Period(S') provide the exact number of repetitions of E in P. More specifically, if P =T Period(S') or P E Period(S') the number of repetitions can be directly extracted from S' (function Get-repetitions; in this case,  4 .
Query answering Given a knowledge base KB in which consistency has been checked and the constraintshave been propagated (via the path-consistency algorirhm plus the repetitions check  99  described in section 3), it is important to provide users and applications with an expressive query language to interact with the knowledge base.
Different types of queries are supported in our approch.
To exemplify the queries, we consider that the input knowledge base KBinp consists of the specifications (Sl), (S4), (S5), (S16) above.
KBinp is consistent and PCforPE infers, among the others, the specifications(S6), (S7).
(S8).
and (S17)).
For the sake of brevity, in the following we do not consider frame times.
analogous to E-P-REPETITION-CONSIST.
In particular, if KB contains at least one specification S' concerning E such that P =* Period(S') or P E Period(S') or (P CI-l Period(S') and T-Cover(Period(S'),P)), the exact number of repetitions can be extracted from S' as shown in E-P-REPETITION-CONSIST.
Otherwise, if KB contains a non-empty set of specification S' concerning E such that P E Period(S') or P 4 - l Period(S') or (P c I - 1 Period(S') and NOT T-Cover(Period(S'),P)), the lower bound of the number of repetitions can be extracted from the specifications in such a set as shown in E-P-REPETITION-CONSIST.
e.g., How-Many-Times(KRi,,p,H-Math-III-A* ,Weeks*) --> ATLEAST 3-times  Yeslno queries.
It is important to provide users with the possibility of asking whether a given set Q of constrains is consistent with the knowledge base KB.
The consistency of a conjunction Q of specifications can be checked (in cubic time) by adding the constraints Q to KB, and by checking the consistency of KB U Q as shown in sections 2.2.2 and 3. e .
g .
, C o n s i s ten t ?
( K B i n p , EACH Tuesdays* test-French-III-A* (BEFORE) 2-times HPhys-III-A*) --> YES  5.
Conclusions and comparisons In [17,20], we proposcd an approach dealing with frame times, periods and qualitative relations between periodic events.
In this paper we address the extensions needed in order to (1) deal with numeric quantifiers stating the number of repetitions of periodic events, (2) support specialised forms of reasoning about the number of repetitions of events in periods, and (3) support the possibility of answering different types of queries.
In particular, we showed how the distinction between 10 basic relations between periods in [17,20] is essential in order to achieve all the extensions above.
The approach described in this paper extends to many respect TDB and AI approaches to periodic events.
For example, Morris' algebraic approach [12,13] only deals with quantifiers and qualitative temporal constraints such as those in E x 2 above.
In [13], combinations of existential and universal quantifiers are used in order to specify the mapping relations between two periodic events evl* and ev2* which are in a given qualitative relation.
For instance, the quantifier [v&!
n (v3d!
)9 corresponds to "always and only", i.e., to a 1:l mapping between instances of evl* and instances of ev2*.
Our quantifier EACH roughly corresponds to this meaning, but has a wider application, since it states a 1:l mapping between numerically quantified collections of instances of evl* and ev2*.
A wide range of "fuzzy" quantifiers has been considered,e.g., by Ladkin [7,8],who, on the other hand, did not focus on the development of specialised reasoning and query answering techniques.
However, to the best of our knowledge, no current approach about periodic events covers all of the aspects considered in this paper, neither from the point of view of the formalism (e.g., qualitative relations between numerically quantified periodic events and period-dependent qualitative rclations -see, e.g., Ex.3,3',4,5,6), nor from the point of view of the  Period-based queries.
Given a period P (e.g., EMonday), one may ask which events occurred in P (and in which order).
These queries can be answered by considering that all events E occurred in a period P such that P =T P or P E P or P 4 - l P ' o r P c I - l P' necessarily occurred also in P (e.g., if P=Mondays* and P'=Days*; P E P).
In all the other cases, events might have occurred in P (e.g., if P=Mondays* and P=Weeks* -P 4 P-, since if E occurred exactly n-times each week, it may be the case that E' occurred n'-times (n'rn) each Mondays), but this is not certain given KB.
Thus, it is enough to retrieve all the specifications whose period P is in the relation P =T P or P E P or P 4-l P or P c f l P with P, and possibly to omit the qualitative constraints between events from the answer, if they have not been explicitly requested in the query.
e.g., Events-in-Period?
(KBinp,Tuesdays*) --> (2-times H-Phys-III-A*, H-Math-III-A*, test-French-III-A*] Event-based queries.
Given a periodic event E, one may want to extract all the constraints concerning E from the knowledge base (periods, number of repetitions, and, optionally, qualitative relations with other events).
This can be simply done by a retrieval operation on the KB.
e.g., When-Event?
(KBinp.H-Phys-III-A*)--> (EACH Tuesday* test-French-III-A* (BEFORE) 2-times H-Phys-III-A*, EACH Tuesdays* H-Math-III-A* (BEFORE) 2-times H-Phys-III-A*) Periodicity+Event based queries.
Given an event E and a period P, one may ask how many times did E occur in P .
The algorithm to anwer this type of queries is  100  specialised algorithms for performing temporal reasoning and answering queries.
Especially in the area of TDB, many approaches stressed the importance of dealing with user-definedperiods in application areas such as scheduling, financial trading, workflow analysis.
Moreover, in these and other areas (e.g., planning) also numeric quantifiers may be important in order to express precisely the number of repetitions of events.
The formalism described in this paper deals with these aspects, and has been designed in such a way that path-consistency on a set of statements in our formalism can be computed in polynomial time.
For instance, in [203 we showed that the addition of the AND operator to Leban's language for representing periods (e.g., to define periods such as Tuesdays* AND 1st-Days-of-Months*) makes path-consistency intractable.
We are currently investigating the possibility of extending our framework for dealing with quantifiers such as "only", "sometimes" etc.
in [12] for associating events to periods.
The approach in this paper constitutes the basis of TeMP+, a prototype of temporal manager dealing with periodic events which extends TeMP 117,201 to deal with numeric quantifiers and queries.
[ 111 R. Loganantharaj, and S .
Gimbrone: "Probabilistic  Approach for Representing and Reasoning with Repetitive Events", Proc.
eighth Florida Artificial Intelligence Symp., pp.
26-30,1995.
[12]R.A. Morris, W.D.
Shoaff, L. Khatib: "Path Consistency in a Network of Non-convex Intervals", Proc.
IJCAI'93,  655-660 (1993).
[13] R.A. Morris, W.D.
Shoaff, L. Khatib: "Domain Independent Temporal Reasoning with Recurring Events", Computational Intelligence (to appear), 1997.
[141 M. Niezette, J.-M. Stevenne: "An Efficient Symbolic Representation of Periodic Time".
Proc.
First International Conf.
on Information and Knowledge Management, 1992.
[15]R. Snodgrass: "The TSQL2 Query Language".
Kluver Academic Publ., 1995.
[16]M. Soo, R. Snodgrass: "Multiple Calendar Support for Conventianal Database Management Systems".
Proc.
International Workshop on an Infrastructure for Temporal Databases, June 1993.
[ 171 P. Terenziani: "Reasoning about Periodic Events", Proc.
TIME-95, pp.
137-144,Mclboume, FL, April 1995.
[18] P. Terenziani: "Towards an Ontology dealing with Periodic Events".
Proc.
ECA1'96, 43-47,1996.
[ 191 P. Terenziani: "Reprcsenting and Reasoning with Temporal Constraints about Periodic Events", Tech.Rep.
TR13-96.
Dipartimento di Informatica, Universita' di Torino, 1996.
[20]P. Terenziani: "Integrating calendar-dates and qualitative temporal constraints in tho treatment of periodic events", accepted for publication in IEEE Transactions on Knowledge and Data Engeneering, 1997.
[21]L. Vila: "A Survey of Temporal Reasoning in Artificial 7(1), 4-128,1994.
Intelligence", AI CO".
[22]R. Wetprasit, A. Sattar, and L. Khatib, "Reasoning with sequences of Point Events, Proc.
TIME-96; Chittaro, Goodwin, Hamilton and Montanari eds., IEEE Press, pp.
36-38,Los Alamitos, CA, USA, 1996.
REFERENCES [ 11 J.F.
Allen: "Maintaining Knowledge about Temporal  Intervals", Comm ACM 26(11) 832-843 (1983).
[2] M. Baudinet, J. Chomicki, P. Wolper: "Temporal Databases: Beyond Finite Extensions", Proc.
Int?
Works.
on an Infrastructurefor Temporal Databases, June 1993.
[3] R. Chandra, A. Segev: "Managing Temporal Financial Data in an Extensible Database", Proc.
19th Int'l Conference on Very Large Databases, 1993.
[4] J. Chomicki, T. Imielinsky: "Finite Representation of Infinite Query Answers", ACM Transactions on Database Systems 18(2), 181-223,1993.
[5] D. Cukierman, and J. Delgrande, "Characterizing Temporal Repetition", Proc.
TIME-96, Chittaro, Goodwin, Hamilton and Montanari eds., IEEE Press, pp.
80-87,Los Alamitos, CA, USA, 1996.
[6] F. Kabanza, J.-M. Stevenne, P. Wolper: "Handling Infinite Temporal Data", Proc.
ACM SIGACT-SIGMODSIGART Symposium on Principles of Database Systems, 392-403 (1990).
[7]P. Ladkin: "Primitive and Units for Time Specification", Proc.
AAA1'86, 354-359(1986).
[8] P. Ladkin: "Time Representation: A Taxonomy of Interval Relations", Proc.
AAA1'86, 360-366(1986).
[9] B. Leban, D.D.
McDonald, D.R.
Forster: "A representation for collections of temporal intervals", Proc.
AAAI'86, 367-371 (1986).
[lo] G. Ligozat: "On Generalized Interval Calculi", Proc.
AAAI'91, 234-240 (1991).
101
Efficient Encoding of Temporal XML Documents Mohamed-Amine Baazizi, Nicole Bidoit, Dario Colazzo Univ.
Paris-Sud, Leo Team, & INRIA Saclay last-name@lri.fr  Abstract--The management of temporal data is a crucial issue in many applications.
Recently, XML has become the standard for data exchange and representation.
Consequently, important efforts have been made on the development of temporal extensions for XML.
This paper investigates how to generate or maintain space-efficient time-stamped documents.
We formally define a notion of compactness which allows for comparing documents.
Then, we present two methods.
For the first one, called general method, no restriction is made on the evolution of the XML documents whereas for the second one, called update-based method, changes are assumed to be specified by updates.
For both methods, the issue is to enable processing very large documents, to use existing engines and to comply to XQuery Update Facility.
The two methods are compared in terms of space-efficiency.
The update-based method produces time-stamped XML documents that are more satisfactory wrt space-efficiency than the general method.
This goes to show that the update-based method effectively takes advantage of the updates.
I. I NTRODUCTION The management of temporal data is a crucial issue in many applications such as finance, banking, travel reservations, geographical information systems etc.
With the increasing use of XML for data exchange and representation, the issue of developing temporal extensions for XML is gaining importance.
Temporal extensions have been extensively studied in the relational framework.
Chomiki et al.
[1] pointed out the necessity of separating the abstract model from the concrete one.
The abstract model views temporal data as a sequence of instances.
Although this model facilitates the formal development of query, update, and constraint languages, from a practical point, storing a sequence of database instances requires a very large amount of space.
The concrete model provides a space-efficient format to store temporal data.
Current work on temporal XML concentrate on timestamp XML documents, a concrete model.
Although many proposals have addressed the issue of querying time-stamp XML documents, there has been less in-depth investigation of how to efficiently build or maintain temporal XML documents, keeping track of data evolution over time.
In this paper, we follow the approach of [1] and provide both a notion of abstract temporal XML document and a notion of concrete temporal XML document.
This allows to study the link between abstract temporal documents  and their concrete encodings.
Obviously, a given abstract temporal document may have several concrete encodings, some being more space-saving than others.
Comparing concrete encodings wrt compactness is formally captured by introducing a partial order.
In this paper, we study ways to generate concrete encodings of an abstract temporal document with the following requirements.
The first goal is of course to build or maintain time-stamp document as compact as possible.
The second goal is to provide methods enabling to process very large documents.
This requirement is particularly important as the size of temporal XML documents is expected to be much larger than the size of static ones and as we assume the documents to be processed by in-memory engines.
We develop two methods.
The first one makes no assumption on the abstract temporal document for which an encoding is built whereas the second one assumes that the abstract temporal document is produced by a sequence of updates u1 , .
.
.
, un from an initial document.
The update language considered is the XQuery Update Facility (XUF) update language as described in [2], [3].
The main feature of the first method, next called the general method, is that it can be implemented in a streaming manner based on a SAX parsing [4] overcoming main memory space limitations.
This feature unfortunately entails that, in some cases, the obtained encodings are not as compact as they could be.
The second method, next called the update-based method, is much more sophisticated.
It is based on the type projection paradigm developed for XQuery [5], [6] and XUF [7] in order to overcome the main memory limitation of in-memory engines like Galax [8], Saxon [9], QizX [10], [11], and eXist [12].
The update-based method relies on pruning the timestamp document over which an update should be integrated in order to load only part of the document that are touched or needed by the update.
The pruning phase is very similar to that introduced in [7] and makes use of the schema (DTD) typing the document.
The benefit of extending the update mechanism of [7] is manifold.
First, it enables for processing large timestamp documents which would not be processed by inmemory engines due to their size.
Second, any update engine can be targeted by such a scenario.
Another advantage is that no rewriting of the updates is necessary.
Last but not least, the encodings produced using the update-based method are much more satisfactory from the point of view of  space-efficiency than the encodings produced by the general method.
This goes to show that the update-based method takes advantage of the information given by update in an efficient way.
Related work Temporal relational databases were extensively studied.
Different data models and query languages were proposed (see [1] for a detailed survey).
Chomicki et al.
[1] were the first to consider the abstract and the concrete models of temporal databases.
They also studied the way to encode temporal relational databases in a compact format and addressed efficiency issues wrt temporal queries [13].
Recently, important efforts have focussed on studying temporal extensions for semi-structured data and XML in particular.
For instance, Chawathe et al.
[14] extend the OEM model in order to keep track of updates.
In the context of XML, several proposals have been developed for managing the temporal dimension (see [15] for a survey).
Next, we focus more specifically on those addressing the issue of building or maintaining encodings of temporal XML documents.
Many proposals store the last version of the data together with deltas that represent the changes between the different versions.
These deltas are often given by edit scripts.
They are used for retrieving past versions starting from the document that is stored.
Buneman et al.
[16] pointed out that delta-based approaches raise practical and semantic problems.
First, the cost for recovering past versions, which is required for query evaluation, is considerable and increases when new versions are added.
Second, the information about changes provided by the edit-scripts is syntactical and quite often not meaningful.
Time-stamp approaches are more effective than delta-based approach wrt to querying simply because temporal queries can be directly evaluated on timestamp documents without the need to retrieve the different versions.
Buneman et al.
[16] investigate data structures for managing historical information about scientific data.
They develop a technique for merging versions of scientific documents into a single temporal document.
Their technique is tailored to a special kind of data which has the following characteristics: the node order is not taken into account; each node is uniquely identified on the basis of its content (notion of key); and finally, insertion of new elements is the only kind of update which occurs frequently.
In our study, as opposed to [16], we do not focus on a particular type of data, node order is preserved which has a price to pay.
Wrt changes, we cover both the case where no information is available wrt the evolution of the documents (it may not be the result of structured updates but rather be produced by editing or any other treatment over the documents) and the case where the changes are specified by XUF.
Our approach has some similarities with that of Rizzolo  et al.
[15].
They model temporal data by a DAG and provide two mappings from their DAG structure to XML.
A specific update language is provided for specifying temporal evolution and this language is translated into operations over the graph-based representation.
The space-efficiency issue is addressed although indirectly, as the authors study two mapping strategies: a non-replicating strategy which uses references to optimize the storage of subtrees shared by several nodes and a replicating strategy.
The authors also develop several algorithms to eliminate inconsistencies that may be introduced by the use of references.
The work presented in [15] does not consider changes other than those generated by updates.
They consider a specific update language while we cover XUF and while our method is compatible with any XUF query engine.
Although DAG may be more space-saving than time-stamp XML trees, the approach [15] does not address maintenance of very large temporal XML documents.
Other proposals like [17], [18] address the issue of managing and querying historical XML databases.
They present a technique for computing documents annotated with timestamps (called H-documents) starting from a sequence of versions.
However, preserving node document-order in Hdocuments is not considered and updating H-documents is not fully addressed wrt update language and scalability.
Our presentation is organized as follows.
After a short preliminary section providing the main notations, the abstract and concrete model for capturing temporal XML evolution is provided in Section III together with the compactness order.
Section IV introduces the two methods for generating timestamp documents from a sequence of static XML documents and compares the two methods wrt space-efficiency.
Section V reports the results of a first bunch of experiments for validating the update based approach.
We finally conclude and discuss future work in section VI.
II.
P RELIMINARIES As in [3], XML documents are represented as stores.
Next, I, J, K may designate sets (id-sets) or a list (id-seq) of storeidentifiers denoted i, j ...; () denotes the empty id-seq; I[?
]I ' denotes id-seq composition, and the intersection of I and J preserving the order of the id-seq I is denoted by I|J .
A store s is a mapping from the set of store-identifiers I to constructors k defined as follows: k ::= text[s] | a[J], where s is a string, a a label and J an id-seq such that J[?]I.
We only consider stores that correspond to XML trees and forests.
An XML forest f over I is given by (J, s, g) where s is a forest over I whose tree roots are given by J.
The mapping g over I is optionally used, in some context, to associate explicit identifiers to document nodes.
The following notations are used: - dom(s)=I, - lab(i)=a if s(i)=a[I ' ], lab(i)=String if s(i)=text[st], - child(s, J)={i' | [?]i[?
]J, s(i)=a[I] and i' [?
]I},  - desc(s, J)={i' | i' [?
]child(s, J) or i' [?
]desc(s, child(s, J))}, - roots(s)={i | ![?
]i' , i[?
]child(s, {i' })}, and - f *f ' is the concatenation of two disjoint forests f and f ' .
An XML document d over I is given by (r, s, g) such that roots(s)=r and s is a tree.
Given a store s over I, the projection on J[?
]I of s, is a store over J, denoted PJ (s), defined by: for each j[?
]J, if s(j)=a[K] then PJ (s)(j)=a[K|J ] otherwise PJ (s)(j)=s(j).
The reader should pay attention to the fact that the domain and the "co-domain" of the PJ (s) are equal to J and that, even if s is a tree, PJ (s) may not be a tree.
Finally, for the purpose of capturing time, we use timestamps of the form [t, t' [ with t < t' and [t, N ow[ where t and t' are positive integers capturing instants of time and N ow is a variable indicating the current instant.
III.
T EMPORAL XML M ODELS doc  doc  doc  #0  #0  #0  a  a  a  a  a  e  a  #1  #2  #1  #2  #1  #11  #2  b  b  b  b  #3  #4  #3  #4  e  f  g  #8  b  #12  #13  #4  e #8  f  g  g  f  g  f  g  f  g  #5  #6  #7  #5  #6  #9  #10  #9  #10  d0  d1 Figure 1.  d2  A temporal abstract document d  The first part of this section follows the lines of [1] in providing two alternative models for capturing the evolution of XML documents.
Definition 3.1 (Abstract temporal document): An abstract temporal document d over the temporal domain [0, n] is a sequence d0 , .
.
.
, dn of (static) documents such that, for each document dt =(r, st , gt ), it is assumed that the mapping gt satisfies: [?
]i, i' [?]
dom(st ), i[?
]i' , gt (i)[?
]gt (i' ).
The condition on gt enforces gt (i) to behave as an explicit identifier for the node i.
These explicit identifiers are used to trace node evolution over time in the temporal document d. Fig.
1 presents an abstract document d0 , d1 , d2 .
Explicit node identifiers are prefixed with #.
As already said, in practice, storing an abstract temporal document d may be quite inefficient because of replication of unchanged parts of the document.
The concrete model aims at coping with this by introducing time-stamps (interval labeling) over document elements, providing the validity period of the elements.
Changes are then encoded within a single document.
Definition 3.2 (Time-stamped Document): A time-stamp XML document [?
]=(r, s, g, t) is an XML document enriched with a mapping t:dom(s)-Int, satisfying the following properties:  - [?
]i, j[?
]dom(s)\{r}, j[?
]child(s, {i}) implies t(j)[?
]t(i); - [?
]i, j[?
]dom(s), i[?
]j and g(i)=g(j) implies t(i)[?]t(j)=[?].
The first condition is classical and ensures that time stamps are hierarchically consistent.
The second one ensures that, two nodes i, j representing the evolution of an element, cannot share the same validity intervals.
g(i) are abusively called explicit identifiers, although for time-stamp document, g is no more a bijection.
Fig.
2 depicts two time-stamp documents [?]
and [?]'
.
For the purpose of making easier the manipulation of a timestamp document, the explicit value of N ow is registered at its root.
Temporal projection is now defined by extending XML projection already introduced for (static) XML documents in Section II.
Definition 3.3 (Temporal Projection): The temporal projection of a time-stamp document [?
]=(r, s, g, t) on a time point t, denoted Snap([?
], t), is the (static) XML document d=(r, PT ([?
],t) (s), g|T ([?
],t) ) where T ([?
], t)={i|t[?
]t(i)} if t[?
]t(r) and the undefined document  otherwise.
The following definition links abstract and concrete representations in a natural manner.
Definition 3.4 (Sound & Complete Encoding): Given an abstract document d=d0 , .
.
.
, dn , a time-stamp document [?
]=(r, s, g, t) is a sound, resp.
complete, encoding of d if [?]t[?
]t(r), Snap([?
], t)=dt , resp.
if [?]t[?
][0, n], Snap([?
], t)=dt .
The two time-stamp documents [?]
and [?]'
of Figure 2 are encodings of the abstract document of Figure 1.
In the remainder, we consider sound and complete concrete encodings of abstract temporal documents and the term "concrete encoding" implicitly includes "sound and complete".
The following table summarizes the notation used throughout the article.
static XML doc.
abstract temporal XML doc.
d d  DTD time-stamp XML doc.
D [?]
Space-efficient concrete encodings Obviously, an abstract document may have several concrete encodings.
Some of them may be more space-saving than others because they have less nodes.
Next, we formalize such a notion by introducing a pre-order  that enables to compare time-stamp documents.
Given a timestamp forest f =(I, s, g, t) over J, a label l and an explicit identifier x (meaning x=g(i) for some i[?
]J), Sf (l, x) denotes the set of top-level store identifiers ([?]
I) associated with nodes labeled by l and whose explicit identifier is x: Sf (l, x)={i[?
]I | s(i)=l[K] and g(i)=x}.
Definition 3.5 (Compactness Order): Consider two timestamp forests f1 =(I1 , s1 , t1 , g1 ) and f2 =(I2 , s2 , t2 , g2 ).
Below, l is a label in f1 or f2 and x is an explicit identifier i.e.
x[?
]g1 (dom(f1 ))[?
]g2 (dom(f2 )).
f1 is more compact than f2  [0,N [  [0,N [  doc  doc  #0 [0,N [  #0 [2,N [  [0,2[  [2,N [  [0,N [  [2,N [  [0,N [  a  a  e  a  a  e  a  #1  #2  #11  #2  #1  #11  #2  [2,N [ [2,N [  [2,N [ [2,N [  [0,2[  [2,N [ [2,N [  [0,N [ [1,N [  [0,2[  [0,2[  b  b  #3  #4  [0,2[  [0,2[  [1,2[  e  f  g  #8  b  #12  #13  #4  [1,2[  [0,1[  b  f  g  b  #3  #12  #13  #4  #8  [0,1[  [1,N [ [1,N [  e #8 [2,N [ [2,N [  [1,2[  [0,2[  [0,2[  e  f  g  g  f  g  f  g  f  g  g  f  g  #5  #6  #7  #9  #10  #9  #10  #5  #6  #7  #9  #10  (a) [?]
the result of applying It-Comp on d Figure 2.
Two encodings of the abstract temporal document d  if: (1) for each l and x, we have: |Sf1 (l, x)|<=|Sf2 (l, x)| and (2) for each l and for each x, we have: PK1 (f1 )PK2 (f2 ) where Ki =Sfi (l, x)[?
]desc(si , Sfi (l, x)) Our definition of compactness order is tree-based and relies on comparing the number of subtrees rooted at matching nodes.
Let us go back to the example of Figure 2 presenting encodings of the abstract document of Figure 1: it can be checked that [?]'
is more compact than [?].
Comparing the compactness of two time-stamp documents that differ by their labels and temporal domains does not convey much information.
Next, we will be comparing pairs of time-stamp documents that are concrete encoding of the same abstract temporal XML document.
Under such a restriction, we have: Property 3.6:  is a partial order.
Next, given an abstract temporal document d=d0 , .
.
.
, dn , we denote by [?
](d) the least compact concrete encoding of d. Obviously, given a concrete encoding [?]
of d, we have: [?]
[?](d).
IV.
B UILDING  (b) [?]'
the result of applying It-Update on d  SPACE - EFFICIENT ENCODINGS  This section focuses on building or maintaining a concrete encoding for an abstract temporal document.
The issue is to ensure compactness of the encoding and, at the same time, to treat documents as large as possible.
Two methods are investigated.
The first one is developed without making any hypothesis on the abstract documents, whereas the second one makes the assumption that the abstract document is associated with a given sequence of updates.
Although the methods are not comparable in general, we show that, histories can be encoded into time-stamp document in a significantly more compact manner.
A. Encoding general abstract documents Let us consider an abstract document d=d0 , .
.
.
, dn .
The method, called It-Comp, used to build or maintain a timestamp encoding of d, relies on an iterative process.
The  initial document is trivially transformed into a time-stamp [0,N ow[ and then assuming that [?
]t-1 is the document [?
]0 =d0 time-stamp document encoding d0 , .
.
.
, dt-1 , the document dt is added to [?
]t-1 in a specific manner to produce [?
]t =Comp([?
]t-1 , dt , t).
Informally, Comp proceeds to a parallel and synchronized parsing of [?
]t-1 and dt and attempts to merge nodes in dt with nodes in [?
]t-1 .
The formal specification of Comp is given in Figure 3.
Its inputs are: a timestamp forest Fs (some sub-forest of [?
]t-1 ), a XML forest F (some sub-forest of the static XML document dt ), and t referring to the validity time of dt .
The time-stamp forest f int is obtained by time-stamping each node of the forest f by the interval int.
The time-stamp forest Pha-b is obtained by replacing, in each occurrence of the time-stamp [t, a[ in Ph, the upper bound a by the value b.
Let us now explain the behavior of Comp.
Line 1 is the terminal case: the forest Fs is empty (its root set is empty).
For Line 2, 3 and 4, the nodes parsed by Comp are the root node r[?]
of [?]
for the [?
]t-1 side and the root node rd of d for the dt side.
Line 2 deals with the case where the parsed nodes r[?]
and rd "match" an unchanged element node; then the function TComp([?
], d, t) builds the tree whose root is r[?]
and whose sub-forest is recursively built by a call to Comp involving the sub-forest of [?]
(for [?
]t-1 side) and the sub-forest d (for dt side).
Line 3 deals with the case where both parsed nodes are the same text node.
Line 4 captures the case where changes are identified: either r[?]
has been removed between the instants t-1 and t or it has moved modifying the child order; r[?]
is output first (together with its sub-forest) while closing its time-stamp, followed by the node rd and its sub-forest time-stamped by [t, N ow] as expected.
It can be shown that: Property 4.1: It-Comp(d) is a concrete encoding for the abstract document d, and It-Comp(d)[?](d).
Note that the way Comp is designed allows for a streaming  Comp(Fs , F, t) = l.1  F [t,Now]  if roots(Fs ) = [?]
otherwise  l.1bis  FsNow-t  if roots(F ) = [?]
otherwise assume Fs =[?
]*Ph and F =d*f  l.2  TComp([?
], d, t) * Comp(Ph, f, t)  l.3  [?]
* Comp(Ph, f, t)  l.4  [?
]Now-t * d[t,Now] * Comp(Ph, f, t)  if lab(r[?]
)=lab(rd ) and g[?]
(r[?]
)=gd (rd ) and t+[?]
(r[?]
)=N ow if s[?]
(r[?]
)=sd (rd )=text[st] and t+[?]
(r[?]
)=N ow otherwise  Figure 3.
Definition of Comp  implementation which ensures that large documents can be processed.
Unfortunately, this also explains why Comp is unable to reduce replication of elements, which would require to buffer (in some case a large amount of) information.
This is incompatible with our space-efficiency target.
B. Encoding Histories This section considers a class of abstract temporal documents usually called histories.
An abstract temporal document d=d0 , .
.
.
, dn is an history when each document dt is obtained from dt-1 by some update ut .
Indeed, the history d is equivalently specified by an initial XML document d0 and a sequence of updates u1 , .
.
.
, un .
We consider the XQuery Update Facility (XUF) update language as described in [3].
Although renaming is part of our study, we do not present the technical aspects of its treatment for the sake of simplicity.
We also assume that each document di is valid wrt to some known DTD Di , leaving space for schema evolution.
For the sake of simplicity, we suppose next that [?
]i, Di =D.
Generating a time-stamp encoding of an history d specified by d0 and u1 , .
.
.
, un is an iterative process, called It-Update.
The initial document d0 is trivially transformed [0,N ow[ , and then asinto the time-stamp document [?
]0 =d0 suming that [?
]t-1 is the time-stamp document encoding the history specified by d0 and u1 , .
.
.
, ut-1 , the update ut is propagated, in a specific manner, over [?
]t-1 to produce [?
]t =U pdate([?
]t-1 , ut ).
Recall here that our goal is to provide a compact encoding of the history d, and also to handle time-stamp documents of very large size as well as we want to avoid for devising a new update engine.
This is the reason why we have been investigating a method based on XML projection for specifying U pdate([?
]t-1 , ut ) because such method allows one for reducing main memory space consumption and for being compatible with any XUP query engine.
In [7], a type-based method has been proposed for optimizing main memory XML update processing where optimization should be understood as space optimization1.
Given an update u over a document d valid wrt a DTD D, the idea is to determine, in a static manner, which fragments of the document are necessary for and touched by the update u.
More precisely, from u and the DTD D, a static 1 Experiments of this method show that execution time is also improved for some engines.
analysis infers a type-projector p specified by sets of labels.
Then, the update scenario is as follows.
At loading time, the document d is pruned wrt p in a streaming manner: roughly, nodes whose labels are in p are projected.
The update u is evaluated over the pruned document p(d).
The document u(p(d)) is of course missing the pruned out nodes and thus a last step is necessary in order to reinstate the update of the projected document u(p(d)) in the document d. This is done by merging the documents d and u(p(d)) at writingserializing time.
The update scenario has been designed in order to avoid any rewriting of the update and in order to be independent of any main-memory engine.
The interested reader will find in [7] a full description of the type-projector, its extraction, the projection and merge algorithms as well as experiments validating the space and time optimization.
Below, we proceed to a short introduction of the type-projector.
Type-projector for updates have been devised in order to capture update expression features and also to ensure correctness of the merge phase.
The typeprojector p for an update u is specified by 3 sets of labels: pno (the 'node only' component) is meant to capture labels of nodes that are traversed by u or target of deletion or existential condition; polb (the 'one level below' component) is meant to capture labels of mixed-content nodes or labels of nodes that are targets of insertion or replacement; peb (the 'everything below' component) is meant to capture labels of nodes that are roots of extracted subtrees.
Given a document d valid wrt the DTD D, the behavior of the type-projector is as follows.
If the label of a node (of d) belongs to pno , then it is projected.
If the label of a node belongs to polb , then it is projected together with all its children, even though their labels do not belong to the projector.
If the label of a node belongs to peb , then it is projected together with all its descendants, even though their labels do not belong to the projector.
When a node is projected because its label belongs to p, its children are examined as candidate for projection, according to the above rules.
We are now ready to define U pdate([?
]t-1 , ut ).
We use the example of Figure 4 to illustrate the presentation.
The DTD D considered is specified by the rules doc-(a|e)*, a-b*,c*,e?, c-String|f , and b,d-(f|g)*.
It is assumed that the document [?
]1 is an encoding of d0 , d1 of Fig.
1 and that the update u2 applied on d1 to produce d2 is specified by the XUF expression:  [0,N [  doc  doc  #0  #0  doc #0 [0,N [  a  #1  #2  [0,N [  f #1.1.1  [0,N [  b  b  #1.1  #2.1  [0,N [  [1,N [  a  e  a  #1  #n1  #2  g  #1.1.2 #2.1.1  b  b  #1.1  #2.1  e  f  g  #2.2  #n2  #n3  e  b #2.1  #2.2  g  f  g  f  g  #1.1.2  #2.2.1  #2.2.2  #2.2.1  #2.2.2  e #2.2 [1,N [  [0,1[  g  a #2  [0,N [  a  [0,N [  a #1  [1,N [  f  g  #2.2.1  #2.2.2  p Now ([?
]1 )  [?
]1 Figure 4.  d2 =u2 (p Now ([?
]1 ))  Illustrating the update-based encoding  for $x in /doc/a where $x/b/g return { insert node /doc/a/e after $x; delete$x/b}.
Propagating an update ut on the time-stamp document [?
]t-1 relies on the update scenario of [7] and proceeds as follows: (1) the type-projector p for the update ut is extracted as specified in [7]; for the update u1 of our running example, the type-projector is specified by: pno ={a, b, g}, polb ={doc} and peb ={e}.
(2) the time-stamp document [?
]t-1 is projected at loading time wrt the temporal type-projector p Now , which combines the temporal projection over the time instant t-1 (or N ow) and the type-projection p i.e.
def p Now ([?
]t-1 ) = p(Snap([?
]t-1 , N ow)); the projected document is equivalent to the type-projection of the document dt-1 =ut-1 (u0 (d0 )), i.e.
we have2 : p Now ([?
]t-1 )~p(dt-1 ); for our example, only the node #2.1.1 is pruned out by temporal projection wrt to N ow; the type-projection prunes out the node #1.1.1 because its label f does not belong to p; note that the subtree rooted at #2.2 is projected because its label e belongs to peb .
(3) the update ut is evaluated over the projected document p Now ([?
]t-1 ) producing ut (p Now ([?
]t-1 )); this can be performed by any update engine and update rewriting is not required; Fig.
4 shows the document u2 (p Now ([?
]1 )) for our running example.
(4) the last step integrates the document ut (p Now ([?
]t-1 )) into the time-stamp document [?
]t-1 ; this phase called UMerge differs from [7] as not only it has to propagate the updates executed over p Now ([?
]t-1 ) but it also needs to maintain timestamps.
Wrt our example, the result of UMerge applied over [?
]1 and u2 (p Now ([?
]1 )) is the time-stamp document3 [?]'
of Fig.
2(b).
The information used by UMerge are: the type-projector p, time-stamps and node identifiers as explained below.
The  UMerge phase proceeds by parsing both documents in a syn-  chronized manner.
At each step, the parsed node in [?
]t-1 and the parsed node in ut (p Now ([?
]t-1 )) are examined to decide which one should be output and also to maintain timestamps.
We stress that UMerge does not perform changes on nodes other than time-stamp maintenance.
Clearly, during parsing, the "old" nodes of [?
]t-1 time-stamped by [k1 , k2 [ where k2 [?
]N ow are output directly: they have been pruned out by the temporal projection and were not involved in the update.
For our example, this case applies to node #2.1.1.
More attention should be paid to nodes of [?
]t-1 timestamped by [k1 , N ow[ and thus corresponding to nodes in Snap([?
]t-1 , N ow).
Some of these nodes may have been pruned out although others may have been projected and modified.
Detecting if a node n in ut (p Now ([?
]t-1 )) corresponds to a node m in [?
]t-1 is based on node identifiers comparison with the following setting: (i) node identifiers for [?
]t-1 are node positions; they are not stored within the document which would be quite space and time consuming but rather generated on the fly during the time-projection and the UMerge phases and only for the fragment of [?
]t-1 corresponding to Snap([?
]t-1 , N ow); however node positions are stored in the projection p Now ([?
]t-1 ); moreover, in practice, full position is not required and space is saved in a large extend by storing child order of nodes only; for the sake of the example, the position on the document [?
]1 have been given although, in practice, they are not part of the document; (ii) new nodes introduced by the update ut have no explicit identifiers (positions) as we use the initial update ut without rewriting it; for the sake of the example, special identifiers ni have been used, in the document u2 (p Now ([?
]1 )), for each node of the second subtree of the root because it is a subtree inserted by u2 .
Items (i) and (ii) entail that nodes n and m are identified to be the same (although the label of n may have been changed by the update ut ) just by verifying position equality.
2  ~ denotes value equivalence of XML documents The reader should not pay attention to the explicit identifiers which are different from those used in Fig 4.
3  Formally U pdate([?
]t-1 , ut )=UMerge([?
]t-1 , ut (p Now ([?
]t-1 )), t).
Let us comment on UMerge with our running example.
In order to do that, a node of a document X whose position is #i is denoted by X#i; the document u2 (p Now ([?
]1 )) is denoted with d2 .
While merging [?
]1 and d2 , nothing special happens until after parsing [?
]1 #1 and d2 #1.
Then, the next node to be parsed in [?
]1 is [?
]1 #1.1, child of [?
]1 #1, while d2 #1 has no subtree.
Because the time-stamp of [?
]1 #1 is [0, N [ and its label b belongs to p, the function UMerge detects that the subtree rooted at [?
]1 #1.1 has been deleted by u and thus this subtree is output after replacing the timestamp [0, N [ by [0, 2[.
The two next nodes examined are [?
]1 #2 and the new node d2 #n1 : the new subtree of d2 is then output with the appropriate time-stamp [2, N ow[.
Property 4.2: Given an history d, we have that: (i) It-Update(d) is a concrete encoding for d, and, (ii) It-Update(d)It-Comp(d).
Space limitation does not allow us to present proofs.
Notice that (ii) expresses that when an abstract document is an history, the projection based method is better than the simple one in terms of compactness.
This fact is validated by the experiments.
V. E XPERIMENTS To validate the effectiveness of our approach, we implemented both the It-Comp and It-Update algorithms in Java, and made experiments on a 2.53 Ghz Intel Core 2 Duo machine (2 GB main memory) running Mac OSX 10.6.4.
The main-memory engine used for running the It-Update is QizX [10], [11].
We generated four abstract documents, starting from four initial XMark [19] documents of growing size, from 45MB to 1126MB.
As the goal is to compare It-Comp and It-Update, each abstract document is an history d0 , d1 , d2 , d3 where d0 is the initial document, and di is obtained from di-1 by means of an update ui .
The updates used for the experiments are given below.
It should be clear that, for the purpose of evaluating It-Comp, we generated each di with adequate explicit identifiers, while evaluating It-Update only requires the initial document and the updates.
u1 .
delete node/site/regions/australia u2 .
for $x in/site/closed auctions/closed auction u2 .
where $x/annotation u2 .
return insert node eeeee<amount>to be determined</amount> u2 .insert after $x/price u3 .
for $x in /site/open auctions/open auction u2 .
where $x/privacy return delete $x Test results are reported in Table I.
In this table, [?
]i is the result of compacting d0 .
.
.
di documents applying the It-Comp method while [?
]'i is the encoding obtained after executing the ui update starting from [?
]'i-1 according the It-Update method.
We compared the two methods in terms  d0  45.4MB  112.4MB  454.8MB  1126.8MB  [?
]0 [?
]'0  55.7 45.4  138.5 112.4  563.5 454.8  1351.7 1126.8  [?
]1 [?
]'1 gain  76.5 45.4 40.7%  190.5 112.4 41.0%  774.7 454.8 41.3%  1833.0 1126.8 38.5%  [?
]2 [?
]'2 gain  84.5 45.6 46.0%  210.1 112.9 46.3%  853.6 456.4 46.5%  2027.5 1130.7 44.2%  [?
]3 [?
]'3 gain  91.7 45.6 50.3%  228.6 112.9 50.6%  929.0 456.4 50.9%  2207.5 1130.7 48.8%  Table I C OMPARISON BETWEEN It-Update  AND  It-Comp  of sizes of [?
]i and [?
]'i with i ranging from 0 to 3.
Obviously the difference in size between [?
]0 and [?
]'0 is explained by the presence of explicit identifiers in [?
]0 .
Test results in Table I show the It-Comp method is more space consuming than the It-Update method.
While the size is increasing in the [?
]i 's sequence, the size is almost constant in the [?
]'i 's sequence.
This testifies that the It-Update method succeeds in avoiding node replication under updates.
The method It-Comp instead entails a sensible amount of node replication since information coming from the update is not used.
We can also see that after each update the improvements in terms of size is sensible, going from 38.5%, after the first update, to 50.9%, after the last update.
In these experiments we considered abstract documents of length 4.
Improvements in terms of size are likely to remain sensible in other realistic scenarios, since as the number of documents increases the probability that node duplication arises in the It-Comp method increases as well.
Of course this holds for It-Update method as well, but in a much smaller extend, since update information is used.
We postpone to future works experiments on more complex scenarios.
To conclude, we would like to highlight that both methods are able to process temporal documents of large size.
Recall that the memory limitation of QizX, the update engine used for the experiments of It-Update, makes impossible to process documents whose size exceeds 580MB.
We plan to run our experiments with other engines.
VI.
C ONCLUSION In this paper we developed two techniques for generating and maintaining encodings of abstract temporal documents.
The first technique addresses the case where no information is available on the abstract temporal document.
Although this technique is not fully satisfactory from the point of view of space efficiency, it allows to maintain large temporal documents.
The second one, called update-based, is designed  to manipulate document history.
It takes advantage of a prior technique developed in [7] and enabling the update of large XML files using in-memory engines.
The first experiments validate the effectiveness of both methods wrt to our first goal (processing large documents) and show that our updatebased method is efficient wrt space-saving.
In the future, we plan to improve both methods and develop further experiments.
Concerning the first one, we might investigate how to take advantage of a description of the changes (recall that changes are not updates) in order to develop projection-based method.
For both method, we also plan to investigate parallelization in order to improve time execution.
VII.
ACKNOWLEDGMENTS We would like to thank the anonymous reviewers for their valuable comments.
This work has been partially funded by the Codex project, Agence Nationale de la Recherche, decision ANR-08- DEFIS-004.
R EFERENCES [1] J. Chomicki and D. Toman, Handbook of Temporal Reasoning in Artificial Intelligence.
Elsevier, 2005, ch.
Time in Database Systems.
[2] "Xquery update facility 1.0," http://www.w3.org/TR/xqueryupdate-10.
[3] M. Benedikt and J. Cheney, "Semantics, types and effects for xml updates," in DBPL, 2009, pp.
1-17.
[4] "SAX," http://www.saxproject.org/.
[5] V. Benzaken, G. Castagna, D. Colazzo, and K. Nguyen, "Type-based xml projection," in VLDB, 2006, pp.
271-282.
[6] A. Marian and J. Simeon, "Projecting xml documents," in VLDB, 2003, pp.
213-224.
[7] M. A. Baazizi, N. Bidoit, D. Colazzo, N. Malla, and M. Sahakyan, "Projection for xml update optimization," in EDBT, 2011, pp.
307-318.
[8] "Galax," http://www.galaxquery.org.
[9] "Saxon-ee," http://www.saxonica.com/.
[10] "QizX/open," http://www.xmlmind.com/qizxopen.
[11] "QizX Free-Engine-3.0," http://www.xmlmind.com/qizx/free engine.html.
[12] "eXist," http://exist.sourceforge.net/.
[13] D. Toman, "Point-based temporal extension of temporal sql," in DOOD, 1997, pp.
103-121.
[14] S. S. Chawathe, S. Abiteboul, and J. Widom, "Managing historical semistructured data," TAPOS, pp.
143-162, 1999.
[15] F. Rizzolo and A.
A. Vaisman, "Temporal XML: modeling, indexing, and query processing," VLDB Journal: Very Large Data Bases, pp.
1179-1212, Aug. 2008.
[16] P. Buneman, S. Khanna, K. Tajima, and W. C. Tan, "Archiving scientific data," ACM Trans.
Database Syst, pp.
2-42, 2004.
[17] F. Wang and C. Zaniolo, "Temporal queries and version management in XML-based document archives," Data Knowl.
Eng, pp.
304-324, 2008.
[18] F. Wang, C. Zaniolo, and X. Zhou, "Temporal xml?
sql strikes back!"
in TIME.
IEEE Computer Society, 2005, pp.
47-55.
[19] A. S. 0002, F. Waas, M. L. Kersten, M. J. Carey, I. Manolescu, and R. Busse, "Xmark: A benchmark for xml data management," in VLDB, 2002, pp.
974-985.
On-line transportation Scheduling using Spatio-Temporal Reasoning Maroua Bouzid GREYC - Universite de Caen BD du Marechal Juin 14032 Caen, France bouzid@info.unicaen.fr  Abstract  quires new extensions to existing methods in order to increase their efficiency and expressiveness.
Indeed, these techniques need to be adapted to provide adequate solutions to cope with applications characterized by a high level of uncertainty and rapid change.
These characteristics are common to several domains such as transportation scheduling and crisis situations.
Algorithms used to efficiently solve static scheduling problems such as classical techniques, Operations research, and centralized approaches have failed to address open dynamic scheduling problems in the presence of uncertainty.
Some investigators have offered approaches dedicated to dynamic scheduling problems in presence of uncertainty [6, 7].
Transportation scheduling application consists of distributed transportation companies that have to carry out transportation orders received asynchronously.
Each company is provided with a set of trucks.
Each company should optimize the use of its trucks when executing orders.
Several approaches have been suggested to this application such as MARS [5], TRACONET [1] based on the task delegation method and using the Contract Net Protocol techniques (CNP).
Existing approaches assume that each local plan (for each truck) is known with precision thus ignoring the presence of uncertainties.
However, the task of transportation is characterized by a high level of uncertainty regarding different factors such as the traffic density, the power of the truck used, etc... that cannot be ig-  In this paper we address the problem of the on-line transportation scheduling.
Previous approaches use an off-line scheduling algorithm that accounts for new transportation tasks only when trucks are at a destination.
To increase the performance of the system, we need to introduce the ability to account for the new transportation tasks immediately and to delegate it to a truck while this latter is moving toward a destination.
For this purpose, the system needs to address two issues : (1) to determine the current location of the truck and its proximity to the departure and arrival of the new transportation task, and (2) to respect the temporal constraints.
To this purpose, we use a space-time approach allowing us to deal with the first issue using a spatial reasoning, then address the second issue using a temporal reasoning.
This approach is a first step towards a design of temporally situated multi-agent system that allows us to take location and the time of agents existing at this location to determine the suitable actions.
Topics: Temporal Representation and Reasoning in IA, Temporal aspects of agent-based system  1.
Introduction The growing interest in the development of methods for large and complex applications re1  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  nored.
Consequently, the time at which a truck reaches destination is usually not known with precision, however, the uncertainty of the time interval during which the truck is due to arrive is well known.
During this interval the availability of the truck is weighted by uncertainty.
Consequently, during task transportation, there exist intervals during which truck's availability is not known precisely but it is assumed that it can be represented by a degree of uncertainty.
Furthermore, the formal framework used to represent the availability of trucks, in the existing approach, is so weak that no powerful reasoning can be performed.
to, 1. determine the trucks in movement to be considered and 2. to select the most suitable to respond to this new order.
In what follows, we mean by a system a temporally situated multiagent system that is able to deal with time and space.
This ability allows this situated multiagent system a good interaction between agents and their environments by using spatial structures and when to interact by using the temporal dimension.
This kind of multi-agent system is based on a perception-deliberation and action mechanism where the position in the environment defines the situations in which actions (considering a transportation order) can be considered.
In the same way, this system considers also the time at which agents are located in a given site to take actions and the time when they could be located in a future site to make predictions on their future actions.
In the rest of the paper we introduce some preliminary definitions necessary for our approach, we then recall the off-line scheduling algorithm [4] and finally we describe our new approach to consider real-time issues when executing orders.
We have developed a suitable temporal representation of trucks based on Temporal Characteristic functions (TCF) [3, 4].
A characteristic function is a (possibly partial) function describing for how long a logical property holds (or does not hold) - thus it refers to the idea of characteristic function of a set.
Roughly speaking, it can be also considered as a kind of twovalued 'trajectory' or 'history' characterizing the logical behavior given by certain atemporal formulae.
Moreover, the possibility of representing the uncertainty on the property validity over time follows naturally from our functional approach and leads to the Fuzzy Temporal Characteristic Function (FTCF).
FTCFs allow the representation of the uncertainty on the behavior of trucks due to traffic density and unpredictable events that prevent them from respecting their schedules.
Thus, the FTCFs allow the representation of the uncertainty that characterizes the availability of trucks during task transportation.
2.
Preliminaries 2.1.
Temporal characteristic functions In this Section, we introduce the idea of a temporal approach to knowledge representation based on characteristic functions [3].
Basically, a characteristic function is a function specifying the time intervals during which a logical property holds (or does not hold) - thus it refers to the idea of characteristic functions of a set.
A characteristic function for a given set Ph is any function psPh of the form: psPh : T -- L, where T is the global domain of interest and L denotes some set of values describing to 'what degree' an element of T belongs to Ph.
In this paper, only the sets Ph which have the form of {x : ph(x)} are considered; where ph is a given property.
Thus, a characteristic function is defined to be a function of the form psph : T -- L describing to "what degree" an element of T satisfies the property (formula) ph, or, for convenience, to what degree the property expressed  In [4], we have addressed the problem of allocating orders to trucks using an off-line FTCF-based algorithm, where new orders are taken into consideration only for the parked trucks.
In this paper, we extend our previous approach using a space-time reasoning in order to also consider the new orders that are for trucks in movement and to assess their ability to accept the new order.
For that purpose, a representation of the spatial information of trucks will be introduced, in particular, their itinerary and their neighborhood.
This spatial information allow us to develop a space-time reasoning 2  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  by ph is satisfied by any of the elements of T .
For further discussion, let us first establish the domain (maximal time interval) of interest.
We shall consider events happening after a given time lapse 0 and before +[?].
This defines the domain for all characteristic functions.
The formal definition of a characteristic function is as follows:  The idea of the knowledge representation language to be used consists in associating a propositional symbol p and a TCF ps for expressing explicitly when p is true, false or unknown over the time domain T .
Thus, the elementary objects of the language are pairs of the form p, ps.
The basic intuition concerning the semantics of p, ps is that some property p holds over time if the associated characteristic function ps takes 1 as its value, and does not hold if it takes 0 (for weak episodes, p is undetermined for undefined ps).
TCFs constitute formal means to represent some properties over time; they seem to be more general and more powerful than intervals.
Basically, a single function can represent 'behavior' of some property over the whole time domain T; thus, it represents the "history" of the a certain phenomenon.
Moreover, as opposed to intervals, they can be easily extended to deal with multiple-valued or fuzzy logics.
Definition 1 Let T = [0, +[?
][ be the time domain of interest and let L = {0, 1} be the set of distinguished values.
Any mapping ps : T -- L will be called a Temporal Characteristic  Function (TCF).
Any mapping ps : T -- L will be called a weak (partial) characteristic  function, if T [?]
T It is assumed that a TCF changes its value over T only a finite number of times.
Thus any TCF is an interval-stable function taking values 0 or 1 over time.
Any point of the domain where the function changes its value will be referred to as a change point, specific point or landmark.
When considering weak TCFs one may be especially interested in positive weak functions and negative weak ones.
2.2.
Fuzzy Temporal Characteristic Functions (FTCF) The idea of fuzzy characteristic functions consists in allowing the truth values to cover different degrees of truth from certainly false (0) to certainly true (1) passing through different scales of truth certainty.
i.e.
in terms of characteristic functions between 1 and 0.
Definition 2 A positive weak TCF is a weak TCF which takes as its value only 1 (inf {x|x = ps(t) f or t [?]
T } = 1).
A negative weak TCF is a weak TCF which takes as its value only 0 (sup{x|x = ps(t) f or t [?]
T } = 0).
Definition 3 Let T = [0, +[?
][ be the time domain of interest and let L = [0, 1] be the set (closed interval) of distinguished values.
Any mapping ps : T -- L will be called a Fuzzy Temporal Characteristic Function.
Any map ping ps : T -- L is called a weak (or partial)  FTCF, if T [?]
T .
Note that any TCF can be represented as a finite union of convex intervals denoting the largest intervals within which the function does not change its value[2].
Thus ps can be given by {(a1 , b1 ), .
.
.
, (ak , bk )} provided that avalues denote the beginnings of intervals for which the function has value 1 or the end of intervals for which the function has as value 0 and b-values denote the ends of respective intervals for which the function has the value 1 or the beginnings of intervals for which the function is equal to 0.
A similar representation can be applied to weak TCFs; however, the domain of the function must be given explicitly.
The problem of what value (0 or 1) is taken at the a and b-values can be solved arbitrarily depending on current needs.
Thus any FTCF takes as its values some real numbers from the closed interval [0, 1]; for simplicity we assume that the function is "sufficiently regular".
Note that any characteristic function satisfies the definition of FTCF (the opposite is not necessarily true).
Note that an arbitrary FTCF cannot be represented using just a set of intervals (the a and b values): the discussed extension yields a concept significantly more general than a simple interval.
3  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  3.2.
A truck selection algorithm  Furthermore, we a filtering operation for FTCF is an operation aimed at determining from a FTCF ps defined over T and a condition C, a  weak FTCF ps C defined on T [?]
T and such that ps C (t) satisfies C. In a formal way:  Let us consider O the set of orders received by a given company where each order oi is characterized by its deadlines Di before which the order should be satisfied, a duration dei representing an estimate of the time required to satisfy the order, and the worst-case duration dw i .
These durations are determined from statistical data collected from previous execution of the truck.
We use for the duration dei the average duration over the collected data and the duration dw i the average duration increased with the standard deviation computed from the same collected data.
Furthermore, we consider that each truck r has its FTCF psr that indicates at time t to what degree the truck is allocated psr (t) = p. Given a set of trucks R and a set of orders O, we need to generate a service schedule of the set O.
For this, the company, that we name in the following C1 , uses an algorithm based on the following steps:  Definition 4 Let ps be a FTCF defined over T and C a condition.
filtering(ps, T, C) is the weak fuzzy characteristic function ps C defined  on T [?]
T and such that: [?
]t [?]
T, ps C (t) =   ps(t), if ps(t) satisfies C undefined, otherwise  Example 1 Let ps be a FTCF defined on T and s [?]
[0, 1]: filtering(ps, T, ps(t) <= s) =  ps(t), if ps(t) <= s undefined, otherwise For more details about this formalism, we invite the reader to consult [3].
* Compute expected utilities for all orders oi , U tilityC1 (oi ) = Reward(oi )- Cost(dw i ); where cost is a function depending on the duration and charges to satisfy an order while Reward is a function representing the rewarded value gained when the order is satisfied, it can for example represent the amount of money that the company is wanting to earn.
3.
Off-line transportation scheduling approach: a temporal reasoning algorithm 3.1  Stating the problem  The transportation scheduling problem consists of a set of shipping companies that receive orders of the form "Load amount s of good g at location l1 and transport it to location l2 during a duration equal at most to dw before a deadline D. The system dedicated to solve this problem assigns each company's truck a specific time qualification.
The time qualification is a FTCF ps taking the value 1 when the truck is allocated and the value 0 when the truck is free.
The FTCF allows the representation of the uncertainty on the availability of the truck during the intervals where the availability is not known in a precise way.
The goal for each company is to optimize its service by executing the maximum number of orders and minimizing the allocation of truck-availability intervals.
* Sort the set O according to the utility of orders; * Satisfy the orders one by one as follows: - Search among the set of FTCFs of trucks those which are defined in the interval [Now, Di ] and of which values are less than a threshold s. This step is performed through a filtering operation to find these trucks.
Let Struck be the set of selected trucks.
- For each truck j [?]
Struck , let oi Ij,d e be the intervals such that dui oi w ration( Ij,d (if I = [x, y], e ) > di i duration(I) = y - x) 4  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  oi - Let Ik,min be the interval with the least duration for all trucks j [?]
oi = Struck : [?
]j [?]
Struck Ik,min oi arg(M IN (duration(Ij,de ))).
The i intuition behind the selection of the smallest truck-availability interval is 1. to maximize the utility of the truck and 2. to reduce the allocation of the truck to free it for other tasks.
an FCTF has a minimal tangent is the interval where the likelihood the truck be free is the highest.
* S c  = [?
]: this situation implies that there are intervals [x, y] in the interval [a, b] during which psr takes the value 0.
We select the interval [x, y] with the highest duration, y - x = M AX[xi ,yi ][?
]S c (yi - xi ).
For all cases, in order to compute the value of FTCF ps, we assume that ps is linear between two points (with known values of ps) and we then generate the linear equation representing ps between these points.
In some cases, the allocation of the interval is based on construction of an interval having the same middle as the one computed for the truck.
This strategy is motivated by the fact that we guarantee the allocation of the interval where the degree of availability is highest regardless of the fact that the allocation is not necessarily optimal.
The reader interested on the technical details on this section can see [4].
- Send to the selected truck, the order oi , its estimated and worst-case durations dei and dw i and the interval oi Ik,min .
3.3.
An order allocation algorithm The truck selected to carry out the order should allocate the interval during which it satisfies the order.
This operation consists in updating its FTCF by changing its value for every t [?]
I, where I is the interval during which it satisfies the order.
The truck r receives the order oi , its interval Ii =[a,b] during which it satisfies the order, the duration de required to satisfy the order and the duration dw as the worstcase duration to satisfy the order.
To update the FTCF, the agent has to assess several situations.
To optimize its service, the truck tries to allocate the interval where the probability to be free is the highest.
Consequently, the truck agent performs a filtering operation over its FTCF in Ii such that: filtering(psr , Ii , psr (t) = 0).
This operation enables us to select a set S c of intervals {Irc }.
There are two possible outcomes for the set S c :  4 On-line transportation scheduling : a spatio-temporal reasoning algorithm In the previous section, we consider situations where an agent can take a new order into consideration only when it doesn't move toward a destination.
In other words, an agent (truck) must reach destination to consider a new transportation order.
In this section, we relax this assumption by extending the approach presented in the previous section in order to allow an agent to consider new orders in real time.
An agent, with this extension, is able to analyze its current capacity to accept a new order on-line while moving toward its destination.
To do that, we develop a new approach using simple space-time reasoning.
This reasoning requires, in addition to the characteristic function, spatial information :  * S c = [?
]: this situation means that there is no interval during which the truck is sure to be free.
Then, we select intervals during which the probability that the truck is free is the highest.
Because a FTCF during these intervals is approximated with a linear function, the most important intervals are the ones with the smallest |tangent|.
Since the tangent allows us to measure the overall uncertainty over an interval (other measurements can be used such as the integral over the interval, but the tangent is sufficient to give us the required information).
Therefore, the interval during which  * Truck at a destination : the system needs to know its location.
* Truck moves toward a destination to achieve an order : in such situations, many 5  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  4.2.
Spatial reasoning to determine the appropriate set of trucks in movement  spatial information are necessary for the reasoning.
- the departure and arrival locations consist of the interval [l1 , l2 ] of locations where the truck loads goods to where it should unload them.
The spatial reasoning we have used consists in looking for the trucks moving toward the neighborhood N of the location l1 .
For that, consider truck i that is moving in its subinterval [lik , lik+1 ].
Many situations need to be studied :  - This interval is divided into a set of sub-intervals [[l1 , l11 ], [l11 , l12 ], .
.
.
, [l1k , l2 ]], where l1i is a location that will be visited by the truck while heading toward The locations are obtained l2 .
from a geographical database.
The sub-intervals allow the system to determine in real time and in a precise way the current location of the truck during its moving .
The system needs only to know in which subinterval [l1i , l1i+1 ] the truck is moving.
* lik+1 [?]
N : truck i is considered.
* lik [?]
N and lik+1  [?]
N : truck i should not be considered * lik  [?]
N and lik+1  [?]
N : truck i should not be considered.
4.3.
Spatio-temporal reasoning to allocate the order to moving trucks In the previous section, we introduced a spatial reasoning that determines the trucks in movement that can be considered for the new order allocation.
Let this set be T M. We need to locate each truck e [?]
T M according to departure and arrival locations of the new order.
Many cases need to be addressed.
Before that, let us define what we mean by two spatial relations that we use in the following :  - The deadline before which the truck should reach l2 - Finally, the system uses the time needed for loading and unloading goods at a location that we call thc and thd respectively.
4.1.
Accounting for a new order  1. overlap relation: we use it as an extension of the overlap relation defined by Allen.
Therefore, in our case, [l1 , l2 ] overlaps [l1 , l2 ] if the projection of [l1 , l2 ] on the axis defined by [l1 , l2 ] and [l1 , l2 ] overlap.
Let a new order be characterized by the transportation of goods from location l1 to l2 and the deadline of the delivery is D .
The allocation technique consists in determining the truck to which the order should be allocated.
The system takes all trucks into consideration including the ones moving towards a destination and that are not so far from the departure location l1 .
To determine trucks in movement that should be considered, the system determines trucks moving in the vicinity of the location l1 .
This vicinity can be determined as a circle around the location l1 with a ray K suitable to the distance l2 -l1 .
This approach needs first to determine all trucks able to satisfy the order (all truck moving inside the vicinity) and then select the most suitable for the order.
2. include relation: we use it as an extension of the include relation of Allen.
Therefore, [l1 , l2 ] is included in [l1 , l2 ] if the projection of [l1 , l2 ] on the axis defined by [l1 , l2 ] is included in [l1 , l2 ] 1.
[l1 , l2 ] and [l1 , l2 ] overlap such that l1 < l1 and l2 < l2 : in this situation, we locate exactly the sub-interval in which the truck moves, let it be [l1k , l1k+1 ].
The truck can drive towards l1 , get goods (of the new order) then determine what is the best policy.
Two policies need to be analyzed.
6  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  2.
[l1 , l2 ] [?]
[l1 , l2 ] : as in the previous case, we need to assess many policies 1.
All of the time constraint are modeled as follows :  * policy 1 : driving towards the first destination (l2 ) and then driving towards the new destination l2 .
l 1  k  l1  k'  k+1 l 1  l1  l2  1  * policy 1 : drive towards l2 and then towards l1 .
The time constraints for this policy are:  3  2  l '2  '  l1  l1  Figure 1.
[l1 , l2 ] and [l1 , l2 ] overlap * policy 2 :driving towards the new destination l2 and then driving towards the first destination l2 .
l1  k l1  k+1 l1  k' l1  k+1  k  l1  l2  l1  1  2 3 ' l1  l' 2  l2  1  Figure 3.
[l1 , l2 ] [?]
[l1 , l2 ]  3 2    ,l2  D >= tcurrent + tlk+1 1  D >= tcurrent + tlk+1 ,l2 + thd +  1  tl2 ,l1 + tl1 ,l2 + thc  l '2  '  l1  Figure 2.
[l1 , l2 ] and [l1 , l2 ] overlap In both policies, the time constraints (deadlines) should be respected such that: * policy 1 : The time constraints are :     ,l1 + thc + tl1 ,l2  D >= tcurrent + tlk+1 1   D >= tcurrent + tlk+1 ,l + thc +  1 1  tl1 ,l2 + tl2 ,l2 + thd * policy 2 : the time constraints are then :   D >= tcurrent + tlk+1 ,l + thc + tl1 ,l2   1 1  D >= tcurrent + tlk+1 ,l + thc + 1 1    t   + t  + th l1 ,l2  l2 ,l2  d  Where tl,l is a function measuring the time necessary to move from l to l', tcurrent is the current time, thc time of loading goods and thd time of unloading goods.
We assume that the time tlk ,lk+1 we take from the 1  1  * policy 2 : drives towards l1 , then towards l2 and finally towards l2 .
The time constraints are: l1  k' l1  k  l1  k+1  l2  l1  1 3 2 '  l1  l' 2  Figure 4.
[l1 , l2 ] [?]
[l1 , l2 ]       ,l1 + thc + tl1 ,l2  D >= tcurrent + tlk+1 1 D >= tcurrent + tlk+1 ,l + thc +  1 1  tl1 ,l2 + tl2 ,l2 + thd * policy 3 : drives towards l1 , then towards l2 and finally towards l2 .
The time constraints are:      D >= tcurrent + tlk+1 ,l1 + thc + tl1 ,l2 1   D >= tcurrent + tlk+1 ,l + thc + 1 1   tl1 ,l2 + tl2 ,l2 + thd    current location l1k to l1k+1 is negligible in comparison with tlk+1 ,l .
1 1 This assumption has no effect on our model.
7  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  l1  k  k+1  k' l1  l1 1  l  l1 2  '  l1  5.
Conclusion  2  In this paper, we have presented an extension of an off-line transportation scheduling algorithm to give trucks the ability to take new order online.
This approach combines the temporal reasoning developed in the previous approach based on the FTCF, with a spatial reasoning that allows to determine trucks to be considered.
We have developed a spatial reasoning with different policies and how trucks can determine the best policy using the shortest path criterion.
Finally we discuss the problem when many trucks can respond to the new order.
For this problem, we introduce a new measure based on the opportunity cost that allows a strategy to prefer a truck minimizing this cost over another.
Many other extensions could be studied in future works such as relaxing the assumption that a truck can be derived from its initial destination.
One question is how to extend this approach to derive the truck, once, twice, and so on ... as much as possible.
A stack of secondary orders need to be handled and some new criteria of selection can be merged.
Another issue that appears in this paper is the multi-criteria decision, especially for truck selection.
Finally, this work is an interesting step towards the development of time-situated Multi-Agents Systems.
The design of such systems will motivate our next research.
3  l' 2  Figure 5.
[l1 , l2 ] [?]
[l1 , l2 ] The policy selection is simply the policy with the shortest path strategy.
We can introduce other criteria to select policies or use some multi-criteria decision approaches that we do not address in this paper.
4.4.
A truck selection algorithm In the neighborhood N , many trucks can be selected for the new order.
In this section, we discuss the selection criterion to select the optimal truck.
In general, we can use the length of the path that a truck will follow and select the truck that will travel toward the shortest path.
This criterion should be considered by it is not sufficient because the truck moving towards the shortest path could be different from the cheapest.
To introduce this notion, we need to take into account the cost of loading the new goods given a current load of the truck.
For that, we introduce an opportunity cost OC measure of loading new goods (NO) such that :  References  OC(N O) = Cost(current load)-  [1] M. Andersson and T. Sandholm.
Timequality tradeoffs in reallocative negotiation with combinatorial contract types.
In Proceedings of the National Conference on Artificial Intelligence AAAI'99, pages 3-10, 1999.
Cost(current load + LOAD(N O)) where LOAD gives the weight of new goods and Cost function measures the cost of moving with a load at a given level.
The strategy simply consists in selecting the truck that minimizes this opportunity cost.
The selected truck is then determined by :  [2] M. Bouzid and P. Ladkin.
Simple reasoning with time-dependent propositions.
International Journal of Interest Group in Pure and Applied Logic (IGPL), 10(4):1- 21, 2002.  truck = min OCt (N O) t[?
]T M  When we have more than one truck, we can at this moment selects the truck with the shortest path.
[3] M. Bouzid and A. Ligeza.
Temporal logic based on characteristic functions.
In 8  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE  C. Rolliger I. Wachsmuth and W. Brauer, editors, Advances in Atrificial Intelligence, 19th Annual German Conference on Artificial Intelligence, volume 981 of Lecture Notes in Artificial Intelligence, pages 221- 232.
Speinger Verlag, 1995.
[4] M. Bouzid and A.I Mouaddib.
Uncertain temporal reasoning for the distributed transportation.
In Proceedings of TIME98, International Workshop on Temporal Representaion and Reasoning, pages 45- 56, 1998.
[5] K. Fischer, J.P. Muller, and M. Pischal.
Cooperative transportation scheduling: an application domain for dai.
Technical Report RR-95-01, 1995.
[6] A.I.
Mouaddib.
A study of a dynamic progressive reasoning system.
International Journal of Theoretic and Experimental Artificial Intelligence, 12:101-122, 2000.
[7] Thierry Vidal.
Dealing with temporal uncertainty and reactivity in a space mission plan.
In Proceedings of 2nd NASA International Workshop on Planning and Scheduling for Space, 2000.
9  Proceedings of the 10th International Symposium on Temporal Representation and Reasoning and Fourth International Conference on Temporal Logic (TIME-ICTL'03) 1530-1311/03 $17.00 (c) 2003 IEEE
2009 16th International Symposium on Temporal Representation and Reasoning  Building Logical Specifications of Temporal Granularities through Algebraic Operators Carlo Combi and Sara Degani Department of Computer Science, University of Verona (Italy) Strada le Grazie 15, I-37134 Verona, Italy e-mail: {carlo.combi, sara.degani}@univr.it  means of logical formulae defined over these structures.
The algebraic approach assumes a bottom granularity, and defines a finite number of algebraic operators that generate new granularities from existing ones.
A time granularity may therefore be identified by means of an algebraic expression [7].
Algebraic operators are intuitive and new granularities may be easily derived from existing ones.
Nevertheless, in algebraic frameworks few attention is given to the investigation of algorithms checking the existence of meaningful relationships between granularities, e.g., verifying if the granularity G1 is equivalent to the granularity G2 ; moreover, only a finite number of time granularities may be expressed in this kind of approach through a finite number of expressions.
In this paper we consider the logical framework of Combi et al.
[3], that represents time granularities by means of labelled linear time structures, identified by suitable Past Propositional Linear Time Logic formulae; this framework allows one to express both anchored and unanchored granularities (i.e., possibly infinite granule sets, anchored to different time points on the time domain).
However, formulae corresponding to real world granularities may correspond to long formulae, not easy to read and understand.
The goal of the approach we present here is to allow one to define a large set of granularities, possibly corresponding to complex formulae, in a natural way, as it happens with the algebraic approach.
To this end, we show how the basic operators of the calendar algebra proposed by Ning et al.
[7] can be expressed on a labelled linear time structure by means of suitable logical PPLTL formulae.
This way, it is possible to specify granularity-related formulae in an easier way through operators that are intuitive and natural as the algebraic ones, still maintaining the capability of reasoning on granularities through the (formal) techniques that have been deeply investigated for the logical framework [3].
The paper is structured as follows.
Section II introduces time granularities and presents the considered logical and algebraic frameworks; Section III defines new logical formulae encoding algebraic operators.
Section IV concludes the paper with final discussion and remarks.
Abstract--Logical and algebraic approaches have been proposed in the literature for the definition and the management of time granularities.
In the algebraic framework, a bottom granularity is assumed, and new granularities are created by means of suitable algebraic operators.
In the logical framework, mathematical structures are used to represent time granularities, and logical formulae are used to specify meaningful properties betweeen them.
In this paper we extend the logical approach of Combi et al.
[3] for representing and reasoning about temporal granularities, defining logical formulae corresponding to calendar algebra operators.
This approach allows one to define a large set of granularities, possibly corresponding to complex formulae, in a natural way.
Indeed, it gives the possibility of specifying granularityrelated formulae in an easier way through operators that are intuitive and natural as the algebraic ones, still maintaining the capability of reasoning on granularities through the (formal) techniques that have been deeply investigated for the logical framework.
Keywords-temporal granularity; temporal logics; granularity specifications.
I. I NTRODUCTION A fact of the real world is usually described with reference to the temporal dimension, expressed by means of time units such as day, month, year.
A time unit can be represented by means of a time granularity, i.e., the representation on a temporal domain of groups of elements that are perceived as inseparable units called granules.
Furthermore, in several research areas of computer science, such as formal specifications, data mining, and temporal databases it is considered of primary importance the capability of providing temporal representations of the temporal dimension of a fact at different levels of granularity, and of managing the relationships between them.
Logical and algebraic approaches have been proposed for the definition and the management of time granularities in the literature [3], [4], [7].
In the logical approach, mathematical structures are used to represent granularities and relationships between them; these structures consist of a possibily infinite set of different time domains.
Specific operators allows one either to express properties in a given time domain or to relate different time domains.
Properties involving different time granularities are then expressed by 978-0-7695-3727-6/09 $25.00 (c) 2009 IEEE DOI 10.1109/TIME.2009.19  107  II.
BACKGROUND  specifically, a linear temporal axis is considered, and the granules of a granularity G are represented on it by means of the propositional symbols PG and QG : the starting point of a granule of G in the structure is labelled by PG , the ending point is labelled by QG .
Definition 3: Let G = {G1 , .
.
.
, Gn } be a finite set of granularites, (a calendar), and PG = {PGi , QGi |1 <= i <= n} a finite set of proposition symbols associated with the calendar G. Given an alphabet of proposition symbols P [?]
PG , a P-labelled linear time structure has the form (N, <, V ), where (N, <) is the set of natural numbers with the usual order and V : N - 2P is a labelling function that maps natural numbers to sets of proposition symbols.
  The main approaches for representing and managing time granularities, we will consider in this paper, are the logical [3], [4] and the algebraic [7] ones.
Before presenting in sections II-A and II-B the logical framework of Combi et al.
[3], and the algebraic approach of Ning et al.
[7], let us start with the introduction of some basic and widely acknowledged notions about time granularities [1].
Definition 1: A time domain is a pair (T , <=), where T is a non-empty set of time instants and <= is a total order on T.  A time domain is a set of primitive temporal entities used to define and interpret time-related concepts, and ordered according to the relationship <=.
Examples of time domains are natural numbers (N, <=), integers (Z, <=), rational (Q, <=) and real numbers (R, <=).
Definition 2: A granularity is a mapping G : Z - 2T such that (1) if i < j and G(i) and G(j) are non-empty, then each element of G(i) is less than all the elements of G(j), and (2) if i < k < j and G(i) and G(j) are nonempty, then G(k) is non-empty.
  Not every assignment of proposition symbols to the timeline by means of the labelling function V defines a time granularity.
Therefore, the framework introduces the notion of consistency of a labelled structure with respect to a temporal granularity G, allowing one to identify assignments that define time granularities: a labelled linear time structure (N, <, V ) is said G-consistent if every point labelled with PG matches with a unique point labelled with QG , and vice versa.
The syntax of PPLTL is defined as follows.
Definition 4 (Syntax of PPLTL): Formulae of PPLTL are inductively defined as follows:  The domain of a granularity G is called index set, and an element of the codomain G(i) is called a granule.
A granularity G is said externally continuous if there are no gaps between non-empty granules of G; internally continuous if there are no gaps inside the granules of G; continuous if it is both externally and internally continuous.
Given two granularities G1 and G2 , a number of meaningful relationships can be estabilished between them: * FinerThan(G1 , G2 ) holds if for each index i, there exists an index j such that G1 (i) [?]
G2 (j).
For instance, FinerThan(day, month) holds.
* SubGranularity(G1 , G2 ) holds if for each index i there exists an index j such that G1 (i) = G2 (j).
For example, Subgranularity(Sunday, day) holds.
* Group(G1 , G2 ) holds if for each index i, there exists a set S (either empty or in S the form {j, j + 1, .
.
.
, j + k}) such that G2 (i) = l[?
]S G1 (l).
For example, Group(day, week) holds.
* Shift(G1 , G2 ) holds if there exists p >= 0 such that, for each index i, G2 (i) = G1 (i + p).
* Partition(G1 , G2 ) holds if both Group(G1 , G2 ) and FinerThan(G1 , G2 ) hold.
* * *  a proposition symbol P [?]
P is a PPLTL formula; if ph and ps are PPLTL formulae, then ph [?]
ps and !ph are PPLTL formulas; if ph and ps are PPLTL formulae, then Xph, phUps, X-1 ph and phSps are PPLTL formulae.
 Moreover: * * * * * * *  ph [?]
ps is defined as: !
(!ph [?]
!ps); ph - ps is defined as: !ph [?]
ps; ph - ps is defined as: (ph - ps) [?]
(ps - ph); Fa (a will hold in the future) is defined as: trueUa; Ga (a will always hold in the future) is defined as: !F!a; Pa (a held in the past) is defined as: trueSa; Ha (a always held in the past) is defined as: !P!a.
where true = P [?]
!P , for a some P [?]
P. Temporal operators in {X, U, X-1 , S} have priority over boolean operators {[?
], [?
]}; moreover !
has priority over [?]
and over [?
], and [?]
has priority over [?].
PPLTL logic is interpreted in P-labelled linear time structures.
The semantics of PPLTL is defined as follows.
Definition 5 (Semantics of PPLTL): Let M = (N, <, V ) be a P-labelled linear time structure and i [?]
N. The truth of a PPLTL formula ph in M with respect to the point i, denoted with M, i |= ph, is defined as follows:  A.
The Logical Framework In the logical framework introduced in [3], time granularities are modelled by means of a specialized version of Definition 2, assuming that both the index set and the domain of granules are the linear discrete domain (N, <=).
Moreover, an internally continuous time granularity G is represented by means of a labelled linear time structure, identified by a suitable linear time formula in the context of Past Propositional Linear Time Logic (PPLTL).
More 108  M, i |= P M, i |= ph [?]
ps M, i |= !ph M, i |= phUps  iff iff iff iff  M, i |= Xph M, i |= phSps  iff iff  M, i |= X-1 ph  iff  P [?]
V (i) for P [?]
P; M, i |= ph and M, i |= ps; is not the case that M, i |= ph; M, j |= ps for some j >= i and M, k |= ph for each i <= k < j; M, i + 1 |= ph; M, j |= ps for some j <= i and M, k |= ph for each j < k <= i; i > 0 and M, i - 1 |= ph.
The above proposal copes with internally continuous granularities.
In [3] it is extended to also cope with granularities with gaps inside the granules.
The set of proposition symbols associated with the granularity G is extended to include the symbols PHG and QHG , that are used to delimit any gap inside the granules of G. The description of gaps of G is therefore the description of a granularity HG , such that FinerThan(HG , G) holds.
The definition of consistency is extended to also consider gaps inside the granules of a granularity: a labelled linear time structure is said G-gapconsistent if it is G-consistent and HG -consistent, if each granule of HG is a subset of some granule of G, and if no granule of G is the union of some granules of HG .
A G-gap-consistent linear time structure corresponds to a (not necessarely continuous) granularity, and vice versa.
The set of (not necessarely continuous) granularities Gran(G) is then expressed by the following formula:   We say that: * M is a model of ph if M, 0 |= ph; * a formula ph is satisfiable if and only if there exists a P-labelled linear time structure that is a model of ph; * a formula ph is valid if and only if every P-labelled linear time structure is a model ph.
Note that ph is valid if and only if !ph is not satisfiable.
A PPLTL formula intensionally defines a possibly infinite set of labelled linear time structures (the set of the models of the formula).
Since a linear time structure defines a granularity, linear time logic formulae can be used to define possibly infinite sets of granularities.
In the following, according to [3], we show some examples of PPLTL formulae defining sets of time granularities.
The set of internally continuous granularities is captured by the following PPLTL formula:  FinerThan(HG , G) [?]
G(PG - !a) where a is PHG [?]
((QG [?]
QHG )[?]
X((!QG [?]
(QHG - XPHG ))U(QG [?]
QHG ))).
B.
The Algebraic Framework  IntContGran(G) = G((PG - a) [?]
(QG - b)), where a is b is  The algebraic framework presented in [7] considers an extension of Definition 2, that associates a label to each granule.
A labelled granularity is a pair (L, G), where L is a (possibly non contiguous) subset of Z, and G is a mapping from L to the subset of the time domain T ; L is the set of labels of G, and its elements may be used to identify the granules of a granularity.
An extension of some of the relationships between granularities to labelled granularities is then provided [7].
In particular:  QG [?]
X(!
(PG [?]
QG )U(!PG [?]
QG )), and PG [?]
X-1 (!
(PG [?]
QG )S(PG [?]
!QG )).
The first conjunct (PG - a) of the formula states that every point labelled with PG matches with a unique point labelled with QG , while the second conjunct (QG - b) states that every point labelled with QG matches with a unique point labelled with PG ; in other words, they capture the definition of consistency of a granularity.
The set of continuous granularities is captured by the formula ContGran(G), defined in a similar way [3].
Binary relations between granularities may be expressed by means of PPLTL formulae involving a finite number of different granularities, represented with different pairs of marking proposition symbols.
The relationship FinerThan(G1 , G2 ) is captured by the following formula:  *  *  IntContGran(G1 ) [?]
IntContGran(G2 ) [?]
G(PG1 - ((!PG2 [?]
!QG2 )SPG2 [?]
(!PG2 [?]
!QG2 )UQG1 )).
LabelledAlignedSubgranularity(G1 , G2 ) holds if each label i of G1 is also a label of G2 , and G1 (i) = G2 (i).
For instance, if each Sunday has the same label of the corresponding day, then LabelledAlignedSubgranularity(Sunday, day) holds.
Partition(G1 , G2 ) hols if both Group(G1 , G2 ) and FinerThan(G1 , G2 ) hold.
For instance, Partition(day, week) holds.
The algebraic approach defines a symbolic representation of granularities termed calendar algebra [7].
This representation captures the relationships between the granularities of a calendar, and it is composed of two kinds of operations: grouping-oriented operations and granule-oriented operations.
Grouping oriented operations combine certain granules of a granularity together to form the granules of the new granularity, while granule oriented operations make  The formula requires each granule of G1 to be included into some granule of G2 ; more than one granule of G1 may be included in the same granule of G2 .
The relationship SubGranularity(G1 , G2 ) is expressed in a similar way [3].
109  3) The Shifting Operation: The shifting operation is defined as follows [7].
Definition 8: Let G be a granularity, and let m be an integer.
The operation Shiftm (G) generates a new granularity G0 by performing a shift of m positions of the labels of G. Formally, for each integer i, G0 (i) = G(i - m).
  choices of which granules should remain in the new granularity.
In the following we present the grouping oriented operations, and the granule-oriented operation Select-down.
1) The Grouping Operation: The grouping operation is defined as follows [7].
Definition 6: Let G be a granularity, and m a positive.
The grouping operation Groupm (G) generates a new granularity G0 by partitioning the granules of G in groups of m granules; each group is a granule of G0 .
Formally, for each Si*m 0 integer i, G (i) = j=(i-1)*m+1 G(j).
  The shifting operation can be used to express time differences.
For example, since the hours of New York are three hours later than those of Los Angeles, the hours of New York can be generated by LA-Hour = Shift-3 (NY-Hour), where the granularities NY-Hour and LA-Hour stand for the hours of New York and Los Angeles respectively.
4) The Combining Operation: The combining operation is defined as follows [7].
Definition 9: Let G1 and G2 be two granularities with label sets L1 and L2 respectively.
The operation Combine(G1 , G2 ) generates a new granularity G0 by combining in a granule of G0 all the granules of G2 that are included in a granule of G1 .
Formally, for each i [?]
L1 , let  [?]
if G1 (i) = [?
], s(i) = {j [?]
L2 |[?]
= 6 G2 (j) [?]
G1 (i)} otherwise.
For example, week = Group7 (day).
2) The Altering-tick Operation: Let G1 and G2 be two granularities, such that Partition(G2 , G1 ) holds, and let l, k, m be integers such that 1 <= l <= m. Since Partition(G2 , G1 ) holds, each granule of G1 is composed of some contiguous granules of G2 ; moreover, the granules of G1 may be partitioned in groups of m granules.
The altering-tick operation Alterm l,k (G2 , G1 ) modifies the granules of G1 so that the lth granule of each of each group has |k| additional (or fewer when k < 0) granules of G2 .
For each i = l + m * n, with n [?]
Z, G1 (i) is the granule that is expanded (or shrunk).
If i > 0, G1 (i) expands (or shrinks) by taking in (or pushing out) later granules of G2 , and the effect is propagated to later granules of G1 ; if i < 0, G1 (i) expands (or shrinks) by taking in (or pushing out) earlier granules of G2 , and the effect is propagated to earlier granules of G1 .
Definition 7: Let G1 and G2 be two granularities, such that Partition(G2 , G1 ) holds, and let l, k, m be integers such that 1 <= l <= m. For each integer i such thatSG2 (i) 6= [?
], ti let bi and ti two integers such that G2 (i) = j=b G1 (j) i (they exist because Partition(G1 , G2 ) holds).
Then G0 = Alterm l,k (G1 , G2 ) is the granularity such that for each integer i: ( [?]
if G2 (i) = [?]
St01 G0 (i) = G (j) otherwise 1 j=b0  Then, G0 = Combine(G1 , G2 ) is the granularity with label 0 set LG0 = S {i [?]
L1 |s(i) 6= [?]}
such that for each i [?]
LG , G0 (i) = j[?
]s(i) G2 (j).
 For example, given the granularities month and businessDay, the granularity for business months can be generated by businessMonth = Combine(month, businessDay), since each business month is the union of all the business days in the month.
5) The Anchored-grouping Operation: The anchoredgrouping operation is defined as follows [7].
Definition 10: Let G1 be a simple granularity and G2 be a granularity with label set L2 , such that LabelAlignedSubGranularity(G2 , G1 ) holds.
The operation Anchored-group(G1 , G2 ) generates a new granularity G0 by combining in a granule of G0 all the granules of G1 that are between two granules of G2 .
Formally, G0 = Anchored-group(G1 , G2 ) is the granularity with label set LG0 = L2 such that for each i [?]
LG0 , G0 (i) = Si0 -1 0 j=1 G1 (j), where i is the next label of G2 after i.
  i  where b0i =    bi + (h - 1) * k bi + h * k  if i = (h - 1) * m + l, otherwise,  For example, if a fiscal year begins in October and ends in the next September, the granularity that corresponds to fiscal years can be generated by fiscal-year = Anchoredgroup(month, October).
6) The Select-down operator: The select-down operation is defined as follows [7].
Definition 11: Let G1 and G2 be two granularities and let k, l two integers such that k 6= 0 and l > 0 be two integers.
The operation Select-downlk (G1 , G2 ) selects granules of G1 by picking up l granules starting from the kth one in each set of granules of G1 that are contained in a granule of G1 .
t0i = ti + h * k, and h=b  i-l c + 1. m   For example, if G = Group30 (day) (i.e., G has granules of 30 days), the operation Alter12 5,1 (day, G) adds a day (k = 1) to the fifth group of each set of 12 of such groups (l = 5, m = 12).
110  1  l Formally, G0 = Select-down k (G1 , G2 ) is the granularity S with label set LG0 = i[?
]L2 dkl ({j [?]
L1 |[?]
= 6 G1 (j) [?]
G2 (i)}), and for each i [?]
LG0 , G0 (i) = G1 (i).
  2  Q  P  P  G1  III.
L OGICAL D EFINITION OF A LGEBRAIC O PERATORS  6  Q  9 10 11 12 13 14 15 16 17 18 19  8  7  Q  G1  G1  H G2  G2  In this section we define the previously described algebraic operators by means of logical formulae according to the framework presented in Section II-A.
In the following, instead of providing formal proofs that the introduced formulae are equivalent to the corresponding algebraic expressions, we discuss and introduce the meaning of the different parts of formulae, to allow one to have an overall comprehension of the proposed approach.
Examples throughout this section and a real-world example related to the medical domain at the end of this section will complete the description of the introduced formula.
5  P  P  For example, Sunday = Select-down11 (day, week)  4  3  H G2  Figure 1.
Q  Q  P  G1 G1  P  G1  G1  Q  P  H G1  P  P  Q  H G2  G2 G2  Q  H G1 G 1  H G2  Q  G2  The grouping operation  conditions on the beginning and the end of the granules of G3 it is necessary to quantify such expansion, since it affects the definition of later granules of G3 .
Figure 2 represents the grouping of the granules of G2 in groups of two granules (m=2); for the first two of such groups (p=2) the operation expands the second granule (l=2) with a granule of G1 (k=1).
In order to mark the beginning and the end of the granule of G3 corresponding to the second group of granules of G2 , it is necessary to consider the expansion of the earlier group; indeed, the label PG3 of the second granule of G3 is shifted of a granule of G1 with respect to the third granule of G2 .
More in general, the size of the expansion depends on which group of m granules of G2 we consider to generate a granule of G3 .
Since the number of such groups cannot be a priori determined, it is not possible to define a formula that captures the altering-thick operation on all of these groups; we can however define a formula capturing the operation on the first p groups, with p > 0.
We therefore define a new m,p operation Alterl,k (G1 , G2 , G3 ) that performs the altering thick operation on the first p groups of m granules of G2 .
A.
The Grouping Operation We define the grouping operation Group(G1 , G2 , m) as follows.
Definition 12: Let G1 be a granularity, and m an integer.
The granularity G2 obtained through the algebraic operator Groupm (G1 ), may be specified by the following PPLTL formula: Group(G1 , G2 , m) = Gran(G1 ) [?
]Gran(G2 ) [?
](!PG1 [?
]!PG2 )U(PG1 [?
]PG2 ) [?]
G(PHG2 - (PHG1 [?]
(!PG1 [?]
X-1 QG1 )) [?]
QHG2 - (QHG1 [?]
(!QG1 [?]
XPG1 ))) [?]
G(PG2 - am-1 ), where a0 is PG1 - !
(QG1 [?]
QG2 )U(QG1 [?]
QG2 ) [?]
!PG1 - (!PG1 )U(PG1 [?]
!
(QG1 [?]
QG2 )U(QG1 [?]
QG2 )), and, for k > 0, ak is recursively defined as PG1 - !
(QG1 [?]
QG2 )U(QG1 [?]
!QG2 [?]
Xak-1 ) [?]
!PG1 - (!PG1 )U(PG1 [?]
!
(QG1 [?]
QG2 )U(QG1 [?]
!QG2 [?]
Xak-1 )).
  1  2  P Q  3  4  P Q  5  6  P Q  7  8  P Q  9 10 11 12 13 14 15 16 17 18 19 20  P Q  P Q  P Q  P Q  P Q  P Q  G1 G1 G1 G1 G1 G1 G1 G1 G1 G1 G1 G1 G1 G1 G1 G1 G1 G1 G1 G1  With the first two conjuncts, the formula requires G1 and G2 to be (possibly non continuous) granularities.
The third conjunct requires G1 and G2 to begin at the same time instant; the fourth conjunct states that a gap in G2 may exists only if there exists a gap in a granule of G1 , or if there exists a gap between two granules of G1 that belong to the same granule of G2 .
The last conjunct states that a granule of G2 is composed of m consecutive granules of G1 .
Figure 1 shows an example where m = 2.
P  Q  P  Q  G2  G3  Q  P  G2 G2  P  G3 G3  Figure 2.
Q  P  G2 G2  Q  P  G3 G3  Q  P  G2 G2  Q  P  G2 G2  P  G3 G3  Q  G2  Q  G3  The altering-tick operation  Definition 13: Let G1 and G2 be two granularities such that Partition(G1 , G2 ) holds, and let l, k, m, p be positive integers such that 1 <= l <= m. The granularity G3 obtained through the algebraic operator Alterm l,k (G2 , G1 ) (for the first p groups of m granules of G2 ), may be specified by the following PPLTL formula:  B.
The Altering-tick Operation In order to define a logical formula corresponding to the operator Alterm l,k (G1 , G2 ), we assume l, k, m to be positive integers (i.e., we only consider the expansion of the granules of G2 with later granules of G1 ).
Moreover, we need to make a further assumption.
The granules of G3 are defined by expanding some granules of G2 .
In order to estabilish the  m,p Alterl,k (G1 , G2 , G3 ) = ContGran(G1 ) [?]
ContGran(G2V ) [?]
ContGran(G3 )[?]
p Partition(G1 , G2 ) [?]
i=0 am*i  111  Shif t(G1 , G2 , m) = Gran(G1 ) [?]
Gran(G2 ) [?]
am where a0 = G((PG1 - PG2 ) [?]
(QG1 - QG2 )[?]
(PHG1 - PHG2 ) [?]
(QHG1 - QHG2 )) and, for q > 0, am = !
(QG1 [?]
PG2 )U(QG1 [?]
!PG2 [?]
Xam-1 )  The first row of the formula requires G1 and G2 to be (possibly non internally continuous) granularities.
Formula am requires that there are no granules of G2 until the mth granule of G1 ; finally, formula a0 states that, starting from the (m + 1)th granule onwards, G1 and G2 coincide, including gaps inside granules.
where, for n > 0, a0 =PG2 - dl-1 an =(!QG2 )U(QG2 [?]
Xan-1 ) d0 =bk*i [?]
(!QG2 )U(QG2 [?]
gk*(i+1) )[?]
X((!PG2 )U(PG2 - phm-l-1 )) dn =bk*i [?]
(!QG2 )U(QG2 [?]
gk*i )[?]
X((!PG2 )U(PG2 - dn-1 )) ph0 =bk*(i+1) [?]
(!QG2 )U(QG2 [?]
gk*(i+1) ) phn =bk*(i+1) [?]
(!QG2 )U(QG2 [?]
gk*(i+1) )[?]
X((!PG2 )U(PG2 - phn-1 )) b0 =PG3 bn =(!QG1 )U(QG1 [?]
Xbn-1 )  1  2  3  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20  8  g0 =QG3 gn =(!QG1 )U(QG1 [?]
Xgn-1 )  P  G1    Q  P G1  G1  Q  P  G2  The first three conjuncts require G1 , G2 and G3 to be continuous granularities; the formula Partition(G1 , G2 ) is defined as Group(G1 , G2 ) [?]
FinerThan(G1 , G2 ).
The formula am*i identifies the ith group of m granules of G2 ; in such group, the formula dn , for n > 0, estabilishes that the granules of G3 corresponding to the granules of G2 before the lth granule, are shifted of k * i granules of G1 with respect to the corresponding granules of G2 .
The formula d0 estabilishes that the granule of G3 corresponding to the lth granule of G2 of the considered group, begins k * i granules of G1 after the beginning of the corresponding granule of G2 , and ends k * (i + 1) granules of G1 after the ending of the same granule.
Finally, the formula phn , with n >= 0, requires the granules of G3 corresponding to the granules of G2 after the lth granule in the considered group, to be shifted of k * (i + 1) granules of G1 with respect to the granules of G2 .
Figure 2 shows the granularities identified by the formula 2,2 Alter2,1 (G1 , G2 , G3 ).
The formula a2 identifies the second group of two granules of G2 ; d1 estabilishes that the granule of G3 corresponding to the first granule of G2 in such group, is shifted of 1 granule of G1 with respect to the same granule.
Such shifting is caused by the expansion performed on the (only) earlier group of 2 granules of G2 .
The granule of G3 corresponding to the second granule of G2 of the considered group, starts 1 granules of G1 after the beginning, and ends 2 granules of G1 after the ending of the same granule.
Q  P  G1 G1  Q  Figure 3.
P G1  P G2  P  G 1 H G1  G2  P  H G2  Q Q  H G 1G 1  Q Q  H G2 G 2  P  G1  P  G2  Q  G1  Q  G2  The shifting operation  D. The Combining Operation We define the operation Combine(G1 , G2 , G3 ) as follows.
Definition 15: Let G1 and G2 be two granularities.
The granularity G3 obtained through the algebraic operator Combine(G1 , G2 ) may be specified by the following PPLTL formula: Combine(G1 , G2 , G3 ) = Gran(G1 ) [?]
Gran(G2 ) [?]
Gran(G3 ) [?]
G((PG3 - a) [?]
(QG3 - b))[?]
G(PHG3 - (PHG2 [?]
(!PG2 [?]
X-1 QG2 )) [?]
QHG3 - (QHG2 [?]
(!QG2 [?]
XPG2 ))).
where a is (!PG1 [?]
!QG1 )U(QG3 [?]
QG2 [?]
(QG1 [?]
X((!PG3 [?]
!QG2 )UQG1 ))), and b is (!PG1 [?]
!QG1 )S(PG3 [?]
PG2 [?]
(PG1 [?]
X-1 ((!QG3 [?]
!PG2 )SPG1 )))  The first implication (PG3 - a) states that if a granule of G1 is included in at least a granule of G2 , then the granule of G3 that contains it ends with the last granule of G2 that is included in that granule of G1 .
The second implication (QG3 - b) states that if a granule of G1 is included in at least one granule of G2 , then the granule of G3 containing it begins with the first granule of G2 that is included in that granule of G1 .
The last two rows of the formula state that a gap in a granule of G3 may exist only if the same gap exists in the corresponding granule of G2 , or if a gap exists between two  C. The Shifting Operation We define the operation Shif t(G1 , G2 ) as follows.
Definition 14: Let G1 be a granularity and m an integer.
The granularity G2 obtained through the algebraic operator Shiftm (G1 ) may be specified by the following PPLTL formula: 112  granules of G2 belonging to that granule of G1 .
Figure 4 shows an example.
1  2  4  3  5  6  P Q  Q  G2  G3  Q  G1  P  G2  Q  G2  P  P  Q  H G3  G3  Q  P G2  H G3  Figure 4.
Q  G1  P  Q  G1  Q  P  G2 G2  G2  P  G2  P  Q  G3  P October  Month  P Month  Month  P Month  Q Month  Q  P Month  P October  P  October  P Q  P  G3 G3  Q  Q  P  G2  P Q G3  G3  P Q  G3 G3  P  G3 G3  Q  G2  G3  The Select-down operation  ak =(!PG1 [?]
!PG3 )U(PG1 [?]
Xan-1 ) b0 =!PG3 UQG2 bn =(!PG1 [?]
!PG3 )U(PG1 [?]
PG3 [?]
Xbn-1 )  The third conjunct of the formula (PG2 - ak ) considers each granule of G2 separately; the formula ak identifies the k th granule of G1 in the group of granules of G1 that are included in the considered granule of G2 ; the formula bl specifies that, for the subsequent l granules of G1 , a corresponding granule of G3 exists.
Figure 6 shows an example where k = 2 and l = 3.
G. A Clinical Example In order to show the benefits of the proposed logical specifications of the algebraic operations, we consider a realworld example taken from the clinical scenario.
The example deals with one of the several chemotherapy recommendations that can be represented through a set of time granularities, and models these time granularities in a labelled linear time structure, identified by a suitable PPLTL formula.
The chemotherapy recommendation is defined as follows: "The ChlVPP regimen consists of chlorambucil (6 mg/m2 /day) on days 1 through 14, vinblastine (6 mg/m2 ) on days 1 and 8, procarbazine (100 mg/m2 /day) on days 1 through 14, and prednisone (40 mg/day) on days 1 through 14.
Patients treated with this regimen receive 6 cycles repeated every 28 days."
Figure 7 shows a graphical representation of a single cycle of the regimen.
The labelled linear time structure corresponding to the granularities representing the chemotherapy-related granu-  Q Month  Q October  FiscalYear  Figure 5.
P Q  a0 =PG1 [?]
PG3 [?]
bl  Q  FiscalYear  P Q  The operation SelDownlk (G1 , G2 , G3 ) is defined as follows.
Definition 17: Let G1 and G2 be two granularities, and k, l two integers.
The granularity G3 obtained through the algebraic operator Select-downlk (G1 , G2 ) may be specified by the following PPLTL formula: SelDownlk (G1 , G2 , G3 ) = Partition(G1 , G2 ) [?]
Subgranularity(G1 , G3 ) [?]
PG2 - ak where  The combining operation  Q  P Q  F. The Select-down Operation  The second and the third row of the formula state that each granule of G2 has to be equal to a granule of G1 , and possible gaps inside a granule of G2 has to be equal to the gaps inside the corresponding granule of G1 (i.e., that the relationship SubGranularity(G1 , G2 ) holds).
The fourth and the fifth rows state that a granule of G3 may have gaps only if G1 has the same gaps inside its granules, or corresponding to a gap between two non consecutive granuels of G1 .
Finally, the last row states that a ganule of G3 begins at the same time instant of a granule of G2 , and ends at the time instant that precedes the beginning of the next granule of G2 .
Figure 5 shows an example.
Q  P Q  G3 G3  G2  Figure 6.
The operation AnchoredGroup(G1 , G2 , G3 ) is defined as follows.
Definition 16: Let G1 and G2 be two granularities such that SubGranularity(G1 , G2 ) holds.
The granularity G3 obtained through the algebraic operator Anchoredgroup(G1 , G2 ) may be specified by the following PPLTL formula: AnchoredGroup(G1 , G2 , G3 ) = Gran(G1 ) [?]
Gran(G2 ) [?]
Gran(G3 ) [?]
G(PG1 - (PG2 [?]
!
(QG1 [?]
QG2 )U(QG1 [?]
QG2 )) [?]
PHG1 - (PHG2 [?]
!
(QHG1 [?]
QHG2 )U(QHG1 [?]
QHG2 ))) [?]
G(PHG3 - (PHG1 [?]
(!PG1 [?]
X-1 QG1 )) [?]
QHG3 - (QHG1 [?]
(!QG1 [?]
XPG1 ))) [?]
G(PG3 - (PG2 [?]
X((!PG2 [?]
!QG3 )U(QG3 [?]
XPG2 ))).
  Month  P Q  G1  E. The Anchored-grouping Operation  P  P Q  G2  Q  G3  G3  P Q  P  P Q P  P Q  G1 G1 G1 G1 G1 G1 G1 G1 G1 G1 G1 G1 G1 G1 G1 G1 G1 G1 G1 G1  9 10 11 12 13 14 15 16 17 18 19 20  8  7  P Q  The anchored grouping operation  113  P  approach.
Moreover, calendar algebra provides users with a large set of operators simplifying the definition of even complex granularities.
We proposed logical formulae encoding grouping-oriented operators of the calendar algebra [7] such as grouping, altering-tick, shifting, combining and anchoredgrouping, and the granule-oriented operation select-down.
The logical formula encoding the altering-tick operator performs the operation only on the first p groups of m-granules of the first granularity, and deals with internally continuous granularities.
The remaining logical formulae deal even with non internally continuous granularities.
Founding the operators of the calendar algebra on a logical framework allow us to use all the machinery studied and developed for temporal logics; as an example, in the considered medical domain it could be of interest to instantiate some chemotherapy cycles.
It corresponds to the generation of granularities, that can be reduced to the problem of generating models for the formula corresponding to chemotherapy-related granularities [3].
With an approach similar to that we proposed in this paper, in [2] efficient conversion methods are defined from an algebraic granularity representation to the equivalent lowlevel representation based on periodical sets, where many reasoning tools are available.
Even though the final goal of this proposal is similar to that of our proposal, it is worth noting that we do not provide conversion methods to a lower level representation: indeed, we provide within the logical framework based on PPLTL some "formula constructors" which allow one to specify in an algebraiclike concise fashion possibly long and not easily readable logical formulae defining temporal granularities.
ChlVPP  P  P  Chl  Chl  P  P  P  .......  P  P  P  ....... P  P  P  P  .......  P  P  P  ....... P  P  P  P  .......  P  P  P  ....... P  Q  ....... Q C h l Q C h l Q C h l ....... ....... Q P c Q P c Q P c .......  Pc Pd  V  Chl Pc Pd  Chl Pc Pd  Pc Pd  V  Chl Pc Pd  Chl Pc Pd  Chl Pc Pd  Time  Q Q Q Q  Chl Pc Pd V  Q Q Q  Chl Pc Pd  Q Q  Chl Pc Pd  ....... Q Q  Figure 7.
Pd  Q  Pd  Q  Pd  .......  Q Q Q  Q Chl  ChlVPP  Pc Pd  V  ChlVPP chemotherapy regimen  larities may be expressed as: Count(PChDay , 168) [?]
Subgranularity(Chday, day)[?]
Group(ChDay, ChlVPP, 28) [?]
SelDown14 1 (day, ChlVPP, Chl)[?]
SelDown11 (day, ChlVPP, V) [?]
SelDown18 (day, ChlVPP, V)[?]
14 SelDown14 1 (day, ChlVPP, Pc) [?]
SelDown1 (day, ChlVPP, Pd)  The above formula defines granularities Chl (chlorambucil), V (vinblastine), Pc (procarbazine), Pd (prednisone) and ChlVPP according to the recommendation; the formula is defined on the time domain N of days (we assume that the granularity day is composed of a granule for each point in the considered time domain).
The first two conjuncts state that the granularity ChDay consists of 128 days, which is the overall duration of 6 cycles: Count(a, n) stands for "a holds exactly n times in the future" and is defined as follows [3]: Count(a, 0) = G!a and Count(a, n) = !aU(a & XCount(a, k - 1)).
The next conjunct states that each granule of ChlVPP is of 28 elements (days of chemotherapy).
The next conjunct states that the granularity Chl covers the first 14 days of each cycle; the next two conjuncts state that the granularity V covers the first and the eighth days of each cycle; finally, the last two conjuncts state that the granularities Pc and Pd cover the first 14 days of each cycle.
The adoption of the logical specification of the grouping and select-down operations makes the overall formula more natural and intuitive than the corresponding one specified only through the PPLTL syntax.
R EFERENCES [1] C. Bettini, S. Jajodia, and X. Wang.
Time granularities in Databases, Data Mining, and Temporal Reasoning.
Springer, 2000.
[2] C. Bettini, S. Mascetti, and X. S. Wang.
Supporting temporal reasoning by mapping calendar expressions to minimal periodic sets.
Journal of Artificial Intellicence Research, 28:299- 348, 2007.
[3] C. Combi, M. Franceschet, and A. Peron.
Representing and reasoning about temporal granularities.
Journal of Logic and Computation, 14(1):51-77, 2004.
[4] S. Demri.
LTL over integer periodicity constraints.
Theoretical Compututer Science, 360(1-3):96-123, 2006.
IV.
D ISCUSSION AND C ONCLUSIONS  [5] D. Foster, B. Leban, and D. McDonald.
A representation for collections of temporal intervals.
In Proceedings of the American National Conference on Artificial Intelligence, pages 367-371, 1986.
In this paper we considered the logical framework of Combi et al.
[3] for representing and reasoning about temporal granularities, and the algebraic framework of Ning et al.
[7].
Among the approaches for symbolic specification of granularities, such as those proposed by Foster, Leban, and McDonald [5] and by Niezette and Stevenne [6], we focused on the calendar algebra, which is, differently from the other proposals, based on the same general framework for time granularities [1] adopted by the considered logical  [6] M. Niezette and J. Stevenne.
An efficient symbolic representation of periodic time.
In Implementing Temporal Reasoning: workshop notes - AAAI'92, pages 130-140, 1992.
[7] P. Ning, S. Jajodia, and X. S. Wang.
An algebraic representation of calendars.
Annals of Mathematics and Artificial Intelligence, 36:5-38, 2002.
114
Temporal Knowledge Representation and Organization for Case-Based Reasoning.
I. Bichindaritz  E. Conlon  LIAP-5, UFR de Math-Informatique Universiti: Rent5 Descartes - Paris 5 45 rue des Saints-Pkres 75006 Paris, France  LIAP-5, UFR de Math-Informatique Universiti!
Renk Descartes - Paris 5 45 rue des Saints-Pkres 75006 Paris, France  Abstract  follow-up and clinical research [3].
A first version of the system, which did not deal with temporal data, was able to provide assistance to the work of the clinical team.
The second version of the system, the memory of which is presented here, has been enriched with the processing of temporal data.
The second section presents the temporal knowledge representation of the cases and the trends.
The third section deals with the composition of the case memory and with its organization around trends, and the fourth section explains how trends are learnt.
The case-based reasoning process of the system is then briefly presented in the fifth section.
It is followed by a comparison with related work, then by the conclusion.
Thas artacle presents the temporal knowledge representataon and ats organzzataon an a case-based reasonang system called MNAOMIA.
The general casebased reasonzng methodology as grounded on a model of reasonang such that memory, learnzng and reasonang are anseparable.
Thas partacular focus forces pertanent knowledge representataon and organazataon an the system memory The maan aspects of the temporal damensaon of knowledge representataon and organzzataon of MNAOMIA are detaaled an thas paper, such as' the temporal representataon language for the cases, the automatac learnzng oftrends from the cases durang the reasonang process, and the organazatzon of the memory an generalazataon-based haerarchaes of trends under whach the cases are andexed  1  2 Knowledge Representation 2.1 Case Representation  Introduction  Initial Case Representation.
A case represents a patient evolution over time.
It is a conjunction of representation elements, each one associated to a definite date.
Some of these elements may be fixed in time, such as the surname, others can be recorded periodically, such as the weight for every day, else others can be recorded at different dates, such as the result of a diagnosis from an evaluation.
As stated before, an evolution is made up of the conjunction of all the above elements, also called events (see Figure 1).
An event is any fact about the patient recorded at a given time.
Case-based reasoning [lo] is an artificial intelligence methodology for the processing of empirical knowledge.
It reasons from cases, which are sets of empirical data, such as patients cases in a medical domain.
Previously processed cases are stored in a case-base, or memory, and used by such a system to process new cases.
The processing of a new case uses one or several memorized cases similar to the new case.
The specificity of the case-based approach of reasoning lies in its focus on the inseparability of reasoning from memory and from learning.
In particular, the organization and the composition of the memory, giving it the ability to be accessed and updated constantly during the reasoning process, are a key issue for a case-based reasoning system, even when the temporal dimension is not addressed.
The description of the knowledge representation language and of its organization permits to describe the memory adequately, and is thus the main subject of this paper, in relation with the temporal dimension.
The case-based reasoning (CBR) system presented here, called MNAOMIA, is a complete case-based reasoning system applied to the domain of eating disorders in psychiatry.
Its aim is to provide assistance to experts in the different cognitive tasks they perform, namely diagnosis, treatment planning, patients  Event  For example : Surname Event = <15/06/95, Surname, Catherine> Weight Event = <21/06/95, Weight, 38.9kg> Diagnosis Event = <17/06/95, Diagnosis, Anorexia> Abstract Case Representation.
An abstract case represents a patient evolution over time, such as given in the initial case representation, but in a summarized way.
A sequence of similar events can be summarized as an interval where the value of the attribute is rising or falling or remaining constant.
An interval is described thus :  152 0-8186-7528/96 $5.00 0 1996 IEEE  =< Time,Attribute, V a l u e >  where ti,s is the start and t;,f is the end of the interval.
The attribute value Fi(ti,,,t , , f )is produced by a function on the set of parameter values covered by the time interval.
For a discrete quantitative value, if the value chosen is the velocity of the parameter over the interval, the function to obtain the velocity is : thf  Vel(t,,s,& , f ) =  E sva~,/(t,,f tz,s) -  3=t,,s  Figure 1: Extract from a patient's evolution.
If the value is a continuous quantitative value, the function can be replaced by an integral.
If the value is a qualitative value obtained from the parameter values over the interval, the function produces a value either from predefined categorizations by experts (e.g.
High blood pressure is defined as 80Pa), or by a function which calculates such categorizations automatically.
From Time, to Time the attribute Attribute has overadvalue Vaiue.
This is represented by :  AbstractEvent  =< Time,, T i m e j , Attribute, Value > 2.2  So abstract case representations are conjunctions of such summarized events, also called abstract events.
Case Representation Language.
So a case Case is a conjunction of representation elements El, :  Case =  Trend Representation  Trend Definition.
A trend is a typical evolution [8,9], one which a significant number of patients are observed as having.
It is a generalization of summarized events, and thus from intervals and the values attached to these.
Trend Representation Language.
A Trend is a con.junction of description elements Eli.
A E/, i  Each element of representation Eli contains a precise attribute, Att, i.e.
a parameter type.
Associated with each Att is a function which returns the range of this parameter type, range(Att) (for example range(Ca1ories) = 100kcal).
Three types of element of representation are defined:  Trend =  i  'The types of description elements in a trend are dernved from those in a case, since they are generalized from them.
They are :  1.
A time-independent attribute-value pair (such as the patient name) :  1.
A time-independent attribute-value pair :  El, =< Att,, Vali > .
El, =< Att,, Val, >  2.
A time-point attached to an attribute-value pair can be generalized by itself if this attribute is not repeated in time :  A time can be associated to it, for example the date when this data element has been recorded, but it is not necessary.
2.
A time-point attached to an attribute-value pair corresponds to a qualitative value (such as high blood-pressure at arrival time) or to a discrete quantitative value (such as weight equals 37.8 kg at arrival date) : Eli =< t,, Atti, Vali  Eli  =< t,, Atti, Vali > .
3.
A time-point attached to an attribute-value pair is generalized if this attribute is repeated j , times :  >  El;  3.
A time-interval attached to an attribute-value pair corresponds to a qualitative value or a discrete quantitative value constant over a time interval (such as high blood-pressure from week 2 to week 3, result of a test 125 from month 1 to month 2), or to the evolution of a continuous qualitative value over a time interval (such as weight 39kg from day 2 to day 4) : Eli  A El,  =< t i , Atti,  c,'2,,=1 Valj,/n> .
4.
A time-interval attached to an attribute-value pair can be generalized by itself if this attribute is not repeated in time :  Eli =< ti,s,ti,f,Atti,Gi(ti,s,ti,f) >  5.
A time-interval attached to an attribute-value  pair can be generalized if this attribute is repeated in time :  =< ti,s,ti,f,Att,,Fi(t,,s,t,,f) >  153  2.
A EL =< ta,s,t,,f,lb,,ubz,Att,,F,(tz,,,t,,f) >.
value attached to a time-interval has two components to be generalized, the interval and the value of the function on the attribute.
where t,,s is the biggest of the starting times, t,,j is the smallest of the final times, lb, is the lower bound, i.e.
the smallest of the starting times, ub, is the upper bound, i.e.
the biggest of the final times, calculated from the set of all the intervals generalized for the same attribute Att,.
3  Generalization of time-intervals.
Let inti and intJ be two intervals :  inti =< t , + , t , >>,int, f =< t j , s , t j , f> .
The intersection between two intervals, inti n int, , does not capture the overall generalization of the previous intervals.
A reference to previous intervals is needed, i.e.
the knowledge that int, and intj produced the current generalized interval.
An effective means of keeping track of the intervals that make up a generalization is to store the highest and lowest time-points previously met, called the lower bound and the higher bound.
So the generalization of the intervals int, and int, is the interval intk (see Figure 2 ) given by :  Memory Representation  3.1 Memory Composition The memory M is a network of cases c; and trends T j linked by relationships Rk, i.e.
more-speczfic-than between trends, and anstance-of between trends and cases.
Cases and trends are nodes in this network.
M = (T,I<, R) where T = {..TI...}, K = {..c,...},  R = {..Rk(nl,n2)...}, with  121,122  TrendInterval = intk =< tk,s,tk,f,lbk,Ubk>  E ( T U K).
A new patient case presented to the system must be matched to memory and inserted in memory where there is a correspondence between one of memory's trends or cases and the new case.
So a matching predicate must be defined between a case and a trend.
Close versions of this predicate will also be used to match cases between themselves, as well as to match trends between themselves.
3.2 Trend Formation A trend T is a conjunction of representation elements El,.
where t k , s = sup(t,,s,tj,s),tk,j = i n f ( t t , f , t j , j ) 161, , = i n f ( t ; , , tjLs) and ubk = sup(t,,j,t,,f).
This gives the interval a range within which it can  in11  T=/\EE,  ub  2  Each element is the generalization of one of the element types of a case representation.
When these element types are not repeated thru time, they are generalized by themselves.
So a trend representation has the three basic types of representation elements as an abstract case representation : timeindependent attribute-value pairs, time-points attached t o attribute-value pairs, and time-intervals attached t o attribute-value pairs.
But a trend generally has two other types of elements, generalized from the previous ones, for a given attribute Att, : 1.
A value attached to a time-point is generalized as the mean of all the attribute Att, values V a l J , which fall within the original time-point ti range, called timerange(Att,) :  Figure 2: Generalization of two intervals.
expand.
If the new patient interval's start point is within the lowerbound, the intersection is extended to this startpoint, similarly with the upperbound.
These bounds allow the system to take previous instances into account when generalizing.
What is required for a match of two intervals is given by the operator during [l] :  inti during intj  ((ti,$2 tj,.)
A ( t j , f 2 t ; , f ) ) .
Upperbound and lowerbound are not used during the actual match.
They are used when generalizing a trend interval that matches.
n  V a l J , / n> .
El, =< t , , Att,, .
?>=I  Generalization of the value attached to a timeinterval.
The generalization is the mean value of all velocities matched against this time-interval.
To generate this it was necessary to store the sum of all velocities and the count of values.
So :  More precisely, each time-point t has a (min,max) range for matching.
This depends on the time type Typ (such as Y e a r , Month, Week, D a y , etc.)
and on the attribute, and is returned by the function timerange(Att,T y p ) .
Then the timepoint range IS (min=t - tzmerange(dtt,Typ), max=t timerange(Att,T y p ) ) .
By simplification, this function is also noted timerange(Att).
n  +  Val,, n >  Trendvalue =< z t l  154  So the overall generalization for an interval bound to a numeric value is :  where C is an instance of T (linked to it in memory by an instance - of relation).
Depending of the type of the representation element, the match predicate takes the following forms :  match - indep(Eli, , Eli,) independent elements :  matches two time-  The overall generalization for a qualitative value attached to a time-interval is :  Gi(ti s, t i , f )in this case is a qualitative value which cannot be generalized any further, We can assimilate the two representations if we define (this simplification is only for the clarity of this text, it is not used in the actual system where keeping n explicitely is required) :  match-point( Eli, , Eli,) matches two time-point elements :  The Matching Predicate to Relate Cases to Trends  match - point(El,, , EliT) I Vi, , 3,,[(tjc 2t,, - timerange(Att,,)) A(t,, f ti, timerange(Att,,))] A(Atti, = Att,,) A (Vali, E [(Val,, - range(Att,,), (Val,, range(Att,,)].
nomatch - point(El,, , El,,) %, 3tc[(tic 2 t,, - timerange(Att,,)) A(&, 5 t,, timerange(Att,,))] A(Att,, = Att,,) A (Valzc$2 [(Vali, - range(Att,,), (Val,, range(Att,,)].
+  3.3  +  In the paradigm of reasoning by analogy, which is used in MNAOMIA [4],an analogy is a relation of resemblance between two entities D and DI, one from domain D , the other from domain DI.
The resemblance between D and DI supposes the existence of an element of description in common d .
Most of the time this resemblance is not found in the initial descriptions of D and D I , but can be obtained by the transformation of their descriptions, i.e.
abstraction.
Thus the common property, d , can be decomposed into two initial properties d ( D ) and d(D!).
Such is the situation in this problem domain, we are matching between the description of a patient history and the trends so the paradigm applies here.
We need a matching predicate between the description of a case C and the description of a trend T .
Let C = Eli,  +  +  In addition to the timerange function previously defined, each attribute Att has a (min,max) range for matching.
This depends on the attribute, and is returned by the function range(Att).
Then the attribute ran e is (min=val - range(Att), max=val -t- rongefAtt)).
match - interval(El,, , E l z T )matches two timeinterval elements :  A iC  and  T=  A El,, iT  be the descriptions of the trend T and the case C. An element in the trend description Eli., and an element in the case description Eli, match if all their components match.
Three main types of element (and their generalizations) have been defined for a case, and so a matching predicate must be defined for each of these :  match(C,T ) W  Vi,  , 3ic [match(Eli,, Eli,)]  3.4  Also, a non-matching predicate has to be defined for each of these :  nomatch(C,T )  Hierarchical Organisation of Trends  Tirends are linked to other trends by the relations more - specific - than (and its reverse relation more - general - than).
This relation permits the building of a hierarchy  3,,Vic [nomatch(Eli,, Eli,)]  155  VmG > 3,s  [ ( t m G , s ,t m G , f ) d u r i n g ( t m S , S  1  tmS,.f)l  A(AttmG= Att,,) A G m s ( t m s , s !
t m s , j )E [(GmG @ m G , S l t m G , f ) - r a n g e ( A t t m G ) l (GmG(tmG,S> ?
m G , f )  +  More precisely, the memory is organized around points of view.
Each point of view is specific of a precise cognitive task performed (i.e.
diagnosis, treatment planning, etc.).
These points of view are situated in a different part of the memory than the entities (trends, cases) previously defined : the theoretical memory (see Figure 4), but a different hierarchy of trends is associated to each of them.
4  Trends Learning  MNAOMIA performs an incremental concept learning during its case-based reasoning [ 5 ] .
The search thru the memory, from a new case, starts from the root of the hierarchy, which is the most general trend of the memory.
Most of the time, this root is empty, and the set of the most general trends in the memory are searched.
For each of these, the match between the new case, and the trend, is evaluated] so for each description element of the trend.
A successful match is returned if at least one interval of the case matches the concept description.
The new case will be stored at any node it matches.
Therefore the match predicate generalizes any trend elements that match at the time of the match.
The pruning of trends and attributes will eventually remove any redundant concepts.
The pruning is done by Pos and Neg attributes which are associated with each description element in a trend.
If a description element's negative score falls below a certain level it will be removed.
If a trend's number of description elements falls below a certain level it is removed These attributes are updated while the match is evaluated.
When a match is successful for the whole trend, the algorithm passes the list of description elements of the case not contained in the trend to the sub-trends of this concept.
The sub-trends are the trends linked to the current trend by as-more-specafic-than relations.
Figure 3 : The hierarchical organization of the memory around several points of view.
Search(Trend, Case) result := {3 for a l l representation elements in Case if match(Trend.Element, Case-Element) then POS := POS + I else NEG := NEG + I if NEG - POS < Threshold then remove the element from the Trend description if Trend is empty then suppress the Trend from the Memory link its sub-trends to its super-trends in the hierarchy (fusion) if there is at least one contradictory description element between Trend and Cas e then return Nil  156  important features from a patient case, selecting hypotheses from this abstraction, deriving expectations from these hypotheses, comparing the expected consequences of the first ranked hypothesis to the patient's actual condition, and confirming in memory the right hypotheses.
So this case-based reasoning general framework is well adapted to medical reasoning.
else for all Trend sub-trends newresult := Search(sub-trend, unmatched elements in Case) if newresult <> Nil then result := result + newresult if result <> 4 3 then return result else return (Trend)  5.1  Abstract  Tlhe initial patient case is a conjunction of attrilbute-value pairs attached to time.
This representation is abstracted into an abstract case representation, in which the relevant features are calculated, in this ]problem time intervals bound to overall attribute values.
A new case has an initial representation such as :  This algorithm is a modified version of the GBM algorithm [14].
The main difference is in the match function.
Matching a Case against a Trend is not only comparing values by an equality function, but a more complex matching process as the one defined previously.
As a matter of fact, the Search algorithm could be adapted to other incremental concept learning algorithms than GBM, such as COBWEB [6,7]for example.
Initialcase =  A < T i m e i ,Atti, Vali > i  For example :  New trends are learnt when a new, revised case is added to the memory, at a given T r e n d .
The algorithm is as follows :  , S u r n a m e ,Catherine> <21/06/95,Diagnosis, Anorexia > <21/06/95, W e i g h t , 38.9kg > <22/06/95, W e i g h t , 38.5kg > (Case1 = <24/06/95, W e i g h t , 39.0kg > <  Insert(Trend, Case) UnMatched := Elements of the Case not explained by the Trend's Elements for all cases Ct indexed under the Trend nb := number of successful matches of match(Unmatched, Ct.Element) if nb > threshold then create a new sub-trend Tn of Trend with these successful matches index Case under Tn index Ct under Tn remove Ct from the cases indexed under Trend if Case has not yet been indexed under a new sub-trend of Trend then index Case under Trend  <  ...  ,  ...  ,  ...  <21/06/95, Calories <28/06/95, Calories  <  ...  ,  ...  >  , 1532kcal > , 1621kcal > , ... >  T i m e i can take several forms, and the user of the svstem is able to chose the granularity of the description, such as week, month: or like here day.
From  Here again, this algorithm is that of GBM [14],but the match function, and the generalization of a Trend, are much more complex than in GBM, due to the temporal elements in the cases descriptions.
5  Overview of the Reasoning Process  This system is a case-based reasoning system.
Case-based reasoning is both an artificial intelligence methodology, and a cognitive model of reasoning such that reasoning is inseparable from memory and from learning.
This cognitive model is a very high aim, which current case-based reasoning systems are just beginning to pursue.
The tight interaction between reasoning, memory and learning is one characteristic of the general description of the system, as presented on figure 4 , which has been inspired by Aamodt [2] and Bichindaritz [4].
The main steps in the reasoning process are to abstract, to retrieve, to reuse, to revise and to retain.
They also correspond to the STMODEL [18]of medical reasoning.
According to this model medical reasoning consists of abstracting the  ~  Figure 4: Architecture of the system.
these data, an abstract case representation is calculated.
The advantage of an abstract representation is that it summarizes the main facts in the patient evolution, thus getting away too specific dates and values.
The abstraction process calculates intervals, and attaches values to these intervals.
The user also has the  157  option of defining the intervals in terms of days or in terms of weeks.
If the evolution is to be defined in terms of weeks, the weekly velocity of the parameter, for example weight gain, is calculated and entered as the parameter value for that week.
Search(Trend, Case) algorithm described in the previous section.
The result returned by this algorithm is a set of Trends.
The algorithm FindIntervals abstracts intervals from this representation, for all attributes having an evolution.
Under these Trends, cases are indexed by instance - of relations.
These cases are ranked by a similarity measure, such as sim(Case1,Casea) =  Abstractedcase =  Result = { T r e n d k } .
A < Timei,,,T i m e i , f ,Atti, Vali >  E:-,  -  17Xs'n(E't,k,E'l,k)tCkn_l  " k , , p r e d Xnt,pert Xsrm(Elt,k,Ell,k)  a x n t ~ ~ =" kl, , p r e d  2  X",,pert  where CY is a weight associated to the representation elements which are not important for the current process, and where n,,pertand nk,,predare some weighting variables learnt by the system during previous cases processing.
These variables are associated to the representation elements of cases and trends.
In the Retain part of the reasoning cycle, the learnt case will be stored at every trend that it matched, here the Trendk.
This is why a pruning is performed here, with the POS and NEG variables, associated to each trend representation element.
Some parts of trends are removed, others are generalized, and finally, new trends are added, while previous trends can be deleted.
So the matching, generalizing and pruning are performed simultaneously.
For example :  < , , Surname ,Catherine> < 8 , ,Diagnosis, Anorexia > < 8 , 9 , Weight , -0.411-g > AbstractedCasel = < 9,11, Weight , s0.511-g > < ...,..., ... , ... > < 8,15, Calories , +89kcal > < .
.
., .
.
.
, .
.
.
, ... > The FindIntervals algorithm calculates the start Tz,sand the finish T,,f of intervals, for each attribute Att, given an evolution of this attribute ranked in increasing time order :  The Result set is the set of all most specific matching trends in the memory, for the new abstracted case.
The set of all the cases indexed under these trends is the set Cand of all the cases similar to this new case.
It is ranked by decreasing order of similarity by the sim function.
i := I T i , s = t h e f i r s t time p o i n t i n t h e evolution for each t i m e - i n t e r v a l s t a r t e d t a k e t h e c u r r e n t time-point i n t h e evolution c a l c u l a t e t h e v e l o c i t y between t h e t i m e - p o i n t and t h e n e x t one c a l c u l a t e t h e v e l o c i t y with t h e next t i m e - p o i n t added i f t h e extended v e l o c i t y i s s i g n i f i c a n t l y d i f f e r e n t from t h e i n i t i a l v e l o c i t y t h e n end t h e c u r r e n t t i m e i n t e r v a l w i t h T i , f = t h e c u r r e n t time-point Vi = i n i t i a l velocity  Cand = { C a s e k } .
5.3  Reuse  This step is very dependent on the task performed.
In problem-solving, it is an adaptation of the most similar case solution to fit the new case.
In other tasks, such as interpretation, it can be the construction of an argumentation linking the new case with the retrieved cases.
5.4 Revise Depending on the success or the failure of the new case processing, the reused case is revised and transformed into an updated case, now ready to be stored in the memory.
i : = i + l s t a r t a new t i m e - i n t e r v a l w i t h T i , s = t h e n e x t time-point  t a k e t h e next time-point i n t h e evolution  5.2 Retrieve The abstracted case is then matched with the trends in memory, in order to retrieve the most similar memorized cases.
First of all, this matching process is directed by the task to perform.
This comes from the organization of the memory around points of view from the theoretical memory Each task performed is associated to a unique point of view.
So the retrieval step must be performed for a specific cognitive task, such as diagnosis or treatment.
The trends are organized in hierarchies dependent upon the points of view in memory, as is represented in Figure 3.
The case is first matched with the most general trends, and following with more and more specific trends.
The Search thru the memory is performed by the  5.5  Retain  The algorithm performing the enrichment of the memory with the new revised case has been given in the previous section: Insert(Trend, C a s e ) .
6  Related Work  This work is related both to temporal reasoning in general, and to time processing in case-based reasoning.
In case-based reasoning, very few systems have tackled time processing.
This is due to the novelty of the domain: more crucial problems had to be solved first.
For instance, MNAOMIA has addressed  158  In : Proceedangs of the 1st UIi' Workshop on Case-Based Reasonang, I. Watson and M. Fahrir (Edts.
), Springer-Verlag LNCS/LNAI, 1995, (in press).
many important problems before dealing with time, although it was obvious from the start that time processing could not be avoided.
The other case-based reasoning systems that handle time use a continuous representation of their cases [15, 161.
They do not learn trends from these temporal data, and focus on matching between cases.
The weakness of this approach is first of all its limitation to small amounts of cases and data, and the lack of all the advantages of qualitative reasoning, such as explainability, understandability and knowledge discovery.
On the other hand, thorough work has been performed in temporal reasoning.
Since time processing has been driven by the application domain, only subsets of this work has been used in this system, such as the during operator [1,11],or the matching of known trends with new evolutions [8,9,17.
More theoretical work, such as the event calculus 12,131 would have been suited for the domain, but it exceeded the requirements of the application at its current state.
[6] Fisher D.H., "Knowledge acquisition via incremental conceptual clustering" In : Machane Learning 2, 1987, 138-172.
[7] Gennari J.H., Langley P. and Fisher D. "Models of Incremental Concept Formation", Artzficzal Intelligence, 40, 1989, 11-61.
[8] Haimowitz I.J.
and Kohane IS., "An epistomology for clinically significant trends", In :Proceedangs of the Tenth Nataonal Conference on Artzjicaul Intellagence, 1994, 178-181.  i  7  [9] Haimowitz I.J.
and Kohane I.S., "Automated trend detection with alternate temporal hypotheses", In2: Internataonal Jotnt Conference on Artificaal Intellzgence, 1993, 146-151.
Conclusion  [lo]Kolodner Janet L., Case-Based Reasonang.
Morgan Kaufmann Publishers, San Mateo, California, 1993.
The MNAOMIA case-based reasoning system reasons from the temporal data in the patients cases in close interaction with its memory.
MNAOMIA organizes its memory around hierarchies of trends learnt by incremental concept learning from the cases.
This approach is more related to the temporal reasoning strategy of Allen [l than to the event calculus of Kowalski and Sergot [13 because it updates its memory at input data processing time with the incremental concept learning previously presented.
So interesting work can be accomplished by integrating more of Allen's formalisms in MNAOMIA.
Moreover, the memory organization supports the subsequent reasoning of the system.
This reasoning, and the complex problems related to the resuse step and to the revise step, are our current research topic.
For this work, common formalisms with the event calculus [13] and those of temporal deductive databases seem promising.
[ll]Kouramajian V., Fowler J., "Modelling past Current and Future Time in medical Databases", In .
Proceedangs of the 18th Symposaum on Computer Applzcations an Medzcal Care.
1994, 315-319.
\  [12] Kowalski R.A., "Database updates in the event calculus", Journal of Logzc Programmang, 12, 1992, 121-146.
[13] Kowalski R.A., Sergot M.J., "A logic-based calculus of events", New Generataon Computang, 4(1), 1986, 67-95.
[14] Lebowitz M., "Concept learning in a rich input domain" ,In : R. S. Michalslti, J. G. Carbonell and T .
M. Mitchell (Eds.
), Machzne Learning: an A.I.
approach, 2, Morgan Kaufmann, Los Altos, Ca., 1986.
References [l]Allen J.F., "Maintaining Knowledge about temporal intervals", Communacataons of the ACM, 26(11), 1983, 832-843.
[15] Ram A. and Santamaria J.C., "Continous CaseBased Reasoning", In : Proceedangs of the AAAI93 Workshop on Case-Based Reasoning, 1993, 86-93.
[2] Aamodt A., "Towards Expert Systems that Learn from Experience", In : Proceedings of a Workshop on case-based reasonang (DARPA), Pensacola Beach, Florzda, K.J.
Hammond (Edt.
), Morgan Kaufmann, San Mateo, CA, 1989, 181187.
[l6] Rougegrez-Loriette S., "Prddiction de processus 2, partir de comportements observds : le systbme REBECAS", LAFORIA Doctoral Report (unpublished), 1994.
[l'] Shahar Y., Das A.K., Musen M.A., Kraemar F.B., "Knowledge-Based Temporal Abstraction for Diabetic Monitoring", In :Proceedangs of the 18th Symposaum on Computer Applacatzons an Medical Care, 1994, 697-701.
[3] Bichindaritz I., "A case-based assistant for clinical psychiatry expertise", In : Proceedzngs 18th Symposaum on Computer Applacataons an Medacal Care, AMIA, Washington DC, 1994, 673-677.
141 Bichindaritz I., Apprentzssage d e concepts duns une mkmoire dynamique : raisonnement Ci partir de cas adaptable Ci la tciche cognitive, Thesis of University Rend Descartes, Paris, 1994.
[18] Stefanelli M., Ramoni M. "Epistomological Constraints on Medical Knowledge-Based Systems", In : D.A.
Evans and V.L.
Pate1 (Eds.
), Advanced Models of Cognataon f o r Medacal Traanzng and Practice, Springer-Verlag, 1991, 3-20.
[5] Bichindaritz I., "A case-based reasoning and conceptual clustering : for a cooperative approach",  159
Applying Local Search to Temporal Reasoning J. Thornton, M. Beaumont and A. Sattar School of Information Technology, Griffith University Gold Coast, Southport, Qld, Australia 4215 {j.thornton, m.beaumont, a.sattar}@mailbox.gu.edu.au  Abstract Local search techniques have attracted considerable interest in the Artificial Intelligence (AI) community since the development of GSAT [9] and the min-conflicts heuristic [5] for solving large propositional satisfiability (SAT) problems and binary Constraint Satisfaction Problems (CSPs) respectively.
Newer SAT techniques, such as the Discrete Langrangian Method (DLM) [10], have significantly improved on GSAT and can also be applied to general constraint satisfaction and optimisation.
However, local search has yet to be successfully employed in solving Temporal Constraint Satisfaction Problems (TCSPs).
In this paper we argue that current formalisms for representing TCSPs are inappropriate for a local search approach, and we propose an alternative CSP-based endpoint ordering model for temporal reasoning.
In particular we look at modelling and solving problems formulated using Allen's interval algebra (IA) [1] and propose a new constraint weighting algorithm derived from DLM.
Using a set of randomly generated IA problems, we show that our local search outperforms Nebel's backtracking algorithm [6] on larger and more difficult consistent problems.
1.
Introduction Representing and reasoning with temporal information is a basic requirement for many AI applications, such as scheduling, planning and natural language processing [6].
In these domains temporal information can be qualitative as well as quantitative.
For instance, an event may need to be before or during another event, but we may not be concerned with actual durations, start times or end times.
Such information is not handled well using a simple linear timestamping model, and requires more expressive constructs to capture the notion of events and the constraints between them.
To answer this need, various approaches have been developed in the constraint satisfaction community under  Michael Maher Dept.
of Math.
and CS, Loyola University, Chicago, IL 60626, USA mjm@cs.luc.edu  the heading of Temporal Constraint Satisfaction.
A Temporal Constraint Satisfaction Problem (TCSP) shares the basic features of a standard CSP, i.e.
variables with domains and constraints that define the possible domain values that can be assigned to each variable [4].
However, in a TCSP constraints are modelled as intensional disjunctions of temporal relations [8] rather than as extensions of allowable domain value combinations.
Finding a consistent scenario is then a matter of searching for a consistent set of temporal relations for each constraint.
For harder problems this usually means using a combination of backtracking and a constraint propagation technique such as path-consistency [8].
In this paper we look at applying local search to solving TCSPs.
Local search techniques such as GSAT [9] and the min-conflicts heuristic [5] have already proved effective both for propositional satisfiability (SAT) and in the general CSP domain, particularly on problems beyond the reach of standard constructive search methods.
However, when applied to a TCSP, a local search is unable to exploit the constraint-propagation approach used with backtracking, as it is an incomplete method that necessarily moves through inconsistent scenarios.
Further, local search requires exact cost feedback when deciding between candidate moves.
In a binary CSP, a move changes a variable instantiation and cost feedback is obtained from a simple count of violated constraints.
However, in a TCSP, a move consists of instantiating a set of temporal relations for a particular constraint.
As no variables are actually instantiated, finding an exact move cost becomes a significant search problem in its own right [2].
Given these difficulties, our approach has been to reformulate temporal reasoning as a more standard CSP, i.e.
searching by instantiating variables with domain values rather than instantiating constraints with temporal relations.
Once in this form, a local search can be applied in a straightforward manner.
The main task has been to develop a representation that does not cause an excessive increase in problem size.
Our work has resulted in the end-point ordering model  for temporal reasoning, described in Section 3.3.
To evaluate the model we have used Allen's Interval Algebra (IA) [1] and have developed an efficient temporal tree constraint representation to capture the full set of IA relations.
Additionally, we propose a new constraint weighting local search algorithm for temporal reasoning, derived from a state-ofthe-art SAT technique (the Discrete Lagrangian Method or DLM [10]).
In Section 4.3 we give an empirical comparison of this approach with Nebel's backtracking algorithm and finally discuss the future direction of our work.
2.
Interval Algebra Allen's Interval Algebra (IA) provides a rich formalism for expressing quantitative and qualitative relations between interval events [1].
Additionally, reasoning with the full set of IA relations is known to be NP-complete [12].
For both these reasons (expressivity and difficulty) IA appeared ideal for the implementation of our local search approach.
In IA, a time interval X is an ordered pair of real-valued time points or end-points (X - , X + ) such that X - < X + .
Hence we can map actual time values to (X - , X + ) that satisfy X - < X + .
Such mappings are called interval- or Iinterpretations [6].
Allen further defined a set B of 13 basic interval relations such that for any pair of time intervals the combined I-interpretation can be described by exactly one basic relation.
These relations capture the qualitative aspect of event pairs being before, meeting, overlapping, starting, during, equal or finishing each other.
As shown in Table 1, each relation can be defined in terms of constraints on the end-points of the constituent time intervals X and Y .
Hence we can say that an I-interpretation satisfies a relation iff it satisfies these corresponding end-point constraints.
Indefinite information is expressed in IA as a disjunction of basic relations, known as an interval formula: X{B1 ..Bn }Y where Bi [?]
B.
For example, the interval formula X{m, o}Y represents the disjunction (X meets Y) or (X overlaps Y).
Using this notation, an IA problem can be simply represented as a finite set of interval formulas Th.
Further, we can say that Th is I-satisfiable iff there exists an I-interpretation such that at least one basic relation in each interval formula is satisfied.
ISAT is the problem of deciding whether Th is satisfiable and is one of the basic tasks of temporal reasoning [6].
3.
Representing ISAT for Local Search 3.1.
Current TCSP Approaches to ISAT Current techniques for solving the ISAT problem follow the general TCSP approach outlined in the introduction  Basic Relation  Explanation  b :X before Y bi :Y after X   3/4   m :X meets Y mi :Y met by X   3/4   o :X overlaps Y   3/4   -  3/4   X  X  - 3/4    3/4   - (X - < Y - ) [?]
(X - < Y + )[?]
X   3/4   -   3/4   X  -  -  Y   3/4   3/4   X  -  -  Y   3/4    3/4   X   3/4   3/4   X  (X - < Y - ) [?]
(X - < Y + )[?]
(X + > Y - ) [?]
(X + < Y + ) (X - > Y - ) [?]
(X - < Y + )[?]
(X + > Y - ) [?]
(X + < Y + ) (X - = Y - ) [?]
(X - < Y + )[?]
(X + > Y - ) [?]
(X + < Y + )  - (X - > Y - ) [?]
(X - < Y + )[?]
(X + > Y - ) [?]
(X + = Y + )  Y  fi :Y finished by X eq :X equals Y  Y  Y  si :Y started by X f :X finishes Y  -  (X + = Y - ) [?]
(X + < Y + )  di :Y includes X s :X starts Y  End-point Relations (X - < Y - ) [?]
(X - < Y + )[?]
(X + < Y - ) [?]
(X + < Y + )  oi :Y olapped by X d :X during Y  Y  - (X - = Y - ) [?]
(X - < Y + )[?]
-  Y  (X + > Y - ) [?]
(X + = Y + )  Table 1.
The 13 basic interval relations (note: the relations (X - < X + ) [?]
(Y - < Y + ) are implicitly assumed in each end-point relation)  [6, 11], using a combination of specialised path-consistency and backtracking algorithms.
These techniques search for a consistent solution by eliminating basic relations from each disjunctive constraint (or interval formula).
A significant group of tractable sub-classes of IA have been identified for which finding a path-consistent scenario is sufficient to guarantee full consistency [7].
These sub-classes are subsets of the 213 possible interval formulas allowed in the full IA.
IA algorithms exploit this information by searching for path-consistent scenarios that only contain formulas from a given tractable subset.
This is more efficient than searching for a single basic relation from each formula.
In addition, specialised ordering heuristics have been developed that further improve the performance of backtracking on full IA [11].
3.2.
Local Search and TCSPs Unfortunately, little of this work is of direct relevance in applying local search to IA.
The basic principle behind a hill-climbing local search is to find the set of local moves (changes of instantiation) that most improve the overall solution cost [5].
In a TCSP approach to IA, a change of instantiation means changing the status of a basic relation contained in an interval formula: either it has currently been removed from the formula, hence it can be re-included, or it is currently included and can be removed.
In either case we need to calculate the change in the overall solution cost.
The standard CSP approach would be to treat each interval formula as a constraint and to measure cost in terms of unsatisfied constraints.
However, in a TCSP, the time interval end-points are not instantiated and so we cannot obtain a direct measure of the number of unsatisfied constraints.
In fact, unless we infer information about end-point values, we can only measure the consistency of a solution and so can only distinguish between instantiations on the basis of consistency.
This means a local search will need to test for the level of consistency of each competing instantiation to obtain the cost guidance needed to select a move.
As such consistency checking would, at best, be equivalent to solving a problem using existing consistency-enforcing techniques [8], we can conclude that a local search of this sort will not achieve any benefits over existing approaches.
3.3.
End-Point Ordering As Th can easily be expressed in conjunctive normal form (CNF), an obvious alternative for representing the ISAT problem is to translate Th into a propositional satisfiability (SAT) formula.
This would enable the application of existing SAT local search techniques without modification.
However, as Nebel has already pointed out [6], expressing the implicit dependencies between time interval end-points in CNF produces a cubic increase in problem size, making it unlikely that a SAT approach will yield significant benefits.
Consequently, our work has focussed on finding a more compact representation of the ISAT problem that still captures end-point dependencies.
This has resulted in the endpoint ordering model: End-point ordering translates the ISAT problem into a standard CSP, taking the IA interval formulas to be constraints and the time interval end-points to be variables.
The main innovation of our approach is that we define the domain value of each time interval end-point to be the integer valued position or rank of that end-point within the total ordering of all end-points.
For example, consider the following solution S to a hypothetical IA problem: S = X{b}Y [?]
Y {m}Z [?]
Z{bi}X Given the solution is consistent, a set of possible Iinterpretations must exist that satisfy S. One member of this set is given by Ia = (X - = 12, X + = 15, Y - = 27, Y + = 30, Z - = 30, Z + = 45).
For each I-interpretation, In , there must also exist a unique ordering of the timeinterval end-points that corresponds to In .
For example, the ordering of Ia is given by (X - < X + < Y - < Y + = Z - < Z + ) and is shown in the following diagram:  3/4   3/4  - 3/4  X Y Z From this we can assign an integer ordering to each of the end-points, i.e.
(X - = 1, X + = 2, Y - = 3, Y + = 4,  Z - = 4, Z + = 5).
As any I-interpretation can be translated into a unique end-point ordering, it follows that the search space of all possible end-point orderings will necessarily contain all possible solutions for a particular problem.
The advantage of using end-point ordering is that we can now directly determine the truth or falsity of any interval formula whose end-points have been instantiated.
For example, consider the interval formula X{m, o}Y and the instantiation (X - = 2, X + = 4, Y - = 3, Y + = 7).
From Table 1 it follows that X{m, o}Y can be expanded to: ((X - < Y - ) [?]
(X - < Y + ) [?]
(X + = Y - ) [?]
(X + < Y + ))[?]
((X - < Y - ) [?]
(X - < Y + ) [?]
(X + > Y - ) [?]
(X + < Y + ))  and substituting in the end-point order values gives: ((2 < 3) [?]
(2 < 7) [?]
(4 = 3) [?]
(4 < 7))[?]
((2 < 3) [?]
(2 < 7) [?]
(4 > 3) [?]
(4 < 7)) resulting in X{m, o}Y evaluating to true (note, this representation causes several redundant comparisons which are eliminated using the temporal tree constraint representation described in Section 4.2).
As an interval formula or constraint only contains ordering comparisons of the form X{<, =, >}Y , it follows that an end-point ordering is the minimal amount of information required to evaluate such a constraint.
This further implies that an integer domain consisting of all feasible order values for a particular end-point is the smallest possible domain size for that variable that allows unambiguous constraint evaluation.
Going back to our discussion in relation to local search, this is exactly what we required, i.e.
the most compact model that also allows us to evaluate solution cost in terms of violated constraints.
In general, it is not practical to find the smallest possible domain size for each variable, as this would first involve finding all feasible solutions.
However, we can set an upper bound E to the domain size, equal to the total number of end-points in a problem, such that each end-point domain is of the form (1, 2, .., E).
Depending on the method of constraint representation (i.e.
binary or non-binary) we can then further prune domains using standard pre-processing techniques such as arc- and path-consistency.
In summary, the end-point ordering model expresses ISAT as the problem of ordering the end-points of each timeinterval such that all the interval formulas in the problem are satisfied.
We define the problem of deciding whether such an ordering exists as the OSAT problem and an Ointerpretation as a mapping of time order positions onto time interval end-points.
As every I-interpretation has a corresponding O-interpretation, it follows that iff a solution exists to the OSAT problem (i.e.
it is O-satisfiable) there necessarily exists an I-interpretation that satisfies Th (i.e.
it is also I-satisfiable).
4.
Solving OSAT using Local Search 4.1.
Constraint Weighting Local Search A local search differs from a constructive technique (such as backtracking) as the search begins with a complete, but inconsistent, instantiation of variables.
It then proceeds to repair the solution by making a series of local moves that minimise the overall cost [5].
The crucial questions for a local search are: how to measure solution cost, choosing a local move operator and what to do when no local move exists that can reduce the overall cost.
For OSAT the solution cost has already been defined, i.e.
it is a count of the number of false interval formulas for a given variable instantiation.
However, the question of defining a local move is still open: In a standard binary CSP, a move involves changing values for a single variable.
When applied to end-point ordering this approach would search by changing single end-points.
Alternatively we can define a move in terms of intervals and search by simultaneously changing the interval start and end points.
This interval domain approach tries every possible position for a given interval, ensuring that the best domain value pairs are found, but also performing a greater number of comparisons.
In preliminary tests the improved guidance of the interval domain outweighed the comparison cost and so we continued with this approach in our final algorithm.
To deal with situations where no improving move exists we have adopted the general DLM SAT trap escaping strategy proposed in [10].
We chose DLM as it represents the current state-of-the-art for SAT problems and can be simply adapted to the general CSP domain.
DLM escapes traps by adding weight to all currently violated constraints.
Cost is measured as the sum of weights on violated constraints, hence adding weight changes the cost surface of the problem, producing alternative cost reducing moves.
In addition, DLM periodically reduces constraint weights to avoid losing sensitivity to local search conditions.
The TSAT algorithm (see Figure 1) applies the basic DLM heuristics to the temporal reasoning domain, and is controlled by three parameters: MAX FLATS (set to 4) which specifies how many consecutive non-improving (flat) moves can be taken before constraint weights are increased, MAX WEIGHTS (set to 10) which specifies how many constraint weight increases can occur before the weights are reduced and MAX FLAT WEIGHTS (set to 50) which specifies how many consecutive weight increases can occur without an improving move before the search is randomly restarted 1 .
4.2.
Temporal Tree Constraints Although there are 213 possible disjunctions of the 13 basic IA relations, evaluating these disjunctions as interval 1 DLM  does not use a random restart strategy  procedure TSAT Randomly instantiate every event (e- , e+ ) [?]
Events i i Cost - number of unsatisfied constraints in Events F latM oves - F latW eights - W eightIncreases - 0 while(Cost > 0) StartCost - Cost M oves - [?]
for each (e- , e+ ) [?]
Events do i i ) let Di be the domain of (e- , e+ i i + for each domain pair (d- , d ) [?]
Di do ij ij - d+ ) , e+ T estCost - cost of (e- - d- ij i i ij if T estCost < Cost then Cost - T estCost M oves - [?]
end if if T estCost = Cost then add (d- , d+ ) to M oves ij ij end for Randomly select and instantiate (d- , d+ ) [?]
M oves ij ij end for if Cost < StartCost then F latM oves - F latW eights - 0 else if (++F latM oves) > MAX FLATS then increment weight on all unsatisfied constraints increase Cost by the number of unsatisfied constraints F latM oves - 0 if (++W eightIncreases) > MAX WEIGHTS then decrement weight on all constraints with weight > 1 decrease Cost by number of decremented constraints W eightIncreases - 0 else if (++F latW eights) > MAX FLAT WEIGHTS then Randomly instantiate every event (e- , e+ )[?
]E i i Cost - number of unsatisfied constraints in E F latW eights - W eightIncreases - 0 end if end if end while end  Figure 1.
The TSAT Local Search Algorithm for Temporal Reasoning  end-point constraints is relatively easy.
This is because all constraints involve four basic evaluations: ((X - {r}Y - ), (X - {r}Y + ), (X + {r}Y - ), (X + {r}Y + )) where r = {<, =, >} and any fully instantiated pair of intervals must satisfy a single basic relation [6].
This is illustrated in the comparison tree of Figure 2: here all constraints that evaluate true follow a single path from root to leaf, skipping the bracketed comparisons (as these are implied by X - < X + or Y - < Y + ).
For example, the shortest path to b (assuming the best ordering) is given by: (X - < Y - ) [?]
(X + < Y - ) as (X - < Y - ) - (X - < Y + ) and (X + < Y - ) - (X + < Y + ).
Similarly, the longest path to oi (assuming the worst ordering) is given by: !
(X - < Y - ) [?]
!
(X - = Y - ) [?]
!
(X - = Y + )[?]
!
(X - > Y + ) [?]
!
(X + < Y + ) [?]
!
(X + = Y + ) Using interval formulas, we can construct comparison trees for each member of the subset of the 213 possible disjunctions that appear in a particular problem.
We term this type  X- X- X+  >>>> ?XXXX >>>>>> XX >> >> 9 z > X < = Y- (c) ?
?
(c) ?
R @  1/4  (c) (< ) (< ) < = > Y+ H a?H !
?
?
?
?
j> H < = (> ) (> ) (> ) (> ) Y - ?
?
!
a?
R @  X + (< ) (< ) < b m o  = fi  > di  !
a?
R @  < s  = eq  > si  !
a?
R @  < d  = f  > oi  ?
?
(> ) (> ) Y + mi bi  Figure 2.
End-point Comparison Tree for the 13 Basic Relations  X- X-  <  >>>> 9 >>  ?
a HH !
j H  >>>>>>  >> XX XX  XX z X  (< )  X+ <  ?
X + (< ) b m  ?
>  fi  s  eq  si  d  f  Y+  ?
@ R di  Y-  (> ) Y -  (> )  !
a  < o  >  (c)(c) @ R  1/4  (c) < >  > oi  ?
(> ) Y + mi bi  Figure 3.
The Temporal Tree Constraint for X{b, bi, o, oi}Y  of constraint representation a temporal tree constraint.
Processing these trees we can then detect failure with fewer comparisons, leaving the best and worst cases for success unchanged.
The tree in Figure 2 represents the temporal tree constraint for all 13 possible disjunctions between X and Y and so is redundant (i.e.
X and Y are unconstrained).
Figure 3 shows the more useful temporal tree constraint for X{b, bi, o, oi}Y .
Here we can see that an instantiation of X - = Y - will fail at the first level and no further processing of the tree will occur.
An alternative method of constraint representation would be to express the problem as a true binary CSP, developing binary constraint extensions representing all possible combinations of end-points for a given pair of intervals.
In such a model a constraint could be evaluated in a single look-up.
However, we rejected this approach due to the large space overhead required.
4.3.
Results The TSAT algorithm is specifically intended to solve problems which are too large or difficult for a standard backtracking and path-consistency approach.
To address this we set out to create a problem set on which backtracking has difficulty.
Using Nebel's generator [6], we firstly created two large sets of random, consistent problems.
The  first set was made up of 40 node problems with degree = 75% and label size = 9.5.
In the second set the number of nodes was increased to 80.
We then ran both test sets on Nebel's backtracking-based problem solver [6] until 100 problems were found in each set that backtracking had failed to solve (the 40 node problems were timed out after 5 minutes and the 80 node after 10 minutes).
We then solved each of these problems 10 times using the TSAT algorithm (all experiments were conducted on a Intel Pentium Celeron 450MHz machine with 160Mb of RAM running FreeBSD 4.2).
The graphs in Figures 4 and 5 show the proportion of problems solved against the average run-times for TSAT.
Each problem is considered solved if an answer is found in at least one of the 10 runs of TSAT.
The graph run-times are then calculated by dividing the total time taken for all 10 runs by the number of successful runs (in parallel with Nebel's algorithm, TSAT was timed out at 5 minutes for the 40 node problems and 10 minutes for 80 nodes).
As TSAT solved all problems in less than 10 runs, both graphs show 100% success, whereas the % solved value in Table 2 reports the overall success rate for the 1000 runs on each problem set.
The 40 node problem results indicate that TSAT finds these instances relatively easy, with 100% of problems solved within 20 seconds and a median run-time of 4.11 seconds.
This is in contrast to Nebel's algorithm which failed to solve any of these problems after 300 seconds.
As would be expected, the 80 node problems proved harder for TSAT, with 18% failure at 600 seconds and a median run-time of 215 seconds.
However, on average, TSAT is still able to solve any of the randomly generated 80-node instances in less than the 600 second time-out at which Nebel's algorithm was terminated.
While it can be argued that we have chosen just those problems on which Nebel's algorithm has difficulty, and hence that our comparison is biased, our intention is not to show that TSAT is superior to backtracking in all situations, just in those cases where backtracking has difficulty.
If the lessons from SAT are carried across to the temporal reasoning domain, we would expect backtracking to be better on smaller/easier problems and always to be used when the objective is to prove inconsistency.
The current TSAT algorithm represents a first pass at using local search for temporal reasoning and so has not been optimised for the temporal reasoning domain.
For instance, the TSAT parameter settings were taken directly from the SAT algorithm from which it was developed.
Also, given the relatively small number of average moves taken during the search (224 for 40 nodes and 1235 for 80 nodes), it is unlikely the TSAT weight reduction heuristic is having a significant effect.
Finally, TSAT is performing many redundant consistency checks, which we anticipate can be eliminated us-  CPU Time Number of Moves Problem % Mean Median Std Mean Median Std Size Solved Dev Dev 40 nodes 100.0 6.38 4.11 8.43 339 224 418.14 80 nodes 82.0 244.21 215.10 116.48 1415 1235 666.81  100 90 80  percent solved  70 60 50  Table 2.
Average TSAT results for 1000 runs on each problem set  40 30 20 10 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 time (sec.)
References  Figure 4.
TSAT plot for 40 nodes 100 90 80  percent solved  70 60 50 40 30 20 10 0 150  190  230  270  310 350 time (sec.)
390  430  pecially promising for over-constrained temporal reasoning problems, where standard consistency-checking techniques become ineffective [2, 3].
470  Figure 5.
TSAT plot for 80 nodes ing simple domain skipping heuristics.
We are currently addressing all these areas and expect to present a more detailed empirical study of the relative merits local search for temporal reasoning in our future work.
5.
Conclusion In conclusion, the paper has demonstrated that an endpoint ordering local search approach to temporal reasoning is both feasible and practical.
The TSAT algorithm is a first indication that local search can outperform the traditional TCSP backtracking approach on larger, more difficult problems.
Our work opens up several avenues for further research.
Firstly, we have not explored alternative local search heuristics, such as tabu search or random walk.
Also the TSAT algorithm can be further improved with the use of domain skipping techniques that avoid redundant tests and we have yet to perform an exhaustive search for the optimum TSAT parameter settings.
Finally, local search appears es-  [1] J. Allen.
Maintaining knowledge about temporal intervals.
Communications of the ACM, 26(11):832-843, 1983.
[2] M. Beaumont, A. Sattar, M. Maher, and J. Thornton.
Solving over-constrained temporal reasoning problems.
In Proceedings of the 14th Australian Joint Conference on Artificial Intelligence (AI 01), pages 37-49, 2001.
[3] E. Freuder and R. Wallace.
Partial constraint satisfaction.
Artificial Intelligence, 58(1):21-70, 1992.
[4] A. Mackworth.
Constraint satisfaction.
Technical report, TR-85-15, University of British Columbia, Vancouver, Canada, 1985.
[5] S. Minton, M. Johnston, A. Philips, and P. Laird.
Minimizing conflicts: a heuristic repair method for constraint satisfaction and scheduling problems.
Artificial Intelligence, 58:161-205, 1992.
[6] B. Nebel.
Solving hard qualitative temporal reasoning problems: Evaluating the efficiency of using the ORD-Horn class.
Constraints, 1:175-190, 1997.
[7] B. Nebel and H. Burckert.
Reasoning about temporal relations: A maximal tractable subclass of Allen's interval algebra.
Journal of the ACM, 42(1):43-66, 1995.
[8] E. Schwalb and L. Vila.
Temporal constraints: A survey.
Constraints, 3:129-149, 1998.
[9] B. Selman, H. Levesque, and D. Mitchell.
A new method for solving hard satisfiability problems.
In Proceedings of the Tenth National Conference on Artificial Intelligence (AAAI92), pages 440-446, 1992.
[10] Y. Shang and B. Wah.
A discrete Lagrangian-based global search method for solving satisfiability problems.
J.
Global Optimization, 12:61-99, 1998.
[11] P. van Beek and D. Manchak.
The design and an experimental analysis of algorithms for temporal reasoning.
Journal of AI Research, 4:1-18, 1996.
[12] M. Vilain and H. Kautz.
Constraint propagation algorithms for temporal reasoning.
In Proceedings of the Fifth National Conference on Artificial Intelligence (AAAI-86), pages 377- 382, 1986.
Probabilistic Temporal Interval Networks Vladimir Ryabov University of Jyvaskyla P.O.
Box 35 Jyvaskyla, FIN-40351, Finland vlad@it.jyu.fi  Abstract A Probabilistic Temporal Interval Network is a constraint satisfaction problem where the nodes are temporal intervals and the edges are uncertain interval relations.
We attach a probability to each of Allen's basic interval relations.
An uncertain relation between two temporal intervals is represented as a disjunction of Allen's probabilistic basic relations.
Using the operations of inversion, composition, and addition, defined for this probabilistic representation, we present a path consistency algorithm.
1.
Introduction A Constraint Satisfaction Problem (CSP) [9] can be represented by a finite set of variables (or nodes), their associated domains, and a set of constraints on these variables.
The domain of a variable is the set over which the variable takes its values.
Each element of the domain is called a label.
Solving a CSP consists of finding assignments of labels to variables that satisfy the given constraints.
A Probabilistic Temporal Interval (PTI) network is a special type of CSP.
A PTI network consists of a set of nodes (temporal intervals) and the edges represent the uncertain relations between them.
An uncertain relation between two intervals is a set of Allen's [1] basic relations, where a probability is attached to each basic relation.
We re-define three reasoning operations: inversion, composition, and addition.
A standard pathconsistency algorithm is modified to deal with uncertain interval relations.
Due to a lack of space, we omit our algorithm for finding consistent scenarios in PTI networks using a backtracking algorithm, heuristic methods for optimizing the algorithm's performance, and methods for computing the probability of a consistent scenario using the probabilities associated with Allen's basic relations on each edge.
A complete  Andre Trudel Jodrey School of Computer Science Acadia University Wolfville, Nova Scotia, B4P 2R6, Canada Andre.Trudel@AcadiaU.ca  version of this paper which includes omitted algorithms appears as a technical report [8].
Our path-consistency algorithm, backtracking algorithm, and ordering heuristics are partially based on van Beek and Manchak's work [3].
Our probabilistic representation is more general than their standard temporal representation.
Our paper can be viewed as an extension of the work presented in [3].
Other formalisms for handling uncertainty, such as possibility theory, have been applied to temporal representation and reasoning.
A fuzzy extension of Allen's Interval Algebra was proposed in [2].
In that paper, a possibility theory was utilized to model uncertain temporal relations by assigning a preference degree to every basic Allen's relation within an uncertain interval relation.
Three reasoning operations, namely, inverse, conjunctive combination (analogous to our operation of addition), and composition were defined for that representation.
A path-consistency algorithm for interval-based networks with fuzzy temporal constraints has been proposed, and a tractable sub-algebra has been identified.
In addition to the above, other related work is: the paper of Ladkin and Reinefeld [5] describes algebraic methods for interval constraint problems; a theoretical evaluation of selected backtracking algorithm was presented in [4]; a description of backtracking algorithms for disjunctions of temporal constraints in [10], and analysis of symbolic and numeric constraint satisfaction techniques for temporal reasoning [6].
In Section 2 we define uncertain interval relations and present reasoning operations.
In Section 3 we define PTI networks, give an example of such a network, and describe the types of input that are accepted by the algorithms presented in this paper.
In Section 4 we define a path consistency algorithm.
Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIME'04) 1530-1311/04 $20.00 (c) 2004 IEEE  2.
Uncertain Interval Relations In this section, we define uncertain interval relations and the reasoning operations inversion, composition, and addition.
We denote temporal intervals with capital non-bold letters, i.e.
A, B.
The relation between two intervals is denoted with a subscripted capital letter R. For example, the relation between intervals A and B is written as RA,B.
There are thirteen basic mutually exclusive relations [1] that can hold between two temporal intervals.
The set of these relations is denoted as X={eq, b, bi, d, di, o, oi, m, mi, s, si, f, fi}.
We refer to an element of this set as kh[?]X.
An uncertain relation between two temporal intervals is represented as a set of probabilities of all the basic relations that can hold between them.
The probability of a basic temporal relation between two intervals is further denoted using the letter "e" with a superscript indicating the basic relation and possibly a subscript indicating the intervals, e.g., eAeq, B .
The uncertain relation between intervals A and B is written as RA,B={ekh"kh[?]X}.
The set RA,B has a cardinality of 13, one entry for each of Allen's basic temporal relations.
The probabilities in RA,B sum to 1.
For example, RA,B={eeq=0.5, eb=0.2,ebi=0.3} means that the relationship between intervals A and B is "eq [?]
b [?]
bi" and, "eq" is the sole relationship between intervals A and B with probability 0.5.
Similarly for "b" with probability 0.2 and "bi" with 0.3.
Note that in this and all subsequent examples, zero entries are omitted.
For example, "m" has a probability of 0 of being the relationship between A and B.
The operation of inversion (~) derives the relation ~ RB,A when the relation RA,B is defined, and RB,A = RA , B .
Given the probability values eAkh , B , the probability values eBkh, A are calculated according to the inversion table for Allen's interval relations [1], i.e.
eBoi, A = eAo , B .
For example, the inverted relation for RA,B={eeq=0.05, eb=0.2, ebi=0.1, ed=0.35, edi=0.01, eo=0.2, eoi=0.09}is RB,A={eeq=0.05, eb=0.1, ebi=0.2, ed=0.01, edi=0.35, eo=0.09, eoi=0.2}.
The operation of composition ([?])
derives the relation RA,C, when the relations RA,B and RB,C are defined, and RA,C=RA,B[?]RB,C.
We assume that the probability values  eA,kh B and eB,kh C , where kh[?
]X, are  known.
The probability values  kh  eA ,C are calculated  according to the algorithm for composition (Figure 1) presented in [7].
1. e khA,C =0, where kh[?
]X; 2. for i=1 to 13 do 3. for j=1 to 13 do 4. begin 5.
X'={kh1,kh1,...,khm}, where X' is a set of all Allen's relations which are possible between A and C when e khAi,B and kh  e B,j C are combined; 6.  for k=1 to m do  7.  e khA,kC = e khA,kC +  1 khi khj e A, B e B,C m  //khk[?
]X'; khi,khj [?
]X; 8.  end.
Figure 1.
Composition algorithm [7]  The algorithm in Figure 1 considers all possible combinations of the probability values from RA,B and RB,C.
For example, the result of the standard nonprobabilistic composition of "b" and "d" is {b,d,o,m,s}.
We need to distribute the probability eAb , BeBd ,C between the values from the set {b,d,o,m,s}.
For example, the composition of the two uncertain relations RA,B={eeq=0.3,eb=0.7} and RB,C={ed=0.5,eo=0.5} b d results in RA,C={e =0.42, e =0.22, eo=0.22, em=0.07, es=0.07}.
The operation of addition ([?])
combines the relations RA' ,B and RA'' ,B into a single relation RA,B.
In this case, we write RA,B =  RA' ,B [?]
RA'' ,B .
We use the  algorithm from [7] for performing addition (Figure 2).
For example, the addition of two uncertain relations RA' ,B ={eeq=0.3,eb=0.5,eo=0.2} and RA'' ,B ={eeq=0.05, ed=0.2, eo=0.75} is RA,B={eeq=0.214, eo=0.786}.
1. e khA, B =0, where kh[?
]X; 2. for i=1 to 13 do 3.  e  khi A, B  e'Akh,iB e'A' kh, Bi , //where e'A,khiB from = khi khi e'A, B + e'A' , B // R 'A ,B and e'A' kh, Bi from R 'A' ,B ;  4. for i=1 to 13 do 5.  e  khi A ,B  e khAi ,B = .
| e khA,B kh[?
]X  Figure 2.
Addition algorithm [7]  Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIME'04) 1530-1311/04 $20.00 (c) 2004 IEEE  3.
PTI Networks A PTI network N is a directed graph where the nodes represent intervals and the arcs represent the uncertain temporal relations between these intervals.
We represent such a graph as a set of n variables (intervals) V={v1,v2,...,vn} and the relations between them as R v , v ={ekh"kh[?
]X}, where vi,vj[?]V.
The set of all i  j  uncertain temporal relations for the network N is denoted as Ps.
For example, the PTI network N shown in Figure 3 has 4 intervals A, B, C, D, and 5 uncertain relations between them RA,B={eeq=0.3, eb=0.7}, RB,C={eb=0.5, ebi=0.5}, RA,C={eb=1}, RB,D={eb=1}, and RC,D= {eb=0.2, ebi=0.8}.
Note that there is no edge between A and D. In this case, we assume it is a totally uncertain relation with all possible entries having equal probability values.
In later sections, we define a path consistency and a backtracking algorithm for PTI networks.
The algorithms accept three types of input: A PTI network: The interval relations within this network include probabilistic values for Allen's relations (e.g., the network shown in Figure 3).
{eeq=0.3,eb=0.7}  B  {eb=0.5,ebi=0.5}  {eb=1}  A  C  {eb=1}  {eb=0.2,ebi=0.8}  D Figure 3.
The PTI network N A standard qualitative temporal CSP: In this case, the network does not include probability values and needs to be converted to a PTI network.
We assume that the Allen relations contained in the label on an edge are equally likely.
For example, if we have the label {eq,b} on an edge, we convert this to a PTI with {eeq=0.5,eb=0.5}.
In general, each of the n entries in a label are assigned the probability 1/n.
A qualitative temporal CSP with preferences: Assume we have a standard qualitative temporal CSP.
In addition, we are given relation preferences for each edge.
For example, if we have the label {b,m,o} and we also know that b is preferred over m and o.
There is no preference between m and o.
The PTI network label becomes {eb=2/(2+1+1), em=eo=1/(2+1+1)}, i.e.
{eb=0.5, em=0.25, eo=0.25}.
In general, we create a  partial order and rank each element in the order.
The smallest ranked element is assigned a rank of 1.
The probability assigned to an element is its rank over the sum of the ranks.
4.
Path Consistency Algorithm The path consistency algorithm (PC-algorithm) can be used to test an Interval Algebra (IA) network for consistency as was proposed by Allen [1], as well as a part of the backtracking search algorithm ([5] and [3]).
In this section we present a PC-algorithm (shown in Figure 4) adapted to PTI networks.
Note that our PCalgorithm is almost identical to the one in [3].
We use probabilistic versions of inversion, addition, composition, and the test conditions in lines 7 and 14 in Figure 4.
As the name implies, the PC-algorithm repeatedly checks for 3-consistency for every possible three nodes i, j, and k. The values of the uncertain relations Ri,j and Rj,k potentially constrain the value of the relation Ri,k.
Using the operations of composition and addition we compute a possible value for the relation Ri,k using the triangle t=Ri,k[?](Ri,j[?]Rjk).
Analogous to standard qualitative temporal CSPs, Ri,k and t have the following properties: If ekh is zero in Ri,k then the same entry is also zero in t. If ekh is non-zero in t then the same entry is also non-zero in Ri,k.
From the above, t has the same number or fewer zero entries as Ri,k.
Also, all non-zero entries in t are also non-zero in Ri,k.
If the derived relation t is more certain than the initial value of Rik, we update Rik with t. The derived relation is more certain than the initial one if: It has more zero entries (probability values for the basic relations).
Or, the relations are not equal and have the same number of zero entries, but the initial relation is lexicographically smaller than the derived one, when the entries are ordered in descending order.
To illustrate the latter case, let us consider an example: Ri,k = {eeq=0.2,eb=0.5,em=0.3} and t = {eeq=0.25, eb=0.25, em=0.5}.
Ordering the entries in descending order we obtain Ri,k={eb=0.5,em=0.3,eeq=0.2} and t={em=0.5, eeq=0.25, eb=0.25}.
The maximum entries are equal to 0.5; therefore we need to compare the next ones.
The second entry of 0.3 for Ri,k is bigger than 0.25 for t, so we conclude that the relation Ri,k is more certain than t. In this case, Ri,k would not be updated.
Let us underline, that such a lexicographical comparison is utilized only when the two relations are not equal and have the same number of zero entries.
The described procedure is also performed to tighten the relation Rj,k in a similar way.
The motivation for the  Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIME'04) 1530-1311/04 $20.00 (c) 2004 IEEE  lexicographic comparison is to favor relations with comparatively larger probabilities.
The computational complexity of the path consistency algorithm is O(n3) when counting composition operations as taking unit time.
As it was pointed out by many authors (e.g., [3] and [5]), for an implementation of the path consistency algorithm to be efficient, the reasoning operations used must be efficient.
Particularly, the time performance of the algorithm in Figure 4 strongly depends on the method of calculating the composition of relations.
1.
L - {(i,j) | 1 <= i < j <= n} 2. while (L [?]
{[?]})
do 3. select and delete an (i,j) from L 4. for k = 1 to n do 5. if (k [?]
i) AND (k [?]
j) do 6. t = Ri,k [?]
(Ri,j [?]
Rjk); 7. if (t has more zero entries than Ri,k) OR ((t [?]
Ri,k) AND (t has the same number of 8. zero entries as Ri,k) AND (Ri,k is lexicographically smaller than t, when entries in Ri,k and t are ordered in descending order)) 9. then do 10.
Ri,k = t; 11.
Rk,i = Inverse (t); 12.
L = L [?]
{(i,k)}; 13. t = Rk,j [?]
(Rk,i [?]
Ri,j); 14 if (t has more zero entries than Rk,j) OR ((t [?]
Rk,j) AND (t has the same number of 15. zero entries as Rk,j) AND (Rk,j is lexicographically smaller than t, when entries in Rk,j and t are ordered in descending order)) 16. then do 17.
Rk,j = t; 18.
Rj,k = Inverse (t); 19.
L = L [?]
{(k,j)}; Figure 4.
Path-consistency algorithm for PTI networks  Our results are theoretical, and experiments will be carried out in the near future.
After implementing the algorithms, we will test them on PTI networks.
The relative speed and the order that the scenarios are generated will be studied.
6.
References [1] Allen, J.: Maintaining Knowledge about Temporal Intervals.
Communications of the ACM 26(11) (1983) 832843.
[2] Badaloni, S., Giacomin, M.: A Fuzzy Extension of Allen's Interval Algebra, In Proceedings of the 6-th Congress of the Italian Association for AI, Lecture Notes in Artificial Intelligence 1792 (2000) 155-165.
[3] van Beek, P., Manchak, D.: The Design and Experimental Analysis of Algorithms for Temporal Reasoning, Journal of Artificial Intelligence Research 4 (1996) 1-18.
[4] Kondrack, G., van Beek, P.: A Theoretical Evaluation of Selected Backtracking Algorithms, Artificial Intelligence 89 (1997) 365-387.
[5] Ladkin, P., Reinefeld, A.: Fast Algebraic Methods for Interval Constraint Problems, Annals of Mathematics and Artificial Intelligence 19(3-4) (1997) 383-411.
[6] Mouhoub, M., Charpillet, F., Haton, J.-P.: Experimental Analysis of Numeric and Symbolic Constraint Satisfaction Techniques for Temporal Reasoning, Constraints: An International Journal 3(2-3) (1998) 151-164.
[7] Ryabov, V.: Handling Uncertain Interval Relations, In Proceedings of the 2-nd IASTED International Conference on AI and Applications, ACTA Press (2002) 291-296.
[8] Ryabov, V., Trudel, A.: Probabilistic Temporal Interval Networks: Extended version, Technical Report TR-2004001, Acadia University (2004).
[9] Schwalb, E., Vila, L.: Temporal Constraints: A Survey, Constraints: An International Journal 3(2-3) (1998) 129-149.
[10] Stergiou, K., Koubarakis, M.: Backtracking Algorithms for Disjunctions of Temporal Constraints, Artificial Intelligence 120 (2000) 81-117.
5.
Conclusions We defined a PTI network whose nodes represent temporal intervals and edges represent the uncertain relations between them.
We then proposed a PTI path consistency algorithm.
Other related algorithms can be found in [8].
Proceedings of the 11th International Symposium on Temporal Representation and Reasoning (TIME'04) 1530-1311/04 $20.00 (c) 2004 IEEE
